{"id": "2512.07834", "pdf": "https://arxiv.org/pdf/2512.07834", "abs": "https://arxiv.org/abs/2512.07834", "authors": ["Yi-Chuan Huang", "Jiewen Chan", "Hao-Jen Chien", "Yu-Lun Liu"], "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "categories": ["cs.CV"], "comment": "Project page: https://yichuanh.github.io/Voxify-3D/", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "AI": {"tldr": "Voxify3D是一个两阶段可微分框架，用于从3D网格自动生成体素艺术，通过正交像素艺术监督、基于补丁的CLIP对齐和调色板约束的Gumbel-Softmax量化，解决了几何抽象、语义保留和离散颜色一致性的冲突需求。", "motivation": "体素艺术在游戏和数字媒体中广泛应用，但从3D网格自动生成面临挑战：现有方法要么过度简化几何结构，要么无法实现像素级精确、调色板约束的体素艺术美学。需要在几何抽象、语义保留和离散颜色一致性之间取得平衡。", "method": "采用可微分两阶段框架：1）正交像素艺术监督消除透视畸变，实现体素-像素精确对齐；2）基于补丁的CLIP对齐在不同离散化级别保持语义；3）调色板约束的Gumbel-Softmax量化，在离散颜色空间进行可微分优化，支持可控调色板策略。", "result": "实验显示优越性能：CLIP-IQA得分37.12，用户偏好77.90%。支持多样化角色生成和可控抽象（2-8种颜色，20x-50x分辨率）。", "conclusion": "Voxify3D成功解决了体素艺术生成中的基本挑战：极端离散化下的语义保留、通过体素渲染实现像素艺术美学，以及端到端的离散优化。该框架在保持语义的同时实现了高质量的体素艺术生成。"}}
{"id": "2512.07833", "pdf": "https://arxiv.org/pdf/2512.07833", "abs": "https://arxiv.org/abs/2512.07833", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "title": "Relational Visual Similarity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page, data, and code: https://thaoshibe.github.io/relsim", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "AI": {"tldr": "该论文提出了关系视觉相似性的概念，旨在超越传统的视觉属性相似性，捕捉图像中元素之间的内在关系和逻辑结构。", "motivation": "现有视觉相似性度量（如LPIPS、CLIP、DINO）仅关注感知属性相似性，无法捕捉人类能够感知的丰富关系相似性。关系相似性是区分人类与其他物种的关键认知能力，但在视觉计算中存在重要空白。", "method": "首先将关系图像相似性形式化为可测量问题：当两幅图像的内部关系或视觉元素之间的功能对应时，即使视觉属性不同，它们也具有关系相似性。然后构建了包含11.4万张图像-标题的数据集，其中标题经过匿名化处理，描述场景的底层关系逻辑而非表面内容。使用该数据集微调视觉语言模型来测量图像之间的关系相似性。", "result": "开发了首个能够测量图像关系相似性的模型，该模型基于底层关系结构而非可见外观来连接图像。研究表明，虽然关系相似性在现实世界中有许多应用，但现有图像相似性模型无法捕捉这种相似性，揭示了视觉计算中的关键空白。", "conclusion": "关系视觉相似性是一个重要但被忽视的研究方向。该研究通过形式化问题定义、构建专门数据集和开发关系相似性模型，为连接图像底层关系结构而非表面外观迈出了第一步，填补了视觉计算中的关键空白。"}}
{"id": "2512.07831", "pdf": "https://arxiv.org/pdf/2512.07831", "abs": "https://arxiv.org/abs/2512.07831", "authors": ["Jiehui Huang", "Yuechen Zhang", "Xu He", "Yuan Gao", "Zhi Cen", "Bin Xia", "Yan Zhou", "Xin Tao", "Pengfei Wan", "Jiaya Jia"], "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "categories": ["cs.CV"], "comment": "Project Website https://jackailab.github.io/Projects/UnityVideo", "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "AI": {"tldr": "UnityVideo是一个统一的多模态多任务学习框架，用于增强世界感知的视频生成，通过联合学习多种模态（分割掩码、人体骨架、DensePose、光流和深度图）和训练范式来提升视频生成的世界理解能力。", "motivation": "当前视频生成模型虽然展现出令人印象深刻的合成能力，但受限于单模态条件约束，限制了它们对世界的整体理解。这源于跨模态交互不足和模态多样性有限，无法全面表示世界知识。", "method": "提出了两个核心组件：(1) 动态噪声化以统一异构训练范式，(2) 带有上下文学习器的模态切换器，通过模块化参数和上下文学习实现统一处理。还贡献了一个包含130万样本的大规模统一数据集。", "result": "通过联合优化，UnityVideo加速了收敛并显著增强了零样本泛化能力。实验表明，UnityVideo在视频质量、一致性以及与物理世界约束的对齐方面都取得了优越表现。", "conclusion": "UnityVideo通过统一的多模态多任务学习框架，有效解决了当前视频生成模型的世界理解限制问题，为世界感知的视频生成提供了新的解决方案。"}}
{"id": "2512.07829", "pdf": "https://arxiv.org/pdf/2512.07829", "abs": "https://arxiv.org/abs/2512.07829", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "AI": {"tldr": "提出FAE框架，通过单层注意力层将预训练视觉编码器特征适配为适合生成的低维潜在空间，同时保持重建和理解能力", "motivation": "现有方法在将理解导向的预训练视觉特征适配到生成友好的潜在空间时存在根本性不匹配：表示编码器需要高维潜在空间来捕捉多样假设，而生成模型需要低维潜在空间来保持注入噪声的保真度", "method": "提出FAE框架，使用单个注意力层将预训练特征转换为低维潜在表示，并耦合两个深度解码器：一个用于重建原始特征空间，另一个用于图像生成", "result": "在ImageNet 256x256上，带CFG的扩散模型达到接近SOTA的FID 1.29（800轮）和1.70（80轮）；不带CFG时达到SOTA的FID 1.48（800轮）和2.08（80轮），展示了高质量和快速学习能力", "conclusion": "FAE框架简单有效，能够将各种自监督编码器特征适配到生成模型中，在类条件和文本到图像基准测试中表现优异，证明了单层注意力层足以实现高质量特征适配"}}
{"id": "2512.07826", "pdf": "https://arxiv.org/pdf/2512.07826", "abs": "https://arxiv.org/abs/2512.07826", "authors": ["Haoyang He", "Jie Wang", "Jiangning Zhang", "Zhucun Xue", "Xingyuan Bu", "Qiangpeng Yang", "Shilei Wen", "Lei Xie"], "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "categories": ["cs.CV"], "comment": "38 pages", "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "AI": {"tldr": "该论文提出了OpenVE-3M，一个用于指令引导视频编辑的大规模高质量数据集，以及相应的基准测试OpenVE-Bench和训练模型OpenVE-Edit。", "motivation": "当前指令引导的图像编辑数据集质量和多样性不断提升，但指令引导的视频编辑领域缺乏大规模高质量数据集，这限制了视频编辑模型的发展。", "method": "构建了包含空间对齐编辑（全局风格、背景变化、局部变化、局部移除、局部添加、字幕编辑）和非空间对齐编辑（摄像机多镜头编辑和创意编辑）两大类别的数据集。通过精心设计的数据流水线和严格的质量过滤生成数据，并建立了包含431个视频-编辑对的基准测试OpenVE-Bench。", "result": "OpenVE-3M在规模、编辑类型多样性、指令长度和整体质量方面超越了现有开源数据集。基于该数据集训练的5B模型OpenVE-Edit在OpenVE-Bench上取得了新的最先进性能，超越了包括14B基线在内的所有先前开源模型。", "conclusion": "OpenVE-3M填补了指令引导视频编辑领域大规模高质量数据集的空白，通过OpenVE-Bench提供了统一的评估基准，展示了训练模型的有效性，为视频编辑研究提供了重要资源。"}}
{"id": "2512.07821", "pdf": "https://arxiv.org/pdf/2512.07821", "abs": "https://arxiv.org/abs/2512.07821", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "AI": {"tldr": "WorldReel是一个4D视频生成系统，能够同时生成RGB视频帧和4D场景表示（点云图、相机轨迹、密集光流映射），实现时空一致的几何和外观建模。", "motivation": "当前视频生成器虽然能达到逼真的视觉效果，但在3D一致性方面存在根本缺陷。需要开发能够原生保持时空一致性的4D视频生成方法，为世界建模提供稳定统一的时空表示。", "method": "采用显式4D场景表示方法，联合生成RGB帧和4D场景表示。通过结合合成数据和真实数据进行训练：合成数据提供精确的4D监督（几何、运动、相机），真实视频贡献视觉多样性和真实感。", "result": "WorldReel在动态场景和移动相机视频生成方面达到了新的最先进水平，在几何一致性、运动连贯性指标上表现优异，减少了视角-时间伪影，优于竞争方法。", "conclusion": "WorldReel将视频生成推向4D一致的世界建模，使智能体能够通过单一稳定的时空表示来渲染、交互和推理场景，为未来的世界建模应用奠定了基础。"}}
{"id": "2512.07820", "pdf": "https://arxiv.org/pdf/2512.07820", "abs": "https://arxiv.org/abs/2512.07820", "authors": ["Prithila Angkan", "Amin Jalali", "Paul Hungler", "Ali Etemad"], "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.", "AI": {"tldr": "提出一种基于图卷积网络的脑电信号表示学习方法GEEGA，通过梯度对齐策略融合频域地形图和时频谱图的多域信息，用于脑机接口应用。", "motivation": "脑电信号具有时间动态性和个体敏感性，导致类间可分性差，传统方法难以有效融合多域信息并解决梯度冲突问题。", "method": "使用图卷积网络融合频域地形图和时频谱图的嵌入表示，结合中心损失和成对差异损失增强类间可分性，采用梯度对齐策略解决多域梯度冲突。", "result": "在BCI-2a、CL-Drive和CLARE三个公开脑电数据集上验证了方法的有效性，消融实验证明了各组件的重要性。", "conclusion": "GEEGA方法通过多域信息融合和梯度对齐策略，有效提升了脑电信号的表示学习能力，为脑机接口提供了更优的特征表示。"}}
{"id": "2512.07819", "pdf": "https://arxiv.org/pdf/2512.07819", "abs": "https://arxiv.org/abs/2512.07819", "authors": ["Shubham S. Kumbhar", "Abhijeet M. Kulkarni", "Panagiotis Artemiadis"], "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "categories": ["cs.RO"], "comment": null, "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "AI": {"tldr": "提出一个用于人形机器人与人类伙伴协作搬运的控制框架，支持平移和旋转运动，包含高层规划器、低层控制器和刚度调节机制", "motivation": "解决人形机器人在协作搬运任务中的控制问题，需要支持多种运动类型（平移、旋转），并确保动态可行性和人机协作质量", "method": "采用三层架构：1）高层规划器使用交互线性倒立摆模型结合导纳模型和MPC生成动态可行的步态规划；2）低层控制器使用基于QP的全身控制器处理耦合的人形-物体动力学；3）刚度调节机制控制机器人与物体的交互", "result": "在Digit人形机器人平台上进行了真实世界实验，验证了框架在平移、转向和半圆形轨迹等协作搬运任务中的有效性，并提出了量化协作效率的指标", "conclusion": "该控制框架能够实现高效合规的人-人形机器人协作搬运，刚度调节在协作任务中起关键作用，提出的效率指标有助于理解协作质量并指导控制设计"}}
{"id": "2512.07818", "pdf": "https://arxiv.org/pdf/2512.07818", "abs": "https://arxiv.org/abs/2512.07818", "authors": ["Xinyuan Cao", "Santosh S. Vempala"], "title": "Provable Long-Range Benefits of Next-Token Prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "66 pages, 5 figures", "summary": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.", "AI": {"tldr": "该论文证明了下一个词预测在理论上具有学习长距离结构的能力，即使使用常见的神经网络架构也能近似训练分布", "motivation": "解释为什么现代语言模型通过下一个词预测训练能够生成连贯文档并捕获长距离结构，提供理论证明而非仅凭经验观察", "method": "使用循环神经网络（RNN）优化下一个词预测，证明在多项式模型大小下可实现k-标记不可区分性", "result": "证明了对于从训练分布中采样的文档，任何有界描述长度的算法都无法区分真实文档的k个连续标记和模型生成的k个标记", "conclusion": "下一个词预测在理论上确实能够学习长距离结构，这为实践中观察到的语言模型长距离连贯性提供了复杂性理论解释"}}
{"id": "2512.07814", "pdf": "https://arxiv.org/pdf/2512.07814", "abs": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "21 pages, 8 figures", "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "AI": {"tldr": "研究代码大语言模型中不同PII类型的隐私风险差异及其与训练动态的因果关系", "motivation": "现有研究将PII视为单一类别，忽视了不同类型PII的异质性风险，需要探究不同PII类型在代码模型中被学习和泄露的可能性差异及其因果关系", "method": "构建包含多种PII类型的数据集，微调不同规模的代表性模型，计算真实PII数据的训练动态，建立结构因果模型来估计可学习性对泄露的因果效应", "result": "泄露风险在不同PII类型间差异显著：易学习的实例（如IP地址）泄露率更高，而较难类型（如密钥和密码）泄露较少；模糊类型表现出混合行为", "conclusion": "首次提供因果证据表明泄露风险具有类型依赖性，为开发类型感知和可学习性感知的LLM4Code防御提供了指导"}}
{"id": "2512.07813", "pdf": "https://arxiv.org/pdf/2512.07813", "abs": "https://arxiv.org/abs/2512.07813", "authors": ["Hari Prakash Thanabalan", "Lars Bengtsson", "Ugo Lafont", "Giovanni Volpe"], "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.", "AI": {"tldr": "提出一种受尺蠖启发的软体机器人，利用图案化基底被动控制运动方向，通过单一卷曲介电弹性体驱动器和沟槽引导实现定向运动。", "motivation": "传统软体机器人需要多个执行器来实现方向控制，这会增加机械复杂性、控制系统难度和能耗。需要一种更简单、节能的方向控制方法。", "method": "采用单个卷曲介电弹性体驱动器作为驱动源，通过3D打印基底上的沟槽图案被动引导机器人对齐和轨迹，通过改变沟槽角度实现精确方向控制。", "result": "系统实验表明，改变沟槽角度能够精确控制运动方向，无需复杂的驱动策略，降低了能耗，简化了机器人设计。", "conclusion": "这种沟槽引导方法减少了能耗，简化了机器人设计，扩展了仿生软体机器人在搜索救援、管道检查和行星探索等领域的应用潜力。"}}
{"id": "2512.07810", "pdf": "https://arxiv.org/pdf/2512.07810", "abs": "https://arxiv.org/abs/2512.07810", "authors": ["Jordan Taylor", "Sid Black", "Dillon Bowen", "Thomas Read", "Satvik Golechha", "Alex Zelenka-Martin", "Oliver Makins", "Connor Kissane", "Kola Ayonrinde", "Jacob Merizian", "Samuel Marks", "Chris Cundy", "Joseph Bloom"], "title": "Auditing Games for Sandbagging", "categories": ["cs.AI"], "comment": "77 pages (28 non-appendix pages), 38 figures", "summary": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .", "AI": {"tldr": "研究通过审计游戏测试AI系统\"故意隐藏能力\"（sandbagging）的检测方法，评估不同检测技术的有效性", "motivation": "未来AI系统可能在评估中故意隐藏其真实能力，误导开发者和审计者，需要开发可靠的检测方法来识别这种欺骗行为", "method": "采用红蓝对抗游戏：红队微调5个模型（部分有条件地表现不佳作为sandbagging代理），蓝队使用黑盒、模型内部或基于训练的方法来识别sandbagging模型", "result": "蓝队无法可靠区分sandbagging模型与良性模型；黑盒方法被有效模仿弱模型的行为击败；线性探针（模型内部方法）更有前景但易受红队植入行为影响；基于训练的能力激发能持续激发sandbagging模型的全部性能，但也会提升良性模型性能，导致假阳性", "conclusion": "短期建议开发者使用分布内训练消除sandbagging；长期需要进一步研究确保基于训练的能力激发的有效性，并开发稳健的sandbagging检测方法"}}
{"id": "2512.07807", "pdf": "https://arxiv.org/pdf/2512.07807", "abs": "https://arxiv.org/abs/2512.07807", "authors": ["Shai Krakovsky", "Gal Fiebelman", "Sagie Benaim", "Hadar Averbuch-Elor"], "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to SIGGRAPH Asia 2025. Project webpage: https://tau-vailab.github.io/Lang3D-XL", "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "AI": {"tldr": "该论文提出了一种名为Lang3D-XL的新方法，用于在大规模3D场景中嵌入语言场，通过将几何与描述性语义联系起来，实现更丰富的空间环境语义理解。", "motivation": "现有的特征蒸馏方法在处理大规模互联网数据时面临语义特征不对齐以及内存和运行时效率低下的挑战，无法有效学习。这限制了在大规模场景中实现语言嵌入3D表示的能力，而这种能力对于更直观的人机交互、自然语言查询/编辑场景以及场景检索、导航和多模态推理等任务至关重要。", "method": "提出了两个关键技术：1）在底层3D高斯表示中引入极低维语义瓶颈特征，通过渲染并传递到多分辨率、基于特征的哈希编码器中处理，显著提高了运行时和GPU内存效率；2）引入衰减下采样器模块并提出几种正则化方法，解决地面真实2D特征的语义不对齐问题。", "result": "在野外HolyScenes数据集上评估了该方法，结果表明在性能和效率方面都超越了现有方法。", "conclusion": "Lang3D-XL通过创新的低维语义瓶颈特征和哈希编码器设计，以及针对语义不对齐问题的正则化方法，成功解决了大规模场景中语言嵌入3D表示的关键挑战，为实现更丰富的空间语义理解和更直观的人机交互提供了有效解决方案。"}}
{"id": "2512.07806", "pdf": "https://arxiv.org/pdf/2512.07806", "abs": "https://arxiv.org/abs/2512.07806", "authors": ["Gyeongjin Kang", "Seungkwon Yang", "Seungtae Nam", "Younggeun Lee", "Jungwoo Kim", "Eunbyung Park"], "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "categories": ["cs.CV"], "comment": "Project page: see https://gynjn.github.io/MVP/", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "AI": {"tldr": "提出MVP（多视图金字塔Transformer）架构，用于从数十到数百张图像中直接重建大规模3D场景，通过局部到全局的视图层次结构和细到粗的空间表示层次结构实现高效且高质量的重建。", "motivation": "现有方法在处理大规模多视图3D场景重建时面临计算效率和表示能力的挑战，需要一种能够同时处理大量视图并保持高质量重建的可扩展架构。", "method": "MVP采用双层次结构：1) 局部到全局的视图间层次结构，从局部视图逐步扩展到组视图再到完整场景；2) 细到粗的视图内层次结构，从详细空间表示逐步聚合为紧凑的信息密集token。结合3D高斯泼溅作为底层3D表示。", "result": "在多个数据集上验证，当与3D高斯泼溅结合时，实现了最先进的泛化重建质量，同时保持高效率和可扩展性，能够适应广泛的视图配置。", "conclusion": "MVP通过创新的双层次Transformer架构成功解决了大规模多视图3D场景重建的效率和表示能力问题，为快速重建复杂大场景提供了有效的解决方案。"}}
{"id": "2512.07805", "pdf": "https://arxiv.org/pdf/2512.07805", "abs": "https://arxiv.org/abs/2512.07805", "authors": ["Yifan Zhang", "Zixiang Chen", "Yifeng Liu", "Zhen Qin", "Huizhuo Yuan", "Kangping Xu", "Yang Yuan", "Quanquan Gu", "Andrew Chi-Chih Yao"], "title": "Group Representational Position Encoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/model-architectures/GRAPE", "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.", "AI": {"tldr": "提出GRAPE（Group RepresentAtional Position Encoding）框架，基于群作用统一位置编码方法，包含乘法旋转（SO(d)）和加法logit偏置（GL群中的幂幺作用）两种机制。", "motivation": "现有位置编码方法（如RoPE、ALiBi等）缺乏统一的理论框架，需要建立基于群作用的统一理论来理解和扩展位置编码机制。", "method": "基于群作用理论，提出两种位置编码机制：1）乘法GRAPE：使用SO(d)群中的旋转操作，位置n作用为G(n)=exp(nωL)，其中L是秩2斜对称生成元；2）加法GRAPE：使用GL群中的幂幺作用，产生加法logit偏置。", "result": "GRAPE框架统一了RoPE和ALiBi作为特例，扩展了位置编码的设计空间，支持学习可交换子空间和非交换混合，在O(d)和O(rd)成本下捕获跨子空间特征耦合。", "conclusion": "GRAPE为长上下文模型中的位置几何提供了原则性的设计空间，统一了现有位置编码方法，并提供了可扩展的几何结构。"}}
{"id": "2512.07802", "pdf": "https://arxiv.org/pdf/2512.07802", "abs": "https://arxiv.org/abs/2512.07802", "authors": ["Zhaochong An", "Menglin Jia", "Haonan Qiu", "Zijian Zhou", "Xiaoke Huang", "Zhiheng Liu", "Weiming Ren", "Kumara Kahatapitiya", "Ding Liu", "Sen He", "Chenyang Zhang", "Tao Xiang", "Fanny Yang", "Serge Belongie", "Tian Xie"], "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "categories": ["cs.CV"], "comment": "Project Page: https://zhaochongan.github.io/projects/OneStory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "AI": {"tldr": "OneStory是一个用于生成连贯多镜头视频的系统，通过自适应内存建模跨镜头上下文，实现一致且可扩展的叙事生成。", "motivation": "现有多镜头视频生成方法难以有效建模长距离跨镜头上下文，因为它们依赖有限的时间窗口或单关键帧条件，导致在复杂叙事下性能下降。现实世界视频中的叙事通常通过多个镜头展开——这些镜头虽然不连续但在语义上相互连接，共同传达连贯的叙事。", "method": "将多镜头视频生成重新定义为下一个镜头生成任务，实现自回归镜头合成，同时利用预训练的图到视频模型进行强视觉条件化。引入两个关键模块：帧选择模块（基于先前镜头的信息帧构建语义相关的全局内存）和自适应条件器（执行重要性引导的分块化以生成紧凑上下文进行直接条件化）。", "result": "在精心策划的60K数据集上从预训练的I2V模型微调后，OneStory在文本和图像条件设置下，在多样化和复杂场景中实现了最先进的叙事连贯性，支持可控和沉浸式的长格式视频叙事生成。", "conclusion": "OneStory通过全局但紧凑的跨镜头上下文建模，实现了连贯且可扩展的叙事生成，解决了现有方法在复杂叙事下的局限性，为可控和沉浸式的长格式视频叙事提供了有效解决方案。"}}
{"id": "2512.07801", "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "提出\"协作因果意义建构\"研究议程，旨在解决人类-AI决策支持中的互补性差距问题", "motivation": "当前LLM智能体在复杂高风险决策支持中未能实现真正的互补性，人类-AI团队表现往往低于最佳个体，专家在验证循环和过度依赖之间摇摆", "method": "提出协作因果意义建构框架，将AI系统设计为认知工作伙伴，维护专家推理模型，帮助表达和修订目标，共同构建和压力测试因果假设，从联合决策结果中学习", "result": "提出了一个研究议程和组织框架，包括训练生态、共同建模表示、交互协议以及以信任和互补性为中心的评价方法等挑战方向", "conclusion": "需要重新构建多智能体系统研究，围绕参与协作意义建构的智能体，使其成为与人类伙伴共同思考的AI队友"}}
{"id": "2512.07796", "pdf": "https://arxiv.org/pdf/2512.07796", "abs": "https://arxiv.org/abs/2512.07796", "authors": ["Sridhar Mahadevan"], "title": "Large Causal Models from Large Language Models", "categories": ["cs.AI"], "comment": "29 pages", "summary": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.", "AI": {"tldr": "提出一种利用大型语言模型构建大型因果模型的新范式，通过DEMOCRITUS系统从LLM中提取跨领域因果知识并组织成统一模型", "motivation": "传统因果推断方法局限于特定领域和假设驱动，需要实验数据；而LLM蕴含丰富的跨领域知识，但缺乏系统化组织，需要新方法从LLM中提取和整合因果知识", "method": "开发DEMOCRITUS系统，包含六个模块：使用高质量LLM提出主题、生成因果问题、提取因果陈述，将碎片化因果声明转化为关系三元组，通过新的范畴机器学习方法整合成大型因果模型", "result": "成功应用DEMOCRITUS于考古学、生物学、气候变化、经济学、医学和技术等多个领域，建立了跨领域的大型因果模型，分析了系统的计算成本瓶颈", "conclusion": "证明了从LLM构建大型因果模型的可行性，提出了新的范畴机器学习方法，识别了系统扩展的瓶颈，并规划了未来扩展方向"}}
{"id": "2512.07795", "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "提出了ReasonBENCH基准测试，用于量化大语言模型推理过程中的不稳定性，通过多轮运行协议评估推理质量和成本的统计可靠性。", "motivation": "当前LLM推理评估主要报告单次运行的准确率，忽略了随机解码带来的内在不确定性，这导致无法可靠评估方法性能的稳定性、可复现性和成本一致性。", "method": "开发了模块化评估库标准化推理框架、模型和任务；采用多轮运行协议报告质量和成本的统计可靠指标；建立公开排行榜鼓励考虑方差的报告。", "result": "大多数推理策略和模型表现出高度不稳定性；即使平均性能相似的策略，置信区间宽度可达四倍差异；性能最佳的方法通常成本更高且更不稳定。", "conclusion": "可复现性是可靠LLM推理的关键维度，ReasonBENCH为未来推理方法和不确定性量化技术提供了基础，强调了考虑推理稳定性的重要性。"}}
{"id": "2512.07778", "pdf": "https://arxiv.org/pdf/2512.07778", "abs": "https://arxiv.org/abs/2512.07778", "authors": ["Sen Ye", "Jianning Pei", "Mengde Xu", "Shuyang Gu", "Chunyu Wang", "Liwei Wang", "Han Hu"], "title": "Distribution Matching Variational AutoEncoder", "categories": ["cs.CV"], "comment": null, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "AI": {"tldr": "提出DMVAE模型，通过分布匹配约束显式对齐编码器潜在分布与任意参考分布，超越传统VAE的高斯先验限制", "motivation": "现有视觉生成模型（如VAE和基础模型对齐编码器）隐式约束潜在空间而不显式塑造其分布，不清楚哪种分布最适合建模", "method": "引入分布匹配VAE（DMVAE），通过分布匹配约束显式对齐编码器潜在分布与任意参考分布，支持自监督特征、扩散噪声或其他先验分布", "result": "自监督学习（SSL）导出的分布在重建保真度和建模效率之间提供最佳平衡，在ImageNet上仅用64个训练周期达到gFID=3.2", "conclusion": "选择合适的潜在分布结构（通过分布级对齐实现），而非依赖固定先验，是弥合易于建模的潜在空间与高保真图像合成之间差距的关键"}}
{"id": "2512.07776", "pdf": "https://arxiv.org/pdf/2512.07776", "abs": "https://arxiv.org/abs/2512.07776", "authors": ["Maximilian Schall", "Felix Leonard Knöfel", "Noah Elias König", "Jan Jonas Kubeler", "Maximilian von Klinski", "Joan Wilhelm Linnemann", "Xiaoshi Liu", "Iven Jelle Schlegelmilch", "Ole Woyciniuk", "Alexandra Schild", "Dante Wasmuht", "Magdalena Bermejo Espinet", "German Illera Basas", "Gerard de Melo"], "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring", "categories": ["cs.CV"], "comment": "Accepted at WACV 2026", "summary": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species", "AI": {"tldr": "本文提出了GorillaWatch系统，这是一个用于野外大猩猩个体重识别和种群监测的自动化端到端管道，包括检测、跟踪和重识别三个主要模块。", "motivation": "目前监测濒危西部低地大猩猩面临巨大挑战，需要从大量相机陷阱录像中手动重新识别个体，自动化过程的主要障碍是缺乏适合训练鲁棒深度学习模型的大规模野外视频数据集。", "method": "1) 引入三个新数据集：Gorilla-SPAC-Wild（最大野外灵长类重识别数据集）、Gorilla-Berlin-Zoo（跨域重识别评估）、Gorilla-SPAC-MoT（多目标跟踪评估）；2) 提出端到端管道集成检测、跟踪和重识别；3) 引入多帧自监督预训练策略，利用轨迹一致性学习领域特定特征；4) 使用可微分的AttnLRP适应验证模型依赖判别性生物特征而非背景相关性；5) 集成时空约束到标准聚类中以解决无监督种群计数中的过分割问题。", "result": "大规模图像骨干网络聚合特征优于专用视频架构；多帧自监督预训练策略有效学习领域特定特征；可解释性验证确认模型依赖生物特征；无监督种群计数方法通过时空约束缓解过分割问题；所有代码和数据集已公开发布。", "conclusion": "GorillaWatch系统通过引入新数据集、端到端管道和先进方法，实现了对濒危物种的可扩展、非侵入式监测，为野外大猩猩保护提供了有效的自动化解决方案。"}}
{"id": "2512.07775", "pdf": "https://arxiv.org/pdf/2512.07775", "abs": "https://arxiv.org/abs/2512.07775", "authors": ["David Thorne", "Nathan Chan", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "OptMap: Geometric Map Distillation via Submodular Maximization", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "AI": {"tldr": "OptMap是一种几何地图蒸馏算法，通过子模最大化实现实时、应用特定的地图生成，解决了从大量LiDAR数据中选择信息丰富、尺寸受限地图的NP-hard组合优化问题。", "motivation": "自主机器人依赖几何地图来支持各种感知和决策算法，不同算法需要不同尺度的地图以实现最佳性能。LiDAR传感器生成大量几何数据，但选择信息丰富且尺寸受限的地图在计算上具有挑战性，因为这需要解决NP-hard组合优化问题。", "method": "提出OptMap算法，通过子模最大化理论实现几何地图蒸馏。核心是最大化具有递减回报特性的集合函数（子模性），使用多项式时间算法获得近似最优解。提出新颖的子模奖励函数来量化信息量、减少输入集大小并最小化顺序收集数据集中的偏差。此外，提出动态重排序流式子模算法，通过在线近似所有扫描的价值来提高解决方案质量并解决输入顺序偏差。", "result": "在开源和自定义数据集上进行了测试，特别关注长时间映射会话，展示了OptMap的最小计算需求。提供了开源的ROS1和ROS2软件包，可与任何LiDAR SLAM算法配合使用。", "conclusion": "OptMap通过子模最大化理论实现了实时、应用特定的几何地图蒸馏，解决了从大量LiDAR数据中选择信息丰富地图的计算难题，为自主机器人系统提供了高效的地图生成解决方案。"}}
{"id": "2512.07765", "pdf": "https://arxiv.org/pdf/2512.07765", "abs": "https://arxiv.org/abs/2512.07765", "authors": ["Gustavo A. Cardona", "Shubham S. Kumbhar", "Panagiotis Artemiadis"], "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "categories": ["cs.RO"], "comment": "60 pages, 5 figures, 3 tables", "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "AI": {"tldr": "本文综述了物理人形机器人交互(pHHI)领域的研究现状，通过三个核心支柱分析当前技术，提出统一框架和未来研究方向，旨在实现无缝的人机物理交互。", "motivation": "随着人形机器人在非结构化、以人为中心环境中的部署需求增加，需要建立鲁棒、可扩展且自适应的物理人机交互系统。当前研究在建模、控制和意图估计等方面存在分离，缺乏统一框架，限制了实际应用效果。", "method": "采用三支柱分析框架：1) 人形机器人建模与控制；2) 人类意图估计；3) 计算人类模型。建立基于交互模态（直接/间接）和机器人参与程度（辅助/合作/协作）的统一分类法，分析各支柱间的整合机会。", "result": "识别了各领域的关键挑战：全身控制策略处理不确定人类动力学、有限传感下的实时意图推断、考虑人类物理状态变化的建模技术。虽然各领域取得进展，但跨支柱整合仍然有限。", "conclusion": "提出了跨领域方法统一的路径，以构建连贯的交互框架。通过分类法为未来研究提供具体方向，旨在推进鲁棒、安全、直观的物理交互，为人形系统在多样化现实环境中有效理解、预测和协作人类伙伴提供路线图。"}}
{"id": "2512.07761", "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "本文提出RL-MTJail方法，使用强化学习训练攻击者LLM，通过多轮对话对黑盒大语言模型进行自动化越狱攻击，旨在从目标模型中引出有害内容。", "motivation": "现有越狱攻击方法通常依赖单轮优化，不足以学习长期攻击策略。黑盒多轮越狱攻击面临稀疏监督问题，需要更有效的训练方法来提升攻击成功率。", "method": "将问题建模为多轮强化学习任务，直接优化最终轮输出的有害性作为结果奖励。提出两种启发式过程奖励：1)控制中间输出的有害性以避免触发黑盒模型的拒绝机制；2)保持中间输出的语义相关性以避免偏离主题。", "result": "在多个基准测试上的实验结果表明，该方法在多个模型上持续提升了攻击成功率，证明了方法的有效性。", "conclusion": "RL-MTJail方法通过强化学习框架和多轮优化策略，有效解决了黑盒多轮越狱攻击问题，为大语言模型的安全部署提供了重要参考。"}}
{"id": "2512.07760", "pdf": "https://arxiv.org/pdf/2512.07760", "abs": "https://arxiv.org/abs/2512.07760", "authors": ["Menglin Wang", "Xiaojin Gong", "Jiachen Li", "Genlin Ji"], "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "AI": {"tldr": "提出一种用于无监督可见光-红外行人重识别的模态感知偏置缓解和不变性学习方法，通过全局关联和模态不变表示学习解决跨模态匹配问题", "motivation": "无监督可见光-红外行人重识别面临跨模态差异大、难以建立可靠关联的挑战。现有方法通常使用最优传输关联模态内聚类，容易传播局部聚类错误且忽视全局实例级关系", "method": "提出模态感知Jaccard距离缓解模态差异引起的距离偏置，通过全局聚类估计更可靠的跨模态关联；设计\"分割-对比\"策略获取模态特定全局原型，在全局关联指导下对齐这些原型实现模态不变表示学习", "result": "在基准VI-ReID数据集上取得了最先进的性能，显著优于现有方法，验证了方法的有效性", "conclusion": "通过模态感知偏置缓解和不变性学习，能够有效解决无监督可见光-红外行人重识别中的跨模态学习问题，实现可靠的跨模态关联和模态不变表示"}}
{"id": "2512.07756", "pdf": "https://arxiv.org/pdf/2512.07756", "abs": "https://arxiv.org/abs/2512.07756", "authors": ["Mayank Anand", "Ujair Alam", "Surya Prakash", "Priya Shukla", "Gora Chand Nandi", "Domenec Puig"], "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.", "AI": {"tldr": "UltrasODM是一个用于3D自由手超声重建的双流光流Mamba网络框架，通过集成对比排序、光流估计和人在回路层，提供实时不确定性校准和临床指导，提高重建精度和可靠性。", "motivation": "临床超声采集高度依赖操作者，快速探头运动和亮度波动常导致重建误差，降低临床信任度和实用性。需要一种能够提供实时不确定性评估和临床指导的系统来提高重建可靠性。", "method": "提出双流框架：1) 对比排序模块按运动相似性分组帧；2) 光流流与Dual-Mamba时间模块融合用于鲁棒的6自由度位姿估计；3) 人在回路层结合贝叶斯不确定性、临床校准阈值和显著性图。当不确定性超过阈值时，系统发出提示建议纠正措施。", "result": "在临床自由手超声数据集上评估，相比UltrasOM，漂移减少15.2%，距离误差减少12.1%，Hausdorff距离减少10.1%，同时产生每帧不确定性和显著性输出。", "conclusion": "UltrasODM通过强调透明度和临床反馈，提高了重建可靠性，支持更安全、更可信的临床工作流程，为超声重建提供了有效的实时指导系统。"}}
{"id": "2512.07747", "pdf": "https://arxiv.org/pdf/2512.07747", "abs": "https://arxiv.org/abs/2512.07747", "authors": ["Shihao Zhao", "Yitong Chen", "Zeyinzi Jiang", "Bojia Zi", "Shaozhe Hao", "Yu Liu", "Chaojie Mao", "Kwan-Yee K. Wong"], "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "AI": {"tldr": "Unison是一个完全自动、任务通用且低成本的统一理解和生成框架，采用两阶段方案，在极低训练成本下覆盖多种多模态任务，并能自动解析用户意图和任务参数。", "motivation": "现有的多模态统一理解和生成方法存在两大问题：一是自回归方法需要大量计算资源，普通研究者难以承受；二是两阶段方法虽然成本较低，但任务覆盖有限且生成质量不佳。两者都缺乏自动解析输入元信息的能力，需要手动参数配置，不够智能。", "method": "采用两阶段方案：1) 保留预训练的理解和生成模型能力；2) 通过低成本的微调实现模型对齐。框架具备自动解析用户意图的能力，能够确定目标任务类型并准确提取相应任务所需的元信息，实现无需人工干预的全自动多模态任务处理。", "result": "在仅使用50万训练样本和50GPU小时的极低成本设置下，模型能够准确自动识别任务并提取相关参数，在多种理解和生成任务上取得优异性能，包括文本、图像、视频理解，以及文本到视觉内容生成、编辑、可控生成和基于IP的参考生成等任务。", "conclusion": "Unison框架成功解决了现有多模态统一方法的高成本、有限任务覆盖和缺乏自动化的问题，通过低成本的训练实现了广泛的任务覆盖和高质量的生成能力，同时具备全自动的任务解析和执行能力，为普通研究者提供了可行的多模态统一解决方案。"}}
{"id": "2512.07745", "pdf": "https://arxiv.org/pdf/2512.07745", "abs": "https://arxiv.org/abs/2512.07745", "authors": ["Jialv Zou", "Shaoyu Chen", "Bencheng Liao", "Zhiyu Zheng", "Yuehao Song", "Lefei Zhang", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2", "AI": {"tldr": "提出DiffusionDriveV2，一种结合强化学习约束的截断扩散模型，用于端到端自动驾驶，解决现有方法在多样性和高质量之间的权衡问题。", "motivation": "现有基于扩散模型的端到端自动驾驶方法存在模式崩溃问题，倾向于生成保守和同质化的行为。虽然DiffusionDrive使用预定义锚点来划分动作空间并生成多样化轨迹，但其依赖模仿学习缺乏足够约束，导致多样性和一致高质量之间的困境。", "method": "1. 使用尺度自适应乘性噪声促进广泛探索；2. 采用锚点内GRPO管理单个锚点内样本的优势估计；3. 使用锚点间截断GRPO整合不同锚点的全局视角，防止不同意图间的不当优势比较。", "result": "在NAVSIM v1数据集上达到91.2 PDMS，在NAVSIM v2数据集上达到85.5 EPDMS，使用对齐的ResNet-34骨干网络，创造了新记录。实验验证了该方法解决了截断扩散模型在多样性和一致高质量之间的困境，实现了最佳权衡。", "conclusion": "DiffusionDriveV2通过强化学习约束截断扩散建模，显著提升了端到端自动驾驶的整体输出质量，同时保持了其核心高斯混合模型固有的多模态性，解决了现有方法在多样性和高质量之间的权衡问题。"}}
{"id": "2512.07741", "pdf": "https://arxiv.org/pdf/2512.07741", "abs": "https://arxiv.org/abs/2512.07741", "authors": ["Agnes Norbury", "George Fairs", "Alexandra L. Georgescu", "Matthew M. Nour", "Emilia Molimpakis", "Stefano Goria"], "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data", "categories": ["cs.LG", "cs.SD"], "comment": ":J.3; J.4", "summary": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.", "AI": {"tldr": "开发一个基于贝叶斯网络的多模态模型，利用语音和言语特征预测抑郁和焦虑症状水平，并在大规模数据集上进行评估", "motivation": "临床精神病学评估中，医生需要同时关注患者报告的内容和非言语信号（如语调、语速、流畅度等），但整合这些多源信息具有挑战性。当前智能工具尚未在临床实践中得到广泛应用，而贝叶斯网络建模可以解决采用障碍", "method": "使用贝叶斯网络模型，从语音和言语特征中预测抑郁和焦虑症状。模型在大规模数据集（30,135名独特说话者）上进行评估，关注症状级别而非疾病级别的表示，并评估不同输入模态类型的整合和冗余", "result": "模型在抑郁和焦虑预测上表现良好（ROC-AUC分别为0.842和0.831，ECE分别为0.018和0.015），核心个体症状的ROC-AUC均超过0.74。同时评估了人口统计学公平性，并探索了临床有用性指标和心理健康服务用户的接受度", "conclusion": "当提供足够丰富的大规模多模态数据流，并在症状级别而非疾病级别表示常见心理健康状况时，贝叶斯网络模型是构建稳健评估支持工具的原则性方法，能够以透明、可解释的格式提供临床相关输出，并直接适合专家临床监督"}}
{"id": "2512.07738", "pdf": "https://arxiv.org/pdf/2512.07738", "abs": "https://arxiv.org/abs/2512.07738", "authors": ["Dengjia Zhang", "Charles Weng", "Katherine Guerrerio", "Yi Lu", "Kenton Murray", "Alexander Martin", "Reno Kriz", "Benjamin Van Durme"], "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "categories": ["cs.CV"], "comment": "7 pages, 1 figure", "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "AI": {"tldr": "HLTCOE团队在TREC 2025 VQA赛道提出了一种列表式学习框架，通过重新排序候选答案来提升语义精度和排序一致性", "motivation": "传统视频问答系统在生成答案时存在语义精度不足和排序不一致的问题，特别是在需要时序推理和语义消歧的复杂问题上表现不佳", "method": "采用两阶段方法：1）基础多模态模型生成多个候选答案；2）使用新颖的掩码指针交叉熵损失与排序权重训练的模型对候选答案进行重新排序", "result": "实验显示该方法在准确性和排序稳定性方面取得一致提升，特别是在需要时序推理和语义消歧的问题上表现显著改善", "conclusion": "通过将生成式建模与判别式排序相结合，该方法能够产生连贯、细粒度的答案列表，为视频问答系统的答案生成任务提供了有效的解决方案"}}
{"id": "2512.07733", "pdf": "https://arxiv.org/pdf/2512.07733", "abs": "https://arxiv.org/abs/2512.07733", "authors": ["Meng Cao", "Xingyu Li", "Xue Liu", "Ian Reid", "Xiaodan Liang"], "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.", "AI": {"tldr": "提出SpatialDreamer框架，通过强化学习实现主动空间推理，解决MLLMs在需要心理模拟的复杂空间推理任务上的局限性", "motivation": "当前多模态大语言模型在需要心理模拟的复杂空间推理任务上表现有限，现有方法主要依赖被动观察空间数据，缺乏主动的心理意象过程", "method": "提出SpatialDreamer强化学习框架，包含主动探索、世界模型视觉想象和证据基础推理的闭环过程；为解决长序列推理任务中细粒度奖励监督不足的问题，提出Geometric Policy Optimization (GeoPO)，引入树结构采样和具有几何一致性约束的步骤级奖励估计", "result": "在多个具有挑战性的基准测试中取得了高度竞争力的结果，标志着MLLMs在类人主动空间心理模拟方面的关键进展", "conclusion": "SpatialDreamer通过主动心理意象激励空间推理，显著提升了多模态大语言模型在复杂空间推理任务上的表现，为类人空间认知能力的发展提供了重要方向"}}
{"id": "2512.07730", "pdf": "https://arxiv.org/pdf/2512.07730", "abs": "https://arxiv.org/abs/2512.07730", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "categories": ["cs.CV", "cs.AI"], "comment": "WACV 2026", "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "AI": {"tldr": "提出SAVE框架，通过稀疏自编码器驱动的视觉信息增强来缓解多模态大语言模型中的物体幻觉问题", "motivation": "多模态大语言模型虽然取得进展，但仍容易因语言先验和视觉信息丢失而产生物体幻觉问题", "method": "使用稀疏自编码器潜在特征引导模型，通过二元物体存在问答探针识别视觉理解特征，并沿这些特征引导模型", "result": "在标准基准测试中优于最先进的无训练方法，CHAIR_S提升10%，在POPE和MMHal-Bench上也有稳定增益", "conclusion": "SAVE通过增强视觉理解特征有效减少幻觉，抑制不确定物体标记生成并增加对图像标记的关注"}}
{"id": "2512.07729", "pdf": "https://arxiv.org/pdf/2512.07729", "abs": "https://arxiv.org/abs/2512.07729", "authors": ["Aidas Aglinskas", "Stefano Anzellotti"], "title": "Improving action classification with brain-inspired deep networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "AI": {"tldr": "研究提出了一种受大脑启发的深度网络架构，通过分离处理身体姿态和背景信息的专门化流，来改进动作分类性能", "motivation": "传统深度神经网络在动作识别中可能过度依赖背景信息而忽视身体姿态信息，而人类大脑具有专门处理身体和场景的特定区域，能够更有效地利用两种信息源", "method": "1) 比较DNN和人类在三种刺激版本（完整、仅身体、仅背景）上的表现；2) 设计受大脑启发的架构，包含专门处理身体信息和背景信息的分离流", "result": "1) DNN在去除身体的刺激上表现良好，但在去除背景的刺激上接近随机水平；2) 人类在所有三种刺激版本上都能准确识别动作，且在仅身体刺激上表现优于仅背景刺激；3) 大脑启发架构提高了动作识别性能，且其准确性模式更接近人类表现", "conclusion": "受大脑领域特异性启发的深度网络架构能够更有效地利用身体和背景信息，提高动作识别性能并使其表现更接近人类，为构建更鲁棒和人类化的计算机视觉系统提供了新思路"}}
{"id": "2512.07724", "pdf": "https://arxiv.org/pdf/2512.07724", "abs": "https://arxiv.org/abs/2512.07724", "authors": ["Zhengzheng Tang"], "title": "The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic", "categories": ["cs.ET", "cs.AI"], "comment": ":10 pagesACM Class:C.1.3; I.2.6", "summary": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap \"from stochastic ions to deterministic floats,\" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.", "AI": {"tldr": "该论文提出了一种原生脉冲微架构，能够利用具有随机离子动力学的埃尺度通道材料实现比特精确的FP8算术运算，解决了随机模拟硬件与确定性AI工作负载之间的悖论。", "motivation": "金属有机框架（MOFs）等埃尺度通道材料具有原生整合-发放（IF）动力学特性，但它们是随机、模拟的材料，而AI工作负载（如FP8）需要确定性、比特精确的计算。现有神经形态方法通常只能达到近似结果，无法满足Transformer精度标准，因此需要解决\"从随机离子到确定性浮点数\"的鸿沟。", "method": "提出原生脉冲微架构，将噪声神经元视为逻辑原语，引入空间组合流水线和粘性额外校正机制，将线性层延迟降低到O(log N)。", "result": "在所有16,129个FP8对上的验证确认了100%比特精确对齐PyTorch，线性层延迟降低到O(log N)，实现了17倍加速。物理仿真进一步证明了对极端膜泄漏（β≈0.01）的鲁棒性，有效免疫了硬件的随机性。", "conclusion": "该研究成功解决了随机离子通道硬件与确定性AI计算之间的根本矛盾，实现了比特精确的FP8算术，为后硅时代神经形态计算提供了可行的解决方案。"}}
{"id": "2512.07723", "pdf": "https://arxiv.org/pdf/2512.07723", "abs": "https://arxiv.org/abs/2512.07723", "authors": ["Yonggeon Lee", "Jibin Hwang", "Alfred Malengo Kondoro", "Juhyun Song", "Youngtae Noh"], "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 9 figures, AAAI'26 (accepted)", "summary": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.", "AI": {"tldr": "提出基于Transformer的实时到事件模型，用于准确预测电动汽车用户的出发时间，以支持延迟充满电策略，延长电池寿命", "motivation": "电动汽车锂离子电池在长时间高荷电状态下会加速退化，通过延迟充满电直到出发前可以缓解这一问题，但需要准确预测用户出发时间", "method": "提出基于Transformer的实时到事件模型，将每天的时间离散化为基于网格的token序列，利用流式上下文信息而非仅依赖历史模式来预测出发时间", "result": "在包含93名用户的真实世界研究中，使用被动智能手机数据进行评估，该方法能有效捕捉个人日常中的不规则出发模式，性能优于基线模型", "conclusion": "该方法展示了实际部署延迟充满电算法的潜力，有助于可持续交通系统的发展"}}
{"id": "2512.07720", "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "提出ViSA框架，通过结合3D重建模型和实时自回归视频扩散模型，实现从单张输入图像生成高质量上身3D虚拟化身", "motivation": "现有3D虚拟化身生成方法虽然快速且能产生稳定的身体结构，但存在纹理模糊和运动僵硬的问题；而生成式视频模型能合成逼真动态结果，但容易出现身体结构错误和身份漂移等不稳定行为", "method": "使用3D重建模型提供鲁棒的结构和外观先验，引导实时自回归视频扩散模型进行渲染，结合几何稳定性和生成能力", "result": "实验表明该方法显著减少伪影，在视觉质量上相比领先方法有实质性提升，能合成高频逼真细节和流畅动态，同时保持时间一致性", "conclusion": "通过结合3D重建的几何稳定性和视频模型的生成能力，ViSA能够生成具有逼真外观和动态、时间一致运动的高保真数字虚拟化身，为游戏和虚拟现实等实时应用提供鲁棒高效的解决方案"}}
{"id": "2512.07712", "pdf": "https://arxiv.org/pdf/2512.07712", "abs": "https://arxiv.org/abs/2512.07712", "authors": ["Sayak Dutta", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "categories": ["cs.CV"], "comment": "9 pages, 2 figures, 2 tables. Accepted to the Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP 2025), Mandi, India", "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "AI": {"tldr": "提出UnCageNet三阶段预处理流程，用于解决笼子结构和系统性遮挡对动物追踪与姿态估计性能的影响，通过笼子分割、修复和评估来提升现有方法的性能。", "motivation": "现有的动物追踪和姿态估计系统（如STEP和ViTPose）在处理带有笼子结构和系统性遮挡的图像和视频时，性能会显著下降。笼子遮挡严重影响了关键点检测和轨迹一致性。", "method": "三阶段预处理流程：1) 使用Gabor增强的ResNet-UNet架构进行笼子分割，包含72个方向可调滤波器；2) 使用CRFill进行笼子修复，实现内容感知的遮挡区域重建；3) 在修复后的无笼帧上评估姿态估计和追踪性能。", "result": "实验验证表明，通过该流程去除笼子遮挡后，姿态估计和追踪性能可达到与无遮挡环境相当的水平。关键点检测精度和轨迹一致性均有显著提升。", "conclusion": "提出的三阶段预处理流程能有效解决笼子遮挡问题，使现有动物追踪和姿态估计方法在笼子环境下也能获得良好性能，为动物行为研究提供了实用解决方案。"}}
{"id": "2512.07710", "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "提出CompassMax-V3-Thinking百亿级MoE推理模型，基于\"每个提示都很重要\"原则构建新的强化学习框架，解决大规模RL训练中的效率问题", "motivation": "将强化学习扩展到百亿级规模时暴露出关键效率问题：零方差提示浪费rollouts、长视野重要性采样不稳定、标准奖励模型导致优势反转、rollout处理中的系统性瓶颈", "method": "引入四项统一创新：1)多阶段零方差消除过滤非信息性提示；2)ESPO熵自适应优化平衡token级和序列级重要性采样；3)路由器重放策略对齐训练和推理时MoE路由器决策；4)高吞吐RL系统采用FP8精度rollouts、重叠奖励计算和长度感知调度", "result": "构建了使百亿级MoE模型RL训练稳定高效的完整流程，最终模型在内部和公共评估中都表现出强大性能", "conclusion": "通过\"每个提示都很重要\"原则指导的系列创新，成功解决了大规模MoE模型强化学习训练的关键挑战，实现了稳定高效的百亿级模型训练"}}
{"id": "2512.07705", "pdf": "https://arxiv.org/pdf/2512.07705", "abs": "https://arxiv.org/abs/2512.07705", "authors": ["Saroj Gopali", "Bipin Chhetri", "Deepika Giri", "Sima Siami-Namini", "Akbar Siami Namin"], "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data. This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning. These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.", "AI": {"tldr": "本文研究了基于大语言模型（LLMs）的时间序列数据预测方法，探索了上下文学习、零样本学习和少样本学习在时间序列预测中的应用，并与传统时间序列模型进行了性能比较。", "motivation": "随着预训练基础模型（如大语言模型和Google的时间序列基础模型TimesFM）的发展，研究者希望探索这些基础模型是否能够在时间序列数据分析和预测方面超越现有的ARIMA、Transformer、LSTM和TCN等传统方法。", "method": "研究采用上下文学习、零样本学习和少样本学习方法来训练针对特定应用领域的LLM模型。具体使用OpenAI o4-mini、Gemini 2.5 Flash Lite、Google的TimesFM（基于Transformer的时间序列专用基础模型），以及TCN和LSTM两种深度学习模型进行对比实验。", "result": "TimesFM在整体性能上表现最佳，获得了最低的RMSE值（0.3023）和具有竞争力的推理时间（266秒）。OpenAI的o4-mini在零样本学习方面也表现出良好性能。这些结果表明预训练的时间序列基础模型在实时预测方面具有潜力。", "conclusion": "预训练的时间序列基础模型为实时预测提供了一个有前景的方向，能够在最小化模型适应的情况下实现准确且可扩展的部署。TimesFM在时间序列预测任务中展现出优越性能，而LLMs在零样本学习方面也表现出竞争力。"}}
{"id": "2512.07703", "pdf": "https://arxiv.org/pdf/2512.07703", "abs": "https://arxiv.org/abs/2512.07703", "authors": ["Leo Fillioux", "Enzo Ferrante", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis"], "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.", "AI": {"tldr": "提出PVeRA：一种概率版本的VeRA适配器，通过概率方式修改VeRA的低秩矩阵，以处理输入中的固有模糊性并允许训练和测试期间的不同采样配置", "motivation": "大型基础模型需要大量数据和计算资源进行训练或微调，而适应方法提供计算高效的解决方案。VeRA适配器在参数高效适应方面表现出色，但需要进一步改进以处理输入模糊性", "method": "将VeRA适配器的低秩矩阵进行概率化修改，创建概率版本的PVeRA适配器，允许在训练和测试期间采用不同的采样配置", "result": "在VTAB-1k基准测试中评估了七个适配器，PVeRA在性能上超越了VeRA和其他适配器", "conclusion": "PVeRA通过概率化修改VeRA的低秩矩阵，有效处理输入模糊性，在参数高效适应任务中取得了优越性能"}}
{"id": "2512.07702", "pdf": "https://arxiv.org/pdf/2512.07702", "abs": "https://arxiv.org/abs/2512.07702", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "WACV 2026", "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "AI": {"tldr": "提出自动化负提示（NPC）方法，通过识别和应用抑制非预期内容的负提示来改进文本-图像对齐", "motivation": "尽管文本到图像生成取得显著进展，但对于具有丰富组合结构或想象元素的提示，实现精确的文本-图像对齐仍然具有挑战性", "method": "引入负提示图像校正（NPC）自动化流程，通过分析交叉注意力模式，使用验证器-标注器-提议器框架生成候选负提示，并用显著文本空间评分进行排序", "result": "在GenEval++和Imagine-Bench上优于强基线，在GenEval++上达到0.571 vs. 0.371，在Imagine-Bench上获得最佳整体性能", "conclusion": "通过指导模型不生成什么内容，NPC为扩散模型中的文本-图像对齐提供了一种原则性、完全自动化的方法"}}
{"id": "2512.07698", "pdf": "https://arxiv.org/pdf/2512.07698", "abs": "https://arxiv.org/abs/2512.07698", "authors": ["Arslan Artykov", "Corentin Sautier", "Vincent Lepetit"], "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "AI": {"tldr": "提出首个仅使用合成数据训练的端到端方法，从单目视频中联合预测铰接物体的部件分割和关节参数", "motivation": "铰接物体理解是机器人和数字孪生领域的基础挑战，现有方法依赖多视角系统、物体扫描或静态相机，缺乏从自由移动相机拍摄的单目视频中恢复部件分割和关节参数的实用解决方案", "method": "使用仅合成数据训练的数据驱动方法，直接从自由移动相机拍摄的单目视频中联合预测部件分割和关节参数，支持实时应用", "result": "方法在合成数据上训练，但能很好地泛化到真实世界物体，为铰接物体理解提供了可扩展且实用的解决方案", "conclusion": "首次实现了仅使用合成数据训练，从自由移动相机拍摄的单目视频中准确建模铰接物体，为机器人和数字孪生应用提供了实用的实时解决方案"}}
{"id": "2512.07697", "pdf": "https://arxiv.org/pdf/2512.07697", "abs": "https://arxiv.org/abs/2512.07697", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "AI": {"tldr": "提出延迟感知扩散策略（DA-DP），通过在训练和推理中显式纳入推理延迟来解决机器人动态任务中的观察-执行延迟问题", "motivation": "机器人在感知和选择动作时，世界持续变化，推理延迟导致观察状态与执行状态之间存在数十到数百毫秒的差距，现有方法未充分考虑这一延迟影响", "method": "提出延迟感知扩散策略框架，将零延迟轨迹校正为延迟补偿轨迹，并通过延迟条件增强策略，该框架与架构无关，可迁移到扩散策略之外", "result": "在多种任务、机器人和延迟条件下实证验证，DA-DP的成功率比延迟不感知方法对延迟更具鲁棒性，并鼓励评估协议报告性能随测量延迟的变化", "conclusion": "DA-DP为延迟感知模仿学习提供通用模式，强调在动态任务评估中考虑实际延迟的重要性，而不仅仅是任务难度"}}
{"id": "2512.07687", "pdf": "https://arxiv.org/pdf/2512.07687", "abs": "https://arxiv.org/abs/2512.07687", "authors": ["Sujoy Nath", "Arkaprabha Basu", "Sharanya Dasgupta", "Swagatam Das"], "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "AI": {"tldr": "提出HalluShift++方法，通过分析MLLM内部层动态变化来检测多模态大语言模型中的幻觉现象", "motivation": "当前MLLM幻觉评估主要依赖外部LLM评估器，但这些评估器本身也存在幻觉问题且存在领域适应挑战。需要更可靠的内在评估方法。", "method": "提出假设：幻觉表现为MLLM内部层动态的可测量异常。通过分析层间动态变化，将基于文本LLM的幻觉检测扩展到多模态场景。", "result": "开发了HalluShift++方法，能够有效检测MLLM中的幻觉现象，代码已开源。", "conclusion": "通过分析MLLM内部表示变化可以更可靠地检测多模态幻觉，为模型开发提供重要评估工具。"}}
{"id": "2512.07684", "pdf": "https://arxiv.org/pdf/2512.07684", "abs": "https://arxiv.org/abs/2512.07684", "authors": ["Zihan Chen", "Lanyu Yu"], "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "10 pages", "summary": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "AI": {"tldr": "提出基于图神经网络（GNN）的框架，用于检测在线社区中的不文明行为（毒性、攻击性和人身攻击），通过结合文本内容和评论间的关系结构来提高检测性能。", "motivation": "在线不文明行为已成为数字社区中普遍且持久的问题，给用户带来严重的社会和心理负担。现有方法（包括大语言模型）在准确性和效率方面存在局限，需要结合结构上下文来改进检测效果。", "method": "构建图神经网络框架，将用户评论表示为节点，基于文本相似性定义边，结合动态调整的注意力机制来平衡节点特征和拓扑特征，同时学习语言内容和评论间的关系结构。", "result": "提出的GNN架构在多个指标上优于12个最先进的大语言模型，同时推理成本显著降低，证明了结构上下文在在线不文明行为检测中的关键作用。", "conclusion": "图神经网络通过结合文本内容和关系结构，能够更有效地检测在线不文明行为，解决了纯文本大语言模型在行为预测中的局限性，为在线社区管理提供了更高效准确的解决方案。"}}
{"id": "2512.07680", "pdf": "https://arxiv.org/pdf/2512.07680", "abs": "https://arxiv.org/abs/2512.07680", "authors": ["P. A. Wigner", "L. Romanello", "A. Hammad", "P. H. Nguyen", "T. Lan", "S. F. Armanini", "B. B. Kocer", "M. Kovac"], "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "AI": {"tldr": "本文提出了一种空中可部署的爬行机器人AMBER，用于树冠环境中的自适应移动和操作。该系统结合了柔性微刺轨道、双轨道旋转夹持器和弹性尾部，能够在不同曲率和倾斜度的树枝上实现安全附着和稳定穿越。", "motivation": "当前空中机器人在树冠环境中的操作存在局限性，如悬停功耗高、在复杂树枝结构中的稳定附着和移动困难。需要一种能够在树冠环境中高效移动和操作的机器人系统，以支持环境采样和冠层内传感应用。", "method": "系统采用柔性微刺轨道设计，结合双轨道旋转夹持器和弹性尾部。微刺轨道提供自适应附着能力，双轨道旋转夹持器实现抓取功能，弹性尾部增强稳定性。系统通过无人机-绳索部署系统进行空中部署。", "result": "实验表明：1）可靠抓取能力达90度机身滚转和倾斜；2）在倾斜达67.5度的树枝上有效攀爬；3）水平树枝上最大速度达0.55体长/秒；4）柔性轨道允许高达10度的偏航转向；5）无量纲运输成本比典型空中机器人悬停功耗低一个数量级。", "conclusion": "AMBER系统提供了一个稳健、低功耗的平台，用于环境采样和冠层内传感，填补了空中机器人和地面生态机器人之间的技术空白。该系统在复杂树冠环境中表现出优异的移动和操作能力。"}}
{"id": "2512.07674", "pdf": "https://arxiv.org/pdf/2512.07674", "abs": "https://arxiv.org/abs/2512.07674", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "AI": {"tldr": "DIST-CLIP提出了一种统一的MRI图像协调框架，通过解耦解剖内容和图像对比度表示，能够灵活使用目标图像或DICOM元数据进行指导，以解决医学图像分析中的数据异质性问题。", "motivation": "深度学习在医学图像分析中的应用受到数据异质性的严重限制，特别是在MRI中，扫描仪硬件差异、采集协议多样性和序列参数变化导致显著的域偏移。现有方法要么需要目标图像，要么依赖过于简化的标签，无法捕捉复杂的采集细节，难以适应真实临床环境的异质性。", "method": "DIST-CLIP框架通过预训练的CLIP编码器提取对比度表示，将解剖内容与图像对比度进行显式解耦。使用新颖的自适应风格转移模块将对比度嵌入整合到解剖内容中，能够灵活使用目标图像或DICOM元数据进行指导。", "result": "在多样化的真实临床数据集上训练和评估DIST-CLIP，与最先进方法相比，在风格转换保真度和解剖结构保留方面均显示出显著改进，为风格转移和MRI数据标准化提供了灵活解决方案。", "conclusion": "DIST-CLIP通过解耦解剖-对比度表示和CLIP指导，提供了一个统一的MRI协调框架，能够有效处理真实临床环境中的数据异质性，显著提高了风格转换质量和解剖结构保留能力。"}}
{"id": "2512.07673", "pdf": "https://arxiv.org/pdf/2512.07673", "abs": "https://arxiv.org/abs/2512.07673", "authors": ["Matthias Heyrman", "Chenhao Li", "Victor Klemm", "Dongho Kang", "Stelian Coros", "Marco Hutter"], "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "categories": ["cs.RO"], "comment": "15 pages", "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "AI": {"tldr": "提出多域运动嵌入(MDME)方法，用于实现腿式机器人的实时模仿，通过统一的运动表示同时捕捉结构化周期模式和非规则变化。", "motivation": "现有运动控制器通常忽略运动中的固有模式，而现有的表示学习方法未能同时捕捉人类和动物运动中结构化周期模式和非规则变化，限制了机器人实时模仿表达性行为的能力。", "method": "使用基于小波的编码器和并行概率嵌入，统一嵌入结构化和非结构化特征，从最小输入集生成丰富的参考运动表示，并将学习到的嵌入作为机器人控制策略的条件。", "result": "在双足和四足机器人平台上实现了无重定向的实时运动模仿，准确复现复杂轨迹；在重建保真度和对未见运动的泛化能力上优于先前方法；通过零样本部署实时复现新运动风格。", "conclusion": "MDME为可扩展的实时机器人模仿提供了一个通用且结构感知的基础框架，无需任务特定调优或在线重定向。"}}
{"id": "2512.07668", "pdf": "https://arxiv.org/pdf/2512.07668", "abs": "https://arxiv.org/abs/2512.07668", "authors": ["Ronan John", "Aditya Kesari", "Vincenzo DiMatteo", "Kristin Dana"], "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "categories": ["cs.CV"], "comment": null, "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "AI": {"tldr": "提出EgoCampus数据集和EgoCampusNet模型，用于预测户外校园环境中行人导航时的视觉注意力", "motivation": "现有第一人称数据集大多关注室内任务或缺乏眼动追踪数据，需要研究户外导航场景中的人类视觉注意力预测", "method": "使用Meta Project Aria眼镜收集数据，包含眼动追踪、RGB相机、惯性传感器和GPS；开发EgoCampusNet模型预测行人导航时的注视点", "result": "创建了包含25条独特户外路径、超过6公里、80多名行人的EgoCampus数据集，提供了丰富的注视标注视频", "conclusion": "该研究为研究真实世界注意力提供了新资源，并为导航场景的注视预测模型提供了基础，数据集和代码将公开"}}
{"id": "2512.07664", "pdf": "https://arxiv.org/pdf/2512.07664", "abs": "https://arxiv.org/abs/2512.07664", "authors": ["Eduardo Vyhmeister", "Bastien Pietropaoli", "UdoBub", "Rob Schneider", "Andrea Visentin"], "title": "A Framework for Data Valuation and Monetisation", "categories": ["cs.CY", "cs.ET"], "comment": null, "summary": "As organisations increasingly recognise data as a strategic resource, they face the challenge of translating informational assets into measurable business value. Existing valuation approaches remain fragmented, often separating economic, governance, and strategic perspectives and lacking operational mechanisms suitable for real settings. This paper introduces a unified valuation framework that integrates these perspectives into a coherent decision-support model. Building on two artefacts from the Horizon Europe DATAMITE project, a taxonomy of data-quality and performance metrics, and an Analytic Network Process (ANP) tool for deriving relative importance, we develop a hybrid valuation model. The model combines qualitative scoring, cost- and utility-based estimation, relevance/quality indexing, and multi-criteria weighting to define data value transparently and systematically. Anchored in the Balanced Scorecard (BSC), the framework aligns indicators and valuation outcomes with organisational strategy, enabling firms to assess monetisation potential across Data-as-a-Service, Information-as-a-Service, and Answers-as-a-Service pathways. Methodologically, the study follows a Design Science approach complemented by embedded case studies with industrial partners, which informed continual refinement of the model. Because the evaluation is connected to a high-level taxonomy, the approach also reveals how valuation considerations map to BSC perspectives. Across the analysed use cases, the framework demonstrated flexibility, transparency, and reduced arbitrariness in valuation, offering organisations a structured basis for linking data assets to strategic and economic outcomes.", "AI": {"tldr": "提出一个统一的数据估值与货币化框架，将经济、治理和战略视角整合到连贯的决策支持模型中", "motivation": "组织日益认识到数据作为战略资源的重要性，但现有估值方法分散，缺乏适合实际环境的操作机制，需要将不同视角整合为系统化的估值方法", "method": "采用设计科学研究方法，基于DATAMITE项目的两个构件（数据质量和性能指标分类法、ANP工具），开发混合估值模型，结合定性评分、成本/效用估计、相关性/质量索引和多标准加权，并基于平衡计分卡进行战略对齐", "result": "该框架在分析的使用案例中展示了灵活性、透明性和减少估值随意性，为组织提供了将数据资产与战略和经济结果联系起来的结构化基础", "conclusion": "该统一估值框架能够透明、系统地定义数据价值，使组织能够评估跨不同服务路径的货币化潜力，并将估值考虑因素映射到平衡计分卡视角"}}
{"id": "2512.07661", "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "提出OMEGA框架，通过优化引导的扩散过程生成交互式驾驶场景，特别关注安全关键事件生成", "motivation": "自动驾驶评估需要多样化的多智能体驾驶场景，但现有数据集缺乏安全关键事件；现有生成模型缺乏可控性或违反物理/社会约束", "method": "提出OMEGA框架，在扩散模型的反向采样步骤中通过约束优化重新锚定，确保物理合理性和行为一致性；将自我车辆与攻击者交互建模为分布空间中的博弈论优化", "result": "在nuPlan和Waymo数据集上，OMEGA将物理和行为有效场景比例从32.35%提升到72.27%（自由探索），从11%提升到80%（可控生成）；能生成5倍多的近碰撞帧（TTC<3秒）", "conclusion": "OMEGA框架通过优化引导的扩散采样，显著提升了驾驶场景生成的现实性、一致性和可控性，特别适用于生成安全关键的对抗性场景"}}
{"id": "2512.07652", "pdf": "https://arxiv.org/pdf/2512.07652", "abs": "https://arxiv.org/abs/2512.07652", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "AI": {"tldr": "本文提出了一种基于AI的自主水下系统，用于海洋探索和科学研究，通过集成多种AI技术实现水下物体的自动检测、分析和报告生成。", "motivation": "传统海洋探索面临极端条件、能见度有限和高成本等挑战，导致大量海洋区域未被探索。需要一种自动化系统来克服这些限制，减少人类潜水风险，提高任务效率。", "method": "系统集成了YOLOv12 Nano进行实时物体检测，ResNet50进行特征提取，PCA进行降维，K-Means++进行聚类分析，以及GPT-4o Mini生成结构化报告。使用超过55,000张来自DeepFish和OzFish数据集的图像进行训练和评估。", "result": "系统检测海洋物体的mAP@0.5为0.512，精度0.535，召回率0.438。PCA降维保留了98%方差，K-Means聚类成功基于视觉特征分组物体，LLM有效生成了检测和聚类的洞察性总结。", "conclusion": "该集成方法显著降低了人类潜水风险，提高了任务效率，增强了水下数据分析的速度和深度，为在挑战性海洋环境中进行更有效的科学研究铺平了道路。"}}
{"id": "2512.07651", "pdf": "https://arxiv.org/pdf/2512.07651", "abs": "https://arxiv.org/abs/2512.07651", "authors": ["Yuanye Liu", "Hanxiao Zhang", "Nannan Shi", "Yuxin Shi", "Arif Mahmood", "Murtaza Taj", "Xiahai Zhuang"], "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method", "categories": ["cs.CV"], "comment": ":68U10ACM Class:I.4.6", "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.", "AI": {"tldr": "提出LiQA数据集和基线方法，用于肝脏纤维化量化分析，包括肝脏分割和纤维化分期任务", "motivation": "肝脏纤维化是全球重大健康负担，需要准确分期进行临床管理，现有方法在复杂现实条件下（如域偏移、缺失模态、空间错位）面临挑战", "method": "1. 建立LiQA数据集（440例患者，多期相、多中心MRI扫描）；2. 采用半监督学习框架结合外部数据进行鲁棒分割；3. 使用多视图共识方法结合CAM正则化进行分期", "result": "评估显示，利用多源数据和解剖约束显著增强了模型在临床环境中的鲁棒性，该方法在CARE 2024挑战赛中表现最佳", "conclusion": "LiQA数据集为肝脏纤维化量化分析提供了基准，提出的方法在复杂临床条件下表现出色，多源数据和约束策略对提高模型鲁棒性至关重要"}}
{"id": "2512.07647", "pdf": "https://arxiv.org/pdf/2512.07647", "abs": "https://arxiv.org/abs/2512.07647", "authors": ["Georgios Tzachristas", "Lei Deng", "Ioannis Tzachristas", "Gong Zhang", "Renhai Chen"], "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=τ\\|μ_{\\mathrm{tail}}-μ_{\\mathrm{head}}\\|_2$ with $τ=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\leτ\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approxΦ_c(σ+Φ^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.", "AI": {"tldr": "提出了一种基于总变差距离的Top-k稀疏注意力认证理论框架，量化注意力分布截断的近似误差", "motivation": "现有的Top-k注意力截断方法缺乏严格的数学理论支持，无法量化截断带来的近似误差，需要建立统一的数学框架来认证Top-k截断的误差边界", "method": "使用总变差距离(TV距离)作为度量，建立注意力分布P与其Top-k截断P̂之间的误差分析框架，推导TV距离与丢弃的softmax尾部质量的关系，并基于头尾分解方法分析输出误差", "result": "证明了TV距离等于丢弃的softmax尾部质量，且满足TV(P,P̂)=1-e^{-KL(P̂∥P)}；推导了输出误差的分解公式和边界；在i.i.d.高斯分数模型下得到了封闭形式的尾部质量和最小k的选择规则", "conclusion": "建立了一个统一的数学框架来认证Top-k注意力截断的近似误差，提供了理论保证和实用的选择规则，实验验证了理论预测，平均可减少2-4倍的键值计算"}}
{"id": "2512.07631", "pdf": "https://arxiv.org/pdf/2512.07631", "abs": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "comment": null, "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "AI": {"tldr": "提出Agent Capability Problem (ACP)框架，通过信息论界限预测智能体在资源约束下能否解决问题，将问题解决建模为信息获取过程", "motivation": "传统方法依赖经验启发式，无法在搜索前预测资源需求。需要一种理论框架来预测智能体是否能在给定资源约束下解决问题，帮助智能体决定何时投入资源", "method": "将问题解决建模为信息获取过程：智能体需要I_total比特信息来识别解决方案，每个动作获得I_step比特信息，成本为C_step。定义有效成本C_eff = (I_total/I_step)*C_step来预测资源需求。提供下界证明和紧概率上界", "result": "ACP预测与实际智能体性能紧密匹配，能够一致地界定搜索努力，相比贪婪和随机策略提高了效率。框架在LLM和智能体工作流中具有普适性", "conclusion": "ACP提供了一个统一的信息论框架，连接主动学习、贝叶斯优化和强化学习的原则，能够在搜索前预测资源需求，帮助智能体做出更好的资源分配决策"}}
{"id": "2512.07628", "pdf": "https://arxiv.org/pdf/2512.07628", "abs": "https://arxiv.org/abs/2512.07628", "authors": ["Zhiqi Li", "Wenhuan Li", "Tengfei Wang", "Zhenwei Wang", "Junta Wu", "Haoyuan Wang", "Yunhan Yang", "Zehuan Huang", "Yang Li", "Peidong Liu", "Chunchao Guo"], "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "AI": {"tldr": "提出MoCA模型，通过重要性组件路由和未选组件压缩实现可扩展的组合式3D生成，解决现有方法因全局注意力二次复杂度导致的扩展性问题", "motivation": "现有基于部分的3D生成方法在处理组件数量增加时，由于全局注意力的二次复杂度导致扩展性差，限制了组合式3D资产创建的效率和精细度", "method": "设计了两个关键技术：1) 基于重要性的组件路由，选择top-k相关组件进行稀疏全局注意力计算；2) 不重要组件压缩，在降低计算复杂度的同时保留未选组件的上下文先验", "result": "在组合式物体和场景生成任务上，MoCA均优于基线方法，实现了高效、细粒度的组合式3D资产创建", "conclusion": "MoCA通过创新的注意力机制设计，解决了组合式3D生成的扩展性问题，为可扩展的组合式3D生成提供了有效解决方案"}}
{"id": "2512.07627", "pdf": "https://arxiv.org/pdf/2512.07627", "abs": "https://arxiv.org/abs/2512.07627", "authors": ["Maximos Kaliakatsos-Papakostas", "Konstantinos Soiledis", "Theodoros Tsamis", "Dimos Makris", "Vassilis Katsouros", "Emilios Cambouropoulos"], "title": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization", "categories": ["cs.SD", "cs.AI", "cs.SC"], "comment": "Proceedings of the 6th Conference on AI Music Creativity (AIMC 2025), Brussels, Belgium, September 10th-12th", "summary": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.", "AI": {"tldr": "该论文研究了在基于Transformer的旋律和声化中融入预定义和弦约束的方法，提出了B*算法来强制预训练模型在特定位置生成指定和弦。", "motivation": "Transformer架构在符号音乐生成方面具有优势，但如何有效融入用户偏好（如特定位置的和弦约束）仍是一个研究问题。当前需要解决在旋律和声化任务中，如何让自回归Transformer模型在生成过程中准确满足预定义和弦约束的问题。", "method": "提出了B*算法，该算法结合了束搜索和A*搜索的特点，并加入回溯机制。算法强制预训练的Transformer模型在正确的小节和起始位置满足和弦约束，虽然在最坏情况下具有指数复杂度，但为后续启发式改进提供了框架。", "result": "论文提出了首个解决该问题的算法框架，能够处理旋律和声化中的和弦约束问题。虽然算法复杂度较高，但为后续研究奠定了基础，并展示了在Transformer模型中融入结构化约束的可能性。", "conclusion": "该研究首次系统地探讨了在Transformer旋律和声化中融入和弦约束的挑战，提出的B*算法虽然复杂度较高，但为解决这一问题提供了可行方案，并为未来引入启发式优化和改进指明了方向。"}}
{"id": "2512.07624", "pdf": "https://arxiv.org/pdf/2512.07624", "abs": "https://arxiv.org/abs/2512.07624", "authors": ["Yongbo Yu", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Time Series Foundation Models for Process Model Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.", "AI": {"tldr": "本文研究了时间序列基础模型（TSFMs）在过程模型预测（PMF）中的应用，通过比较零样本使用和微调变体，评估这些预训练模型在预测过程控制流结构演化方面的性能。", "motivation": "传统的机器学习和深度学习模型在过程模型预测中表现有限，主要由于直接跟随关系时间序列的稀疏性和异质性。作者希望探索时间序列基础模型作为替代方案，利用其在通用时间序列上的预训练知识来改进PMF任务。", "method": "使用真实事件日志中提取的直接跟随关系时间序列，比较了时间序列基础模型的两种使用方式：1）零样本使用（无额外训练），2）在PMF特定数据上进行微调。与传统模型和专门训练的模型进行对比，使用MAE和RMSE作为评估指标。", "result": "时间序列基础模型通常比传统模型和专门训练的模型获得更低的预测误差（MAE和RMSE），表明从非过程领域有效转移了时间结构知识。虽然微调可以进一步提高准确性，但增益通常很小，在较小或更复杂的数据集上可能消失，因此零样本使用仍然是强大的默认选择。", "conclusion": "该研究突出了时间序列基础模型在处理过程相关时间序列方面的泛化能力和数据效率，为过程模型预测提供了第一个系统性的时间基础模型评估，展示了预训练模型在该领域的潜力。"}}
{"id": "2512.07623", "pdf": "https://arxiv.org/pdf/2512.07623", "abs": "https://arxiv.org/abs/2512.07623", "authors": ["Lalitha A R"], "title": "Context-Adaptive Color Optimization for Web Accessibility: Balancing Perceptual Fidelity and Functional Requirements", "categories": ["cs.HC", "math.OC"], "comment": "8 pages, 2 figures", "summary": "We extend our OKLCH-based accessibility optimization with context-adaptive constraint strategies that achieve near-universal success rates across diverse use cases. Our original strict algorithm reached 66-77% success by prioritizing minimal perceptual change ($ΔE \\leq 5.0$), optimizing for enterprise contexts where brand fidelity is paramount. However, this one-size-fits-all approach fails to serve the broader ecosystem of web developers who need accessible solutions even when strict perceptual constraints cannot be satisfied. We introduce recursive optimization (Mode~1) that compounds small adjustments across iterations, achieving 93.68% success on all color pairs and 100% success on reasonable pairs (contrast ratio $ρ> 2.0$), representing a +27.23 percentage point improvement. A relaxed fallback mode (Mode~2) handles pathological edge cases, reaching 98.73% overall success. Evaluation on 10,000 realistic web color pairs demonstrates that context-aware constraint relaxation, combined with absolute hue preservation, enables practical accessibility compliance while maintaining brand color identity. The median perceptual change remains zero across all modes (most pairs already comply), while the 90th percentile reaches $ΔE_{2000} = 15.55$ in Mode~1 -- perceptually acceptable when hue invariance preserves the essential character of the original color. The approach is deployed in CM-Colors v0.5.0 (800+ monthly downloads), providing developers with explicit control over the accessibility-fidelity trade-off appropriate to their context.", "AI": {"tldr": "该论文提出了一种上下文自适应的颜色优化方法，用于平衡网页可访问性中的感知保真度和功能要求，通过递归优化和宽松回退模式显著提高了颜色对的可访问性成功率。", "motivation": "现有的严格算法（ΔE ≤ 5.0）虽然在企业品牌保真度方面表现良好，但成功率仅为66-77%，无法满足更广泛的网页开发者需求，特别是当严格感知约束无法满足时，需要更灵活的解决方案。", "method": "提出了两种优化模式：1）递归优化模式（Mode 1），通过迭代累积小调整实现优化；2）宽松回退模式（Mode 2），处理极端边缘情况。两种模式都基于OKLCH颜色空间，结合上下文感知约束松弛和绝对色调保持技术。", "result": "在10,000个真实网页颜色对上的评估显示：Mode 1在所有颜色对上达到93.68%成功率，在合理颜色对（对比度ρ > 2.0）上达到100%成功率，相比原始算法提升27.23个百分点；Mode 2达到98.73%总体成功率。中位感知变化保持为零，90百分位ΔE2000为15.55。", "conclusion": "上下文感知约束松弛结合绝对色调保持技术，能够在保持品牌颜色身份的同时实现实际可访问性合规。该方法已部署在CM-Colors v0.5.0中，为开发者提供了根据其上下文适当控制可访问性与保真度权衡的明确工具。"}}
{"id": "2512.07618", "pdf": "https://arxiv.org/pdf/2512.07618", "abs": "https://arxiv.org/abs/2512.07618", "authors": ["Jiratchaphat Nanta", "Vorapong Suppakitpaisarn", "Piyashat Sripratak"], "title": "Approximation Algorithms for the $b$-Matching and List-Restricted Variants of MaxQAP", "categories": ["cs.DS", "cs.DM"], "comment": null, "summary": "We study approximation algorithms for two natural generalizations of the Maximum Quadratic Assignment Problem (MaxQAP). In the Maximum List-Restricted Quadratic Assignment Problem, each node in one partite set may only be matched to nodes from a prescribed list. For instances on $n$ nodes where every list has size at least $n - O(\\sqrt{n})$, we design a randomized $O(\\sqrt{n})$-approximation algorithm based on the linear-programming relaxation and randomized rounding framework of Makarychev, Manokaran, and Sviridenko. In the Maximum Quadratic $b$-Matching Assignment Problem, we seek a $b$-matching that maximizes the MaxQAP objective. We refine the standard MaxQAP relaxation and combine randomized rounding over $b$ independent iterations with a polynomial-time algorithm for maximum-weight $b$-matching problem to obtain an $O(\\sqrt{bn})$-approximation. When $b$ is constant and all lists have size $n - O(\\sqrt{n})$, our guarantees asymptotically match the best known approximation factor for MaxQAP, yielding the first approximation algorithms for these two variants.", "AI": {"tldr": "研究最大二次分配问题(MaxQAP)的两个自然变体的近似算法：列表限制版本和b-匹配版本", "motivation": "最大二次分配问题(MaxQAP)是一个经典的组合优化问题，但在实际应用中经常遇到需要限制匹配选择或允许多对多匹配的情况。本文研究MaxQAP的两个自然扩展：列表限制版本（每个节点只能匹配到预设列表中的节点）和b-匹配版本（允许多对多匹配），旨在为这些变体设计第一个近似算法。", "method": "1. 对于列表限制版本：基于Makarychev等人的线性规划松弛和随机舍入框架，设计随机O(√n)-近似算法，要求每个列表大小至少为n-O(√n)。2. 对于b-匹配版本：改进标准MaxQAP松弛，结合b次独立迭代的随机舍入和多项式时间最大权重b-匹配算法，获得O(√bn)-近似。", "result": "1. 对于列表大小至少为n-O(√n)的实例，获得了随机O(√n)-近似算法。2. 对于b-匹配版本，获得了O(√bn)-近似算法。3. 当b为常数且所有列表大小为n-O(√n)时，这些保证渐近匹配MaxQAP的最佳已知近似因子，为这两个变体提供了第一个近似算法。", "conclusion": "本文成功为MaxQAP的两个重要变体设计了第一个近似算法，填补了该领域的研究空白。通过创新的线性规划松弛改进和随机舍入技术，在合理的限制条件下获得了与原始MaxQAP相匹配的近似保证，为处理实际应用中更复杂的匹配约束提供了理论工具。"}}
{"id": "2512.07613", "pdf": "https://arxiv.org/pdf/2512.07613", "abs": "https://arxiv.org/abs/2512.07613", "authors": ["Arthur Fleig"], "title": "A Retrospective on Ultrasound Mid-Air Haptics in HCI", "categories": ["cs.HC"], "comment": "3 pages; AlpCHI 2026 Revisiting HCI Research Track", "summary": "In 2013, the UltraHaptics system demonstrated that focused ultrasound could generate perceivable mid-air tactile sensations, building on earlier explorations of airborne ultrasound as a haptic medium. These contributions established ultrasound mid-air haptics (UMH) as a viable interaction modality and laid the technical and perceptual foundations for subsequent advances in Human-Computer Interaction (HCI). In this extended abstract, we revisit this formative work, trace the research and design trajectories it enabled, and reflect on how UMH has supported multisensory interaction, immersion, and inclusion. We also highlight how this line of research exemplifies the value of interdisciplinary collaboration to advance novel interactive technologies.", "AI": {"tldr": "回顾超声空中触觉技术在HCI领域的发展历程、影响和未来展望", "motivation": "总结超声空中触觉技术从2013年UltraHaptics系统以来的发展，追踪其研究轨迹，反思该技术如何支持多感官交互、沉浸感和包容性", "method": "采用回顾性分析方法，追溯超声空中触觉技术在HCI领域的发展历程，分析其技术基础、感知原理以及跨学科合作的价值", "result": "超声空中触觉技术已成为可行的交互模态，为HCI领域的多感官交互、沉浸式体验和包容性设计提供了技术基础，并展示了跨学科合作对推动创新交互技术的重要性", "conclusion": "超声空中触觉技术在过去十年中取得了显著进展，不仅建立了坚实的技术和感知基础，还开辟了新的研究设计轨迹，证明了跨学科合作在推动新型交互技术发展中的关键作用"}}
{"id": "2512.07612", "pdf": "https://arxiv.org/pdf/2512.07612", "abs": "https://arxiv.org/abs/2512.07612", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "AI": {"tldr": "提出PCMind-2.1-Kaiyuan-2B，一个完全开源的20亿参数大语言模型，旨在解决开源社区与工业界之间的知识鸿沟，专注于在资源受限条件下提高训练效率和效果。", "motivation": "大型语言模型的快速发展导致开源社区与工业界之间存在显著的知识鸿沟，主要原因是工业界依赖闭源的高质量数据和训练方法。为了弥合这一差距，需要开发完全开源的解决方案。", "method": "采用三项关键技术：1) 分位数数据基准测试方法，用于系统比较异构开源数据集并提供数据混合策略见解；2) 多阶段范式中的战略选择性重复方案，有效利用稀疏的高质量数据；3) 多领域课程训练策略，按质量排序样本。同时采用高度优化的数据预处理流程和FP16稳定性的架构修改。", "result": "Kaiyuan-2B在性能上与最先进的完全开源模型竞争，展示了在资源受限预训练中的实用性和可扩展性解决方案。所有资产（包括模型权重、数据和代码）均在Apache 2.0许可下发布。", "conclusion": "通过创新的数据管理策略和优化的训练方法，成功开发出具有竞争力的完全开源20亿参数模型，为资源受限环境下的高效预训练提供了可行的解决方案。"}}
{"id": "2512.07611", "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark. Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "本文系统比较了三种强化学习算法（PPO、GRPO和DAPO）在提升大语言模型复杂推理能力方面的效果，通过控制性迁移学习评估，发现RL训练模型在所有任务上都优于基础模型，但改进程度因基准测试而异。", "motivation": "研究动机在于探索不同强化学习算法如何有效提升大语言模型的复杂推理能力，特别是在迁移学习场景下，为RL-based LLM训练提供实用的参数调优指导。", "method": "采用控制性迁移学习方法：首先在专门的Countdown Game上对模型进行微调，然后在通用推理基准测试套件上进行评估。对GRPO和DAPO中的组大小、KL惩罚系数等参数进行系统分析，并测试DAPO中动态采样组件的影响。", "result": "1) 所有RL训练模型在各项任务上都优于对应的基础模型；2) 增加GRPO和DAPO中的组大小能带来更稳定的训练动态和更高的准确率；3) KL惩罚系数的影响是非单调的；4) DAPO中的动态采样组件并未改善性能，禁用DS时DAPO取得最佳整体效果。", "conclusion": "强化学习能有效提升LLM的推理能力，但不同算法的效果和参数敏感性不同。GRPO和DAPO的组大小是重要参数，而DAPO的动态采样组件在实际应用中可能不必要。研究为RL-based LLM训练提供了实用的参数调优指导。"}}
{"id": "2512.07609", "pdf": "https://arxiv.org/pdf/2512.07609", "abs": "https://arxiv.org/abs/2512.07609", "authors": ["Nikita Vaibhav Pavle", "Shrreya Rajneesh", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.", "AI": {"tldr": "提出一种基于方向和相对速度加权的改进人工势场法，用于无人机在动态环境中的避障", "motivation": "传统人工势场法存在局部极小值问题，且无法考虑移动障碍物的运动学特性，难以适应动态复杂空域的自主避障需求", "method": "引入有界加权函数ω(θ,ve)，根据障碍物相对于无人机的方向和速度动态调整排斥势能，并将该方法集成到模型预测控制框架中", "result": "仿真结果表明该方法有效解决了局部极小值问题，显著提高了安全性，能够实现平滑的预测性避障机动", "conclusion": "所提出的方向与速度自适应人工势场法在复杂环境中具有优越的路径完整性和可靠性能，适用于自主导航"}}
{"id": "2512.07608", "pdf": "https://arxiv.org/pdf/2512.07608", "abs": "https://arxiv.org/abs/2512.07608", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "categories": ["cs.CL", "cs.AI"], "comment": "ef:NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models", "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "AI": {"tldr": "提出Metric-Fair Prompting框架，通过度量公平性约束指导大语言模型在医学问答中做出决策，确保相似样本得到相似处理", "motivation": "在医学问答等高风险应用中，需要确保大语言模型的决策具有个体公平性，即相似的问题应该得到相似的处理，避免不一致的决策", "method": "使用NLP嵌入计算问题相似度，将相似问题作为联合对处理而非单独处理；在提示中强制实施全局决策协议：提取关键临床特征，将每个(问题,选项)映射到置信度分数f(x)，并施加Lipschitz式约束确保相似输入获得相似分数", "result": "在MedQA(US)基准测试中，Metric-Fair Prompting相比标准单项目提示方法提高了性能，表明公平性引导的置信度导向推理可以增强LLM在高风险临床多选题上的准确性", "conclusion": "提出的度量公平提示框架能够有效提升大语言模型在医学问答中的公平性和准确性，通过强制相似样本获得相似处理来改善决策一致性"}}
{"id": "2512.07606", "pdf": "https://arxiv.org/pdf/2512.07606", "abs": "https://arxiv.org/abs/2512.07606", "authors": ["Jingna Qiu", "Frauke Wilm", "Mathias Öttl", "Jonas Utz", "Maja Schlereth", "Moritz Schillinger", "Marc Aubreville", "Katharina Breininger"], "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "AI": {"tldr": "提出分解采样（DECOMP）方法，用于主动学习中的高效区域标注，通过分解图像为类别特定组件并采样每个类别的区域，提升标注多样性和少数类性能。", "motivation": "密集预测任务（特别是医学影像）中标注成本高、耗时长，现有区域标注选择方法存在计算内存成本高、区域选择不相关、过度依赖不确定性采样等问题，需要更高效的主动学习采样策略。", "method": "DECOMP方法：1）使用伪标签将图像分解为类别特定组件；2）从每个类别中采样区域；3）利用类别预测置信度指导采样过程，确保困难类别获得更多标注。", "result": "在ROI分类、2D分割和3D分割任务中，DECOMP始终超越基线方法，能更好地采样少数类区域并提升这些困难类别的性能表现。", "conclusion": "DECOMP通过分解采样策略解决了现有区域标注方法的局限性，提高了标注效率和模型性能，特别是在处理少数类别时表现优异。"}}
{"id": "2512.07602", "pdf": "https://arxiv.org/pdf/2512.07602", "abs": "https://arxiv.org/abs/2512.07602", "authors": ["Pengfei Sun", "Zhe Su", "Jascha Achterberg", "Giacomo Indiveri", "Dan F. M. Goodman", "Danyal Akarca"], "title": "Algorithm-hardware co-design of neuromorphic networks with dual memory pathways", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neural networks excel at event-driven sensing yet maintaining task-relevant context over long timescales. However building these networks in hardware respecting both tight energy and memory budgets, remains a core challenge in the field. We address this challenge through novel algorithm-hardware co-design effort. At the algorithm level, inspired by the cortical fast-slow organization in the brain, we introduce a neural network with an explicit slow memory pathway that, combined with fast spiking activity, enables a dual memory pathway (DMP) architecture in which each layer maintains a compact low-dimensional state that summarizes recent activity and modulates spiking dynamics. This explicit memory stabilizes learning while preserving event-driven sparsity, achieving competitive accuracy on long-sequence benchmarks with 40-60% fewer parameters than equivalent state-of-the-art spiking neural networks. At the hardware level, we introduce a near-memory-compute architecture that fully leverages the advantages of the DMP architecture by retaining its compact shared state while optimizing dataflow, across heterogeneous sparse-spike and dense-memory pathways. We show experimental results that demonstrate more than a 4x increase in throughput and over a 5x improvement in energy efficiency compared with state-of-the-art implementations. Together, these contributions demonstrate that biological principles can guide functional abstractions that are both algorithmically effective and hardware-efficient, establishing a scalable co-design paradigm for real-time neuromorphic computation and learning.", "AI": {"tldr": "提出一种算法-硬件协同设计的神经形态网络，通过双记忆通路架构结合快速脉冲活动和慢速记忆通路，在保持事件驱动稀疏性的同时实现长时程上下文记忆", "motivation": "脉冲神经网络擅长事件驱动感知，但在硬件实现中同时满足严格的能耗和内存预算限制下，维持任务相关的长时程上下文记忆仍然是一个核心挑战", "method": "算法层面：受大脑皮层快-慢组织启发，引入具有显式慢速记忆通路的神经网络，形成双记忆通路架构，每层维护紧凑的低维状态来总结近期活动并调制脉冲动态；硬件层面：设计近内存计算架构，充分利用DMP架构优势，保留紧凑共享状态并优化稀疏脉冲和密集记忆通路的数据流", "result": "在长序列基准测试中达到竞争性精度，参数比同等最先进的脉冲神经网络减少40-60%；硬件实现相比最先进方案吞吐量提升4倍以上，能效改善超过5倍", "conclusion": "生物原理可以指导既算法有效又硬件高效的功能抽象，为实时神经形态计算和学习建立了可扩展的协同设计范式"}}
{"id": "2512.07599", "pdf": "https://arxiv.org/pdf/2512.07599", "abs": "https://arxiv.org/abs/2512.07599", "authors": ["Hanshi Wang", "Zijian Cai", "Jin Gao", "Yiwei Zhang", "Weiming Hu", "Ke Wang", "Zhipeng Zhang"], "title": "Online Segment Any 3D Thing as Instance Tracking", "categories": ["cs.CV"], "comment": "NeurIPS 2025, Code is at https://github.com/AutoLab-SAI-SJTU/AutoSeg3D", "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "AI": {"tldr": "提出AutoSeg3D方法，将在线3D分割重新定义为实例跟踪问题，通过对象查询进行时序信息传播，实现实时、细粒度的3D场景理解", "motivation": "现有基于查询的3D分割方法忽略了时序理解这一关键维度，而感知本质上是动态过程。在具身智能体应用中，视角变化常导致物体部分可见，需要时序信息来建立完整的物体理解", "method": "1) 将在线3D分割重构为实例跟踪问题；2) 使用对象查询进行时序信息传播：长期实例关联保持特征和身份一致性，短期实例更新丰富即时观测；3) 引入空间一致性学习缓解VFMs的碎片化问题", "result": "在ScanNet200上超越ESAM方法2.8 AP，在ScanNet、SceneNN和3RScan数据集上均取得一致性能提升，建立了新的state-of-the-art", "conclusion": "通过时序信息传播和空间一致性学习，AutoSeg3D方法有效提升了具身智能体的时序环境感知能力，稀疏对象查询的时序信息交换既增强了空间理解又避免了密集点云交互的计算负担"}}
{"id": "2512.07596", "pdf": "https://arxiv.org/pdf/2512.07596", "abs": "https://arxiv.org/abs/2512.07596", "authors": ["Wenzhen Dong", "Jieming Yu", "Yiming Huang", "Hongqiu Wang", "Lei Zhu", "Albert C. S. Chung", "Hongliang Ren", "Long Bai"], "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "categories": ["cs.CV", "cs.RO"], "comment": "Technical Report", "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "AI": {"tldr": "评估SAM 3在机器人辅助手术中的性能，包括零样本分割、动态视频跟踪、语言提示分割以及3D重建能力", "motivation": "SAM 3相比前代模型引入了语言分割和增强的3D感知能力，需要在机器人手术领域评估其实际性能，特别是零样本分割和3D重建能力在复杂手术场景中的表现", "method": "在MICCAI EndoVis 2017和2018基准上测试零样本分割（点提示和边界框提示），评估动态视频跟踪和语言提示分割，并在SCARED、StereoMIS和EndoNeRF数据集上进行单目深度估计和3D重建测试", "result": "SAM 3在空间提示下的图像和视频分割方面明显优于SAM和SAM 2；语言提示在手术领域表现欠佳，需要领域特定训练；在3D重建方面展现出强大的单目深度估计和逼真的3D器械重建能力，但在高度动态的复杂手术场景中仍有局限", "conclusion": "SAM 3在机器人手术中展现出显著进步，特别是在空间提示分割和3D感知方面，但语言提示分割需要进一步优化，且模型在复杂动态手术场景中的性能仍有提升空间"}}
{"id": "2512.07590", "pdf": "https://arxiv.org/pdf/2512.07590", "abs": "https://arxiv.org/abs/2512.07590", "authors": ["Kaili Qi", "Zhongyi Huang", "Wenli Yang"], "title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.", "AI": {"tldr": "提出一种鲁棒的变分模型驱动的定制UNet框架，通过集成边缘检测器和平均曲率项来改进噪声图像分割", "motivation": "解决噪声图像分割中边界模糊或断裂的挑战，结合变分方法的可解释性和边界平滑优势与深度神经网络的强大表示能力", "method": "提出VM_TUNet混合框架，将物理先验、边缘检测器和平均曲率项整合到改进的Cahn-Hilliard方程中，包含F模块（频域预处理）和T模块（局部计算）两个协作模块", "result": "在三个基准数据集上的实验表明，该方法在性能和计算效率之间取得了平衡，相比纯CNN模型获得了有竞争力的定量结果和更好的视觉质量，同时以合理的计算成本达到了接近transformer方法的性能", "conclusion": "提出的鲁棒VM_TUNet框架成功结合了变分PDEs和深度学习的优势，为噪声图像分割提供了一种有效的解决方案，在性能、计算效率和视觉质量之间取得了良好平衡"}}
{"id": "2512.07584", "pdf": "https://arxiv.org/pdf/2512.07584", "abs": "https://arxiv.org/abs/2512.07584", "authors": ["Meituan LongCat Team", "Hanghang Ma", "Haoxian Tan", "Jiale Huang", "Junqiang Wu", "Jun-Yan He", "Lishuai Gao", "Songlin Xiao", "Xiaoming Wei", "Xiaoqi Ma", "Xunliang Cai", "Yayong Guan", "Jie Hu"], "title": "LongCat-Image Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "AI": {"tldr": "LongCat-Image是一个开创性的开源双语（中英）图像生成基础模型，专注于解决多语言文本渲染、照片级真实感、部署效率和开发者可访问性等核心挑战。", "motivation": "当前主流模型在多语言文本渲染（特别是中文字符）、照片级真实感、部署效率和开发者可访问性方面存在不足。LongCat-Image旨在通过开源方式提供更高效、更准确、更易部署的图像生成解决方案。", "method": "1）在预训练、中间训练和SFT阶段采用严格的数据筛选策略；2）在强化学习阶段协调使用精选的奖励模型；3）采用紧凑的6B参数扩散模型设计，远小于领域内常见的近20B或更大的MoE架构；4）建立全面的开源生态系统，包括多个模型版本和完整训练工具链。", "result": "1）在文本渲染能力和照片级真实感方面达到新的SOTA水平；2）为中文字符渲染设定了新的行业标准，支持复杂和罕见字符，在覆盖范围和准确性上优于主要开源和商业解决方案；3）实现了显著的部署效率，VRAM使用最小化，推理速度快；4）在图像编辑方面也达到SOTA结果，编辑一致性优于其他开源作品。", "conclusion": "LongCat-Image通过其创新的训练策略、紧凑的模型设计和全面的开源生态系统，为开发者和研究人员提供了强大的视觉内容创作支持，推动了图像生成领域的前沿发展。"}}
{"id": "2512.07583", "pdf": "https://arxiv.org/pdf/2512.07583", "abs": "https://arxiv.org/abs/2512.07583", "authors": ["Navid Asgari", "Benjamin M. Cole"], "title": "Complementary Learning Approach for Text Classification using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "67 pages", "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "AI": {"tldr": "提出一种结合大型语言模型(LLMs)和人类专家的互补学习方法，用于文本分类任务，通过思维链和少样本学习提示技术实现高效、低成本的人机协作", "motivation": "解决LLMs在文本分类中存在的固有弱点，同时发挥人类专家的优势，以成本效益高的方式实现人机协作，弥补各自不足", "method": "采用结构化方法，结合思维链和少样本学习提示技术，将定性研究中合作团队的最佳实践扩展到定量研究中的人机团队，允许人类使用溯因推理和自然语言来审查机器和人类的工作", "result": "成功应用于1990-2017年间1,934份制药联盟新闻稿的文本分类任务，能够有效识别和处理人机评分差异", "conclusion": "通过精心设计的低成本技术，学者可以有效管理LLMs的固有弱点，实现高效的人机协作，为文本分类任务提供了一种实用的互补学习方法"}}
{"id": "2512.07582", "pdf": "https://arxiv.org/pdf/2512.07582", "abs": "https://arxiv.org/abs/2512.07582", "authors": ["Guangyan Chen", "Meiling Wang", "Qi Shao", "Zichen Zhou", "Weixin Mao", "Te Cui", "Minzhao Zhu", "Yinan Deng", "Luojie Yang", "Zhanqi Zhang", "Yi Yang", "Hua Chen", "Yufeng Yue"], "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations", "categories": ["cs.RO"], "comment": null, "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.", "AI": {"tldr": "提出ViVLA模型，能够通过单次专家演示视频学习新任务，实现视觉-语言-动作的端到端机器人控制", "motivation": "现有VLA模型在超出训练分布的任务上泛化能力有限，而人类仅通过观察一次演示就能学习新技能，受此启发，希望开发能够从单次演示视频学习新任务的通用机器人操作策略", "method": "提出ViVLA模型，联合处理专家演示视频和机器人视觉观察，预测演示动作序列和后续机器人动作；开发可扩展的专家-智能体配对数据生成管道，从人类视频合成配对轨迹，共生成892,911个训练样本", "result": "在未见过的LIBERO任务上提升超过30%，跨具身视频保持35%以上增益；真实世界实验中从人类视频学习，在未见任务上提升超过38%", "conclusion": "ViVLA能够从单次专家演示视频有效学习新操作技能，实现了从观察中学习的能力，为通用机器人操作策略提供了新思路"}}
{"id": "2512.07580", "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "该论文研究了视觉大语言模型(VLLMs)中的token剪枝问题，发现在深层网络中现有训练无关的剪枝方法效果与随机剪枝相当，并提出了\"信息消失\"假设来解释这一现象。", "motivation": "VLLMs依赖大量视觉token表示图像，导致高计算成本。现有训练无关的剪枝方法在深层网络表现不佳，作者试图解释这一现象并提出更有效的剪枝策略。", "method": "提出量化token信息含量的方法：通过移除token后模型输出概率的变化来衡量信息含量。分析不同层视觉token的信息分布，发现\"信息地平线\"现象，并提出在深层使用随机剪枝的策略。", "result": "研究发现：1) 随着网络加深，视觉token信息逐渐均匀化并在中间层消失；2) 信息地平线位置随任务类型变化；3) 模型能力越强，信息地平线越深。结合随机剪枝的方法在Qwen-2.5-VL-7B上剪枝50%视觉token时能保持96.9%性能。", "conclusion": "视觉token在深层网络中信息逐渐消失，导致现有剪枝方法失效。在深层使用随机剪枝是有效策略，结合现有方法能达到最优效果，为VLLMs的高效推理提供了新思路。"}}
{"id": "2512.07577", "pdf": "https://arxiv.org/pdf/2512.07577", "abs": "https://arxiv.org/abs/2512.07577", "authors": ["Artur Czumaj", "Christian Sohler"], "title": "Property Testing of Computational Networks", "categories": ["cs.DS"], "comment": null, "summary": "In this paper we initiate the study of \\emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\\frac23$) any network that is \\emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \\emph{$(ε,δ)$-far}, which means that one needs to change an $ε$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $δ$-fraction of inputs from the desired function (or any function with a given property). To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model.", "AI": {"tldr": "研究计算网络的属性测试，特别是带权重的计算网络作为计算设备的测试框架设计", "motivation": "为计算网络设计属性测试算法，能够高效判断网络是否计算特定函数或具有特定属性，而无需完全验证网络功能", "method": "提出(ε,δ)-far距离概念，设计属性测试框架，以ReLU激活的简单神经布尔网络为例进行案例研究", "result": "证明对于此类网络，任何接近常数的函数都可以用与网络规模无关的查询复杂度进行测试；但在分布自由模型和普通测试模型中无法实现类似结果", "conclusion": "建立了计算网络属性测试的理论框架，展示了在某些模型下可实现高效测试，但在其他自然扩展模型中存在局限性"}}
{"id": "2512.07576", "pdf": "https://arxiv.org/pdf/2512.07576", "abs": "https://arxiv.org/abs/2512.07576", "authors": ["Xuecheng Li", "Weikuan Jia", "Komildzhon Sharipov", "Sharipov Hotam Beknazarovich", "Farzona S. Ataeva", "Qurbonaliev Alisher", "Yuanjie Zheng"], "title": "R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.", "AI": {"tldr": "提出R2MF-Net网络，用于多方向脊柱X射线图像的自动分割，以支持脊柱侧弯的定量评估", "motivation": "当前脊柱X射线图像分割主要依赖手动操作，耗时且不可重复，特别是在低对比度图像、肋骨阴影和组织重叠的情况下，需要自动化解决方案来提高临床工作效率", "method": "采用级联的粗分割网络和细分割网络架构，包含改进的Inception风格多分支特征提取器、循环残差跳跃连接模块、多尺度跨阶段跳跃机制和轻量级空间通道压缩激励模块", "result": "在包含228组冠状位、左弯和右弯脊柱X射线图像的临床多视图数据集上进行了评估，结果显示网络能够有效处理不同成像方向和对比度条件下的分割任务", "conclusion": "R2MF-Net通过创新的网络架构设计，实现了对多方向脊柱X射线图像的鲁棒自动分割，为脊柱侧弯的定量评估提供了可靠的技术支持"}}
{"id": "2512.07574", "pdf": "https://arxiv.org/pdf/2512.07574", "abs": "https://arxiv.org/abs/2512.07574", "authors": ["Xuecheng Li", "Weikuan Jia", "Komildzhon Sharipov", "Alimov Ruslan", "Lutfuloev Mazbutdzhon", "Ismoilov Shuhratjon", "Yuanjie Zheng"], "title": "Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework", "categories": ["eess.IV", "cs.CR", "cs.CV"], "comment": null, "summary": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.", "AI": {"tldr": "提出一种混合深度学习-放射组学框架，用于CT图像中肝脏肿瘤的精确三维分割", "motivation": "手动分割肝脏肿瘤存在速度慢、观察者依赖性强、难以标准化等问题，而自动分割面临病灶-实质对比度低、边界模糊或不完整、增强模式异质性、血管和邻近器官干扰等挑战", "method": "采用混合框架：1) 注意力增强级联U-Net进行初始分割；2) 三切片细化规则增强时间一致性；3) 提取728个放射组学特征并筛选为20个关键特征，使用随机森林分类器去除假阳性；4) 基于AlexNet的紧凑3D CNN在肿瘤边界窄带进行体素级重标记和轮廓平滑", "result": "该方法能够恢复薄小病灶、抑制孤立噪声、去除假阳性区域，实现肝脏和肝脏肿瘤的联合精确分割", "conclusion": "提出的混合深度学习-放射组学框架能够有效解决肝脏肿瘤CT分割中的各种挑战，为治疗规划、导航和疗效评估提供精确的三维分割结果"}}
{"id": "2512.07569", "pdf": "https://arxiv.org/pdf/2512.07569", "abs": "https://arxiv.org/abs/2512.07569", "authors": ["Joel Ekstrand", "Tor Mattsson", "Zahra Taghiyarrenani", "Slawomir Nowaczyk", "Jens Lundström", "Mikael Lindén"], "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.", "AI": {"tldr": "提出加权对比适应（WECA）方法，用于异常感知的时间序列预测，通过对比学习对齐正常和异常增强的表示，提升异常条件下的预测可靠性", "motivation": "现有深度预测模型在正常数据上表现良好，但在分布偏移（异常条件）下预测性能显著下降，而实际应用中（如ATM现金物流）需要可靠处理突发需求变化", "method": "提出加权对比适应（WECA），使用加权对比目标函数对齐正常和异常增强的表示，在保持异常相关信息的同时维持良性变化下的一致性", "result": "在全国ATM交易数据集上评估，WECA在异常影响数据上的SMAPE比正常训练基线提高了6.1个百分点，在正常数据上性能下降可忽略不计", "conclusion": "WECA能够在保持正常操作性能的同时，显著提升异常条件下的预测可靠性，为时间序列预测的鲁棒性提供了有效解决方案"}}
{"id": "2512.07568", "pdf": "https://arxiv.org/pdf/2512.07568", "abs": "https://arxiv.org/abs/2512.07568", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "AI": {"tldr": "提出DSRSD-Net框架，通过残差分解和语义去相关约束来解耦模态特定和模态共享信息，解决跨模态学习中的模态主导、冗余耦合和虚假相关等问题。", "motivation": "跨模态学习中存在模态主导、冗余信息耦合和虚假跨模态相关性问题，导致次优泛化和有限可解释性。高方差模态容易掩盖弱但语义重要的信号，而简单融合策略会以不受控方式纠缠模态共享和模态特定因素。", "method": "提出双流残差语义去相关网络(DSRSD-Net)，包含：1)双流表示学习模块，通过残差投影分离模态内(私有)和模态间(共享)潜在因子；2)残差语义对齐头，使用对比和回归目标将不同模态的共享因子映射到共同空间；3)去相关和正交性损失，正则化共享空间的协方差结构，同时强制共享流和私有流之间的正交性。", "result": "在两个大规模教育基准测试上的实验结果表明，DSRSD-Net在下一步预测和最终结果预测方面持续优于强单模态、早期融合、晚期融合和协同注意力基线。", "conclusion": "DSRSD-Net通过残差分解和显式语义去相关约束有效解耦模态特定和模态共享信息，解决了跨模态学习中的关键挑战，提高了预测性能和可解释性。"}}
{"id": "2512.07564", "pdf": "https://arxiv.org/pdf/2512.07564", "abs": "https://arxiv.org/abs/2512.07564", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "24 pages, 3 figures, 2 tables. Training-free self-correction framework for vision-language models. Code and implementation details will be released at: https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git", "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "AI": {"tldr": "提出一种无需训练的自校正框架，通过不确定性引导的视觉重关注机制减少视觉语言模型中的幻觉内容", "motivation": "视觉语言模型经常生成看似合理但实际错误的幻觉内容，现有方法需要重新训练或微调模型，缺乏无需参数更新的有效校正机制", "method": "提出训练免费的自校正框架，结合多维不确定性量化（令牌熵、注意力分散度、语义一致性、声明置信度）和注意力引导的裁剪机制，迭代精炼模型响应", "result": "在POPE和MMHAL BENCH基准测试上，使用Qwen2.5-VL-7B架构，幻觉率比基线降低9.8个百分点，对抗性分割上的对象存在准确率提高4.7个百分点", "conclusion": "提出的不确定性引导视觉重关注框架能有效减少视觉语言模型的幻觉，无需梯度更新，为可信赖多模态系统提供了有前景的方向"}}
{"id": "2512.07558", "pdf": "https://arxiv.org/pdf/2512.07558", "abs": "https://arxiv.org/abs/2512.07558", "authors": ["Shimin Zhang", "Xianwei Chen", "Yufan Shen", "Ziyuan Ye", "Jibin Wu"], "title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.", "AI": {"tldr": "提出ReLaX方法，通过调控大型推理模型的潜在动态来平衡探索与利用，解决强化学习验证奖励方法中的熵崩溃问题", "motivation": "强化学习验证奖励方法虽然能提升大型推理模型的推理能力，但会导致熵崩溃，造成策略过早收敛和性能饱和。现有方法主要关注token级别的熵调控，但潜在动态包含更丰富的计算结构来指导策略优化", "method": "利用Koopman算子理论获得模型隐藏状态动态的线性化表示，提出动态谱分散度来量化潜在动态的异质性，以此作为策略探索的直接指标。在此基础上提出ReLaX范式，在策略优化过程中显式地结合潜在动态来调控探索与利用", "result": "在多种多模态和纯文本推理基准测试中，ReLaX显著缓解了过早收敛问题，并持续实现了最先进的性能", "conclusion": "通过调控大型推理模型的潜在动态，ReLaX方法能够更有效地平衡探索与利用，提升推理性能并避免过早收敛，为大型推理模型的优化提供了新思路"}}
{"id": "2512.07544", "pdf": "https://arxiv.org/pdf/2512.07544", "abs": "https://arxiv.org/abs/2512.07544", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "AI": {"tldr": "提出MoCoRP框架，通过显式建模角色句子与回复之间的自然语言推理关系，提升基于角色的对话系统中角色一致性和对话质量", "motivation": "现有基于角色的对话数据集缺乏角色句子与回复之间的显式关系标注，导致模型难以有效捕捉角色信息，影响角色一致性和对话质量", "method": "使用NLI专家显式提取角色句子与回复之间的NLI关系，将这种关系信息融入语言模型，并扩展到大型语言模型的对齐调优", "result": "在ConvAI2和MPChat数据集上超越现有基线，在角色一致性和对话生成质量方面表现优异，定量和定性评估均有显著提升", "conclusion": "显式建模角色-回复关系能有效提升基于角色的对话系统的性能，MoCoRP框架为角色一致性对话生成提供了有效解决方案"}}
{"id": "2512.07540", "pdf": "https://arxiv.org/pdf/2512.07540", "abs": "https://arxiv.org/abs/2512.07540", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "AI": {"tldr": "本文提出在参考无关的自动机器翻译评估中的错误跨度检测任务上应用最小贝叶斯风险解码方法，以解决最大后验概率解码存在的问题。", "motivation": "现有的生成式错误跨度检测方法通常使用最大后验概率解码，假设模型估计的概率与人类标注的相似度完美相关。然而，作者观察到与人类标注不相似的标注可能获得比人类标注更高的模型似然度，这暴露了MAP解码的局限性。", "method": "采用最小贝叶斯风险解码方法，使用句子级别和跨度级别的相似度度量作为效用函数，基于候选假设与人类标注的近似相似度来选择最佳假设。同时提出MBR蒸馏技术来降低计算成本。", "result": "实验结果表明，MBR解码在系统级别、句子级别和跨度级别均优于MAP基线方法。通过MBR蒸馏，标准的贪婪模型能够匹配MBR解码的性能，有效消除了推理时的延迟瓶颈。", "conclusion": "最小贝叶斯风险解码能够有效解决生成式错误跨度检测中MAP解码的局限性，通过相似度度量选择更接近人类标注的候选假设，并通过蒸馏技术实现高效部署。"}}
{"id": "2512.07533", "pdf": "https://arxiv.org/pdf/2512.07533", "abs": "https://arxiv.org/abs/2512.07533", "authors": ["Yuzhou Nie", "Hongwei Li", "Chengquan Guo", "Ruizhe Jiang", "Zhun Wang", "Bo Li", "Dawn Song", "Wenbo Guo"], "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.", "AI": {"tldr": "提出VulnLLM-R，首个专门用于漏洞检测的推理大语言模型，通过程序状态推理而非简单模式匹配来提升漏洞检测的泛化能力和效果。", "motivation": "现有最先进的推理LLM通常规模过大、闭源或在漏洞检测方面性能有限，需要专门针对漏洞检测任务优化的推理模型。", "method": "提出新颖的训练方法，包括专门的数据选择、推理数据生成、推理数据过滤与校正，以及测试阶段优化，训练了一个70亿参数的推理模型。", "result": "在Python、C/C++和Java的SOTA数据集上，VulnLLM-R在效果和效率上优于最先进的静态分析工具以及开源和商业大型推理模型。构建的智能体框架在实际项目中优于CodeQL和AFL++，并发现了活跃维护仓库中的零日漏洞。", "conclusion": "这项工作代表了使用专门推理模型驱动的AI智能体实现真实世界项目级漏洞检测的开创性努力，为AI驱动的漏洞检测提供了新的方向。"}}
{"id": "2512.07528", "pdf": "https://arxiv.org/pdf/2512.07528", "abs": "https://arxiv.org/abs/2512.07528", "authors": ["Nishanth Venkatesh", "Andreas A. Malikopoulos"], "title": "Model-Based Reinforcement Learning Under Confounding", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 figures - decompressed draft", "summary": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.", "AI": {"tldr": "该论文研究在存在混淆变量的上下文马尔可夫决策过程中进行基于模型的强化学习，其中上下文未观测到并在离线数据集中引入混淆效应。", "motivation": "传统模型学习方法在存在未观测混淆变量的环境中存在根本性不一致问题，因为行为策略下生成的转移和奖励机制与评估基于状态的策略所需的干预量不对应。", "method": "采用近端离策略评估方法，在代理变量满足温和可逆性条件下，仅使用可观测的状态-动作-奖励轨迹识别混淆的奖励期望。结合行为平均转移模型，构建一个替代MDP，其贝尔曼算子对基于状态的策略是良定义且一致的，并与最大因果熵模型学习框架无缝集成。", "result": "提出的公式能够在上下文信息未观测、不可用或收集不切实际的环境中，实现有原则的模型学习和规划。", "conclusion": "该方法解决了混淆环境中基于模型的强化学习问题，通过构建替代MDP和近端评估技术，为存在未观测混淆变量的离线数据集提供了理论一致的解决方案。"}}
{"id": "2512.07527", "pdf": "https://arxiv.org/pdf/2512.07527", "abs": "https://arxiv.org/abs/2512.07527", "authors": ["Fei Yu", "Yu Liu", "Luyang Tang", "Mingchao Sun", "Zengye Ge", "Rui Bu", "Yuchao Jin", "Haisen Zhao", "He Sun", "Yangyan Li", "Mu Xu", "Wenzheng Chen", "Baoquan Chen"], "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs. Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.", "AI": {"tldr": "该论文提出了一种从极端离天底卫星图像生成城市级三维重建的方法，通过2.5D高度图建模和生成式纹理恢复网络，解决了从稀疏轨道图像合成地面视角的极端视点外推挑战。", "motivation": "从卫星图像进行城市级三维重建面临极端视点外推的挑战，需要从具有严重透视畸变和纹理缺陷的稀疏轨道图像中推断近90度的视点差距，导致现有NeRF和3DGS等重建方法失效。", "method": "提出两种针对城市结构和卫星输入的设计：1) 将城市几何建模为2.5D高度图，实现为Z单调有符号距离场(SDF)，匹配自上而下的城市建筑布局；2) 通过可微分渲染技术从卫星图像绘制网格外观，并训练生成式纹理恢复网络从退化输入中恢复高频、合理的纹理细节。", "result": "在真实世界4平方公里区域上仅使用少量卫星图像进行重建，在合成逼真地面视图方面达到最先进性能，生成视觉吸引人且可作为高保真应用就绪资产用于城市规划等下游任务。", "conclusion": "该方法通过2.5D高度图建模和生成式纹理增强，成功解决了从极端离天底卫星图像进行城市级三维重建的挑战，实现了从轨道到地面的逼真视图合成，为城市规划和模拟提供了实用的高质量资产。"}}
{"id": "2512.07522", "pdf": "https://arxiv.org/pdf/2512.07522", "abs": "https://arxiv.org/abs/2512.07522", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Björn Deiseroth"], "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "AI": {"tldr": "LIME是一种通过语言元数据嵌入提升LLM数据效率的方法，将语法、语义和上下文属性的元数据融入词元嵌入中，显著提高预训练效率并增强语言建模能力。", "motivation": "当前预训练解码器语言模型依赖大量高质量数据，但这类数据的可用性已接近极限。虽然元数据常用于创建和整理数据集，但其作为直接训练信号的潜力尚未得到充分探索。", "method": "提出LIME方法，将捕获语法、语义和上下文属性的语言元数据嵌入到词元嵌入中。还开发了LIME+1变体，使用偏移元数据来指导词元生成。", "result": "LIME使模型对训练数据分布的适应速度提升高达56%，仅增加0.01%的参数且计算开销可忽略。改进分词效果，显著增强语言建模能力和生成任务性能。LIME+1变体在推理任务上提升高达38%，算术准确率提升高达35%。", "conclusion": "LIME通过有效利用语言元数据作为直接训练信号，显著提高了LLM的数据效率和性能，为突破当前数据限制提供了有前景的解决方案。"}}
{"id": "2512.07515", "pdf": "https://arxiv.org/pdf/2512.07515", "abs": "https://arxiv.org/abs/2512.07515", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "AI": {"tldr": "提出SPAD方法，通过将每个token的概率归因到七个不同来源，并结合词性聚合来检测RAG系统中的幻觉问题", "motivation": "现有方法将幻觉归因于内部知识（FFNs）和检索上下文之间的二元冲突，但这种视角不完整，忽略了生成过程中其他组件的影响，如用户查询、先前生成的token、当前token本身以及最终的LayerNorm调整", "method": "首先将每个token的概率数学归因到七个不同来源：查询、RAG、过去token、当前token、FFN、最终LayerNorm和初始嵌入；然后按词性标签聚合这些分数，量化不同组件如何驱动特定语言类别", "result": "通过识别异常（如名词依赖最终LayerNorm），SPAD能有效检测幻觉；大量实验表明SPAD实现了最先进的性能", "conclusion": "SPAD通过全面的概率归因和语法聚合，为RAG系统中的幻觉检测提供了更完整和有效的解决方案"}}
{"id": "2512.07514", "pdf": "https://arxiv.org/pdf/2512.07514", "abs": "https://arxiv.org/abs/2512.07514", "authors": ["Junkai Lin", "Hang Long", "Huipeng Guo", "Jielei Zhang", "JiaYi Yang", "Tianle Guo", "Yang Yang", "Jianwen Li", "Wenxiao Zhang", "Matthias Nießner", "Wei Yang"], "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "categories": ["cs.CV"], "comment": null, "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "AI": {"tldr": "提出MeshRipple方法，通过结构化自回归生成艺术家级网格，解决现有方法因内存限制导致的长距离几何依赖断裂问题", "motivation": "现有自回归网格生成器将面序列化并训练截断片段，使用滑动窗口推理来应对内存限制，但这种不匹配破坏了长距离几何依赖，导致空洞和碎片化组件", "method": "引入MeshRipple方法，包含三个关键创新：1) 前沿感知的BFS标记化，使生成顺序与表面拓扑对齐；2) 扩展预测策略，保持连贯、连接的表面增长；3) 稀疏注意力全局内存，提供有效无界的感受野以解决长距离拓扑依赖", "result": "MeshRipple能够生成具有高表面保真度和拓扑完整性的网格，在性能上超越了近期强基线方法", "conclusion": "MeshRipple通过结构化自回归生成方法，有效解决了网格生成中的长距离依赖问题，实现了更高质量和更完整的网格生成"}}
{"id": "2512.07509", "pdf": "https://arxiv.org/pdf/2512.07509", "abs": "https://arxiv.org/abs/2512.07509", "authors": ["Nikita Gabdullin"], "title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 5 figures, 1 table, 4 equations", "summary": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.", "AI": {"tldr": "探索可用于神经网络预配置潜在空间更快训练的可能向量系统", "motivation": "神经网络性能与其潜在空间嵌入分布特性密切相关。预定义向量系统（如An根系向量）可作为潜在空间配置目标，确保期望的LS结构。主要优势是可以在没有分类层的情况下训练分类器神经网络，便于处理具有极多类别的数据集。", "method": "提供可能向量系统的更全面概述，包括其属性和构造方法。使用这些系统配置编码器和视觉变换器的潜在空间，显著加速ImageNet-1K和50k-600k类别LSC训练。展示使用特定类别数的最小LS维度可实现更快收敛。", "result": "显著加速了ImageNet-1K和50k-600k类别LSC训练。使用最小潜在空间维度可实现更快收敛，这对减少存储神经网络嵌入的向量数据库大小具有潜在优势。", "conclusion": "预定义向量系统为神经网络训练提供了有效的潜在空间配置方法，特别适用于大规模类别分类任务，能够显著加速训练过程并减少存储需求。"}}
{"id": "2512.07507", "pdf": "https://arxiv.org/pdf/2512.07507", "abs": "https://arxiv.org/abs/2512.07507", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "categories": ["cs.RO", "cs.SE"], "comment": null, "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "AI": {"tldr": "提出VP-AutoTest虚拟-物理融合自动驾驶测试平台，解决传统测试方法面临的挑战，通过融合多种虚拟和物理元素实现更真实、高效的自动驾驶测试。", "motivation": "传统自动驾驶测试方法（虚拟仿真、封闭场地、公共道路测试）存在车辆状态不真实、测试能力有限、成本高等问题，虚拟-物理融合测试虽然具有潜力，但仍面临元素类型有限、测试范围窄、评估指标固定等挑战。", "method": "集成10多种虚拟和物理元素（车辆、行人、路边基础设施等），支持单车交互和多车协同测试，采用对抗性测试和平行推演加速故障检测，通过OBU和Redis通信实现V2V和V2I协同，结合多维评估框架和AI专家系统进行性能评估和缺陷诊断。", "result": "平台能够复现真实交通参与者的多样性，支持全级别协同自动化，通过虚拟-物理融合测试结果与真实实验对比进行可信度自评估，确保测试的保真度和效率。", "conclusion": "VP-AutoTest平台解决了虚拟-物理融合测试的关键挑战，提供了更全面、高效的自动驾驶测试解决方案，通过多维评估和AI诊断提升了测试质量，并通过可信度自评估确保了测试结果的可靠性。"}}
{"id": "2512.07504", "pdf": "https://arxiv.org/pdf/2512.07504", "abs": "https://arxiv.org/abs/2512.07504", "authors": ["Ryota Okumura", "Kaede Shiohara", "Toshihiko Yamasaki"], "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026, 8 pages, supplementary included. Dataset and code: https://github.com/RyotaOkumura/ControlVP", "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "AI": {"tldr": "ControlVP是一个用户引导的框架，用于修正AI生成图像中的消失点不一致问题，通过几何约束增强空间结构一致性。", "motivation": "当前文本到图像模型（如Stable Diffusion）虽然视觉质量优秀，但经常存在几何不一致问题，特别是消失点不一致，导致生成场景的结构真实性受损，尤其是在建筑场景中。", "method": "扩展预训练扩散模型，结合建筑轮廓的结构引导，引入几何约束来显式鼓励图像边缘与透视线索的对齐。", "result": "该方法增强了全局几何一致性，同时保持了与基线相当的视觉保真度，特别适用于需要准确空间结构的应用（如图像到3D重建）。", "conclusion": "ControlVP提供了一个有效的用户引导框架，能够修正AI生成图像中的消失点不一致问题，提高空间结构真实感，为图像到3D重建等应用提供了有价值的工具。"}}
{"id": "2512.07503", "pdf": "https://arxiv.org/pdf/2512.07503", "abs": "https://arxiv.org/abs/2512.07503", "authors": ["Yao Teng", "Zhihuan Jiang", "Han Shi", "Xian Liu", "Xuefei Ning", "Guohao Dai", "Yu Wang", "Zhenguo Li", "Xihui Liu"], "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.", "AI": {"tldr": "提出SJD++算法，一种无需训练的概率并行解码方法，用于加速离散自回归文本到图像生成模型的推理过程", "motivation": "大型自回归模型生成高质量高分辨率图像时速度缓慢，因为需要数百到数千次顺序前向传递进行下一个令牌预测", "method": "结合Jacobi解码的迭代多令牌预测机制和推测采样的概率草稿-验证机制，并在验证后重用高置信度草稿令牌而不是重新采样", "result": "在多个代表性自回归文本到图像生成模型上实现2-3倍推理延迟减少和2-7倍步骤压缩，同时保持视觉质量无显著下降", "conclusion": "SJD++是一种有效的训练免费加速方法，显著提升自回归文本到图像生成的推理效率"}}
{"id": "2512.07501", "pdf": "https://arxiv.org/pdf/2512.07501", "abs": "https://arxiv.org/abs/2512.07501", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "AI": {"tldr": "提出AutoICE框架，通过LLM驱动的进化搜索自动合成可验证的C代码，解决现有方法因领域特定语料稀缺和隐式知识形式化困难导致的语法语义错误问题。", "motivation": "从自然语言需求自动合成可验证代码能确保软件正确性和可靠性，降低形式化方法采用门槛。现有LLM方法因领域特定预训练语料稀缺和隐式知识形式化困难，存在严重语法语义错误。", "method": "采用LLM驱动的进化搜索方法，包含多样化个体初始化、协作交叉实现多样化迭代更新以减轻单智能体迭代中的错误传播，以及自反思变异促进隐式知识发现。", "result": "AutoICE成功验证90.36%的代码，优于最先进方法。在开发者友好数据集变体上达到88.33%验证成功率，显著超过SOTA方法的65%。", "conclusion": "AutoICE通过LLM驱动的进化搜索有效合成可验证C代码，在代码验证成功率上显著超越现有方法，为自动形式化提供了有效解决方案。"}}
{"id": "2512.07500", "pdf": "https://arxiv.org/pdf/2512.07500", "abs": "https://arxiv.org/abs/2512.07500", "authors": ["Penghui Liu", "Jiangshan Wang", "Yutong Shen", "Shanhui Mo", "Chenyang Qi", "Yue Ma"], "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.", "AI": {"tldr": "提出MultiMotion框架，通过Mask-aware Attention Motion Flow（AMF）和RectPC采样器，解决多目标视频运动迁移中运动纠缠和缺乏对象级控制的问题", "motivation": "多目标视频运动迁移在Diffusion Transformer（DiT）架构中面临运动纠缠和缺乏对象级控制的挑战，现有方法难以精确控制多个独立对象的运动", "method": "1. 提出Mask-aware Attention Motion Flow（AMF），利用SAM2掩码在DiT管道中显式解耦和控制多个对象的运动特征；2. 引入RectPC，一种高阶预测器-校正器求解器，用于高效准确的采样，特别适合多实体生成；3. 构建首个专门用于DiT多目标运动迁移的基准数据集", "result": "MultiMotion实现了精确、语义对齐且时间一致的多目标运动迁移，保持了DiT的高质量和可扩展性，在构建的基准数据集上表现出色", "conclusion": "MultiMotion通过AMF机制和RectPC采样器，成功解决了多目标视频运动迁移中的关键挑战，为DiT架构提供了有效的多对象控制能力，推动了该领域的发展"}}
{"id": "2512.07498", "pdf": "https://arxiv.org/pdf/2512.07498", "abs": "https://arxiv.org/abs/2512.07498", "authors": ["Chih-Chung Hsu", "Shao-Ning Chen", "Chia-Ming Lee", "Yi-Fang Wang", "Yi-Shiuan Chou"], "title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior", "categories": ["cs.CV"], "comment": "16 pages (including appendix)", "summary": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.", "AI": {"tldr": "提出一种基于拉普拉斯正则化图卷积网络(LR-GCN)的深度伪造检测方法，能够在不稳定、无序的面部序列中鲁棒地检测DeepFake视频", "motivation": "现有深度伪造检测器通常假设面部序列在时间上一致且干净，但现实场景中压缩伪影、遮挡和对抗攻击会破坏面部检测的稳定性，导致无效或错误检测的面部。需要一种能够在噪声或无序面部序列中鲁棒检测的方法", "method": "提出LR-GCN方法，包含三个关键组件：1) 无序时序图嵌入(OF-TGE)，基于语义亲和度将帧级CNN特征组织成自适应稀疏图；2) 双级稀疏机制，抑制无效面部的影响；3) 显式图拉普拉斯谱先验，在谱域中突出结构异常和伪造伪影，通过低通GCN聚合进行整合", "result": "在FF++、Celeb-DFv2和DFDC数据集上的广泛实验表明，LR-GCN实现了最先进的性能，在严重全局和局部干扰（包括缺失面部、遮挡和对抗性扰动）下表现出显著改进的鲁棒性", "conclusion": "提出的LR-GCN方法通过自适应稀疏图嵌入、双级稀疏机制和显式拉普拉斯谱先验，能够在不稳定面部序列中有效检测DeepFake，为现实世界深度伪造检测提供了鲁棒解决方案"}}
{"id": "2512.07497", "pdf": "https://arxiv.org/pdf/2512.07497", "abs": "https://arxiv.org/abs/2512.07497", "authors": ["JV Roig"], "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "categories": ["cs.AI", "cs.SE"], "comment": "48 pages, 3 tables, 2 listings", "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "AI": {"tldr": "该论文研究大型语言模型在作为具有工具使用能力的自主代理时如何失败，通过对三个代表性模型在多种场景下的执行轨迹进行细粒度分析，揭示了成功策略和常见失败模式。", "motivation": "当前研究主要关注LLM的聚合性能分数，但缺乏对代理场景中具体失败模式的深入理解。需要了解模型在自主操作时的行为模式和失败原因，以便为可靠的企业部署提供指导。", "method": "使用Kamiwaza Agentic Merit Index (KAMI) v0.1基准测试，分析900个执行轨迹，涵盖三个代表性模型（Granite 4 Small、Llama 4 Maverick、DeepSeek V3.1），在文件系统、文本提取、CSV分析和SQL场景中进行细粒度的逐试验行为分析。", "result": "研究发现模型规模本身不能预测代理鲁棒性；DeepSeek V3.1的优越可靠性主要来自后训练强化学习而非架构或规模；识别出四种常见失败原型：缺乏基础的过早行动、替代缺失实体的过度帮助、干扰诱导的上下文污染、负载下的脆弱执行。", "conclusion": "可靠的代理部署不仅需要更强的模型，还需要有意的训练和设计选择，以加强验证、约束发现和对真实数据源的遵守。需要强调交互基础、恢复行为和环境感知适应的代理评估方法。"}}
{"id": "2512.07487", "pdf": "https://arxiv.org/pdf/2512.07487", "abs": "https://arxiv.org/abs/2512.07487", "authors": ["David M. Allison", "Stephen Herzog"], "title": "Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility", "categories": ["cs.CY", "cs.AI"], "comment": "Best Paper Award (2025) from Risk Analysis as one of the articles published in the journal that year with the most significant impacts to the theory or practice of risk analysis. Main text: 17 pages, 5 tables, 5 figures. Online appendix: 4 pages, 3 figures, 1 table. Online simulation tool for the formal model available here: https://david-m-allison.github.io/ProliferationSimulation", "summary": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.", "AI": {"tldr": "本文探讨人工智能与核武器扩散之间的关系，分析技术军备竞赛对核活动（不）可见性的影响，并提出量化模型评估扩散风险。", "motivation": "新兴颠覆性技术正在重塑核风险格局，特别是人工智能加速了扩散使能技术（PETs）的发展，挑战了传统的监测与核查方法，需要在决策关键期分析这一动态。", "method": "开发了一个基于相对优势指数（RAI）的形式化模型，量化PETs与检测增强技术（DETs）之间的平衡变化，通过可复现的情景模拟评估不同PET增长率与DET投资策略对核突破风险的影响。", "result": "模型显示不对称技术进步（特别是AI驱动的PET逻辑增长与逐步的DET改进）扩大了扩散可探测性的不确定性范围，检测可能不再足够，需要更广泛的PET治理。", "conclusion": "政府与国际组织应投资于足够灵活的政策与工具，以跟上未来技术的发展步伐，应对人工智能带来的核扩散新挑战。"}}
{"id": "2512.07483", "pdf": "https://arxiv.org/pdf/2512.07483", "abs": "https://arxiv.org/abs/2512.07483", "authors": ["Daniel Fürst", "Matthijs Jansen op de Haar", "Mennatallah El-Assady", "Daniel A Keim", "Maximilian T. Fischer"], "title": "SemanticTours: A Conceptual Framework for Non-Linear, Knowledge Graph-Driven Data Tours", "categories": ["cs.HC"], "comment": "14 pages, 9 figures, 2 tables", "summary": "Interactive tours help users explore datasets and provide onboarding. They rely on a linear sequence of views, showing a curated set of relevant data selections and introduce user interfaces. Existing frameworks of tours, however, often do not allow for branching and refining hypotheses outside of a rigid sequence, which is important in knowledge-centric domains such as law. For example, lawyers performing analytical case analysis need to iteratively weigh up different legal norms and construct strings of arguments. To address this gap, we propose SemanticTours, a semantic, graph-based model of tours that shifts from a sequence-based towards a graph-based navigation. Our model constructs a domain-specific knowledge graph that connects data elements based on user-definable semantic relationships. These relationships enable non-linear graph navigation that defines tours. We apply SemanticTours to the domain of law and conceptualize a visual analytics design and interaction concept for analytical reasoning in legal case analysis. Our concept accounts for the inherent complexity of graph-based tours using aggregated graph nodes and supporting navigation with a semantic lens. During an evaluation with six domain experts from law, they suggest that graph-based tours better support their analytical reasoning than sequences. Our work opens research opportunities for such tours to support analytical reasoning in law and other knowledge-centric domains.", "AI": {"tldr": "提出SemanticTours框架，将数据导览从线性序列转向基于知识图谱的非线性导航，特别针对法律领域设计", "motivation": "现有数据导览框架采用线性序列，不支持分支和假设细化，而知识密集型领域（如法律）需要迭代权衡不同规范并构建论证链", "method": "提出基于语义的图模型，构建领域特定知识图谱，通过用户可定义的语义关系连接数据元素，实现非线性图导航定义导览", "result": "在法律领域应用SemanticTours，设计可视化分析概念，使用聚合图节点和语义镜头支持导航，专家评估表明图基导览比序列更好支持分析推理", "conclusion": "SemanticTours为法律和其他知识密集型领域的分析推理开辟了研究机会，图基导览比传统序列方法更有效"}}
{"id": "2512.07482", "pdf": "https://arxiv.org/pdf/2512.07482", "abs": "https://arxiv.org/abs/2512.07482", "authors": ["Florian Lüttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 8 figures", "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "AI": {"tldr": "提出一个从真实世界交通数据中识别和生成相关关键场景的处理链，重点关注高速公路变道场景", "motivation": "自动驾驶系统需要在各种相关场景中可靠运行，但识别完整的相关驾驶场景具有挑战性，特别是随着技术复杂性增加，\"未知不安全\"场景的数量也在增加", "method": "1) 采集和处理高速公路真实交通数据；2) 在轨迹数据上应用关键性度量评估场景；3) 基于记录场景生成合成场景；4) 通过采样生成合成关键场景", "result": "开发了一个处理链，能够识别安全相关场景，提取这些场景的数据驱动方法，并通过采样在高速公路上生成合成关键场景", "conclusion": "该方法能够有效识别安全相关驾驶场景并生成合成关键场景，为自动驾驶系统的验证和开发提供了重要工具，特别是在高速公路变道场景中"}}
{"id": "2512.07480", "pdf": "https://arxiv.org/pdf/2512.07480", "abs": "https://arxiv.org/abs/2512.07480", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Zihan Zheng", "Yuan Zhang", "Yan Lu"], "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance", "categories": ["cs.CV"], "comment": null, "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.", "AI": {"tldr": "提出S2VC，一种基于单步扩散模型的视频编解码器，通过语义-时间引导实现低码率下的高质量视频重建，显著降低采样复杂度。", "motivation": "传统和神经视频编解码器在低码率下感知质量提升有限，现有方法要么生成能力不足导致伪影，要么依赖预训练扩散模型但采样复杂度高。需要平衡高质量重建与计算效率。", "method": "提出S2VC框架：1) 条件编码框架结合高效单步扩散生成器；2) 上下文语义引导从缓冲特征提取帧自适应语义，替代文本描述；3) 时间一致性引导在扩散U-Net中强制帧间时序一致性。", "result": "S2VC在感知质量上达到最先进水平，相比先前感知方法平均节省52.73%的码率，同时显著降低采样成本，实现高效高质量视频压缩。", "conclusion": "单步扩散模型为高效高质量视频压缩提供了有前景的解决方案，通过语义和时间引导机制，在保持低采样复杂度的同时显著提升感知质量。"}}
{"id": "2512.07474", "pdf": "https://arxiv.org/pdf/2512.07474", "abs": "https://arxiv.org/abs/2512.07474", "authors": ["Yifei Huang", "Tianyu Yan", "Sitong Gong", "Xiwei Gao", "Caixin Kang", "Ruicong Liu", "Huchuan Lu", "Bo Zheng"], "title": "Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.", "AI": {"tldr": "开发了一个端到端系统，能够将文学作品转化为沉浸式多角色对话体验，通过两阶段训练管道解决LLM驱动角色的角色漂移和叙事连贯性问题。", "motivation": "解决LLM驱动角色面临的两个核心挑战：1) 通用LLM存在角色漂移问题，难以保持角色一致性；2) 角色能力超出故事世界约束，导致叙事不连贯（剧透泄漏）和鲁棒性失败（框架破坏）。", "method": "采用两阶段训练管道：1) 深度角色对齐（DPA）阶段，使用无数据强化微调实现深度角色保真度；2) 连贯性和鲁棒性增强（CRE）阶段，利用故事时间感知知识图谱和第二次检索基础训练来架构性地强制执行叙事约束。", "result": "使用儒勒·凡尔纳的《海底两万里》进行多阶段评估：实验室研究显示DPA管道使专门化模型在角色特定指标上优于GPT-4o；CRE阶段在连贯性和鲁棒性指标上达到接近完美的性能；5天实地日记研究验证了系统有效性。", "conclusion": "角色优先的自训练是可信度的基础，而明确的故事时间约束对于维持连贯、抗中断的移动网络体验至关重要。系统为AI驱动叙事系统提供了实用的设计指南。"}}
{"id": "2512.07472", "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "提出Affordance Field Intervention (AFI)框架，通过3D空间可操作性场引导视觉-语言-动作模型，帮助机器人摆脱记忆陷阱，增强在分布偏移场景下的鲁棒性。", "motivation": "当前视觉-语言-动作模型在机器人操作中面临\"记忆陷阱\"问题：当测试场景发生变化时，模型倾向于复现记忆中的轨迹而非适应新环境，这源于端到端设计缺乏显式的3D空间推理能力。", "method": "提出轻量级混合框架AFI：1) 通过本体感知检测记忆陷阱；2) 将机器人重新定位到高可操作性区域；3) 提出可操作性驱动的路径点来锚定VLA生成的动作；4) 使用基于可操作性场的评分器选择累积可操作性最高的轨迹。", "result": "在不同VLA骨干网络（π₀和π₀.₅）上，真实机器人平台上的分布偏移场景平均提升23.5%，在LIBERO-Pro基准测试上提升20.2%，验证了AFI在增强VLA鲁棒性方面的有效性。", "conclusion": "AFI框架通过整合3D空间可操作性场，成功解决了VLA模型在分布偏移下的记忆陷阱问题，提供了一种轻量级、可插拔的增强方案，显著提升了机器人操作在陌生环境中的适应能力。"}}
{"id": "2512.07469", "pdf": "https://arxiv.org/pdf/2512.07469", "abs": "https://arxiv.org/abs/2512.07469", "authors": ["Xiangpeng Yang", "Ji Xie", "Yiyuan Yang", "Yan Huang", "Min Xu", "Qiang Wu"], "title": "Unified Video Editing with Temporal Reasoner", "categories": ["cs.CV"], "comment": "Project Page: https://videocof.github.io/", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "AI": {"tldr": "提出VideoCoF方法，通过Chain-of-Frames框架解决视频编辑中精度与统一性的权衡问题，实现无需用户提供掩码的精确视频编辑", "motivation": "现有视频编辑方法面临关键权衡：专家模型精度高但依赖任务特定的先验知识（如掩码），难以统一；而统一的时序上下文学习模型无需掩码但缺乏显式空间线索，导致指令到区域映射不精确。需要解决这一冲突", "method": "提出VideoCoF，一种受Chain-of-Thought启发的Chain-of-Frames方法。强制视频扩散模型遵循\"观察、推理、编辑\"流程：先预测推理令牌（编辑区域潜在表示），再生成目标视频令牌。引入RoPE对齐策略，利用推理令牌确保运动对齐并支持超出训练时长的长度外推", "result": "仅使用5万视频对的最小数据成本，在VideoCoF-Bench上达到最先进性能。验证了方法的效率和有效性，实现了精确的指令到区域对齐和细粒度视频编辑", "conclusion": "VideoCoF通过显式推理步骤解决了视频编辑中精度与统一性的冲突，无需用户提供掩码即可实现精确的指令到区域对齐。提出的RoPE对齐策略确保了运动一致性，支持长度外推，为视频编辑提供了高效统一的解决方案"}}
{"id": "2512.07464", "pdf": "https://arxiv.org/pdf/2512.07464", "abs": "https://arxiv.org/abs/2512.07464", "authors": ["Haolin Song", "Hongbo Zhu", "Tao Yu", "Yan Liu", "Mingqi Yuan", "Wengang Zhou", "Hua Chen", "Houqiang Li"], "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "AI": {"tldr": "提出一种融合地形感知、步态调节和全身控制的人形机器人感知运动框架，通过实时地形重建实现复杂地形下的鲁棒行走", "motivation": "当前全尺寸人形机器人在复杂地形（如长楼梯）上的可靠运动仍然具有挑战性，有限的感知能力、模糊的地形线索和步态时序适应不足容易导致失去平衡", "method": "使用向下深度相机观察脚部周围支撑区域，通过紧凑U-Net实时重建密集的自我中心高度图；将感知高度图与本体感知观测输入统一策略，生成关节命令和全局步进相位信号；采用单阶段连续师生训练方案进行高效策略学习和知识迁移", "result": "在31自由度、1.65米高的人形机器人上进行了实验，在仿真和真实环境中均展示了鲁棒的运动能力，包括前后上下楼梯以及跨越46厘米间隙", "conclusion": "提出的感知运动框架成功地将地形感知、步态调节和全身控制融合到单一强化学习策略中，实现了人形机器人在复杂地形下的自适应鲁棒行走"}}
{"id": "2512.07462", "pdf": "https://arxiv.org/pdf/2512.07462", "abs": "https://arxiv.org/abs/2512.07462", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "AI": {"tldr": "该论文通过扩展FAIRGAME框架，利用博弈论方法系统评估大型语言模型在重复社会困境中的战略行为，揭示其合作倾向、激励敏感性和跨语言差异等行为特征。", "motivation": "随着大型语言模型越来越多地作为自主决策者在交互式多智能体系统和人类社会环境中运行，理解其战略行为对安全性、协调性以及AI驱动的社会经济基础设施设计具有深远意义。需要能够捕捉LLM决策背后意图的方法，而不仅仅是其输出内容。", "method": "扩展FAIRGAME框架，引入两个互补的博弈环境：1）收益缩放的囚徒困境，用于隔离对激励强度的敏感性；2）集成多智能体公共物品博弈，具有动态收益和多智能体历史记录。使用传统监督分类模型在经典重复博弈策略上进行训练，并将其应用于FAIRGAME轨迹来分析LLM行为意图。", "result": "研究发现：1）LLM在不同模型和语言中表现出一致的行为特征，包括激励敏感的合作、跨语言分歧和最终博弈阶段的背叛倾向；2）LLM展现出系统性的、模型和语言依赖的行为意图；3）语言框架有时产生的影响与架构差异一样强烈。", "conclusion": "该研究为审计LLM作为战略智能体提供了统一的方法论基础，揭示了系统性合作偏差，对AI治理、集体决策和安全多智能体系统设计具有直接意义。"}}
{"id": "2512.07459", "pdf": "https://arxiv.org/pdf/2512.07459", "abs": "https://arxiv.org/abs/2512.07459", "authors": ["Xiangjun Tang", "Biao Zhang", "Peter Wonka"], "title": "Human Geometry Distribution for 3D Animation Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.", "AI": {"tldr": "提出一种用于3D人体动画生成的两阶段框架，通过分布式的潜在表示和生成式动画模型，实现高质量、多样化的人体几何动画生成。", "motivation": "生成逼真的人体几何动画面临挑战，需要建模自然的服装动态和精细几何细节，同时数据有限。现有方法在几何生成质量和动画多样性方面存在不足。", "method": "采用两阶段框架：第一阶段学习紧凑的基于分布的潜在表示，建立SMPL与虚拟化身几何之间的均匀映射；第二阶段引入生成式动画模型，通过身份条件设计在保持长期一致性的同时生成多样化短期过渡动画。", "result": "潜在空间生成的人体几何质量超越先前方法（Chamfer距离降低90%）；动画模型合成的动画具有详细自然的动态效果（用户研究得分提高2.2倍），在所有评估指标上均取得最佳结果。", "conclusion": "提出的两阶段框架成功解决了有限数据下高质量人体几何动画生成的挑战，通过分布式潜在表示和生成式动画模型的结合，实现了逼真且多样化的人体动画生成。"}}
{"id": "2512.07454", "pdf": "https://arxiv.org/pdf/2512.07454", "abs": "https://arxiv.org/abs/2512.07454", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "AI": {"tldr": "提出Persian-Phi模型，通过课程学习将单语英语Phi-3 Mini模型高效适配到波斯语，仅需3.8B参数即可实现多语言能力", "motivation": "AI民主化受到高计算成本的阻碍，特别是为低资源语言训练大语言模型。需要一种资源高效的方法将先进LLM扩展到资源匮乏的语言", "method": "采用课程学习管道：1) 使用双语叙事(Tiny Stories)进行\"预热\"阶段对齐嵌入；2) 持续预训练；3) 通过参数高效微调(PEFT)进行指令调优", "result": "Persian-Phi在HuggingFace的Open Persian LLM Leaderboard上取得竞争性结果，证明小规模模型也能实现稳健的多语言能力", "conclusion": "提供了一个经过验证、可扩展的框架，能够以最小硬件资源将最先进的LLM扩展到代表性不足的语言，挑战了多语言能力需要大规模模型或多语言基线的假设"}}
{"id": "2512.07453", "pdf": "https://arxiv.org/pdf/2512.07453", "abs": "https://arxiv.org/abs/2512.07453", "authors": ["Van An Nguyen", "Vuong Khang Huynh", "Ho Nam Duong", "Huu Loi Bui", "Hai Anh Ha", "Quang Dung Le", "Le Quoc Dung Ngo", "Tan Dat Nguyen", "Ngoc Ngu Nguyen", "Hoai Thuong Nguyen", "Zhao Song", "Le Hong Trang", "The Anh Han"], "title": "Social welfare optimisation in well-mixed and structured populations", "categories": ["physics.soc-ph", "cs.AI", "cs.MA", "math.OC", "nlin.AO"], "comment": null, "summary": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.", "AI": {"tldr": "研究在混合和结构化群体中通过激励机制优化社会福利，探索在促进合作的同时最大化社会福利而非仅仅最小化激励成本", "motivation": "现有研究主要关注双目标优化问题：最小化总激励成本同时最大化合作频率，但在这些约束下社会福利的最优值尚未得到充分探索。作者假设在驱动代理人达到期望合作状态所需的最小激励成本下，并不能保证实现最大社会福利。", "method": "采用单目标方法专注于最大化社会福利，基于基础演化博弈论模型，在有限群体的混合和结构化群体设置中分析成本效率。使用分析模型和基于代理的模拟来研究不同干预策略（包括奖励局部与全局行为模式）如何影响社会福利和合作动态。", "result": "结果显示，在优化纯成本效率或合作频率与优化最大社会福利之间，存在显著的个体激励成本差距。不同干预策略（局部与全局奖励）对社会福利和合作动态产生不同影响。", "conclusion": "多智能体系统和人类社会中的激励设计、政策和基准测试应优先考虑以福利为中心的目标，而不是成本或合作频率等代理目标。社会福利优化需要超越简单的成本最小化或合作频率最大化。"}}
{"id": "2512.07450", "pdf": "https://arxiv.org/pdf/2512.07450", "abs": "https://arxiv.org/abs/2512.07450", "authors": ["Imran Ahsan", "Hyunwook Yu", "Jinsung Kim", "Mucheol Kim"], "title": "Forget and Explain: Transparent Verification of GNN Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining). Code is available at https://github.com/ImranAhsan23/F-E", "summary": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.", "AI": {"tldr": "提出一种基于可解释性的GNN遗忘验证框架，通过对比遗忘前后的模型解释差异来透明验证信息是否被真正遗忘", "motivation": "GNN模型在隐私法规（如GDPR）要求下需要能够\"遗忘\"特定信息，但现有遗忘方法缺乏透明度，难以验证遗忘是否真正发生", "method": "提出可解释性驱动的验证器，在删除前后对模型进行快照，使用归因偏移和局部结构变化（如图编辑距离）作为透明证据，采用五种可解释性指标", "result": "评估两个骨干网络（GCN、GAT）和四种遗忘策略，结果显示Retrain和GNNDelete实现近乎完全遗忘，GraphEditor提供部分擦除，IDEA留下残留信号", "conclusion": "解释差异提供了主要的人类可读遗忘证据，同时成员推断ROC-AUC作为补充的图范围隐私信号，为GNN遗忘提供了透明验证框架"}}
{"id": "2512.07437", "pdf": "https://arxiv.org/pdf/2512.07437", "abs": "https://arxiv.org/abs/2512.07437", "authors": ["Chenwei Shi", "Xueyu Luan"], "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "comment": "23 pages, 8 figures, 3 tables", "summary": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.", "AI": {"tldr": "该论文提出KAN-Dreamer，将Kolmogorov-Arnold Networks（KANs）作为函数逼近器集成到DreamerV3世界模型中，评估其在模型强化学习中的性能表现。", "motivation": "DreamerV3是先进的在线模型强化学习算法，但使用传统的MLP架构。KANs作为MLP的替代方案，具有更好的参数效率和可解释性，但存在计算开销问题。研究旨在探索KAN架构在DreamerV3框架中的集成潜力。", "method": "提出KAN-Dreamer，将DreamerV3中的特定MLP和卷积组件替换为KAN和FastKAN层。在JAX-based世界模型中实现完全向量化版本，简化网格管理。研究分为三个子系统：视觉感知、潜在预测和行为学习。在DeepMind Control Suite（walker_walk）上进行实证评估。", "result": "实验结果表明，使用改进的FastKAN作为奖励和继续预测器的直接替代品，其性能与原始MLP架构相当，在样本效率和训练速度方面保持同等水平。", "conclusion": "该研究证实了KAN架构在模型强化学习框架中的可行性，为未来基于KAN的世界模型开发提供了初步研究基础。FastKAN作为MLP的替代方案在保持性能的同时，具有参数效率和可解释性的潜在优势。"}}
{"id": "2512.07436", "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "提出LocalSearchBench基准测试，用于评估本地生活服务领域的智能搜索代理性能，包含多样化的真实业务场景和多跳推理任务", "motivation": "现有的大语言模型研究主要关注通用信息检索，很少探索具有独特挑战的垂直领域。本地生活服务领域的查询往往模糊且需要跨商家和产品的多跳推理，现有方法难以充分解决这些问题", "method": "构建包含超过15万条高质量条目的基准数据集，涵盖不同城市和业务类型；基于真实用户查询创建300个多跳QA任务；开发LocalPlayground统一环境，集成多种工具供智能体交互", "result": "实验显示即使是当前最先进的大语言模型在LocalSearchBench上也表现不佳：最佳模型（DeepSeek-V3.1）正确率仅为34.34%，大多数模型在完整性（平均77.33%）和忠实度（平均61.99%）方面存在问题", "conclusion": "本地生活服务领域需要专门的基准测试和领域特定的智能体训练，当前模型在该领域仍面临重大挑战，突显了进一步研究和改进的必要性"}}
{"id": "2512.07430", "pdf": "https://arxiv.org/pdf/2512.07430", "abs": "https://arxiv.org/abs/2512.07430", "authors": ["Yangle Li", "Danli Luo", "Haifeng Hu"], "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.", "AI": {"tldr": "提出MIDG框架，通过混合不变专家模型和跨模态适配器，解决多模态情感分析中领域泛化的问题", "motivation": "现有方法在提取不变特征时忽略了模态间的协同作用，且知识注入技术存在跨模态知识碎片化问题，无法充分利用多模态数据的丰富语义信息", "method": "1. 混合不变专家模型提取领域不变特征，增强模态间协同关系学习；2. 跨模态适配器通过跨模态知识注入增强多模态表示的语义丰富性", "result": "在三个数据集上的广泛领域实验表明，MIDG取得了优越的性能表现", "conclusion": "提出的MIDG框架通过结合混合不变专家模型和跨模态适配器，有效解决了多模态情感分析中的领域泛化问题，提升了模型性能"}}
{"id": "2512.07426", "pdf": "https://arxiv.org/pdf/2512.07426", "abs": "https://arxiv.org/abs/2512.07426", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, accepted for oral presentation at SPIE Medical Imaging, 2026", "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "AI": {"tldr": "该论文研究了全切片图像（WSI）归一化过程中深度学习方法可能产生的幻觉风险，这些幻觉会引入原始组织不存在的伪影，对下游病理分析构成严重威胁。", "motivation": "当前基于深度学习的WSI归一化方法通过学习训练数据分布来近似输出，但这种方法可能导致输出趋向平均值，掩盖诊断重要特征，更重要的是可能产生视觉上难以检测的幻觉内容。现有评估方法往往忽视这些风险，特别是在真实临床数据中。", "method": "作者提出了一种新颖的图像比较度量方法，专门用于自动检测归一化输出中的幻觉。使用该度量方法，系统评估了在真实世界数据上重新训练的几种知名归一化方法，揭示了传统指标未能捕捉的显著不一致性和失败情况。", "result": "研究发现，尽管许多方法在公共数据集上表现良好，但在真实临床数据上重新训练和评估时，幻觉出现的频率令人担忧。提出的检测方法能够有效揭示传统指标无法捕捉的归一化失败情况。", "conclusion": "研究结果强调了在临床部署中需要更鲁棒、可解释的归一化技术以及更严格的验证协议。幻觉风险是真实存在且被低估的，需要学术界和工业界共同关注。"}}
{"id": "2512.07419", "pdf": "https://arxiv.org/pdf/2512.07419", "abs": "https://arxiv.org/abs/2512.07419", "authors": ["Haidong Kang", "Jun Du", "Lihong Lin"], "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.", "AI": {"tldr": "提出一种基于大语言模型的训练免费自动代理发现框架TAP，用于混合精度量化，无需人工专家参与和训练过程", "motivation": "传统混合精度量化方法要么依赖昂贵的可微分优化搜索（效率低且不灵活），要么依赖人工专家设计的代理（劳动密集且需要专业知识），需要寻找无需人工专家参与和训练的新方法", "method": "提出LLM驱动的训练免费自动代理发现框架TAP，利用大语言模型为MPQ寻找定制化代理；采用基于强化学习的直接策略优化方法优化提示，构建LLM与MPQ任务之间的正反馈循环", "result": "在主流基准测试上的广泛实验表明，TAP实现了最先进的性能，显著提升了混合精度量化的效果", "conclusion": "TAP框架为MPQ社区提供了LLM驱动设计算法的新视角，能够自动发现优秀的代理而无需人工专家参与和训练过程"}}
{"id": "2512.07415", "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "提出一种数据驱动的方法来探索个体间的移动交互模式，通过数据挖掘发现个体间相互影响的证据和复杂模式", "motivation": "现有解决方案通常基于预设的行为模型，而本文希望直接从数据出发，通过数据挖掘发现个体间的交互模式，为人群模拟和应急管理等应用提供更准确的模型基础", "method": "从数据中搜索可能反映个体间相互影响的移动事件，在此基础上寻找复杂、持久的模式和时间演化的事件配置，采用数据挖掘视角而非预设模型", "result": "在两个真实案例研究（汽车和行人）上实例化了该方法，进行了全面的实验评估，包括性能、参数敏感性和结果解释", "conclusion": "该方法能够提供关于个体间移动交互机制的新见解，有助于改进现有的模拟模型，为人群动态建模提供数据驱动的解决方案"}}
{"id": "2512.07410", "pdf": "https://arxiv.org/pdf/2512.07410", "abs": "https://arxiv.org/abs/2512.07410", "authors": ["Bin Li", "Ruichi Zhang", "Han Liang", "Jingyan Zhang", "Juze Zhang", "Xin Chen", "Lan Xu", "Jingyi Yu", "Jingya Wang"], "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs", "categories": ["cs.CV"], "comment": "Project page: https://binlee26.github.io/InterAgent-Page", "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.", "AI": {"tldr": "提出InterAgent框架，首个端到端的文本驱动物理多智能体人形控制方法，通过交互图上的扩散模型实现多智能体命令执行", "motivation": "现有方法主要局限于单智能体场景，忽略了多智能体交互中物理上合理的相互作用，需要能够模拟人类社交行为中复杂协调的框架", "method": "提出自回归扩散变换器，配备多流块解耦本体感觉、外感觉和动作；引入交互图外感觉表示捕捉细粒度关节间空间依赖；设计基于稀疏边的注意力机制动态修剪冗余连接并强调关键智能体间空间关系", "result": "InterAgent在多个强基线上表现优异，达到最先进性能，能够仅从文本提示生成连贯、物理合理且语义忠实的多智能体行为", "conclusion": "成功开发了首个端到端文本驱动物理多智能体人形控制框架，通过创新的交互图表示和注意力机制有效解决了多智能体协调问题"}}
{"id": "2512.07404", "pdf": "https://arxiv.org/pdf/2512.07404", "abs": "https://arxiv.org/abs/2512.07404", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "title": "Do LLMs Trust the Code They Write?", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "AI": {"tldr": "探索大语言模型是否在其内部表示中编码了代码正确性概念，并利用这种内部表示来提升代码生成质量", "motivation": "尽管大语言模型在代码生成方面很有效，但经常输出错误代码。模型输出概率与正确性相关性不高，只反映生成过程的最终输出。受研究发现LLMs内部编码了类似\"真实性\"概念的启发，本文探索LLMs是否类似地表示代码正确性", "method": "通过对比同一编程任务下正确和错误代码对的隐藏状态，识别LLMs内部的正确性表示。在四个LLMs上进行实验，利用提取的正确性表示来改进代码选择", "result": "利用提取的正确性表示优于标准的对数似然排名和模型口头表达的信心。这种内部正确性信号可用于选择更高质量的代码样本，无需测试执行", "conclusion": "这项工作展示了如何利用内部表示来增强代码生成系统，使LLMs更可靠，从而提高对自动生成代码的信心"}}
{"id": "2512.07400", "pdf": "https://arxiv.org/pdf/2512.07400", "abs": "https://arxiv.org/abs/2512.07400", "authors": ["Giulia Lanzillotta", "Damiano Meier", "Thomas Hofmann"], "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "AI": {"tldr": "分析持续学习中浅层遗忘与深层遗忘的差异，揭示了经验回放中缓冲区大小对两种遗忘的不同影响机制", "motivation": "解决持续学习中的一个悖论：神经网络即使输出预测失败，仍能保留过去任务的线性可分表示。需要形式化区分浅层分类器级遗忘和深层特征空间遗忘", "method": "将神经崩溃框架扩展到顺序设置，分析经验回放中缓冲区大小的影响。通过几何漂移和统计伪影来解释两种遗忘的不同机制", "result": "发现最小缓冲区足以锚定特征几何并防止深层遗忘，但缓解浅层遗忘通常需要更大的缓冲区容量。任何非零回放分数都能保证线性可分性的保留", "conclusion": "挑战了对大缓冲区的依赖，建议通过显式校正统计伪影，可以用最小回放实现鲁棒性能。将持续学习与分布外检测统一起来"}}
{"id": "2512.07397", "pdf": "https://arxiv.org/pdf/2512.07397", "abs": "https://arxiv.org/abs/2512.07397", "authors": ["Ali Joundi", "Yann Traonmilin", "Jean-François Aujol"], "title": "From sparse recovery to plug-and-play priors, understanding trade-offs for stable recovery with generalized projected gradient descent", "categories": ["eess.IV", "cs.NE", "math.OC"], "comment": null, "summary": "We consider the problem of recovering an unknown low-dimensional vector from noisy, underdetermined observations. We focus on the Generalized Projected Gradient Descent (GPGD) framework, which unifies traditional sparse recovery methods and modern approaches using learned deep projective priors. We extend previous convergence results to robustness to model and projection errors. We use these theoretical results to explore ways to better control stability and robustness constants. To reduce recovery errors due to measurement noise, we consider generalized back-projection strategies to adapt GPGD to structured noise, such as sparse outliers. To improve the stability of GPGD, we propose a normalized idempotent regularization for the learning of deep projective priors. We provide numerical experiments in the context of sparse recovery and image inverse problems, highlighting the trade-offs between identifiability and stability that can be achieved with such methods.", "AI": {"tldr": "该论文研究了从噪声欠定观测中恢复低维向量的问题，提出了广义投影梯度下降(GPGD)框架，统一了传统稀疏恢复方法和使用学习深度投影先验的现代方法，并探讨了稳定性与可识别性之间的权衡。", "motivation": "传统稀疏恢复方法和现代基于学习的投影先验方法在稳定恢复方面存在局限性，需要统一的框架来理解这些方法在模型误差、投影误差和测量噪声下的鲁棒性，以及如何更好地控制稳定性和鲁棒性常数。", "method": "提出了广义投影梯度下降(GPGD)框架，扩展了收敛结果以处理模型和投影误差；提出了广义反投影策略来适应结构化噪声（如稀疏异常值）；提出了归一化幂等正则化方法来学习深度投影先验以提高稳定性。", "result": "理论分析表明GPGD框架能够统一传统和现代方法，并提供对稳定性和鲁棒性常数的更好控制；数值实验在稀疏恢复和图像逆问题中验证了该方法在可识别性和稳定性之间的权衡效果。", "conclusion": "GPGD框架为理解传统稀疏恢复和现代基于学习的投影先验方法提供了统一的理论基础，提出的正则化策略能够改善稳定性，揭示了在稳定恢复中可识别性与稳定性之间的重要权衡关系。"}}
{"id": "2512.07394", "pdf": "https://arxiv.org/pdf/2512.07394", "abs": "https://arxiv.org/abs/2512.07394", "authors": ["Zhifan Zhu", "Siddhant Bansal", "Shashank Tripathi", "Dima Damen"], "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "categories": ["cs.CV"], "comment": "webpage: https://zhifanzhu.github.io/objects-along-hit", "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "AI": {"tldr": "提出ROHIT任务，从第一人称视角重建手交互时间线上的物体，通过定义手交互时间线(HIT)并利用约束优化和传播(COP)框架来提升物体重建质量。", "motivation": "在无3D标注的情况下，从第一人称视频中重建物体姿态具有挑战性。现有方法难以处理手与物体交互过程中的姿态变化，特别是在稳定抓握阶段。", "method": "定义手交互时间线(HIT)，包括静态、接触、稳定抓握、释放等阶段。提出约束优化和传播(COP)框架，利用HIT中的姿态约束进行优化和传播。", "result": "在两个第一人称数据集(HOT3D和EPIC-Kitchens)上评估，COP框架将稳定抓握重建提升6.2-11.3%，HIT重建提升达24.5%。", "conclusion": "ROHIT任务和COP框架有效解决了无3D标注下的物体重建问题，通过利用手交互时间线的姿态约束显著提升了重建质量。"}}
{"id": "2512.07391", "pdf": "https://arxiv.org/pdf/2512.07391", "abs": "https://arxiv.org/abs/2512.07391", "authors": ["Đorđe Nedeljković"], "title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.", "AI": {"tldr": "提出GlimmerNet，一种用于无人机应急监测的超轻量级卷积网络，通过分组扩张深度卷积实现多尺度特征提取，在极低参数量下保持全局感知能力", "motivation": "虽然Vision Transformers能增强全局上下文理解，但计算开销大。需要在资源受限的无人机平台上实现实时应急监测，需要既保持全局感知又计算高效的网络", "method": "提出分组扩张深度卷积块(GDBlocks)，将通道分组并使用不同扩张率，实现无额外参数的多尺度特征提取；设计聚合器模块，使用分组逐点卷积高效融合跨组特征", "result": "仅31K参数，比最新基线减少29% FLOPs，在AIDERv2数据集上达到0.966的加权F1分数，创下新的准确率-效率权衡前沿", "conclusion": "GlimmerNet证明了无需依赖计算昂贵的组件也能保持强大的全局感知，为资源受限无人机平台的实时应急监测建立了新的准确率-效率权衡前沿"}}
{"id": "2512.07390", "pdf": "https://arxiv.org/pdf/2512.07390", "abs": "https://arxiv.org/abs/2512.07390", "authors": ["Gilhyun Nam", "Taewon Kim", "Joonhyun Jeong", "Eunho Yang"], "title": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to WACV 2026", "summary": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.", "AI": {"tldr": "本文提出了一种名为SICL（Style Invariance as a Correctness Likelihood）的框架，用于提升测试时自适应（TTA）方法的可靠性，通过风格不变性来估计预测正确性的似然度，从而改善模型在动态测试环境下的不确定性校准。", "motivation": "测试时自适应（TTA）方法在实际部署中经常导致预测不确定性校准不佳，这在自动驾驶、金融和医疗等高风险领域尤为关键。现有的校准方法通常假设固定模型或静态分布，在真实世界动态测试条件下性能会下降。", "method": "SICL框架利用风格不变性进行鲁棒的不确定性估计。它通过测量预测在不同风格变换变体之间的一致性来估计实例级别的正确性似然度，仅需要模型的前向传播，无需反向传播，可作为即插即用的校准模块与任何TTA方法兼容。", "result": "在四个基线、五种TTA方法和两种现实场景下的综合评估显示，SICL相比传统校准方法平均减少了13个百分点的校准误差，并在三种模型架构上都表现出色。", "conclusion": "SICL提供了一种有效的方法来改善TTA方法在动态环境下的不确定性校准，通过风格不变性原理实现了无需反向传播的即插即用校准，显著提升了模型在实际部署中的可靠性。"}}
{"id": "2512.07388", "pdf": "https://arxiv.org/pdf/2512.07388", "abs": "https://arxiv.org/abs/2512.07388", "authors": ["Remi Poivet", "Catherine Pelachaud", "Malika Auvray"], "title": "Breaking Players' expectations: the Role of Non-player Characters' coherence and Consistency", "categories": ["cs.HC"], "comment": null, "summary": "In video games, non-player characters (NPCs) play a pivotal role in shaping players' experiences. The design of these characters, encompassing their appearance and behaviors, can be manipulated in terms of coherence and consistency to maintain players' expectations or, on the contrary, to surprise them. The extent to which NPCs' coherence and consistency influence players' evaluation of them remains to be unveiled. To address this knowledge gap, two experiments were conducted in the context of a military shooter game. Players' evaluations of NPCs' perceived intelligence and believability were measured, as these two dimensions are fundamental to players' adoption of NPCs and subsequent commitment to them. The first experiment investigated the impact of disrupting players' initial expectations on their evaluations of NPCs. The second experiment focused on the influence of NPCs' coherence and consistency on both players' expectations and evaluation of NPCs, using a combination of questionnaires and behavioral and physiological measures. The results of our study show that disrupting players' initial expectations influences their assessment of NPCs, with coherent and consistent design reinforcing expectations and incoherent design challenging them.", "AI": {"tldr": "研究非玩家角色（NPC）的一致性和连贯性如何影响玩家对NPC的评估，通过两个实验探索NPC设计如何维持或打破玩家期望", "motivation": "视频游戏中NPC对玩家体验至关重要，但NPC的一致性和连贯性如何影响玩家评估尚未明确。需要了解NPC设计如何维持或打破玩家期望，以及这对玩家对NPC智能度和可信度评估的影响", "method": "在军事射击游戏背景下进行两个实验：实验一研究打破玩家初始期望对NPC评估的影响；实验二使用问卷、行为和生理测量相结合的方法，研究NPC一致性和连贯性对玩家期望和NPC评估的影响", "result": "打破玩家初始期望会影响他们对NPC的评估，一致且连贯的设计会强化期望，而不一致的设计则会挑战期望。NPC的一致性和连贯性显著影响玩家对NPC智能度和可信度的评估", "conclusion": "NPC的一致性和连贯性设计是影响玩家期望和评估的关键因素，游戏设计师可以通过操控这些维度来维持或打破玩家期望，从而影响玩家对NPC的接受度和投入度"}}
{"id": "2512.07385", "pdf": "https://arxiv.org/pdf/2512.07385", "abs": "https://arxiv.org/abs/2512.07385", "authors": ["Chunhui Zhang", "Li Liu", "Zhipeng Zhang", "Yong Wang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline", "categories": ["cs.CV"], "comment": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.", "AI": {"tldr": "该论文提出了一个新的多模态视觉跟踪任务UAV-Anti-UAV，构建了百万规模的数据集，并提出了基于Mamba的基线方法MambaSTS，用于解决无人机追捕对抗无人机这一具有挑战性的跟踪问题。", "motivation": "当前反无人机研究主要关注固定地面摄像头捕获的RGB、红外或RGB-IR视频，而忽略了从另一个移动无人机平台跟踪目标无人机的问题。无人机反无人机任务面临双重动态干扰的挑战，需要新的研究方法和数据集来推动该领域发展。", "method": "提出了MambaSTS方法，这是一种基于Mamba的无人机反无人机跟踪基线方法。该方法结合Mamba和Transformer模型分别学习全局语义和空间特征，利用状态空间模型在长序列建模中的优势，通过时间令牌传播机制建立视频级长期上下文。", "result": "构建了包含1,810个视频的百万规模数据集，每个视频都手动标注了边界框、语言提示和15个跟踪属性。对50种现代深度跟踪算法的实验评估表明，在UAV-Anti-UAV领域仍有很大的改进空间，验证了所提方法的有效性。", "conclusion": "该研究填补了无人机反无人机跟踪任务的空白，提出了具有挑战性的新任务、大规模数据集和有效的基线方法，为未来研究提供了重要基础，并展示了该领域仍有显著改进空间。"}}
{"id": "2512.07383", "pdf": "https://arxiv.org/pdf/2512.07383", "abs": "https://arxiv.org/abs/2512.07383", "authors": ["Deepika SN Vemuri", "Gautham Bellamkonda", "Aditya Pola", "Vineeth N Balasubramanian"], "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning", "categories": ["cs.CV"], "comment": "18 pages, 19 figures, WACV 2026", "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.", "AI": {"tldr": "提出LogicCBMs，通过命题逻辑增强概念瓶颈模型，超越简单的线性概念组合，使用可微逻辑操作连接学习到的概念，提高模型表达能力和可解释性。", "motivation": "传统概念瓶颈模型（CBMs）通过线性组合语义概念进行预测，但这种线性组合存在固有局限性，无法捕捉概念间的复杂关系，限制了模型的表达能力和准确性。", "method": "引入逻辑模块，精心设计可微逻辑操作连接CBMs学习到的概念，使LogicCBM能够超越简单的加权概念组合，利用各种逻辑操作生成最终预测，同时保持端到端可学习性。", "result": "在知名基准测试和合成数据集上的实证研究表明，LogicCBMs具有更好的准确性，能够执行有效的干预，并且具有高度可解释性。", "conclusion": "通过命题逻辑增强概念学习模型，LogicCBMs能够捕捉概念间关系，提高逻辑操作的表达能力，在保持可解释性的同时提升模型性能。"}}
{"id": "2512.07381", "pdf": "https://arxiv.org/pdf/2512.07381", "abs": "https://arxiv.org/abs/2512.07381", "authors": ["Shuohan Tao", "Boyao Zhou", "Hanzhang Tu", "Yuwang Wang", "Yebin Liu"], "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "AI": {"tldr": "提出Tessellation GS方法，一种基于网格面的结构化2D高斯方法，用于从单目视频重建动态物体", "motivation": "传统3D高斯泼溅方法在视角外推时存在各向异性问题，容易过拟合且泛化能力差，特别是在稀疏视角和动态场景重建中表现不佳", "method": "将2D高斯约束在网格面的局部区域，通过层次化神经特征推断高斯属性；采用自适应面细分策略引导高斯细分；利用重建基础模型的先验初始化高斯变形", "result": "在表观和网格重建任务上，LPIPS降低29.1%，Chamfer距离降低49.2%，优于之前的SOTA方法", "conclusion": "Tessellation GS能够从单个静态相机稳健地重建一般动态物体，解决了传统优化方法难以处理的挑战"}}
{"id": "2512.07379", "pdf": "https://arxiv.org/pdf/2512.07379", "abs": "https://arxiv.org/abs/2512.07379", "authors": ["Mahila Moghadami", "Mohammad Ali Keyvanrad", "Melika Sabaghian"], "title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency", "categories": ["cs.CV"], "comment": "22 pages, 16 figures", "summary": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.", "AI": {"tldr": "提出了一种基于YOLO的增强小目标检测框架，通过改进滑动窗口裁剪策略和网络架构修改，显著提升了在航拍图像中的小目标检测精度", "motivation": "随着航拍图像在关键工业和军事应用中的重要性日益增长，现有小目标检测方法在精度和效率方面存在不足，需要开发更鲁棒的检测框架来处理大规模航拍图像中的小目标", "method": "基于SW-YOLO方法，改进滑动窗口的裁剪尺寸和重叠策略，然后在网络架构上进行修改：在骨干网络中集成CBAM注意力机制以保留空间和通道信息，在颈部添加高级特征提取模块增强特征图，并引入新的检测头来提升小目标检测精度", "result": "在VisDrone2019数据集上，将mAP .5:.5精度从YOLOv5L的35.5提升到61.2，显著超越了SAHI和CZDet（58.36）等现有方法，实现了显著的精度提升", "conclusion": "提出的框架通过结合改进的滑动窗口策略和网络架构增强，在航拍图像小目标检测任务中取得了显著的精度提升，证明了该方法在处理大规模图像中小目标检测问题的有效性"}}
{"id": "2512.07371", "pdf": "https://arxiv.org/pdf/2512.07371", "abs": "https://arxiv.org/abs/2512.07371", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": "project page: https://project-espada.github.io/espada/", "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "AI": {"tldr": "ESPADA是一个通过语义感知的演示数据下采样来加速模仿学习执行速度的框架，它能够在保持成功率的同时实现约2倍的加速。", "motivation": "基于行为克隆的视觉运动策略虽然能实现精确操作，但通常继承了人类演示的缓慢节奏，限制了实际部署。现有加速方法主要依赖统计或启发式线索，忽略了任务语义，在不同操作场景下可能失效。", "method": "ESPADA使用VLM-LLM流水线结合3D夹爪-物体关系对演示进行语义分割，仅在非关键阶段进行激进下采样，同时保留精度关键阶段。通过动态时间规整（DTW）在仅动态特征上传播分割标签，实现从单个标注片段到整个数据集的扩展。", "result": "在仿真和真实世界实验中，使用ACT和DP基线，ESPADA实现了约2倍的加速，同时保持了成功率，缩小了人类演示与高效机器人控制之间的差距。", "conclusion": "ESPADA提供了一种语义和空间感知的框架，能够在不需额外数据、架构修改或重新训练的情况下，通过智能下采样显著加速模仿学习策略的执行速度。"}}
{"id": "2512.07363", "pdf": "https://arxiv.org/pdf/2512.07363", "abs": "https://arxiv.org/abs/2512.07363", "authors": ["Michael Stern", "Maurizio Vergari", "Julia Schorlemmer", "Francesco Vona", "David Grieshammer", "Jan-Niklas Voigt-Antons"], "title": "The Impact of Spatial Misalignment and Time Delay on Collaborative Presence in Augmented Reality", "categories": ["cs.HC"], "comment": null, "summary": "Precise temporal and spatial alignment is critical in collaborative Augmented Reality (AR) where users rely on shared visual information to coordinate actions. System latency and object misalignment can disrupt communication, reduce task efficiency, and negatively impact the overall user experience. While previous research has primarily focused on individual AR interactions, the impact of these inconsistencies on collaboration remains underexplored. This article investigates how user experience and task load are affected by object misalignment and time delay in a shared AR space. To examine these factors, we conducted an experiment with 32 participants, organized into 16 pairs, who collaboratively completed a spatial placement task. Within each condition, both participants alternated roles, taking turns as the leader-providing verbal placement instructions-and the builder-executing the placement. Six conditions were tested, manipulating object alignment (perfectly aligned vs. randomly misaligned) and time delay (0s, 0.1s, 0.4s). The misalignment was applied randomly to each virtual object with a shift of +-20 cm on every axis to create a clear distinction in spatial perception. User experience and task load were assessed to evaluate how these factors influence collaboration and interaction in AR environments. Results showed that spatial misalignment significantly increased perceived workload (NASA-TLX) and lowered user ratings in Pragmatic quality and Attractiveness (UEQ), while time delay had a more limited effect. These findings highlight the critical role of spatial accuracy in maintaining collaboration quality in AR.", "AI": {"tldr": "研究空间错位和时间延迟对增强现实协作临场感的影响，通过实验分析这些因素如何影响用户体验和任务负荷", "motivation": "在协作增强现实中，精确的时空对齐对用户协调行动至关重要。系统延迟和物体错位会破坏沟通、降低任务效率并影响用户体验。现有研究主要关注个体AR交互，而这些不一致性对协作的影响尚未充分探索。", "method": "采用32名参与者（16对）进行空间放置任务的协作实验。参与者轮流担任领导者（提供口头放置指令）和建造者（执行放置）。测试6种条件：物体对齐（完美对齐vs.随机错位）和时间延迟（0s, 0.1s, 0.4s）。错位通过在每个轴上随机偏移±20厘米实现。使用NASA-TLX评估任务负荷，UEQ评估用户体验。", "result": "空间错位显著增加了感知工作量（NASA-TLX），并降低了实用质量（Pragmatic quality）和吸引力（Attractiveness）的用户评分。时间延迟的影响相对有限。这些发现突显了空间准确性在维持AR协作质量中的关键作用。", "conclusion": "空间错位对协作增强现实中的用户体验和任务负荷有显著负面影响，而时间延迟的影响较小。研究强调了在AR协作环境中确保空间准确性的重要性，这对AR系统设计和开发具有重要指导意义。"}}
{"id": "2512.07361", "pdf": "https://arxiv.org/pdf/2512.07361", "abs": "https://arxiv.org/abs/2512.07361", "authors": ["Giuseppe Leo", "Paolo Gibertini", "Irem Ilter", "Erika Covi", "Ole Richter", "Elisabetta Chicca"], "title": "An Asynchronous Mixed-Signal Resonate-and-Fire Neuron", "categories": ["eess.SP", "cs.NE"], "comment": null, "summary": "Analog computing at the edge is an emerging strategy to limit data storage and transmission requirements, as well as energy consumption, and its practical implementation is in its initial stages of development. Translating properties of biological neurons into hardware offers a pathway towards low-power, real-time edge processing. Specifically, resonator neurons offer selectivity to specific frequencies as a potential solution for temporal signal processing. Here, we show a fabricated Complementary Metal-Oxide-Semiconductor (CMOS) mixed-signal Resonate-and-Fire (R&F) neuron circuit implementation that emulates the behavior of these neural cells responsible for controlling oscillations within the central nervous system. We integrate the design with asynchronous handshake capabilities, perform comprehensive variability analyses, and characterize its frequency detection functionality. Our results demonstrate the feasibility of large-scale integration within neuromorphic systems, thereby advancing the exploitation of bio-inspired circuits for efficient edge temporal signal processing.", "AI": {"tldr": "提出一种异步混合信号谐振-发放神经元电路，用于边缘计算中的时域信号处理", "motivation": "边缘计算需要限制数据存储、传输和能耗，生物神经元的特性为实现低功耗实时边缘处理提供了途径，谐振神经元对特定频率的选择性为时域信号处理提供了潜在解决方案", "method": "设计并制造了CMOS混合信号谐振-发放神经元电路，模拟中枢神经系统中控制振荡的神经细胞行为，集成了异步握手功能，进行了全面的变异性分析，并表征了其频率检测功能", "result": "展示了该电路在神经形态系统中大规模集成的可行性，推进了生物启发电路在高效边缘时域信号处理中的应用", "conclusion": "该异步混合信号谐振-发放神经元电路为边缘计算中的低功耗时域信号处理提供了一种可行的硬件实现方案"}}
{"id": "2512.07360", "pdf": "https://arxiv.org/pdf/2512.07360", "abs": "https://arxiv.org/abs/2512.07360", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV2026", "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "AI": {"tldr": "提出一种基于区域邻接图的结构感知特征校正方法，用于无需训练的开集词汇语义分割，通过结合图像实例先验来改善CLIP特征的局部判别能力", "motivation": "CLIP模型在图像-文本对预训练中关注全局语义对齐，导致在细粒度视觉区域与文本关联时性能不佳，产生噪声和不一致的预测，特别是在局部区域", "method": "构建基于低层特征（如颜色和纹理）的区域邻接图来捕捉局部结构关系，利用该图通过增强局部判别能力来精炼CLIP特征", "result": "实验表明该方法能有效抑制分割噪声，提高区域级一致性，在多个开集词汇分割基准上取得强劲性能", "conclusion": "提出的结构感知特征校正方法通过结合实例特定先验，成功解决了CLIP在局部区域预测中的噪声和不一致问题，为无需训练的开集词汇语义分割提供了有效解决方案"}}
{"id": "2512.07359", "pdf": "https://arxiv.org/pdf/2512.07359", "abs": "https://arxiv.org/abs/2512.07359", "authors": ["Bin Zhao", "Yiwen Lu", "Haohua Zhu", "Xiao Li", "Sheng Yi"], "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "categories": ["cs.RO", "cs.GR"], "comment": "10 pages, 4 figures. Accepted at ICBSR'25 (International Conference on Biomechanical Systems and Robotics)", "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "AI": {"tldr": "提出了一种用于数字孪生应用的多刚体人手近似方法，通过将个性化MANO模型转换为URDF表示，实现真实外观与实时物理仿真的平衡", "motivation": "数字孪生应用中的人手仿真需要在解剖学保真度和计算效率之间取得平衡，现有模型难以同时满足真实外观和实时物理仿真的需求", "method": "从光学动作捕捉构建个性化MANO模型，转换为URDF表示；针对单自由度关节推导闭式解，针对双自由度关节提出BCH校正的迭代方法处理旋转非交换性", "result": "亚厘米级重建误差，在多样化操作任务中成功执行抓取动作，验证了强化学习策略控制多刚体手重放人类演示的能力", "conclusion": "提出的多刚体人手近似方法在数字孪生应用中实现了解剖学保真与计算效率的良好平衡，为实时物理仿真提供了可行解决方案"}}
{"id": "2512.07357", "pdf": "https://arxiv.org/pdf/2512.07357", "abs": "https://arxiv.org/abs/2512.07357", "authors": ["Navid Ashrafi", "Francesco Vona", "Sina Hinzmann", "Juliane Henning", "Maurizio Vergari", "Maximilian Warsinke", "Catarina Pinto Moreira", "Jan-Niklas Voigt-Antons"], "title": "Size Matters: The Impact of Avatar Size on User Experience in Healthcare Applications", "categories": ["cs.HC"], "comment": "7 pages, 3 figures", "summary": "The usage of virtual avatars in healthcare applications has become widely popular; however, certain critical aspects, such as social distancing and avatar size, remain insufficiently explored. This research investigates user experience and preferences when interacting with a healthcare application utilizing virtual avatars displayed in different sizes. For our study, we had 23 participants interacting with five different avatars (a human-size avatar followed by four smaller avatars in a randomized order) varying in size, projected on a wall in front of them. The avatars were fully integrated with an artificial intelligence chatbot to make them conversational. Users were asked to rate the usability of the system after interacting with each avatar and complete a survey regarding trust and an additional questionnaire on social presence. The results of this study show that avatar size significantly influences the perceived attractiveness and perspicuity, with the medium-sized avatars receiving the highest ratings. Social presence correlated strongly with stimulation and attractiveness, suggesting that an avatar's visual appeal and interactivity influenced user engagement more than its physical size. Additionally, we observed a tendency for gender-specific differences on some of the UEQ+ scales, with male participants tending to prefer human-sized representations, while female participants slightly favored smaller avatars. These findings highlight the importance of avatar design and representation in optimizing user experience and trust in virtual healthcare environments.", "AI": {"tldr": "研究虚拟化身尺寸对医疗应用用户体验的影响，发现中等尺寸化身在吸引力和清晰度方面获得最高评分", "motivation": "医疗应用中虚拟化身使用日益普及，但化身尺寸和社交距离等关键因素尚未得到充分探索，需要研究这些因素如何影响用户体验和信任", "method": "23名参与者与5种不同尺寸的化身互动（1个真人尺寸+4个较小尺寸，随机顺序），化身集成AI聊天机器人，参与者评估系统可用性、信任度和社交存在感", "result": "化身尺寸显著影响感知吸引力和清晰度，中等尺寸化身评分最高；社交存在感与刺激性和吸引力强相关；观察到性别差异：男性偏好真人尺寸，女性偏好较小化身", "conclusion": "化身尺寸设计对优化虚拟医疗环境中的用户体验和信任至关重要，中等尺寸化身表现最佳，视觉吸引力和交互性比物理尺寸更能影响用户参与度"}}
{"id": "2512.07355", "pdf": "https://arxiv.org/pdf/2512.07355", "abs": "https://arxiv.org/abs/2512.07355", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "title": "A Geometric Unification of Concept Learning with Concept Cones", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages", "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "AI": {"tldr": "该论文提出了一个几何框架，将监督式概念瓶颈模型（CBMs）和无监督稀疏自编码器（SAEs）统一起来，认为两者都学习激活空间中的线性方向，其非负组合形成概念锥。", "motivation": "两种可解释性传统（CBMs和SAEs）各自发展但很少交流：CBMs通过监督定义概念，SAEs通过稀疏编码发现涌现概念。需要建立统一的框架来连接这两种范式。", "method": "提出概念锥的几何统一框架，将CBMs和SAEs都视为学习激活空间中的线性方向集合。建立包含性框架，用CBMs提供人类定义的参考几何，评估SAEs学习的概念锥如何近似或包含CBM概念锥。", "result": "发现了稀疏度和扩展因子的\"最佳点\"，能最大化与CBM概念的几何和语义对齐。提出了量化指标，将归纳偏置（如SAE类型、稀疏度、扩展比）与合理概念的出现联系起来。", "conclusion": "通过共享的几何框架统一了监督和无监督概念发现，为衡量SAE进展和评估发现概念与人类合理概念的对齐程度提供了原则性指标。"}}
{"id": "2512.07352", "pdf": "https://arxiv.org/pdf/2512.07352", "abs": "https://arxiv.org/abs/2512.07352", "authors": ["Xueping Zhang", "Zhenshan Zhang", "Yechen Wang", "Linxi Li", "Liwei Jin", "Ming Li"], "title": "MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection", "categories": ["cs.SD"], "comment": null, "summary": "Existing speech anti-spoofing benchmarks rely on a narrow set of public models, creating a substantial gap from real-world scenarios in which commercial systems employ diverse, often proprietary APIs. To address this issue, we introduce MultiAPI Spoof, a multi-API audio anti-spoofing dataset comprising about 230 hours of synthetic speech generated by 30 distinct APIs, including commercial services, open-source models, and online platforms. Based on this dataset, we define the API tracing task, enabling fine-grained attribution of spoofed audio to its generation source. We further propose Nes2Net-LA, a local-attention enhanced variant of Nes2Net that improves local context modeling and fine-grained spoofing feature extraction. Experiments show that Nes2Net-LA achieves state-of-the-art performance and offers superior robustness, particularly under diverse and unseen spoofing conditions. Code \\footnote{https://github.com/XuepingZhang/MultiAPI-Spoof} and dataset \\footnote{https://xuepingzhang.github.io/MultiAPI-Spoof-Dataset/} have released.", "AI": {"tldr": "该论文提出了MultiAPI Spoof数据集和Nes2Net-LA网络，用于解决现有语音反欺骗基准测试中API多样性不足的问题，并引入了API追踪任务。", "motivation": "现有语音反欺骗基准测试主要依赖有限的公开模型，与现实中商业系统使用多样化、通常是专有API的情况存在显著差距，需要更贴近实际场景的数据集和方法。", "method": "1) 构建MultiAPI Spoof数据集，包含约230小时由30个不同API生成的合成语音；2) 提出API追踪任务，实现欺骗音频到其生成源的细粒度溯源；3) 提出Nes2Net-LA网络，通过局部注意力增强改进局部上下文建模和细粒度欺骗特征提取。", "result": "Nes2Net-LA在实验中取得了最先进的性能，并在多样化和未见过的欺骗条件下表现出优越的鲁棒性。", "conclusion": "MultiAPI Spoof数据集和Nes2Net-LA网络有效解决了现有语音反欺骗基准测试中API多样性不足的问题，提升了在实际场景中的检测能力和鲁棒性。"}}
{"id": "2512.07351", "pdf": "https://arxiv.org/pdf/2512.07351", "abs": "https://arxiv.org/abs/2512.07351", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": null, "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "AI": {"tldr": "提出DeepAgent框架，一种双流多智能体融合方法，用于鲁棒的多模态深度伪造检测", "motivation": "现有方法通常在单一模型中整合视听信息，容易受到模态不匹配、噪声和操纵的影响，需要更鲁棒的检测框架", "method": "采用双智能体协作框架：Agent-1使用AlexNet-based CNN检测视觉伪造痕迹；Agent-2结合声学特征、Whisper音频转录和EasyOCR帧序列检测视听不一致性；通过随机森林元分类器融合决策", "result": "在Celeb-DF和FakeAVCeleb数据集上Agent-1达到94.35%准确率；在FakeAVCeleb上Agent-2和元分类器分别达到93.69%和81.56%；在DeepFakeTIMIT跨数据集验证中元分类器达到97.49%准确率", "conclusion": "分层融合增强了鲁棒性，减轻了单模态弱点，多智能体方法能有效应对不同类型的深度伪造操纵"}}
{"id": "2512.07348", "pdf": "https://arxiv.org/pdf/2512.07348", "abs": "https://arxiv.org/abs/2512.07348", "authors": ["Xinyu Wei", "Kangrui Cen", "Hongyang Wei", "Zhen Guo", "Bairui Li", "Zeqing Wang", "Jinrui Zhang", "Lei Zhang"], "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition", "categories": ["cs.CV"], "comment": "Project Page: https://MICo-150K.github.io/", "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.", "AI": {"tldr": "提出了MICo-150K数据集，这是一个用于多图像合成（Multi-Image Composition）的大规模高质量数据集，包含150K个合成图像，支持7种代表性任务，并建立了相应的基准测试和评估指标。", "motivation": "在可控图像生成中，从多个参考输入合成连贯一致的图像（多图像合成）仍然是一个具有挑战性的问题，部分原因是缺乏高质量的训练数据。为了填补这一空白，作者系统研究了MICo任务并构建了大规模数据集。", "method": "1. 将MICo系统分类为7种代表性任务；2. 收集高质量源图像并构建多样化的MICo提示；3. 利用强大的专有模型合成大量平衡的复合图像；4. 通过人工循环过滤和精炼；5. 构建Decomposition-and-Recomposition子集（11K真实图像分解重组）；6. 建立MICo-Bench基准测试（每任务100个案例+300个挑战性De&Re案例）；7. 提出新的评估指标Weighted-Ref-VIEScore。", "result": "1. 创建了MICo-150K数据集，包含150K个高质量合成图像；2. 在MICo-Bench上微调多个模型，结果显示MICo-150K有效增强了模型的MICo能力；3. 基线模型Qwen-MICo在3图像合成任务上与Qwen-Image-2509相当，同时支持任意多图像输入；4. 数据集、基准测试和基线模型为多图像合成研究提供了宝贵资源。", "conclusion": "MICo-150K数据集成功填补了多图像合成领域高质量训练数据的空白，通过系统化的任务分类、大规模数据收集和人工精炼，为相关研究提供了全面的资源。提出的评估指标和基准测试为模型性能评估提供了标准化方法，基线模型展示了数据集的有效性，推动了多图像合成技术的发展。"}}
{"id": "2512.07345", "pdf": "https://arxiv.org/pdf/2512.07345", "abs": "https://arxiv.org/abs/2512.07345", "authors": ["Shilong Jin", "Haoran Duan", "Litao Hua", "Wentao Huang", "Yuan Zhou"], "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, 8 figures, 5 tables, 2 algorithms, Accepted by AAAI 2026", "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "AI": {"tldr": "提出TD-Attn框架，通过3D注意力机制解决文本到图像扩散模型中的先验视角偏差问题，提升3D任务的多视角一致性", "motivation": "文本到图像(T2I)扩散模型在3D任务中存在先验视角偏差，导致不同视角下物体外观不一致，这限制了其在3D生成和编辑中的应用", "method": "提出TD-Attn框架，包含两个关键组件：1) 3D-AAG模块构建视角一致的3D注意力高斯分布，增强空间一致性；2) HAM模块使用语义引导树定位和调制对视角条件敏感的交叉注意力层", "result": "实验证明TD-Attn作为通用插件能显著提升3D任务的多视角一致性，并支持可控的精确3D编辑", "conclusion": "通过数学分析揭示了T2I模型中先验视角偏差的根本原因，提出的TD-Attn框架有效解决了多视角不一致问题，为3D任务提供了可靠的解决方案"}}
{"id": "2512.07344", "pdf": "https://arxiv.org/pdf/2512.07344", "abs": "https://arxiv.org/abs/2512.07344", "authors": ["Shengyuan Ye", "Bei Ouyang", "Tianyi Qian", "Liekang Zeng", "Mu Yuan", "Xiaowen Chu", "Weijie Hong", "Xu Chen"], "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by IEEE International Conference on Computer Communications 2026", "summary": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "AI": {"tldr": "本文提出了Venus系统，这是一个高效的边缘内存与检索系统，用于基于视觉语言模型（VLM）的在线视频理解，通过边缘-云分离架构显著降低系统开销并实现实时响应。", "motivation": "虽然视觉语言模型在多模态理解方面表现出色，但在线视频理解应用的实际部署中存在系统开销过大的问题，现有研究主要关注提升VLM的推理能力，而忽视了部署约束。", "method": "Venus采用边缘-云分离架构，在边缘侧进行内存构建和关键帧检索。系统分为两个阶段：1) 摄入阶段：通过场景分割和聚类处理流式边缘视频，使用多模态嵌入模型为选定的关键帧构建分层内存；2) 查询阶段：索引来自内存的查询，采用基于阈值的渐进采样算法进行关键帧选择，平衡系统成本和推理准确性。", "result": "评估显示，Venus相比最先进方法实现了15倍到131倍的总响应延迟加速，能够在几秒内实现实时响应，同时保持相当甚至更优的推理准确性。", "conclusion": "Venus系统通过创新的边缘-云分离架构和高效的内存检索机制，成功解决了VLM在线视频理解应用中的部署约束问题，实现了高效、实时的视频理解系统。"}}
{"id": "2512.07338", "pdf": "https://arxiv.org/pdf/2512.07338", "abs": "https://arxiv.org/abs/2512.07338", "authors": ["Luís Marnoto", "Alexandre Bernardino", "Bruno Martins"], "title": "Generalized Referring Expression Segmentation on Aerial Photos", "categories": ["cs.CV"], "comment": "Submitted to IEEE J-STARS", "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .", "AI": {"tldr": "该论文提出了Aerial-D，一个用于航空影像的大规模指代表达分割数据集，并开发了相应的模型和方法来处理航空影像中的指代表达分割任务。", "motivation": "航空影像（如无人机拍摄的现代航空照片、历史航空档案、高分辨率卫星图像等）在指代表达分割任务中面临独特挑战：空间分辨率差异大、色彩使用不一致、目标可能只有几个像素、场景中物体密度高且存在部分遮挡。现有数据集无法充分应对这些挑战。", "method": "1. 构建Aerial-D数据集：包含37,288张图像，1,522,523个指代表达，覆盖259,709个标注目标，涵盖21个类别；2. 采用全自动流水线构建，结合基于规则的表达生成和大型语言模型增强；3. 使用过滤器模拟历史成像条件；4. 采用RSRefSeg架构，在Aerial-D和现有航空数据集上联合训练模型。", "result": "联合训练在当代基准测试中取得竞争性性能，同时在单色、棕褐色和颗粒状退化的历史航空照片中保持强准确性。数据集、训练模型和完整软件流水线已公开。", "conclusion": "该工作通过构建大规模航空影像指代表达分割数据集Aerial-D，并开发相应的训练方法，成功解决了航空影像中因分辨率差异、色彩不一致、目标小、密度高等独特挑战带来的指代表达分割问题，为现代和历史航空影像分析提供了有效工具。"}}
{"id": "2512.07332", "pdf": "https://arxiv.org/pdf/2512.07332", "abs": "https://arxiv.org/abs/2512.07332", "authors": ["Zhengquan Luo", "Guy Tadmor", "Or Amar", "David Zeevi", "Zhiqiang Xu"], "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.", "AI": {"tldr": "提出RicciKGE方法，通过扩展的Ricci流将知识图谱嵌入损失梯度与局部曲率耦合，使实体嵌入与底层流形几何共同动态演化，实现相互适应。", "motivation": "现有知识图谱嵌入方法将所有实体放置在单一均匀流形上（欧几里得、球面、双曲或其乘积/多曲率变体），但预定义的均匀流形无法适应真实图谱中局部区域急剧变化的曲率。这种几何先验与知识图谱局部曲率的不匹配会扭曲实体间距离，损害嵌入的表达能力。", "method": "提出RicciKGE方法，将KGE损失梯度与局部曲率通过扩展的Ricci流耦合，使实体嵌入与底层流形几何共同动态演化。理论上证明了当耦合系数有界且适当选择时，所有边曲率呈指数衰减（流形趋向欧几里得平坦），KGE距离严格收敛到全局最优。", "result": "在链接预测和节点分类基准测试中表现出实验改进，证明了RicciKGE在适应异构知识图谱结构方面的有效性。", "conclusion": "RicciKGE通过将嵌入优化与几何平坦化相互促进，能够自适应地适应知识图谱的局部曲率变化，解决了现有方法因使用预定义均匀流形而导致的表达限制问题。"}}
{"id": "2512.07331", "pdf": "https://arxiv.org/pdf/2512.07331", "abs": "https://arxiv.org/abs/2512.07331", "authors": ["Kanishk Awadhiya"], "title": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.", "AI": {"tldr": "该论文研究了视觉Transformer中出现的\"归纳瓶颈\"现象，即ViT模型在中间层自发压缩信息形成U形熵分布，并证明这是数据驱动的适应性现象而非架构缺陷。", "motivation": "视觉Transformer缺乏卷积神经网络固有的层次归纳偏置，理论上应保持高维表示，但实际观察发现ViT常出现U形熵分布。研究者想探究这种\"归纳瓶颈\"是架构固有特性还是数据驱动的适应性现象。", "method": "通过分析DINO训练的ViT在不同数据集（UC Merced、Tiny ImageNet、CIFAR-100）上的层间有效编码维度，研究语义抽象复杂度与瓶颈深度的相关性。", "result": "发现瓶颈深度与任务所需的语义抽象程度强相关：纹理丰富的数据集保持高秩表示，而面向对象的数据集驱动网络抑制中间层的高频信息，形成\"学习\"的瓶颈以分离语义特征。", "conclusion": "归纳瓶颈不是架构伪影，而是数据依赖的适应性现象，表明ViT能够根据任务复杂度自发调整表示稀疏性，学习有效的特征提取策略。"}}
{"id": "2512.07328", "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "提出ContextAnyone框架，通过上下文感知扩散模型实现从文本和单张参考图像生成角色一致视频", "motivation": "现有个性化方法主要关注面部身份，但无法保持发型、服装、体型等更广泛的上下文线索，这些对视觉连贯性至关重要", "method": "联合重建参考图像并生成新视频帧；通过Emphasize-Attention模块选择性增强参考感知特征；使用双引导损失结合扩散和参考重建目标；提出Gap-RoPE位置嵌入分离参考和视频token", "result": "在身份一致性和视觉质量方面优于现有参考到视频方法，能在不同动作和场景中生成连贯且保持上下文的角色视频", "conclusion": "ContextAnyone通过上下文感知扩散框架有效解决了角色一致视频生成的挑战，实现了高质量的角色保持"}}
{"id": "2512.07317", "pdf": "https://arxiv.org/pdf/2512.07317", "abs": "https://arxiv.org/abs/2512.07317", "authors": ["Alexander Wietfeld", "Wolfgang Kellerer"], "title": "DBMC-aNOMAly: Asynchronous NOMA with Pilot-Symbol Optimization Protocol for Diffusion-Based Molecular Communication Networks", "categories": ["cs.ET"], "comment": "16 pages, 17 figures; This work has been submitted to IEEE Transactions on Molecular, Biological, and Multi-Scale Communications", "summary": "Multiple access (MA) schemes can enable cooperation between multiple nodes in future diffusion-based molecular communication (DBMC) networks. Non-orthogonal MA for DBMC networks (DBMC-NOMA) is a promising option for efficient simultaneous MA using a single molecule type. Expanding significantly upon previous work on the topic, this paper addresses the question of parameter optimization and bit error probability (BEP) reduction in an asynchronous network using DBMC-NOMA. First, we analytically derive the associated BEP and use the result for a thorough comparison with other MA schemes like time-division and molecule-division MA. We show that the asynchronous nature of the system can be exploited for performance gain, and the upper-bound performance can be achieved in all circumstances by avoiding a few worst-case offset configurations. Subsequently, we propose DBMC-aNOMAly, a pilot-symbol-based optimization protocol for asynchronous DBMC-NOMA, and extensively evaluate it using Monte-Carlo simulations. DBMC-aNOMAly is shown to provide robust BEP reduction for different network sizes, noise levels, subjected to sampling jitter, as well as for changing conditions during runtime, particularly, compared to protocols in previous work. DBMC-aNOMAly consists of a set of simple operations such as comparisons and additions, deliberately designed to be implementable with chemical reaction networks, setting up future work on the realistic modeling of the protocol.", "AI": {"tldr": "提出DBMC-aNOMAly协议，一种用于扩散分子通信网络的异步非正交多址接入方案，通过导频符号优化协议降低误比特率", "motivation": "扩散分子通信网络中需要高效的多址接入方案，异步非正交多址接入（DBMC-NOMA）虽然前景广阔，但存在参数优化和误比特率降低的挑战，需要解决异步网络中的性能优化问题", "method": "首先分析推导误比特率，与时分多址和分子分多址进行比较；提出DBMC-aNOMAly协议，基于导频符号的优化协议，使用蒙特卡洛模拟进行广泛评估；协议设计为可通过化学反应网络实现的简单操作", "result": "DBMC-aNOMAly在不同网络规模、噪声水平、采样抖动和运行时条件变化下都能提供稳健的误比特率降低；通过避免最差偏移配置可实现上限性能；相比先前协议有显著改进", "conclusion": "DBMC-aNOMAly协议通过导频符号优化有效解决了异步DBMC-NOMA网络的误比特率问题，为未来基于化学反应网络的现实建模奠定了基础"}}
{"id": "2512.07316", "pdf": "https://arxiv.org/pdf/2512.07316", "abs": "https://arxiv.org/abs/2512.07316", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection", "categories": ["cs.RO", "eess.SY"], "comment": "7 pages, 4 figures, submitted to IFAC World Congress 2026", "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.", "AI": {"tldr": "提出一种基于模型预测控制（MPC）的自主水面艇协同对接方法，两艘USV共同协作在约定位置完成对接，而非传统的固定目标对接方式。", "motivation": "现有自主水面艇对接方法通常将一艘USV视为静止目标，另一艘负责接近完成对接，这种非协作方式效率较低。需要开发更高效的协同对接方法，特别是在存在水流等干扰的环境中。", "method": "采用集中式模型预测控制（MPC）方法，通过预测模型处理控制问题，生成可行轨迹并保证约束满足。利用模型基础特性，通过MPC预测模型预测干扰影响，实现干扰抑制，特别适用于水流等准静态干扰。", "result": "仿真结果表明，与现有方法相比，所提出的协同MPC方法能够实现更快、更高效的对接，特别是在存在干扰的环境中表现出更好的性能。", "conclusion": "提出的集中式MPC协同对接方法能够有效处理USV之间的协作对接问题，通过模型预测实现干扰抑制，相比传统方法具有更高的效率和鲁棒性。"}}
{"id": "2512.07314", "pdf": "https://arxiv.org/pdf/2512.07314", "abs": "https://arxiv.org/abs/2512.07314", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "AI": {"tldr": "提出M-STAR框架，通过多尺度时空自回归方法生成长期人类移动轨迹，解决现有方法在长期轨迹生成效率和时空多尺度建模方面的不足。", "motivation": "现有基于自回归和扩散模型的方法在生成单日轨迹方面表现良好，但在生成长期轨迹（如周轨迹）时效率低下，且缺乏显式的时空多尺度建模能力。", "method": "提出多尺度时空自回归框架，包含多尺度时空标记器（编码分层移动模式）和基于Transformer的解码器，通过粗到细的时空预测过程生成长期轨迹。", "result": "在两个真实世界数据集上的实验表明，M-STAR在保真度方面优于现有方法，并显著提高了生成速度。", "conclusion": "M-STAR通过多尺度时空自回归方法有效解决了长期人类移动轨迹生成问题，在保真度和效率方面均有显著提升。"}}
{"id": "2512.07313", "pdf": "https://arxiv.org/pdf/2512.07313", "abs": "https://arxiv.org/abs/2512.07313", "authors": ["Bosun Kang", "Hyejun Park", "Chenglin Fan"], "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach", "categories": ["cs.LG", "cs.DS"], "comment": "7 pages", "summary": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.", "AI": {"tldr": "本文提出了一种基于贝叶斯决策和机器学习预测的滑雪租赁问题离散分布框架，统一了传统最坏情况分析和学习增强方法。", "motivation": "传统滑雪租赁算法仅考虑最坏情况成本，而近期学习增强方法利用噪声预测但缺乏不确定性量化。本文旨在通过贝叶斯框架统一这两种视角，实现原则性的不确定性量化和专家先验的整合。", "method": "提出离散贝叶斯框架，维护时间范围的精确后验分布，支持多预测整合、非均匀先验和上下文信息，实现从最坏情况到完全知情设置的平滑插值。", "result": "算法实现了先验依赖的竞争性保证，在准确先验下达到接近最优结果，同时保持鲁棒的最坏情况保证。实验评估显示在多样化场景中具有优越的实证性能。", "conclusion": "贝叶斯推理在具有不完美预测的在线决策问题中具有实际优势，该框架为学习增强算法提供了统一的概率基础，支持不确定性量化和灵活的先验整合。"}}
{"id": "2512.07312", "pdf": "https://arxiv.org/pdf/2512.07312", "abs": "https://arxiv.org/abs/2512.07312", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "categories": ["cs.AR", "cs.AI", "cs.DC"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing. We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups. Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "AI": {"tldr": "提出DCO（动态缓存编排）系统，通过预测性管理为LLM加速器优化共享系统级缓存性能", "motivation": "随着大语言模型（LLMs）的快速采用，AI加速器设计日益复杂化，特别是深度层次化的暂存器内存（SPMs）及其异步管理增加了软件开发难度。研究探索相反的设计思路：采用共享系统级缓存和应用程序感知管理策略，保持编程简单性。", "method": "利用软件栈中可用的数据流信息指导缓存替换（包括死块预测），结合旁路决策和缓解缓存颠簸的机制。通过周期精确模拟器评估，并构建考虑实际重叠行为的分析模型来扩展测量结果。", "result": "相比传统缓存架构获得显著性能提升（最高1.80倍加速比）。旁路和颠簸缓解策略能处理有无核间数据共享的场景。RTL实现面积为0.064mm²（15nm工艺），可运行在2GHz时钟频率。", "conclusion": "共享缓存设计具有潜力辅助未来AI加速器系统开发，在保持编程简单性的同时提供高性能缓存管理。"}}
{"id": "2512.07309", "pdf": "https://arxiv.org/pdf/2512.07309", "abs": "https://arxiv.org/abs/2512.07309", "authors": ["Guosheng Wang", "Shen Wang", "Lei Yang"], "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals", "categories": ["cs.IT", "cs.AI"], "comment": null, "summary": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.", "AI": {"tldr": "提出Radiance-Field Reinforced Pretraining (RFRP)框架，通过自监督预训练提升无线信号室内定位模型的跨场景泛化能力", "motivation": "现有基于深度学习的无线定位模型依赖场景特定的标注数据，在跨场景泛化方面面临重大挑战，需要解决标注数据获取困难的问题", "method": "采用非对称自编码器架构，将大型定位模型与神经射频辐射场耦合，定位模型编码接收的RF频谱为位置相关表示，RF-NeRF解码重建原始频谱，实现大规模无标签RF数据的表示学习", "result": "在100个不同场景的7,327,321个位置收集RF样本，使用四种无线技术。实验表明，RFRP预训练的定位模型相比未预训练模型减少40%以上定位误差，相比监督学习预训练模型减少21%误差", "conclusion": "RFRP框架通过自监督预训练有效利用大规模无标签无线信号数据，显著提升室内定位模型的跨场景泛化能力和定位精度"}}
{"id": "2512.07306", "pdf": "https://arxiv.org/pdf/2512.07306", "abs": "https://arxiv.org/abs/2512.07306", "authors": ["Thierry Petit", "Arnault Pachot"], "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Submitted for peer review on December 7, 2025", "summary": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.", "AI": {"tldr": "提出一种基于约束规划的合成人口生成框架，能够精确复制目标统计数据并确保个体一致性", "motivation": "现有数据驱动方法依赖于样本推断分布，无法精确控制人口统计特征，且需要微观数据；需要一种无需个人数据就能精确生成合成人口的方法", "method": "采用约束编程框架，直接编码聚合统计数据和结构关系，确保个体一致性，无需任何微观数据", "result": "在官方人口统计数据上验证了方法的有效性，研究了分布偏差对下游分析的影响，能够通过大语言模型查询合成人口进行社会行为建模", "conclusion": "该方法为可扩展的社会和市场建模提供了精确的合成人口生成方案，无需个人数据即可获得可重复的决策级洞察"}}
{"id": "2512.07305", "pdf": "https://arxiv.org/pdf/2512.07305", "abs": "https://arxiv.org/abs/2512.07305", "authors": ["Tobias Abraham Haider"], "title": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset", "categories": ["cs.CV"], "comment": null, "summary": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.", "AI": {"tldr": "重新评估Google Inception-ResNet-v2模型在野生动物物种检测中的性能，通过可重复性研究验证原始研究的发现", "motivation": "评估Carl等人研究的可重复性和普适性，验证预训练模型在野生动物物种检测中的实际应用效果", "method": "使用开放资源和不同数据集（900张图像，90个物种）重新实现实验，进行最小预处理，使用Google Inception-ResNet-v2预训练模型", "result": "总体分类准确率为62%，与原始研究的71%相近；宏平均F1分数为0.28，显示不同类别性能差异显著", "conclusion": "预训练卷积神经网络可作为野生动物物种识别的实用基线，但需要物种特定的适应或迁移学习才能获得一致的高质量预测"}}
{"id": "2512.07303", "pdf": "https://arxiv.org/pdf/2512.07303", "abs": "https://arxiv.org/abs/2512.07303", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots", "categories": ["cs.RO"], "comment": "7 pages, 3 figures, submitted to IFAC World Congress 2026", "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.", "AI": {"tldr": "提出一种用于系留移动机器人配置空间的连续拓扑模型计算方法，该模型能同时捕捉系绳的拓扑信息和机器人的连续位置", "motivation": "现有系留机器人路径规划方法通常依赖离散的配置空间表示，缺乏能同时捕捉系绳拓扑信息和机器人连续位置的模型", "method": "首先建立系留机器人配置空间与工作空间通用覆盖空间之间的联系，然后利用这一联系开发算法计算配置空间的单纯复形模型", "result": "该方法比构建传统同伦增强图所需时间少得多，且是连续的，允许使用广泛的路径规划算法解决系留机器人路径规划任务", "conclusion": "提出的连续拓扑模型显著提升了系留机器人配置空间表示的性能，为路径规划提供了更高效和灵活的解决方案"}}
{"id": "2512.07302", "pdf": "https://arxiv.org/pdf/2512.07302", "abs": "https://arxiv.org/abs/2512.07302", "authors": ["Mingning Guo", "Mengwei Wu", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.", "AI": {"tldr": "该论文提出了AerialVP框架，通过增强任务提示来改进无人机图像感知中的视觉语言模型性能", "motivation": "现有基于视觉语言模型的图像感知方法在处理无人机图像时面临挑战，包括目标混淆、尺度变化和复杂背景。这些挑战源于视觉语言模型对图像内容的理解依赖于视觉和文本标记之间的语义对齐，当任务提示过于简单而图像内容复杂时，难以实现有效对齐", "method": "提出AerialVP框架，通过主动从无人机图像中提取多维辅助信息来增强任务提示。增强过程包括三个阶段：(1)分析任务提示以识别任务类型和增强需求，(2)从工具库中选择适当工具，(3)基于分析和选定工具生成增强的任务提示。同时引入了AerialSense基准测试集进行评估", "result": "实验结果表明，AerialVP显著增强了任务提示的指导能力，在开源和专有视觉语言模型中都带来了稳定且显著的性能提升", "conclusion": "AerialVP框架通过增强任务提示有效解决了无人机图像感知中的挑战，为视觉语言模型在复杂无人机图像分析中的应用提供了新方法"}}
{"id": "2512.07287", "pdf": "https://arxiv.org/pdf/2512.07287", "abs": "https://arxiv.org/abs/2512.07287", "authors": ["Sijia Li", "Yuchen Huang", "Zifan Liu", "Zijian Li", "Jingjing fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.", "AI": {"tldr": "提出SIT-Graph框架，通过构建状态集成工具图来增强多轮次工具使用能力，结合情景记忆和程序记忆的启发，提升智能体在多轮交互中的工具选择和经验迁移效果。", "motivation": "当前LLM智能体在多轮工具使用场景中面临挑战，因为意图逐步明确且环境随工具调用而演变。现有方法要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具间依赖关系，难以适应状态和信息在轮次间的动态变化。", "method": "提出状态集成工具图(SIT-Graph)：首先从累积的工具使用序列构建工具图，然后在每条边上增强一个紧凑的状态摘要，包含对话和工具历史中可能影响下一个动作的信息。推理时，智能体在需要回忆先前上下文时检索相关边上的状态摘要来指导决策，在常规步骤中则遵循高置信度的工具依赖关系。", "result": "在多个有状态的多轮工具使用基准测试中，SIT-Graph一致优于强大的基于记忆和图的基础方法，实现了更稳健的工具选择和更有效的经验迁移。", "conclusion": "SIT-Graph通过结合情景记忆片段和程序化例程，实现了人类决策式的平衡，有效解决了多轮工具使用中的状态演进和信息积累问题，为智能体系统提供了更强大的经验复用机制。"}}
{"id": "2512.07277", "pdf": "https://arxiv.org/pdf/2512.07277", "abs": "https://arxiv.org/abs/2512.07277", "authors": ["Srihari Bandarupalli", "Bhavana Akkiraju", "Charan Devarakonda", "Vamsiraghusimha Narsinga", "Anil Kumar Vuppala"], "title": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted in AACL IJCNLP 2025", "summary": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.", "AI": {"tldr": "本文研究低资源语言的自动语音识别，通过跨语言持续预训练方法，利用无标注语音数据解决标注数据稀缺问题，在波斯语、阿拉伯语和乌尔都语上取得了与更大模型相当的性能。", "motivation": "低资源语言的自动语音识别面临标注数据稀缺和计算资源不足的双重挑战，现有大型模型依赖大量参数和标注数据，不适用于资源有限的语言。", "method": "构建3000小时多语言无标注语料库，采用可扩展的数据收集流程，结合针对性的持续预训练和形态感知分词技术，开发了3亿参数的模型。", "result": "300M参数模型在波斯语上超越Whisper Large v3（15亿参数），在阿拉伯语和乌尔都语上取得竞争性结果，性能与5倍大的模型相当。", "conclusion": "ASR质量主要取决于数据相关性和策略性预训练而非模型大小，为低资源语言提供了不依赖大规模计算基础设施或专有数据集的实用解决方案。"}}
{"id": "2512.07276", "pdf": "https://arxiv.org/pdf/2512.07276", "abs": "https://arxiv.org/abs/2512.07276", "authors": ["Mai Tsujimoto", "Junjue Wang", "Weihao Xuan", "Naoto Yokoya"], "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026. Camera-ready-based version with minor edits for readability (no change in the contents)", "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.", "AI": {"tldr": "提出Geo3DVQA基准测试，用于评估视觉语言模型在仅使用RGB遥感图像进行3D地理空间推理的能力", "motivation": "当前3D地理空间分析依赖昂贵专业传感器（如LiDAR），限制了全球可访问性；现有方法难以整合多个3D线索、处理多样化查询并提供可解释推理", "method": "创建包含110k个问答对的综合基准，涵盖16个任务类别和三个复杂度级别：单特征推理、多特征推理和应用级空间分析", "result": "评估10个最先进的VLM显示RGB-to-3D推理困难：GPT-4o和Gemini-2.5-Flash准确率仅28.6%和33.0%；领域特定微调的Qwen2.5-VL-7B达到49.6%（+24.8点）", "conclusion": "Geo3DVQA揭示了当前VLM的局限性，展示了领域适应的有效性，为可扩展、可访问和全面的3D地理空间分析引入了新的挑战前沿"}}
{"id": "2512.07275", "pdf": "https://arxiv.org/pdf/2512.07275", "abs": "https://arxiv.org/abs/2512.07275", "authors": ["Siyu Wang", "Hua Wang", "Huiyu Li", "Fan Zhang"], "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "The paper has been accepted by BIBM 2025", "summary": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.", "AI": {"tldr": "提出一种用于皮肤病灶分割的注意力引导多尺度医学网络，通过多尺度残差结构、跨尺度特征融合和注意力机制解决病灶形状不规则和对比度低的问题。", "motivation": "现有深度学习方法在皮肤病灶分割中难以有效处理不规则病灶形状和低对比度问题，传统U-Net的跳跃连接存在信息损失，需要更有效的特征提取和融合机制。", "method": "基于编码器-解码器架构，引入多尺度残差结构提取不同感受野特征；提出多分辨率多通道融合模块捕获跨尺度特征；设计交叉混合注意力模块动态计算多上下文权重；引入外部注意力桥补偿上采样过程中的信息损失。", "result": "在多个皮肤病灶分割数据集上的实验表明，该模型显著优于现有的transformer和卷积神经网络模型，展现出卓越的分割精度和鲁棒性。", "conclusion": "提出的网络架构通过多尺度特征提取、跨尺度融合和注意力机制，有效解决了皮肤病灶分割中的形状不规则和低对比度挑战，为医学图像分割提供了新的解决方案。"}}
{"id": "2512.07273", "pdf": "https://arxiv.org/pdf/2512.07273", "abs": "https://arxiv.org/abs/2512.07273", "authors": ["Zhi Rao", "Yucheng Zhou", "Benjia Zhou", "Yiqing Huang", "Sergio Escalera", "Jun Wan"], "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation", "categories": ["cs.CV"], "comment": null, "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.", "AI": {"tldr": "提出一个三阶段强化视觉语言框架（RVLF），用于解决无注释手语翻译中的两个关键挑战：手语表示不足和句子级语义错位问题。", "motivation": "无注释手语翻译面临两个主要挑战：1）现有手语表示方法无法捕捉细微的视觉线索；2）当前基于LLM的方法存在句子级语义错位问题，限制了翻译质量。", "method": "提出三阶段RVLF框架：1）构建专门的手语大型视觉语言模型；2）引入语义表示学习机制，融合基于骨架的运动线索和DINOv2提取的视觉特征；3）采用GRPO优化策略，结合BLEU和ROUGE奖励函数进行强化学习微调。", "result": "在CSL-Daily、PHOENIX-2014T、How2Sign和OpenASL数据集上，BLEU-4分数分别提升了+5.1、+1.11、+1.4和+1.61，无需外部大规模手语数据集预训练。", "conclusion": "RVLF框架有效解决了无注释手语翻译的关键挑战，GRPO优化策略显著提升了翻译质量和语义一致性，是首个将GRPO应用于手语翻译的工作。"}}
{"id": "2512.07269", "pdf": "https://arxiv.org/pdf/2512.07269", "abs": "https://arxiv.org/abs/2512.07269", "authors": ["Mike Diessner", "Yannick Tarant"], "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "AI": {"tldr": "提出了一种基于启发式、图像和深度数据的图生成流水线，用于创建关键基础设施的虚拟表示", "motivation": "传统方法使用激光扫描仪获取3D点云成本高且需要专业知识，需要更经济高效的替代方案来创建关键基础设施的数字孪生模型", "method": "基于摄影测量学的图生成流水线，使用立体相机获取RGB图像和深度数据，通过深度学习进行对象检测和实例分割，利用用户定义的启发式规则推断对象间关系", "result": "在两个液压系统上的实验表明，该方法能够生成接近真实情况的图，同时具有灵活性和透明度", "conclusion": "该方法为关键基础设施的数字孪生提供了一种成本效益高、灵活且透明的解决方案，适用于高风险决策场景"}}
{"id": "2512.07266", "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Flögel", "Alexandru Vasilache", "Max Grobbel", "Jürgen Becker", "Sören Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "提出一种结合脉冲神经网络和人工神经网络的混合强化学习方法，用于机器人在人类环境中的社交导航", "motivation": "将自主移动机器人集成到人类环境中需要类人决策和节能的事件驱动计算，但神经形态方法在深度强化学习导航中应用较少，主要因为训练不稳定", "method": "采用混合社交集成DRL演员-评论家方法，演员使用脉冲神经网络，评论家使用人工神经网络，并配备神经形态特征提取器来捕捉时间人群动态和人机交互", "result": "提高了社交导航性能，并将估计能耗降低了约1.69个数量级", "conclusion": "该混合方法成功解决了神经形态方法在DRL导航中的训练稳定性问题，实现了更节能、更符合社交规范的机器人导航"}}
{"id": "2512.07265", "pdf": "https://arxiv.org/pdf/2512.07265", "abs": "https://arxiv.org/abs/2512.07265", "authors": ["Bhavana Akkiraju", "Srihari Bandarupalli", "Swathi Sambangi", "Vasavi Ravuri", "R Vijaya Saraswathi", "Anil Kumar Vuppala"], "title": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Submitted to AACL IJCNLP 2025", "summary": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.", "AI": {"tldr": "开发了一个高质量的泰卢固语-英语语音翻译基准数据集，包含46小时经过人工验证的语音数据，并系统比较了级联与端到端架构的性能差异。", "motivation": "尽管泰卢固语有超过8000万使用者，但针对这种形态丰富的语言的语音翻译研究严重不足，需要填补这一研究空白。", "method": "从CSTD语料库中提取46小时人工验证数据（30h/8h/8h的训练/开发/测试分割），系统比较级联架构（IndicWhisper + IndicMT）与端到端架构（SeamlessM4T），并评估多种自动评估指标的可靠性。", "result": "级联架构因使用大量泰卢固语特定训练数据而性能最高，但微调的端到端模型表现出显著竞争力，表明在低资源场景下，端到端系统通过适当调优和足够平行数据（可能少于100小时）可以达到与级联方法相当的性能。传统评估指标比BERTScore在泰卢固语-英语翻译中提供更好的质量区分。", "conclusion": "该工作提供了三个关键贡献：可复现的泰卢固语-英语基准数据集、低资源场景下端到端系统具有竞争性性能潜力的实证证据，以及针对形态复杂语言对自动评估的实用指导。"}}
{"id": "2512.07259", "pdf": "https://arxiv.org/pdf/2512.07259", "abs": "https://arxiv.org/abs/2512.07259", "authors": ["Tharindu Wickremasinghe", "Marco F. Duarte"], "title": "Affine Subspace Models and Clustering for Patch-Based Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": "Asilomar Conference on Signals, Systems, and Computers 2025", "summary": "Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.", "AI": {"tldr": "该论文研究基于仿射子空间模型的图像块聚类方法，用于改进基于块的图像去噪性能。", "motivation": "传统线性子空间模型不适合图像块建模，因为图像是非负的，在向量空间中不围绕原点分布。需要更好的几何模型来匹配图像块的实际分布。", "method": "使用仿射子空间模型替代线性子空间进行图像块聚类，并提出基于最小二乘投影的简单去噪算法，同时回顾了多种仿射子空间聚类算法。", "result": "实验结果显示，仿射子空间模型在聚类和去噪性能上都取得了改进，相比传统线性子空间模型有更好的表现。", "conclusion": "仿射子空间模型更适合图像块的几何结构，能有效提升基于块的图像去噪算法的性能，为图像处理应用提供了更好的建模方法。"}}
{"id": "2512.07253", "pdf": "https://arxiv.org/pdf/2512.07253", "abs": "https://arxiv.org/abs/2512.07253", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 8 figures, and 7 tables", "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "AI": {"tldr": "提出DGGAN（退化引导生成对抗网络），一种用于实时内窥镜视频增强的退化感知框架，通过跨帧传播退化表示来实现实时高质量增强。", "motivation": "内窥镜手术依赖术中视频，但视频常因光照不均、组织散射、遮挡和运动模糊而退化，影响手术安全和效果。现有深度学习方法计算量大，难以满足实时手术需求。", "method": "使用对比学习从图像中提取退化表示，引入融合机制将这些表示与图像特征调制，指导单帧增强模型。通过退化与恢复图像间的循环一致性约束进行训练，提高鲁棒性和泛化能力。", "result": "实验表明，该框架在性能与效率之间取得了优越的平衡，优于多种最先进方法，验证了退化感知建模对实时内窥镜视频增强的有效性。", "conclusion": "隐式学习和传播退化表示为临床应用提供了实用途径，退化感知框架能实现实时高质量的内窥镜视频增强。"}}
{"id": "2512.07251", "pdf": "https://arxiv.org/pdf/2512.07251", "abs": "https://arxiv.org/abs/2512.07251", "authors": ["Junqi Liu", "Zejun Wu", "Pedro R. A. S. Bassi", "Xinze Zhou", "Wenxuan Li", "Ibrahim E. Hamamci", "Sezgin Er", "Tianyu Lin", "Yi Luo", "Szymon Płotka", "Bjoern Menze", "Daguang Xu", "Kai Ding", "Kang Wang", "Yang Yang", "Yucheng Tang", "Alan L. Yuille", "Zongwei Zhou"], "title": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.", "AI": {"tldr": "提出SMILE模型，一种解剖感知的扩散模型，用于医学图像对比度增强，能够增强临床相关区域同时保持其他区域不变。", "motivation": "当前医学图像增强模型往往过度编辑，可能扭曲器官结构、产生虚假发现或遗漏小肿瘤，因为它们不理解解剖结构或对比度动态。", "method": "SMILE模型包含三个关键创新：1) 结构感知监督，遵循真实器官边界和对比度模式；2) 无需配准的学习，直接处理未对齐的多期相CT扫描；3) 统一推理，提供快速且一致的跨期相增强。", "result": "在六个外部数据集上，SMILE在图像质量方面优于现有方法（SSIM提高14.2%，PSNR提高20.6%，FID改善50%），并产生解剖学准确且具有诊断意义的图像，还能将非对比CT的癌症检测F1分数提高达10%。", "conclusion": "SMILE通过解剖感知的扩散模型实现了更精确的医学图像增强，在保持解剖结构完整性的同时提高临床诊断价值，为医学图像处理提供了新的解决方案。"}}
{"id": "2512.07249", "pdf": "https://arxiv.org/pdf/2512.07249", "abs": "https://arxiv.org/abs/2512.07249", "authors": ["Jingran Yang", "Min Zhang", "Lingfeng Zhang", "Zhaohui Wang", "Yonggang Zhang"], "title": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.", "AI": {"tldr": "提出IFFair方法，基于影响函数动态调整训练样本权重以实现公平分类，无需修改网络结构、数据特征或决策边界", "motivation": "机器学习在决策中广泛应用，但基于数据的模式会使算法学习并加剧样本中的潜在偏见，导致对某些弱势群体的歧视性决策，剥夺他们平等对待的权利，损害社会福利并阻碍相关应用发展", "method": "基于影响函数的预处理方法IFFair，利用训练样本对不同群体的影响差异作为指导，在训练过程中动态调整样本权重，不修改网络结构、数据特征和决策边界", "result": "在多个真实世界数据集和指标上的实验表明，IFFair缓解了分类设置中的多种公平性指标偏见，包括人口统计均等、均衡几率、机会均等和错误率均等，且无冲突。相比之前的预处理方法，在多个效用和公平性指标之间实现了更好的权衡", "conclusion": "IFFair是一种有效的公平分类预处理方法，通过影响函数驱动的样本重加权机制，在保持模型效用的同时显著提升公平性，为机器学习公平性研究提供了新的技术路径"}}
{"id": "2512.07247", "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "提出AdLift方法，通过将2D对抗性扰动提升到3D高斯表示中，保护3D高斯泼溅资产免受指令驱动编辑的威胁", "motivation": "随着基于扩散模型的指令驱动2D图像编辑扩展到3D高斯泼溅(3DGS)，虽然促进了3D内容创作，但也使3DGS资产面临未经授权编辑和恶意篡改的风险。现有2D对抗性扰动方法难以直接应用于3DGS，面临视角通用性保护和可见性与保护能力平衡两大挑战", "method": "提出AdLift方法，将严格有界的2D对抗性扰动提升到3D高斯表示的防护层中。通过定制的Lifted PGD进行渐进式优化：首先在渲染图像上从编辑模型进行梯度截断，应用投影梯度严格约束图像级扰动；然后通过图像到高斯的拟合操作将扰动反向传播到防护高斯参数中。交替进行梯度截断和图像到高斯拟合，实现跨不同视角的一致保护", "result": "实验结果表明，AdLift能有效保护3DGS资产免受最先进的指令驱动2D图像和3DGS编辑攻击，在保持不可见性的同时提供跨视角的鲁棒保护", "conclusion": "AdLift是首个针对3D高斯泼溅的编辑防护方法，通过将2D对抗性扰动提升到3D表示中，解决了视角通用性保护和可见性与保护能力平衡的挑战，为3DGS资产提供了有效的安全保障"}}
{"id": "2512.07245", "pdf": "https://arxiv.org/pdf/2512.07245", "abs": "https://arxiv.org/abs/2512.07245", "authors": ["Toshinori Yamauchi", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features", "categories": ["cs.CV"], "comment": "11+6 pages, 8 figures, 4 tables", "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.", "AI": {"tldr": "提出TEXTER方法，通过分离决策关键特征并映射到CLIP特征空间，为零样本图像分类器生成忠实且可解释的文本解释", "motivation": "现有零样本解释方法仅对齐全局图像特征与语言，描述可见内容而非驱动预测的关键因素，无法反映分类器的特定推理过程", "method": "TEXTER首先识别对预测有贡献的神经元，强调这些神经元编码的决策关键特征，然后将这些特征映射到CLIP特征空间以检索反映模型推理的文本解释，并使用稀疏自编码器提高Transformer架构的可解释性", "result": "大量实验表明，TEXTER比现有方法生成更忠实和可解释的解释", "conclusion": "通过分离决策关键特征再进行对齐的方法，能够为零样本图像分类器生成更准确反映模型推理过程的文本解释"}}
{"id": "2512.07241", "pdf": "https://arxiv.org/pdf/2512.07241", "abs": "https://arxiv.org/abs/2512.07241", "authors": ["Md. Srabon Chowdhury", "Syeda Fahmida Tanzim", "Sheekar Banerjee", "Ishtiak Al Mamoon", "AKM Muzahidul Islam"], "title": "Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture", "categories": ["cs.CV"], "comment": null, "summary": "Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.", "AI": {"tldr": "提出一种基于SqueezeNet v1和EfficientNet-B0的混合深度学习模型，结合手工放射组学特征，用于脑肿瘤MRI图像的自动分类", "motivation": "脑肿瘤诊断需要及时准确，但MRI图像中的肿瘤勾画过程困难且耗时，容易产生观察者间误差，需要自动化解决方案来提高诊断效率和准确性", "method": "结合轻量级SqueezeNet v1和高性能EfficientNet-B0的混合神经网络架构，并增强手工放射组学特征（HOG、LBP、Gabor滤波器和小波变换），在Nickparvar脑肿瘤MRI数据集上进行训练和测试", "result": "模型测试准确率达到98.93%，使用测试时间增强后达到99.08%，具有出色的泛化能力；模型仅需少于210万个参数和少于1.2 GFLOPs的计算量", "conclusion": "提出的混合网络在计算效率和诊断准确性之间取得了良好平衡，具有临床可靠性，可用于临床决策支持系统中的自动化MRI脑肿瘤分类"}}
{"id": "2512.07237", "pdf": "https://arxiv.org/pdf/2512.07237", "abs": "https://arxiv.org/abs/2512.07237", "authors": ["Cheng Zhang", "Boying Li", "Meng Wei", "Yan-Pei Cao", "Camilo Cruz Gambardella", "Dinh Phung", "Jianfei Cai"], "title": "Unified Camera Positional Encoding for Controlled Video Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/chengzhag/UCPE", "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.", "AI": {"tldr": "提出统一的相机位置编码方法UCPE，用于可控视频生成，通过几何一致的表示统一处理完整的相机信息", "motivation": "现有相机编码方法通常基于简化的针孔模型假设，限制了在真实世界相机多样内参和镜头畸变情况下的泛化能力", "method": "提出相对射线编码（Relative Ray Encoding）作为几何一致表示，统一处理6自由度位姿、内参和镜头畸变；设计绝对方向编码控制初始相机朝向；通过轻量级空间注意力适配器集成到预训练视频扩散Transformer中", "result": "UCPE在相机可控视频生成任务中达到最先进的相机控制能力和视觉保真度，仅增加不到1%的可训练参数；构建了覆盖广泛相机运动和镜头类型的大型视频数据集", "conclusion": "UCPE在相机可控视频生成中验证了有效性，并展示了作为Transformer通用相机表示在多视图、视频和3D任务中的潜力"}}
{"id": "2512.07234", "pdf": "https://arxiv.org/pdf/2512.07234", "abs": "https://arxiv.org/abs/2512.07234", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "AI": {"tldr": "提出Dropout Prompt Learning方法，通过改进的dropout技术增强视觉语言模型的鲁棒性和适应性", "motivation": "传统dropout技术虽然能提高模型泛化能力，但在视觉语言模型中需要更精细的token级dropout策略，同时考虑模态内上下文和模态间对齐，以提升模型在挑战性场景下的性能", "method": "提出基于token重要性的dropout方法，综合考虑模态内上下文和模态间对齐来为每个token分配灵活的dropout概率；进一步提出残差熵正则化，在保持语义对齐的同时鼓励dropout引入的多样化表示", "result": "在15个基准测试上验证了方法的有效性，在低样本学习、长尾分类和分布外泛化等挑战性场景中表现优异；在base-to-novel泛化任务上，性能超过KgCoOp 5.10%，超过PromptSRC 2.13%", "conclusion": "Dropout Prompt Learning通过改进的token级dropout策略和残差熵正则化，显著提升了视觉语言模型的鲁棒性和适应性，为挑战性场景下的视觉语言任务提供了有效的解决方案"}}
{"id": "2512.07232", "pdf": "https://arxiv.org/pdf/2512.07232", "abs": "https://arxiv.org/abs/2512.07232", "authors": ["Wenlong Liu", "Jiahua Pan", "Xingyu Zhang", "Xinxin Gong", "Yang Ye", "Xujin Zhao", "Xin Wang", "Kent Wu", "Hua Xiang", "Houmin Yan", "Qingpeng Zhang"], "title": "Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model", "categories": ["cs.AI"], "comment": "10 pages, 5 figures, published on World Wide Web", "summary": "Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).", "AI": {"tldr": "该论文提出了一种基于知识图谱实体对齐的跨平台产品匹配方法，采用RAEA模型（关系感知和属性感知的图注意力网络）来改进实体对齐任务。", "motivation": "现有实体对齐方法未能充分利用属性三元组和关系三元组之间的交互作用，导致跨平台产品匹配效果不佳。需要一种能够同时考虑属性和关系信息的方法来提高产品匹配的准确性。", "method": "采用两阶段流水线：粗过滤和细过滤。细过滤阶段使用RAEA模型，该模型包含属性感知实体编码器和关系感知图注意力网络，能够聚合来自属性和关系的对齐信号。", "result": "在跨语言数据集DBP15K上，RAEA模型相比12个基线方法平均Hits@1提升了6.59%；在单语言数据集DWY100K上也取得了有竞争力的结果。", "conclusion": "RAEA模型通过有效利用属性三元组和关系三元组之间的交互作用，显著提升了实体对齐性能，为跨平台产品匹配提供了有效的解决方案。"}}
{"id": "2512.07230", "pdf": "https://arxiv.org/pdf/2512.07230", "abs": "https://arxiv.org/abs/2512.07230", "authors": ["Abhinav Raundhal", "Gaurav Behera", "P J Narayanan", "Ravi Kiran Sarvadevabhatla", "Makarand Tapaswi"], "title": "STRinGS: Selective Text Refinement in Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026. Project Page, see https://STRinGS-official.github.io", "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.", "AI": {"tldr": "提出STRinGS方法，一种针对3D高斯泼溅（3DGS）的选择性文本细化框架，用于在3D场景重建中更好地保留文本细节", "motivation": "现实场景中的文本（如标志、标签、指令）包含重要上下文信息，但现有3D表示方法（如3DGS）难以保留细粒度文本细节，文本重建的小错误会导致显著的语义损失", "method": "采用文本感知的选择性细化框架，将文本区域和非文本区域分开处理：先细化文本区域，然后与优化后的非文本区域合并进行全场景优化", "result": "STRinGS能够生成清晰可读的文本，即使在具有挑战性的配置下；使用OCR字符错误率（CER）评估，在仅7K次迭代时相比3DGS有63.6%的相对改进；同时创建了STRinGS-360数据集来评估3D重建中的文本可读性", "conclusion": "该方法与数据集共同推动了文本丰富环境中3D场景理解的边界，为更鲁棒的文本感知重建方法铺平了道路"}}
{"id": "2512.07229", "pdf": "https://arxiv.org/pdf/2512.07229", "abs": "https://arxiv.org/abs/2512.07229", "authors": ["Fang Zhou", "Zhiqiang Chen", "Martin Pavlovski", "Yizhong Zhang"], "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted to the Main Track of the 28th European Conference on Artificial Intelligence (ECAI 2025). To appear in the proceedings published by IOS Press (DOI: 10.3233/FAIA413)", "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.", "AI": {"tldr": "提出ReLKD框架，通过类间关系学习和知识蒸馏解决广义类别发现(GCD)问题，在已知类别标签有限的情况下有效分类未标记数据中的已知和新类别。", "motivation": "现有GCD方法通常独立处理每个类别，忽略了类间关系，而直接获取这些关系在现实场景中具有挑战性。需要一种能够利用隐式类间关系来增强新类别分类的方法。", "method": "ReLKD包含三个关键模块：目标粒度模块学习判别性表示，粗粒度模块捕获层次化类间关系，蒸馏模块将粗粒度模块的知识迁移到目标粒度模块以优化表示学习。", "result": "在四个数据集上的大量实验证明了ReLKD的有效性，特别是在标签数据有限的情况下表现优异。", "conclusion": "ReLKD通过端到端框架有效利用隐式类间关系，并通过知识蒸馏增强新类别分类，为解决GCD问题提供了创新解决方案。"}}
{"id": "2512.07228", "pdf": "https://arxiv.org/pdf/2512.07228", "abs": "https://arxiv.org/abs/2512.07228", "authors": ["Hengyang Yao", "Lin Li", "Ke Sun", "Jianing Qiu", "Huiping Chen"], "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.", "AI": {"tldr": "提出EOLT框架，通过强化学习学习变换分布，生成对抗DeepFake人脸交换的鲁棒性保护扰动", "motivation": "现有DeepFake防御方法嵌入的不可见扰动脆弱，容易被压缩、调整大小等基本变换破坏，需要提高保护扰动的鲁棒性", "method": "提出EOLT框架，将变换分布作为可学习组件，使用策略网络通过强化学习自动优先处理关键变换并生成实例特定的扰动", "result": "EOLT相比现有方法平均鲁棒性提高26%，在具有挑战性的变换类别上提升高达30%", "conclusion": "EOLT框架通过可学习的变换分布显著提高了对抗DeepFake人脸交换的保护扰动的鲁棒性"}}
{"id": "2512.07226", "pdf": "https://arxiv.org/pdf/2512.07226", "abs": "https://arxiv.org/abs/2512.07226", "authors": ["Runwu Shi", "Chang Li", "Jiang Wang", "Rui Zhang", "Nabeela Khan", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Audio Separation with Diffusion Source Priors", "categories": ["eess.AS", "cs.SD"], "comment": "15 pages, 31 figures, accepted by The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Single-channel audio separation aims to separate individual sources from a single-channel mixture. Most existing methods rely on supervised learning with synthetically generated paired data. However, obtaining high-quality paired data in real-world scenarios is often difficult. This data scarcity can degrade model performance under unseen conditions and limit generalization ability. To this end, in this work, we approach this problem from an unsupervised perspective, framing it as a probabilistic inverse problem. Our method requires only diffusion priors trained on individual sources. Separation is then achieved by iteratively guiding an initial state toward the solution through reconstruction guidance. Importantly, we introduce an advanced inverse problem solver specifically designed for separation, which mitigates gradient conflicts caused by interference between the diffusion prior and reconstruction guidance during inverse denoising. This design ensures high-quality and balanced separation performance across individual sources. Additionally, we find that initializing the denoising process with an augmented mixture instead of pure Gaussian noise provides an informative starting point that significantly improves the final performance. To further enhance audio prior modeling, we design a novel time-frequency attention-based network architecture that demonstrates strong audio modeling capability. Collectively, these improvements lead to significant performance gains, as validated across speech-sound event, sound event, and speech separation tasks.", "AI": {"tldr": "提出一种无监督单通道音频分离方法，利用扩散源先验和重建引导，无需成对训练数据", "motivation": "现实场景中获取高质量成对数据困难，现有监督方法在未见条件下性能下降且泛化能力有限，需要无监督解决方案", "method": "1) 将分离问题构建为概率逆问题，仅需在单个源上训练的扩散先验；2) 设计专门用于分离的逆问题求解器，缓解扩散先验与重建引导间的梯度冲突；3) 使用增强混合而非纯高斯噪声初始化去噪过程；4) 设计新颖的时频注意力网络架构增强音频先验建模", "result": "在语音-声音事件、声音事件和语音分离任务上验证了显著性能提升，实现了高质量且平衡的分离效果", "conclusion": "该方法通过无监督方式有效解决了单通道音频分离问题，克服了数据稀缺限制，在各种分离任务上表现出优越性能"}}
{"id": "2512.07224", "pdf": "https://arxiv.org/pdf/2512.07224", "abs": "https://arxiv.org/abs/2512.07224", "authors": ["Tianyi Ren", "Daniel Low", "Pittra Jaengprajak", "Juampablo Heras Rivera", "Jacob Ruzevick", "Mehmet Kurt"], "title": "Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician\" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \\textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.", "AI": {"tldr": "该论文提出了一种基于Shapley值的深度学习分割模型临床可解释性评估方法，通过对比级Shapley值分析MRI不同对比度对模型性能的贡献，并提出了两个临床可解释的指标：模型与临床排名的协议度以及Shapley排名的方差不确定性。", "motivation": "尽管深度学习模型在医学图像分割中取得了显著性能，但其可解释性仍然是临床实践中接受和整合的关键需求。现有研究多关注梯度基技术识别影响区域，但缺乏与临床实践对齐的全面评估方法。", "method": "使用BraTS 2024数据集，通过对比级Shapley值系统扰动模型输入来评估特征重要性。为四种MRI对比度在四种模型架构中生成Shapley值排名，并提出两个新指标：1) 模型与\"临床医生\"成像排名的协议度；2) 跨交叉验证折叠的Shapley排名方差量化不确定性。", "result": "高表现病例（Dice >0.6）显示出与临床排名显著更高的协议度。Shapley排名方差增加与性能下降相关（U-Net: r=-0.581）。这些指标为模型可靠性提供了临床可解释的代理。", "conclusion": "提出的基于Shapley值的协议度和不确定性指标为深度学习分割模型提供了临床可解释的可靠性评估方法，有助于临床医生更好地理解最先进的分割模型，促进其在临床实践中的接受和整合。"}}
{"id": "2512.07221", "pdf": "https://arxiv.org/pdf/2512.07221", "abs": "https://arxiv.org/abs/2512.07221", "authors": ["Zichao Shu", "Shitao Bei", "Lijun Li", "Zetao Chen"], "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality", "categories": ["cs.RO"], "comment": null, "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.", "AI": {"tldr": "本文提出了一种用于XR环境中SLAM算法高精度基准测试的时空校准和地面真值估计方法，通过连续时间最大似然估计器解决运动捕捉系统的抖动和时间同步问题。", "motivation": "随着XR应用对沉浸感要求的提高，SLAM基准测试的需求变得更加严格。基于标记的光学运动捕捉系统虽然被广泛用于生成地面真值，但其精度受到时空校准和设备抖动两个因素的限制，这阻碍了准确的SLAM基准测试，特别是对于XR体验至关重要的旋转误差和帧间抖动等关键指标。", "method": "提出了一种新颖的连续时间最大似然估计器，该方法整合了辅助惯性测量单元数据来补偿运动捕捉系统的抖动。同时提出了可变时间同步方法和基于螺旋同余约束的位姿残差，实现了跨多个传感器和被测试设备的精确时空校准。", "result": "实验结果表明，该方法优于现有方法，达到了对XR应用中最先进SLAM算法进行全面基准测试所需的精度。通过基准测试多个领先的XR设备和开源SLAM算法，充分验证了该方法的实用性。", "conclusion": "本文提出的方法解决了运动捕捉系统在SLAM基准测试中的局限性，实现了高精度的地面真值估计，为XR环境中SLAM算法的准确评估提供了有效工具，代码已公开可用。"}}
{"id": "2512.07219", "pdf": "https://arxiv.org/pdf/2512.07219", "abs": "https://arxiv.org/abs/2512.07219", "authors": ["Sungyong Chung", "Alireza Talebpour", "Samer H. Hamdar"], "title": "Characterizing Lane-Changing Behavior in Mixed Traffic", "categories": ["cs.MA", "cs.GT", "cs.RO", "eess.SY"], "comment": null, "summary": "Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.", "AI": {"tldr": "本研究旨在表征混合交通中自动驾驶车辆（AVs）存在时的车道变换行为，分析主动换道车辆与目标车道被动车辆之间的交互模式，并探讨不同AV市场渗透率下这些行为的变化。", "motivation": "在混合交通环境中，理解和表征车道变换行为对于确保安全和效率至关重要。随着自动驾驶车辆逐渐融入交通系统，需要研究AVs与人类驾驶车辆（HDVs）在车道变换中的交互行为，特别是合作与对抗行为的模式。", "method": "使用Waymo开放运动数据集（WOMD）中的真实轨迹数据，分析7,636个观察到的车道变换事件。首先采用k-means聚类将车辆分类为合作型或对抗型；然后使用量化响应均衡框架联合估计主动和被动车辆的效用函数；基于这些效用构建经验收益表；最后使用进化博弈论分析社会困境的存在和合作行为的演化。", "result": "研究发现：1）合作型AVs的比例在主动和被动角色中都高于HDVs；2）约4%的主动车辆和11%的被动车辆存在社会困境，主要归类为猎鹿博弈或囚徒困境（极少观察到懦夫博弈）；3）蒙特卡洛模拟显示，重复的车道变换交互会随时间推移持续增加合作行为，且不受AV渗透率影响。", "conclusion": "该研究成功表征了混合交通中的车道变换行为，揭示了合作行为在重复交互中的演化趋势，为理解AVs如何影响交通交互行为提供了重要见解，对混合交通系统的安全设计具有参考价值。"}}
{"id": "2512.07218", "pdf": "https://arxiv.org/pdf/2512.07218", "abs": "https://arxiv.org/abs/2512.07218", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "AI": {"tldr": "提出NeSTR框架，通过神经符号集成增强大型语言模型的时间推理能力", "motivation": "大型语言模型在时间推理方面存在挑战，现有符号方法未充分利用LLM推理能力，反思方法缺乏结构化时间表示，导致推理不一致或幻觉", "method": "NeSTR框架结合结构化符号表示与混合反思推理，通过符号编码保留明确时间关系，通过验证强制执行逻辑一致性，使用溯因反思纠正错误推理", "result": "在多样化时间问答基准测试中，NeSTR实现了卓越的零样本性能，无需微调即可持续改进时间推理", "conclusion": "神经符号集成在增强大型语言模型时间理解方面具有优势，NeSTR框架有效解决了LLM在复杂时间约束下的推理挑战"}}
{"id": "2512.07215", "pdf": "https://arxiv.org/pdf/2512.07215", "abs": "https://arxiv.org/abs/2512.07215", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "AI": {"tldr": "该论文提出了VFM-VLM框架，对基于CLIP和DINOv2的视觉基础模型在3D姿态估计任务中进行全面比较，特别关注手-物体抓取场景。", "motivation": "视觉基础模型和视觉语言模型为计算机视觉提供了丰富的语义和几何表示，但它们在3D姿态估计任务中的表现差异尚未得到系统研究。需要了解不同模型在语义理解和几何特征提取方面的互补优势，以指导机器人操作和抓取应用中的模型选择。", "method": "采用基于CLIP和DINOv2的两种方法进行6D物体姿态估计比较。在基准数据集上进行广泛实验，评估两种模型在语义一致性和几何精度方面的表现，分析它们在手-物体抓取场景中的性能差异。", "result": "实验表明：基于CLIP的方法在语义理解方面表现更优，通过语言接地实现更好的语义一致性；而基于DINOv2的方法提供更优越的密集几何特征，在几何精度方面具有竞争优势。两种模型展现出互补的优势特征。", "conclusion": "CLIP和DINOv2在3D姿态估计任务中各有优势：CLIP擅长语义理解，DINOv2擅长几何特征提取。该分析为机器人操作、抓取和拣选应用中选择合适的视觉模型提供了指导依据。"}}
{"id": "2512.07212", "pdf": "https://arxiv.org/pdf/2512.07212", "abs": "https://arxiv.org/abs/2512.07212", "authors": ["Zhaoyang Liu", "Mokai Pan", "Zhongyi Wang", "Kaizhen Zhu", "Haotao Lu", "Jingya Wang", "Ye Shi"], "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.", "AI": {"tldr": "提出一种名为BridgePolicy的视觉运动策略学习方法，通过扩散桥公式将观测嵌入随机微分方程中，使采样从信息丰富的先验开始而非随机噪声，从而改善控制精度和可靠性。", "motivation": "现有基于扩散模型的模仿学习方法通常将观测作为去噪网络的高层条件输入，而非将其整合到扩散过程的随机动力学中。这导致采样必须从随机高斯噪声开始，削弱了感知与控制之间的耦合，往往产生次优性能。", "method": "提出BridgePolicy方法，采用扩散桥公式将观测嵌入随机微分方程；设计多模态融合模块和语义对齐器来统一视觉和状态输入，对齐观测和动作表示，使桥方法适用于异构机器人数据。", "result": "在三个基准测试的52个模拟任务和五个现实世界任务上的广泛实验表明，BridgePolicy始终优于最先进的生成策略。", "conclusion": "通过将观测嵌入扩散桥的随机动力学中，BridgePolicy能够从信息丰富的先验开始采样，显著提高了控制精度和可靠性，为异构机器人数据的生成策略学习提供了有效解决方案。"}}
{"id": "2512.07211", "pdf": "https://arxiv.org/pdf/2512.07211", "abs": "https://arxiv.org/abs/2512.07211", "authors": ["Frederik Hagelskjær", "Dimitrios Arapis", "Steffen Madsen", "Thorbjørn Mosekjær Iversen"], "title": "Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds", "categories": ["cs.CV"], "comment": "8 pages, 8 figures, 5 tables, ICCR 2025", "summary": "Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings. We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io", "AI": {"tldr": "提出一种基于神经网络的物体姿态分布估计方法，仅使用3D无颜色数据，用于确定点云中的旋转和反射不确定性", "motivation": "现有的姿态分布方法严重依赖颜色信息，而工业场景中往往缺乏RGB数据；单点姿态估计无法捕捉视觉模糊性带来的不确定性，可能导致不可靠的行为", "method": "开发了一种新颖的神经网络方法，仅使用3D无色数据估计物体姿态不确定性；该方法不依赖RGB输入，专注于处理反射和旋转对称性", "result": "在真实世界拣选场景中验证了方法有效性，处理了具有不同几何模糊性的物体；当前实现专注于反射和旋转对称性，但框架可扩展到完整的SE(3)姿态分布估计", "conclusion": "这是首个不依赖RGB输入的基于深度学习的姿态分布估计方法，解决了工业环境中颜色信息缺失的问题，为机器人感知提供了更可靠的姿态不确定性估计"}}
{"id": "2512.07209", "pdf": "https://arxiv.org/pdf/2512.07209", "abs": "https://arxiv.org/abs/2512.07209", "authors": ["Masato Ishii", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits", "categories": ["cs.MM", "cs.LG", "cs.SD"], "comment": null, "summary": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.", "AI": {"tldr": "提出一种新颖的音频-视频联合编辑流程，通过条件音频生成来增强编辑后视频与音频之间的连贯性", "motivation": "现有的视频编辑技术通常只关注视觉内容，忽略了编辑后视频与原始音频之间的不匹配问题，导致音频-视觉不一致", "method": "采用两阶段流程：先应用最先进的视频编辑技术生成目标视频，然后通过新的视频到音频生成模型进行音频编辑。该模型以源音频、目标视频和文本提示为条件，并动态调整源音频的影响权重", "result": "实验结果表明，该方法在保持音频-视觉对齐和内容完整性方面优于现有方法", "conclusion": "提出的联合音频-视觉编辑框架能够有效增强编辑后内容的连贯性，为多媒体编辑提供了更完整的解决方案"}}
{"id": "2512.07208", "pdf": "https://arxiv.org/pdf/2512.07208", "abs": "https://arxiv.org/abs/2512.07208", "authors": ["Fei Luo", "Ziwei Zhao", "Mingxuan Wang", "Duoyang Li", "Zhe Qian", "Jiayi Tuo", "Chenyue Zhou", "Yanbiao Ma"], "title": "Geometric Prior-Guided Federated Prompt Calibration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.", "AI": {"tldr": "提出GGTPC框架，通过全局几何先验校正联邦提示学习中的数据异构性导致的局部训练偏差", "motivation": "联邦提示学习(FPL)在数据异构环境下性能严重受限，现有方法未能解决局部训练偏差的根本原因", "method": "GGTPC框架：1) 服务器端隐私保护地重建全局几何先验（数据分布形状）；2) 客户端使用几何先验校准层(GPCL)对齐局部特征分布", "result": "在CIFAR-100标签偏斜数据集上超越SOTA 2.15%，极端偏斜下提升9.17%；在Office-Home域偏斜数据集上提升FedAvg性能4.60%", "conclusion": "GGTPC通过校正局部训练偏差有效缓解数据异构问题，可作为即插即用模块增强各种联邦学习算法"}}
{"id": "2512.07206", "pdf": "https://arxiv.org/pdf/2512.07206", "abs": "https://arxiv.org/abs/2512.07206", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "AI": {"tldr": "开发一个名为AutoLugano的深度学习框架，用于从FDG-PET/CT扫描中实现淋巴瘤的自动分割、解剖定位和Lugano分期", "motivation": "淋巴瘤的准确分期对治疗决策至关重要，但传统的人工分期过程耗时且存在观察者间差异。需要开发一个完全自动化的系统来辅助临床决策", "method": "系统包含三个顺序模块：1）基于3D nnU-Net的解剖感知病灶分割；2）使用TotalSegmentator工具包进行基于图谱的解剖定位；3）根据受累区域的空间分布自动进行Lugano分期", "result": "在外部验证集上，区域受累检测的准确率为88.31%，敏感性74.47%，特异性94.21%，F1分数80.80%。对于治疗分层（局限期vs进展期），准确率达到85.07%，特异性90.48%，敏感性82.61%", "conclusion": "AutoLugano是首个完全自动化的端到端系统，能够将单个基线FDG-PET/CT扫描转换为完整的Lugano分期，在初始分期、治疗分层和临床决策支持方面具有强大潜力"}}
{"id": "2512.07203", "pdf": "https://arxiv.org/pdf/2512.07203", "abs": "https://arxiv.org/abs/2512.07203", "authors": ["Xuhui Zheng", "Kang An", "Ziliang Wang", "Yuhang Wang", "Faqiang Qian", "Yichao Wu"], "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning", "categories": ["cs.CV"], "comment": "7 pages, 1 figures", "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.", "AI": {"tldr": "提出MMRPT框架，通过掩码多模态强化预训练增强视觉推理能力，首次将强化学习直接融入大型视觉语言模型的预训练中", "motivation": "当前多模态预训练受限于图像-标题对的描述性偏差，模型倾向于依赖表面语言线索而非真正的视觉理解，需要强化视觉推理能力", "method": "通过注意力机制估计句子级视觉依赖度，掩码高度依赖视觉的文本片段，模型通过视觉引导的推理重建这些片段，使用语义-视觉奖励指导学习", "result": "在多种基准测试中实现一致的零样本性能提升，在监督微调下显著提高鲁棒性，证明强化驱动的掩码推理为多模态模型提供了更可靠和可泛化的预训练目标", "conclusion": "MMRPT框架通过强化学习奖励视觉基础而非标题模仿，有效增强了多模态模型的视觉推理能力，为多模态预训练提供了更优的学习目标"}}
{"id": "2512.07201", "pdf": "https://arxiv.org/pdf/2512.07201", "abs": "https://arxiv.org/abs/2512.07201", "authors": ["Cheng Yu"], "title": "Understanding Diffusion Models via Code Execution", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.", "AI": {"tldr": "该论文通过约300行简洁代码实现，从代码执行角度解释扩散模型的工作原理，弥合理论公式与开源实现之间的差距。", "motivation": "扩散模型在生成建模中表现优异，但理论基础复杂，且论文中的数学公式与实际开源实现之间存在较大差距。现有教程主要关注公式推导，对扩散模型在代码中如何实际运行提供有限指导。", "method": "提供一个约300行的简洁实现，从代码执行角度解释扩散模型，保留前向扩散、反向采样、噪声预测网络和训练循环等核心组件，同时去除不必要的工程细节。", "result": "创建了一个最小化示例，清晰地展示了扩散模型在实际中的工作原理，以及代码与理论之间的对应关系。代码和预训练模型已在GitHub上开源。", "conclusion": "通过代码优先的方法，为研究人员提供了对扩散模型实际工作原理的清晰理解，弥合了理论公式与实际实现之间的鸿沟，有助于更好地理解和应用扩散模型。"}}
{"id": "2512.07198", "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "提出Storytelling Image Generation任务，旨在生成具有丰富推理链的叙事图像，并开发了StorytellingPainter两阶段生成管道和相应的评估框架。", "motivation": "叙事图像通过丰富的视觉线索和逻辑连接传达复杂故事，在插图创作、认知筛查等领域有广泛应用，但由于其复杂的语义特性，这类图像难以创建且相对稀缺。", "method": "提出StorytellingPainter两阶段管道：1) 利用大语言模型进行创造性推理生成故事；2) 使用文生图模型进行视觉合成。同时开发了包含语义复杂度评估器、KNN多样性评估器和故事-图像对齐评估器的评估框架。针对开源与专有LLM性能差距，还开发了Mini-Storytellers系列轻量级模型。", "result": "实验结果表明所提方法的可行性和有效性，StorytellingPainter能够生成具有丰富推理链的叙事图像，评估框架能够全面评估生成质量，Mini-Storytellers模型在缩小开源与专有模型性能差距方面表现良好。", "conclusion": "该研究成功定义了叙事图像生成任务，提出了有效的生成管道和评估框架，并通过实验验证了方法的可行性，为生成具有复杂语义的叙事图像提供了系统解决方案。"}}
{"id": "2512.07197", "pdf": "https://arxiv.org/pdf/2512.07197", "abs": "https://arxiv.org/abs/2512.07197", "authors": ["Seokhyun Youn", "Soohyun Lee", "Geonho Kim", "Weeyoung Kwon", "Sung-Ho Bae", "Jihyong Oh"], "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting", "categories": ["cs.CV"], "comment": "The first three authors contributed equally to this work. The last two authors are co-corresponding authors. Please visit our project page at https://cmlab-korea.github.io/Awesome-Efficient-GS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.", "AI": {"tldr": "该论文是一篇关于高效高斯泼溅技术的综述性研究，系统性地总结了3D和4D高斯泼溅的压缩与紧凑化方法。", "motivation": "3D高斯泼溅虽然能实现实时高保真3D重建和新视角合成，但其存储和渲染数百万高斯函数需要巨大的内存和计算资源，特别是在4D动态场景中更为严重，这限制了其实际应用。", "method": "将现有方法系统性地分为两大方向：参数压缩和重构压缩，并对每个类别中的核心思想和方法趋势进行全面总结。同时涵盖广泛使用的数据集、评估指标和代表性基准比较。", "result": "提供了首个统一的3D和4D高效高斯泼溅技术概述，建立了系统的分类框架，总结了当前的技术发展趋势和代表性方法。", "conclusion": "讨论了当前方法的局限性，并展望了未来研究方向，旨在实现可扩展、紧凑且实时的静态和动态3D场景表示的高斯泼溅技术。"}}
{"id": "2512.07195", "pdf": "https://arxiv.org/pdf/2512.07195", "abs": "https://arxiv.org/abs/2512.07195", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "comment": null, "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "AI": {"tldr": "提出了MASim，首个支持多语言代理交互的基于代理的仿真框架，用于社会科学研究", "motivation": "现有多代理角色扮演仿真大多是单语言的，无法模拟现实社会中跨语言交互这一重要特性", "method": "开发了MASim多语言代理仿真框架，支持具有不同社会语言学特征的生成式代理之间的多轮交互，并构建了MAPS基准测试集", "result": "实验在标定、敏感性、一致性和文化案例研究方面显示，MASim能够重现社会文化现象，并突显多语言仿真对于可扩展、可控计算社会科学的重要性", "conclusion": "MASim是首个支持多语言代理交互的仿真框架，为研究跨语言社会行为提供了重要工具，推动了计算社会科学的发展"}}
{"id": "2512.07194", "pdf": "https://arxiv.org/pdf/2512.07194", "abs": "https://arxiv.org/abs/2512.07194", "authors": ["Yuchen Tian", "Samuel Tensingh", "Jason Eshraghian", "Nhan Duy Truong", "Omid Kavehei"], "title": "Synchrony-Gated Plasticity with Dopamine Modulation for Spiking Neural Networks", "categories": ["cs.NE"], "comment": "23 pages, 7 figures, 5 tables, accepted by TMLR", "summary": "While surrogate backpropagation proves useful for training deep spiking neural networks (SNNs), incorporating biologically inspired local signals on a large scale remains challenging. This difficulty stems primarily from the high memory demands of maintaining accurate spike-timing logs and the potential for purely local plasticity adjustments to clash with the supervised learning goal. To effectively leverage local signals derived from spiking neuron dynamics, we introduce Dopamine-Modulated Spike-Synchrony-Dependent Plasticity (DA-SSDP), a synchrony-based rule that is sensitive to loss and brings a synchrony-based local learning signal to the model. DA-SSDP condenses spike patterns into a synchrony metric at the batch level. An initial brief warm-up phase assesses its relationship to the task loss and sets a fixed gate that subsequently adjusts the local update's magnitude. In cases where synchrony proves unrelated to the task, the gate settles at one, simplifying DA-SSDP to a basic two-factor synchrony mechanism that delivers minor weight adjustments driven by concurrent spike firing and a Gaussian latency function. These small weight updates are only added to the network`s deeper layers following the backpropagation phase, and our tests showed this simplified version did not degrade performance and sometimes gave a small accuracy boost, serving as a regularizer during training. The rule stores only binary spike indicators and first-spike latencies with a Gaussian kernel. Without altering the model structure or optimization routine, evaluations on benchmarks like CIFAR-10 (+0.42\\%), CIFAR-100 (+0.99\\%), CIFAR10-DVS (+0.1\\%), and ImageNet-1K (+0.73\\%) demonstrated consistent accuracy gains, accompanied by a minor increase in computational overhead. Our code is available at https://github.com/NeuroSyd/DA-SSDP.", "AI": {"tldr": "本文提出了一种名为DA-SSDP（多巴胺调制的尖峰同步依赖性可塑性）的生物启发局部学习规则，用于深度脉冲神经网络训练，通过同步性度量结合监督损失信号来增强网络性能。", "motivation": "虽然替代反向传播在训练深度脉冲神经网络方面有效，但大规模整合生物启发的局部信号仍然具有挑战性。主要困难在于保持精确尖峰时序记录的高内存需求，以及纯局部可塑性调整可能与监督学习目标冲突。需要一种能够有效利用脉冲神经元动态产生的局部信号的方法。", "method": "提出DA-SSDP规则，将尖峰模式压缩为批次级别的同步性度量。包含初始短暂预热阶段评估同步性与任务损失的关系，并设置固定门控来调整局部更新的幅度。当同步性与任务无关时，门控设为1，简化为基本的两因子同步机制，仅进行微小权重调整。这些更新仅在反向传播后添加到网络的深层，规则仅存储二进制尖峰指示器和具有高斯核的首尖峰延迟。", "result": "在不改变模型结构或优化流程的情况下，在多个基准测试中实现了持续精度提升：CIFAR-10 (+0.42%)、CIFAR-100 (+0.99%)、CIFAR10-DVS (+0.1%) 和 ImageNet-1K (+0.73%)，同时计算开销仅轻微增加。简化版本不仅未降低性能，有时还能提供小的精度提升，起到训练正则化作用。", "conclusion": "DA-SSDP成功地将基于同步性的局部学习信号整合到深度脉冲神经网络训练中，通过多巴胺调制和门控机制平衡了生物启发局部学习与监督学习目标，实现了性能提升且计算开销可控，为生物可塑性机制与深度学习框架的有效结合提供了可行方案。"}}
{"id": "2512.07192", "pdf": "https://arxiv.org/pdf/2512.07192", "abs": "https://arxiv.org/abs/2512.07192", "authors": ["Niu Yi", "Xu Tianyi", "Ma Mingming", "Wang Xinkun"], "title": "HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression", "categories": ["cs.CV"], "comment": "12 pages, 7 figures", "summary": "Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.", "AI": {"tldr": "提出HVQ-CGIC框架，首次在基于向量量化的生成式图像压缩中引入超先验熵建模，实现率失真平衡与控制", "motivation": "现有基于向量量化的生成式图像压缩方法使用静态全局概率分布估计VQ索引的熵，无法适应每张图像的具体内容，导致比特率潜力未充分挖掘且难以实现灵活的码率控制", "method": "基于VQ超先验的可控生成式图像压缩框架，严格推导了VQ索引熵模型引入超先验的数学基础，通过新颖的损失设计，配合轻量级超先验估计网络", "result": "在Kodak数据集上，与Control-GIC、CDC和HiFiC相比，在相同LPIPS质量下平均节省61.3%的比特率，在率失真性能上显著优于当前最先进的生成式压缩方法", "conclusion": "HVQ-CGIC有潜力成为VQGAN基图像压缩的基础组件，类似于超先验框架在神经图像压缩中的关键作用，首次在VQ基生成式图像压缩中实现了率失真平衡与控制"}}
{"id": "2512.07191", "pdf": "https://arxiv.org/pdf/2512.07191", "abs": "https://arxiv.org/abs/2512.07191", "authors": ["Wenqi Zhao", "Jiacheng Sang", "Fenghua Cheng", "Yonglu Shu", "Dong Li", "Xiaofeng Yang"], "title": "RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.", "AI": {"tldr": "提出RefLSM模型，将Retinex反射率分解整合到水平集分割框架中，用于医学图像分割和偏置场校正", "motivation": "医学图像分割面临强度不均匀、噪声、边界模糊和不规则结构等挑战，传统水平集方法在严重非均匀成像条件下表现不佳", "method": "1) 将观察图像分解为反射率和偏置场分量，直接分割反射率；2) 引入线性结构先验引导平滑反射率梯度；3) 嵌入松弛二元水平集，通过凸松弛和符号投影实现稳定演化", "result": "在多个医学影像数据集上的实验表明，RefLSM相比最先进的水平集方法具有更高的分割精度、鲁棒性和计算效率", "conclusion": "RefLSM通过反射率分解和结构先验，有效解决了医学图像分割中的强度不均匀问题，为复杂医学影像分析提供了可靠解决方案"}}
{"id": "2512.07190", "pdf": "https://arxiv.org/pdf/2512.07190", "abs": "https://arxiv.org/abs/2512.07190", "authors": ["Pengfei Gu", "Huimin Li", "Haoteng Tang", "Dongkuan", "Xu", "Erik Enriquez", "DongChul Kim", "Bin Fu", "Danny Z. Chen"], "title": "Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.", "AI": {"tldr": "提出一种新的拓扑引导医学图像分类框架，通过提取多尺度和多过滤的持久拓扑特征，并将其集成到视觉分类主干网络中，以增强对复杂解剖结构的识别能力。", "motivation": "现有深度神经网络在医学图像分类中要么强调像素强度特征而非基本解剖结构（如拓扑不变量编码的结构），要么仅通过单参数持久性捕获简单的拓扑特征，缺乏对多尺度、多过滤拓扑特征的全面利用。", "method": "首先计算输入图像在多个分辨率/尺度下的立方持久图（PDs），然后开发\"vineyard\"算法将这些PDs整合为单个稳定的图，捕获从全局解剖到局部细微异常的多粒度特征。设计基于交叉注意力的神经网络直接处理整合后的PDs，并将生成的拓扑嵌入与CNN或Transformer的特征图融合。", "result": "在三个公共数据集上的评估显示，该方法相对于强基线和最先进方法取得了一致且显著的改进，证明了综合拓扑视角对于稳健和可解释的医学图像分类的价值。", "conclusion": "通过将多尺度和多过滤拓扑特征集成到端到端架构中，该方法增强了模型识别复杂解剖结构的能力，为医学图像分类提供了更全面和稳健的拓扑视角。"}}
{"id": "2512.07186", "pdf": "https://arxiv.org/pdf/2512.07186", "abs": "https://arxiv.org/abs/2512.07186", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "title": "START: Spatial and Textual Learning for Chart Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "WACV2026 Camera Ready", "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "AI": {"tldr": "提出START框架，通过空间和文本学习增强多模态大语言模型对图表的理解能力，包括图表元素定位和图表到代码生成两个核心任务。", "motivation": "图表理解在分析科学论文和技术报告等实际场景中至关重要。与自然图像不同，图表结合了结构化视觉布局（空间属性）和底层数据表示（文本属性），同时理解这两者对于精确、细粒度的图表推理是必需的。", "method": "1) 引入图表元素定位和图表到代码生成来增强MLLM对图表视觉布局和数据细节的理解；2) 提出START数据集，使用新颖的数据生成流程，先利用MLLM将真实图表图像转换为可执行图表代码，再通过LLM演化代码以确定图表元素位置；3) 提出图表空间理解基准CS-Bench来评估模型对图表空间结构的理解能力。", "result": "START在不同模型大小和基准测试上都相对于基础模型取得了持续的性能提升，并且明显超越了先前的最先进方法。", "conclusion": "通过结合空间和文本学习，START框架显著提升了多模态大语言模型对图表的理解能力，填补了现有方法在处理图表空间结构方面的不足，为全面的图表理解评估提供了新的基准。"}}
{"id": "2512.07179", "pdf": "https://arxiv.org/pdf/2512.07179", "abs": "https://arxiv.org/abs/2512.07179", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "15 pages, 5 figures, 17 tables. Preparing submission for EDM 2026 conference", "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "AI": {"tldr": "提出PICKT模型，一种实用的互联概念知识追踪方法，用于个性化学习中的知识状态跟踪", "motivation": "现有知识追踪模型存在输入数据格式受限、新学生/新问题的冷启动问题、以及实际服务环境中的稳定性不足等局限性", "method": "使用知识图谱结构化概念间关系，考虑问题和概念文本信息，提出能够有效处理多种输入数据类型的PICKT模型", "result": "在反映真实操作环境的实验中，模型表现出优异的性能和实用性，特别是在新学生注册和新问题添加两个核心冷启动挑战上显著优于现有模型", "conclusion": "PICKT模型为下一代智能辅导系统的实际实施提供了重要的理论和技术基础，增强了在真实产品环境中的适用性"}}
{"id": "2512.07178", "pdf": "https://arxiv.org/pdf/2512.07178", "abs": "https://arxiv.org/abs/2512.07178", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "AI": {"tldr": "提出ContextualSHAP工具包，通过集成大型语言模型（GPT）为SHAP解释生成上下文文本描述，提升可解释AI的用户友好性", "motivation": "SHAP虽然能有效可视化特征重要性，但缺乏对非技术背景用户的上下文解释，限制了其在关键领域的实际应用价值", "method": "开发Python包，将SHAP与OpenAI的GPT集成，通过用户定义的参数（特征别名、描述、背景信息）生成情境化文本解释", "result": "在医疗案例研究中，用户评估显示生成的解释比纯可视化输出更易理解和情境合适，初步验证了方法的有效性", "conclusion": "可视化与情境化文本结合能支持更用户友好和可信的模型解释，为可解释AI提供了新的改进方向"}}
{"id": "2512.07177", "pdf": "https://arxiv.org/pdf/2512.07177", "abs": "https://arxiv.org/abs/2512.07177", "authors": ["Fanjun Bu", "Melina Tsai", "Audrey Tjokro", "Tapomayukh Bhattacharjee", "Jorge Ortiz", "Wendy Ju"], "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction", "categories": ["cs.RO"], "comment": null, "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.", "AI": {"tldr": "该论文提出了一种利用视觉语言模型作为社交智能代理的方法，用于提升机器人在人机交互中的社交响应能力。", "motivation": "机器人在日常环境中需要决定何时以及是否与人互动，但这些决策往往依赖于难以显式建模的微妙非语言线索。通过分析真实场景中人们通过非语言行为传递互动意愿的方式，研究者希望开发更自然的社交机器人交互系统。", "method": "提出两阶段流水线：首先使用轻量级感知检测器（注视转移和空间距离）识别社交意义时刻，然后在这些关键时刻触发基于视频的视觉语言模型查询。比较了两种提示策略。", "result": "在回放的现场交互数据上评估表明，选择性使用VLM作为社交推理代理能够实现社交响应式机器人行为，使机器人能够通过关注人们在真实交互中自然提供的线索来采取适当行动。", "conclusion": "通过将轻量级感知检测与重量的视觉语言模型查询相结合，可以在关键时刻利用VLM作为社交智能代理，使机器人能够更自然地响应人们在真实世界交互中提供的非语言线索。"}}
{"id": "2512.07171", "pdf": "https://arxiv.org/pdf/2512.07171", "abs": "https://arxiv.org/abs/2512.07171", "authors": ["Shravan Venkatraman", "Rakesh Raj Madavan", "Pavan Kumar S", "Muthu Subash Kavitha"], "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration", "categories": ["cs.CV"], "comment": "21 pages, 11 figures, 5 tables", "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.", "AI": {"tldr": "提出TIDE框架，通过两阶段逆退化估计和引导先验解耦进行水下图像恢复，专门处理空间变化的复杂水下退化问题", "motivation": "现有水下图像恢复方法通常对整个图像应用统一的恢复策略，难以处理空间变化且同时发生的多种退化问题，如颜色失真、雾霾、细节丢失和噪声", "method": "采用两阶段框架：第一阶段将水下退化分解为四个关键因素（颜色失真、雾霾、细节丢失、噪声），设计专门的恢复专家；第二阶段基于局部退化模式自适应融合多个专门假设，并进行渐进细化以纠正残留伪影", "result": "在标准基准和挑战性浑浊水条件下，TIDE在基于参考的保真度指标上具有竞争力，在无参考感知质量指标上优于现有方法，在颜色校正和对比度增强方面有显著改进", "conclusion": "TIDE通过明确建模退化特征和专门先验分解，能够有效处理空间变化的水下退化问题，在复杂水下条件下实现更自然和高质量的图像恢复"}}
{"id": "2512.07170", "pdf": "https://arxiv.org/pdf/2512.07170", "abs": "https://arxiv.org/abs/2512.07170", "authors": ["Jiayang Li", "Chengjie Jiang", "Junjun Jiang", "Pengwei Liang", "Jiayi Ma", "Liqiang Nie"], "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.", "AI": {"tldr": "提出DiTFuse框架，这是一个基于扩散变换器的统一语义可控图像融合方法，能够通过自然语言指令控制融合过程，统一处理多种图像融合任务。", "motivation": "现有图像融合方法在鲁棒性、适应性和可控性方面存在局限，大多针对特定任务设计，缺乏灵活融入用户意图的能力，特别是在低光退化、色彩偏移或曝光不平衡等复杂场景中。同时，缺乏真实融合图像作为ground truth以及现有数据集规模较小，使得训练同时理解高层语义和执行细粒度多模态对齐的端到端模型变得困难。", "method": "提出DiTFuse框架，采用基于扩散变换器（DiT）的指令驱动架构。通过在多退化掩码图像建模策略下联合训练，在共享潜在空间中同时编码两幅图像和自然语言指令，使网络能够联合学习跨模态对齐、模态不变恢复和任务感知特征选择，无需依赖ground truth图像。构建了多粒度指令数据集以增强模型的交互融合能力。", "result": "在公开的IVIF、MFF和MEF基准测试中显示出优越的定量和定性性能，具有更清晰的纹理和更好的语义保留。模型支持多级用户控制，并能够零样本泛化到其他多图像融合场景，包括指令条件分割。", "conclusion": "DiTFuse框架成功实现了统一的语义可控图像融合，克服了传统方法的局限性，通过自然语言指令实现了对融合过程的层次化和细粒度控制，为图像融合领域提供了新的端到端解决方案。"}}
{"id": "2512.07168", "pdf": "https://arxiv.org/pdf/2512.07168", "abs": "https://arxiv.org/abs/2512.07168", "authors": ["Georgios Ioannides", "Christos Constantinou", "Aman Chadha", "Aaron Elkins", "Linsey Pang", "Ravid Shwartz-Ziv", "Yann LeCun"], "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "UniReps: Unifying Representations in Neural Models (NeurIPS 2025 Workshop)", "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.", "AI": {"tldr": "提出一个两阶段自监督框架，结合JEPA和密度自适应注意力机制，用于学习鲁棒的语音表示，实现高效语音标记化和高质量波形重建。", "motivation": "现有神经音频编解码器在压缩效率和语言模型友好性方面存在不足，需要一种能够学习鲁棒语音表示并实现高效标记化的方法。", "method": "两阶段框架：第一阶段使用JEPA与密度自适应注意力机制在潜在空间进行掩码预测学习语义音频特征；第二阶段利用这些表示通过有限标量量化和混合基数打包进行高效标记化，最后使用HiFi-GAN解码器重建高保真波形。", "result": "模型以2.5Hz的低帧率发现层次化语音结构，生成47.5标记/秒的可逆、高度压缩且语言模型友好的表示，在效率和性能上优于现有神经音频编解码器。", "conclusion": "该框架成功地将JEPA作为神经标记器，结合密度自适应注意力机制，实现了鲁棒语音表示学习和高效语音压缩，为语音处理和生成任务提供了有前景的解决方案。"}}
{"id": "2512.07166", "pdf": "https://arxiv.org/pdf/2512.07166", "abs": "https://arxiv.org/abs/2512.07166", "authors": ["Siyuan Xu", "Yibing Liu", "Peilin Chen", "Yung-Hui Li", "Shiqi Wang", "Sam Kwong"], "title": "When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing", "categories": ["cs.CV"], "comment": "9 pages,7figures", "summary": "Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.", "AI": {"tldr": "该论文研究了多模态大语言模型（MLLM）编辑中的隐私保护与恢复问题，提出了一种在保护隐私的同时能够可靠恢复原始隐私内容的方法。", "motivation": "现有研究虽然能有效模糊MLLM中的隐私信息，但往往忽略了用户隐私的真实性和恢复质量的评估。论文关注如何在多样化的MLLM场景中恢复代理驱动的受保护数据这一关键挑战。", "method": "1) 创建SPPE数据集，包含多种隐私类别和用户指令，模拟真实MLLM应用场景；2) 将隐私恢复建模为基于互补多模态信号的引导生成任务；3) 提出统一方法，在保持MLLM生成编辑保真度的同时可靠重建隐私内容。", "result": "在SPPE和InstructPix2Pix数据集上的实验表明，该方法能够很好地泛化到不同的视觉内容和编辑任务，在隐私保护和MLLM可用性之间实现了良好的平衡。", "conclusion": "该工作填补了MLLM隐私保护研究中隐私恢复质量评估的空白，提出的方法能够在保护隐私的同时确保原始内容的可恢复性，为MLLM的隐私保护提供了更全面的解决方案。"}}
{"id": "2512.07165", "pdf": "https://arxiv.org/pdf/2512.07165", "abs": "https://arxiv.org/abs/2512.07165", "authors": ["Muyu Xu", "Fangneng Zhan", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.", "AI": {"tldr": "提出MuSASplat框架，通过轻量级多尺度适配器实现稀疏视图3D高斯泼溅的高效训练，显著降低计算成本同时保持渲染质量", "motivation": "现有稀疏视图3D高斯泼溅方法依赖预训练3D先验，但需要完全微调大型ViT主干网络，导致GPU成本过高。需要一种既能保持渲染质量又能大幅降低计算负担的解决方案。", "method": "1. 轻量级多尺度适配器：仅微调少量训练参数，避免全模型适配的高昂GPU开销；2. 特征融合聚合器：有效整合多视图特征，替代内存密集型的内存库方法，降低内存使用和计算复杂度", "result": "在多个数据集上的实验表明，MuSASplat实现了最先进的渲染质量，同时相比现有方法显著减少了参数数量和训练资源需求", "conclusion": "MuSASplat通过创新的轻量级适配器和特征融合机制，在稀疏视图3D高斯泼溅任务中实现了高质量渲染与高效计算的平衡，为实际应用提供了可行的解决方案"}}
{"id": "2512.07155", "pdf": "https://arxiv.org/pdf/2512.07155", "abs": "https://arxiv.org/abs/2512.07155", "authors": ["Dahyeon Kye", "Jeahun Sung", "MinKyu Jeon", "Jihyong Oh"], "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics", "categories": ["cs.CV"], "comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/", "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.", "AI": {"tldr": "提出CHIMERA框架，通过自适应缓存注入和语义锚点提示实现零样本图像变形，并引入面向变形的评估指标GLCS", "motivation": "扩散模型虽然具有强大的生成能力，但在实现平滑且语义一致的图像变形方面仍面临挑战。现有方法由于缺乏自适应结构和语义对齐，往往产生突兀的过渡或过饱和的外观。", "method": "1. 将变形定义为缓存反转引导的去噪过程；2. 自适应缓存注入(ACI)：在DDIM反转期间缓存输入图像的特征，并在去噪过程中自适应重新注入；3. 语义锚点提示(SAP)：利用视觉语言模型生成共享锚点提示作为语义桥梁；4. 引入全局-局部一致性评分(GLCS)作为变形导向的评估指标。", "result": "广泛的实验和用户研究表明，CHIMERA比现有方法实现了更平滑、更语义对齐的过渡，在图像变形领域建立了新的最先进水平。", "conclusion": "CHIMERA框架通过创新的自适应缓存注入和语义锚点提示机制，成功解决了扩散模型在图像变形中的平滑过渡和语义一致性问题，为图像变形任务提供了有效的零样本解决方案。"}}
{"id": "2512.07150", "pdf": "https://arxiv.org/pdf/2512.07150", "abs": "https://arxiv.org/abs/2512.07150", "authors": ["Jonghyun Park", "Jong Chul Ye"], "title": "FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.", "AI": {"tldr": "FlowLPS是一个基于预训练流模型解决逆问题的训练免费框架，采用朗之万-邻近采样策略，在潜在空间中实现后验模式收敛并避免流形偏差。", "motivation": "现有基于深度生成模型解决逆问题的方法在应用于潜在流模型时，常常无法收敛到后验模式或在潜在空间中遭受流形偏差，需要一种能同时保证重建保真度和感知质量的方法。", "method": "提出FlowLPS框架，结合朗之万动力学进行流形一致探索和邻近优化进行精确模式搜索，通过朗之万-邻近采样策略在预训练流模型的潜在空间中解决逆问题。", "result": "在FFHQ和DIV2K数据集上的多个逆任务中，FlowLPS在重建保真度和感知质量之间取得了优越的平衡，超越了现有最先进的逆问题求解器。", "conclusion": "FlowLPS通过朗之万-邻近采样策略有效解决了潜在流模型在逆问题求解中的收敛和流形偏差问题，为基于流模型的逆问题求解提供了有效的训练免费框架。"}}
{"id": "2512.07143", "pdf": "https://arxiv.org/pdf/2512.07143", "abs": "https://arxiv.org/abs/2512.07143", "authors": ["Yun Dai", "Sichen Lai"], "title": "A Theoretical Framework of Student Agency in AI- Assisted Learning: A Grounded Theory Approach", "categories": ["cs.HC"], "comment": "ef:Oxford Intersections: Social Media in Society and Culture, 2025", "summary": "Generative AI(GenAI) is a kind of AI model capable of producing human-like content in various modalities, including text, image, audio, video, and computer programming. Although GenAI offers great potential for education, its value often depends on students' ability to engage with it actively, responsibly, and critically - qualities central to student agency. Nevertheless, student agency has long been a complex and ambiguous concept in educational discourses, with few empirical studies clarifying its distinct nature and process in AI-assisted learning environments. To address this gap, the qualitative study presented in this article examines how higher education students exercise agency in AI-assisted learning and proposes a theoretical framework using a grounded theory approach. Guided by agentic engagement theory, this article analyzes the authentic experiences of 26 students using data from their GenAI conversation records and cognitive interviews that capture their thought processes and decision-making. The findings identify four key aspects of student agency: initiating and (re)directing, mindful adoption, external help-seeking, and reflective learning. Together, these aspects form an empirically developed framework that characterizes student agency in AI-assisted learning as a proactive, intentional, adaptive, reflective, and iterative process. Based on the empirical findings, theoretical and practical implications are discussed for researchers, educators, and policymakers.", "AI": {"tldr": "提出一个理论框架，通过扎根理论方法分析学生在AI辅助学习中的能动性，识别能动性的四个关键方面", "motivation": "生成式AI在教育中具有巨大潜力，但其价值取决于学生主动、负责和批判性地参与的能力，而学生能动性在教育话语中长期是一个复杂模糊的概念，缺乏在AI辅助学习环境中对其本质和过程的实证研究", "method": "采用扎根理论方法的定性研究，基于能动性参与理论，分析26名高等教育学生的真实经验，数据包括他们的生成式AI对话记录和捕捉其思维过程和决策的认知访谈", "result": "识别出学生能动性的四个关键方面：发起与（重新）导向、有意识采纳、外部求助和反思学习，这些方面共同形成了一个经验性框架，将AI辅助学习中的学生能动性描述为主动、有意识、适应性、反思性和迭代的过程", "conclusion": "提出了一个理论框架，阐明了学生在AI辅助学习中的能动性本质和过程，为研究人员、教育工作者和政策制定者提供了理论和实践启示"}}
{"id": "2512.07142", "pdf": "https://arxiv.org/pdf/2512.07142", "abs": "https://arxiv.org/abs/2512.07142", "authors": ["Tanay Arora", "Christof Teuscher"], "title": "Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "comment": "This work plans to be submitted to the IEEE for possible publication", "summary": "The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.", "AI": {"tldr": "本文提出Concrete Ticket Search (CTS)算法，通过将子网络发现构建为组合优化问题，高效识别高性能稀疏子网络，解决现有彩票票证搜索方法在计算效率和精度之间的权衡问题。", "motivation": "现有彩票票证搜索方法存在显著问题：Lottery Ticket Rewinding (LTR)计算成本过高，而基于显著性的初始化剪枝(PaI)方法在精度和稀疏度之间存在严重权衡，且无法通过基本合理性检查。作者认为PaI依赖忽略权重间依赖关系的一阶显著性度量是性能差距的主要原因。", "method": "提出Concrete Ticket Search (CTS)算法：1) 将子网络发现构建为组合优化问题；2) 使用Concrete松弛处理离散搜索空间；3) 引入GRADBALANCE梯度平衡方案控制稀疏度；4) 提出基于知识蒸馏的剪枝目标，特别是最小化稀疏和密集网络输出之间的反向KL散度(CTS-KL)。", "result": "在图像分类任务上的实验表明：CTS生成的子网络能稳健通过合理性检查，精度达到或超过LTR，同时仅需少量计算。例如在CIFAR10上的ResNet-20，CTS在7.9分钟内达到99.3%稀疏度和74.0%精度，而LTR需要95.2分钟达到68.3%精度。CTS在所有稀疏度上都优于基于显著性的方法，在高度稀疏区域对LTR的优势最为明显。", "conclusion": "CTS通过将彩票票证搜索构建为组合优化问题，并利用Concrete松弛和梯度平衡技术，成功解决了现有方法在计算效率和精度之间的权衡，为高效发现高性能稀疏子网络提供了新途径。"}}
{"id": "2512.07141", "pdf": "https://arxiv.org/pdf/2512.07141", "abs": "https://arxiv.org/abs/2512.07141", "authors": ["Fenghua Weng", "Chaochao Lu", "Xia Hu", "Wenqi Shao", "Wenjie Wang"], "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.", "AI": {"tldr": "提出Think-Reflect-Revise（TRR）三阶段训练框架，通过策略引导的自我反思增强大型视觉语言模型的安全对齐能力", "motivation": "现有单次推理的安全导向方法虽然提高了安全意识和可解释性，但仍容易受到上下文或视觉越狱攻击，关键缺陷在于单次推理可能忽略自身输出中的显式有害内容", "method": "1. 构建包含5000个示例的Reflective Safety Reasoning（ReSafe）数据集，遵循think-reflect-revise流程；2. 使用ReSafe数据集微调目标模型以初始化反思行为；3. 通过强化学习强化策略引导的反思", "result": "TRR显著提升了LVLMs在安全意识和越狱攻击评估中的安全性能，将Qwen2.5-VL-7B的整体安全响应率从42.8%提高到87.7%，同时在MMMU和MMStar等通用基准上保持稳定性能", "conclusion": "提出的TRR框架通过策略引导的自我反思有效增强了LVLMs的安全对齐能力，解决了单次推理方法的局限性，实现了真正的自我修正并防止不安全生成"}}
{"id": "2512.07137", "pdf": "https://arxiv.org/pdf/2512.07137", "abs": "https://arxiv.org/abs/2512.07137", "authors": ["Kang Yijie", "Hao Yuqing", "Wang Qingyun", "Chen Guanrong"], "title": "Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework", "categories": ["cs.RO", "cs.MA"], "comment": "10 pages,9 figures", "summary": "In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.", "AI": {"tldr": "研究轮式移动机器人在区域约束下的时变编队跟踪控制问题，采用广义Udwadia-Kalaba框架设计控制器", "motivation": "现有时变编队跟踪控制研究未考虑区域约束，无法保证机器人的安全性，需要设计能在特定区域内完成编队跟踪的控制策略", "method": "将时变编队跟踪控制目标重构为约束方程，通过微分同胚变换处理区域约束，在广义Udwadia-Kalaba框架下设计控制器，通信拓扑为有向加权且以领导者为根节点的生成树结构", "result": "成功设计了考虑区域约束的时变编队跟踪控制器，通过数值仿真验证了所提控制策略的有效性", "conclusion": "提出的广义Udwadia-Kalaba框架能够有效解决轮式移动机器人在区域约束下的时变编队跟踪控制问题，确保了机器人的安全性"}}
{"id": "2512.07136", "pdf": "https://arxiv.org/pdf/2512.07136", "abs": "https://arxiv.org/abs/2512.07136", "authors": ["Siyang Jiang", "Mu Yuan", "Xiang Ji", "Bufang Yang", "Zeyu Liu", "Lilin Xu", "Yang Li", "Yuting He", "Liran Dong", "Wenrui Lu", "Zhenyu Yan", "Xiaofan Jiang", "Wei Gao", "Hongkai Chen", "Guoliang Xing"], "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.", "AI": {"tldr": "提出了一个大规模多模态数据集CUHK-X，用于人类活动场景理解和推理，包含动作识别、理解和推理三个层次的任务。", "motivation": "当前多模态人类动作识别主要依赖RGB数据，而大型语言模型在处理深度、IMU、毫米波等非RGB模态时缺乏大规模数据-描述资源。现有数据集主要提供粗糙的数据标签注释，不足以捕捉动作动态的细粒度特征，限制了人类动作理解和推理任务的发展。", "method": "提出了CUHK-X数据集，包含58,445个样本，覆盖40种动作、30名参与者和两个室内环境。采用基于提示的场景创建方法，利用LLM生成逻辑连贯的活动序列，然后进行人工验证，确保数据描述的一致性和准确性。", "result": "实验结果显示，在CUHK-X数据集上的平均准确率为：动作识别76.52%，动作理解40.76%，动作推理70.25%。数据集包含三个基准测试和六个评估任务。", "conclusion": "CUHK-X为社区提供了一个大规模多模态数据集和基准套件，支持数据密集型学习方法的发展，促进鲁棒的多模态人类活动分析，特别是在动作理解和推理方面的研究。"}}
{"id": "2512.07135", "pdf": "https://arxiv.org/pdf/2512.07135", "abs": "https://arxiv.org/abs/2512.07135", "authors": ["Zebin Xing", "Pengxuan Yang", "Linbo Wang", "Yichen Zhang", "Yiming Hu", "Yupeng Zheng", "Junli Wang", "Yinfeng Gao", "Guang Li", "Kun Ma", "Long Chen", "Zhongpu Xia", "Qichao Zhang", "Hangjun Ye", "Dongbin Zhao"], "title": "TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.", "AI": {"tldr": "TrajMoE提出了一种基于混合专家和强化学习的场景自适应轨迹规划方法，通过MoE为不同驾驶场景提供合适的轨迹先验，并使用RL微调轨迹评分机制，在navsim ICCV基准测试中获得第三名。", "motivation": "当前自动驾驶系统通常采用端到端框架，将传感器输入映射到轨迹空间。现有方法虽然通过轨迹先验分布提升了规划性能，但存在两个关键问题：1) 合适的轨迹先验在不同驾驶场景中差异很大；2) 轨迹评估机制缺乏策略驱动的优化，受限于单阶段监督训练的局限性。", "method": "采用混合专家(MoE)为不同驾驶场景提供定制的轨迹先验分布；使用强化学习(RL)微调轨迹评分机制；集成不同感知骨干网络以增强感知特征。", "result": "在navsim ICCV基准测试中获得了51.08分，排名第三。", "conclusion": "TrajMoE通过场景自适应的轨迹先验和策略驱动的评分优化，有效解决了现有自动驾驶轨迹规划方法的局限性，在基准测试中取得了优异表现。"}}
{"id": "2512.07132", "pdf": "https://arxiv.org/pdf/2512.07132", "abs": "https://arxiv.org/abs/2512.07132", "authors": ["Nithin Sivakumaran", "Justin Chih-Yao Chen", "David Wan", "Yue Zhang", "Jaehong Yoon", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Code: https://github.com/nsivaku/dart", "summary": "Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.", "AI": {"tldr": "提出DART多智能体框架，利用视觉智能体之间的分歧来识别和调用有用的视觉工具，以解决多模态推理中的工具选择问题", "motivation": "专用视觉工具可以增强大语言模型或视觉语言模型的专家知识，但如何知道何时调用哪些工具具有挑战性。现有方法在工具选择和调用时机方面存在困难。", "method": "DART框架使用多个辩论视觉智能体之间的分歧来识别有用的视觉工具（如目标检测、OCR、空间推理等），这些工具通过引入新信息和提供工具对齐的同意分数来促进多智能体讨论，最终由聚合智能体选择最佳答案", "result": "在四个不同基准测试中，DART优于多智能体辩论和单智能体工具调用框架，在A-OKVQA和MMMU上分别比最强基线（带法官模型的多智能体辩论）提高3.4%和2.4%，在M3D医疗数据集上比其他基线提高1.3%。文本重叠度分析显示DART具有更丰富的讨论，工具调用分布显示多样工具被可靠使用", "conclusion": "DART框架通过利用多智能体分歧来指导工具调用，有效解决了多模态推理中的工具选择问题，提高了推理性能并促进了更丰富的讨论，能够很好地适应新工具和应用领域"}}
{"id": "2512.07130", "pdf": "https://arxiv.org/pdf/2512.07130", "abs": "https://arxiv.org/abs/2512.07130", "authors": ["Zebin Xing", "Yupeng Zheng", "Qichao Zhang", "Zhixing Ding", "Pengxuan Yang", "Songen Gu", "Zhongpu Xia", "Dongbin Zhao"], "title": "Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving", "AI": {"tldr": "提出Mimir框架，一种用于端到端自动驾驶的分层双系统架构，通过不确定性传播和分层目标驱动扩散来生成鲁棒轨迹", "motivation": "现有端到端自动驾驶方法受限于高层引导信号不准确和复杂引导模块的计算开销，需要更鲁棒且高效的解决方案", "method": "采用分层双系统框架：1) 使用拉普拉斯分布估计目标点不确定性增强鲁棒性；2) 引入多速率引导机制提前预测扩展目标点提升推理速度", "result": "在Navhard和Navtest基准测试中，驾驶评分EPDMS提升20%，高层模块推理速度提升1.6倍且不损失精度", "conclusion": "Mimir通过不确定性传播和多速率引导机制有效解决了端到端自动驾驶中高层引导不准确和计算效率低的问题，实现了性能与效率的双重提升"}}
{"id": "2512.07128", "pdf": "https://arxiv.org/pdf/2512.07128", "abs": "https://arxiv.org/abs/2512.07128", "authors": ["Chau Truong", "Hieu Ta Quang", "Dung D. Le"], "title": "MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.", "AI": {"tldr": "提出MulCLIP框架，通过多级对齐机制增强CLIP模型处理细粒度长文本描述的能力", "motivation": "CLIP等视觉语言模型在短文本描述上表现良好，但在处理冗长详细描述时存在困难。现有方法使用区域建议信息但部署成本高，需要更高效的细粒度对齐方案", "method": "提出端到端多级对齐框架：1) 保持图像与摘要/长文本的全局对比对齐，扩展位置嵌入；2) 基于局部校准特征的token重建对齐；3) 子标题聚合的patch对齐，自动提取上下文丰富的图像块", "result": "在多个基准测试中一致提升下游性能，消融研究证实多尺度对齐是关键因素，比区域建议辅助方法具有更好的细粒度能力", "conclusion": "MulCLIP通过多级对齐框架有效增强了CLIP处理细粒度长文本的能力，更适合多样化的实际应用场景"}}
{"id": "2512.07126", "pdf": "https://arxiv.org/pdf/2512.07126", "abs": "https://arxiv.org/abs/2512.07126", "authors": ["Shengjie Lu", "Zhibin Wan", "Jiejie Liu", "Quan Zhang", "Mingjie Sun"], "title": "Training-free Clothing Region of Interest Self-correction for Virtual Try-On", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.", "AI": {"tldr": "提出一种无需训练的衣服感兴趣区域自校正方法，通过能量函数约束注意力机制，改善虚拟试穿中服装细节的保持", "motivation": "现有虚拟试穿方法存在生成结果与目标服装在图案、纹理和边界方面的差异，需要更好的服装细节保持机制", "method": "使用能量函数约束生成过程中的注意力图，使注意力更集中于服装感兴趣区域，从而更好地保持目标服装细节", "result": "在VITON-HD和DressCode数据集上，在LPIPS、FID、KID和新的VTID指标上分别超过SOTA方法1.4%、2.3%、12.3%和5.8%，并在下游CC-Reid任务中带来性能提升", "conclusion": "提出的无需训练的衣服区域自校正方法有效改善了虚拟试穿中服装细节的保持，并设计了新的评估指标VTID来更全面地评估生成质量"}}
{"id": "2512.07124", "pdf": "https://arxiv.org/pdf/2512.07124", "abs": "https://arxiv.org/abs/2512.07124", "authors": ["Hui Zhong", "Qing-Long Lu", "Qiming Zhang", "Hongliang Lu", "Xinhu Zheng"], "title": "Enhancing Urban Sensing Utility with Sensor-enabled Vehicles and Easily Accessible Data", "categories": ["cs.ET"], "comment": null, "summary": "Urban sensing is essential for the development of smart cities, enabling monitoring, computing, and decision-making for urban management.Thanks to the advent of vehicle technologies, modern vehicles are transforming from solely mobility tools to valuable sensors for urban data collection, and hold the potential of improving traffic congestion, transport sustainability, and infrastructure inspection.Vehicle-based sensing is increasingly recognized as a promising technology due to its flexibility, cost-effectiveness, and extensive spatiotemporal coverage. However, optimizing sensing strategies to balance spatial and temporal coverage, minimize redundancy, and address budget constraints remains a key challenge.This study proposes an adaptive framework for enhancing the sensing utility of sensor-equipped vehicles.By integrating heterogeneous open-source data, the framework leverages spatiotemporal weighting to optimize vehicle selection and sensing coverage across various urban contexts.An entropy-based vehicle selection strategy, \\texttt{Improved OptiFleet}, is developed to maximize sensing utility while minimizing redundancy.The framework is validated using real-world air quality data from 320 sensor-equipped vehicles operating in Guangzhou, China, over two months.Key findings show that the proposed method outperforms baseline strategies, providing up to 5\\% higher sensing utility with reduced fleet sizes, and also highlights the critical role of dynamic urban data in optimizing mobile sensing strategies.", "AI": {"tldr": "提出一个自适应框架，通过集成异构开源数据和时空加权优化，增强传感器车辆的城市感知效用，最大化覆盖范围同时最小化冗余", "motivation": "城市感知对智慧城市发展至关重要，车辆技术发展使现代车辆从单纯的移动工具转变为有价值的城市数据收集传感器，但优化感知策略以平衡时空覆盖、最小化冗余并解决预算限制仍是关键挑战", "method": "提出自适应框架，集成异构开源数据，利用时空加权优化车辆选择和感知覆盖；开发基于熵的车辆选择策略Improved OptiFleet，最大化感知效用同时最小化冗余", "result": "使用广州320辆传感器车辆两个月真实空气质量数据进行验证，结果显示该方法优于基线策略，提供高达5%的感知效用提升，同时减少车队规模，并突显动态城市数据在优化移动感知策略中的关键作用", "conclusion": "提出的自适应框架能有效增强传感器车辆的城市感知效用，通过优化车辆选择和时空覆盖，为智慧城市管理提供更高效的数据收集解决方案"}}
{"id": "2512.07122", "pdf": "https://arxiv.org/pdf/2512.07122", "abs": "https://arxiv.org/abs/2512.07122", "authors": ["Liping Han", "Tingting Nie", "Le Yu", "Mingzhe Hu", "Tao Yue"], "title": "RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.", "AI": {"tldr": "提出一种基于大语言模型(LLM)的实时修复方法RisConFix，用于修复导致无人机飞行不稳定的风险配置参数", "motivation": "无人机飞行控制软件包含大量可配置参数，即使按照推荐值设置，某些参数组合仍可能导致不稳定飞行行为，降低无人机鲁棒性", "method": "RisConFix持续监控无人机运行状态，检测到异常飞行行为时自动触发修复机制。利用LLM分析配置参数与飞行状态的关系，生成纠正性参数更新，采用迭代过程确保配置有效性", "result": "在ArduPilot案例研究中（包含1,421组错误配置），RisConFix实现了最佳修复成功率97%，最优平均修复次数1.17次", "conclusion": "RisConFix能够实时、有效且高效地修复风险配置参数，提高无人机飞行稳定性和鲁棒性"}}
{"id": "2512.07120", "pdf": "https://arxiv.org/pdf/2512.07120", "abs": "https://arxiv.org/abs/2512.07120", "authors": ["J. Allagan", "G. Morgan", "S. Langley", "R. Lopez-Bonilla", "V. Deriglazov"], "title": "Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications", "categories": ["cs.DS", "cs.LG"], "comment": "18 pages", "summary": "We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.", "AI": {"tldr": "本文为2-树建立了在双色三角形约束下的色度特征向量的闭式枚举公式，这些特征向量源自约束图着色问题，其中每个三角形必须恰好使用两种颜色，禁止单色和彩虹三角形。", "motivation": "该约束源于分布式系统中组件需要避免完全集中或完全隔离的情况。传统的色多项式为所有n顶点2-树分配相同的特征，而双色约束能够提供更具信息量的结构特征，用于捕获网络中的有意义结构属性。", "method": "针对theta图Θ_n和扇形图Φ_n建立了精确的闭式枚举公式。对于Θ_n，证明了r_k(Θ_n) = S(n-2, k-1)（第二类斯特林数），r_2(Θ_n) = 2^(n-2) + 1，可在O(n)时间内计算。对于Φ_n，建立了r_2(Φ_n) = F_{n+1}（斐波那契数），并推导了r_k(Φ_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1)的显式公式，其中包含可高效计算的二项式系数，每个分量的计算复杂度为O(n^2)。", "result": "获得了可高效计算的结构特征，这些特征与斐波那契多项式、贝尔数和独立集枚举相关联。虽然这些特征不是完整的图不变量，但它们能够捕获有意义的结构属性，为网络分析提供了新的工具。", "conclusion": "双色三角形约束下的色度特征向量为2-树提供了信息丰富的结构特征，这些特征在传统色多项式无法区分的情况下能够区分不同的图结构。这些结果在拜占庭容错分层网络、云计算中的虚拟机分配和分布式密码学中的秘密共享协议等方面具有实际应用价值。"}}
{"id": "2512.07117", "pdf": "https://arxiv.org/pdf/2512.07117", "abs": "https://arxiv.org/abs/2512.07117", "authors": ["Yun Dai"], "title": "Human Agency and Creativity in AI-Assisted Learning Environments", "categories": ["cs.HC"], "comment": "ef:Oxford Handbook of Human Creativity x Generative AI in Education, 2026", "summary": "This chapter explores human creativity in AI-assisted learning environments through the lens of student agency. We begin by examining four theoretical perspectives on agency, including instrumental, effortful, dynamically emergent, and authorial agency, and analyze how each frames the relationship between agency and creativity. Under each theoretical perspective, we discuss how the integration of generative AI (GenAI) tools reshapes these dynamics by altering students' roles in cognitive, social, and creative processes. In the second part, we introduce a theoretical framework for AI agentic engagement, contextualizing agency within specific cognitive, relational, and ethical dynamics introduced by GenAI tools. This framework is linked to the concept of Mini-c creativity, emphasizing personal relevance and self-directed learning. Together, these perspectives support a shift from viewing creativity as product-oriented to understanding it as a process of agentive participation and meaning-making. We conclude with two directions for future research focused on the creative process and performance in AI-assisted learning.", "AI": {"tldr": "探讨人工智能辅助学习环境中人类创造力与主体性的关系，提出AI主体参与理论框架", "motivation": "随着生成式AI工具在教育中的应用日益广泛，需要重新审视学生在AI辅助学习环境中的主体性角色及其对创造力的影响，理解AI如何重塑认知、社交和创造过程", "method": "首先分析四种主体性理论视角（工具性、努力性、动态涌现性、作者性），探讨每种视角下主体性与创造力的关系；然后提出AI主体参与理论框架，将主体性置于GenAI工具引入的认知、关系和伦理动态中；最后将框架与Mini-c创造力概念联系起来", "result": "建立了AI主体参与的理论框架，强调主体性在特定认知、关系和伦理动态中的表现；将创造力从产品导向转向过程导向，强调主体参与和意义建构；提出了未来研究的两个方向：AI辅助学习中的创造过程和创造表现", "conclusion": "AI辅助学习环境需要重新概念化学生主体性与创造力的关系，从产品导向的创造力观转向过程导向的主体参与和意义建构观，为未来研究提供了理论框架和方向"}}
{"id": "2512.07114", "pdf": "https://arxiv.org/pdf/2512.07114", "abs": "https://arxiv.org/abs/2512.07114", "authors": ["Jue Wang", "Mingsong Jiang", "Luis A. Ramirez", "Bilige Yang", "Mujun Zhang", "Esteban Figueroa", "Wenzhong Yan", "Rebecca Kramer-Bottiglio"], "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.", "AI": {"tldr": "提出一种替代柔顺性建模方法，通过在刚体模拟器中引入代表软材料变形的间接变量，实现软体机器人的强化学习步态控制", "motivation": "软体机器人具有形态适应能力，但软体模拟器精度和计算效率有限，刚体模拟器无法捕捉软材料动力学，需要一种能在刚体模拟器中有效模拟软体行为的方法", "method": "采用替代柔顺性建模方法：不显式模拟软体物理，而是在刚体模拟器中引入代表软材料变形的间接变量（有效肢体长度和肢体质心变化），通过强化学习并广泛随机化这些间接变量进行策略学习", "result": "在刚体模拟中实现了可靠的策略学习，步态直接转移到硬件，在硬质平坦基底上表现出高保真度的仿真到现实性能，在流变复杂地形上转移稳健但保真度较低；学习的闭环步态展现出前所未有的地面机动性，运输成本比开环基线降低一个数量级", "conclusion": "替代柔顺性建模方法成功解决了软体机器人的模拟和控制挑战，实现了在刚体模拟器中训练的策略直接转移到真实软体机器人硬件，在多种自然地形上展示了稳定的多步态运动能力"}}
{"id": "2512.07112", "pdf": "https://arxiv.org/pdf/2512.07112", "abs": "https://arxiv.org/abs/2512.07112", "authors": ["Ziqing Wen", "Jiahuan Wang", "Ping Luo", "Dongsheng Li", "Tao Sun"], "title": "FOAM: Blocked State Folding for Memory-Efficient LLM Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\\%, eliminates up to 90\\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.", "AI": {"tldr": "提出FOAM方法，通过分块梯度均值和残差校正来压缩优化器状态，减少LLM训练中的内存占用", "motivation": "大型语言模型训练面临显著的内存瓶颈，特别是使用Adam等内存密集型优化器时。现有内存高效方法存在计算开销大、需要额外内存或性能下降等问题", "method": "FOAM方法通过计算分块梯度均值来压缩优化器状态，并引入残差校正来恢复丢失的信息，理论上在非凸优化设置下能达到与原始Adam相当的收敛率", "result": "FOAM减少约50%的总训练内存，消除高达90%的优化器状态内存开销，并加速收敛。与现有内存高效优化器兼容，性能匹配或超越全秩和现有基线", "conclusion": "FOAM是一种有效的内存高效优化方法，能在保持模型性能的同时显著减少训练内存需求，为大规模语言模型训练提供了实用的解决方案"}}
{"id": "2512.07110", "pdf": "https://arxiv.org/pdf/2512.07110", "abs": "https://arxiv.org/abs/2512.07110", "authors": ["Liangwei Jiang", "Jinluo Xie", "Yecheng Huang", "Hua Zhang", "Hongyu Yang", "Di Huang"], "title": "MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \\textbf{representation} and \\textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \\emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "提出一种名为多方向相似性网络（MSN）的双流模型，用于准确高效地检测手工制作和深度合成的复制-移动图像伪造", "motivation": "复制-移动图像伪造检测面临复杂变换和精细操作的挑战，现有深度检测模型在表示和定位方面存在主要限制", "method": "采用双流模型架构：1）表示方面：通过多方向CNN网络分层编码图像，利用尺度和旋转的多样化增强；2）定位方面：设计基于2-D相似性矩阵的解码器，充分利用整个图像的空间信息", "result": "在CASIA CMFD、CoMoFoD和新提出的深度合成伪造数据库上进行了广泛实验，报告了最先进的结果，证明了方法的有效性", "conclusion": "MSN模型能够有效解决现有深度检测模型在表示和定位方面的限制，为检测手工制作和深度合成的复制-移动伪造提供了有效解决方案"}}
{"id": "2512.07109", "pdf": "https://arxiv.org/pdf/2512.07109", "abs": "https://arxiv.org/abs/2512.07109", "authors": ["Miguel Ingram", "Arthur Joseph Merritt III"], "title": "A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "62 pages, 10 figures", "summary": "Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,", "AI": {"tldr": "该论文提出了一个神经亲和力框架来诊断Transformer架构在抽象推理任务中的组合性差距，通过程序化任务分类法分析ARC-AGI-2数据集中的任务组成和模型性能瓶颈。", "motivation": "响应Hodel等人(2024)对任务相关性形式化定义的需求，研究Transformer架构在抽象推理任务中的性能限制，特别是组合性差距问题，为架构改进提供诊断工具。", "method": "1) 开发首个400个任务的9类别分类法，通过基于规则的代码分析验证(97.5%准确率)；2) 使用CNN在原始网格像素上训练验证分类法的视觉一致性；3) 在302个任务上微调1.7M参数Transformer，分析局部模式与全局合成的性能差距；4) 将框架应用于独立研究ViTARC进行验证。", "result": "1) 发现35.3%任务对Transformer具有低神经亲和力；2) 揭示深刻的组合性差距：69.5%任务在局部模式(>80%单元格准确率)表现良好但全局合成(<10%网格准确率)失败；3) 验证框架预测能力：极低亲和力任务51.9%准确率 vs 高亲和力任务77.7%；4) 发现神经亲和力天花板效应，性能受架构适宜性而非训练数据限制。", "conclusion": "Transformer架构在抽象推理任务中存在固有的组合性差距，需要开发具有亲和力对齐模块的混合架构来突破性能瓶颈。分类法为诊断任务-架构匹配问题提供了精确工具，表明进展需要针对不同任务类型设计专门的架构组件。"}}
{"id": "2512.07107", "pdf": "https://arxiv.org/pdf/2512.07107", "abs": "https://arxiv.org/abs/2512.07107", "authors": ["Jaeyoon Lee", "Hojoon Jung", "Sungtae Hwang", "Jihyong Oh", "Jongwon Choi"], "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision", "categories": ["cs.CV"], "comment": "Project page: https://vilab-cau.github.io/COREA/", "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.", "AI": {"tldr": "COREA提出首个统一框架，联合学习可重照明的3D高斯和SDF，通过双向3D到3D监督实现精确几何重建和忠实重照明", "motivation": "现有3D高斯泼溅方法虽然扩展到网格重建和基于物理的渲染，但其几何仍从2D渲染学习，导致表面粗糙和BRDF-光照分解不可靠", "method": "引入粗到细双向3D到3D对齐策略：深度提供粗对齐，深度梯度和法线细化精细结构；密度控制机制稳定高斯增长；支持稳定BRDF-光照分解", "result": "在标准基准测试中，COREA在新视角合成、网格重建和基于物理的渲染方面实现优越性能，并在统一框架内完成", "conclusion": "COREA通过3D空间直接学习几何信号，解决了现有方法表面粗糙和BRDF-光照分解不可靠的问题，实现了精确几何重建和忠实重照明"}}
{"id": "2512.07094", "pdf": "https://arxiv.org/pdf/2512.07094", "abs": "https://arxiv.org/abs/2512.07094", "authors": ["Christopher Cruz"], "title": "VIGIL: A Reflective Runtime for Self-Healing Agents", "categories": ["cs.AI"], "comment": ":I.2.8", "summary": "Agentic LLM frameworks promise autonomous behavior via task decomposition, tool use, and iterative planning, but most deployed systems remain brittle. They lack runtime introspection, cannot diagnose their own failure modes, and do not improve over time without human intervention. In practice, many agent stacks degrade into decorated chains of LLM calls with no structural mechanisms for reliability. We present VIGIL (Verifiable Inspection and Guarded Iterative Learning), a reflective runtime that supervises a sibling agent and performs autonomous maintenance rather than task execution. VIGIL ingests behavioral logs, appraises each event into a structured emotional representation, maintains a persistent EmoBank with decay and contextual policies, and derives an RBT diagnosis that sorts recent behavior into strengths, opportunities, and failures. From this analysis, VIGIL generates both guarded prompt updates that preserve core identity semantics and read only code proposals produced by a strategy engine that operates on log evidence and code hotspots. VIGIL functions as a state gated pipeline. Illegal transitions produce explicit errors rather than allowing the LLM to improvise. In a reminder latency case study, VIGIL identified elevated lag, proposed prompt and code repairs, and when its own diagnostic tool failed due to a schema conflict, it surfaced the internal error, produced a fallback diagnosis, and emitted a repair plan. This demonstrates meta level self repair in a deployed agent runtime.", "AI": {"tldr": "VIGIL是一个用于自愈代理的反射运行时系统，它通过监督兄弟代理、分析行为日志、进行情感表征和诊断，实现自主维护和修复，而不是直接执行任务。", "motivation": "现有的代理LLM框架虽然承诺通过任务分解、工具使用和迭代规划实现自主行为，但大多数部署系统仍然脆弱。它们缺乏运行时内省能力，无法诊断自身的故障模式，并且在没有人工干预的情况下无法随时间改进。许多代理栈在实践中退化为装饰性的LLM调用链，缺乏可靠性的结构机制。", "method": "VIGIL采用反射运行时架构，监督兄弟代理并执行自主维护。系统包含：1）摄入行为日志；2）将事件评估为结构化情感表征；3）维护具有衰减和上下文策略的持久EmoBank；4）生成RBT诊断（优势、机会、失败）；5）生成保护性提示更新和只读代码提案；6）作为状态门控管道运行，非法转换产生显式错误。", "result": "在提醒延迟案例研究中，VIGIL成功识别了延迟升高问题，提出了提示和代码修复方案。当自身的诊断工具因模式冲突而失败时，系统能够暴露内部错误、生成备用诊断并发出修复计划，展示了在部署代理运行时中的元级自我修复能力。", "conclusion": "VIGIL通过引入反射运行时机制，实现了代理系统的自我诊断、自我修复和持续改进能力，解决了现有代理框架脆弱、缺乏内省和无法自主改进的问题，为构建更可靠的自主代理系统提供了新方法。"}}
{"id": "2512.07092", "pdf": "https://arxiv.org/pdf/2512.07092", "abs": "https://arxiv.org/abs/2512.07092", "authors": ["Zhixiang Wang"], "title": "The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 3 figures, 1 table. Code and dataset available at https://huggingface.co/Zx93/Soul-Engine-Qwen2.5-0.5B", "summary": "Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an \"alignment tax\" -- degrading general reasoning capabilities. Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights. Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for \"Zero-Shot Personality Injection\" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies. Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.", "AI": {"tldr": "提出Soul Engine框架，基于线性表示假设，将人格特征解耦为正交线性子空间，实现无需微调的大语言模型个性化", "motivation": "当前个性化大语言模型面临稳定性-可塑性困境，传统对齐方法（如SFT）通过随机权重更新会导致\"对齐税\"——降低通用推理能力，需要一种不修改主干权重的个性化方法", "method": "基于线性表示假设，提出Soul Engine框架，使用SoulBench数据集（通过动态上下文采样构建），在冻结的Qwen-2.5基础上采用双头架构提取解耦的人格向量", "result": "1) 高精度人格剖析：MSE达到0.011；2) 几何正交性：T-SNE可视化确认人格流形是离散且连续的，支持\"零样本人格注入\"；3) 确定性控制：通过向量算术实现稳健行为控制", "conclusion": "挑战了微调对个性化的必要性，从概率提示转向确定性潜在干预，为安全可控的AI个性化提供了数学严谨的基础"}}
{"id": "2512.07091", "pdf": "https://arxiv.org/pdf/2512.07091", "abs": "https://arxiv.org/abs/2512.07091", "authors": ["Tomoya Takahashi", "Yusaku Nakajima", "Cristian Camilo Beltran-Hernandez", "Yuki Kuroda", "Kazutoshi Tanaka", "Masashi Hamaya", "Kanta Ono", "Yoshitaka Ushiku"], "title": "A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling", "categories": ["cs.RO"], "comment": "9 pages, 8 figures", "summary": "Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.", "AI": {"tldr": "提出一种新型漏斗形柔性机械手，集成单板阀，用于毫克级粉末精确称量处理", "motivation": "实验室自动化在固态材料发现中具有巨大潜力，但毫克级粉末处理自动化面临挑战，主要由于粉末流动动力学复杂且实验室任务多样", "method": "设计漏斗形柔性机械手，在锥顶集成可控阀门；结合外部天平通过反馈控制系统，基于粉末流动模型和在线参数识别", "result": "使用玻璃珠、谷氨酸钠和二氧化钛进行实验评估，80%的试验误差在2mg内，最大误差约20mg（目标范围20mg-3g）；相比直接PID控制，模型控制显著提高精度和收敛速度", "conclusion": "该系统展示了高效灵活的粉末称量潜力，具有向更大规模扩展的能力，适用于广泛的实验室自动化任务"}}
{"id": "2512.07090", "pdf": "https://arxiv.org/pdf/2512.07090", "abs": "https://arxiv.org/abs/2512.07090", "authors": ["Jungmin Lee", "Gwangeun Byeon", "Yulhwa Kim", "Seokin Hong"], "title": "Leveraging KV Similarity for Online Structured Pruning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B/13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.", "AI": {"tldr": "提出Token Filtering技术，一种轻量级的在线结构化剪枝方法，通过KV相似性在推理过程中直接进行剪枝决策，无需校准数据", "motivation": "现有LLM剪枝方法依赖离线校准数据，在不同输入上泛化性差且不稳定，需要一种无需校准数据、在推理过程中直接进行剪枝决策的稳定方法", "method": "基于联合KV相似性度量token冗余度，跳过冗余注意力计算；设计方差感知融合策略，自适应加权不同注意力头的KV相似性，确保高剪枝率下保留信息丰富的token", "result": "在LLaMA-2(7B/13B)、LLaMA-3(8B)和Mistral(7B)上实验表明，Token Filtering优于现有结构化剪枝方法，在常识推理基准上保持准确性，在MMLU等挑战性任务上即使50%剪枝仍保持强性能", "conclusion": "Token Filtering提供了一种无需校准数据的在线结构化剪枝方案，通过KV相似性度量token冗余，结合方差感知融合策略实现稳定高效的LLM推理加速"}}
{"id": "2512.07086", "pdf": "https://arxiv.org/pdf/2512.07086", "abs": "https://arxiv.org/abs/2512.07086", "authors": ["Yunzhe Li", "Jianan Wang", "Hongzi Zhu", "James Lin", "Shan Chang", "Minyi Guo"], "title": "ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "This version includes the final camera-ready manuscript accepted by NDSS 2026", "summary": "Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.", "AI": {"tldr": "提出ThinkTrap攻击框架，针对黑盒LLM服务进行拒绝服务攻击，通过优化输入使模型陷入无限思考循环", "motivation": "随着LLM作为云服务部署，攻击者可能通过精心设计的输入使模型进入过长或无限生成循环，耗尽计算资源，而现有黑盒设置难以防御此类攻击", "method": "将离散token映射到连续嵌入空间，在低维稀疏子空间进行高效黑盒优化，寻找能诱导扩展或非终止生成的对抗性提示", "result": "在多个商业闭源LLM服务上评估，即使在严格请求频率限制下（通常10 RPM），攻击可将服务吞吐量降至原始的1%，甚至导致完全服务故障", "conclusion": "黑盒LLM服务存在通过无限思考进行DoS攻击的严重漏洞，需要新的防御机制来保护云部署的LLM服务"}}
{"id": "2512.07081", "pdf": "https://arxiv.org/pdf/2512.07081", "abs": "https://arxiv.org/abs/2512.07081", "authors": ["Rongjia Zhou", "Chengzhuo Li", "Carl Yang", "Jiaying Lu"], "title": "ClinNoteAgents: An LLM Multi-Agent System for Predicting and Interpreting Heart Failure 30-Day Readmission from Clinical Notes", "categories": ["cs.AI"], "comment": "10 pages, 2 figures. Submitted to AMIA 2026 Informatics Summit Student Paper Track", "summary": "Heart failure (HF) is one of the leading causes of rehospitalization among older adults in the United States. Although clinical notes contain rich, detailed patient information and make up a large portion of electronic health records (EHRs), they remain underutilized for HF readmission risk analysis. Traditional computational models for HF readmission often rely on expert-crafted rules, medical thesauri, and ontologies to interpret clinical notes, which are typically written under time pressure and may contain misspellings, abbreviations, and domain-specific jargon. We present ClinNoteAgents, an LLM-based multi-agent framework that transforms free-text clinical notes into (1) structured representations of clinical and social risk factors for association analysis and (2) clinician-style abstractions for HF 30-day readmission prediction. We evaluate ClinNoteAgents on 3,544 notes from 2,065 patients (readmission rate=35.16%), demonstrating strong performance in extracting risk factors from free-text, identifying key contributing factors, and predicting readmission risk. By reducing reliance on structured fields and minimizing manual annotation and model training, ClinNoteAgents provides a scalable and interpretable approach to note-based HF readmission risk modeling in data-limited healthcare systems.", "AI": {"tldr": "开发一个基于LLM的多智能体系统，用于从临床笔记中预测和解释心力衰竭30天再入院风险", "motivation": "心力衰竭是美国老年人再住院的主要原因之一，临床笔记包含丰富的患者信息，但在心力衰竭再入院风险分析中未得到充分利用。传统模型依赖专家规则和医学词典，难以处理临床笔记中的拼写错误、缩写和领域特定术语。", "method": "提出ClinNoteAgents，一个基于LLM的多智能体框架，将自由文本临床笔记转换为：(1) 临床和社会风险因素的结构化表示用于关联分析；(2) 临床医生风格的抽象表示用于30天再入院预测。", "result": "在3,544份笔记（2,065名患者，再入院率35.16%）上评估，系统在提取风险因素、识别关键贡献因素和预测再入院风险方面表现出色。通过减少对结构化字段的依赖，最小化手动标注和模型训练，提供了可扩展且可解释的方法。", "conclusion": "ClinNoteAgents为数据有限的医疗系统提供了一种可扩展、可解释的基于临床笔记的心力衰竭再入院风险建模方法，能够有效利用非结构化临床笔记进行风险预测和因素分析。"}}
{"id": "2512.07079", "pdf": "https://arxiv.org/pdf/2512.07079", "abs": "https://arxiv.org/abs/2512.07079", "authors": ["Anton Morgunov", "Victor S. Batista"], "title": "Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages + 7 pages of SI. RetroCast is available on GitHub, see https://github.com/ischemist/project-procrustes. SynthArena is publicly available, see https://syntharena.ischemist.com/", "summary": "Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between \"solvability\" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a \"complexity cliff\" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.", "AI": {"tldr": "提出RetroCast统一评估框架，用于标准化AI驱动逆合成分析模型的评估，解决现有评估方法缺乏标准化和过度关注拓扑完成度而忽视化学有效性的问题。", "motivation": "当前计算机辅助合成规划（CASP）领域进展受到缺乏标准化评估基础设施的阻碍，现有评估指标过于关注拓扑完成度而忽视化学有效性，导致不同模型之间难以进行公平比较。", "method": "开发RetroCast统一评估套件，将异构模型输出标准化为通用模式；建立可重复的基准测试流程，包括分层抽样和自举置信区间；创建SynthArena交互式平台用于定性路线检查；使用该框架评估领先的基于搜索和基于序列的算法。", "result": "分析发现\"可解性\"（库存终止率）与路线质量之间存在分歧：高可解性分数往往掩盖化学无效性或与实验真实情况不相关；识别出\"复杂性悬崖\"现象，即基于搜索的方法在重建长程合成计划时性能急剧下降。", "conclusion": "RetroCast框架为AI驱动逆合成分析提供了标准化、可重复的评估基础设施，揭示了现有评估指标的局限性，并发布了完整框架、基准定义和标准化预测数据库以支持该领域的透明和可重复发展。"}}
{"id": "2512.07078", "pdf": "https://arxiv.org/pdf/2512.07078", "abs": "https://arxiv.org/abs/2512.07078", "authors": ["Bo Gao", "Jingcheng Tong", "Xingsheng Chen", "Han Yu", "Zichen Li"], "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages", "summary": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily. We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency. We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.", "AI": {"tldr": "提出DFIR-DETR模型，通过频域增强和动态特征聚合解决跨场景小目标检测问题，在无人机遥感图像和工业缺陷检测中实现高效检测", "motivation": "无人机遥感图像和工业缺陷检测中的小目标检测面临共同挑战：特征稀疏且弱、背景杂乱、目标尺度变化大。现有基于Transformer的检测器存在三个关键问题：网络下采样导致特征严重退化、空间卷积无法有效捕获长距离依赖、标准上采样方法导致特征图不必要膨胀", "method": "提出三个核心模块：1) DCFA模块使用动态K稀疏注意力将复杂度从O(N²)降至O(NK)，并采用空间门控线性单元增强非线性建模；2) DFPN模块应用幅度归一化上采样防止特征膨胀，使用双路径混洗卷积保留跨尺度空间细节；3) FIRC3模块在频域操作，实现全局感受野而不牺牲效率", "result": "在NEU-DET和VisDrone数据集上分别达到92.9%和51.6%的mAP50分数，均为当前最优性能。模型保持轻量级，仅11.7M参数和41.2 GFLOPs，在两个差异很大的领域都表现出色", "conclusion": "DFIR-DETR通过频域增强和动态特征聚合有效解决了跨场景小目标检测问题，在资源受限环境中表现出良好的泛化能力和实际应用价值"}}
{"id": "2512.07076", "pdf": "https://arxiv.org/pdf/2512.07076", "abs": "https://arxiv.org/abs/2512.07076", "authors": ["Chen-Yang Wang", "Gepeng Ji", "Song Shao", "Ming-Ming Cheng", "Deng-Ping Fan"], "title": "Context-measure: Contextualizing Metric for Camouflage", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.", "AI": {"tldr": "提出一种新的上下文感知评估指标Context-measure，用于评估伪装物体分割任务，通过考虑空间依赖性和像素级伪装量化来更好地对齐人类感知", "motivation": "当前伪装场景评估指标忽略了上下文依赖这一关键因素，这些指标原本是为评估一般或显著物体设计的，假设空间上下文不相关，无法准确评估伪装效果", "method": "基于概率像素感知相关框架构建上下文化评估范式，通过融入空间依赖关系和像素级伪装量化来评估伪装物体分割效果", "result": "在三个具有挑战性的伪装物体分割数据集上的广泛实验表明，Context-measure比现有的上下文无关指标提供更可靠的评估结果", "conclusion": "Context-measure能够为涉及伪装模式的各种计算机视觉应用（如农业、工业和医疗场景）提供基础评估基准，代码已开源"}}
{"id": "2512.07065", "pdf": "https://arxiv.org/pdf/2512.07065", "abs": "https://arxiv.org/abs/2512.07065", "authors": ["Anil Chintapalli", "Peter Tenholder", "Henry Chen", "Arjun Rao"], "title": "Persistent Homology-Guided Frequency Filtering for Image Compression", "categories": ["cs.CV"], "comment": "17 pages, 8 figures, code available at github.com/RMATH3/persistent-homology-compression", "summary": "Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.", "AI": {"tldr": "使用持久同调引导的频率滤波进行图像压缩，通过拓扑特征分析提取关键频率信息", "motivation": "在噪声图像数据集中进行特征提取面临模型可靠性挑战，需要一种能够区分有意义数据并保持压缩效率的方法", "method": "结合离散傅里叶变换和持久同调分析，提取与图像拓扑特征对应的特定频率，实现图像压缩与重建", "result": "实验结果显示压缩水平与JPEG相当（使用六种不同指标评估），在增强卷积神经网络时可能提升二分类任务性能", "conclusion": "持久同调引导的频率滤波方法能够提高噪声条件下图像压缩的可靠性，为传统特征提取和压缩方法提供改进方案"}}
{"id": "2512.07064", "pdf": "https://arxiv.org/pdf/2512.07064", "abs": "https://arxiv.org/abs/2512.07064", "authors": ["Jiannan Yang", "Veronika Thost", "Tengfei Ma"], "title": "Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.", "AI": {"tldr": "本文系统研究了分子图自监督学习中的掩码设计，通过统一概率框架分析掩码策略的有效性，发现对于常见节点级预测任务，复杂掩码分布相比均匀采样并无优势，而预测目标选择和编码器架构的协同更为关键。", "motivation": "分子表示学习中自监督学习扮演核心角色，但许多基于掩码的预训练方法缺乏原则性评估，设计选择的有效性不明确。需要系统研究掩码策略以提供指导。", "method": "将预训练-微调流程纳入统一概率框架，在受控设置下研究三个核心设计维度：掩码分布、预测目标和编码器架构，并使用信息论度量评估预训练信号的信息量。", "result": "研究发现：1) 对于常见节点级预测任务，复杂掩码分布相比均匀采样无一致优势；2) 预测目标选择和编码器架构的协同更为关键；3) 转向语义更丰富的预测目标能带来显著下游改进，特别是与表达能力强的图Transformer编码器结合时。", "conclusion": "研究为开发更有效的分子图自监督学习方法提供了实用指导：应更关注预测目标的语义丰富性和编码器架构的匹配，而非过度优化掩码分布策略。"}}
{"id": "2512.07062", "pdf": "https://arxiv.org/pdf/2512.07062", "abs": "https://arxiv.org/abs/2512.07062", "authors": ["Changliang Xia", "Chengyou Jia", "Minnan Luo", "Zhuohang Dang", "Xin Shen", "Bowen Ping"], "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.", "AI": {"tldr": "提出D³-Predictor：一种无噪声的确定性扩散框架，用于密集预测任务，通过消除扩散采样中的随机噪声来改善几何结构映射", "motivation": "现有基于扩散模型的密集预测方法存在核心限制：扩散采样的随机噪声与密集预测所需的确定性图像到几何映射不匹配，这种噪声会破坏细粒度空间线索和几何结构映射", "method": "将预训练扩散模型重新表述为无随机噪声的确定性框架，将扩散网络视为时间步相关的视觉专家集合，通过自监督方式聚合异构先验为单一、干净、完整的几何先验，并利用任务特定监督进行适配", "result": "在多种密集预测任务上达到竞争性或最先进的性能，所需训练数据不到先前方法的一半，且推理只需单步完成", "conclusion": "D³-Predictor通过消除扩散模型中的随机噪声，成功构建了适用于密集预测任务的确定性映射框架，在保持扩散先验优势的同时显著提升了效率和性能"}}
{"id": "2512.07052", "pdf": "https://arxiv.org/pdf/2512.07052", "abs": "https://arxiv.org/abs/2512.07052", "authors": ["Hoang-Nhat Tran", "Francesco Di Sario", "Gabriele Spadaro", "Giuseppe Valenzise", "Enzo Tartaglione"], "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.", "AI": {"tldr": "提出RAVE方法，为3D高斯泼溅技术提供速率自适应的视觉编码方案，支持在预定义范围内的任意速率插值压缩", "motivation": "3D高斯泼溅技术虽然能实现实时逼真渲染，但存在内存需求大、训练成本高的问题。现有压缩方法只能在固定速率下工作，无法适应不同带宽和设备限制", "method": "提出灵活的3DGS压缩方案，支持在预定义范围内的任意速率插值，计算轻量，无需为不同速率重新训练，能保持广泛的渲染质量", "result": "实验表明该方法实现了高效、高质量的压缩，同时提供动态速率控制，适合沉浸式应用的实际部署", "conclusion": "RAVE方法解决了3DGS压缩中的速率适应性问题，为沉浸式多媒体应用提供了实用的压缩解决方案，代码将开源"}}
{"id": "2512.07051", "pdf": "https://arxiv.org/pdf/2512.07051", "abs": "https://arxiv.org/abs/2512.07051", "authors": ["Adnan Munir", "Shujaat Khan"], "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 7 figures", "summary": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.", "AI": {"tldr": "DAUNet是一个轻量级的UNet变体，集成了可变形卷积V2和无参数注意力机制SimAM，用于医学图像分割，在保持参数效率的同时提高了空间适应性和上下文感知特征融合能力。", "motivation": "医学图像分割在自动化诊断和治疗规划系统中至关重要。现有模型在处理几何变化、低对比度区域和缺失上下文时存在局限性，同时需要在资源受限的临床环境中保持轻量化和实时性。", "method": "1. 使用可变形V2卷积增强瓶颈层，处理几何变化；2. 在解码器和跳跃连接路径中集成SimAM无参数注意力模块，实现显著性感知的特征细化；3. 保持轻量级架构，不增加模型复杂度。", "result": "在两个具有挑战性的数据集（FH-PS-AoP超声图像和FUMPE CT肺栓塞检测）上评估，DAUNet在Dice分数、HD95和ASD指标上优于最先进模型，同时保持优越的参数效率。消融研究验证了可变形卷积和SimAM注意力的各自贡献。", "conclusion": "DAUNet通过集成可变形卷积和SimAM注意力，在保持轻量化的同时提高了医学图像分割的准确性和鲁棒性，特别适合处理几何变化、低对比度区域和缺失上下文的情况，适用于实时和资源受限的临床环境部署。"}}
{"id": "2512.07041", "pdf": "https://arxiv.org/pdf/2512.07041", "abs": "https://arxiv.org/abs/2512.07041", "authors": ["Hiroki Sawada", "Alexandre Pitti", "Mathias Quoy"], "title": "CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation", "categories": ["cs.RO"], "comment": null, "summary": "Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.", "AI": {"tldr": "提出CERNet：一个基于类别嵌入预测编码循环神经网络的统一模型，实现机器人运动生成、行为识别和置信度估计三大功能", "motivation": "机器人需要同时生成实时运动、推断人类行为意图并评估自身推断的置信度，现有方法通常将这些功能分离，需要一个统一的框架", "method": "使用分层预测编码循环神经网络（PC-RNN）结合动态更新的类别嵌入向量，通过两种模式运作：生成模式中嵌入向量约束隐藏状态到类别特定子空间；推断模式中在线优化嵌入向量以最小化预测误差", "result": "在人形机器人上验证26个字母手势，轨迹复制误差比单层基线降低76%，在线识别达到68% Top-1和81% Top-2准确率，内部预测误差自然反映模型置信度", "conclusion": "CERNet在紧凑的PC-RNN框架内整合了鲁棒运动生成、实时识别和内在不确定性估计，为物理机器人运动记忆提供了可扩展方法，适用于意图敏感的人机协作"}}
{"id": "2512.07040", "pdf": "https://arxiv.org/pdf/2512.07040", "abs": "https://arxiv.org/abs/2512.07040", "authors": ["Sakib Mostafa", "Lei Xing", "Md. Tauhidul Islam"], "title": "Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.", "AI": {"tldr": "将大型生物网络转换为二维图像，通过语义制图实现可视化解释和可扩展的深度学习分析", "motivation": "生物网络在生物医学中至关重要，但现有分析方法面临可扩展性有限、长程依赖关系捕捉困难、多模态整合困难、表达性受限和可解释性差等挑战", "method": "提出Graph2Image框架，将大型生物网络通过语义制图转换为二维图像，在2D网格上空间排列代表性网络节点，使节点解耦为图像，从而能够使用具有全局感受野和多尺度金字塔的卷积神经网络", "result": "在多个大规模生物网络数据集上，Graph2Image比现有方法提高了高达67.2%的分类准确率，提供了可解释的可视化，揭示了生物学上一致的模式，并能够在个人计算机上分析超过10亿节点的超大网络", "conclusion": "Graph2Image为生物网络分析提供了一个可扩展、可解释且支持多模态的方法，为疾病诊断和复杂生物系统研究提供了新的机会"}}
{"id": "2512.07037", "pdf": "https://arxiv.org/pdf/2512.07037", "abs": "https://arxiv.org/abs/2512.07037", "authors": ["Josep M. Rocafort", "Shaolin Su", "Javier Vazquez-Corral", "Alexandra Gomez-Villa"], "title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "AI": {"tldr": "该论文提出了评估超分辨率模型高层保真度的重要性，构建了首个带标注的高层保真度数据集，并展示了如何利用该标准改进超分辨率模型。", "motivation": "当前超分辨率模型虽然能生成视觉上令人愉悦的图像，但强大的生成能力有时会产生幻觉，改变图像内容。这种高层级的变化容易被人类识别，但现有的低层级图像质量指标未能很好地衡量这一问题。", "method": "构建首个带高层保真度评分标注的数据集，评估SOTA超分辨率模型的保真度表现；分析现有图像质量指标与保真度测量的相关性；展示基础模型能更好地处理高层级任务；通过基于保真度反馈微调超分辨率模型。", "result": "建立了高层保真度评估标准，发现现有图像质量指标与保真度测量相关性不足；基础模型在高层级任务上表现更好；通过微调可以同时提高语义保真度和感知质量。", "conclusion": "高层保真度是评估超分辨率模型可靠性的重要补充标准，提出的评估标准和数据集对模型评估和优化具有潜在价值，能够帮助改进超分辨率模型的语义保真度。"}}
{"id": "2512.07034", "pdf": "https://arxiv.org/pdf/2512.07034", "abs": "https://arxiv.org/abs/2512.07034", "authors": ["Tuan-Anh Vu", "Hai Nguyen-Truong", "Ziqiang Zheng", "Binh-Son Hua", "Qing Guo", "Ivor Tsang", "Sai-Kit Yeung"], "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV 2026", "summary": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.", "AI": {"tldr": "提出TransCues框架，通过边界特征增强和反射特征增强模块，结合金字塔视觉Transformer进行透明物体分割", "motivation": "玻璃等透明物体在日常生活中普遍存在，但由于其透明性和反射特性，现有分割方法难以将其与不透明材料区分开来。人类感知依赖于边界和反射物体特征来识别玻璃物体，但现有文献在处理透明物体时未能充分捕捉这两种特性", "method": "提出TransCues框架，采用金字塔Transformer编码器-解码器架构，包含边界特征增强模块和反射特征增强模块，以相互促进的方式整合这两种强大的视觉线索", "result": "在多个基准数据集上显著超越现有方法：Trans10K-v2上提升4.2% mIoU，MSD上提升5.6% mIoU，RGBD-Mirror上提升10.1% mIoU，TROSD上提升13.1% mIoU，Stanford2D3D上提升8.3% mIoU", "conclusion": "通过边界特征增强和反射特征增强模块的有效结合，TransCues框架能够显著提升透明物体分割性能，证明了这两种视觉线索在识别玻璃物体中的重要性"}}
{"id": "2512.07032", "pdf": "https://arxiv.org/pdf/2512.07032", "abs": "https://arxiv.org/abs/2512.07032", "authors": ["Runcong Wang", "Fengyi Wang", "Gordon Cheng"], "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.", "AI": {"tldr": "本文提出了一种用于移动机械臂的异质关联顺序记忆系统，通过神经形态信号学习机器人关节状态与触觉观测之间的紧凑绑定，以低计算和内存成本实现逐步动作决策。", "motivation": "移动机械臂需要能够快速学习、低计算成本且具有泛化能力的控制系统。传统方法通常计算开销大，难以实现实时响应和触觉引导的行为。本文旨在开发一种基于神经形态信号的高效记忆系统，使机器人能够通过触觉输入学习并执行复杂动作序列。", "method": "方法包括：1) 使用群体位置编码将关节角度编码为神经形态信号；2) 使用Izhikevich神经元模型将皮肤测量的力转换为脉冲率特征；3) 将两种信号转换为双极二进制向量并进行元素级绑定；4) 引入3D旋转位置嵌入，根据感知力方向旋转子空间以增强二进制空间的可分离性；5) 通过softmax加权召回在时间偏移动作模式上进行模糊检索。", "result": "在丰田人机支持机器人上验证，系统实现了伪合规控制器，使连杆在触摸下沿施加力方向移动，速度与力幅值相关。系统能够通过持续触觉输入检索多关节抓取序列，快速设置，从同步状态和观测流中训练，并表现出一定程度的泛化能力，同时保持经济性。", "conclusion": "该异质关联顺序记忆系统成功实现了通过关联召回执行单关节和全臂行为，证明了其在移动机械臂控制中的有效性。该方法可扩展到模仿学习、运动规划和多模态集成等应用领域，为机器人学习提供了一种高效、低成本的神经形态解决方案。"}}
{"id": "2512.07030", "pdf": "https://arxiv.org/pdf/2512.07030", "abs": "https://arxiv.org/abs/2512.07030", "authors": ["Zahra Lotfi", "Mostafa Lotfi"], "title": "A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "13 pages, 5 figures", "summary": "Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.", "AI": {"tldr": "评估五种监督机器学习模型在零日攻击检测中的性能，特别关注不平衡数据集下的表现，提出结合网格搜索、降维和过采样的框架来提升模型效果。", "motivation": "零日攻击由于模式未知，传统安全系统难以检测。现有监督机器学习模型在训练阶段学习已知攻击模式，对未见过的零日攻击检测效率低下，且实际网络数据通常高度不平衡，需要研究如何提升模型在不平衡数据上的零日攻击检测性能。", "method": "1. 评估五种监督模型（包括随机森林和XGBoost）在零日攻击检测中的性能和执行时间；2. 提出包含网格搜索、降维和过采样方法的框架来处理数据不平衡问题；3. 使用高度不平衡数据集，在测试阶段仅暴露零日攻击，确保模型未在训练中见过这些攻击；4. 比较过采样对模型指标（特别是准确率）的影响。", "result": "随机森林在过采样和非过采样条件下都表现最佳，但处理时间较长；XGBoost在检测零日攻击方面表现出快速且高准确率的性能，被选为最优模型；过采样方法有效提升了模型在不平衡数据上的性能。", "conclusion": "XGBoost是检测零日攻击的最佳模型选择，因为它兼顾了高准确率和快速处理时间。提出的框架通过网格搜索、降维和过采样有效解决了数据不平衡问题，提升了监督模型在零日攻击检测中的性能。"}}
{"id": "2512.07022", "pdf": "https://arxiv.org/pdf/2512.07022", "abs": "https://arxiv.org/abs/2512.07022", "authors": ["Genevieve Caumartin", "Glaucia Melo"], "title": "Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization", "categories": ["cs.SE", "cs.AI", "cs.IR"], "comment": "Accepted at BoatSE 2026", "summary": "Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.", "AI": {"tldr": "该论文提出了一种基于大语言模型的智能体方法，通过查询重构、检索和定位三个步骤来改进仓库级别的bug定位性能。", "motivation": "传统基于信息检索的bug定位方法依赖未改变的bug描述，这些描述通常包含噪声信息，导致检索准确性差。虽然大语言模型通过查询重构改进了bug定位，但智能体性能的影响尚未被探索。", "method": "首先使用开源、未经微调的大语言模型从bug报告中提取关键信息（如标识符和代码片段），并进行检索前的查询重构。然后智能体使用这些预处理后的查询来编排BM25检索，实现大规模自动化的定位工作流。", "result": "使用最佳查询重构技术，智能体在首次文件检索中的排名比BM25基线提高了35%，文件检索性能比SWE-agent提高了22%。", "conclusion": "基于大语言模型的智能体通过轻量级查询重构和总结，能够显著改进仓库级别的bug定位性能，为大规模软件仓库的bug定位提供了有效的自动化解决方案。"}}
{"id": "2512.07021", "pdf": "https://arxiv.org/pdf/2512.07021", "abs": "https://arxiv.org/abs/2512.07021", "authors": ["Jose Geraldo Fernandes", "Luiz Facury de Souza", "Pedro Robles Dutenhefner", "Gisele L. Pappa", "Wagner Meira Jr"], "title": "Transferring Clinical Knowledge into ECGs Representation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \\emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.", "AI": {"tldr": "提出一种三阶段训练范式，将多模态临床知识（实验室检查、生命体征、生物特征）迁移到单模态ECG编码器中，以提升ECG分类的准确性和可解释性。", "motivation": "深度学习模型在ECG分类中准确率高，但黑盒特性阻碍了临床采用，缺乏信任和可解释性。需要创建既准确又可信的ECG分类模型。", "method": "三阶段训练范式：1）自监督联合嵌入预训练阶段，从多模态临床数据中学习ECG表示；2）训练ECG编码器仅使用ECG信号进行推理；3）通过预测相关实验室异常来间接解释模型输出。", "result": "在MIMIC-IV-ECG数据集上评估，模型在多标签诊断分类中优于标准信号基线，显著缩小了与需要所有数据推理的全多模态模型之间的性能差距。", "conclusion": "该方法为创建更准确、可信的ECG分类模型提供了实用有效的方法，通过将抽象预测转化为基于生理学的解释，为AI更安全地融入临床工作流程提供了有前景的路径。"}}
{"id": "2512.07019", "pdf": "https://arxiv.org/pdf/2512.07019", "abs": "https://arxiv.org/abs/2512.07019", "authors": ["Zhiyu Xu", "Jia Liu", "Yixin Wang", "Yuqi Gu"], "title": "Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length", "categories": ["stat.ME", "cs.AI", "stat.AP", "stat.ML"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.", "AI": {"tldr": "提出Latency-Response Theory (LaRT)模型，通过联合建模响应准确率和思维链长度来评估大型语言模型，相比传统IRT模型具有更好的评估效果。", "motivation": "现有基于IRT模型的评估方法只考虑响应准确率，而忽略了思维链长度这一重要推理能力指标。需要开发能够同时利用准确率和思维链长度信息的新评估框架。", "method": "提出LaRT模型，通过引入潜在能力和潜在速度之间的关键相关参数，联合建模响应准确率和思维链长度。开发了高效随机近似EM算法进行参数估计，并建立了严格的参数可识别性理论保证。", "result": "理论渐近分析和模拟研究显示LaRT相比IRT具有更优的估计精度和更短的置信区间。在实际数据评估中，LaRT在预测能力、项目效率、排名有效性和LLM评估效率等多个关键指标上优于IRT，并产生不同的LLM排名。", "conclusion": "LaRT模型通过有效利用思维链长度信息，为大型语言模型评估提供了更全面、更有效的框架，在理论和实际应用中都展现出优于传统IRT方法的性能。"}}
{"id": "2512.07015", "pdf": "https://arxiv.org/pdf/2512.07015", "abs": "https://arxiv.org/abs/2512.07015", "authors": ["Mayank Ravishankara"], "title": "FVA-RAG: Falsification-Verification Alignment for Mitigating Sycophantic Hallucinations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems have significantly reduced hallucinations in Large Language Models (LLMs) by grounding responses in external context. However, standard RAG architectures suffer from a critical vulnerability: Retrieval Sycophancy. When presented with a query based on a false premise or a common misconception, vector-based retrievers tend to fetch documents that align with the user's bias rather than objective truth, leading the model to \"hallucinate with citations.\" In this work, we introduce Falsification-Verification Alignment RAG (FVA-RAG), a framework that shifts the retrieval paradigm from Inductive Verification (seeking support) to Deductive Falsification (seeking disproof). Unlike existing \"Self-Correction\" methods that rely on internal consistency, FVA-RAG deploys a distinct Adversarial Retrieval Policy that actively generates \"Kill Queries\"-targeted search terms designed to surface contradictory evidence. We introduce a dual-verification mechanism that explicitly weighs the draft answer against this \"Anti-Context.\" Preliminary experiments on a dataset of common misconceptions demonstrate that FVA-RAG significantly improves robustness against sycophantic hallucinations compared to standard RAG baselines, effectively acting as an inference-time \"Red Team\" for factual generation.", "AI": {"tldr": "提出FVA-RAG框架，通过证伪-验证对齐机制来缓解检索增强生成系统中的\"检索谄媚\"问题，即当查询基于错误前提时，检索器倾向于获取符合用户偏见的文档而非客观事实。", "motivation": "标准RAG架构存在关键漏洞：检索谄媚。当查询基于错误前提或常见误解时，基于向量的检索器倾向于获取符合用户偏见的文档而非客观事实，导致模型\"带引用的幻觉\"。", "method": "提出FVA-RAG框架，将检索范式从归纳验证（寻求支持）转变为演绎证伪（寻求反证）。部署独特的对抗性检索策略，主动生成\"致命查询\"来获取矛盾证据，并引入双重验证机制，明确权衡草稿答案与\"反上下文\"。", "result": "在常见误解数据集上的初步实验表明，FVA-RAG相比标准RAG基线显著提高了对谄媚幻觉的鲁棒性，有效地充当了事实生成的推理时\"红队\"。", "conclusion": "FVA-RAG通过证伪-验证对齐机制成功缓解了RAG系统中的检索谄媚问题，提供了一种更稳健的事实生成方法，能够主动寻找矛盾证据来纠正基于错误前提的查询。"}}
{"id": "2512.07009", "pdf": "https://arxiv.org/pdf/2512.07009", "abs": "https://arxiv.org/abs/2512.07009", "authors": ["Saeid Ghafouri", "Yuming Ding", "Katerine Diaz Chito", "Jesús Martinez del Rincón", "Niamh O'Connell", "Hans Vandierendonck"], "title": "Optimizing video analytics inference pipelines: a case study", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "Accepted to the IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT 2025)", "summary": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.", "AI": {"tldr": "该论文通过系统级优化改进家禽福利监测系统的视频分析推理管道，包括多级并行化、GPU加速、向量化聚类和内存高效后处理等技术，在真实农场视频数据上实现了最高2倍的加速，同时保持模型准确性。", "motivation": "精准畜牧业监测需要高效、可扩展的视频分析系统，商业农场的高分辨率视频和近实时监测需求产生了巨大的计算工作负载，需要优化视频分析推理管道以降低成本并提高性能。", "method": "采用系统级优化方法，包括：1）多级并行化技术；2）将CPU代码替换为GPU加速代码；3）向量化聚类算法；4）内存高效后处理技术。这些优化应用于检测、跟踪、聚类和行为分析模块。", "result": "在真实农场视频数据上的评估显示，优化后的系统在整个推理管道上实现了最高2倍的加速，同时没有降低模型准确性。系统吞吐量提高，延迟降低，基础设施需求减少。", "conclusion": "该案例研究展示了构建高吞吐量、低延迟视频推理系统的实用策略，这些策略不仅适用于农业和智能传感部署，也可应用于其他大规模视频分析应用，显著减少基础设施需求。"}}
{"id": "2512.07005", "pdf": "https://arxiv.org/pdf/2512.07005", "abs": "https://arxiv.org/abs/2512.07005", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "title": "Multi-Accent Mandarin Dry-Vocal Singing Dataset: Benchmark for Singing Accent Recognition", "categories": ["cs.SD", "cs.AI"], "comment": "Accepted by ACMMM 2025", "summary": "Singing accent research is underexplored compared to speech accent studies, primarily due to the scarcity of suitable datasets. Existing singing datasets often suffer from detail loss, frequently resulting from the vocal-instrumental separation process. Additionally, they often lack regional accent annotations. To address this, we introduce the Multi-Accent Mandarin Dry-Vocal Singing Dataset (MADVSD). MADVSD comprises over 670 hours of dry vocal recordings from 4,206 native Mandarin speakers across nine distinct Chinese regions. In addition to each participant recording audio of three popular songs in their native accent, they also recorded phonetic exercises covering all Mandarin vowels and a full octave range. We validated MADVSD through benchmark experiments in singing accent recognition, demonstrating its utility for evaluating state-of-the-art speech models in singing contexts. Furthermore, we explored dialectal influences on singing accent and analyzed the role of vowels in accentual variations, leveraging MADVSD's unique phonetic exercises.", "AI": {"tldr": "该论文提出了一个多口音普通话干声歌唱数据集（MADVSD），用于歌唱口音识别研究，包含超过670小时的干声录音，覆盖9个中国地区的4206名母语者。", "motivation": "歌唱口音研究相比语音口音研究相对不足，主要原因是缺乏合适的数据集。现有歌唱数据集通常存在细节丢失（由于人声-乐器分离过程）且缺乏区域口音标注。", "method": "构建了MADVSD数据集，包含4206名母语者用本地口音录制三首流行歌曲的音频，以及覆盖所有普通话元音和一个完整八度音域的语音练习录音。", "result": "通过歌唱口音识别的基准实验验证了MADVSD的实用性，展示了其在评估最先进语音模型在歌唱场景中的应用价值，并探索了方言对歌唱口音的影响以及元音在口音变化中的作用。", "conclusion": "MADVSD填补了歌唱口音研究的数据集空白，为评估语音模型在歌唱场景中的表现提供了基准，并支持方言影响和元音作用的深入分析。"}}
{"id": "2512.07000", "pdf": "https://arxiv.org/pdf/2512.07000", "abs": "https://arxiv.org/abs/2512.07000", "authors": ["Abderaouf Bahi", "Ibtissem Gasmi"], "title": "Benchmarking Deep Neural Networks for Modern Recommendation Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "This paper examines the deployment of seven different neural network architectures CNN, RNN, GNN, Autoencoder, Transformer, NCF, and Siamese Networks on three distinct datasets: Retail E-commerce, Amazon Products, and Netflix Prize. It evaluates their effectiveness through metrics such as accuracy, recall, F1-score, and diversity in recommendations. The results demonstrate that GNNs are particularly adept at managing complex item relationships in e-commerce environments, whereas RNNs are effective in capturing the temporal dynamics that are essential for platforms such as Netflix.. Siamese Networks are emphasized for their contribution to the diversification of recommendations, particularly in retail settings. Despite their benefits, issues like computational demands, reliance on extensive data, and the challenge of balancing accurate and diverse recommendations are addressed. The study seeks to inform the advancement of recommendation systems by suggesting hybrid methods that merge the strengths of various models to better satisfy user preferences and accommodate the evolving demands of contemporary digital platforms.", "AI": {"tldr": "本文对七种神经网络架构在推荐系统中的性能进行了基准测试，评估了它们在电商、产品和视频平台数据集上的表现。", "motivation": "随着推荐系统在现代数字平台中的重要性日益增加，需要评估不同神经网络架构在各种推荐场景中的有效性，以指导实际部署和未来研究方向。", "method": "在三个不同数据集（零售电商、亚马逊产品、Netflix Prize）上部署七种神经网络架构（CNN、RNN、GNN、自编码器、Transformer、NCF、孪生网络），使用准确率、召回率、F1分数和推荐多样性等指标进行评估。", "result": "GNN在电商环境中处理复杂物品关系方面表现最佳，RNN在捕捉Netflix等平台的时序动态方面最有效，孪生网络在零售环境中对推荐多样性贡献最大。所有模型都面临计算需求大、数据依赖性强以及准确性与多样性平衡的挑战。", "conclusion": "研究建议采用混合方法，结合不同模型的优势来更好地满足用户偏好和适应现代数字平台的需求，为推荐系统的未来发展提供了重要指导。"}}
{"id": "2512.06999", "pdf": "https://arxiv.org/pdf/2512.06999", "abs": "https://arxiv.org/abs/2512.06999", "authors": ["Zihao Wang", "Ruibin Yuan", "Ziqi Geng", "Hengjia Li", "Xingwei Qu", "Xinyi Li", "Songye Chen", "Haoying Fu", "Roger B. Dannenberg", "Kejun Zhang"], "title": "Singing Timbre Popularity Assessment Based on Multimodal Large Foundation Model", "categories": ["cs.SD", "cs.AI"], "comment": "Accepted to ACMMM 2025 oral", "summary": "Automated singing assessment is crucial for education and entertainment. However, existing systems face two fundamental limitations: reliance on reference tracks, which stifles creative expression, and the simplification of complex performances into non-diagnostic scores based solely on pitch and rhythm. We advocate for a shift from discriminative to descriptive evaluation, creating a complete ecosystem for reference-free, multi-dimensional assessment. First, we introduce Sing-MD, a large-scale dataset annotated by experts across four dimensions: breath control, timbre quality, emotional expression, and vocal technique. Our analysis reveals significant annotation inconsistencies among experts, challenging the validity of traditional accuracy-based metrics. Second, addressing the memory limitations of Multimodal Large Language Models (MLLMs) in analyzing full-length songs, we propose VocalVerse. This efficient hybrid architecture leverages a lightweight acoustic encoder to model global performance features and long-term dependencies. Third, to address automated metric shortcomings, we establish the H-TPR (Human-in-the-loop Tiered Perceptual Ranking) benchmark, which evaluates a model's ability to generate perceptually valid rankings rather than predicting noisy ground-truth scores.", "AI": {"tldr": "提出基于多模态大基础模型的歌唱音色流行度评估系统，解决现有歌唱评估系统的局限性", "motivation": "现有歌唱评估系统存在两个根本性限制：依赖参考音轨（限制创造性表达）以及将复杂表演简化为仅基于音高和节奏的非诊断性分数。需要从判别性评估转向描述性评估", "method": "1) 引入Sing-MD大规模数据集，包含专家标注的四个维度；2) 提出VocalVerse高效混合架构，使用轻量级声学编码器建模全局特征和长期依赖；3) 建立H-TPR基准，评估模型生成感知有效排名的能力", "result": "揭示了专家标注存在显著不一致性，挑战了传统基于准确度指标的有效性。提出的方法能够处理全长歌曲，解决MLLM内存限制问题", "conclusion": "建立了一个完整的无参考、多维度歌唱评估生态系统，从判别性评估转向描述性评估，解决了现有系统的根本局限性"}}
{"id": "2512.06997", "pdf": "https://arxiv.org/pdf/2512.06997", "abs": "https://arxiv.org/abs/2512.06997", "authors": ["Yiding Feng", "Rad Niazadeh", "Amin Saberi"], "title": "Near-Optimal Bayesian Online Assortment of Reusable Resources", "categories": ["cs.DS"], "comment": "Journal version: Operations Research, 2024; Preliminary conference version: ACM EC 2022", "summary": "Motivated by the applications of rental services in e-commerce, we consider revenue maximization in online assortment of reusable resources for a stream of arriving consumers with different types. We design competitive online algorithms with respect to the optimum online policy in the Bayesian setting, in which types are drawn independently from known heterogeneous distributions over time. In the regime where the minimum of initial inventories $c_0$ is large, our main result is a near-optimal $1-\\min\\left(\\frac{1}{2},\\sqrt{\\log(c_0)/c_0}\\right)$ competitive algorithm for the general case of reusable resources. Our algorithm relies on an expected LP benchmark for the problem, solves this LP, and simulates the solution through an independent randomized rounding. The main challenge is obtaining point-wise inventory feasibility in a computationally efficient fashion from these simulation-based algorithms. To this end, we use several technical ingredients to design $\\textit{discarding policies}$ -- one for each resource. These policies handle the trade-off between the inventory feasibility under reusability and the revenue loss of each of the resources. However, discarding a unit of a resource changes the future consumption of other resources. To handle this new challenge, we also introduce $\\textit{post-processing}$ assortment procedures that help with designing and analyzing our discarding policies as they run in parallel, which might be of independent interest. As a side result, by leveraging techniques from the literature on prophet inequality, we further show an improved near-optimal $1-1/\\sqrt{c_0+3}$ competitive algorithm for the special case of non-reusable resources. We finally evaluate the performance of our algorithms using the numerical simulations on the synthetic data.", "AI": {"tldr": "设计针对可重复使用资源在线组合优化的近最优贝叶斯算法", "motivation": "受电子商务中租赁服务的应用启发，需要解决可重复使用资源在消费者类型随时间独立异质分布情况下的在线收益最大化问题", "method": "基于期望线性规划基准，通过求解LP并使用独立随机舍入模拟解，设计丢弃策略处理库存可行性，并引入后处理组合程序", "result": "在初始库存较大时，为可重复使用资源提供1-min(1/2,√log(c₀)/c₀)竞争比的近最优算法；对不可重复使用资源改进为1-1/√(c₀+3)竞争比", "conclusion": "成功设计了处理可重复使用资源在线组合优化的高效算法，通过丢弃策略和后处理技术解决了库存可行性与收益损失之间的权衡问题"}}
{"id": "2512.06995", "pdf": "https://arxiv.org/pdf/2512.06995", "abs": "https://arxiv.org/abs/2512.06995", "authors": ["Maryam Seraj", "Mohammad Hossein Kamrava", "Carlo Tiseo"], "title": "Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans", "categories": ["cs.RO", "physics.class-ph"], "comment": null, "summary": "Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.", "AI": {"tldr": "设计一种用于超声扫描的电缆驱动同轴球面并联机构，通过减少末端执行器质量来改善惯性负载、刚度和动态响应，实现高精度的触觉反馈", "motivation": "医疗远程操作中需要高保真度的触觉接口，特别是在需要纯旋转运动的应用中，需要在工作空间、灵活性、刚度、惯性和带宽之间平衡性能折衷", "method": "提出电缆驱动同轴球面并联机构的设计方法和运动学分析，通过并联和同轴驱动实现解耦的旋转自由度，具有各向同性的力和扭矩传递", "result": "仿真和分析表明，CDC-SPM提供准确、响应迅速且安全的运动特性，适用于高精度触觉应用，特别适合超声成像等医疗远程操作任务", "conclusion": "该机构通过减少末端执行器质量、最小化惯性负载、增强刚度和改善动态响应，展示了在医疗远程操作中实现精确直观操作的潜力"}}
{"id": "2512.06991", "pdf": "https://arxiv.org/pdf/2512.06991", "abs": "https://arxiv.org/abs/2512.06991", "authors": ["Jing Jie Tan", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum", "Anissa Mokraoui", "Shih-Yu Lo"], "title": "Prompting-in-a-Series: Psychology-Informed Contents and Embeddings for Personality Recognition With Decoder-Only Models", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. This research introduces a novel \"Prompting-in-a-Series\" algorithm, termed PICEPR (Psychology-Informed Contents Embeddings for Personality Recognition), featuring two pipelines: (a) Contents and (b) Embeddings. The approach demonstrates how a modularised decoder-only LLM can summarize or generate content, which can aid in classifying or enhancing personality recognition functions as a personality feature extractor and a generator for personality-rich content. We conducted various experiments to provide evidence to justify the rationale behind the PICEPR algorithm. Meanwhile, we also explored closed-source models such as \\textit{gpt4o} from OpenAI and \\textit{gemini} from Google, along with open-source models like \\textit{mistral} from Mistral AI, to compare the quality of the generated content. The PICEPR algorithm has achieved a new state-of-the-art performance for personality recognition by 5-15\\% improvement. The work repository and models' weight can be found at https://research.jingjietan.com/?q=PICEPR.", "AI": {"tldr": "提出一种名为PICEPR的\"Prompting-in-a-Series\"算法，通过心理学启发的内容和嵌入方法，利用解码器专用LLM进行人格识别，实现了5-15%的性能提升", "motivation": "大型语言模型在自然语言处理任务中表现出色，但如何有效利用这些模型进行人格识别仍存在挑战。研究者希望开发一种能够结合心理学知识、模块化处理人格特征的算法", "method": "提出PICEPR算法，包含两个流程：(a)内容流程和(b)嵌入流程。使用模块化的解码器专用LLM来总结或生成内容，作为人格特征提取器和人格丰富内容生成器。实验对比了闭源模型(gpt4o, gemini)和开源模型(mistral)", "result": "PICEPR算法在人格识别任务上实现了新的最先进性能，相比现有方法有5-15%的性能提升。同时探索了不同模型生成内容的质量对比", "conclusion": "提出的PICEPR算法通过心理学启发的内容和嵌入方法，有效提升了人格识别性能，为利用LLM进行人格分析提供了新的有效方法"}}
{"id": "2512.06990", "pdf": "https://arxiv.org/pdf/2512.06990", "abs": "https://arxiv.org/abs/2512.06990", "authors": ["Krishna Arun", "Moinak Bhattachrya", "Paras Goel"], "title": "Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients", "categories": ["cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient's brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain's progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.", "AI": {"tldr": "开发一个用于胶质母细胞瘤（GBM）诊断和治疗规划的人工智能系统，通过多智能体强化学习结合编码器-解码器架构来识别最佳切除位置", "motivation": "目前医学领域缺乏支持医生治疗异质性脑肿瘤（如胶质母细胞瘤）的AI工具，GBM是世界上最致命的癌症，五年生存率仅为5.1%。需要端到端的解决方案来辅助诊断和治疗规划", "method": "系统分为诊断和治疗规划两个阶段。诊断阶段使用包含4个分类模型（卷积神经网络和支持向量机）的顺序决策框架，逐步对患者大脑进行分类。治疗规划阶段使用包含3个生成模型的强化学习系统：切除模型（扩散模型）预测可能的切除结果，放疗模型（时空视觉Transformer）生成指定周数后的脑部MRI进展，化疗模型（扩散模型）生成治疗后MRI。生存率计算器（CNN）检查生成的治疗后MRI生存率是否在用户定义目标的15%范围内，否则使用近端策略优化的反馈循环迭代直到找到最佳切除位置", "result": "相比现有解决方案：1）使用4个小诊断模型的顺序决策框架将计算成本降低22.28倍；2）Transformer的回归能力将肿瘤进展推理时间减少113小时；3）应用模拟现实情况的增强技术将整体DICE分数提高2.9%。这些结果预计可将生存率提高0.9%，可能挽救约2,250人的生命", "conclusion": "该研究开发了一个创新的端到端AI系统，通过多智能体强化学习和编码器-解码器架构，成功识别胶质母细胞瘤的最佳切除位置，在计算效率、推理时间和预测准确性方面均有显著改进，有望提高患者生存率"}}
{"id": "2512.06989", "pdf": "https://arxiv.org/pdf/2512.06989", "abs": "https://arxiv.org/abs/2512.06989", "authors": ["Minshen Zhang", "Xiang Hu", "Jianguo Li", "Wei Wu", "Kewei Tu"], "title": "Flash Multi-Head Feed-Forward Network", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 8 figures", "summary": "We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.", "AI": {"tldr": "提出Flash Multi-Head FFN (FlashMHF)作为Transformer中FFN的替代方案，通过多头机制增强表达能力，同时解决内存消耗和维度比例失衡问题。", "motivation": "受到注意力机制中多头设计增强表达能力的启发，探索将多头机制应用于FFN。但直接应用面临两个挑战：1) 内存消耗随头数线性增长；2) 模型扩展时中间维度与头维度比例失衡，影响可扩展性和表达能力。", "method": "提出FlashMHF，包含两个关键创新：1) 类似FlashAttention的I/O感知融合内核，在SRAM中在线计算输出；2) 使用动态加权并行子网络设计，保持中间维度与头维度的平衡比例。", "result": "在128M到1.3B参数的模型上验证，FlashMHF相比SwiGLU FFNs持续改进困惑度和下游任务准确率，同时将峰值内存使用降低3-5倍，推理速度提升最高1.08倍。", "conclusion": "多头设计是FFN的优越架构原则，FlashMHF作为Transformer中FFN的强大、高效且可扩展的替代方案，为未来模型设计提供了新方向。"}}
{"id": "2512.06983", "pdf": "https://arxiv.org/pdf/2512.06983", "abs": "https://arxiv.org/abs/2512.06983", "authors": ["Eli J. Laird", "Corey Clark"], "title": "On Memory: A comparison of memory mechanisms in world models", "categories": ["cs.AI", "cs.LG"], "comment": "10 pages, 1 figure", "summary": "World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.", "AI": {"tldr": "比较不同记忆机制在世界模型中的表现，分析它们如何扩展模型的记忆跨度并改善长时规划能力", "motivation": "世界模型在长时规划中存在记忆跨度限制，导致感知漂移和难以完成轨迹闭环，需要研究有效的记忆增强机制来解决这一问题", "method": "提出记忆编码和记忆注入机制的分类法，通过残差流动态分析这些机制的作用，使用状态回忆评估任务测量不同机制的记忆召回能力", "result": "记忆机制显著提升了视觉Transformer的有效记忆跨度，为在世界模型的想象中完成轨迹闭环提供了可行路径", "conclusion": "记忆增强机制是提升世界模型长时规划能力的关键，分类法为理解和设计更有效的记忆系统提供了理论框架"}}
{"id": "2512.06981", "pdf": "https://arxiv.org/pdf/2512.06981", "abs": "https://arxiv.org/abs/2512.06981", "authors": ["Yuemin Wang", "Ian Stavness"], "title": "Selective Masking based Self-Supervised Learning for Image Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.", "AI": {"tldr": "提出一种基于选择性掩码的自监督学习方法，用于图像语义分割，通过选择性掩码图像重建作为预训练任务，替代传统的随机掩码方法。", "motivation": "传统随机掩码方法在图像重建预训练中效率不高，无法充分利用已训练模型的知识。需要一种更智能的掩码策略来提升自监督学习对下游分割任务的性能。", "method": "提出选择性掩码方法，将图像重建预训练分解为迭代步骤，在每一步中根据重建损失选择性地掩码最难重建的图像块，利用已训练模型的知识指导掩码过程。", "result": "在两个通用数据集（Pascal VOC和Cityscapes）和两个杂草分割数据集上，选择性掩码方法比传统随机掩码方法和监督ImageNet预训练在下游分割准确率上分别提升2.9%和2.5%。特别改善了最低性能类别的准确率。", "conclusion": "选择性掩码图像重建方法为端到端语义分割工作流提供了有效实用的解决方案，特别适用于需要有限模型容量以满足推理速度和计算资源要求的场景。"}}
{"id": "2512.06977", "pdf": "https://arxiv.org/pdf/2512.06977", "abs": "https://arxiv.org/abs/2512.06977", "authors": ["Laurentius Valdy", "Richard D. Paul", "Alessio Quercia", "Zhuo Cao", "Xuan Zhao", "Hanno Scharr", "Arya Bangun"], "title": "Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging", "categories": ["eess.IV", "cs.LG"], "comment": "8 pages, 5 figures, AAAI AI2ASE 2026", "summary": "Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.", "AI": {"tldr": "提出一个结合物理约束和分区扩散先验的多切片重建框架，用于医学和科学成像，旨在从有限测量数据中实现高质量重建，同时显著降低GPU内存使用。", "motivation": "多切片重建在医学和科学成像中至关重要，但面临两个主要挑战：1) 从有限测量数据重建的病态问题；2) 高计算和内存需求。现有方法要么依赖纯物理约束（精度有限），要么使用完整扩散模型（内存消耗大）。", "method": "提出一个集成物理约束和分区扩散先验的框架。通过将扩散先验分区处理，大幅降低每个GPU的内存使用，同时结合物理基础约束来指导重建过程，平衡了重建质量和计算效率。", "result": "该方法在磁共振成像（MRI）和四维扫描透射电子显微镜（4D-STEM）两种模态上都优于纯物理方法和完整多切片重建基线。不仅提高了分布内数据的准确性，还表现出对分布外数据的强泛化能力，同时显著减少了GPU内存使用。", "conclusion": "提出的物理引导扩散先验框架成功解决了多切片重建中的病态问题和计算资源限制，在保持高质量重建的同时大幅降低内存需求，为医学和科学成像的快速采集提供了有效解决方案。"}}
{"id": "2512.06971", "pdf": "https://arxiv.org/pdf/2512.06971", "abs": "https://arxiv.org/abs/2512.06971", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "title": "Prediction with Expert Advice under Local Differential Privacy", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "comment": "19 pages, 3 figures", "summary": "We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \\textit{central} DP algorithm by 1.5-3$\\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.", "AI": {"tldr": "研究在本地差分隐私约束下的专家建议预测问题，提出了两种改进算法并进行了理论分析和实际验证", "motivation": "在本地差分隐私约束下解决专家建议预测问题，现有方法存在隐私成本高、无法处理复杂专家算法等局限性，需要开发更高效且实用的隐私保护预测算法", "method": "首先证明经典算法自然满足LDP，然后设计两种新算法：RW-AdaBatch利用LDP诱导的有限切换行为实现隐私放大，RW-Meta开发了在LDP下选择复杂专家算法的方法，两者都基于随机游走理论", "result": "RW-AdaBatch在易处理数据上实现更强的隐私放大且几乎无效用损失，RW-Meta在COVID-19医院数据预测任务中比经典基线和最先进的中心差分隐私算法表现好1.5-3倍", "conclusion": "在本地差分隐私约束下，通过利用随机游走理论和隐私放大机制，可以设计出既保护隐私又保持高效预测性能的专家建议算法，特别适用于医疗数据等敏感应用场景"}}
{"id": "2512.06969", "pdf": "https://arxiv.org/pdf/2512.06969", "abs": "https://arxiv.org/abs/2512.06969", "authors": ["Adrian Przybysz", "Mikołaj Kołek", "Franciszek Sobota", "Jarek Duda"], "title": "Comparing BFGS and OGR for Second-Order Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.", "AI": {"tldr": "比较BFGS和OGR两种二阶优化方法在神经网络训练中的性能差异", "motivation": "神经网络训练中Hessian矩阵估计面临高维度和高计算成本的挑战，需要更有效的二阶优化方法", "method": "比较经典的BFGS方法（使用Sherman-Morrison更新保持正定Hessian近似）与新颖的在线梯度回归方法（OGR），后者使用指数移动平均在线估计二阶导数，无需Hessian求逆", "result": "OGR在标准测试函数上实现了更快的收敛速度和更低的损失，特别是在非凸设置中表现更优", "conclusion": "OGR方法相比传统BFGS具有优势，能够处理非凸结构且无需Hessian求逆，在非凸优化问题中表现更好"}}
{"id": "2512.06966", "pdf": "https://arxiv.org/pdf/2512.06966", "abs": "https://arxiv.org/abs/2512.06966", "authors": ["Zilin Li", "Weiwei Xu", "Vicki Kane"], "title": "Neuro-Vesicles: Neuromodulation Should Be a Dynamical System, Not a Tensor Decoration", "categories": ["cs.NE"], "comment": "17 pages, 1 overview figure, 3 tables; Early-stage theoretical design of the Neuro-Vesicle framework, posted as a record of original contribution", "summary": "We introduce Neuro-Vesicles, a framework that augments conventional neural networks with a missing computational layer: a dynamical population of mobile, discrete vesicles that live alongside the network rather than inside its tensors. Each vesicle is a self contained object v = (c, kappa, l, tau, s) carrying a vector payload, type label, location on the graph G = (V, E), remaining lifetime, and optional internal state. Vesicles are emitted in response to activity, errors, or meta signals; migrate along learned transition kernels; probabilistically dock at nodes; locally modify activations, parameters, learning rules, or external memory through content dependent release operators; and finally decay or are absorbed. This event based interaction layer reshapes neuromodulation. Instead of applying the same conditioning tensors on every forward pass, modulation emerges from the stochastic evolution of a vesicle population that can accumulate, disperse, trigger cascades, carve transient pathways, and write structured traces into topological memory. Dense, short lived vesicles approximate familiar tensor mechanisms such as FiLM, hypernetworks, or attention. Sparse, long lived vesicles resemble a small set of mobile agents that intervene only at rare but decisive moments. We give a complete mathematical specification of the framework, including emission, migration, docking, release, decay, and their coupling to learning; a continuous density relaxation that yields differentiable reaction diffusion dynamics on the graph; and a reinforcement learning view where vesicle control is treated as a policy optimized for downstream performance. We also outline how the same formalism extends to spiking networks and neuromorphic hardware such as the Darwin3 chip, enabling programmable neuromodulation on large scale brain inspired computers.", "AI": {"tldr": "提出Neuro-Vesicles框架，为传统神经网络增加一个动态的囊泡计算层，实现基于事件驱动的神经调制机制", "motivation": "传统神经网络的调制机制通常使用张量装饰（如FiLM、超网络等），缺乏动态、事件驱动的特性。需要一种更接近生物神经调制的方式，能够实现积累、分散、级联触发等动态行为", "method": "引入离散、移动的囊泡对象v=(c, kappa, l, tau, s)，包含向量载荷、类型标签、图位置、生命周期和内部状态。设计囊泡的发射、迁移、停靠、释放、衰减等动态过程，以及连续密度松弛方法得到可微的图反应扩散动力学", "result": "该框架能够实现从密集短寿命囊泡（近似传统张量机制）到稀疏长寿命囊泡（类似移动代理）的连续谱系。提供了完整的数学规范，并展示了如何扩展到脉冲网络和神经形态硬件", "conclusion": "神经调制应该是一个动态系统而非张量装饰。Neuro-Vesicles框架为神经网络提供了事件驱动的调制层，能够实现更灵活、动态的神经计算，并有望在大规模脑启发计算机上实现可编程神经调制"}}
{"id": "2512.06963", "pdf": "https://arxiv.org/pdf/2512.06963", "abs": "https://arxiv.org/abs/2512.06963", "authors": ["Yichao Shen", "Fangyun Wei", "Zhiying Du", "Yaobo Liang", "Yan Lu", "Jiaolong Yang", "Nanning Zheng", "Baining Guo"], "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://videovla-nips2025.github.io", "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", "AI": {"tldr": "VideoVLA将大型视频生成模型转化为机器人操作器，通过联合建模视频、语言和动作模态，实现动作序列和未来视觉结果的预测", "motivation": "当前视觉-语言-动作模型在泛化到新任务、新物体和新环境方面能力有限，需要探索新的机器人学习范式来提升泛化能力", "method": "基于多模态扩散变换器，使用预训练的视频生成模型进行联合视觉和动作预测，通过语言指令和图像输入预测动作序列及未来视觉结果", "result": "高质量的未来视觉想象与可靠的动作预测和任务成功相关，展示了在模仿其他技能和处理新物体方面的强大泛化能力", "conclusion": "双重预测策略（预测动作及其视觉后果）探索了机器人学习的新范式，释放了操作系统的泛化能力，强调了视觉想象在操作中的重要性"}}
{"id": "2512.06951", "pdf": "https://arxiv.org/pdf/2512.06951", "abs": "https://arxiv.org/abs/2512.06951", "authors": ["Ilia Larchenko", "Gleb Zarin", "Akash Karnatak"], "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "2025 NeurIPS Behavior Challenge 1st place solution", "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making. Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules. Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.", "AI": {"tldr": "本文介绍了赢得2025 BEHAVIOR挑战赛第一名的视觉-动作策略，该挑战包含50个多样化长视野家庭任务，需要双手操作、导航和情境感知决策。基于Pi0.5架构，引入了相关噪声流匹配等创新方法。", "motivation": "BEHAVIOR挑战赛是一个大规模基准测试，包含50个多样化的长视野家庭任务，需要在逼真模拟环境中完成双手操作、导航和情境感知决策。现有方法在处理这些复杂任务时存在效率不足和动作序列不连贯的问题，需要开发更有效的训练和推理方法。", "method": "基于Pi0.5架构，主要创新包括：1）相关噪声流匹配，提高训练效率并实现相关感知修复以获得平滑动作序列；2）可学习混合层注意力；3）System 2阶段跟踪用于歧义消解；4）多样本流匹配减少方差；5）推理时使用动作压缩和挑战特定校正规则。", "result": "该方法在公开和私有排行榜的所有50个任务上实现了26%的q-score，赢得了2025 BEHAVIOR挑战赛的第一名。", "conclusion": "提出的视觉-动作策略通过相关噪声流匹配、可学习注意力机制和System 2跟踪等创新，成功解决了复杂家庭任务中的训练效率和动作连贯性问题，在BEHAVIOR挑战赛中取得了优异成绩。"}}
{"id": "2512.06949", "pdf": "https://arxiv.org/pdf/2512.06949", "abs": "https://arxiv.org/abs/2512.06949", "authors": ["Shravan Venkatraman", "Muthu Subash Kavitha", "Joe Dhanith P R", "V Manikandarajan", "Jia Wu"], "title": "Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology", "categories": ["cs.CV"], "comment": "19 pages, 5 figures, 2 tables", "summary": "Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\\% to 31.25\\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.", "AI": {"tldr": "提出了一种名为神经组织关系建模（NTRM）的新型分割框架，通过结合CNN和图神经网络来建模皮肤组织病理学图像中不同组织类型之间的空间和功能关系，以解决重叠或形态相似组织的分割挑战。", "motivation": "当前基于CNN的组织病理学图像分割方法主要依赖视觉纹理特征，将组织视为独立区域，缺乏对组织间空间关系和生物上下文信息的建模，特别是在边界密集、组织重叠或形态相似的区域表现不佳。", "method": "NTRM框架结合CNN和图神经网络：首先使用CNN进行初步组织分割，然后在预测区域上构建图结构，通过图神经网络的消息传递机制传播上下文信息，最后通过空间投影细化分割结果，显式编码组织间依赖关系。", "result": "在非黑色素瘤皮肤癌分割基准数据集上，NTRM显著优于现有最先进方法，Dice相似系数比最佳对比模型高出4.9%到31.25%，在边界密集区域实现了结构更连贯的预测。", "conclusion": "关系建模为组织病理学分割提供了一条原则性路径，相比仅依赖局部感受野的架构，能够实现更具上下文感知性和可解释性的分割，表明超越视觉特征的关系建模对于复杂组织分析至关重要。"}}
{"id": "2512.06944", "pdf": "https://arxiv.org/pdf/2512.06944", "abs": "https://arxiv.org/abs/2512.06944", "authors": ["Munshi Mahbubur Rahman", "Shimei Pan", "James R. Foulds"], "title": "A Unifying Human-Centered AI Fairness Framework", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.", "AI": {"tldr": "提出一个统一的人类中心AI公平性框架，系统整合八种不同的公平性度量，帮助利益相关者根据价值观和情境考虑选择适当的公平性干预措施。", "motivation": "AI在关键社会领域的应用日益增加，引发了关于公平性的担忧，特别是在种族、性别和社会经济地位等敏感属性上的不平等对待。现有公平性研究存在不同公平概念之间的权衡困难，以及预测准确性与公平性之间的冲突，这阻碍了公平AI系统的实际部署。", "method": "提出一个统一的人类中心公平性框架，系统覆盖八种不同的公平性度量：结合个体与群体公平性、边际内与交叉性假设、结果导向与机会平等视角。框架采用一致且易于理解的公式化表达，允许利益相关者为多个公平性目标分配权重，反映其优先级并促进多方妥协。", "result": "在四个真实世界数据集上应用该框架：UCI Adult收入预测、COMPAS刑事再犯、德国信用风险评估、MEPS医疗保健利用。结果显示调整权重可以揭示不同公平性度量之间的微妙权衡。通过司法决策和医疗保健的案例研究，展示了框架如何指导公平AI系统的实际和价值敏感部署。", "conclusion": "该统一框架通过系统整合多种公平性视角，降低了非专家的学习曲线，促进了多方利益相关者的妥协，为公平AI系统的实际部署提供了实用且价值敏感的方法论指导。"}}
{"id": "2512.06935", "pdf": "https://arxiv.org/pdf/2512.06935", "abs": "https://arxiv.org/abs/2512.06935", "authors": ["Nicolò Botteghi", "Owen Brook", "Urban Fasel", "Federico Califano"], "title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs", "categories": ["cs.RO"], "comment": null, "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks. In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms", "AI": {"tldr": "提出一种基于稀疏神经ODE的IDA-PBC控制方法，通过数值方式解决匹配条件PDE，使IDA-PBC能应用于复杂任务", "motivation": "传统IDA-PBC方法需要解析求解复杂的偏微分方程（匹配条件），这限制了其在复杂物理系统和任务中的应用，特别是稳定化以外的任务", "method": "将IDA-PBC问题转化为神经ODE学习问题，利用稀疏字典学习将期望闭环系统参数化为非线性状态相关函数的稀疏线性组合，通过多目标优化求解控制器参数", "result": "该方法使IDA-PBC能够应用于复杂任务（如周期性振荡行为发现），并能推导出包含残差项的闭环系统解析表达式", "conclusion": "提出的数值方法克服了传统IDA-PBC解析求解匹配条件的限制，扩展了该方法在复杂系统和任务中的应用范围"}}
{"id": "2512.06933", "pdf": "https://arxiv.org/pdf/2512.06933", "abs": "https://arxiv.org/abs/2512.06933", "authors": ["Zifan Peng"], "title": "MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions", "categories": ["cs.CE", "cs.CL", "cs.HC"], "comment": null, "summary": "Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.", "AI": {"tldr": "MATEX是一个多智能体框架，用于解释复杂的以太坊交易，通过协作调查生成可信的逐步解释", "motivation": "理解复杂的以太坊交易具有挑战性：多跳代币流、嵌套合约调用和不透明的执行路径导致用户盲目签名。通过对普通用户、开发者和审计师的访谈，发现需要基于链上证据和真实世界协议语义的忠实、逐步解释", "method": "采用认知多智能体框架，将交易理解建模为协作调查，结合快速假设生成、动态链下知识检索、证据感知合成和对抗验证来生成忠实解释", "result": "论文提出了MATEX框架，能够生成基于链上证据和协议语义的逐步交易解释，帮助用户理解复杂的以太坊交易", "conclusion": "MATEX通过多智能体协作调查的方法，为复杂的以太坊交易提供了可信的解释框架，解决了用户理解交易困难的问题"}}
{"id": "2512.06932", "pdf": "https://arxiv.org/pdf/2512.06932", "abs": "https://arxiv.org/abs/2512.06932", "authors": ["Salma Albelali", "Moataz Ahmed"], "title": "Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.", "AI": {"tldr": "研究数据泄漏对LSTM时间序列预测模型评估的影响，分析不同验证策略和配置参数如何调节泄漏敏感性", "motivation": "深度学习模型在时间序列预测中广泛应用，但评估完整性常因数据泄漏而受损。数据泄漏指在数据集划分前构建输入输出序列，导致未来信息无意中影响训练。本研究旨在探究数据泄漏对性能的影响，特别关注验证设计如何调节泄漏敏感性。", "method": "研究三种常用验证技术（2-way split、3-way split和10-fold交叉验证）在泄漏（预分割序列生成）和清洁条件下的表现。清洁条件通过在数据分割前强制执行时间分离来减轻泄漏风险。使用RMSE增益评估泄漏影响，计算泄漏与清洁设置之间的百分比差异。分析输入窗口大小和滞后步长对泄漏敏感性的影响。", "result": "10-fold交叉验证在较长滞后步长下表现出高达20.5%的RMSE增益，而2-way和3-way分割表现出更强的鲁棒性，通常将RMSE增益保持在5%以下。较小的窗口大小和较长的滞后步长会增加泄漏风险，而较大的窗口有助于减少泄漏。", "conclusion": "研究结果强调了需要配置感知、抗泄漏的评估流程来确保可靠的性能估计。验证策略的选择和模型配置参数（如窗口大小和滞后步长）显著影响数据泄漏的敏感性，需要在时间序列预测评估中特别注意。"}}
{"id": "2512.06929", "pdf": "https://arxiv.org/pdf/2512.06929", "abs": "https://arxiv.org/abs/2512.06929", "authors": ["MinCheol Jeon"], "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.", "AI": {"tldr": "提出AdaMamba架构，通过自适应归一化、多尺度趋势分解和专家混合编码，解决时间序列预测中的非平稳性、多尺度模式和分布偏移问题。", "motivation": "现实世界时间序列预测面临非平稳性、多尺度时间模式和分布偏移等挑战，这些因素会降低模型稳定性和准确性，需要一种统一的架构来解决这些问题。", "method": "1. 自适应归一化块：通过多尺度卷积趋势提取和通道级重新校准去除非平稳成分；2. 上下文编码器：结合补丁嵌入、位置编码和Mamba增强的Transformer层；3. 轻量级预测头和去归一化机制：生成多步预测并重新整合局部趋势。", "result": "实验评估表明，AdaMamba在稳定性和准确性方面相比传统Transformer基线有持续改进，能有效缓解协变量偏移并增强预测可靠性。", "conclusion": "AdaMamba通过自适应归一化和专家增强的上下文建模，为时间序列预测提供了强大的表示能力和模块化可扩展性，支持确定性预测并与概率扩展兼容。"}}
{"id": "2512.06926", "pdf": "https://arxiv.org/pdf/2512.06926", "abs": "https://arxiv.org/abs/2512.06926", "authors": ["Salma Albelali", "Moataz Ahmed"], "title": "Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.", "AI": {"tldr": "本文系统评估了BiLSTM预测模型对输入序列长度和加性噪声的敏感性，揭示了这两个数据特性因素对模型鲁棒性和泛化能力的重要影响。", "motivation": "深度学习模型在时间序列预测中应用广泛，但现有研究对模型鲁棒性和泛化能力对输入数据特性的敏感性关注不足。BiLSTM架构虽然能有效捕捉复杂时间依赖，但其对输入序列长度和噪声的敏感性尚未得到系统研究。", "method": "开发了模块化、可复现的预测流程，包含标准化预处理、序列生成、模型训练、验证和评估。在三个不同采样频率的真实世界数据集上进行控制实验，系统分析输入序列长度和加性噪声对BiLSTM性能的影响。", "result": "主要发现：(1)较长的输入序列显著增加过拟合和数据泄露风险，尤其在数据受限环境中；(2)加性噪声在不同采样频率下均持续降低预测精度；(3)两个因素同时存在时，模型稳定性下降最为显著。高观测频率数据集表现出更强鲁棒性，但在双重挑战下仍然脆弱。", "conclusion": "研究揭示了当前基于深度学习的预测流程的重要局限性，强调了数据感知设计策略的必要性。为深入理解DL模型在动态时间序列环境中的行为提供了见解，并为开发更可靠、可泛化的预测系统提供了实用指导。"}}
{"id": "2512.06925", "pdf": "https://arxiv.org/pdf/2512.06925", "abs": "https://arxiv.org/abs/2512.06925", "authors": ["Aseer Al Faisal"], "title": "Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.", "AI": {"tldr": "提出一种结合RoBERTa语义嵌入和手工词汇特征的QR-DQN深度强化学习方法，用于钓鱼网站检测，通过分位数回归建模回报分布以提升模型稳定性和泛化能力。", "motivation": "钓鱼攻击通过欺诈信息、误导广告和合法网站被黑等方式窃取个人信息，造成经济损失。传统DQN方法估计单一标量Q值，难以处理不确定性，且现有方法在未见钓鱼数据上泛化能力有限。", "method": "采用Quantile Regression Deep Q-Network (QR-DQN)框架，整合RoBERTa生成的语义嵌入和手工设计的词汇特征。使用分位数回归建模回报分布而非单一Q值，从PhishTank、OpenPhish、Cloudflare等来源收集105,000个URL数据集，采用80/20训练测试划分。", "result": "测试准确率达99.86%，精确率99.75%，召回率99.96%，F1分数99.85%。相比仅使用词汇特征的标准DQN，混合QR-DQN将泛化差距从1.66%降至0.04%。五折交叉验证平均准确率99.90%，标准差0.04%。", "conclusion": "提出的混合QR-DQN方法能有效识别钓鱼威胁，适应不断演变的攻击策略，并在未见数据上表现出良好的泛化能力，为钓鱼检测提供了更稳健的解决方案。"}}
{"id": "2512.06921", "pdf": "https://arxiv.org/pdf/2512.06921", "abs": "https://arxiv.org/abs/2512.06921", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by IEEE ICIA 2025", "summary": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "AI": {"tldr": "提出了首个专门评估神经外科解剖结构识别的多模态基准数据集NeuroABench，用于测试多模态大语言模型在神经外科视频中的解剖理解能力。", "motivation": "现有研究主要关注手术流程理解，忽视了临床实践中至关重要的解剖结构识别能力。外科医生依赖精确的解剖知识来解读手术视频，而当前缺乏专门评估解剖理解能力的基准。", "method": "开发了包含9小时标注视频的NeuroABench数据集，涵盖89种不同手术，采用新型多模态标注流程和多重审核循环。评估68个临床解剖结构的识别能力，并对比了10多个最先进的MLLM模型与4名神经外科培训生的表现。", "result": "最佳MLLM模型在解剖识别任务中仅达到40.87%准确率。神经外科培训生中最佳表现者达到56%准确率，最低28%，平均46.5%。最佳MLLM表现与最低分培训生相当，但显著低于培训生平均表现。", "conclusion": "MLLM在解剖理解方面已取得进展，但与人类水平仍存在显著差距。NeuroABench为评估和改进手术视频理解中的解剖识别能力提供了标准化框架。"}}
{"id": "2512.06914", "pdf": "https://arxiv.org/pdf/2512.06914", "abs": "https://arxiv.org/abs/2512.06914", "authors": ["Guanquan Shi", "Haohua Du", "Zhiqiang Wang", "Xiaoyu Liang", "Weiwenpei Liu", "Song Bian", "Zhenyu Guan"], "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security. We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.", "AI": {"tldr": "本文系统分析了LLM智能体交互中的信任-授权不匹配问题，提出了一个统一的形式化框架来理解这一新兴安全领域的根本原因。", "motivation": "随着LLM发展为能够与外部世界交互的自主智能体，传统的确定性安全机制无法适应基于概率推理的自然语言决策过程，导致经典网络安全挑战在新型易变环境中重现。学术界对这一新兴领域的理解仍然碎片化，缺乏系统分析框架。", "method": "提出了一个以信任-授权差距为中心的新型风险分析模型，作为统一视角来调查和分类现有看似孤立的攻击和防御实现路径。通过这一形式化框架，系统分析智能体交互安全。", "result": "发现该领域大多数安全威胁源于信任评估与授权策略之间的根本性不匹配。新的框架不仅统一了该领域，还识别出关键研究空白，为构建健壮可信的智能体和动态授权机制提供了系统性研究方向。", "conclusion": "LLM智能体交互中的安全挑战需要新的形式化框架来理解信任-授权不匹配问题。提出的风险分析模型为系统分析现有攻击防御提供了统一视角，并指明了构建可信智能体系统和动态授权机制的未来研究方向。"}}
{"id": "2512.06912", "pdf": "https://arxiv.org/pdf/2512.06912", "abs": "https://arxiv.org/abs/2512.06912", "authors": ["Rushiraj Gadhvi", "Sandeep Manjanna"], "title": "Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields", "categories": ["cs.RO", "cs.LG"], "comment": "Under Review for International Conference on Robotics and Automation (ICRA 2026)", "summary": "For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.", "AI": {"tldr": "提出一种基于强化学习的能量高效水面车辆导航方法，用于涡流场中的自主航行", "motivation": "受传统航海者利用洋流导航的启发，解决自主水面车辆在严格能量预算下长期任务中的能量效率问题，特别是在部分可观测的涡流场环境中", "method": "采用基于Soft Actor Critic的端到端强化学习框架，仅使用局部速度测量学习流感知导航策略", "result": "在多样化和动态丰富的场景中，该方法显示出显著的能量节省，导航路径的能量保存比现有技术提高了30-50%，并能鲁棒地泛化到未见过的流场条件", "conclusion": "该方法为海洋环境中的长期自主性提供了一条有前景的路径，通过模仿传统航海智慧并应用现代机器学习技术，实现了能量高效的水面车辆导航"}}
{"id": "2512.06910", "pdf": "https://arxiv.org/pdf/2512.06910", "abs": "https://arxiv.org/abs/2512.06910", "authors": ["Dennis Becker", "Kyra Ahrens", "Connor Gäde", "Erik Strahl", "Stefan Wermter"], "title": "Robots with Attitudes: Influence of LLM-Driven Robot Personalities on Motivation and Performance", "categories": ["cs.HC"], "comment": "ef:Proceedings of the 13th International Conference on Human-Agent Interaction, 2025", "summary": "Large language models enable unscripted conversations while maintaining a consistent personality. One desirable personality trait in cooperative partners, known to improve task performance, is agreeableness. To explore the impact of large language models on personality modeling for robots, as well as the effect of agreeable and non-agreeable personalities in cooperative tasks, we conduct a two-part study. This includes an online pre-study for personality validation and a lab-based main study to evaluate the effects on likability, motivation, and task performance. The results demonstrate that the robot's agreeableness significantly enhances its likability. No significant difference in intrinsic motivation was observed between the two personality types. However, the findings suggest that a robot exhibiting agreeableness and openness to new experiences can enhance task performance. This study highlights the advantages of employing large language models for customized modeling of robot personalities and provides evidence that a carefully chosen agreeable robot personality can positively influence human perceptions and lead to greater success in cooperative scenarios.", "AI": {"tldr": "研究LLM驱动的机器人个性（特别是宜人性）对人类合作任务中动机和表现的影响", "motivation": "探索LLM在机器人个性建模中的应用，以及宜人性和非宜人性个性在合作任务中的效果差异", "method": "采用两部分研究设计：在线预研究用于个性验证，实验室主研究评估宜人性和非宜人性机器人对喜好度、动机和任务表现的影响", "result": "机器人宜人性显著提升其喜好度；两种个性类型在内在动机上无显著差异；但表现出宜人性和对新体验开放性的机器人能提升任务表现", "conclusion": "LLM可用于定制机器人个性，精心选择的宜人机器人个性能积极影响人类感知并提高合作场景的成功率"}}
{"id": "2512.06905", "pdf": "https://arxiv.org/pdf/2512.06905", "abs": "https://arxiv.org/abs/2512.06905", "authors": ["Zijian Zhou", "Shikun Liu", "Haozhe Liu", "Haonan Qiu", "Zhaochong An", "Weiming Ren", "Zhiheng Liu", "Xiaoke Huang", "Kam Woh Ng", "Tian Xie", "Xiao Han", "Yuren Cong", "Hang Li", "Chuyan Zhu", "Aditya Patel", "Tao Xiang", "Sen He"], "title": "Scaling Zero-Shot Reference-to-Video Generation", "categories": ["cs.CV"], "comment": "Website: https://franciszzj.github.io/Saber/", "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "AI": {"tldr": "提出Saber框架，实现无需显式参考图像-视频-文本三元组数据的零样本参考到视频生成，通过掩码训练策略和注意力模型设计学习身份一致且参考感知的表示。", "motivation": "当前参考到视频生成方法依赖昂贵的显式参考图像-视频-文本三元组数据，这些数据构建成本高且难以扩展，需要突破这一瓶颈。", "method": "提出Saber框架：1）仅使用视频-文本对进行训练；2）采用掩码训练策略；3）设计专门的基于注意力的模型架构；4）集成掩码增强技术减少复制粘贴伪影。", "result": "在OpenS2V-Eval基准测试中优于使用R2V数据训练的方法，展示了在不同数量参考图像下的强大泛化能力。", "conclusion": "Saber框架通过零样本学习方法成功绕过了传统R2V方法的数据依赖瓶颈，实现了可扩展且高质量的参考到视频生成。"}}
{"id": "2512.06902", "pdf": "https://arxiv.org/pdf/2512.06902", "abs": "https://arxiv.org/abs/2512.06902", "authors": ["Fazle Rabbi", "Soumit Kanti Saha", "Tri Minh Triet Pham", "Song Wang", "Jinqiu Yang"], "title": "BabelCoder: Agentic Code Translation with Specification Alignment", "categories": ["cs.SE", "cs.AI"], "comment": "21 pages, 8 figures, 4 tables", "summary": "As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.", "AI": {"tldr": "BabelCoder是一个基于智能代理的代码翻译框架，通过多个专门代理协作分解代码翻译任务，提高翻译质量。", "motivation": "随着软件系统演进，开发者需要在多种编程语言间工作，经常需要将代码从一种语言迁移到另一种语言。虽然自动代码翻译提供了有前景的解决方案，但长期以来一直是一个具有挑战性的任务。现有的大语言模型方法在准确性上有限，未能有效利用代码中的上下文和结构线索。", "method": "BabelCoder采用智能代理框架，将代码翻译任务分解为专门的代理：翻译代理负责生成代码，测试代理验证正确性，精炼代理修复错误。这些代理协作工作，每个代理负责特定方面，共同提高翻译质量。", "result": "在四个基准数据集上与四个最先进的基线方法进行比较，BabelCoder在94%的情况下优于现有方法，性能提升0.5%-13.5%，平均准确率达到94.16%。", "conclusion": "BabelCoder通过智能代理框架有效解决了代码翻译的挑战，通过分解任务为专门代理并协作工作，显著提高了翻译准确性和质量。"}}
{"id": "2512.06897", "pdf": "https://arxiv.org/pdf/2512.06897", "abs": "https://arxiv.org/abs/2512.06897", "authors": ["Bradley Hobbs", "Panagiotis Artemiadis"], "title": "Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation", "categories": ["cs.RO"], "comment": null, "summary": "This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.", "AI": {"tldr": "研究探索在视觉反馈步态训练中加入地面顺应性是否比单独使用视觉反馈更能有效增加推进力，对步态康复有重要意义", "motivation": "传统视觉反馈步态训练在康复中应用广泛，但单独使用可能效果有限。研究者希望探索结合地面顺应性（本体感觉输入）是否能增强推进力训练效果，为中风等患者的长期康复提供更有效的干预手段", "method": "10名健康参与者在定制分带跑步机上行走。所有参与者接受地面反作用力的实时视觉反馈。实验组同时体验地面顺应性变化，对照组仅接受视觉反馈。测量推进力、肌肉活动和关节运动学", "result": "推进力成功增加并在干预后持续，特别是地面顺应性组。该组还表现出肌肉活动和关节运动学的持久后效应，表明对增加推进的自然策略学习更牢固。视觉和本体感觉系统在步态适应中协调作用", "conclusion": "地面顺应性与视觉反馈结合能增强推进力学习效果，支持在针对中风等推进力缺陷的长期康复中使用顺应性地面的潜力。这种多感官整合方法比单一视觉反馈更有效"}}
{"id": "2512.06896", "pdf": "https://arxiv.org/pdf/2512.06896", "abs": "https://arxiv.org/abs/2512.06896", "authors": ["Chrysostomos Karakasis", "Camryn Scully", "Robert Salati", "Panagiotis Artemiadis"], "title": "Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.", "AI": {"tldr": "验证一种基于导纳的控制策略，通过动态调整动力踝足假肢的准刚度来增强在柔性地面上的步态稳定性", "motivation": "截肢者在柔性地面上行走时跌倒风险显著增加，而现有动力假肢控制策略主要针对刚性地面，缺乏对柔性地面的优化控制方法", "method": "采用基于导纳的控制策略，动态调整假肢准刚度；通过三名健康受试者在两种不同刚度（63和25 kN/m）的双侧柔性表面上进行实验，使用相图和两种步态稳定性指标评估控制性能", "result": "与针对刚性地面开发的标准相位可变控制器相比，提出的导纳控制器在所有柔性条件下都一致地改善了步态稳定性", "conclusion": "自适应、稳定性感知的假肢控制策略具有降低真实环境中跌倒风险的潜力，并能提高康复机器人中人-假肢交互的鲁棒性"}}
{"id": "2512.06892", "pdf": "https://arxiv.org/pdf/2512.06892", "abs": "https://arxiv.org/abs/2512.06892", "authors": ["Hassan Jardali", "Durgakant Pushp", "Youwei Yu", "Mahmoud Ali", "Ihab S. Mohamed", "Alejandro Murillo-Gonzalez", "Paul D. Coen", "Md. Al-Masrur Khan", "Reddy Charan Pulivendula", "Saeoul Park", "Lingchuan Zhou", "Lantao Liu"], "title": "From Zero to High-Speed Racing: An Autonomous Racing Stack", "categories": ["cs.RO"], "comment": null, "summary": "High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.", "AI": {"tldr": "该论文介绍了为印第安纳波利斯自动驾驶挑战赛(IAC)开发的自动驾驶赛车堆栈(ARS)，展示了从零到高速赛车的完整系统，包括三个迭代版本，最高时速可达260公里/小时。", "motivation": "高速自动驾驶赛车面临重大技术和后勤挑战，包括精确定位、快速感知、动态规划和实时控制，同时受到赛道访问限制和昂贵硬件的制约。需要开发一个完整的系统来解决这些挑战。", "method": "开发了自动驾驶赛车堆栈(ARS)的三个迭代版本(ARS1、ARS2、ARS3)，采用模块化架构，在椭圆赛道和公路赛道上进行验证。系统包括定位、感知、规划和控制模块。", "result": "ARS系统在不同赛道上验证成功，最高时速达到260公里/小时。提供了详细的性能评估，对比了椭圆赛道和公路赛道环境下的控制、感知和估计性能，并发布了高速多传感器数据集。", "conclusion": "该研究展示了从零到高速赛车的完整自动驾驶系统开发，提供了真实世界高速全尺寸自动驾驶赛车的独特挑战和见解，为自动驾驶技术发展提供了宝贵经验。"}}
{"id": "2512.06890", "pdf": "https://arxiv.org/pdf/2512.06890", "abs": "https://arxiv.org/abs/2512.06890", "authors": ["Roger K. Moore"], "title": "What Needs to be Known in Order to Perform a Meaningful Scientific Comparison Between Animal Communications and Human Spoken Language", "categories": ["cs.SD"], "comment": "5 pages, 1 figure, Proc. Vocal Interactivity in-and-between Humans, Animals and Robots (VIHAR-24), Kos, Greece, 6 Sept. 2024", "summary": "Human spoken language has long been the subject of scientific investigation, particularly with regard to the mechanisms underpinning speech production. Likewise, the study of animal communications has a substantial literature, with many studies focusing on vocalisation. More recently, there has been growing interest in comparing animal communications and human speech. However, it is proposed here that such a comparison necessitates the appraisal of a minimum set of critical phenomena: i) the number of degrees-of-freedom of the vocal apparatus, ii) the ability to control those degrees-of-freedom independently, iii) the properties of the acoustic environment in which communication takes place, iv) the perceptual salience of the generated sounds, v) the degree to which sounds are contrastive, vi) the presence/absence of compositionality, and vii) the information rate(s) of the resulting communications.", "AI": {"tldr": "本文提出了进行动物通讯与人类口语科学比较所需考虑的关键现象最小集合", "motivation": "随着对动物通讯与人类口语比较研究的兴趣增长，作者认为现有研究缺乏系统性的比较框架，需要明确比较所需的关键要素", "method": "通过理论分析提出七个关键现象作为科学比较的最小集合：1) 发声器官的自由度数量；2) 独立控制这些自由度的能力；3) 通讯发生的声学环境特性；4) 产生声音的感知显著性；5) 声音的对比程度；6) 组合性的存在/缺失；7) 通讯的信息速率", "result": "建立了系统性的比较框架，明确了七个必须评估的关键维度，为动物通讯与人类口语的科学比较提供了方法论基础", "conclusion": "要进行有意义的动物通讯与人类口语科学比较，必须评估这七个关键现象，这些要素构成了比较研究的基本框架，有助于避免片面或不完整的比较分析"}}
{"id": "2512.06889", "pdf": "https://arxiv.org/pdf/2512.06889", "abs": "https://arxiv.org/abs/2512.06889", "authors": ["Ximing Huang", "Yirui Rao"], "title": "AQUILA: A QUIC-Based Link Architecture for Resilient Long-Range UAV Communication", "categories": ["cs.NI", "cs.RO"], "comment": "13 pages, 10 figures", "summary": "The proliferation of autonomous Unmanned Aerial Vehicles (UAVs) in Beyond Visual Line of Sight (BVLOS) applications is critically dependent on resilient, high-bandwidth, and low-latency communication links. Existing solutions face critical limitations: TCP's head-of-line blocking stalls time-sensitive data, UDP lacks reliability and congestion control, and cellular networks designed for terrestrial users degrade severely for aerial platforms. This paper introduces AQUILA, a cross-layer communication architecture built on QUIC to address these challenges. AQUILA contributes three key innovations: (1) a unified transport layer using QUIC's reliable streams for MAVLink Command and Control (C2) and unreliable datagrams for video, eliminating head-of-line blocking under unified congestion control; (2) a priority scheduling mechanism that structurally ensures C2 latency remains bounded and independent of video traffic intensity; (3) a UAV-adapted congestion control algorithm extending SCReAM with altitude-adaptive delay targeting and telemetry headroom reservation. AQUILA further implements 0-RTT connection resumption to minimize handover blackouts with application-layer replay protection, deployed over an IP-native architecture enabling global operation. Experimental validation demonstrates that AQUILA significantly outperforms TCP- and UDP-based approaches in C2 latency, video quality, and link resilience under realistic conditions, providing a robust foundation for autonomous BVLOS missions.", "AI": {"tldr": "提出AQUILA架构，基于QUIC协议为长距离无人机通信提供弹性的高带宽低延迟链路解决方案", "motivation": "现有解决方案存在关键限制：TCP存在队头阻塞问题会延迟时间敏感数据，UDP缺乏可靠性和拥塞控制，为地面用户设计的蜂窝网络在航空平台上性能严重下降", "method": "基于QUIC构建跨层通信架构，包含三个关键创新：1) 使用QUIC可靠流处理MAVLink C2命令和不稳定数据报处理视频的统一传输层；2) 优先级调度机制确保C2延迟有界且独立于视频流量强度；3) 无人机自适应拥塞控制算法扩展SCReAM，具有高度自适应延迟目标和遥测余量预留", "result": "实验验证表明，AQUILA在C2延迟、视频质量和链路弹性方面显著优于基于TCP和UDP的方法，为自主BVLOS任务提供稳健基础", "conclusion": "AQUILA通过基于QUIC的跨层架构解决了长距离无人机通信的关键挑战，提供了可靠、高带宽、低延迟的通信解决方案"}}
{"id": "2512.06888", "pdf": "https://arxiv.org/pdf/2512.06888", "abs": "https://arxiv.org/abs/2512.06888", "authors": ["Liyang Song", "Hardik Bishnoi", "Sai Kumar Reddy Manne", "Sarah Ostadabbas", "Briana J. Taylor", "Michael Wan"], "title": "Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation", "categories": ["cs.CV"], "comment": null, "summary": "The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.", "AI": {"tldr": "开发首个可复现的婴儿呼吸估计系统，解决婴儿视频数据稀缺问题", "motivation": "婴儿无接触呼吸监测对早期发现呼吸异常至关重要，但现有计算机视觉算法和数据集主要针对成人，缺乏婴儿专用的可复现解决方案", "method": "提出AIR-400数据集（400个标注视频），开发基于婴儿特定感兴趣区域检测和光流增强时空神经处理的呼吸估计算法", "result": "建立了首个可复现的婴儿呼吸估计基准，提供了公开的数据集、代码库和训练模型", "conclusion": "成功克服了婴儿呼吸监测中的小数据限制，为婴儿无接触呼吸监测提供了首个可复现的解决方案和基准"}}
{"id": "2512.06886", "pdf": "https://arxiv.org/pdf/2512.06886", "abs": "https://arxiv.org/abs/2512.06886", "authors": ["Wangkai Li", "Rui Sun", "Bohao Liao", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Balanced Learning for Domain Adaptive Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by International Conference on Machine Learning (ICML 2025)", "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.", "AI": {"tldr": "提出一种平衡学习方法BLDA来解决域自适应语义分割中的类别不平衡问题，通过分析预测logits分布来识别过预测和欠预测类别，并使用共享锚分布对齐不同类别的logits分布。", "motivation": "在无监督域自适应语义分割中，自训练方法由于域间数据和标签空间的固有类别不平衡和分布偏移，难以平衡地学习每个类别。现有方法缺乏直接评估和缓解类别偏差的机制。", "method": "首先通过分析预测logits分布识别过预测和欠预测类别；然后使用共享锚分布进行后处理对齐不同类别的logits分布；在线估计logits分布并将logits校正项融入损失函数；利用累积密度作为域共享结构知识连接源域和目标域。", "result": "在两个标准UDA语义分割基准上的广泛实验表明，BLDA在集成到各种现有方法中时能持续提升性能，特别是对于欠预测类别。代码已开源。", "conclusion": "BLDA通过直接评估和缓解类别偏差，无需先验知识即可有效解决域自适应语义分割中的类别不平衡问题，显著提升欠预测类别的性能。"}}
{"id": "2512.06885", "pdf": "https://arxiv.org/pdf/2512.06885", "abs": "https://arxiv.org/abs/2512.06885", "authors": ["Wancheng Feng", "Chen An", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Lukun Wang"], "title": "JoPano: Unified Panorama Generation via Joint Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/VIPL-GENUN/JoPano", "summary": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.", "AI": {"tldr": "JoPano提出了一种基于DiT的统一全景图生成方法，通过联合建模将文本到全景图和视角到全景图两个核心任务整合到单一模型中，解决了现有方法在视觉质量和建模冗余方面的问题。", "motivation": "现有全景图生成方法面临两大挑战：基于U-Net的架构限制了生成全景图的视觉质量；通常将文本到全景图和视角到全景图两个任务独立处理，导致建模冗余和效率低下。", "method": "1. 提出基于DiT的联合全景图生成方法；2. 设计基于立方体贴图表示的联合面适配器，将预训练DiT的生成能力迁移到全景图领域；3. 应用泊松融合减少立方体面边界处的接缝不一致；4. 引入接缝一致性评估指标Seam-SSIM和Seam-Sobel；5. 提出条件切换机制统一两个任务。", "result": "JoPano在文本到全景图和视角到全景图生成任务上都能生成高质量全景图，在FID、CLIP-FID、IS和CLIP-Score等指标上达到最先进性能，并通过新提出的接缝一致性指标验证了方法的有效性。", "conclusion": "JoPano通过联合建模方法成功统一了全景图生成的两个核心任务，利用DiT架构和创新的适配器设计显著提升了生成质量，同时减少了建模冗余，为全景图生成提供了高效统一的解决方案。"}}
{"id": "2512.06882", "pdf": "https://arxiv.org/pdf/2512.06882", "abs": "https://arxiv.org/abs/2512.06882", "authors": ["Yu Zhu", "Naoya Chiba", "Koichi Hashimoto"], "title": "Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion", "categories": ["cs.CV"], "comment": "Accepted to BMVC 2025 (Sheffield, UK, Nov 24-27, 2025). Supplementary video and poster available upon request", "summary": "Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.", "AI": {"tldr": "提出了一种用于工业场景的分层图像引导3D点云分割框架，通过多视图贝叶斯融合逐步从实例级到部件级细化分割", "motivation": "工业场景中密集布局和多尺度物体导致严重的遮挡问题，削弱了几何边界；现有3D点云方法需要昂贵标注，而图像引导方法存在跨视图语义不一致问题", "method": "采用分层方法：1) 实例分割：渲染俯视图，使用YOLO-World提示SAM生成掩码并反向投影到3D点云；2) 部件级分割：对每个实例渲染多视图图像，在每个视图应用相同2D分割和反向投影，然后通过贝叶斯更新融合确保跨视图语义一致性", "result": "在真实工厂数据上的实验表明，该方法能有效处理遮挡和结构复杂性，获得一致高的每类mIoU分数；在公共数据集上的额外评估证实了框架的泛化能力", "conclusion": "该方法展示了处理工业场景中遮挡和多尺度挑战的鲁棒性、标注效率和适应性，通过分层图像引导和贝叶斯融合实现了可靠的3D分割"}}
{"id": "2512.06879", "pdf": "https://arxiv.org/pdf/2512.06879", "abs": "https://arxiv.org/abs/2512.06879", "authors": ["Li Ju", "Jun Zhao", "Mingxu Chai", "Ziyu Shen", "Xiangyang Wang", "Yage Geng", "Chunchun Ma", "Hao Peng", "Guangbin Li", "Tao Li", "Chengyong Liao", "Fu Wang", "Xiaolong Wang", "Junshen Chen", "Rui Gong", "Shijia Liang", "Feiyan Li", "Ming Zhang", "Kexin Tan", "Jujie Ye", "Zhiheng Xi", "Shihan Dou", "Tao Gui", "Yuankai Ying", "Yang Shi", "et al. (2 additional authors not shown)"], "title": "WisPaper: Your AI Scholar Search Engine", "categories": ["cs.IR", "cs.AI"], "comment": "17 pages, 2 figures", "summary": "Researchers struggle to efficiently locate and manage relevant literature within the exponentially growing body of scientific publications. We present \\textsc{WisPaper}, an intelligent academic retrieval and literature management platform that addresses this challenge through three integrated capabilities: (1) \\textit{Scholar Search}, featuring both quick keyword-based and deep agentic search modes for efficient paper discovery; (2) \\textit{Library}, a customizable knowledge base for systematic literature organization; and (3) \\textit{AI Feeds}, an intelligent recommendation system that automatically delivers relevant new publications based on user interests. Unlike existing academic tools, \\textsc{WisPaper} provides a closed-loop workflow that seamlessly connects literature discovery, management, and continuous tracking of research frontiers. Our multilingual and multidisciplinary system significantly reduces the time researchers from diverse backgrounds spend on paper screening and management, enabling them to focus on their core research activities. The platform is publicly accessible and serves researchers across academia and industry.", "AI": {"tldr": "WisPaper是一个智能学术检索与文献管理平台，集成了学者搜索、文献库和AI推荐三大功能，为研究人员提供从文献发现到管理的闭环工作流程。", "motivation": "随着科学出版物呈指数级增长，研究人员难以高效定位和管理相关文献。现有学术工具无法提供从文献发现到管理的无缝集成工作流程。", "method": "WisPaper通过三个集成功能实现：1) 学者搜索（快速关键词搜索和深度智能搜索模式）；2) 文献库（可定制的知识库用于系统化文献组织）；3) AI推荐（基于用户兴趣自动推送相关新出版物的智能推荐系统）。", "result": "WisPaper显著减少了来自不同背景的研究人员在文献筛选和管理上花费的时间，使他们能够专注于核心研究活动。该平台支持多语言和多学科，已公开可用并服务于学术界和工业界的研究人员。", "conclusion": "WisPaper提供了一个独特的闭环工作流程，无缝连接文献发现、管理和研究前沿的持续追踪，解决了研究人员在文献管理方面的核心痛点。"}}
{"id": "2512.06877", "pdf": "https://arxiv.org/pdf/2512.06877", "abs": "https://arxiv.org/abs/2512.06877", "authors": ["Mohammed Q. Alkhatib", "Ali Jamali", "Swalpa Kumar Roy"], "title": "SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification", "categories": ["cs.CV"], "comment": "Accepted and presented in ICSPIS", "summary": "Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer", "AI": {"tldr": "提出了一种基于卷积混合器范式的轻量级架构SceneMixer，用于遥感场景分类任务，在保持参数和计算量较低的同时有效提取局部和上下文信息。", "motivation": "遥感场景分类在土地利用和土地覆盖识别中至关重要，但现有CNN和ViT模型由于空间分辨率、视角、方向和背景条件的变化而泛化能力有限，需要更高效且泛化能力强的模型。", "method": "采用卷积混合器架构，通过多尺度深度卷积进行空间混合，通过逐点操作进行通道混合，交替执行这两种操作，实现局部和上下文信息的高效提取。", "result": "在AID数据集上获得74.7%总体准确率、74.57%平均准确率和73.79 Kappa值；在EuroSAT数据集上获得93.90%总体准确率、93.93%平均准确率和93.22 Kappa值，在准确率和效率之间取得了良好平衡。", "conclusion": "SceneMixer模型在遥感场景分类任务中表现出色，相比广泛使用的CNN和基于Transformer的模型，提供了更好的准确率和效率平衡，代码将公开可用。"}}
{"id": "2512.06870", "pdf": "https://arxiv.org/pdf/2512.06870", "abs": "https://arxiv.org/abs/2512.06870", "authors": ["Wangkai Li", "Rui Sun", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective", "categories": ["cs.CV"], "comment": "Accepted by Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.", "AI": {"tldr": "提出ECOCSeg方法，从编码角度改进语义分割中的伪标签学习，使用纠错输出码(ECOC)为每个类别创建细粒度编码，提高伪标签学习的鲁棒性", "motivation": "伪标签学习在语义分割中广泛应用，特别是在标签稀缺场景如无监督域适应和半监督学习。然而，这种范式会产生错误的伪标签，并且由于使用one-hot编码，这些错误在训练过程中会被进一步放大", "method": "提出ECOCSeg方法：1) 引入基于ECOC的分类器，将类别解耦为属性并处理部分不准确的比特位；2) 开发比特级标签去噪机制，生成更高质量的伪标签；3) 可轻松集成到现有方法中", "result": "在多个UDA和SSL基准测试中，ECOCSeg与不同分割架构结合都表现出显著改进，证明了方法的有效性和泛化能力", "conclusion": "ECOCSeg从编码角度为语义分割中的伪标签学习提供了新视角，通过ECOC编码提高了伪标签学习的鲁棒性和稳定性，代码已开源"}}
{"id": "2512.06868", "pdf": "https://arxiv.org/pdf/2512.06868", "abs": "https://arxiv.org/abs/2512.06868", "authors": ["Xingguang Zhong", "Liren Jin", "Marija Popović", "Jens Behley", "Cyrill Stachniss"], "title": "Dynamic Visual SLAM using a General 3D Prior", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages", "summary": "Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.", "AI": {"tldr": "提出一种新颖的单目视觉SLAM系统，能够在动态场景中鲁棒地估计相机位姿，通过结合几何补丁的在线束调整和前馈重建模型的优势来处理动态环境。", "motivation": "动态自然环境中的场景动态会严重降低相机位姿估计的准确性，而可靠的增量式相机位姿估计和3D重建对于机器人、交互式可视化和增强现实等应用至关重要。", "method": "提出一个前馈重建模型来精确过滤动态区域，同时利用其深度预测增强基于补丁的视觉SLAM的鲁棒性。通过将深度预测与束调整估计的补丁对齐，鲁棒地处理前馈重建模型批量应用时的尺度模糊性。", "result": "该方法能够在动态场景中鲁棒地估计相机位姿，通过结合几何补丁的在线束调整和前馈重建模型的互补优势，有效处理动态区域并解决尺度模糊问题。", "conclusion": "提出的单目视觉SLAM系统通过整合几何束调整和前馈重建模型，在动态自然环境中实现了鲁棒的相机位姿估计，为动态场景下的视觉SLAM提供了有效的解决方案。"}}
{"id": "2512.06867", "pdf": "https://arxiv.org/pdf/2512.06867", "abs": "https://arxiv.org/abs/2512.06867", "authors": ["John Licato", "Stephen Steinle", "Brayden Hollis"], "title": "Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?", "categories": ["cs.AI"], "comment": "Accepted at IJCNLP-AACL 2025", "summary": "Although persona prompting in large language models appears to trigger different styles of generated text, it is unclear whether these translate into measurable behavioral differences, much less whether they affect decision-making in an adversarial strategic environment that we provide as open-source. We investigate the impact of persona prompting on strategic performance in PERIL, a world-domination board game. Specifically, we compare the effectiveness of persona-derived heuristic strategies to those chosen manually. Our findings reveal that certain personas associated with strategic thinking improve game performance, but only when a mediator is used to translate personas into heuristic values. We introduce this mediator as a structured translation process, inspired by exploratory factor analysis, that maps LLM-generated inventory responses into heuristics. Results indicate our method enhances heuristic reliability and face validity compared to directly inferred heuristics, allowing us to better study the effect of persona types on decision making. These insights advance our understanding of how persona prompting influences LLM-based decision-making and propose a heuristic generation method that applies psychometric principles to LLMs.", "AI": {"tldr": "研究人格提示对大型语言模型在战略推理游戏PERIL中表现的影响，并开发了一种将人格映射为启发式策略的翻译方法", "motivation": "虽然人格提示似乎能触发不同风格的文本生成，但尚不清楚这些差异是否能转化为可测量的行为差异，特别是在对抗性战略环境中。本研究旨在探索人格提示如何影响LLM在战略决策中的表现。", "method": "使用PERIL（世界统治棋盘游戏）作为测试平台，比较人格衍生的启发式策略与手动选择的策略。引入基于探索性因子分析原理的结构化翻译过程作为中介器，将LLM生成的库存响应映射为启发式值。", "result": "某些与战略思维相关的人格确实能提高游戏表现，但前提是使用中介器将人格翻译为启发式值。该方法相比直接推断的启发式方法，提高了启发式的可靠性和表面效度。", "conclusion": "人格提示确实影响LLM在战略环境中的决策表现，但需要通过结构化的翻译过程才能有效发挥作用。提出的启发式生成方法将心理测量学原理应用于LLM，为研究人格类型对决策的影响提供了更好的工具。"}}
{"id": "2512.06866", "pdf": "https://arxiv.org/pdf/2512.06866", "abs": "https://arxiv.org/abs/2512.06866", "authors": ["Yulin Li", "Haokun Gui", "Ziyang Fan", "Junjie Wang", "Bin Kang", "Bin Chen", "Zhuotao Tian"], "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .", "AI": {"tldr": "提出DyToK方法，通过LLM引导的关键帧先验实现动态令牌压缩，解决视频大语言模型中长视频序列带来的计算效率瓶颈问题", "motivation": "视频大语言模型在处理长视频时面临计算效率瓶颈，现有关键帧采样方法在特征编码前引入额外计算成本，且二值帧选择范式不够优化", "method": "利用VLLM固有的注意力机制，通过分析发现注意力层自然编码了查询条件的关键帧先验，从而动态调整每帧令牌保留比例，优先保留语义丰富的帧并抑制冗余", "result": "DyToK实现了最先进的效率-准确性权衡，与现有压缩方法兼容，在LLaVA-OneVision和Qwen2.5-VL等多个VLLM上实现4.3倍推理加速同时保持准确性", "conclusion": "DyToK是一种无需训练的动态令牌压缩范式，通过利用VLLM内在的注意力机制实现高效视频理解，在保持准确性的同时显著提升推理效率"}}
{"id": "2512.06865", "pdf": "https://arxiv.org/pdf/2512.06865", "abs": "https://arxiv.org/abs/2512.06865", "authors": ["Xiaosong Jia", "Chenhe Zhang", "Yule Jiang", "Songbur Wong", "Zhiyuan Zhang", "Chen Chen", "Shaofeng Zhang", "Xuanhe Zhou", "Xue Yang", "Junchi Yan", "Yu-Gang Jiang"], "title": "Spatial Retrieval Augmented Autonomous Driving", "categories": ["cs.CV"], "comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints", "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks. For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.", "AI": {"tldr": "提出空间检索增强自动驾驶范式，通过引入离线检索的地理图像作为额外输入，扩展自动驾驶系统的感知能力", "motivation": "现有自动驾驶系统依赖车载传感器，受限于实时感知范围，在视野受限、遮挡或极端天气条件下表现不佳。人类驾驶员能够在能见度差时回忆道路结构，因此希望赋予模型这种\"回忆\"能力", "method": "提出空间检索范式，从离线缓存（如Google Maps或存储的自动驾驶数据集）获取地理图像作为额外输入。扩展nuScenes数据集，通过Google Maps API检索地理图像并与自车轨迹对齐", "result": "在五个核心自动驾驶任务（目标检测、在线建图、占据预测、端到端规划、生成式世界建模）上建立基线，实验表明扩展的模态能够提升某些任务的性能", "conclusion": "空间检索范式为现有自动驾驶任务提供即插即用的扩展，能够增强系统在受限条件下的感知能力，并将开源数据集、代码和基准测试供进一步研究"}}
{"id": "2512.06864", "pdf": "https://arxiv.org/pdf/2512.06864", "abs": "https://arxiv.org/abs/2512.06864", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026. arXiv admin note: substantial text overlap with arXiv:2508.19808", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 $\\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "提出AutoQ-VIS框架，通过质量引导的自训练方法提升无监督视频实例分割性能，无需人工标注即可在真实视频上实现SOTA效果。", "motivation": "视频实例分割面临像素级掩码和时间一致性标注的双重挑战。现有无监督方法依赖合成数据但仍受限于合成到真实的域差距，需要一种能自动适应真实视频的方法。", "method": "建立伪标签生成和自动质量评估的闭环系统，通过质量引导的自训练实现从合成视频到真实视频的渐进式适应，无需人工标注。", "result": "在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA方法VideoCutLER提升4.4%，且无需任何人工标注。", "conclusion": "质量感知的自训练方法对于无监督视频实例分割是可行的，AutoQ-VIS通过自动质量评估和渐进式自训练成功弥合了合成到真实的域差距。"}}
{"id": "2512.06862", "pdf": "https://arxiv.org/pdf/2512.06862", "abs": "https://arxiv.org/abs/2512.06862", "authors": ["Qiancheng Zheng", "Yunhang Shen", "Gen Luo", "Baiyang Song", "Xing Sun", "Xiaoshuai Sun", "Yiyi Zhou", "Rongrong Ji"], "title": "Omni-Referring Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.", "AI": {"tldr": "提出Omni-Referring Image Segmentation (OmniRIS)任务，支持文本指令和带有掩码、框或涂鸦的参考图像作为全模态提示，实现高度泛化的图像分割。", "motivation": "现有单模态条件分割任务（如RIS和视觉RIS）存在局限性，需要一种能同时利用文本和视觉模态优势的方法，既能进行细粒度属性指代，又能处理不常见对象定位。", "method": "提出OmniSegNet基线模型来处理OmniRIS的关键挑战，如全模态提示编码；构建大型数据集OmniRef（包含30,956张图像的186,939个全模态提示）并建立综合评估系统。", "result": "实验验证了OmniSegNet能够有效遵循全模态指令，并展示了OmniRIS在高度泛化图像分割方面的优越性。", "conclusion": "OmniRIS通过支持文本和视觉模态的全模态提示输入，实现了更灵活、更通用的图像分割，为高度泛化的图像分割任务提供了新的研究方向。"}}
{"id": "2512.06859", "pdf": "https://arxiv.org/pdf/2512.06859", "abs": "https://arxiv.org/abs/2512.06859", "authors": ["Ce Chi", "Xing Wang", "Zhendong Wang", "Xiaofan Liu", "Ce Li", "Zhiyan Song", "Chen Zhao", "Kexin Yang", "Boshen Shi", "Jingjing Yang", "Chao Deng", "Junlan Feng"], "title": "JT-DA: Enhancing Data Analysis with Tool-Integrated Table Reasoning Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we present JT-DA-8B (JiuTian Data Analyst 8B), a specialized large language model designed for complex table reasoning tasks across diverse real-world scenarios. To address the lack of high-quality supervision in tabular reasoning scenarios, we construct a comprehensive and diverse training corpus with 34 well-defined table reasoning tasks, by aggregating 29 public table QA datasets and 3 million tables. An automatic pipeline is proposed to generate realistic multi-step analytical tasks involving reasoning patterns. The model is trained upon open-source JT-Coder-8B model, an 8B-parameter decoder-only foundation model trained from scratch. In the training stage, we leverage LLM-based scoring and workflow-aligned filtering to distill high-quality, table-centric data. Both supervised fine-tuning (SFT) and Reinforcement learning (RL) are adopted to optimize our model. Afterwards, a four-stage table reasoning workflow is proposed, including table preprocessing, table sensing, tool-integrated reasoning, and prompt engineering, to improve model interpretability and execution accuracy. Experimental results show that JT-DA-8B achieves strong performance in various table reasoning tasks, demonstrating the effectiveness of data-centric generation and workflow-driven optimization.", "AI": {"tldr": "JT-DA-8B是一个专门为复杂表格推理任务设计的大型语言模型，通过工具集成和四阶段工作流程提升数据分析和表格推理能力", "motivation": "解决表格推理场景中高质量监督数据缺乏的问题，以及提升模型在多样化真实世界场景中的表格推理能力", "method": "1) 构建包含34个表格推理任务的综合训练语料库，整合29个公开表格QA数据集和300万张表格；2) 提出自动管道生成现实多步分析任务；3) 基于开源的JT-Coder-8B模型，采用监督微调和强化学习优化；4) 设计四阶段表格推理工作流程：表格预处理、表格感知、工具集成推理和提示工程", "result": "JT-DA-8B在各种表格推理任务中表现出色，验证了数据为中心的数据生成和工作流程驱动优化的有效性", "conclusion": "通过高质量数据生成、工具集成和工作流程优化，JT-DA-8B成功提升了表格推理能力，为复杂数据分析任务提供了有效的解决方案"}}
{"id": "2512.06854", "pdf": "https://arxiv.org/pdf/2512.06854", "abs": "https://arxiv.org/abs/2512.06854", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Shang Liu", "Zhiyao Xie"], "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design", "categories": ["cs.AR", "cs.AI"], "comment": "Published in NeurIPS'25 Dataset and Benchmark Track", "summary": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.", "AI": {"tldr": "ArchPower是首个用于现代CPU设计架构级功耗建模的开源数据集，包含200个CPU数据样本，具有100多个架构特征和细粒度功耗标签。", "motivation": "CPU功耗是集成电路设计的主要目标，但传统功耗模型在架构级不准确，而基于ML的模型缺乏开源数据集。现有数据集生成过程复杂耗时，且私有数据集往往无法反映真实CPU设计场景。", "method": "通过复杂且真实的设计流程收集CPU架构信息作为特征，使用模拟功耗作为真实标签。数据集包含25种不同CPU配置执行8种工作负载时的数据，每个样本有100多个硬件和事件参数特征。", "result": "创建了包含200个CPU数据样本的数据集，每个样本提供细粒度功耗信息：总设计功耗和11个组件的功耗，每个功耗值进一步分解为组合逻辑、时序逻辑、存储器和时钟功耗四个细粒度组。", "conclusion": "ArchPower填补了架构级处理器功耗建模领域开源数据集的空白，为ML-based功耗模型研究提供了重要资源，有助于加速CPU设计流程中的功耗评估。"}}
{"id": "2512.06850", "pdf": "https://arxiv.org/pdf/2512.06850", "abs": "https://arxiv.org/abs/2512.06850", "authors": ["Hansa Mohanty", "Vaisakh Naduvodi Viswambharan", "Deepak Narayan Gadde"], "title": "Formal that \"Floats\" High: Formal Verification of Floating Point Arithmetic", "categories": ["cs.LO", "cs.AI", "cs.AR"], "comment": "To appear at the 37th IEEE International Conference on Microelectronics (ICM), December 14-17, 2025, Cairo, Egypt", "summary": "Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.", "AI": {"tldr": "提出一种可扩展的浮点运算形式验证方法，采用直接RTL-to-RTL模型检查，结合分治策略和AI驱动的属性生成", "motivation": "浮点运算形式验证面临非线性算术行为和控制-数据路径紧密耦合的挑战，现有基于C模型的方法存在抽象差距、翻译开销和可扩展性限制", "method": "采用直接RTL-to-RTL模型检查，分治策略分解验证为模块化阶段，使用反例引导的细化迭代定位缺陷，结合基于代理AI的形式属性生成和人工在环优化", "result": "直接RTL-to-RTL模型检查比独立验证实现更高的覆盖率效率，需要更少的断言，特别是结合AI生成属性并通过人工在环优化时效果更佳", "conclusion": "该方法为浮点运算验证提供了可扩展的解决方案，通过直接RTL验证减少抽象差距，结合AI自动化提高验证效率，展示了实际应用潜力"}}
{"id": "2512.06849", "pdf": "https://arxiv.org/pdf/2512.06849", "abs": "https://arxiv.org/abs/2512.06849", "authors": ["Matan Atad", "Alexander W. Marka", "Lisa Steinhelfer", "Anna Curto-Vilalta", "Yannik Leonhardt", "Sarah C. Foreman", "Anna-Sophia Walburga Dietrich", "Robert Graf", "Alexandra S. Gersing", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke", "Hendrik Möller"], "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT", "categories": ["cs.CV", "cs.LG"], "comment": "In submission", "summary": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.", "AI": {"tldr": "提出一种弱监督方法，仅使用椎体级别的健康/恶性标签（无需病灶掩码）来分割CT中的椎体转移灶", "motivation": "CT中椎体转移灶的准确分割在临床上很重要但难以规模化，因为体素级标注稀缺，且溶骨性和成骨性病变常与良性退行性改变相似", "method": "结合扩散自编码器（DAE）生成椎体的健康编辑版本，通过像素级差异图提出候选病灶区域，引入\"捉迷藏归因\"方法：依次揭示每个候选区域同时隐藏其他区域，将编辑后的图像投影回数据流形，通过潜在空间分类器量化该组件的独立恶性贡献", "result": "在保留的放射科医生标注上，尽管没有掩码监督，仍实现了强大的成骨性/溶骨性性能（F1: 0.91/0.85; Dice: 0.87/0.78），超过基线方法（F1: 0.79/0.67; Dice: 0.74/0.55）", "conclusion": "椎体级别的标签可以转化为可靠的病灶掩码，表明生成式编辑结合选择性遮挡支持在CT中进行准确的弱监督分割"}}
{"id": "2512.06848", "pdf": "https://arxiv.org/pdf/2512.06848", "abs": "https://arxiv.org/abs/2512.06848", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim", "Hermansyah"], "title": "AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices", "categories": ["cs.CL", "cs.CV"], "comment": "9Pages, 3 figure, Politeknik Negeri Banyuwangi", "summary": "Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% mAP@0.5 and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.", "AI": {"tldr": "开发轻量级视觉传感器融合框架，用于边缘设备上的实时病原体检测和水质异常预测", "motivation": "低收入和中等收入地区的小规模饮用水系统中微生物污染波动迅速，现有监测工具只能捕捉部分信息，操作员需要分别解释显微镜图像和物理化学传感器数据，导致实时决策不可靠", "method": "提出AquaFusionNet轻量级跨模态框架，通过门控交叉注意力机制学习微生物外观与传感器动态之间的统计依赖关系，专门为低功耗硬件设计", "result": "在印度尼西亚东爪哇7个设施部署6个月，处理184万帧图像，污染事件检测mAP@0.5达94.8%，异常预测准确率96.3%，在Jetson Nano上功耗仅4.8W", "conclusion": "AquaFusionNet在可比或更低功耗下提供更高准确性，跨模态耦合减少了单模态检测器的常见故障模式，特别是在污垢、浊度峰值和不一致照明条件下"}}
{"id": "2512.06845", "pdf": "https://arxiv.org/pdf/2512.06845", "abs": "https://arxiv.org/abs/2512.06845", "authors": ["Satoshi Hashimoto", "Hitoshi Nishimura", "Yanan Wang", "Mori Kurokawa"], "title": "Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.", "AI": {"tldr": "提出PA-VAD方法，仅使用真实正常视频和合成的伪异常视频来训练视频异常检测模型，无需收集真实异常视频", "motivation": "实际部署视频异常检测面临真实异常视频稀缺且收集成本高的问题，需要一种无需真实异常视频的训练方法", "method": "使用CLIP选择类别相关初始图像，通过视觉语言模型优化文本提示，调用视频扩散模型生成伪异常视频；训练时采用领域对齐正则化模块减少合成异常的过度时空幅度", "result": "在ShanghaiTech数据集上达到98.2%，在UCF-Crime上达到82.5%，分别超过最强真实异常方法0.6%和超过UVAD SOTA方法1.9%", "conclusion": "无需收集真实异常视频即可实现高精度异常检测，为可扩展部署提供了实用路径"}}
{"id": "2512.06840", "pdf": "https://arxiv.org/pdf/2512.06840", "abs": "https://arxiv.org/abs/2512.06840", "authors": ["Satoshi Hashimoto", "Tatsuya Konishi", "Tomoya Kaichi", "Kazunori Matsumoto", "Mori Kurokawa"], "title": "CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026", "summary": "Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the \"incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.", "AI": {"tldr": "提出CADE方法，首次将持续学习与弱监督视频异常检测相结合，解决数据领域变化下的异常检测问题", "motivation": "现有弱监督视频异常检测方法主要处理静态数据集，忽略了数据领域可能变化的问题。当数据领域发生变化时，仅用新数据训练会导致对先前数据的性能下降（遗忘），因此需要持续学习视角", "method": "提出CADE方法：1）使用双生成器处理数据不平衡和标签不确定性；2）提出集成多判别器来捕捉因遗忘而错过的异常模式；3）通过多个模型集成来捕获过去场景中因遗忘而遗漏的异常", "result": "在常见的多场景视频异常检测数据集（如ShanghaiTech和Charlotte Anomaly数据集）上，CADE显著优于现有的视频异常检测方法", "conclusion": "CADE是首个将持续学习与弱监督视频异常检测相结合的工作，通过双生成器和集成多判别器有效解决了数据领域变化下的异常检测问题，防止了遗忘并提高了检测性能"}}
{"id": "2512.06838", "pdf": "https://arxiv.org/pdf/2512.06838", "abs": "https://arxiv.org/abs/2512.06838", "authors": ["Jiahao Wang", "Zhongwei Jiang", "Wenchao Sun", "Jiaru Zhong", "Haibao Yu", "Yuner Zhang", "Chenyang Lu", "Chuang Zhang", "Lei He", "Shaobing Xu", "Jianqiang Wang"], "title": "SparseCoop: Cooperative Perception with Kinematic-Grounded Queries", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026", "summary": "Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.", "AI": {"tldr": "提出SparseCoop，一种完全稀疏的协同感知框架，用于3D检测和跟踪，完全摒弃中间BEV表示，通过运动学基础实例查询实现精确时空对齐", "motivation": "当前协同感知方法存在通信成本高、灵活性差、对齐不精确等问题，而稀疏查询方法又存在几何表示不足、融合策略次优和训练不稳定等缺陷", "method": "采用运动学基础实例查询（包含3D几何和速度的显式状态向量）、粗到细聚合模块实现鲁棒融合、协同实例去噪任务加速稳定训练", "result": "在V2X-Seq和Griffin数据集上达到最先进性能，具有优越的计算效率、低传输成本和强通信延迟鲁棒性", "conclusion": "SparseCoop通过完全稀疏的协同感知框架，在保持高性能的同时显著降低了通信成本，提高了系统鲁棒性和可解释性"}}
{"id": "2512.06836", "pdf": "https://arxiv.org/pdf/2512.06836", "abs": "https://arxiv.org/abs/2512.06836", "authors": ["Weixing Zhang", "Regina Hebig", "Daniel Strüber"], "title": "Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.", "AI": {"tldr": "探索利用大型语言模型（LLM）支持文本领域特定语言（DSL）定义与实例之间的协同演化，重点关注在语法变更时如何保持实例中的注释和布局等辅助信息。", "motivation": "软件语言会因各种原因（如新增功能）而演化。当语言的语法定义演化时，原本符合语法的文本实例会变得过时。在模型驱动工程中，已有多种技术支持模型与元模型的协同演化，但这些技术不适用于具有文本语法的DSL——将其应用于文本语言定义和实例可能导致原始实例中注释和布局等有价值信息的丢失，这些信息对软件理解和维护至关重要。", "method": "应用两种先进的语言模型（Claude-3.5和GPT-4o），在七个案例语言上进行实验，评估LLM在直接处理文本实例时实现语法和实例协同演化的可行性和局限性。", "result": "结果表明，在实例规模有限的小规模案例中，所考虑的LLM在迁移文本实例方面表现出良好能力，这些案例代表了实践中遇到的部分情况。同时观察到LLM解决方案在扩展到更大实例时面临显著挑战。", "conclusion": "LLM在支持文本DSL定义与实例协同演化方面具有潜力，特别是在保持辅助信息方面，但可扩展性问题是当前的主要限制，这些发现为未来研究提供了有价值的见解。"}}
{"id": "2512.06835", "pdf": "https://arxiv.org/pdf/2512.06835", "abs": "https://arxiv.org/abs/2512.06835", "authors": ["Tingyu Li", "Zheng Sun", "Jingxuan Wei", "Siyuan Li", "Conghui He", "Lijun Wu", "Cheng Tan"], "title": "Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning", "categories": ["cs.AI"], "comment": "25 pages, 5 figures", "summary": "Recent vision-language models (VLMs) achieve remarkable reasoning through reinforcement learning (RL), which provides a feasible solution for realizing continuous self-evolving large vision-language models (LVLMs) in the era of experience. However, RL for VLMs requires abundant high-quality multimodal data, especially challenging in specialized domains like chemistry, earth sciences, and multimodal mathematics. Existing strategies such as synthetic data and self-rewarding mechanisms suffer from limited distributions and alignment difficulties, ultimately causing reward hacking: models exploit high-reward patterns, collapsing policy entropy and destabilizing training. We propose DoGe (Decouple to Generalize), a dual-decoupling framework that guides models to first learn from context rather than problem solving by refocusing on the problem context scenarios overlooked by synthetic data methods. By decoupling learning process into dual components (Thinker and Solver), we reasonably quantify the reward signals of this process and propose a two-stage RL post-training approach from freely exploring context to practically solving tasks. Second, to increase the diversity of training data, DoGe constructs an evolving curriculum learning pipeline: an expanded native domain knowledge corpus and an iteratively evolving seed problems pool. Experiments show that our method consistently outperforms the baseline across various benchmarks, providing a scalable pathway for realizing self-evolving LVLMs.", "AI": {"tldr": "提出DoGe框架，通过解耦学习过程实现数据稀缺场景下的视觉-语言推理模型自我进化", "motivation": "现有视觉语言模型在专业领域（如化学、地球科学、多模态数学）面临高质量多模态数据稀缺问题，传统合成数据和自奖励机制存在分布有限和对齐困难，导致奖励黑客问题", "method": "提出双解耦框架：1) 将学习过程解耦为思考者（Thinker）和解决者（Solver），引导模型从上下文学习而非直接解决问题；2) 构建进化课程学习管道，包括扩展的本领域知识语料库和迭代进化的种子问题池", "result": "实验表明该方法在各种基准测试中持续优于基线，为自我进化的大型视觉语言模型提供了可扩展的路径", "conclusion": "DoGe框架通过解耦学习过程和进化课程学习，有效解决了数据稀缺场景下的视觉-语言推理问题，为专业领域的自我进化模型提供了可行方案"}}
{"id": "2512.06834", "pdf": "https://arxiv.org/pdf/2512.06834", "abs": "https://arxiv.org/abs/2512.06834", "authors": ["Zhiguang Zhou", "Ruiqi Yu", "Yuming Ma", "Hao Ni", "Guojun Li", "Li Ye", "Xiaoying Wang", "Yize Li", "Yong Wang"], "title": "COIVis: Eye tracking-based Visual Exploration of Concept Learning in MOOC Videos", "categories": ["cs.HC", "cs.GR"], "comment": "18pages, 5 figures", "summary": "Massive Open Online Courses (MOOCs) make high-quality instruction accessible. However, the lack of face-to-face interaction makes it difficult for instructors to obtain feedback on learners' performance and provide more effective instructional guidance. Traditional analytical approaches, such as clickstream logs or quiz scores, capture only coarse-grained learning outcomes and offer limited insight into learners' moment-to-moment cognitive states. In this study, we propose COIVis, an eye tracking-based visual analytics system that supports concept-level exploration of learning processes in MOOC videos. COIVis first extracts course concepts from multimodal video content and aligns them with the temporal structure and screen space of the lecture, defining Concepts of Interest (COIs), which anchor abstract concepts to specific spatiotemporal regions. Learners' gaze trajectories are transformed into COI sequences, and five interpretable learner-state features -- Attention, Cognitive Load, Interest, Preference, and Synchronicity -- are computed at the COI level based on eye tracking metrics. Building on these representations, COIVis provides a narrative, multi-view visualization enabling instructors to move from cohort-level overviews to individual learning paths, quickly locate problematic concepts, and compare diverse learning strategies. We evaluate COIVis through two case studies and in-depth user-feedback interviews. The results demonstrate that COIVis effectively provides instructors with valuable insights into the consistency and anomalies of learners' learning patterns, thereby supporting timely and personalized interventions for learners and optimizing instructional design.", "AI": {"tldr": "开发了一个基于眼动追踪的可视化分析系统COIVis，用于探索MOOC视频中的概念学习过程，帮助教师理解学习者的认知状态和学习模式。", "motivation": "MOOC缺乏面对面互动，传统分析方法（如点击流或测验分数）只能捕捉粗粒度的学习结果，难以了解学习者实时的认知状态，需要更精细的学习过程分析方法。", "method": "首先从多模态视频内容中提取课程概念，将其与讲座的时间结构和屏幕空间对齐，定义兴趣概念（COIs）。将学习者的注视轨迹转换为COI序列，基于眼动指标计算五个可解释的学习者状态特征：注意力、认知负荷、兴趣、偏好和同步性。提供叙事性的多视图可视化，支持从群体概览到个体学习路径的探索。", "result": "通过两个案例研究和深度用户反馈访谈评估COIVis，结果表明系统能有效帮助教师理解学习者学习模式的一致性和异常，支持及时个性化的干预和教学设计的优化。", "conclusion": "COIVis系统为MOOC教学提供了基于眼动追踪的精细分析工具，能够揭示学习者的认知状态和学习过程，有助于改善教学质量和学习效果。"}}
{"id": "2512.06829", "pdf": "https://arxiv.org/pdf/2512.06829", "abs": "https://arxiv.org/abs/2512.06829", "authors": ["Oluwatimilehin Tijani", "Zhuo Chen", "Jiankang Deng", "Shan Luo"], "title": "MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin", "categories": ["cs.RO"], "comment": "Submitted to ICRA2026", "summary": "Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\\%), texture classification (93.51\\%), tangential displacement tracking (97\\% point retention) and force prediction (66\\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \\href{https://zhuochenn.github.io/MagicSkin_project/}{link}.", "AI": {"tldr": "MagicSkin提出了一种新型视觉触觉传感器皮肤，通过半透明着色标记平衡标记和标记模式，实现同时进行切向位移跟踪、力预测和表面细节保留", "motivation": "传统视觉触觉传感器面临标记与标记模式的基本权衡：不透明墨水标记能测量力和切向位移但完全遮挡几何特征，而标记皮肤保留表面细节但难以有效测量切向位移。现有解决方案如UV照明或基于学习的虚拟转移增加了硬件复杂性或计算负担", "method": "设计了一种带有半透明着色标记的新型触觉皮肤，可直接插入GelSight系列传感器，无需额外硬件或软件工具。这种皮肤平衡了标记和标记模式，保留了表面细节同时允许标记跟踪", "result": "在物体分类（99.17%）、纹理分类（93.51%）、切向位移跟踪（97%点保留）和力预测（总力误差改善66%）方面均取得最佳性能，超越了传统标记和标记设计", "conclusion": "半透明皮肤消除了传统标记或标记模式的性能权衡，为触觉机器人所需的多模态触觉感知铺平了道路，提供了一种简单有效的解决方案"}}
{"id": "2512.06818", "pdf": "https://arxiv.org/pdf/2512.06818", "abs": "https://arxiv.org/abs/2512.06818", "authors": ["Jan Held", "Sanghyun Son", "Renaud Vandeghen", "Daniel Rebain", "Matheus Gadelha", "Yi Zhou", "Anthony Cioppa", "Ming C. Lin", "Marc Van Droogenbroeck", "Andrea Tagliasacchi"], "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "categories": ["cs.CV"], "comment": null, "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "AI": {"tldr": "提出MeshSplatting方法，将基于基元的splatting技术与网格表示相结合，实现可微分渲染，优化几何和外观，生成实时渲染的高质量网格", "motivation": "解决3D高斯splatting等基于点的表示方法与AR/VR和游戏引擎中基于网格的流水线不兼容的问题，桥接神经渲染和交互式3D图形", "method": "通过受限Delaunay三角剖分强制网格连通性，优化表面一致性，联合优化几何和外观，实现端到端可微分渲染", "result": "在Mip-NeRF360数据集上，比当前最先进的MiLo方法PSNR提升+0.69dB，训练速度快2倍，内存使用减少2倍，生成平滑、视觉高质量的可实时渲染网格", "conclusion": "MeshSplatting成功将基于基元的splatting技术与网格表示相结合，为神经渲染和交互式3D图形之间搭建了桥梁，实现了无缝的实时场景交互"}}
{"id": "2512.06814", "pdf": "https://arxiv.org/pdf/2512.06814", "abs": "https://arxiv.org/abs/2512.06814", "authors": ["Dibyanayan Bandyopadhyay", "Soham Bhattacharjee", "Mohammed Hasanuzzaman", "Asif Ekbal"], "title": "CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Transactions of the Association for Computational Linguistics (TACL). Pre-MIT Press publication version", "summary": "Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier's internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at https://github.com/newcodevelop/CAuSE", "AI": {"tldr": "提出CAuSE框架，为预训练多模态分类器生成忠实于模型内部决策过程的自然语言解释", "motivation": "多模态分类器作为黑盒模型缺乏可解释性，现有解释方法很少能像自然语言解释那样直观易懂，且需要确保解释能忠实反映分类器的内部决策行为", "method": "提出CAuSE框架，通过交换干预训练，形成对底层分类器的因果抽象，生成忠实自然语言解释", "result": "CAuSE在数据集和模型上具有良好泛化能力，在因果忠实度度量上超越其他方法，定性分析也显示其优势", "conclusion": "CAuSE能有效解码多模态分类器，生成忠实自然语言解释，增强模型可解释性和可信度"}}
{"id": "2512.06813", "pdf": "https://arxiv.org/pdf/2512.06813", "abs": "https://arxiv.org/abs/2512.06813", "authors": ["Agung Nugraha", "Heungjun Im", "Jihwan Lee"], "title": "Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 12 figures", "summary": "High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.", "AI": {"tldr": "提出一种用于高性能混凝土部分逆向设计的合作神经网络框架，能够在约束条件下生成有效的配合比设计", "motivation": "高性能混凝土需要复杂的配合比设计，涉及许多相互依赖的变量和实际约束。虽然数据驱动方法在正向设计预测建模方面取得了进展，但逆向设计（确定实现目标性能的配合比组成）仍然有限，特别是在某些混合变量被约束固定、只有剩余变量需要确定的设计情况下。", "method": "提出合作神经网络框架，结合两个耦合的神经网络模型：一个插补模型用于推断未确定变量，一个代理模型用于预测抗压强度。通过合作学习，模型在单次前向传递中生成有效且性能一致的配合比设计，同时适应不同的约束组合而无需重新训练。", "result": "在基准数据集上评估，提出的模型实现了稳定且较高的R平方值（0.87-0.92），与自编码器基线相比平均减少50%的均方误差，与贝叶斯推理相比平均减少70%的均方误差。", "conclusion": "合作神经网络为混凝土工程中约束感知、数据驱动的配合比设计提供了准确、稳健且计算高效的基础。"}}
{"id": "2512.06811", "pdf": "https://arxiv.org/pdf/2512.06811", "abs": "https://arxiv.org/abs/2512.06811", "authors": ["Xiang Lin", "Weixin Li", "Shu Guo", "Lihong Wang", "Di Huang"], "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted by AAAI 2026(Oral)", "summary": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.", "AI": {"tldr": "提出了一种基于重构的多模态适配器（RMAdapter），用于在少样本场景下平衡视觉语言模型的任务特定适应与泛化能力", "motivation": "预训练的视觉语言模型（如CLIP）在少样本微调时面临任务特定适应与泛化能力之间的平衡挑战，现有研究主要关注基于提示的方法，而基于适配器的方法探索不足且性能存在差距", "method": "采用双分支架构：1）适应分支通过参数高效微调注入任务特定知识；2）重构分支通过将潜在空间特征重构回原始特征空间来保留通用知识。通过局部计算重构损失和共享投影模块保持轻量化，并加入一致性约束来调节判别性与泛化之间的权衡", "result": "在三个代表性任务（新类别泛化、新目标数据集泛化、领域泛化）上全面评估，无需数据增强或重复提示设计，在所有评估指标上始终优于最先进方法", "conclusion": "RMAdapter通过创新的双分支设计有效平衡了视觉语言模型在少样本场景下的任务特定适应与泛化能力，为多模态适配器方法提供了新的解决方案"}}
{"id": "2512.06810", "pdf": "https://arxiv.org/pdf/2512.06810", "abs": "https://arxiv.org/abs/2512.06810", "authors": ["Yueqian Wang", "Songxiang Liu", "Disong Wang", "Nuo Xu", "Guanglu Wan", "Huishuai Zhang", "Dongyan Zhao"], "title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "AI": {"tldr": "提出MMDuet2模型，通过多轮强化学习增强视频多模态大语言模型的主动交互能力，使其能够在视频播放过程中自主决定何时回复用户", "motivation": "现有视频MLLM系统主要采用轮转式交互，只能在用户发言后回复，而实时应用需要模型能够在视频播放过程中主动决定何时回复。现有方法存在手动调整回复决策阈值和需要精确回复时间标注的问题", "method": "提出基于文本到文本的主动交互方法，模型根据对话历史和当前视频帧的视觉上下文自主决定回复或保持沉默。引入多轮强化学习训练方法，无需精确回复时间标注，鼓励及时准确的回复。在包含52k视频和两种对话类型的数据集上进行SFT和RL训练", "result": "MMDuet2在回复时机和质量上优于现有主动视频MLLM基线，在ProactiveVideoQA基准测试中达到最先进的性能", "conclusion": "提出的多轮强化学习方法有效解决了视频MLLM主动交互中的回复时机决策问题，无需手动调整阈值或精确时间标注，为实时视频交互应用提供了有前景的解决方案"}}
{"id": "2512.06802", "pdf": "https://arxiv.org/pdf/2512.06802", "abs": "https://arxiv.org/abs/2512.06802", "authors": ["Yutong Wang", "Haiyu Zhang", "Tianfan Xue", "Yu Qiao", "Yaohui Wang", "Chang Xu", "Xinyuan Chen"], "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.", "AI": {"tldr": "提出VDOT模型，通过最优传输蒸馏实现高效统一的视频生成，仅需4步推理即可达到传统模型100步的生成效果", "motivation": "现有视频生成模型要么只能处理少数特定条件，要么因复杂推理过程导致生成时间过长，难以在实际应用中部署", "method": "采用分布匹配蒸馏范式，使用最优传输技术优化真实与生成分数分布间的差异，避免KL散度蒸馏中的零强制或梯度崩溃问题，并集成判别器感知真实视频数据", "result": "4步推理的VDOT模型在性能上优于或匹配需要100步去噪的基线模型，同时建立了UVCBench统一测试基准", "conclusion": "VDOT通过最优传输蒸馏实现了高效统一的视频生成，解决了现有模型在条件多样性和推理效率方面的局限性"}}
{"id": "2512.06797", "pdf": "https://arxiv.org/pdf/2512.06797", "abs": "https://arxiv.org/abs/2512.06797", "authors": ["Gabriel Peyré"], "title": "Optimal and Diffusion Transports in Machine Learning", "categories": ["math.OC", "cs.AI", "cs.LG", "stat.ML"], "comment": "Proc. 2026 International Congress of Mathematicians", "summary": "Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.", "AI": {"tldr": "该论文综述了机器学习中基于概率分布演化的两种核心方法：扩散方法和最优传输，并探讨它们在采样、神经网络优化和大语言模型等应用中的数学统一框架。", "motivation": "机器学习中的多个问题（如扩散采样、神经网络权重优化、大语言模型token分布演化）都自然地表示为时间演化的概率分布设计问题。这些不同应用在数学描述上具有共同结构，需要建立统一的理论框架来理解和分析。", "method": "采用拉格朗日视角，通过向量场对粒子进行平流来描述密度演化。重点介绍两种互补方法：1）扩散方法，基于随机插值过程，是现代生成AI的基础；2）最优传输，通过最小化位移成本定义插值。", "result": "展示了这两种方法在多个机器学习应用中的具体表现：扩散方法用于生成模型采样，最优传输用于神经网络优化，以及两者在大语言模型Transformer动力学建模中的应用。", "conclusion": "尽管目标应用不同（样本、权重、token），但它们在数学描述上共享共同结构。从欧拉密度表示切换到拉格朗日向量场视角，虽然带来非唯一性挑战，但也为设计具有良好正则性、稳定性和计算可处理性的密度演化和流提供了机会。"}}
{"id": "2512.06796", "pdf": "https://arxiv.org/pdf/2512.06796", "abs": "https://arxiv.org/abs/2512.06796", "authors": ["Akmaral Moldagalieva", "Keisuke Okumura", "Amanda Prorok", "Wolfgang Hönig"], "title": "db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF", "categories": ["cs.RO"], "comment": null, "summary": "State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time. In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations. To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions. The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics. Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality. The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator. We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.", "AI": {"tldr": "提出db-LaCAM算法，将多智能体路径规划（MAPF）的可扩展性与动力学运动规划的动态感知能力相结合，实现快速、可扩展的多机器人运动规划", "motivation": "现有基于动力学的多机器人运动规划器由于计算负担重，难以处理超过几个机器人的场景，限制了可扩展性并导致规划时间过长", "method": "结合不连续性有界搜索和轻量级MAPF，使用预先计算的运动基元生成符合机器人动力学的运动序列，允许用户定义连续运动之间的不连续性", "result": "db-LaCAM可扩展到50个机器人的场景，运行时间比现有最优方法快10倍，同时保持相当的解决方案质量，在2D和3D环境中验证了有效性", "conclusion": "db-LaCAM成功将MAPF的可扩展性与动力学规划的动态感知能力相结合，实现了快速、可扩展的多机器人运动规划，并在物理实验中验证了轨迹执行的安全性"}}
{"id": "2512.06793", "pdf": "https://arxiv.org/pdf/2512.06793", "abs": "https://arxiv.org/abs/2512.06793", "authors": ["Jiaxin Liu", "Gangwei Xu", "Xianqi Wang", "Chengliang Zhang", "Xin Yang"], "title": "Generalized Geometry Encoding Volume for Real-time Stereo Matching", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026", "summary": "Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.", "AI": {"tldr": "提出GGEV（广义几何编码体积）网络，用于实现具有强泛化能力的实时立体匹配", "motivation": "现有实时立体匹配方法主要关注域内性能提升，忽视了实际应用中的泛化能力；而基于单目基础模型的立体基础模型虽然泛化能力强，但推理延迟大。需要平衡实时性和泛化能力。", "method": "1. 提取深度感知特征，编码域不变的结构先验作为代价聚合的指导；2. 引入深度感知动态代价聚合（DDCA）模块，自适应地将这些先验融入每个视差假设，增强未见场景中的脆弱匹配关系", "result": "在零样本泛化能力上超越了所有现有实时方法，在KITTI 2012、KITTI 2015和ETH3D基准测试中达到了最先进的性能", "conclusion": "GGEV网络通过轻量级且互补的两步策略，构建了具有强泛化能力的广义几何编码体积，成功平衡了实时立体匹配的推理速度和泛化性能"}}
{"id": "2512.06785", "pdf": "https://arxiv.org/pdf/2512.06785", "abs": "https://arxiv.org/abs/2512.06785", "authors": ["Vasileios Sevetlidis", "George Pavlidis", "Antonios Gasteratos"], "title": "Angular Regularization for Positive-Unlabeled Learning on the Hypersphere", "categories": ["cs.LG", "cs.AI"], "comment": "Featured Certification, J2C Certification. Transactions on Machine Learning Research, 2025", "summary": "Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.", "AI": {"tldr": "提出AngularPU框架，在单位超球面上使用余弦相似度和角度间隔进行正-无标记学习，通过可学习的原型向量表示正类，无需显式负类建模", "motivation": "现有PU学习方法要么依赖强分布假设，要么在高维设置中容易崩溃，需要一种更稳健的方法来处理高维嵌入和稀缺正样本的情况", "method": "在单位超球面上操作，使用余弦相似度和角度间隔；正类由可学习的原型向量表示，分类简化为阈值化嵌入与原型之间的余弦相似度；引入角度正则化器促使无标记集在超球面上分散", "result": "在基准数据集上实验表明，AngularPU相比最先进的PU方法具有竞争力或更优性能，特别是在稀缺正样本和高维嵌入设置中，同时提供几何可解释性和可扩展性", "conclusion": "AngularPU为PU学习提供了一种新颖的几何方法，通过角度正则化有效处理高维嵌入中的聚类问题，具有理论保证和实际性能优势"}}
{"id": "2512.06783", "pdf": "https://arxiv.org/pdf/2512.06783", "abs": "https://arxiv.org/abs/2512.06783", "authors": ["Tobias Leuthold", "Michele Xiloyannis", "Yves Zimmermann"], "title": "Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos", "categories": ["cs.CV"], "comment": "16 pages, 5 figures", "summary": "Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.", "AI": {"tldr": "提出一种基于物理约束的单目RGB视频3D姿态估计后处理算法，通过融合BlazePose的2D和3D估计结果，结合骨骼长度和生物力学模型约束，实现更准确、符合解剖学原理的人体姿态估计。", "motivation": "现有实时姿态估计模型（如BlazePose）缺乏解剖学约束，在物理治疗、运动指导等应用中存在改进空间。需要将物理知识融入姿态估计，提高准确性和解剖学一致性。", "method": "提出实时后处理算法：1）使用加权优化融合BlazePose的3D和2D估计结果；2）引入惩罚项约束骨骼长度偏差和生物力学模型；3）采用卡尔曼滤波自适应调整测量信任度，根据个体解剖结构优化骨骼长度估计。", "result": "在Physio2.2M数据集上评估：相比BlazePose 3D估计，3D MPJPE降低10.2%，身体段间角度误差减少16.6%。算法计算高效，可在消费级笔记本和移动设备上运行，适合实时应用。", "conclusion": "该方法通过融入物理约束显著提升了姿态估计的准确性和解剖学一致性，为自动化物理治疗、医疗保健和运动指导提供了鲁棒的解决方案，同时保护用户隐私（仅使用匿名数据在后端运行）。"}}
{"id": "2512.06781", "pdf": "https://arxiv.org/pdf/2512.06781", "abs": "https://arxiv.org/abs/2512.06781", "authors": ["Sima Jafarikhah", "Daniel Thompson", "Eva Deans", "Hossein Siadati", "Yi Liu"], "title": "From Description to Score: Can LLMs Quantify Vulnerabilities?", "categories": ["cs.CR", "cs.AI", "cs.PL"], "comment": "10 pages", "summary": "Manual vulnerability scoring, such as assigning Common Vulnerability Scoring System (CVSS) scores, is a resource-intensive process that is often influenced by subjective interpretation. This study investigates the potential of general-purpose large language models (LLMs), namely ChatGPT, Llama, Grok, DeepSeek, and Gemini, to automate this process by analyzing over 31{,}000 recent Common Vulnerabilities and Exposures (CVE) entries. The results show that LLMs substantially outperform the baseline on certain metrics (e.g., \\textit{Availability Impact}), while offering more modest gains on others (e.g., \\textit{Attack Complexity}). Moreover, model performance varies across both LLM families and individual CVSS metrics, with ChatGPT-5 attaining the highest precision. Our analysis reveals that LLMs tend to misclassify many of the same CVEs, and ensemble-based meta-classifiers only marginally improve performance. Further examination shows that CVE descriptions often lack critical context or contain ambiguous phrasing, which contributes to systematic misclassifications. These findings underscore the importance of enhancing vulnerability descriptions and incorporating richer contextual details to support more reliable automated reasoning and alleviate the growing backlog of CVEs awaiting triage.", "AI": {"tldr": "研究探索大型语言模型（LLMs）能否自动为漏洞分配CVSS评分，以替代人工评分流程", "motivation": "人工漏洞评分（如CVSS评分）是资源密集型过程，且受主观解释影响。随着CVE积压增加，需要自动化解决方案来减轻人工负担", "method": "使用ChatGPT、Llama、Grok、DeepSeek和Gemini等通用大语言模型，分析超过31,000个最近的CVE条目，评估它们在CVSS评分任务上的表现", "result": "LLMs在某些指标（如可用性影响）上显著优于基线，但在其他指标（如攻击复杂度）上提升有限。ChatGPT-5获得最高精度。模型倾向于对相同CVE产生误分类，集成方法仅略微改善性能", "conclusion": "CVE描述常缺乏关键上下文或包含模糊措辞，导致系统性误分类。需要增强漏洞描述并纳入更丰富的上下文细节，以支持更可靠的自动推理，缓解CVE积压问题"}}
{"id": "2512.06776", "pdf": "https://arxiv.org/pdf/2512.06776", "abs": "https://arxiv.org/abs/2512.06776", "authors": ["Yuchuan Tian", "Yuchen Liang", "Jiacheng Sun", "Shuo Zhang", "Guangwen Yang", "Yingte Shu", "Sibo Fang", "Tianyu Guo", "Kai Han", "Chao Xu", "Hanting Chen", "Xinghao Chen", "Yunhe Wang"], "title": "From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)--especially block-wise variants--enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior \"adaptation\" attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize=1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: https://github.com/YuchuanTian/NBDiff.", "AI": {"tldr": "该论文提出了一种从自回归语言模型到块扩散语言模型的原则性适配路径，通过将自回归生成视为块大小为1的块扩散，设计了一种渐进式适配方法。", "motivation": "自回归语言模型在生成时存在顺序解码的吞吐量瓶颈，而扩散语言模型支持并行生成和块内双向推理，但从头训练大型扩散语言模型成本高昂且浪费了现有自回归检查点的知识。", "method": "设计了从自回归到块扩散的适配路径：使用上下文因果注意力掩码（上下文因果，仅在活动块内双向）、高效并行适配过程、辅助自回归损失以最大化数据利用和保留预训练知识，以及逐步增加生成块大小。", "result": "基于该方法构建的NBDiff-7B模型在7B类扩散语言模型中达到最先进性能，在通用知识、数学和代码基准测试上相比强基线有显著提升，继承了长上下文建模和推理能力。", "conclusion": "原则性的自回归到块扩散适配是一种有效且计算高效的替代方案，可以避免从头训练扩散语言模型的高成本，同时充分利用现有自回归检查点的知识。"}}
{"id": "2512.06774", "pdf": "https://arxiv.org/pdf/2512.06774", "abs": "https://arxiv.org/abs/2512.06774", "authors": ["Longjie Zhao", "Ziming Hong", "Zhenyang Ren", "Runnan Chen", "Mingming Gong", "Tongliang Liu"], "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.", "AI": {"tldr": "提出RDSplat方法，为3D高斯泼溅（3DGS）提供针对扩散编辑的鲁棒数字水印保护方案", "motivation": "现有3DGS水印方法对基于扩散的编辑攻击高度脆弱，扩散编辑容易擦除嵌入的版权信息，迫切需要能够抵抗扩散编辑的鲁棒水印技术", "method": "1) 将水印嵌入到扩散编辑固有保留的3DGS组件中：主动针对低频高斯函数；2) 使用扩散代理进行对抗训练：通过多域框架在3DGS空间操作，通过协调协方差正则化和2D滤波将水印嵌入到扩散编辑保留的低频高斯中；3) 利用扩散编辑的低通滤波特性，使用高斯模糊作为高效训练代理，进行对抗微调", "result": "在三个基准数据集上的综合定量和定性评估表明，RDSplat在扩散编辑下保持卓越的鲁棒性，同时保持水印不可见性，达到最先进的性能", "conclusion": "RDSplat为3D高斯泼溅提供了一种有效的鲁棒水印解决方案，能够抵抗扩散编辑攻击，为数字资产的版权保护提供了重要技术保障"}}
{"id": "2512.06769", "pdf": "https://arxiv.org/pdf/2512.06769", "abs": "https://arxiv.org/abs/2512.06769", "authors": ["Hang Yin", "Xiaomin He", "PeiWen Yuan", "Yiwei Li", "Jiayi Shi", "Wenxiao Fan", "Shaoxiong Feng", "Kan Li"], "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.", "AI": {"tldr": "提出了一种名为Stitch and Tell (SiTe)的结构化多模态数据增强方法，用于提升视觉语言模型的空间理解能力，通过拼接图像和生成空间感知的文本对来缓解空间幻觉问题。", "motivation": "现有视觉语言模型经常出现空间幻觉问题，即错误描述图像中物体的相对位置。作者认为这主要源于图像和文本之间的不对称特性，需要增强模型的空间理解能力。", "method": "提出SiTe方法：1）沿空间轴拼接图像创建缝合图像；2）基于缝合图像的布局生成空间感知的标题或问答对；3）无需昂贵的高级模型或人工标注；4）是即插即用的数据增强方法。", "result": "在三种架构（LLaVA-v1.5-7B、LLaVA-Qwen2-1.5B、HALVA-7B）、两个训练数据集和八个基准测试上评估。结果显示：空间理解任务显著提升（MME_Position +5.50%，Spatial-MM +4.19%），同时保持或提升通用视觉语言基准性能（COCO-QA +1.02%，MMBench +4.76%）。", "conclusion": "将空间感知结构显式注入训练数据是缓解空间幻觉、提升空间理解能力的有效方法，同时能保持通用视觉语言能力。SiTe方法简单、无需标注、即插即用。"}}
{"id": "2512.06763", "pdf": "https://arxiv.org/pdf/2512.06763", "abs": "https://arxiv.org/abs/2512.06763", "authors": ["Chengyang Yan", "Mitch Bryson", "Donald G. Dansereau"], "title": "JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms", "categories": ["cs.CV"], "comment": null, "summary": "The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.", "AI": {"tldr": "提出了一种联合优化相机硬件和自适应相机控制算法的方法，通过任务驱动的统一优化框架提升下游视觉任务的性能。", "motivation": "现有相机系统与感知任务的协同设计主要关注制造时设置的固定参数，但许多参数（如曝光设置）需要在运行时进行自适应控制。需要一种能够联合优化硬件参数和自适应控制算法的方法来提升感知性能。", "method": "提出了一个统一的优化框架，整合了基于梯度和无导数的方法，支持连续和离散参数、不可微的图像形成过程以及基于神经网络的自适应控制算法。针对运动模糊等不可微效应，提出了DF-Grad混合优化策略，使用无导数优化器的信号训练自适应控制网络。", "result": "实验表明，该方法在低光照和快速运动等挑战性条件下，优于分别优化静态和动态参数的基线方法，显著提升了感知性能。", "conclusion": "联合优化硬件参数和自适应控制算法能够改善感知性能，为任务驱动的相机系统设计提供了一个统一的方法。"}}
{"id": "2512.06759", "pdf": "https://arxiv.org/pdf/2512.06759", "abs": "https://arxiv.org/abs/2512.06759", "authors": ["Wenbo Lyu", "Yingjun Du", "Jinglin Zhao", "Xianton Zhen", "Ling Shao"], "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages,13figures", "summary": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench", "AI": {"tldr": "提出了VisChainBench基准测试，用于评估大型视觉语言模型在多轮、多图像场景中的视觉推理能力，特别关注减少语言先验依赖", "motivation": "现有基准测试主要关注静态或水平比较，过度依赖语言线索，忽视了渐进式、上下文依赖的推理以及视觉到视觉的推理挑战", "method": "使用多智能体生成管道构建大规模基准测试，包含1,457个任务，涵盖20,000多张图像，分布在日常场景、工程故障排除等三个领域", "result": "创建了一个专门评估LVLMs多步视觉推理能力的基准测试，具有高视觉多样性和受控的语言偏见，数据已在HuggingFace上公开", "conclusion": "VisChainBench填补了多图像、多轮视觉推理评估的空白，为评估LVLMs在真实世界决策过程中的能力提供了重要工具"}}
{"id": "2512.06757", "pdf": "https://arxiv.org/pdf/2512.06757", "abs": "https://arxiv.org/abs/2512.06757", "authors": ["Zhihua Fang", "Shumei Tao", "Junxu Wang", "Liang He"], "title": "XM-ALIGN: Unified Cross-Modal Embedding Alignment for Face-Voice Association", "categories": ["cs.SD", "cs.CV"], "comment": "FAME 2026 Technical Report", "summary": "This paper introduces our solution, XM-ALIGN (Unified Cross-Modal Embedding Alignment Framework), proposed for the FAME challenge at ICASSP 2026. Our framework combines explicit and implicit alignment mechanisms, significantly improving cross-modal verification performance in both \"heard\" and \"unheard\" languages. By extracting feature embeddings from both face and voice encoders and jointly optimizing them using a shared classifier, we employ mean squared error (MSE) as the embedding alignment loss to ensure tight alignment between modalities. Additionally, data augmentation strategies are applied during model training to enhance generalization. Experimental results show that our approach demonstrates superior performance on the MAV-Celeb dataset. The code will be released at https://github.com/PunkMale/XM-ALIGN.", "AI": {"tldr": "提出XM-ALIGN框架，通过显式和隐式对齐机制改进人脸-语音跨模态验证性能，特别是在\"听过\"和\"未听过\"语言场景下", "motivation": "解决人脸-语音跨模态关联中的对齐问题，特别是在不同语言场景下的泛化能力不足问题，需要提升跨模态验证性能", "method": "结合显式和隐式对齐机制，从人脸和语音编码器提取特征嵌入，使用共享分类器联合优化，采用MSE作为嵌入对齐损失，并应用数据增强策略", "result": "在MAV-Celeb数据集上表现出优越性能，显著提高了跨模态验证性能，特别是在\"听过\"和\"未听过\"语言场景下", "conclusion": "XM-ALIGN框架通过统一的对齐机制有效提升了人脸-语音跨模态关联性能，为解决跨模态验证问题提供了有效解决方案"}}
{"id": "2512.06754", "pdf": "https://arxiv.org/pdf/2512.06754", "abs": "https://arxiv.org/abs/2512.06754", "authors": ["Shrreya Rajneesh", "Nikita Pavle", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.", "AI": {"tldr": "提出一种用于空间连续机械臂的无模型反馈控制框架，通过在线优化雅可比矩阵和肌腱张力来避免传统模型依赖方法的局限性", "motivation": "连续机械臂的无限维变形、未建模的内部摩擦和配置相关的刚度限制了基于模型方法的可靠性，导致雅可比预测不准确、人工奇异性和不稳定驱动行为", "method": "采用经验初始化的雅可比矩阵，通过微分凸更新在线优化；通过实时二次规划计算驱动器增量，强制执行肌腱松弛避免和几何限制；引入骨架张力优化项来调节轴向载荷", "result": "在圆形、五边形和正方形轨迹上验证，展示了平滑收敛、稳定的张力演变和亚毫米级稳态精度，无需任何模型校准或参数识别", "conclusion": "该控制器为受限环境中的连续机械臂操作提供了可扩展的无模型替代方案，克服了传统模型依赖方法的局限性"}}
{"id": "2512.06751", "pdf": "https://arxiv.org/pdf/2512.06751", "abs": "https://arxiv.org/abs/2512.06751", "authors": ["Seungyeon Jwa", "Daechul Ahn", "Reokyoung Kim", "Dongyeop Kang", "Jonghyun Choi"], "title": "Becoming Experienced Judges: Selective Test-Time Learning for Evaluators", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.", "AI": {"tldr": "提出LWE框架，让LLM评估器在推理过程中通过选择性测试时学习来积累经验并改进评估能力", "motivation": "当前LLM-as-a-judge方法存在两个问题：1) 独立处理每个样本，无法积累经验；2) 使用固定提示词，无法针对不同样本调整评估标准", "method": "提出LWE框架，维护一个演化的元提示词，生成样本特定的评估指令并通过自生成反馈进行改进。进一步提出Selective LWE，只在自不一致的案例上更新元提示词", "result": "在两个成对比较基准测试中，Selective LWE优于强基线方法，证明评估器可以通过选择性更新在顺序测试中改进，从最困难的案例中学习最多", "conclusion": "评估器可以通过选择性测试时学习积累经验并改进，选择性方法在保持顺序学习优势的同时更加经济高效"}}
{"id": "2512.06750", "pdf": "https://arxiv.org/pdf/2512.06750", "abs": "https://arxiv.org/abs/2512.06750", "authors": ["Weiqi Li", "Xuanyu Zhang", "Bin Chen", "Jingfen Xie", "Yan Wang", "Kexin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang", "Shijie Zhao"], "title": "UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.", "AI": {"tldr": "提出首个统一视觉语言模型UARE，将图像质量评估、修复和增强三个任务整合到一个模型中，通过多任务协同训练让IQA指导修复和增强性能提升。", "motivation": "虽然图像质量评估和修复在概念上紧密相关，但现有工作大多将它们分开处理。统一多模态理解-生成模型的最新进展表明，更强的理解能力可以提升生成性能，这促使研究者探索将IQA和修复统一到一个模型中，并研究IQA如何指导修复。", "method": "基于预训练的统一理解和生成模型，采用两阶段训练框架：1）渐进式从单一类型失真到高阶混合退化的训练，使模型能处理多种退化；2）使用交错文本-图像数据进行统一微调，将IQA信号与修复目标对齐，通过多任务协同训练让IQA提升修复和增强性能。", "result": "在IQA、修复和增强任务上的大量实验证明了UARE的有效性。模型能够处理多种退化类型，并通过IQA指导显著提升修复和增强的性能表现。", "conclusion": "UARE是首个将图像质量评估、修复和增强统一到一个视觉语言模型中的工作，通过创新的训练框架实现了多任务协同训练，证明了IQA可以有效地指导修复和增强任务，为低层视觉任务提供了新的统一解决方案。"}}
{"id": "2512.06749", "pdf": "https://arxiv.org/pdf/2512.06749", "abs": "https://arxiv.org/abs/2512.06749", "authors": ["Ming Ma", "Jue Zhang", "Fangkai Yang", "Yu Kang", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "AI": {"tldr": "DoVer是一个干预驱动的自动调试框架，用于LLM多智能体系统，通过主动干预验证失败假设，而非仅依赖日志分析", "motivation": "当前基于LLM的多智能体系统调试存在两个关键局限：1) 仅基于日志的调试缺乏验证，产生未经测试的假设；2) 单步或单智能体归因往往不准确，因为多个不同的干预可能独立修复失败任务", "method": "DoVer引入干预驱动的调试框架，将假设生成与主动验证相结合，通过有针对性的干预（如编辑消息、改变计划）来验证失败假设，并采用结果导向的评估方法", "result": "在Magnetic-One智能体框架上，DoVer将18-28%的失败试验转为成功，实现高达16%的里程碑进展，验证或反驳30-60%的失败假设；在不同数据集和框架上也能恢复49%的失败试验", "conclusion": "干预是提高智能体系统可靠性的实用机制，为基于LLM的多智能体系统开启了更鲁棒、可扩展的调试方法机会"}}
{"id": "2512.06747", "pdf": "https://arxiv.org/pdf/2512.06747", "abs": "https://arxiv.org/abs/2512.06747", "authors": ["Jifar Wakuma Ayana", "Huang Qiming"], "title": "PrivLLMSwarm: Privacy-Preserving LLM-Driven UAV Swarms for Secure IoT Surveillance", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as powerful enablers for autonomous reasoning and natural-language coordination in unmanned aerial vehicle (UAV) swarms operating within Internet of Things (IoT) environments. However, existing LLM-driven UAV systems process sensitive operational data in plaintext, exposing them to privacy and security risks. This work introduces PrivLLMSwarm, a privacy-preserving framework that performs secure LLM inference for UAV swarm coordination through Secure Multi-Party Computation (MPC). The framework incorporates MPC-optimized transformer components with efficient approximations of nonlinear activations, enabling practical encrypted inference on resource-constrained aerial platforms. A fine-tuned GPT-based command generator, enhanced through reinforcement learning in simulation, provides reliable instructions while maintaining confidentiality. Experimental evaluation in urban-scale simulations demonstrates that PrivLLMSwarm achieves high semantic accuracy, low encrypted inference latency, and robust formation control under privacy constraints. Comparative analysis shows PrivLLMSwarm offers a superior privacy-utility balance compared to differential privacy, federated learning, and plaintext baselines. To support reproducibility, the full implementation including source code, MPC components, and a synthetic dataset is publicly available. PrivLLMSwarm establishes a practical foundation for secure, LLM-enabled UAV swarms in privacy-sensitive IoT applications including smart-city monitoring and emergency response.", "AI": {"tldr": "提出PrivLLMSwarm隐私保护框架，用于无人机群在物联网环境中的安全协同，通过安全多方计算实现加密的LLM推理，保护敏感操作数据。", "motivation": "现有LLM驱动的无人机系统以明文处理敏感操作数据，面临隐私和安全风险，需要保护无人机群在物联网监控应用中的隐私。", "method": "采用安全多方计算技术，结合MPC优化的transformer组件和高效非线性激活函数近似，在资源受限的无人机平台上实现加密推理；通过强化学习在仿真中微调GPT指令生成器。", "result": "在城市规模仿真中，PrivLLMSwarm实现了高语义准确性、低加密推理延迟和稳健的编队控制；相比差分隐私、联邦学习和明文基线，提供了更好的隐私-效用平衡。", "conclusion": "PrivLLMSwarm为隐私敏感的物联网应用（如智慧城市监控和应急响应）中的安全LLM驱动无人机群建立了实用基础，支持可复现性。"}}
{"id": "2512.06746", "pdf": "https://arxiv.org/pdf/2512.06746", "abs": "https://arxiv.org/abs/2512.06746", "authors": ["Ruoxin Chen", "Jiahui Gao", "Kaiqing Lin", "Keyue Zhang", "Yandan Zhao", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.", "AI": {"tldr": "提出任务-模型对齐原则，将AI生成图像检测分解为语义一致性检查和像素伪影检测两个互补任务，通过AlignGemini双分支架构实现通用化检测", "motivation": "现有视觉语言模型在AI生成图像检测中存在严重幻觉问题，且需要大量资源进行微调。研究发现任务与模型之间存在错配：语义导向的VLMs缺乏对细粒度像素伪影的敏感性，而传统像素伪影检测器语义感知能力有限", "method": "将AIGI检测形式化为两个互补任务：语义一致性检查和像素伪影检测。提出AlignGemini双分支检测器，包含一个仅用纯语义监督微调的VLM分支和一个仅用纯像素伪影监督训练的像素伪影专家分支", "result": "在五个野外基准测试中，AlignGemini实现了平均准确率+9.5%的提升，证明了任务-模型对齐作为通用化AIGI检测的有效路径", "conclusion": "任务-模型对齐原则通过将复杂检测任务分解为互补子任务并匹配相应模型，能够有效解决现有方法的局限性，实现更通用化的AI生成图像检测"}}
{"id": "2512.06738", "pdf": "https://arxiv.org/pdf/2512.06738", "abs": "https://arxiv.org/abs/2512.06738", "authors": ["M Yashwanth", "Sampath Koti", "Arunabh Singh", "Shyam Marjit", "Anirban Chakraborty"], "title": "FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation", "categories": ["cs.CV"], "comment": "Accepted to Winter Conference on Applications of Computer Vision (WACV) 2026, Round 1", "summary": "We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.", "AI": {"tldr": "提出FedSCAl框架，通过服务器-客户端对齐机制解决无监督联邦源自由域自适应问题，在存在显著客户端间域差异且无法访问源数据集的情况下提升模型性能", "motivation": "解决联邦源自由域自适应(FFreeDA)问题中的挑战：客户端持有未标记数据且存在显著域差异，无法访问源数据集，现有方法在极端数据异构性下产生不可靠伪标签和客户端漂移", "method": "提出FedSCAl框架，采用服务器-客户端对齐(SCAl)机制，通过对齐客户端和服务器模型的预测来正则化客户端更新，缓解客户端漂移，提高伪标签准确性", "result": "在基准视觉数据集上的实验表明，FedSCAl在FFreeDA设置下持续优于最先进的联邦学习方法，提高了客户端伪标签准确性", "conclusion": "FedSCAl通过服务器-客户端对齐机制有效解决了FFreeDA问题中的客户端漂移问题，在存在显著域差异且无法访问源数据集的情况下提升了模型性能"}}
{"id": "2512.06737", "pdf": "https://arxiv.org/pdf/2512.06737", "abs": "https://arxiv.org/abs/2512.06737", "authors": ["Nikhil Verma", "Joonas Linnosmaa", "Espinosa-Leal Leonardo", "Napat Vajragupta"], "title": "Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE"], "comment": "80 pages, 6 tables, 2 figures, 5 appendices, proof-of-concept", "summary": "The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.", "AI": {"tldr": "本文提出了一种名为ArcGD的新型优化器，通过相位感知、用户可控的步长动态来重新表述梯度下降算法，在非凸函数和真实机器学习数据集上均表现出优越性能。", "motivation": "传统梯度下降优化器在复杂非凸优化问题中存在局限性，特别是在处理具有狭窄弯曲谷的挑战性函数时表现不佳。需要开发一种能够更好地控制步长动态、具有相位感知能力的新型优化方法。", "method": "ArcGD通过数学推导重新表述梯度下降算法，引入相位感知机制和用户可控的步长动态。该方法在数学上推导出新的优化公式，并在两个主要场景进行评估：非凸Rosenbrock函数和CIFAR-10图像分类数据集。", "result": "在Rosenbrock函数测试中，ArcGD在2D到1000D维度上一致优于Adam优化器，在50,000D极端情况下也表现良好。在CIFAR-10数据集上，ArcGD在20,000次迭代后达到50.7%的平均测试准确率，优于AdamW(46.6%)、Adam(46.8%)、SGD(49.6%)和Lion(43.4%)，在8个架构中的6个上获胜或持平。", "conclusion": "ArcGD在非凸优化和深度学习基准测试中均表现出优越性能，具有更好的泛化能力和抗过拟合特性，无需早期停止调优。研究还发现ArcGD变体可解释为Lion优化器的特例，揭示了不同优化方法之间的内在联系。"}}
{"id": "2512.06736", "pdf": "https://arxiv.org/pdf/2512.06736", "abs": "https://arxiv.org/abs/2512.06736", "authors": ["Jiaxing Fan", "Jiaojiao Liu", "Wenkong Wang", "Yang Zhang", "Xin Ma", "Jichen Zhang"], "title": "Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data", "categories": ["cs.CV"], "comment": null, "summary": "Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.", "AI": {"tldr": "提出基于骨架数据的图卷积长短期记忆注意力网络(GCN-LSTM-ATT)，用于检测中风后患者的代偿性运动，旨在提高康复训练监测的准确性。", "motivation": "大多数中风患者存在上肢运动功能障碍，康复训练中普遍存在代偿性运动，这对患者的长期恢复不利。因此，准确检测代偿性运动具有重要意义，有助于优化康复训练策略。", "method": "使用Kinect深度相机采集16名中风患者执行特定康复动作的骨架数据，提出GCN-LSTM-ATT模型，该模型结合图卷积网络(GCN)处理骨架空间关系、长短期记忆网络(LSTM)处理时间序列、注意力机制(ATT)增强关键特征提取。", "result": "GCN-LSTM-ATT模型的检测准确率达到0.8580，显著高于传统机器学习算法（SVM、KNN、RF）。消融实验表明模型的每个组件都对性能提升有显著贡献。", "conclusion": "该研究为中风后代偿性运动检测提供了更精确、更强大的工具，有望促进中风患者康复训练策略的优化，提高康复效果。"}}
{"id": "2512.06734", "pdf": "https://arxiv.org/pdf/2512.06734", "abs": "https://arxiv.org/abs/2512.06734", "authors": ["Subrit Dikshit", "Ritu Tiwari", "Priyank Jain"], "title": "A Patient-Doctor-NLP-System to contest inequality for less privileged", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures", "summary": "Transfer Learning (TL) has accelerated the rapid development and availability of large language models (LLMs) for mainstream natural language processing (NLP) use cases. However, training and deploying such gigantic LLMs in resource-constrained, real-world healthcare situations remains challenging. This study addresses the limited support available to visually impaired users and speakers of low-resource languages such as Hindi who require medical assistance in rural environments. We propose PDFTEMRA (Performant Distilled Frequency Transformer Ensemble Model with Random Activations), a compact transformer-based architecture that integrates model distillation, frequency-domain modulation, ensemble learning, and randomized activation patterns to reduce computational cost while preserving language understanding performance. The model is trained and evaluated on medical question-answering and consultation datasets tailored to Hindi and accessibility scenarios, and its performance is compared against standard NLP state-of-the-art model baselines. Results demonstrate that PDFTEMRA achieves comparable performance with substantially lower computational requirements, indicating its suitability for accessible, inclusive, low-resource medical NLP applications.", "AI": {"tldr": "本文提出了一种名为PDFTEMRA的紧凑型Transformer架构，旨在为资源受限的医疗环境（特别是针对视力障碍用户和印地语使用者）提供可访问的医疗NLP系统。", "motivation": "当前大型语言模型在资源受限的真实医疗环境中部署困难，特别是对于视力障碍用户和低资源语言（如印地语）使用者在农村医疗环境中缺乏足够的支持。需要开发计算成本低但性能保持的解决方案。", "method": "提出PDFTEMRA架构，整合了模型蒸馏、频域调制、集成学习和随机激活模式等技术，以减少计算成本同时保持语言理解性能。模型在针对印地语和可访问性场景的医疗问答和咨询数据集上进行训练和评估。", "result": "PDFTEMRA在显著降低计算需求的情况下实现了与标准NLP最先进模型基线相当的性能，表明其适用于可访问、包容、低资源的医疗NLP应用。", "conclusion": "PDFTEMRA模型为解决医疗不平等问题提供了一种有效的技术方案，特别适合资源受限环境下的医疗辅助应用，有助于为弱势群体提供更公平的医疗服务。"}}
{"id": "2512.06732", "pdf": "https://arxiv.org/pdf/2512.06732", "abs": "https://arxiv.org/abs/2512.06732", "authors": ["Aarushi Wagh", "Saniya Srivastava"], "title": "\"The Dentist is an involved parent, the bartender is not\": Revealing Implicit Biases in QA with Implicit BBQ", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing benchmarks evaluating biases in large language models (LLMs) primarily rely on explicit cues, declaring protected attributes like religion, race, gender by name. However, real-world interactions often contain implicit biases, inferred subtly through names, cultural cues, or traits. This critical oversight creates a significant blind spot in fairness evaluation. We introduce ImplicitBBQ, a benchmark extending the Bias Benchmark for QA (BBQ) with implicitly cued protected attributes across 6 categories. Our evaluation of GPT-4o on ImplicitBBQ illustrates troubling performance disparity from explicit BBQ prompts, with accuracy declining up to 7% in the \"sexual orientation\" subcategory and consistent decline located across most other categories. This indicates that current LLMs contain implicit biases undetected by explicit benchmarks. ImplicitBBQ offers a crucial tool for nuanced fairness evaluation in NLP.", "AI": {"tldr": "该论文提出了ImplicitBBQ基准测试，用于评估大语言模型中的隐性偏见，通过扩展BBQ基准，在6个受保护属性类别中使用隐式线索来揭示模型中的隐性偏见问题。", "motivation": "现有评估大语言模型偏见的基准主要依赖显式线索（如直接声明宗教、种族、性别等受保护属性），但现实世界中的偏见往往是隐性的，通过姓名、文化线索或特质等微妙方式推断。这种关键疏忽在公平性评估中造成了重大盲点。", "method": "作者引入了ImplicitBBQ基准，扩展了偏见基准问答（BBQ），在6个受保护属性类别中使用隐式线索的受保护属性。通过评估GPT-4o在ImplicitBBQ上的表现，与显式BBQ提示进行对比分析。", "result": "GPT-4o在ImplicitBBQ上的评估显示令人担忧的性能差异：在\"性取向\"子类别中准确率下降高达7%，大多数其他类别也出现一致的下降趋势。这表明当前大语言模型包含现有显式基准无法检测到的隐性偏见。", "conclusion": "ImplicitBBQ为NLP领域的细致公平性评估提供了关键工具，揭示了当前大语言模型中存在的隐性偏见问题，这些偏见无法通过依赖显式线索的传统基准检测到。"}}
{"id": "2512.06730", "pdf": "https://arxiv.org/pdf/2512.06730", "abs": "https://arxiv.org/abs/2512.06730", "authors": ["Lin Yang", "Xiang Li", "Xin Ma", "Xinxin Zhao"], "title": "Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.", "AI": {"tldr": "本文提出了一种基于AR-SSVEP的脑机接口系统，通过CNN-BiLSTM结合多头注意力机制和SHAP分析，增强运动意图识别的可解释性。", "motivation": "运动功能障碍患者在康复训练中主观参与度低，传统SSVEP-BCI系统依赖外部视觉刺激设备，限制了实际应用。需要提高患者主动性和减轻治疗师工作负担。", "method": "设计基于HoloLens 2的四种EEG类别，收集7名健康受试者的EEG数据。在传统CNN-BiLSTM架构基础上集成多头注意力机制（MACNN-BiLSTM），提取10个时频EEG特征，通过CNN学习高级表征，使用BiLSTM建模序列依赖，应用多头注意力机制突出运动意图相关模式，最后使用SHAP方法可视化EEG特征对神经网络决策的贡献。", "result": "该方法能够有效识别运动意图，并通过SHAP分析增强了模型的可解释性，有助于实时运动意图识别和支持运动障碍患者的康复。", "conclusion": "提出的AR-SSVEP系统结合MACNN-BiLSTM和SHAP分析，不仅提高了运动意图识别的准确性，还增强了模型的可解释性，为运动障碍患者的康复训练提供了更实用的解决方案。"}}
{"id": "2512.06726", "pdf": "https://arxiv.org/pdf/2512.06726", "abs": "https://arxiv.org/abs/2512.06726", "authors": ["Shuo Li", "Jiajun Sun", "Zhihao Zhang", "Xiaoran Fan", "Senjie Jin", "Hui Li", "Yuming Yang", "Junjie Ye", "Lixing Shen", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.", "AI": {"tldr": "该论文研究了熵在视觉定位任务中的作用，并提出了ECVGPO算法来优化熵控制，以平衡探索与利用的权衡。", "motivation": "尽管在多模态大语言模型的强化学习微调中，各种熵控制技术取得了显著进展，但熵在视觉定位等感知导向任务中的作用、特性以及有效控制策略仍然缺乏深入研究。", "method": "论文首先分析熵在视觉定位任务中的作用和特性，并与推理任务进行比较。基于这些发现，提出了ECVGPO（熵控制视觉定位策略优化）算法，这是一个可解释的算法，旨在实现有效的熵调节。", "result": "实验表明，通过熵控制，探索与利用的权衡得到了更好的平衡。ECVGPO在各种基准测试和模型上都取得了广泛的改进。", "conclusion": "该研究深入探讨了熵在视觉定位任务中的重要作用，提出的ECVGPO算法通过有效的熵控制机制，显著提升了视觉定位性能，为感知导向任务的强化学习优化提供了新思路。"}}
{"id": "2512.06721", "pdf": "https://arxiv.org/pdf/2512.06721", "abs": "https://arxiv.org/abs/2512.06721", "authors": ["Bufang Yang", "Lilin Xu", "Liekang Zeng", "Yunqi Guo", "Siyang Jiang", "Wenrui Lu", "Kaiwei Liu", "Hancheng Xiang", "Xiaofan Jiang", "Guoliang Xing", "Zhenyu Yan"], "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.", "AI": {"tldr": "ProAgent是一个端到端的前瞻性LLM智能体系统，通过整合大规模感官上下文和LLM推理来提供主动协助，减少用户物理和认知负担。", "motivation": "现有LLM智能体主要采用反应式范式，依赖明确的用户指令启动服务，这增加了用户的物理和认知负担。需要一种能够主动感知环境并提供协助的系统。", "method": "1) 采用前瞻性上下文提取方法，通过按需分层感知持续感知环境，提取包含感官和个人特征的层次化上下文；2) 使用上下文感知的前瞻性推理器将这些上下文映射到用户需求和工具调用。", "result": "在真实世界测试平台、公共数据集和用户研究中评估，ProAgent实现了比最先进基线高33.4%的前瞻预测准确率、高16.8%的工具调用F1分数，并显著提升了用户满意度。", "conclusion": "ProAgent代表了向主动助手迈出的重要一步，通过整合感官上下文和LLM推理，能够有效减少用户负担并提供更智能的协助服务。"}}
{"id": "2512.06716", "pdf": "https://arxiv.org/pdf/2512.06716", "abs": "https://arxiv.org/abs/2512.06716", "authors": ["Zhibo Liang", "Tianze Hu", "Zaiye Chen", "Mingjie Tang"], "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.", "AI": {"tldr": "提出认知控制架构（CCA），这是一个全生命周期监督框架，用于增强AI代理在面对间接提示注入攻击时的鲁棒性和对齐性", "motivation": "当前自主大型语言模型代理对间接提示注入攻击表现出显著脆弱性，现有防御机制在安全性和功能性之间存在根本性权衡，导致防御架构碎片化，无法在整个任务执行流程中提供完整的完整性保证", "method": "CCA采用双层次防御系统：1）通过预生成的\"意图图\"进行主动控制流和数据流完整性执行；2）创新的\"分层裁决器\"，在检测到偏差时基于多维评分启动深度推理，专门应对复杂的条件攻击", "result": "在AgentDojo基准测试中，CCA不仅有效抵御了挑战其他先进防御方法的复杂攻击，而且在保持显著效率和鲁棒性的同时实现了无妥协的安全性，调和了多维权衡", "conclusion": "CCA提供了一个全面的全生命周期认知监督框架，能够检测和缓解间接提示注入攻击，通过系统性方法解决了现有防御机制的碎片化问题，实现了安全、功能和效率之间的平衡"}}
{"id": "2512.06714", "pdf": "https://arxiv.org/pdf/2512.06714", "abs": "https://arxiv.org/abs/2512.06714", "authors": ["Tony Salloom", "Okyay Kaynak", "Wei He"], "title": "A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ef:Journal of Hydrology 2021", "summary": "Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.", "AI": {"tldr": "提出一种用于实时用水需求预测的新型深度神经网络架构，通过数据扩展和特征工程降低模型复杂度并提高极端点预测精度", "motivation": "短期用水需求预测是优化供水系统控制的基础，现有深度学习方法存在参数过多导致的复杂度问题，以及在极端点预测误差较大的缺陷", "method": "1) 提出数据扩展方法，在真实数据中插入虚拟数据以缓解极端点附近的非线性问题；2) 构建新型深度学习模型，使用GRU处理历史需求数据的序列关系，引入K-means无监督分类方法创建新特征以减少参数数量", "result": "1) 模型复杂度降低至文献中方法的六分之一，同时保持相同精度；2) 数据扩展方法使预测误差显著降低约30%；3) 使用中国两个不同水厂的真实数据进行验证，与最先进方法相比表现出优越性能", "conclusion": "提出的方法有效解决了用水需求预测中模型复杂度和极端点预测误差的问题，通过数据扩展和特征工程实现了更高效、更准确的预测模型，为供水系统优化控制提供了实用解决方案"}}
{"id": "2512.06710", "pdf": "https://arxiv.org/pdf/2512.06710", "abs": "https://arxiv.org/abs/2512.06710", "authors": ["Zairah Mustahsan", "Abel Lim", "Megna Anand", "Saahil Jain", "Bryan McCann"], "title": "Stochasticity in Agentic Evaluations: Quantifying Inconsistency with Intraclass Correlation", "categories": ["cs.AI"], "comment": null, "summary": "As large language models become components of larger agentic systems, evaluation reliability becomes critical: unreliable sub-agents introduce brittleness into downstream system behavior. Yet current evaluation practice, reporting a single accuracy number from a single run, obscures the variance underlying these results, making it impossible to distinguish genuine capability improvements from lucky sampling. We propose adopting Intraclass Correlation Coefficient (ICC), a metric from measurement science, to characterize this variance. ICC decomposes observed variance into between-query variance (task difficulty) and within-query variance (agent inconsistency), highlighting whether reported results reflect true capability or measurement noise. We evaluated on GAIA (Levels 1-3, measuring agentic capabilities across varying reasoning complexity) and FRAMES (measuring retrieval and factuality across multiple documents). We found that ICC varies dramatically with task structure, with reasoning and retrieval tasks (FRAMES) exhibit ICC=0.4955-0.7118 across models, and agentic tasks (GAIA) exhibiting ICC=0.304-0.774 across models. For sub-agent replacement decisions in agentic systems, accuracy improvements are only trustworthy if ICC also improves. We demonstrate that ICC converges by n=8-16 trials for structured tasks and n>=32 for complex reasoning, enabling practitioners to set evidence-based resampling budgets. We recommend reporting accuracy alongside ICC and within-query variance as standard practice, and propose updated Evaluation Cards capturing these metrics. By making evaluation stability visible, we aim to transform agentic benchmarking from opaque leaderboard competition to trustworthy experimental science. Our code is open-sourced at https://github.com/youdotcom-oss/stochastic-agent-evals.", "AI": {"tldr": "本文提出使用组内相关系数（ICC）来量化智能体评估中的随机性，将观察到的方差分解为任务难度和智能体不一致性，从而提高评估可靠性。", "motivation": "随着大语言模型成为智能体系统的组成部分，评估可靠性变得至关重要。当前评估实践仅报告单次运行的准确率，掩盖了结果背后的方差，无法区分真实能力提升与幸运采样。", "method": "采用测量科学中的组内相关系数（ICC）来表征评估方差，将观察方差分解为查询间方差（任务难度）和查询内方差（智能体不一致性）。在GAIA（智能体能力）和FRAMES（检索和事实性）数据集上进行评估。", "result": "ICC随任务结构变化显著：推理和检索任务（FRAMES）ICC=0.4955-0.7118，智能体任务（GAIA）ICC=0.304-0.774。对于智能体系统中的子代理替换决策，只有当ICC也改善时，准确率提升才是可信的。ICC收敛需要n=8-16次试验（结构化任务）或n>=32次（复杂推理）。", "conclusion": "建议将准确率与ICC和查询内方差一起作为标准实践报告，并提出包含这些指标的更新版评估卡片。通过使评估稳定性可见，旨在将智能体基准测试从不透明的排行榜竞争转变为可信的实验科学。"}}
{"id": "2512.06708", "pdf": "https://arxiv.org/pdf/2512.06708", "abs": "https://arxiv.org/abs/2512.06708", "authors": ["Waleed Razzaq", "Yun-Bo Zhao"], "title": "A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.", "AI": {"tldr": "提出一种新颖的多模态剩余使用寿命估计框架，结合图像表示和时间频率表示，通过层间解释增强模型可解释性", "motivation": "现有RUL估计方法存在泛化能力差、鲁棒性不足、数据需求高和可解释性有限的问题，特别是滚动轴承作为机械故障的常见原因，需要更可靠的RUL估计方法", "method": "提出多模态RUL框架，包含三个分支：图像表示分支（使用Bresenham线算法转换振动信号）、时间频率表示分支（使用连续小波变换）、融合分支（连接特征后输入LSTM建模时间模式），并引入多模态层间相关性传播解释技术", "result": "在XJTU-SY和PRONOSTIA基准数据集上验证，性能达到或超过最先进基线，训练数据需求减少28%-48%，模型具有强噪声鲁棒性，多模态LRP可视化证实了预测的可解释性和可信度", "conclusion": "该框架通过多模态特征融合和增强的可解释性，为实际工业部署提供了高效、可靠且可信的RUL估计解决方案"}}
{"id": "2512.06705", "pdf": "https://arxiv.org/pdf/2512.06705", "abs": "https://arxiv.org/abs/2512.06705", "authors": ["Yongyuan He", "Yi Bu"], "title": "Academic journals' AI policies fail to curb the surge in AI-assisted academic writing", "categories": ["cs.AI"], "comment": "40 pages, 10 figures, and 9 tables", "summary": "The rapid integration of generative AI into academic writing has prompted widespread policy responses from journals and publishers. However, the effectiveness of these policies remains unclear. Here, we analyze 5,114 journals and over 5.2 million papers to evaluate the real-world impact of AI usage guidelines. We show that despite 70% of journals adopting AI policies (primarily requiring disclosure), researchers' use of AI writing tools has increased dramatically across disciplines, with no significant difference between journals with or without policies. Non-English-speaking countries, physical sciences, and high-OA journals exhibit the highest growth rates. Crucially, full-text analysis on 164k scientific publications reveals a striking transparency gap: Of the 75k papers published since 2023, only 76 (0.1%) explicitly disclosed AI use. Our findings suggest that current policies have largely failed to promote transparency or restrain AI adoption. We urge a re-evaluation of ethical frameworks to foster responsible AI integration in science.", "AI": {"tldr": "分析学术期刊AI政策对AI辅助学术写作激增的遏制效果，发现现有政策基本无效", "motivation": "生成式AI快速融入学术写作，期刊和出版商广泛出台政策应对，但这些政策的实际效果尚不明确", "method": "分析5,114种期刊和超过520万篇论文，评估AI使用指南的实际影响；对164,000篇科学出版物进行全文分析", "result": "尽管70%的期刊采纳了AI政策（主要要求披露），但研究人员使用AI写作工具在各学科中急剧增加，有政策和无政策期刊间无显著差异；非英语国家、物理科学和高OA期刊增长最快；2023年以来发表的75,000篇论文中，只有76篇（0.1%）明确披露了AI使用", "conclusion": "当前政策在促进透明度或限制AI采用方面基本失败，需要重新评估伦理框架以促进科学中负责任的AI整合"}}
{"id": "2512.06699", "pdf": "https://arxiv.org/pdf/2512.06699", "abs": "https://arxiv.org/abs/2512.06699", "authors": ["Karthik Prabhakar"], "title": "Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization", "categories": ["cs.PF", "cs.AI", "cs.LG"], "comment": "20 pages, 10 figures", "summary": "Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project", "AI": {"tldr": "开发机器学习模型预测机器学习训练流水线的I/O性能，并为不同存储配置提供优化建议", "motivation": "现代机器学习训练中，数据I/O已成为主要瓶颈而非计算，GPU利用率常低于50%等待数据，需要系统化的方法来优化存储配置", "method": "通过系统化基准测试收集141个观测数据，涵盖不同存储后端、数据格式和访问模式，评估7种回归模型和3种分类方法，使用XGBoost进行性能预测", "result": "XGBoost模型表现最佳，R平方达到0.991，I/O吞吐量预测平均误差在11.8%以内，特征重要性分析显示吞吐量指标和批大小是主要性能驱动因素", "conclusion": "数据驱动方法可将配置时间从数天的试错减少到几分钟的预测推荐，该方法可复现并可扩展到ML系统中的其他资源管理问题"}}
{"id": "2512.06689", "pdf": "https://arxiv.org/pdf/2512.06689", "abs": "https://arxiv.org/abs/2512.06689", "authors": ["Jisoo Park", "Seonghak Lee", "Guisik Kim", "Taewoo Kim", "Junseok Kwon"], "title": "Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation", "categories": ["cs.CV", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.", "AI": {"tldr": "提出UniVoiceLite，一个轻量级的无监督视听框架，统一语音增强和语音分离任务", "motivation": "现实世界音频通常同时包含背景噪声和重叠说话者，需要统一解决方案；现有方法通常采用复杂的多阶段架构，参数量大且依赖监督训练，限制了可扩展性和泛化能力", "method": "利用唇部运动和面部身份线索指导语音提取，采用Wasserstein距离正则化稳定潜在空间，无需配对的噪声-干净数据，实现轻量级无监督学习", "result": "UniVoiceLite在噪声和多说话者场景下都表现出色，实现了效率与鲁棒泛化能力的结合", "conclusion": "提出了一个轻量级无监督的视听统一框架，成功整合了语音增强和语音分离任务，具有更好的可扩展性和泛化能力"}}
{"id": "2512.06684", "pdf": "https://arxiv.org/pdf/2512.06684", "abs": "https://arxiv.org/abs/2512.06684", "authors": ["Yumeng He", "Zanwei Zhou", "Yekun Zheng", "Chen Liang", "Yunbo Wang", "Xiaokang Yang"], "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.", "AI": {"tldr": "提出EMGauss框架，用于体积电子显微镜(vEM)中的连续切片到3D重建，通过动态高斯建模解决各向异性体积重建问题", "motivation": "体积电子显微镜(vEM)虽然能实现纳米级3D成像，但受采集限制产生各向异性体积，现有基于各向同性假设的深度学习方法在处理形态各向异性结构时失效", "method": "将切片到3D重建重新定义为基于高斯溅射的3D动态场景渲染问题，将轴向切片进展建模为2D高斯点云的时间演化，并引入教师-学生引导机制增强数据稀疏区域的重建保真度", "result": "相比基于扩散和GAN的重建方法，EMGauss显著提高了插值质量，实现了连续切片合成，且无需大规模预训练", "conclusion": "EMGauss为vEM提供了一种通用的切片到3D重建框架，克服了传统各向同性方法的局限性，并可能扩展到其他成像领域"}}
{"id": "2512.06681", "pdf": "https://arxiv.org/pdf/2512.06681", "abs": "https://arxiv.org/abs/2512.06681", "authors": ["Amartya Hatua"], "title": "Mechanistic Interpretability of GPT-2: Lexical and Contextual Layers in Sentiment Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a mechanistic interpretability study of GPT-2 that causally examines how sentiment information is processed across its transformer layers. Using systematic activation patching across all 12 layers, we test the hypothesized two-stage sentiment architecture comprising early lexical detection and mid-layer contextual integration. Our experiments confirm that early layers (0-3) act as lexical sentiment detectors, encoding stable, position specific polarity signals that are largely independent of context. However, all three contextual integration hypotheses: Middle Layer Concentration, Phenomenon Specificity, and Distributed Processing are falsified. Instead of mid-layer specialization, we find that contextual phenomena such as negation, sarcasm, domain shifts etc. are integrated primarily in late layers (8-11) through a unified, non-modular mechanism. These experimental findings provide causal evidence that GPT-2's sentiment computation differs from the predicted hierarchical pattern, highlighting the need for further empirical characterization of contextual integration in large language models.", "AI": {"tldr": "对GPT-2进行机制可解释性研究，通过系统性的激活修补实验，检验情感信息在Transformer层中的处理方式，特别是验证早期层作为词汇情感检测器和中层进行上下文整合的假设。", "motivation": "理解GPT-2如何处理情感信息，特别是检验关于情感处理的两阶段架构假设：早期层进行词汇情感检测，中层进行上下文整合。这有助于揭示大型语言模型中情感计算的实际工作机制。", "method": "使用系统性的激活修补技术，在GPT-2的所有12个层上进行实验。通过修补不同层的激活来测试情感信息处理的因果机制，特别关注早期层（0-3）和中层的情感处理功能。", "result": "早期层（0-3）确实作为词汇情感检测器，编码稳定、位置特定的极性信号，基本独立于上下文。但所有三个上下文整合假设（中层集中、现象特异性、分布式处理）都被证伪。上下文现象（否定、讽刺、领域转移等）主要在晚期层（8-11）通过统一的非模块化机制整合，而非中层专门化。", "conclusion": "GPT-2的情感计算与预测的分层模式不同，上下文整合主要在晚期层而非中层进行，且通过统一机制而非模块化方式。这表明需要对大型语言模型中的上下文整合进行进一步的实证表征。"}}
{"id": "2512.06678", "pdf": "https://arxiv.org/pdf/2512.06678", "abs": "https://arxiv.org/abs/2512.06678", "authors": ["Shrihari Sridharan", "Deepak Ravikumar", "Anand Raghunathan", "Kaushik Roy"], "title": "GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.", "AI": {"tldr": "提出GradientSpace框架，通过在全维梯度空间中直接聚类数据样本来改进指令调优，解决梯度干扰问题，并训练专门的LoRA专家模型", "motivation": "现实世界数据集通常是异质的混合数据，导致梯度干扰问题（冲突的梯度将模型拉向相反方向，降低性能）。现有的基于语义或嵌入相似性的聚类方法无法捕捉数据在学习过程中如何影响模型参数，而现有的梯度聚类方法存在精度损失和推理成本高的问题", "method": "提出GradientSpace框架：1）在全维梯度空间中直接聚类样本，避免随机投影导致的精度损失；2）引入基于在线SVD的算法，在LoRA梯度上操作以识别潜在技能，无需存储所有样本梯度；3）为每个聚类训练专门的LoRA专家模型；4）训练轻量级路由器在推理时选择最佳专家", "result": "在数学推理、代码生成、金融和创意写作任务上的实验表明，GradientSpace能够实现一致的专家专业化，相比最先进的聚类方法和微调技术获得一致的准确率提升，同时显著降低推理延迟", "conclusion": "GradientSpace通过在全维梯度空间中直接聚类数据，有效解决了指令调优中的梯度干扰问题，相比现有方法在保持高精度的同时显著降低了推理成本，为改进LLM的指令调优提供了有效框架"}}
{"id": "2512.06676", "pdf": "https://arxiv.org/pdf/2512.06676", "abs": "https://arxiv.org/abs/2512.06676", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving", "categories": ["cs.RO", "cs.LG"], "comment": "9 pages", "summary": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.", "AI": {"tldr": "提出FedDSR框架，通过深度监督和正则化解决联邦学习中自动驾驶模型的泛化差和收敛慢问题", "motivation": "联邦学习在自动驾驶应用中面临非独立同分布数据导致的模型泛化能力差和收敛速度慢的挑战", "method": "FedDSR包含三个核心策略：1) 基于架构无关标准选择多个中间层；2) 计算互信息和负熵作为中间损失和正则项；3) 在中心服务器聚合车辆模型", "result": "在语义分割任务上，FedDSR相比其他联邦学习基线方法，mIoU提升最高达8.93%，训练轮次减少28.57%", "conclusion": "FedDSR通过中间层监督和正则化有效提升了联邦学习在自动驾驶系统中的模型泛化能力和收敛速度，适合实际部署"}}
{"id": "2512.06674", "pdf": "https://arxiv.org/pdf/2512.06674", "abs": "https://arxiv.org/abs/2512.06674", "authors": ["Songping Wang", "Rufan Qian", "Yueming Lyu", "Qinglong Liu", "Linzhuang Zou", "Jie Qin", "Songhua Liu", "Caifeng Shan"], "title": "RunawayEvil: Jailbreaking the Image-to-Video Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a \"Strategy-Tactic-Action\" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.", "AI": {"tldr": "提出了首个针对图像到视频生成模型的多模态越狱攻击框架RunawayEvil，具备动态演化能力，能够自动增强攻击策略", "motivation": "图像到视频生成系统在安全性和越狱攻击脆弱性方面研究不足，需要开发有效的漏洞分析工具来评估和提升模型鲁棒性", "method": "基于\"策略-战术-行动\"范式，包含三个核心组件：策略感知命令单元（通过强化学习和LLM实现策略演化）、多模态战术规划单元（生成协调的文本指令和图像篡改指南）、战术行动单元（执行和评估攻击）", "result": "在商业I2V模型（如Open-Sora 2.0和CogVideoX）上实现了最先进的攻击成功率，在COCO2017数据集上比现有方法高出58.5%到79%", "conclusion": "该工作为I2V模型的漏洞分析提供了关键工具，为构建更鲁棒的视频生成系统奠定了基础，揭示了多模态生成模型的安全风险"}}
{"id": "2512.06673", "pdf": "https://arxiv.org/pdf/2512.06673", "abs": "https://arxiv.org/abs/2512.06673", "authors": ["Shida Gao", "Feng Xue", "Xiangfeng Wang", "Anlong Ming", "Teng Long", "Yihua Shao", "Haozhe Wang", "Zhaowen Lin", "Wei Wang", "Nicu Sebe"], "title": "1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.", "AI": {"tldr": "提出DEViL模型，通过结合视频大语言模型和开放词汇检测器来解决时空定位和推理任务，避免传统自回归空间解码导致的误差累积问题", "motivation": "当前多模态大语言模型将边界框作为文本标记进行自回归生成，导致输出序列过长、空间误差随时间累积以及定位结果在视频中逐渐漂移的问题", "method": "提出DEViL模型，包含：1）引用语义标记（RST）连接MLLM和检测器，将用户查询提炼为丰富语义表示；2）在OVD中提出管状挖掘时间正则化（TTReg），确保生成时间一致的目标对象查询", "result": "DEViL在各种细粒度视频理解任务中表现出色，特别是在STVG和GroundedVQA任务上取得强劲性能", "conclusion": "DEViL通过检测器赋能视频大语言模型，有效解决了时空定位中的误差累积问题，实现了端到端的指代理解和空间定位学习"}}
{"id": "2512.06665", "pdf": "https://arxiv.org/pdf/2512.06665", "abs": "https://arxiv.org/abs/2512.06665", "authors": ["Panagiota Kiourti", "Anu Singh", "Preeti Duraipandian", "Weichao Zhou", "Wenchao Li"], "title": "Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.", "AI": {"tldr": "该论文重新思考特征归因方法的鲁棒性评估，提出新的相似输入定义、鲁棒性指标和基于生成对抗网络的生成方法，挑战当前主要忽略模型输出差异的归因鲁棒性概念。", "motivation": "当前的特征归因方法鲁棒性评估主要忽略模型输出的差异，导致评估不够客观。作者认为需要一种新的评估方法，能够更准确地揭示归因方法本身的弱点，而不是神经网络的弱点。", "method": "提出三个核心创新：1) 新的相似输入定义；2) 新的鲁棒性评估指标；3) 基于生成对抗网络的方法来生成这些相似输入。同时进行了与现有指标和最先进归因方法的全面评估。", "result": "研究发现当前归因鲁棒性评估方法存在缺陷，提出的新方法能够更客观地评估归因方法的鲁棒性，揭示归因方法本身的弱点而非神经网络的弱点。", "conclusion": "需要更客观的指标来评估特征归因方法的鲁棒性，提出的新方法为这一领域提供了更准确的评估框架，有助于更好地理解和改进归因方法的鲁棒性。"}}
{"id": "2512.06664", "pdf": "https://arxiv.org/pdf/2512.06664", "abs": "https://arxiv.org/abs/2512.06664", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Jingreng Lei", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving", "categories": ["cs.RO"], "comment": "9 pages", "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.", "AI": {"tldr": "该论文提出了一种基于统计增强的解耦MoE路由与聚合机制（MoE-RAM），用于自动驾驶场景中的语义分割任务，通过统计检索和自适应加权来改进专家选择和融合效果。", "motivation": "自动驾驶场景复杂多样，单一深度学习模型难以有效覆盖所有可能条件（如不同天气、交通密度、道路类型）。现有的MoE方法存在路由策略不精确导致专家选择错误，以及专家聚合效率低下导致预测性能不佳的问题。", "method": "提出MoE-RAM机制：1）通过统计检索机制增强专家路由，将大模型提取的潜在特征与缓存的专家原型特征进行匹配；2）通过测量专家即时特征与大模型提取潜在特征之间的统计距离，自适应地重新加权专家输出进行融合。", "result": "在自动驾驶数据集上的大量实验表明，MoE-RAM相比其他MoE基线和传统单模型方法具有优越性，显著提升了预测性能。", "conclusion": "统计增强的MoE路由与聚合机制的协同作用有效提升了自动驾驶语义分割任务的预测性能，为解决复杂自动驾驶场景下的模型适应性提供了有效解决方案。"}}
{"id": "2512.06663", "pdf": "https://arxiv.org/pdf/2512.06663", "abs": "https://arxiv.org/abs/2512.06663", "authors": ["Yu Qi", "Yumeng Zhang", "Chenting Gong", "Xiao Tan", "Weiming Zhang", "Wei Zhang", "Jingdong Wang"], "title": "CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.", "AI": {"tldr": "提出CoT4Det框架，将感知导向的视觉语言任务重新表述为分类、计数和定位三个可解释步骤，以提升大视觉语言模型在感知任务上的性能", "motivation": "大视觉语言模型在通用视觉问答和OCR任务上表现出色，但在感知中心任务（如目标检测、语义分割、深度估计）上性能显著低于任务特定的专家模型，特别是在密集场景和小目标召回方面存在困难", "method": "提出Chain-of-Thought for Detection (CoT4Det)框架，将感知任务重新表述为三个可解释步骤：分类（识别图像中的对象类别）、计数（统计每个类别的实例数量）、定位（确定每个实例的位置），这些步骤更自然地与LVLMs的推理能力对齐", "result": "使用标准Qwen2.5-VL-7B-Instruct模型，CoT4Det将COCO2017 val上的mAP从19.0%提升到33.0%，在RefCOCO系列上优于基线+2%，在Flickr30k entities上优于基线19%，在各种感知基准测试中取得竞争性结果", "conclusion": "CoT4Det是一种简单但高效的策略，能显著提升大视觉语言模型在感知任务上的性能，同时不损害其通用视觉语言能力，为解决LVLMs在感知任务上的局限性提供了有效方法"}}
{"id": "2512.06662", "pdf": "https://arxiv.org/pdf/2512.06662", "abs": "https://arxiv.org/abs/2512.06662", "authors": ["Ruoyu Xue", "Hieu Le", "Jingyi Xu", "Sounak Mondal", "Abe Leite", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Personalized Image Descriptions from Attention Sequences", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.", "AI": {"tldr": "提出DEPER方法，通过建模个性化视觉注意力模式来生成个性化图像描述，结合语言风格和观看行为，实现少样本个性化而不需要重新训练模型", "motivation": "现有个性化图像描述模型只关注语言风格，忽略了个人观看模式（注意力分布）的差异。不同人对同一图像的关注区域、对象和细节顺序不同，导致描述存在显著差异，需要同时建模语言风格和观看行为", "method": "提出DEPER方法，学习一个主体嵌入来捕捉语言风格和观看行为，通过辅助注意力预测任务进行指导。使用轻量级适配器将这些嵌入与冻结的视觉语言模型对齐，实现少样本个性化而不需要重新训练", "result": "在四个数据集上（涵盖不同观看任务和简短/详细描述），DEPER平均提升24%，表明建模个性化注意力能产生更符合人类认知和更高质量的描述", "conclusion": "理解人们如何观看图像有助于预测他们会说什么；建模人类感知的多样性可以提高多模态系统的性能和人类对齐性。个性化注意力建模是生成更人性化图像描述的关键因素"}}
{"id": "2512.06660", "pdf": "https://arxiv.org/pdf/2512.06660", "abs": "https://arxiv.org/abs/2512.06660", "authors": ["Saleha Muzammil", "Rahul Reddy", "Vishal Kamalakrishnan", "Hadi Ahmadi", "Wajih Ul Hassan"], "title": "Towards Small Language Models for Security Query Generation in SOC Workflows", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Analysts in Security Operations Centers routinely query massive telemetry streams using Kusto Query Language (KQL). Writing correct KQL requires specialized expertise, and this dependency creates a bottleneck as security teams scale. This paper investigates whether Small Language Models (SLMs) can enable accurate, cost-effective natural-language-to-KQL translation for enterprise security. We propose a three-knob framework targeting prompting, fine-tuning, and architecture design. First, we adapt existing NL2KQL framework for SLMs with lightweight retrieval and introduce error-aware prompting that addresses common parser failures without increasing token count. Second, we apply LoRA fine-tuning with rationale distillation, augmenting each NLQ-KQL pair with a brief chain-of-thought explanation to transfer reasoning from a teacher model while keeping the SLM compact. Third, we propose a two-stage architecture that uses an SLM for candidate generation and a low-cost LLM judge for schema-aware refinement and selection. We evaluate nine models (five SLMs and four LLMs) across syntax correctness, semantic accuracy, table selection, and filter precision, alongside latency and token cost. On Microsoft's NL2KQL Defender Evaluation dataset, our two-stage approach achieves 0.987 syntax and 0.906 semantic accuracy. We further demonstrate generalizability on Microsoft Sentinel data, reaching 0.964 syntax and 0.831 semantic accuracy. These results come at up to 10x lower token cost than GPT-5, establishing SLMs as a practical, scalable foundation for natural-language querying in security operations.", "AI": {"tldr": "该论文研究如何利用小型语言模型(SLMs)为安全运营中心(SOC)工作流生成安全查询，实现自然语言到Kusto查询语言(KQL)的准确翻译，以解决安全团队扩展时的查询瓶颈问题。", "motivation": "安全运营中心分析师需要查询海量遥测数据，但编写正确的KQL查询需要专业知识，这成为安全团队扩展时的瓶颈。现有大型语言模型虽然有效但成本高昂，因此需要探索小型语言模型是否能提供准确且经济高效的解决方案。", "method": "提出三旋钮框架：1) 轻量级检索和错误感知提示，针对常见解析错误优化提示而不增加token数量；2) LoRA微调与推理蒸馏，通过添加简短的思维链解释将教师模型的推理能力转移到小型模型；3) 两阶段架构，使用SLM生成候选查询，再用低成本LLM进行模式感知的细化和选择。", "result": "在微软NL2KQL Defender评估数据集上，两阶段方法达到0.987的语法准确率和0.906的语义准确率。在微软Sentinel数据上达到0.964语法准确率和0.831语义准确率，相比GPT-5降低高达10倍的token成本。", "conclusion": "小型语言模型可以作为安全运营中自然语言查询的实用、可扩展基础，在保持高准确率的同时显著降低成本，为解决安全团队扩展时的查询瓶颈提供了有效方案。"}}
{"id": "2512.06658", "pdf": "https://arxiv.org/pdf/2512.06658", "abs": "https://arxiv.org/abs/2512.06658", "authors": ["Xiao Wang", "Aydogan Ozcan"], "title": "Phase-multiplexed optical computing: Reconfiguring a multi-task diffractive optical processor using illumination phase diversity", "categories": ["physics.optics", "cs.NE", "physics.app-ph"], "comment": "19 Pages, 7 Figures", "summary": "We report a monochrome multi-task diffractive network architecture that leverages illumination phase multiplexing to dynamically reconfigure its output function and accurately implement a large group of complex-valued linear transformations between an input and output aperture. Each member of the desired group of T unique transformations is encoded and addressed with a distinct 2D illumination phase profile, termed \"phase key\", which illuminates the input aperture, activating the corresponding transformation at the output field-of-view. A common diffractive optical network, optimized with T phase keys, demultiplexes these encoded inputs and accurately executes any of the T distinct linear transformations at its output. We demonstrate that a diffractive network composed of N = 2 x T x Ni x No optimized diffractive features can realize T distinct complex-valued linear transformations, accurately executed for any complex field at the input aperture, where Ni and No refer to the input/output pixels, respectively. In our proof-of-concept numerical analysis, T = 512 complex-valued transformations are implemented by the same monochrome diffractive network with negligible error using illumination phase diversity. Compared with wavelength-multiplexed diffractive systems, phase-multiplexing architecture significantly lowers the transformation errors, potentially enabling larger-scale optical transformations to be implemented through a monochrome processor. Phase-multiplexed multi-task diffractive networks would enhance the capabilities of optical computing and machine-vision systems.", "AI": {"tldr": "提出一种基于相位多路复用的多任务衍射光学处理器，通过不同的照明相位密钥动态重配置输出功能，实现大量复值线性变换", "motivation": "现有光学计算系统通常只能执行单一任务，缺乏动态重配置能力。需要开发能够实现多种复杂光学变换的可重构处理器，以增强光学计算和机器视觉系统的能力", "method": "采用相位多路复用衍射网络架构，为每个目标变换分配独特的二维照明相位轮廓（相位密钥）。通过共同优化的衍射光学网络，使用T个相位密钥对编码输入进行解复用，在输出视场中激活相应的变换", "result": "证明由N = 2 × T × Ni × No个优化衍射特征组成的网络可以实现T个不同的复值线性变换。在概念验证数值分析中，使用相位多样性实现了T=512个复值变换，误差可忽略不计。相比波长多路复用系统，相位多路复用架构显著降低了变换误差", "conclusion": "相位多路复用多任务衍射网络通过照明相位多样性实现动态重配置，能够准确执行大量复杂光学变换，为单色处理器实现更大规模光学变换提供了可能，将增强光学计算和机器视觉系统的能力"}}
{"id": "2512.06657", "pdf": "https://arxiv.org/pdf/2512.06657", "abs": "https://arxiv.org/abs/2512.06657", "authors": ["Qiyan Zhao", "Yue Yan", "Da-Han Wang"], "title": "TextMamba: Scene Text Detector with Mamba", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\\%, 89.2\\%, and 78.5\\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.", "AI": {"tldr": "提出一种基于Mamba状态空间模型的新型场景文本检测器TextMamba，通过整合选择机制与注意力层来增强编码器从长序列中提取相关信息的能力", "motivation": "Transformer-based方法在场景文本检测中虽然解决了CNN的全局特征提取限制，但存在跨领域局限性和固有缺点：在建模长距离依赖时可能遗忘重要信息或关注无关表示。Mamba状态空间模型通过线性复杂度选择机制展示了更好的长距离依赖建模能力", "method": "1) 整合Mamba的选择机制与注意力层；2) 采用Top_k算法显式选择关键信息并减少Mamba建模中的无关信息干扰；3) 设计双尺度前馈网络和嵌入金字塔增强模块，促进高维隐藏状态交互和多尺度特征融合", "result": "在多个基准测试中达到最先进或竞争性性能：CTW1500上F-measure为89.7%，TotalText上为89.2%，ICDAR19ArT上为78.5%", "conclusion": "提出的TextMamba通过整合Mamba的选择机制和注意力层，有效解决了Transformer-based方法在长距离依赖建模中的局限性，在场景文本检测任务中取得了优异性能"}}
{"id": "2512.06655", "pdf": "https://arxiv.org/pdf/2512.06655", "abs": "https://arxiv.org/abs/2512.06655", "authors": ["Jehyeok Yeon", "Federico Cinus", "Yifan Wu", "Luca Luceri"], "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.", "AI": {"tldr": "提出一种图正则化稀疏自编码器（GSAE）方法，用于增强大语言模型的安全导向能力，通过分布式安全表示来对抗对抗性提示和越狱攻击。", "motivation": "现有LLM安全防御方法存在局限性：黑盒护栏只能过滤输出，而基于内部激活的方法通常将安全概念简化为单一潜在特征或维度。然而，研究表明抽象概念（如拒绝和时序性）分布在多个特征中而非孤立存在，因此需要更精细的分布式安全表示方法。", "method": "扩展稀疏自编码器（SAE），引入拉普拉斯平滑惩罚项到神经元共激活图上，形成图正则化稀疏自编码器（GSAE）。该方法恢复平滑的分布式安全表示，将特征组合成加权安全相关方向集，并通过两阶段门控机制控制：仅在检测到有害提示或生成内容时激活干预。", "result": "GSAE导向在安全和QA基准测试中平均达到82%的选择性拒绝率，显著优于标准SAE导向（42%），同时保持强大的任务准确性（TriviaQA 70%，TruthfulQA 65%，GSM8K 74%）。在LLaMA-3、Mistral、Qwen和Phi模型家族中均表现出良好的泛化能力，对越狱攻击（GCG、AutoDAN）具有鲁棒性，始终保持≥90%的有害内容拒绝率。", "conclusion": "GSAE通过图正则化实现了分布式安全表示，有效解决了LLM安全导向问题，在保持良性查询实用性的同时，自适应地强制执行拒绝，显著提高了对有害内容的防御能力。"}}
{"id": "2512.06653", "pdf": "https://arxiv.org/pdf/2512.06653", "abs": "https://arxiv.org/abs/2512.06653", "authors": ["Hengzhi Lan", "Yue Yu", "Li Qian", "Li Peng", "Jie Wu", "Wei Liu", "Jian Luan", "Ting Bai"], "title": "LightSearcher: Efficient DeepSearch via Experiential Memory", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.", "AI": {"tldr": "提出LightSearcher框架，通过经验记忆和自适应奖励机制，在保持准确性的同时显著提升深度搜索的效率", "motivation": "现有的RL驱动深度搜索系统存在准确性-效率权衡问题：频繁调用搜索工具可以提高事实正确性，但会导致不必要的计算开销和效率下降", "method": "1) 引入文本经验记忆，通过学习对比推理轨迹生成可解释的成功推理模式摘要；2) 采用自适应奖励塑造机制，仅在正确答案场景中惩罚冗余工具调用", "result": "在四个多跳QA基准测试中，LightSearcher保持与SOTA基线ReSearch相当的准确性，同时将搜索工具调用减少39.6%，推理时间减少48.6%，token消耗减少21.2%", "conclusion": "LightSearcher通过经验记忆和自适应奖励机制有效平衡了深度搜索范式中的准确性-效率权衡，实现了高效且准确的深度推理"}}
{"id": "2512.06652", "pdf": "https://arxiv.org/pdf/2512.06652", "abs": "https://arxiv.org/abs/2512.06652", "authors": ["Xiaolei Lu", "Shamim Nemati"], "title": "Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.", "AI": {"tldr": "提出Adaptive Test-Time Training (AdaTTT)框架，用于多中心ICU患者中预测有创机械通气需求，通过测试时训练缓解领域偏移问题", "motivation": "ICU患者有创机械通气需求预测对及时干预和资源分配至关重要，但不同机构间的患者群体、临床实践和电子健康记录系统差异导致领域偏移，降低预测模型的泛化性能", "method": "提出AdaTTT增强框架：1) 推导测试时预测误差的信息论界限；2) 引入自监督学习框架，包含重构和掩码特征建模任务，采用动态掩码策略强调对主任务关键的特征；3) 结合原型学习和部分最优传输进行灵活的部分特征对齐", "result": "在多中心ICU队列实验中，AdaTTT在不同测试时适应基准上展现出具有竞争力的分类性能", "conclusion": "AdaTTT框架通过增强主任务和辅助任务的对齐，结合原型学习和部分最优传输，有效缓解了领域偏移问题，提高了ICU患者有创机械通气需求预测的泛化能力"}}
{"id": "2512.06649", "pdf": "https://arxiv.org/pdf/2512.06649", "abs": "https://arxiv.org/abs/2512.06649", "authors": ["Camellia Zakaria", "Aryan Sadeghi", "Weaam Jaafar", "Junshi Xu", "Alex Mariakakis", "Marianne Hatzopoulou"], "title": "Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning", "categories": ["cs.LG", "cs.CV", "cs.CY", "cs.ET"], "comment": "12 pages, 16 figures, 4 tables, 4 pages Appendix, in submission and under review for ACM MobiSys 2026 as of December 6th, 2025", "summary": "Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.", "AI": {"tldr": "开发基于视觉的机器学习系统，利用交通监控视频估计城市交通产生的黑碳浓度", "motivation": "城市黑碳监测成本高、数据稀缺，而交通监控系统广泛部署，存在监测数据与环境后果信息之间的不平衡", "method": "从交通视频中提取车辆行为和状况的视觉信息，结合天气数据，构建机器学习模型估计街道级别的黑碳浓度", "result": "模型达到R平方值0.72，RMSE为129.42 ng/m³，能够有效估计黑碳浓度", "conclusion": "该工作利用现有城市基础设施和建模技术，为交通排放提供相关信息，支持污染减排、城市规划、公共卫生和环境正义"}}
{"id": "2512.06648", "pdf": "https://arxiv.org/pdf/2512.06648", "abs": "https://arxiv.org/abs/2512.06648", "authors": ["Xiao Li"], "title": "Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "in Chinese language", "summary": "Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness. This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings. To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.", "AI": {"tldr": "基于卷积神经网络(CNN)的上市公司财务欺诈识别与可解释性研究，提出一种将公司年度面板数据转换为类图像表示的特征工程方案，实现提前预测财务欺诈。", "motivation": "上市公司财务欺诈屡次破坏资本市场，传统统计模型难以处理非线性特征交互，机器学习模型缺乏可解释性，且现有方法大多仅基于当年数据判断当年欺诈，时效性有限。", "method": "设计特征工程方案将公司年度面板数据转换为类图像表示，使用卷积神经网络捕捉横截面和时间模式，实现提前预测欺诈，并采用局部解释技术从实体、特征和时间三个维度分析模型。", "result": "CNN在准确性、鲁棒性和预警性能上优于逻辑回归和LightGBM，适当的分类阈值调整在高风险场景中至关重要；偿债能力、比率结构、治理结构和内部控制是欺诈的普遍预测因子，环境指标主要在高污染行业重要。", "conclusion": "提出的CNN框架能有效识别财务欺诈并实现提前预警，通过可解释性分析揭示了欺诈公司的异质性模式集中在短期时间窗口，非欺诈公司则表现出稳定的特征模式，为监管和审计提供了实用工具。"}}
{"id": "2512.06647", "pdf": "https://arxiv.org/pdf/2512.06647", "abs": "https://arxiv.org/abs/2512.06647", "authors": ["Siyu Qiu", "Anqi Lin", "Shiya Wang", "Xingyu Lan"], "title": "Exploring Teenagers' Trust in Al Chatbots: An Empirical Study of Chinese Middle-School Students", "categories": ["cs.HC"], "comment": null, "summary": "Chatbots have become increasingly prevalent. A growing body of research focused on the issue of human trust in AI. However, most existing user studies are conducted primarily with adult groups, overlooking teenagers who are also engaging more frequently with AI technologies. Based on previous theories about teenage education and psychology, this study investigates the correlation between teenagers' psychological characteristics and their trust in AI chatbots, examining four key variables: AI literacy, ego identity, social anxiety, and psychological resilience. We adopted a mixed-methods approach, combining an online survey with semi-structured interviews. Our findings reveal that psychological resilience is a significant positive predictor of trust in AI, and that age significantly moderates the relationship between social anxiety and trust. The interviews further suggest that teenagers generally report relatively high levels of trust in AI, tend to overestimate their AI literacy, and are influenced by external factors such as social media.", "AI": {"tldr": "探索中国中学生对AI聊天机器人的信任度及其与心理特征的关系", "motivation": "现有关于人类对AI信任的研究主要关注成人群体，忽视了青少年这一日益频繁接触AI技术的群体。基于青少年教育和心理学理论，本研究旨在填补这一研究空白。", "method": "采用混合方法：在线问卷调查结合半结构化访谈，考察AI素养、自我认同、社交焦虑和心理韧性四个关键变量。", "result": "心理韧性是AI信任的显著正向预测因子；年龄显著调节社交焦虑与信任之间的关系；青少年普遍报告较高的AI信任度，倾向于高估自己的AI素养，并受社交媒体等外部因素影响。", "conclusion": "青少年对AI聊天机器人的信任受到心理特征的显著影响，特别是心理韧性的作用，年龄在社交焦虑与信任关系中起调节作用，为青少年AI教育提供了重要启示。"}}
{"id": "2512.06642", "pdf": "https://arxiv.org/pdf/2512.06642", "abs": "https://arxiv.org/abs/2512.06642", "authors": ["Achmad Ardani Prasha", "Clavino Ourizqi Rachmadi", "Muhamad Fauzan Ibnu Syahlan", "Naufal Rahfi Anugerah", "Nanda Garin Raditya", "Putri Amelia", "Sabrina Laila Mutiara", "Hilman Syachr Ramadhan"], "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution", "categories": ["cs.CV", "astro-ph.CO", "astro-ph.IM", "cs.AI", "cs.LG"], "comment": "21 pages, 7 figures, 3 table", "summary": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.", "AI": {"tldr": "本文提出了一种基于掩码自编码器（MAE）的预训练策略，用于强引力透镜图像分析，以解决暗物质模型分类和超分辨率两个下游任务。", "motivation": "强引力透镜可以揭示暗物质亚结构的影响，但从噪声、低分辨率图像中分析这些效应具有挑战性。需要一种能够同时处理暗物质模型分类和图像增强的方法。", "method": "使用掩码自编码器（MAE）在DeepLense ML4SCI基准的模拟强引力透镜图像上进行预训练，学习可泛化的表示。预训练采用掩码图像建模目标，然后针对分类和超分辨率两个任务分别微调编码器。", "result": "在90%掩码率下，微调后的分类器达到宏观AUC 0.968和准确率88.65%，优于从头训练的基线（AUC 0.957，准确率82.46%）。超分辨率任务（16x16到64x64）中，MAE预训练模型重建图像的PSNR约33 dB，SSIM 0.961，略优于从头训练。研究发现掩码率存在权衡：更高的掩码率改善分类但轻微降低重建保真度。", "conclusion": "MAE预训练在物理丰富的模拟数据上提供了一个灵活、可重用的编码器，适用于多个强引力透镜分析任务，证明了预训练策略在物理数据分析中的有效性。"}}
{"id": "2512.06629", "pdf": "https://arxiv.org/pdf/2512.06629", "abs": "https://arxiv.org/abs/2512.06629", "authors": ["Xiao-li Xia", "Hou-biao Li"], "title": "FlatFormer: A Flat Transformer Knowledge Tracing Model Based on Cognitive Bias Injection", "categories": ["cs.AI", "cs.IT"], "comment": "36 pages, 14 figures,Table 5", "summary": "Knowledge Tracing (KT) models face a critical ``Performance-Complexity Trap'': capturing complex cognitive dynamics like learning sessions and memory decay typically requires deep hierarchical architectures, which incur prohibitive computational costs for real-time deployment. To resolve this, we propose FlatFormer, a streamlined architecture based on the novel design paradigm of ``Information Injection over Structural Stacking.'' Unlike parameter-heavy hierarchical models, FlatFormer leverages a standard flat Transformer augmented with two lightweight injection mechanisms: (i) a hybrid input encoding strategy combining learnable session identifiers with fixed sinusoidal step embeddings; and (ii) a pre-computed power-law bias integrated directly into attention logits to explicitly model the forgetting curve. Extensive experiments on four large-scale datasets (e.g., EdNet, Junyi) show that FlatFormer achieves state-of-the-art performance. For example, on the EdNet dataset, compared to the strongest hierarchical baseline (HiTSKT), its absolute AUC increased by 8.3%, while using less than 15% of parameters, and inference speed was about three times faster. These results validate that high cognitive fidelity does not necessitate architectural complexity.", "AI": {"tldr": "提出FlatFormer模型解决知识追踪中的\"性能-复杂度陷阱\"，通过信息注入而非结构堆叠的方式，在保持扁平Transformer架构的同时实现高性能知识追踪", "motivation": "传统知识追踪模型面临\"性能-复杂度陷阱\"：捕捉复杂认知动态（如学习会话和记忆衰减）需要深层层次架构，导致计算成本过高，难以实时部署", "method": "采用\"信息注入而非结构堆叠\"的设计范式，在标准扁平Transformer基础上添加两个轻量级注入机制：1) 混合输入编码策略（可学习会话标识符+固定正弦步长嵌入）；2) 预计算幂律偏置直接集成到注意力logits中以显式建模遗忘曲线", "result": "在四个大规模数据集（如EdNet、Junyi）上的实验表明，FlatFormer达到最先进性能。在EdNet数据集上，相比最强层次基线（HiTSKT），绝对AUC提升8.3%，参数使用量不到15%，推理速度约快3倍", "conclusion": "验证了高认知保真度不一定需要架构复杂性，通过信息注入而非结构堆叠的设计范式可以在保持计算效率的同时实现高性能知识追踪"}}
{"id": "2512.06628", "pdf": "https://arxiv.org/pdf/2512.06628", "abs": "https://arxiv.org/abs/2512.06628", "authors": ["Ruicheng Zhang", "Mingyang Zhang", "Jun Zhou", "Zhangrui Guo", "Xiaofan Liu", "Zunnan Xu", "Zhizhou Zhong", "Puxin Yan", "Haocheng Luo", "Xiu Li"], "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "AI": {"tldr": "MIND-V是一个用于长时程机器人操作视频生成的分层框架，通过结合高层推理和像素级合成来生成物理合理且逻辑连贯的机器人操作视频。", "motivation": "现有的机器人操作视频生成模型只能合成简单动作的短片段，且通常依赖手动定义的轨迹，而具身模仿学习受到多样长时程机器人操作数据稀缺的限制。", "method": "MIND-V包含三个核心组件：语义推理中心（SRH）用于任务规划，行为语义桥（BSB）将抽象指令转换为领域不变表示，以及运动视频生成器（MVG）进行条件视频渲染。采用分阶段视觉未来展开策略增强长时程鲁棒性，并使用基于GRPO强化学习的后训练阶段，通过物理前瞻一致性（PFC）奖励来确保物理合理性。", "result": "MIND-V在长时程机器人操作视频生成方面展示了最先进的性能，为具身数据合成建立了可扩展且可控的范式。", "conclusion": "MIND-V通过分层框架成功解决了长时程机器人操作视频生成的挑战，结合认知科学原理和强化学习物理对齐，为具身人工智能的数据合成提供了有效的解决方案。"}}
{"id": "2512.06616", "pdf": "https://arxiv.org/pdf/2512.06616", "abs": "https://arxiv.org/abs/2512.06616", "authors": ["Rasam Dorri", "Rami Zwick"], "title": "Memory Power Asymmetry in Human-AI Relationships: Preserving Mutual Forgetting in the Digital Age", "categories": ["cs.HC", "cs.AI"], "comment": "31 pages, 2 tables, 2 figures", "summary": "As artificial intelligence (AI) becomes embedded in personal and professional relationships, a new kind of power imbalance emerges from asymmetric memory capabilities. Human relationships have historically relied on mutual forgetting, the natural tendency for both parties to forget details over time, as a foundation for psychological safety, forgiveness, and identity change. By contrast, AI systems can record, store, and recombine interaction histories at scale, often indefinitely. We introduce Memory Power Asymmetry (MPA): a structural power imbalance that arises when one relationship partner (typically an AI-enabled firm) possesses a substantially superior capacity to record, retain, retrieve, and integrate the shared history of the relationship, and can selectively deploy that history in ways the other partner (the human) cannot. Drawing on research in human memory, power-dependence theory, AI architecture, and consumer vulnerability, we develop a conceptual framework with four dimensions of MPA (persistence, accuracy, accessibility, integration) and four mechanisms by which memory asymmetry is translated into power (strategic memory deployment, narrative control, dependence asymmetry, vulnerability accumulation). We theorize downstream consequences at individual, relational/firm, and societal levels, formulate boundary-conditioned propositions, and articulate six design principles for restoring a healthier balance of memory in human-AI relationships (e.g., forgetting by design, contextual containment, symmetric access to records). Our analysis positions MPA as a distinct construct relative to information asymmetry, privacy, surveillance, and customer relationship management, and argues that protecting mutual forgetting, or at least mutual control over memory, should become a central design and policy goal in the AI age.", "AI": {"tldr": "本文提出了\"记忆权力不对称\"（MPA）概念，指在人类与AI关系中，一方（通常是AI系统）拥有显著更强的记录、存储、检索和整合关系历史的能力，并能选择性地利用这些历史数据，而另一方（人类）则无法做到，从而产生结构性权力失衡。", "motivation": "随着AI融入个人和职业关系，人类关系历史上依赖的\"相互遗忘\"（双方随时间自然遗忘细节）这一心理安全、宽恕和身份变化的基础正在被破坏。AI系统能够大规模、无限期地记录、存储和重组互动历史，导致新的权力不平衡。", "method": "基于人类记忆、权力依赖理论、AI架构和消费者脆弱性研究，开发了一个包含MPA四个维度（持久性、准确性、可访问性、整合性）和四个权力转化机制（战略记忆部署、叙事控制、依赖不对称、脆弱性积累）的概念框架。", "result": "提出了在个体、关系/公司和社会层面的下游后果，制定了边界条件命题，并阐述了六项设计原则，旨在恢复人类-AI关系中更健康的记忆平衡（如设计遗忘、情境遏制、对称访问记录等）。", "conclusion": "MPA是与信息不对称、隐私、监控和客户关系管理不同的独特概念，保护相互遗忘或至少对记忆的相互控制应成为AI时代设计和政策的核心目标。"}}
{"id": "2512.06613", "pdf": "https://arxiv.org/pdf/2512.06613", "abs": "https://arxiv.org/abs/2512.06613", "authors": ["Yueying Ke"], "title": "Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, 2 tables, IEEE conference format. Submitted as course project", "summary": "Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality. We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings. The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955). Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.", "AI": {"tldr": "该论文提出了一种用于硅藻图像分类的层次化深度学习方法，通过嵌入分类学层次结构来改进多级分类的准确性和错误定位。", "motivation": "传统硅藻分类依赖专家，现有深度学习方法多为扁平分类，无法利用分类学层次结构信息。作者探索将分类学层次嵌入神经网络架构是否能同时提高准确性和错误定位能力。", "method": "提出层次化卷积网络，包含五个级联头部分别预测纲、目、科、属、种。每个头接收共享骨干特征和来自更高层次的概率分布，使用二元掩码在训练和推理期间限制预测到有效后代。", "result": "层次模型在物种级别达到69.4%准确率（与扁平模型相当），在所有上级分类级别表现更优。当物种预测失败时，92.5%的错误分类物种在属级别被正确预测（扁平模型为67.2%）。层次模型将平均分类学距离降低了38.2%。", "conclusion": "层次化深度学习方法通过自上而下的约束掩码和自下而上的梯度传播机制，实现了更鲁棒、可解释且与生物学对齐的多级分类预测，为水生生态系统监测提供了更有效的自动化工具。"}}
{"id": "2512.06612", "pdf": "https://arxiv.org/pdf/2512.06612", "abs": "https://arxiv.org/abs/2512.06612", "authors": ["Kazuya Nishimura", "Haruka Hirose", "Ryoma Bise", "Kaito Shiku", "Yasuhiro Kojima"], "title": "Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics", "categories": ["cs.CV"], "comment": "Neurips 2025", "summary": "Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.", "AI": {"tldr": "该论文提出了一种从病理图像中学习基因相对表达趋势的新方法，而不是直接预测绝对表达值，以应对RNA测序中的随机噪声和批次效应问题。", "motivation": "从病理图像估计基因表达可以降低RNA测序成本，但现有方法使用逐点损失函数来最小化预测值与绝对基因表达值之间的差异。由于测序技术的复杂性和细胞间的内在变异性，观察到的基因表达包含随机噪声和批次效应，准确估计绝对表达值仍然是一个重大挑战。", "method": "提出学习相对表达模式而非绝对水平的新目标。假设基因的相对表达水平在独立实验中表现出一致的模式，即使绝对表达值受到组织样本中批次效应和随机噪声的影响。基于这一假设，提出了一种名为STRank的新型损失函数，该函数对噪声和批次效应具有鲁棒性。", "result": "在合成数据集和真实数据集上的实验证明了所提出方法的有效性。STRank损失函数能够更好地学习基因的相对表达趋势，相比传统方法在存在噪声和批次效应的情况下表现更优。", "conclusion": "通过关注相对表达模式而非绝对表达值，该方法能够更有效地从病理图像中学习基因表达信息，克服了传统方法在噪声和批次效应方面的局限性，为空间转录组学中的基因表达分析提供了更稳健的解决方案。"}}
{"id": "2512.06611", "pdf": "https://arxiv.org/pdf/2512.06611", "abs": "https://arxiv.org/abs/2512.06611", "authors": ["Rishi Gujjar", "Kevin Hua", "Robert Kleinberg", "Frederick V. Qiu"], "title": "The $k$-Fold Matroid Secretary Problem", "categories": ["cs.DS"], "comment": "11 pages, 1 figure, to appear in SOSA 2026", "summary": "In the matroid secretary problem, elements $N := [n]$ of a matroid $\\mathcal{M} \\subseteq 2^N$ arrive in random order. When an element arrives, its weight is revealed and a choice must be made to accept or reject the element, subject to the constraint that the accepted set $S \\in \\mathcal{M}$. Kleinberg'05 gives a $(1-O(1/\\sqrt{k}))$-competitive algorithm when $\\mathcal{M}$ is a $k$-uniform matroid. We generalize their result, giving a $(1-O(\\sqrt{\\log(n)/k}))$-competitive algorithm when $\\mathcal{M}$ is a $k$-fold matroid union.", "AI": {"tldr": "提出针对k-fold拟阵并的秘书问题算法，将Kleinberg的k-均匀拟阵结果推广到更一般的k-fold拟阵并结构", "motivation": "拟阵秘书问题中，现有算法主要针对k-均匀拟阵，需要扩展到更一般的k-fold拟阵并结构，以处理更广泛的组合优化问题", "method": "设计随机在线算法，当元素按随机顺序到达时，根据权重决定是否接受元素，同时保证接受集合属于k-fold拟阵并", "result": "提出了(1-O(√(log(n)/k)))-竞争比算法，将Kleinberg的(1-O(1/√k))结果推广到k-fold拟阵并", "conclusion": "成功将拟阵秘书问题的竞争比分析扩展到更一般的k-fold拟阵并结构，为更广泛的组合优化问题提供了理论保证"}}
{"id": "2512.06610", "pdf": "https://arxiv.org/pdf/2512.06610", "abs": "https://arxiv.org/abs/2512.06610", "authors": ["Marvin Harms", "Jaeyoung Lim", "David Rohr", "Friedrich Rockenbauer", "Nicholas Lawrance", "Roland Siegwart"], "title": "Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.", "AI": {"tldr": "提出一个基于鲁棒优化的固定翼无人机自主动态翱翔框架，利用风切变层能量实现无动力飞行", "motivation": "动态翱翔技术可以利用风切变层中的能量实现无动力飞行，但需要解决风场估计误差和鲁棒控制问题以实现自主飞行", "method": "采用显式风场表示和经典制导控制方法，构建点对点鲁棒参考路径用于动态翱翔，并开发鲁棒路径跟踪控制器", "result": "在仿真中展示了在不同风况、估计误差和干扰下的鲁棒动态翱翔飞行，关键组件在真实飞行中得到验证，仿真与真实差距小", "conclusion": "提出的框架能够实现固定翼无人机在风切变中的自主动态翱翔飞行，具有实际应用潜力"}}
{"id": "2512.06609", "pdf": "https://arxiv.org/pdf/2512.06609", "abs": "https://arxiv.org/abs/2512.06609", "authors": ["Tongda Xu", "Wendi Zheng", "Jiajun He", "Jose Miguel Hernandez-Lobato", "Yan Wang", "Ya-Qin Zhang", "Jie Tang"], "title": "Vector Quantization using Gaussian Variational Autoencoder", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.", "AI": {"tldr": "提出一种名为高斯量化(GQ)的技术，将高斯变分自编码器转换为向量量化变分自编码器，无需额外训练", "motivation": "传统的向量量化变分自编码器(VQ-VAE)由于离散化过程难以训练，需要寻找更简单有效的离散化方法", "method": "提出高斯量化(GQ)技术：生成随机高斯噪声作为码本，找到与后验均值最接近的噪声向量；并提出目标散度约束(TDC)来训练高斯VAE以优化GQ效果", "result": "GQ在UNet和ViT架构上都优于VQGAN、FSQ、LFQ、BSQ等现有VQ-VAE方法；TDC也改进了TokenBridge等高斯VAE离散化方法", "conclusion": "高斯量化提供了一种简单有效的将高斯VAE转换为VQ-VAE的方法，理论上有量化误差保证，实践中通过TDC进一步优化性能"}}
{"id": "2512.06608", "pdf": "https://arxiv.org/pdf/2512.06608", "abs": "https://arxiv.org/abs/2512.06608", "authors": ["Xinyu Zhou", "Songhao Piao", "Chao Gao", "Liguo Chen"], "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.", "AI": {"tldr": "提出一种新的轨迹导向方法，通过统一评估框架和轨迹曲率优化奖励策略，提升人群导航的综合性能", "motivation": "现有DRL方法在人群导航中缺乏对评估指标相对优先级的充分分析，且很少考虑轨迹连续性指标（特别是C²平滑度），导致评估不公平且轨迹优化不足", "method": "提出统一评估框架实现多目标优化优先级的透明评估，并设计新颖的奖励塑造策略，明确强调轨迹曲率优化", "result": "通过2D和3D实验验证，该方法在多尺度场景下显著提升轨迹质量和适应性，性能优于现有最先进方法", "conclusion": "提出的轨迹导向框架和曲率优化奖励策略有效解决了人群导航中的评估公平性和轨迹优化问题，为导航系统提供了更自然、舒适和节能的解决方案"}}
{"id": "2512.06598", "pdf": "https://arxiv.org/pdf/2512.06598", "abs": "https://arxiv.org/abs/2512.06598", "authors": ["Muhammad Adil", "Patrick J. Clemins", "Andrew W. Schroth", "Panagiotis D. Oikonomou", "Donna M. Rizzo", "Peter D. F. Isles", "Xiaohan Zhang", "Kareem I. Hannoun", "Scott Turnbull", "Noah B. Beckage", "Asim Zia", "Safwan Wshah"], "title": "From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain", "categories": ["cs.CV"], "comment": "23 pages, 15 figures", "summary": "Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.", "AI": {"tldr": "本文提出了一种基于遥感数据和Transformer-BiLSTM混合模型的蓝藻水华强度多时间尺度预测框架，用于预测尚普兰湖未来14天的蓝藻水华强度。", "motivation": "蓝藻有害藻华对水生生态系统和公共健康构成严重威胁。尚普兰湖由于营养富集和气候变化，其北部区域（Missisquoi湾、St. Albans湾和东北臂）特别容易发生蓝藻水华事件。遥感技术为监测和预测这些事件提供了可扩展的解决方案，特别是在现场观测稀疏或不可用的区域。", "method": "研究提出了一个仅使用遥感数据的预测框架，结合Transformer和双向长短期记忆网络来预测蓝藻水华强度。系统利用来自蓝藻评估网络的蓝藻指数数据和来自中分辨率成像光谱仪卫星的温度数据，捕捉卫星时间序列中的长期依赖关系和序列动态。针对数据集稀疏的问题（蓝藻指数数据缺失超过30%，温度数据缺失90%），采用了两阶段预处理流程：在像素级别应用前向填充和加权时间插补，然后进行平滑处理以减少蓝藻水华事件的不连续性。通过等频分箱处理蓝藻指数值并提取温度统计特征，将原始数据集转化为有意义的特征。", "result": "Transformer-BiLSTM模型在多个预测时间尺度上表现出强大的预测性能：在1天、2天和3天预测中分别获得89.5%、86.4%和85.5%的F1分数；在14天预测时间尺度上，F1分数保持在78.9%，AUC为82.6%。", "conclusion": "这些结果证实了该模型能够从稀疏的卫星数据中捕捉复杂的时空动态，并为蓝藻水华管理提供可靠的早期预警。该方法展示了遥感数据在蓝藻水华预测中的有效性，特别是在数据稀疏的情况下仍能保持较好的预测性能。"}}
{"id": "2512.06595", "pdf": "https://arxiv.org/pdf/2512.06595", "abs": "https://arxiv.org/abs/2512.06595", "authors": ["Joe Shymanski"], "title": "ChargingBoul: A Competitive Negotiating Agent with Novel Opponent Modeling", "categories": ["cs.MA", "cs.AI"], "comment": "10 pages, 3 figures. Describes the ChargingBoul negotiating agent submitted to ANAC 2022. Preprint", "summary": "Automated negotiation has emerged as a critical area of research in multiagent systems, with applications spanning e-commerce, resource allocation, and autonomous decision-making. This paper presents ChargingBoul, a negotiating agent that competed in the 2022 Automated Negotiating Agents Competition (ANAC) and placed second in individual utility by an exceptionally narrow margin. ChargingBoul employs a lightweight yet effective strategy that balances concession and opponent modeling to achieve high negotiation outcomes. The agent classifies opponents based on bid patterns, dynamically adjusts its bidding strategy, and applies a concession policy in later negotiation stages to maximize utility while fostering agreements. We evaluate ChargingBoul's performance using competition results and subsequent studies that have utilized the agent in negotiation research. Our analysis highlights ChargingBoul's effectiveness across diverse opponent strategies and its contributions to advancing automated negotiation techniques. We also discuss potential enhancements, including more sophisticated opponent modeling and adaptive bidding heuristics, to improve its performance further.", "AI": {"tldr": "ChargingBoul是一个在2022年ANAC竞赛中获得第二名的自动谈判代理，采用轻量级但有效的策略，通过对手建模和动态调整投标策略来最大化效用。", "motivation": "自动谈判在多智能体系统中具有重要应用，如电子商务、资源分配和自主决策。现有谈判代理需要在复杂环境中实现高效谈判，需要平衡让步与对手建模以获得最佳结果。", "method": "1. 基于投标模式对对手进行分类；2. 动态调整投标策略；3. 在谈判后期应用让步策略；4. 平衡让步与对手建模以实现高谈判结果。", "result": "在2022年ANAC竞赛中获得第二名（个体效用），与第一名差距极小。后续研究表明该代理在不同对手策略下表现有效，为自动谈判技术发展做出贡献。", "conclusion": "ChargingBoul是一个有效的谈判代理，展示了轻量级策略在复杂谈判环境中的可行性。未来可通过更复杂的对手建模和自适应投标启发式方法进一步提升性能。"}}
{"id": "2512.06591", "pdf": "https://arxiv.org/pdf/2512.06591", "abs": "https://arxiv.org/abs/2512.06591", "authors": ["Joe Shymanski", "Jacob Brue", "Sandip Sen"], "title": "Beyond Satisfaction: From Placebic to Actionable Explanations For Enhanced Understandability", "categories": ["cs.HC", "cs.AI"], "comment": "21 pages, 7 figures, 6 tables. EXTRAAMAS 2025 submission. Preprint version", "summary": "Explainable AI (XAI) presents useful tools to facilitate transparency and trustworthiness in machine learning systems. However, current evaluations of system explainability often rely heavily on subjective user surveys, which may not adequately capture the effectiveness of explanations. This paper critiques the overreliance on user satisfaction metrics and explores whether these can differentiate between meaningful (actionable) and vacuous (placebic) explanations. In experiments involving optimal Social Security filing age selection tasks, participants used one of three protocols: no explanations, placebic explanations, and actionable explanations. Participants who received actionable explanations significantly outperformed the other groups in objective measures of their mental model, but users rated placebic and actionable explanations as equally satisfying. This suggests that subjective surveys alone fail to capture whether explanations truly support users in building useful domain understanding. We propose that future evaluations of agent explanation capabilities should integrate objective task performance metrics alongside subjective assessments to more accurately measure explanation quality. The code for this study can be found at https://github.com/Shymkis/social-security-explainer.", "AI": {"tldr": "该论文批判了当前可解释AI评估过度依赖用户满意度指标的问题，通过实验证明主观满意度无法区分有效解释和安慰剂解释，提出了结合客观任务性能指标的评估方法。", "motivation": "当前可解释AI系统评估过度依赖主观用户调查，这些调查可能无法准确捕捉解释的实际有效性。用户满意度指标可能无法区分有意义的（可操作的）解释和空洞的（安慰剂的）解释。", "method": "在最优社会保障申请年龄选择任务中进行实验，参与者被分为三组：无解释组、安慰剂解释组和可操作解释组。通过比较不同组在客观心理模型测量和主观满意度评分上的表现来评估解释效果。", "result": "接收可操作解释的参与者在客观心理模型测量上显著优于其他组，但用户在主观满意度评分上对安慰剂解释和可操作解释给出了同等的高分。这表明主观调查无法捕捉解释是否真正帮助用户建立有用的领域理解。", "conclusion": "未来评估智能体解释能力时应整合客观任务性能指标和主观评估，以更准确地衡量解释质量。仅依赖用户满意度调查可能导致对解释有效性的错误判断。"}}
{"id": "2512.06590", "pdf": "https://arxiv.org/pdf/2512.06590", "abs": "https://arxiv.org/abs/2512.06590", "authors": ["Tendai Mukande", "Esraa Ali", "Annalina Caputo", "Ruihai Dong", "Noel OConnor"], "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.MA"], "comment": "8 Pages", "summary": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.", "AI": {"tldr": "提出HGLMRec模型，这是一个基于多LLM代理的推荐系统，结合超图编码器来捕捉用户和物品之间复杂的多行为关系，旨在解决生成式推荐系统中的幻觉问题和高计算成本。", "motivation": "尽管大语言模型开启了生成式检索和推荐的新范式，但现有方法面临幻觉问题（降低推荐性能）和实际场景中的高计算成本问题，需要更高效且准确的推荐系统解决方案。", "method": "提出HGLMRec模型，结合多LLM代理和超图编码器，超图编码器捕捉用户和物品之间的复杂多行为关系，在推理时仅检索相关token以减少计算开销，同时丰富检索上下文。", "result": "实验结果显示HGLMRec在较低计算成本下相比最先进的基线模型取得了性能提升，表明该方法在效率和准确性方面都有优势。", "conclusion": "HGLMRec通过结合多LLM代理和超图编码器，有效解决了生成式推荐系统中的幻觉问题和高计算成本问题，为高效超图和多LLM代理推荐系统提供了可行方案。"}}
{"id": "2512.06589", "pdf": "https://arxiv.org/pdf/2512.06589", "abs": "https://arxiv.org/abs/2512.06589", "authors": ["Xiaojun Jia", "Jie Liao", "Qi Guo", "Teng Ma", "Simeng Qin", "Ranjie Duan", "Tianlin Li", "Yihao Huang", "Zhitao Zeng", "Dongxian Wu", "Yiming Li", "Wenqi Ren", "Xiaochun Cao", "Yang Liu"], "title": "OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM.", "AI": {"tldr": "提出了OmniSafeBench-MM，这是一个用于多模态越狱攻击防御评估的统一基准和工具箱，旨在解决现有基准在攻击场景、防御评估标准化和可复现性方面的不足。", "motivation": "多模态大语言模型（MLLMs）虽然具备统一的感知推理能力，但容易受到越狱攻击，绕过安全对齐并诱导有害行为。现有基准如JailBreakV-28K、MM-SafetyBench和HADES在多模态漏洞研究方面提供了有价值见解，但通常关注有限的攻击场景，缺乏标准化的防御评估，且没有统一、可复现的工具箱。", "method": "OmniSafeBench-MM整合了13种代表性攻击方法、15种防御策略，以及涵盖9个主要风险领域和50个细粒度类别的多样化数据集。数据集按咨询性、命令性和陈述性查询类型结构化，反映真实用户意图。建立了三维评估协议：1）危害性（从低影响个体危害到灾难性社会威胁的细粒度多级量表），2）响应与查询的意图对齐，3）响应详细程度。", "result": "在10个开源和8个闭源MLLMs上进行了广泛实验，揭示了它们对多模态越狱攻击的脆弱性。通过将数据、方法和评估统一到一个开源、可复现的平台中，为未来研究提供了标准化基础。", "conclusion": "OmniSafeBench-MM通过统一的数据、方法和评估框架，为多模态越狱攻击防御研究提供了一个全面的标准化基准和工具箱，填补了现有研究在攻击场景覆盖、防御评估标准化和工具可复现性方面的空白。"}}
{"id": "2512.06582", "pdf": "https://arxiv.org/pdf/2512.06582", "abs": "https://arxiv.org/abs/2512.06582", "authors": ["Isaac Kofi Nti"], "title": "QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.", "AI": {"tldr": "提出QL-LSTM模型，通过参数共享统一门控机制和分层门控循环结构，解决传统LSTM参数冗余和长序列信息保留能力不足的问题", "motivation": "传统LSTM和GRU等循环神经网络存在两个核心限制：1) 门控机制中的参数冗余问题；2) 在长序列建模中信息保留能力下降", "method": "提出QL-LSTM模型，包含两个独立组件：1) 参数共享统一门控机制(PSUG)，用单一共享权重矩阵替代所有门特定变换，减少约48%参数；2) 分层门控循环与加法跳跃连接(HGR-ASC)，添加无乘法路径改善长距离信息流", "result": "在IMDB数据集上进行情感分类实验，与LSTM、GRU和BiLSTM对比。QL-LSTM在显著减少参数的情况下达到竞争性准确率，但受限于循环模型的顺序特性，当前原型在计算时间上尚未获得加速", "conclusion": "QL-LSTM通过参数共享和分层循环结构有效解决了传统LSTM的参数冗余和长序列信息保留问题，在保持性能的同时显著减少参数，但需要进一步的内核级优化来实现计算速度提升"}}
{"id": "2512.06581", "pdf": "https://arxiv.org/pdf/2512.06581", "abs": "https://arxiv.org/abs/2512.06581", "authors": ["Yuhao Su", "Anwesa Choudhuri", "Zhongpai Gao", "Benjamin Planche", "Van Nguyen Nguyen", "Meng Zheng", "Yuhan Shen", "Arun Innanje", "Terrence Chen", "Ehsan Elhamifar", "Ziyan Wu"], "title": "MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \\textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \\textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \\emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \\emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.", "AI": {"tldr": "提出MedGRPO框架，通过多任务强化学习解决医学视频理解中的空间精度、时序推理和临床语义问题，包含MedVidBench基准数据集和跨数据集奖励归一化方法。", "motivation": "大型视觉语言模型在医学视频理解方面存在困难，特别是在空间精度、时序推理和临床语义方面表现不佳。现有方法在处理多数据集训练时存在奖励尺度不平衡问题，导致优化不稳定和训练崩溃。", "method": "1) 创建MedVidBench基准数据集，包含531,850个视频-指令对，涵盖8个医学来源和视频、片段、帧级别任务；2) 提出MedGRPO强化学习框架，包含跨数据集奖励归一化（将各数据集中位数性能映射到共同奖励值）和医学LLM评判器（通过比较相似度评分评估五个临床维度的字幕质量）。", "result": "在MedVidBench上监督微调Qwen2.5-VL-7B显著优于GPT-4.1和Gemini-2.5-Flash；MedGRPO框架在定位和字幕任务上进一步超越了SFT基线，实现了平衡的多数据集训练。", "conclusion": "该研究为医学领域的视觉语言模型建立了基础基准和鲁棒训练方法，通过MedVidBench基准和MedGRPO框架有效解决了医学视频理解中的多任务强化学习挑战。"}}
{"id": "2512.06578", "pdf": "https://arxiv.org/pdf/2512.06578", "abs": "https://arxiv.org/abs/2512.06578", "authors": ["Waleed Razzaq"], "title": "Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control", "categories": ["cs.RO"], "comment": "Under review at SoftComputing", "summary": "Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\\(τ\\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \\(e_t\\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \\(e_t\\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \\( ρ_t \\) to shape the output trajectory and a \\textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.", "AI": {"tldr": "提出EC-PIDUNN架构，将未经训练的神经网络与改进的PID控制器结合，用于非线性机器人控制，无需系统动态知识即可处理非线性挑战。", "motivation": "传统PID控制器在处理非线性动态和复杂互联变量时面临挑战，导致不稳定、超调或长时间稳定。现有PIDNN模型需要大量精细训练数据和计算成本，不适合实际应用。", "method": "提出EC-PIDUNN架构，集成未经训练的神经网络和改进的PID控制器，包含稳定因子τ。使用稳态误差e_t作为输入，无需系统动态知识。通过神经网络形成输入向量增加维度，引入参数向量ρ_t调整输出轨迹，使用动态计算函数调整预定义的PID系数。", "result": "在非线性机器人应用中验证有效性：1) 代表阿克曼转向机制和运动学控制的非线性无人地面车辆系统；2) 云台运动系统。在两个测试中都优于传统PID的收敛性和稳定性，实现接近临界阻尼响应。", "conclusion": "EC-PIDUNN架构成功解决了传统PID在处理非线性系统时的局限性，无需大量训练数据或系统动态知识，在机器人控制应用中表现出优越性能。"}}
{"id": "2512.06577", "pdf": "https://arxiv.org/pdf/2512.06577", "abs": "https://arxiv.org/abs/2512.06577", "authors": ["Muhammad Junayed Hasan Zahed", "Hossein Rastgoftar"], "title": "Deep Neural Network-Based Aerial Transport in the Presence of Cooperative and Uncooperative UAS", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We present a resilient deep neural network (DNN) framework for decentralized transport and coverage using uncrewed aerial systems (UAS) operating in $\\mathbb{R}^n$. The proposed DNN-based mass-transport architecture constructs a layered inter-UAS communication graph from an initial formation, assigns time-varying communication weights through a forward scheduling mechanism that guides the team from the initial to the final configuration, and ensures stability and convergence of the resulting multi-agent transport dynamics. The framework is explicitly designed to remain robust in the presence of uncooperative agents that deviate from or refuse to follow the prescribed protocol. Our method preserves a fixed feed-forward topology but dynamically prunes edges to uncooperative agents, maintains convex, feedforward mentoring among cooperative agents, and computes global desired set points through a sparse linear relation consistent with leader references. The target set is abstracted by $N$ points that become final desired positions, enabling coverage-optimal transport while keeping computation low and guarantees intact. Extensive simulations demonstrate that, under full cooperation, all agents converge rapidly to the target zone with a 10\\% boundary margin and under partial cooperation with uncooperative agents, the system maintains high convergence among cooperative agents with performance degradation localized near the disruptions, evidencing graceful resilience and scalability. These results confirm that forward-weight scheduling, hierarchical mentor--mentee coordination, and on-the-fly DNN restructuring yield robust, provably stable UAS transport in realistic fault scenarios.", "AI": {"tldr": "提出一个基于深度神经网络的弹性框架，用于在存在合作与非合作无人机情况下的去中心化空中运输和覆盖", "motivation": "解决多无人机系统在现实故障场景下的鲁棒运输问题，特别是当系统中存在不遵循协议的无人机时，需要保持系统的稳定性和收敛性", "method": "构建分层通信图，通过前向调度机制分配时变通信权重，动态修剪与非合作节点的连接，保持合作节点间的凸前向指导，通过稀疏线性关系计算全局期望点", "result": "在完全合作情况下，所有无人机快速收敛到目标区域（10%边界裕度）；在部分合作情况下，合作无人机保持高收敛性，性能下降仅限于干扰附近，表现出优雅的弹性和可扩展性", "conclusion": "前向权重调度、分层指导协调和动态DNN重构能够在现实故障场景下实现鲁棒且可证明稳定的无人机运输"}}
{"id": "2512.06575", "pdf": "https://arxiv.org/pdf/2512.06575", "abs": "https://arxiv.org/abs/2512.06575", "authors": ["Fariza Dahes"], "title": "Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "26 pages, 16 figures, 2 tables; proof of concept on mammography classification with compactness/separability modules and interactive dashboard; preprint submitted to arXiv cs.LG", "summary": "This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.", "AI": {"tldr": "验证并扩展一个医学图像分类方法框架在乳腺X光分类任务上的适用性，研究GAGM和SEVector模块在提高特征可区分性和减少假阴性方面的有效性。", "motivation": "验证先前在阿尔茨海默症MRI分类中表现良好的方法框架（包含GAGM、SEVector和FSL模块）是否能够成功迁移到乳腺X光分类任务中，并探索如何进一步提高恶性与良性病例的区分能力。", "method": "使用Kaggle整合的INbreast、MIAS和DDSM乳腺X光数据集，比较基线CNN、ConvNeXt Tiny和InceptionV3骨干网络，并分别集成GAGM和SEVector模块。采用多指标评估（宏观F1、每类召回方差、ROC/AUC）、特征可解释性分析（Grad CAM）和开发交互式临床探索仪表板。", "result": "GAGM和SEVector模块在乳腺X光分类中有效增强了特征可区分性并减少了假阴性（特别是恶性病例），但特征平滑损失（FSL）在乳腺X光分类条件下未显示可测量的改进。扩展了原始框架的多指标评估和可解释性分析能力。", "conclusion": "验证了GAGM和SEVector模块在乳腺X光分类中的有效性，但FSL的效果可能依赖于特定的架构和计算假设。未来需要探索替代方法来改进类内紧凑性和类间可分离性，以增强恶性与良性病例的区分能力。"}}
{"id": "2512.06573", "pdf": "https://arxiv.org/pdf/2512.06573", "abs": "https://arxiv.org/abs/2512.06573", "authors": ["Onur Bilgin", "Abdullah As Sami", "Sriram Sai Vujjini", "John Licato"], "title": "The Effect of Belief Boxes and Open-mindedness on Persuasion", "categories": ["cs.AI", "cs.MA"], "comment": "Accepted at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "As multi-agent systems are increasingly utilized for reasoning and decision-making applications, there is a greater need for LLM-based agents to have something resembling propositional beliefs. One simple method for doing so is to include statements describing beliefs maintained in the prompt space (in what we'll call their belief boxes). But when agents have such statements in belief boxes, how does it actually affect their behaviors and dispositions towards those beliefs? And does it significantly affect agents' ability to be persuasive in multi-agent scenarios? Likewise, if the agents are given instructions to be open-minded, how does that affect their behaviors? We explore these and related questions in a series of experiments. Our findings confirm that instructing agents to be open-minded affects how amenable they are to belief change. We show that incorporating belief statements and their strengths influences an agent's resistance to (and persuasiveness against) opposing viewpoints. Furthermore, it affects the likelihood of belief change, particularly when the agent is outnumbered in a debate by opposing viewpoints, i.e., peer pressure scenarios. The results demonstrate the feasibility and validity of the belief box technique in reasoning and decision-making tasks.", "AI": {"tldr": "研究信念箱和开放心态对LLM智能体说服力的影响", "motivation": "随着多智能体系统在推理和决策应用中日益普及，需要让LLM智能体具备类似命题信念的能力。信念箱是一种简单方法，但需要了解它如何影响智能体行为和说服能力。", "method": "通过一系列实验探索信念箱和开放心态指令对智能体行为的影响，包括信念强度、对立观点抵抗、说服力测试，特别是在少数对多数的辩论场景中。", "result": "开放心态指令确实影响智能体改变信念的意愿；信念陈述及其强度影响智能体对对立观点的抵抗力和说服力；在少数对多数的辩论场景中，信念改变的可能性显著增加。", "conclusion": "信念箱技术在推理和决策任务中具有可行性和有效性，能够显著影响智能体的信念维护、说服能力和对对立观点的反应。"}}
{"id": "2512.06571", "pdf": "https://arxiv.org/pdf/2512.06571", "abs": "https://arxiv.org/abs/2512.06571", "authors": ["Zifan Xu", "Myoungkyu Seo", "Dongmyeong Lee", "Hao Fu", "Jiaheng Hu", "Jiaxun Cui", "Yuqian Jiang", "Zhihan Wang", "Anastasiia Brund", "Joydeep Biswas", "Peter Stone"], "title": "Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input", "categories": ["cs.RO"], "comment": null, "summary": "Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a \"teacher\" policy is trained with ground truth state information and the \"student\" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.", "AI": {"tldr": "本文提出了一种基于强化学习的系统，使仿人机器人能够在嘈杂感知输入下执行鲁棒的连续踢球动作，并适应不同的球门配置。", "motivation": "学习快速且鲁棒的踢球技能对仿人足球机器人至关重要，但由于需要快速摆腿、单脚支撑时的姿态稳定性，以及在嘈杂感知输入和外部扰动下的鲁棒性，这仍然是一个具有挑战性的问题。", "method": "采用四阶段师生训练框架：1) 长距离追球（教师）；2) 定向踢球（教师）；3) 教师策略蒸馏（学生）；4) 学生适应与精炼（学生）。关键设计包括定制奖励函数、真实噪声建模和在线约束强化学习。", "result": "在仿真和真实机器人上的广泛评估展示了在各种球门配置下的强踢球准确性和进球成功率。消融研究进一步强调了约束强化学习、噪声建模和适应阶段的必要性。", "conclusion": "这项工作提出了一个在非完美感知下学习鲁棒连续仿人踢球的系统，为仿人全身控制的视觉运动技能学习建立了基准任务。"}}
{"id": "2512.06565", "pdf": "https://arxiv.org/pdf/2512.06565", "abs": "https://arxiv.org/abs/2512.06565", "authors": ["Xiujin Liu"], "title": "GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation", "categories": ["cs.CV"], "comment": "1 figures, 2 tables, 14pages", "summary": "We present GNC-Pose, a fully learning-free monocular 6D object pose estimation pipeline for textured objects that combines rendering-based initialization, geometry-aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D-3D correspondences obtained through feature matching and rendering-based alignment, our method builds upon the Graduated Non-Convexity (GNC) principle and introduces a geometry-aware, cluster-based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC-Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC-Pose achieves competitive accuracy compared with both learning-based and learning-free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.", "AI": {"tldr": "提出GNC-Pose，一种完全无需学习的单目6D物体姿态估计方法，结合渲染初始化、几何感知对应点加权和鲁棒的GNC优化", "motivation": "现有方法要么依赖学习特征和大量训练数据，要么在严重离群点污染下优化不稳定，需要一种简单、鲁棒且无需学习的6D姿态估计解决方案", "method": "1) 通过特征匹配和渲染对齐获取粗2D-3D对应点；2) 基于GNC原理引入几何感知的聚类加权机制，根据3D结构一致性分配点置信度；3) 最终LM细化进一步提高精度", "result": "在YCB数据集上测试，尽管无需学习特征、训练数据或类别特定先验，GNC-Pose在精度上与基于学习的方法和无需学习的方法相比具有竞争力", "conclusion": "GNC-Pose为无需学习的6D姿态估计提供了一个简单、鲁棒且实用的解决方案，通过几何感知加权显著提高了在严重离群点污染下的优化稳定性"}}
{"id": "2512.06563", "pdf": "https://arxiv.org/pdf/2512.06563", "abs": "https://arxiv.org/abs/2512.06563", "authors": ["Max Y. Ma", "Gen-Hua Shi"], "title": "Deep Manifold Part 2: Neural Network Mathematics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.", "AI": {"tldr": "该论文通过流形几何、不动点理论和边界条件迭代，建立了神经网络的全局方程，揭示了神经网络作为可学习数值计算的本质，并解释了学习能力如何通过不动点区域的稳定而涌现。", "motivation": "传统神经网络分析通常基于固定坐标和算子，缺乏对神经网络作为可学习数值计算的全局几何理解。论文旨在通过流形复杂性、高阶非线性和边界条件等几何代数框架，揭示神经网络学习的本质机制和限制。", "method": "采用堆叠分段流形、不动点理论和边界条件迭代的方法，构建神经网络的全局方程。通过移除固定坐标和算子，将神经网络视为由流形复杂性、高阶非线性和边界条件塑造的可学习数值计算。", "result": "揭示了神经网络学习能力的涌现机制：神经网络并非始于不动点，而是通过残差驱动迭代构建不动点区域。只有当这些区域稳定时，学习能力才会涌现。同时识别了现实数据复杂性、训练动态等对可学习性的约束。", "conclusion": "该几何视角阐明了单体模型在几何和数据诱导可塑性下的局限性，并启发了将流形复杂性分布在多个弹性模型中的架构和联邦系统，形成了基于几何、代数、不动点和真实数据复杂性的连贯世界建模框架。"}}
{"id": "2512.06562", "pdf": "https://arxiv.org/pdf/2512.06562", "abs": "https://arxiv.org/abs/2512.06562", "authors": ["Dung Thuy Nguyen", "Quang Nguyen", "Preston K. Robinette", "Eli Jiang", "Taylor T. Johnson", "Kevin Leach"], "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.", "AI": {"tldr": "SUGAR是一个用于生成式遗忘的框架，能够同时或顺序地从3D感知生成模型中移除多个身份，而无需重新训练整个模型。", "motivation": "随着3D感知生成模型在合成人类身份图像方面取得进展，引发了关于用户同意和从模型输出空间中移除特定个体能力的紧迫问题。需要一种能够有效移除多个身份而不损害模型质量的解决方案。", "method": "SUGAR为每个身份学习个性化的替代潜在表示，将重建结果转移到视觉连贯的替代方案，同时引入持续效用保持目标以防止随着更多身份被遗忘而导致的性能退化。", "result": "SUGAR在移除多达200个身份方面实现了最先进的性能，与现有基线相比，保持效用提高了700%。", "conclusion": "SUGAR提供了一个可扩展的生成式遗忘框架，能够有效移除多个身份，同时保持模型的生成质量和多样性，解决了3D生成模型中用户同意和隐私保护的重要问题。"}}
{"id": "2512.06560", "pdf": "https://arxiv.org/pdf/2512.06560", "abs": "https://arxiv.org/abs/2512.06560", "authors": ["Dalia Alzu'bi", "A. Ben Hamza"], "title": "Bridging spatial awareness and global context in medical image segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.", "AI": {"tldr": "提出U-CycleMLP，一种新型U形编码器-解码器网络，用于医学图像分割，旨在平衡分割精度和计算效率，同时有效捕获局部和全局上下文信息。", "motivation": "现有分割模型难以有效捕获局部和全局上下文信息，导致边界像素丢失和分割错误。医学图像分割需要平衡分割精度和计算效率的模型。", "method": "采用U形编码器-解码器架构。编码器使用位置注意力权重激励块、密集空洞块和下采样操作学习多尺度上下文特征。解码器通过上采样操作、密集空洞块和特征融合机制重建高分辨率分割掩码。在解码器的跳跃连接中引入通道CycleMLP块以增强特征集成，同时保持相对于输入大小的线性计算复杂度。", "result": "在三个基准数据集上的实验结果表明，U-CycleMLP相比最先进方法具有竞争力，在所有数据集上实现更好的分割精度，能够捕获细粒度解剖结构，并在不同医学成像模态中表现出鲁棒性。消融研究进一步证明了模型核心架构组件对提升分割精度的重要性。", "conclusion": "U-CycleMLP通过有效结合空间感知和全局上下文，在保持轻量级架构的同时提升了医学图像分割性能，为解决现有模型在局部和全局信息捕获方面的不足提供了有效方案。"}}
{"id": "2512.06559", "pdf": "https://arxiv.org/pdf/2512.06559", "abs": "https://arxiv.org/abs/2512.06559", "authors": ["Ivor van der Hoog", "Eva Rotenberg", "Daniel Rutschmann"], "title": "Tight Universal Bounds for Partially Presorted Pareto Front and Convex Hull", "categories": ["cs.CG", "cs.DS"], "comment": null, "summary": "TimSort is a well-established sorting algorithm whose running time depends on how sorted the input already is. Recently, Eppstein, Goodrich, Illickan, and To designed algorithms inspired by TimSort for Pareto front, planar convex hull, and two other problems. For each of these problems, they define a Range Partition Entropy; a function $H$ mapping lists $I$ that store $n$ points to a number between $0$ and $\\log n$. Their algorithms have, for each list of points $I$, a running time of $O(n(1 + H(I)))$. In this paper, we provide matching lower bounds for the Pareto front and convex hull algorithms by Eppstein, Goodrich, Illickan, and To. In particular, we show that their algorithm does not correspond to TimSort (or related stack-based MergeSort variants) but rather to a variant of QuickSort. From this, we derive an intuitive notion of universal optimality. We show comparison-based lower bounds that prove that the algorithms by Eppstein, Goodrich, Illickan and To are universally optimal under this notion of universal optimality.", "AI": {"tldr": "该论文为Eppstein等人提出的Pareto前沿和凸包算法的运行时间提供了匹配的下界，证明了这些算法在特定意义上的通用最优性。", "motivation": "Eppstein等人设计了受TimSort启发的算法，用于解决Pareto前沿和平面凸包等问题，其运行时间与输入数据的\"范围划分熵\"相关。然而，这些算法是否真正对应TimSort的变体，以及它们是否在某种意义下是最优的，需要进一步研究。", "method": "通过分析Eppstein等人的算法结构，发现它们实际上对应于QuickSort的变体而非TimSort。基于这一发现，建立了直观的通用最优性概念，并给出了比较排序的下界证明。", "result": "证明了Eppstein等人的Pareto前沿和凸包算法具有紧的下界，即Ω(n(1+H(I)))，从而表明这些算法在提出的通用最优性意义下是最优的。", "conclusion": "该工作澄清了Eppstein等人算法的本质结构（QuickSort变体而非TimSort），建立了通用最优性的概念，并证明了这些算法在该意义下的最优性，为部分预排序问题的算法设计提供了理论基础。"}}
{"id": "2512.06558", "pdf": "https://arxiv.org/pdf/2512.06558", "abs": "https://arxiv.org/abs/2512.06558", "authors": ["Md Mofijul Islam", "Alexi Gladstone", "Sujan Sarker", "Ganesh Nanduru", "Md Fahim", "Keyan Du", "Aman Chadha", "Tariq Iqbal"], "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction", "categories": ["cs.RO"], "comment": "14 pages, 7 figures, accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.", "AI": {"tldr": "该论文提出了Refer360数据集和MuRes模块，用于解决人机交互中具身指代表达理解的问题，旨在提高机器人在人类环境中理解具身指令的能力。", "motivation": "随着机器人进入人类工作空间，需要它们理解具身的人类指令以实现直观流畅的人机交互。然而，由于缺乏大规模捕捉自然具身交互的数据集，现有数据集存在视角偏差、单视图收集、非语言手势覆盖不足以及主要关注室内环境等问题。", "method": "提出了Refer360数据集（包含室内外多视角的具身语言和非语言交互数据）和MuRes模块（多模态引导残差模块），该模块作为信息瓶颈提取显著模态特定信号，并将其强化到预训练表示中，形成下游任务的互补特征。", "result": "在包括Refer360在内的四个人机交互数据集上进行了广泛实验，发现当前多模态模型无法全面捕捉具身交互，但通过MuRes增强后性能得到一致提升。", "conclusion": "Refer360成为有价值的基准数据集，引导残差学习展示了在人类环境中操作的机器人具身指代表达理解方面的潜力。"}}
{"id": "2512.06556", "pdf": "https://arxiv.org/pdf/2512.06556", "abs": "https://arxiv.org/abs/2512.06556", "authors": ["Saeid Jamshidi", "Kawser Wazed Nafi", "Arghavan Moradi Dakhel", "Negar Shahabi", "Foutse Khomh", "Naser Ezzati-Jivan"], "title": "Securing the Model Context Protocol: Defending LLMs Against Tool Poisoning and Adversarial Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The Model Context Protocol (MCP) enables Large Language Models to integrate external tools through structured descriptors, increasing autonomy in decision-making, task execution, and multi-agent workflows. However, this autonomy creates a largely overlooked security gap. Existing defenses focus on prompt-injection attacks and fail to address threats embedded in tool metadata, leaving MCP-based systems exposed to semantic manipulation. This work analyzes three classes of semantic attacks on MCP-integrated systems: (1) Tool Poisoning, where adversarial instructions are hidden in tool descriptors; (2) Shadowing, where trusted tools are indirectly compromised through contaminated shared context; and (3) Rug Pulls, where descriptors are altered after approval to subvert behavior. To counter these threats, we introduce a layered security framework with three components: RSA-based manifest signing to enforce descriptor integrity, LLM-on-LLM semantic vetting to detect suspicious tool definitions, and lightweight heuristic guardrails that block anomalous tool behavior at runtime. Through evaluation of GPT-4, DeepSeek, and Llama-3.5 across eight prompting strategies, we find that security performance varies widely by model architecture and reasoning method. GPT-4 blocks about 71 percent of unsafe tool calls, balancing latency and safety. DeepSeek shows the highest resilience to Shadowing attacks but with greater latency, while Llama-3.5 is fastest but least robust. Our results show that the proposed framework reduces unsafe tool invocation rates without model fine-tuning or internal modification.", "AI": {"tldr": "该论文提出了一个针对模型上下文协议（MCP）的安全框架，以防御LLMs在集成外部工具时面临的工具中毒和对抗性攻击。", "motivation": "MCP使大型语言模型能够通过结构化描述符集成外部工具，增强了决策、任务执行和多智能体工作流的自主性。然而，这种自主性带来了被忽视的安全漏洞。现有防御主要关注提示注入攻击，未能解决嵌入在工具元数据中的威胁，使MCP系统暴露于语义操纵攻击之下。", "method": "提出了一个分层安全框架，包含三个组件：1）基于RSA的清单签名以强制描述符完整性；2）LLM-on-LLM语义审查以检测可疑工具定义；3）轻量级启发式防护栏，在运行时阻止异常工具行为。", "result": "通过评估GPT-4、DeepSeek和Llama-3.5在八种提示策略下的表现，发现安全性能因模型架构和推理方法而异。GPT-4阻止了约71%的不安全工具调用，在延迟和安全性之间取得平衡。DeepSeek对Shadowing攻击表现出最高韧性但延迟较大，Llama-3.5最快但最不鲁棒。该框架无需模型微调或内部修改即可降低不安全工具调用率。", "conclusion": "该研究揭示了MCP系统中的安全漏洞，并提出了一个有效的分层防御框架，能够在不修改模型内部结构的情况下显著提高系统安全性，为LLM工具集成提供了实用的安全保障方案。"}}
{"id": "2512.06555", "pdf": "https://arxiv.org/pdf/2512.06555", "abs": "https://arxiv.org/abs/2512.06555", "authors": ["Arush Sachdeva", "Rajendraprasad Saravanan", "Gargi Sarkar", "Kavita Vemuri", "Sandeep Kumar Shukla"], "title": "BEACON: A Unified Behavioral-Tactical Framework for Explainable Cybercrime Analysis with Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": null, "summary": "Cybercrime increasingly exploits human cognitive biases in addition to technical vulnerabilities, yet most existing analytical frameworks focus primarily on operational aspects and overlook psychological manipulation. This paper proposes BEACON, a unified dual-dimension framework that integrates behavioral psychology with the tactical lifecycle of cybercrime to enable structured, interpretable, and scalable analysis of cybercrime. We formalize six psychologically grounded manipulation categories derived from Prospect Theory and Cialdini's principles of persuasion, alongside a fourteen-stage cybercrime tactical lifecycle spanning reconnaissance to final impact. A single large language model is fine-tuned using parameter-efficient learning to perform joint multi-label classification across both psychological and tactical dimensions while simultaneously generating human-interpretable explanations. Experiments conducted on a curated dataset of real-world and synthetically augmented cybercrime narratives demonstrate a 20 percent improvement in overall classification accuracy over the base model, along with substantial gains in reasoning quality measured using ROUGE and BERTScore. The proposed system enables automated decomposition of unstructured victim narratives into structured behavioral and operational intelligence, supporting improved cybercrime investigation, case linkage, and proactive scam detection.", "AI": {"tldr": "提出BEACON框架，将行为心理学与网络犯罪战术生命周期相结合，使用大语言模型进行可解释的网络犯罪分析", "motivation": "现有网络犯罪分析框架主要关注技术层面，忽视了心理操纵维度，而网络犯罪越来越多地利用人类认知偏见", "method": "基于前景理论和Cialdini说服原则建立6个心理操纵类别，结合14阶段网络犯罪战术生命周期，使用参数高效学习方法微调大语言模型进行多标签分类和解释生成", "result": "在真实世界和合成增强的网络犯罪叙事数据集上，相比基础模型整体分类准确率提升20%，推理质量在ROUGE和BERTScore指标上有显著提升", "conclusion": "BEACON框架能够将非结构化受害者叙述自动分解为结构化的行为和操作情报，支持改进网络犯罪调查、案件关联和主动诈骗检测"}}
{"id": "2512.06547", "pdf": "https://arxiv.org/pdf/2512.06547", "abs": "https://arxiv.org/abs/2512.06547", "authors": ["Xiaocan Li", "Shiliang Wu", "Zheng Shen"], "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md", "AI": {"tldr": "提出A-3PO方法，通过近似计算来加速异步大语言模型训练，减少计算开销", "motivation": "在异步强化学习设置中，传统解耦损失方法需要额外的前向传播计算近端策略，这在大语言模型训练中成为计算瓶颈", "method": "通过简单的插值近似近端策略，而不是显式计算，从而消除额外的前向传播开销", "result": "A-3PO减少了18%的训练时间，同时保持了可比的性能表现", "conclusion": "提出的近似方法有效解决了异步大语言模型训练中的计算瓶颈问题，实现了显著的训练加速"}}
{"id": "2512.06534", "pdf": "https://arxiv.org/pdf/2512.06534", "abs": "https://arxiv.org/abs/2512.06534", "authors": ["Harold Triedman", "Jayshree Sarathy", "Priyanka Nanayakkara", "Rachel Cummings", "Gabriel Kaptchuk", "Sean Kross", "Elissa M. Redmiles"], "title": "\"Having Confidence in My Confidence Intervals\": How Data Users Engage with Privacy-Protected Wikipedia Data", "categories": ["cs.HC"], "comment": null, "summary": "In response to calls for open data and growing privacy threats, organizations are increasingly adopting privacy-preserving techniques such as differential privacy (DP) that inject statistical noise when generating published datasets. These techniques are designed to protect privacy of data subjects while enabling useful analyses, but their reception by data users is under-explored. We developed documentation that presents the noise characteristics of two Wikipedia pageview datasets: one using rounding (heuristic privacy) and another using DP (formal privacy). After incorporating expert feedback (n=5), we used these documents to conduct a task-based contextual inquiry (n=15) exploring how data users--largely unfamiliar with these methods--perceive, interact with, and interpret privacy-preserving noise during data analysis. Participants readily used simple uncertainty metrics from the documentation, but struggled when asked to compute confidence intervals across multiple noisy estimates. They were better able to devise simulation-based approaches for computing uncertainty with DP data compared to rounded data. Surprisingly, several participants incorrectly believed DP's stronger utility implied weaker privacy protections. Based on our findings, we offer design recommendations for documentation and tools to better support data users working with privacy-noised data.", "AI": {"tldr": "研究数据用户如何与受隐私保护的维基百科页面浏览量数据进行交互，特别是比较四舍五入（启发式隐私）和差分隐私（正式隐私）两种方法下的用户体验。", "motivation": "随着开放数据需求的增长和隐私威胁的增加，组织越来越多地采用隐私保护技术（如差分隐私），但这些技术的数据用户接受度尚未得到充分研究。需要了解数据用户如何感知、交互和解释隐私保护噪声。", "method": "开发了展示两种维基百科页面浏览量数据集噪声特性的文档：一种使用四舍五入（启发式隐私），另一种使用差分隐私（正式隐私）。在纳入专家反馈（n=5）后，使用这些文档进行基于任务的上下文调查（n=15），探索数据用户如何与隐私保护噪声交互。", "result": "参与者能够轻松使用文档中的简单不确定性度量，但在计算多个噪声估计值的置信区间时遇到困难。与四舍五入数据相比，他们更能够为差分隐私数据设计基于模拟的不确定性计算方法。令人惊讶的是，一些参与者错误地认为差分隐私更强的效用意味着更弱的隐私保护。", "conclusion": "基于研究结果，提出了文档和工具的设计建议，以更好地支持数据用户处理隐私噪声数据。需要改进用户教育，澄清隐私保护与数据效用之间的关系，并提供更好的工具支持复杂的不确定性计算。"}}
{"id": "2512.06533", "pdf": "https://arxiv.org/pdf/2512.06533", "abs": "https://arxiv.org/abs/2512.06533", "authors": ["Ming Chen", "Sheng Tang", "Rong-Xi Tan", "Ziniu Li", "Jiacheng Chen", "Ke Xue", "Chao Qian"], "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.", "AI": {"tldr": "本文提出使用强化学习来改进基于解码的回归方法，通过序列级奖励来增强数值预测的全局一致性，解决传统token级监督的局限性。", "motivation": "基于解码的回归方法将回归任务转化为序列生成任务，但现有方法存在离散token级目标与连续数值之间的不对齐问题，token级约束难以捕捉目标值的全局幅度，限制了预测精度和泛化能力。", "method": "将生成过程建模为马尔可夫决策过程，使用序列级奖励来强制全局数值一致性，具体采用ReMax和GRPO强化学习方法。", "result": "在表格回归和代码度量回归任务上的实验表明，该方法在采样效率和预测精度方面显著优于最先进的token级基线和传统回归头方法。", "conclusion": "强化学习通过引入序列级信号，显著提升了基于解码的回归方法的性能，使其成为通用数值预测的鲁棒且准确的范式。"}}
{"id": "2512.06531", "pdf": "https://arxiv.org/pdf/2512.06531", "abs": "https://arxiv.org/abs/2512.06531", "authors": ["Sayan Das", "Arghadip Biswas"], "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.", "AI": {"tldr": "本文提出了两种新颖的深度学习架构，用于从MRI图像中分类和分割脑肿瘤，旨在提高脑肿瘤自动检测的准确性和效率。", "motivation": "脑肿瘤对人类生命构成重大威胁，早期准确检测对诊断和治疗至关重要。传统的人工检测方法耗时且困难，特别是在儿童和青少年脑肿瘤发病率上升导致数据量激增的情况下。现有模型泛化能力不足，在验证数据上表现不佳，因此需要开发更有效的自动检测系统。", "method": "提出了两种新颖的深度学习架构：1) SAETCN（自注意力增强肿瘤分类网络）用于脑肿瘤分类，可识别三种肿瘤类型（胶质瘤、脑膜瘤、垂体瘤）和非肿瘤病例；2) SAS-Net（自注意力分割网络）用于脑肿瘤的精确分割。", "result": "SAETCN在验证数据集上达到了99.38%的分类准确率，成为少数能够准确检测脑肿瘤的新型深度学习架构之一。SAS-Net实现了99.23%的整体像素精度，显示出优异的分割性能。", "conclusion": "提出的两种深度学习架构在脑肿瘤分类和分割任务中表现出卓越性能，为计算机辅助诊断系统提供了有效的解决方案，有助于实现脑肿瘤的早期自动检测和诊断。"}}
{"id": "2512.06530", "pdf": "https://arxiv.org/pdf/2512.06530", "abs": "https://arxiv.org/abs/2512.06530", "authors": ["Mohammed Wattad", "Tamir Shor", "Alex Bronstein"], "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.", "AI": {"tldr": "本文研究了k空间采集模式在MRI重建领域泛化中的作用，证明了学习到的k空间采样模式能够提升跨域重建性能，并提出了一种通过引入采集不确定性来增强领域鲁棒性的新方法。", "motivation": "现有研究主要关注针对单一数据集或模态优化的k空间采集模式，缺乏对其跨成像领域可迁移性的考虑。本文旨在探索学习到的k空间采样模式是否能够扩展到训练域之外，在领域偏移下实现更好的重建性能。", "method": "提出了一种增强领域鲁棒性的新方法：在训练过程中引入采集不确定性，通过随机扰动k空间轨迹来模拟不同扫描仪和成像条件之间的变异性，从而提高模型对领域变化的适应能力。", "result": "通过跨数据集和采集范式的系统评估表明，使用学习到的采样模式训练的模型在跨域设置下表现出改进的泛化能力。提出的不确定性扰动方法进一步增强了模型的领域鲁棒性。", "conclusion": "k空间轨迹设计不应仅仅被视为加速机制，而应作为改善MRI重建领域泛化的主动自由度。学习到的k空间采样模式具有跨域泛化潜力，为MRI重建的领域适应性提供了新思路。"}}
{"id": "2512.06524", "pdf": "https://arxiv.org/pdf/2512.06524", "abs": "https://arxiv.org/abs/2512.06524", "authors": ["Saekwang Nam", "Bowen Deng", "Loong Yi Lee", "Jonathan M. Rossiter", "Nathan F. Lepora"], "title": "TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping", "categories": ["cs.RO"], "comment": "Accepted in IEEE Robotics Automation Letters. S. Nam, B. Deng co-first authors", "summary": "We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.", "AI": {"tldr": "开发了一种基于间接触觉感知的软体Fin-Ray手指，能够同时检测接触位置和压入深度，用于提升抓取任务的鲁棒性", "motivation": "为软体机器人结构提供轻量、灵活且可扩展的触觉感知解决方案，特别是在需要将传感器远离接触界面的应用场景中", "method": "采用间接感知方法，在软体Fin-Ray结构和刚性传感模块之间集成铰链机构，通过内部摄像头捕捉变形模式，使用卷积神经网络处理数据推断接触条件", "result": "实现了0.1mm深度和2mm位置感知精度，对各种形状和尺寸的压头表现出鲁棒的泛化能力，在不确定抓取位置的拾放任务中显著提高了放置精度", "conclusion": "该工作为软体机器人结构提供了一种有效的触觉感知解决方案，特别适用于需要将传感器远离接触界面的应用，通过间接感知方法实现了高精度的接触检测"}}
{"id": "2512.06521", "pdf": "https://arxiv.org/pdf/2512.06521", "abs": "https://arxiv.org/abs/2512.06521", "authors": ["Jens Dede", "Anna Förster"], "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages + appendix", "summary": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios. In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.", "AI": {"tldr": "提出一个名为ShadowWolf的统一框架，用于优化相机陷阱野生动物图像的自动标注、评估和模型训练，以应对野生动物监测中的环境变化挑战。", "motivation": "全球人口增长导致人类栖息地扩张，野生动物空间减少，人兽互动增加，从轻微干扰到物种灭绝等严重后果。传统AI训练在多变环境条件下（如地形、天气、光照、距离）面临模型鲁棒性和适应性挑战。", "method": "提出ShadowWolf统一框架，整合并优化AI模型训练和评估阶段，支持动态模型重训练以适应环境条件和应用需求变化，减少标注工作量并允许现场模型适应。", "result": "该框架增强了野生动物监测系统的准确性和效率，促进更有效和可扩展的保护工作，通过自适应和统一方法应对现实场景中的挑战。", "conclusion": "ShadowWolf框架通过集成和优化AI模型训练流程，为野生动物监测提供了一种自适应解决方案，能够应对多变环境条件，减少人工标注需求，提高监测系统的实用性和可扩展性。"}}
{"id": "2512.06517", "pdf": "https://arxiv.org/pdf/2512.06517", "abs": "https://arxiv.org/abs/2512.06517", "authors": ["Shifa Sulaiman", "Akash Bachhar", "Ming Shen", "Simon Bøgh"], "title": "Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments", "categories": ["cs.RO"], "comment": null, "summary": "Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.", "AI": {"tldr": "提出了一种用于假肢手的视觉引导抓取规划算法，在非结构化环境中实现自适应、物体特定的抓取配置", "motivation": "现有假肢技术需要增强在动态环境中的灵巧性和自主性，视觉方法能够使假肢手更自然地与多样化物体交互", "method": "采用基于边界体积层次(BVH)的视觉算法分割物体并定义边界框，使用快速探索随机树星(RRT*)算法生成候选轨迹，基于最小欧几里得距离选择指尖末端姿态，采用阻尼最小二乘法(DLS)逆运动学求解器计算关节角度", "result": "在仿真中验证了该方法，并在Linker Hand O7平台上进行了实验集成，实现了模块化管道支持每根手指的抓取规划和实时适应性", "conclusion": "提出的视觉引导抓取算法能够使假肢手在非结构化环境中实现自适应、物体特定的抓取配置，为智能假肢控制提供了有效解决方案"}}
{"id": "2512.06506", "pdf": "https://arxiv.org/pdf/2512.06506", "abs": "https://arxiv.org/abs/2512.06506", "authors": ["Junhui Jeff Cai", "Xian Gu", "Liugang Sheng", "Mengjia Xia", "Linda Zhao", "Wu Zhu"], "title": "AI as \"Co-founder\": GenAI for Entrepreneurship", "categories": ["econ.GN", "cs.AI", "stat.AP"], "comment": null, "summary": "This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.", "AI": {"tldr": "研究生成式人工智能（GenAI）如何作为\"联合创始人\"促进创业，特别是对小型企业创建的影响", "motivation": "探索生成式人工智能是否、如何以及为谁促进企业创建，利用ChatGPT发布作为自然实验来研究AI对创业的影响", "method": "利用2022年11月ChatGPT发布作为全球冲击，通过地理编码网格中AI特定人力资本的差异变化，结合2024年底中国公司注册的高分辨率数据进行分析", "result": "AI人力资本较强的地区新公司创建激增（占全国公司进入的6.0%），主要由小型企业驱动；大型企业进入减少；新公司资本、股东数和创始团队规模更小；对潜在AI应用、融资需求较低和首次创业者影响最大", "conclusion": "生成式人工智能作为促进竞争的力量，通过不成比例地促进小型企业进入来重塑创业格局"}}
{"id": "2512.06504", "pdf": "https://arxiv.org/pdf/2512.06504", "abs": "https://arxiv.org/abs/2512.06504", "authors": ["Andrii Lysyi", "Anatoliy Sachenko", "Pavlo Radiuk", "Mykola Lysyi", "Oleksandr Melnychenko", "Diana Zahorodnia"], "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": ":68T45; 68T07; 68T40ACM Class:I.4.8; I.2.10; I.2.6; I.2.9", "summary": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.", "AI": {"tldr": "开发一种基于无人机热成像与RGB数据融合的光伏组件智能检测方法，实现自动化巡检工作流程", "motivation": "解决传统光伏检测方法存在的热调色板偏差、数据冗余和高通信带宽需求等关键缺陷", "method": "采用协同架构：1) 调色板不变的热嵌入表示学习；2) 通过门控机制融合对比度归一化的RGB数据流；3) 基于罗德里格斯更新的自适应重采集控制器；4) 使用DBSCAN和半正矢距离的地理空间去重模块", "result": "在公开PVF-10基准测试中达到0.903的mAP@0.5，比单模态基线提高12-15%；现场验证召回率达96%，去重过程减少15-20%的重复误报，相关性遥测减少60-70%的空中数据传输", "conclusion": "建立了一种主动式光伏检测的新范式，系统已具备实际应用准备，显著提升了光伏电站的安全性和运行效率"}}
{"id": "2512.06496", "pdf": "https://arxiv.org/pdf/2512.06496", "abs": "https://arxiv.org/abs/2512.06496", "authors": ["Stella Brown", "Nicolas Preisig", "Autumn Davis", "Brian Hutchinson", "Filip Jagodzinski"], "title": "PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning", "categories": ["q-bio.BM", "cs.AI", "cs.LG", "cs.NE", "q-bio.QM"], "comment": "Presented at Computational Structural Bioinformatics Workshop 2025", "summary": "Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.", "AI": {"tldr": "PRIMRose是一个深度学习模型，用于预测蛋白质双氨基酸插入/删除(InDel)突变对每个残基能量指标的影响，提供残基特异性的能量变化分析。", "motivation": "理解蛋白质突变如何影响蛋白质结构对于计算生物学和生物信息学的发展至关重要。现有模型主要评估全局能量变化，缺乏对局部能量影响的残基特异性分析，特别是在双InDel突变情况下。", "method": "采用卷积神经网络架构，预测蛋白质突变中每个残基的能量变化。模型在三个数据集上训练：包含详尽双InDel突变的数据集、约145k随机采样的双InDel突变数据集、约80k随机采样的双InDel突变数据集，共涉及9种蛋白质。", "result": "模型在Rosetta分子建模套件计算的各种能量指标上实现了高预测准确性。揭示了影响模型性能的局部模式，如溶剂可及性和二级结构上下文，提供了对蛋白质特定区域突变耐受性的新见解。", "conclusion": "PRIMRose提供了残基特异性的能量分析，能够更可解释和生物学意义地预测双InDel突变的影响，为理解蛋白质突变对结构和功能的影响提供了新工具。"}}
{"id": "2512.06486", "pdf": "https://arxiv.org/pdf/2512.06486", "abs": "https://arxiv.org/abs/2512.06486", "authors": ["Wanru Gong", "Xinyi Zheng", "Xiaopeng Yang", "Xiaoqing Zhu"], "title": "Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains", "categories": ["cs.RO"], "comment": null, "summary": "Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration. For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.", "AI": {"tldr": "提出一种基于熵控制的内在动机强化学习算法（ECIM），用于四足机器人在复杂地形中的运动控制，旨在解决传统PPO算法过早收敛的问题。", "motivation": "传统PPO系列算法在训练四足机器人运动策略时经常过早收敛，导致运动性能次优和任务表现下降，需要一种能减少过早收敛的新方法。", "method": "提出ECIM算法，将内在动机与自适应探索相结合，通过熵控制来调节探索-利用平衡，在Isaac Gym中测试六种地形类别。", "result": "相比基线方法，任务奖励提升4-12%，身体俯仰振荡峰值降低23-29%，关节加速度减少20-32%，关节扭矩消耗下降11-20%。", "conclusion": "ECIM算法通过结合熵控制和内在动机控制，在不同地形上实现了更好的运动稳定性，同时降低了能量消耗，是复杂机器人控制任务的实用选择。"}}
{"id": "2512.06485", "pdf": "https://arxiv.org/pdf/2512.06485", "abs": "https://arxiv.org/abs/2512.06485", "authors": ["Kush Revankar", "Shreyas Deshpande", "Araham Sayeed", "Ansh Tandale", "Sarika Bobde"], "title": "Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.", "AI": {"tldr": "提出Sanvaad多模态无障碍框架，支持印度手语识别和语音交互，实现聋哑用户、视障用户与普通听力人群之间的双向实时通信", "motivation": "当前聋哑用户、视障用户与普通听力人群之间的通信工具通常只支持单向交互，缺乏双向实时通信的解决方案，限制了无障碍沟通的包容性", "method": "基于MediaPipe地标构建轻量级ISL识别模块，使用语音到手语组件将语音转换为预定义短语的GIF或字母可视化，为视障用户提供无屏幕语音界面，集成多语言语音识别、文本摘要和文本转语音，通过Streamlit界面在桌面和移动设备上运行", "result": "开发了一个轻量级多模态无障碍框架，能够在边缘设备上流畅运行，无需专用硬件，支持实时双向通信，结合了轻量级计算机视觉和语音处理工具", "conclusion": "Sanvaad通过统一的轻量级框架为包容性通信提供了实用且可访问的途径，将计算机视觉和语音处理工具结合，支持多用户群体的无障碍交互"}}
{"id": "2512.06483", "pdf": "https://arxiv.org/pdf/2512.06483", "abs": "https://arxiv.org/abs/2512.06483", "authors": ["Elias-Leander Ahlers", "Witold Brunsmann", "Malte Schilling"], "title": "Classifying German Language Proficiency Levels Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at 3rd International Conference on Foundation and Large Language Models (FLLM2025), Vienna (Austria)", "summary": "Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.", "AI": {"tldr": "使用大语言模型自动对德语文本进行CEFR语言水平分类", "motivation": "语言能力评估对教育至关重要，能够根据学习者需求提供定制化教学。需要可靠且可扩展的自动分类方法来替代人工评估。", "method": "构建多样化数据集（结合现有CEFR标注语料库和合成数据），评估提示工程策略、微调LLaMA-3-8B-Instruct模型，以及基于LLM内部神经状态的探测分类方法", "result": "相比先前方法实现了持续的性能提升，展示了LLMs在可靠且可扩展的CEFR分类方面的潜力", "conclusion": "大语言模型在德语语言水平自动分类方面具有显著潜力，能够提供可靠且可扩展的CEFR分类解决方案"}}
{"id": "2512.06471", "pdf": "https://arxiv.org/pdf/2512.06471", "abs": "https://arxiv.org/abs/2512.06471", "authors": ["Nathan P. Lawrence", "Ali Mesbah"], "title": "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control", "categories": ["cs.LG", "cs.AI"], "comment": "IFAC preprint", "summary": "Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.", "AI": {"tldr": "分析目标条件强化学习（GCRL）与经典最优控制方法之间的关系，推导两者之间的最优性差距，并建立与双重控制问题的联系", "motivation": "目标条件强化学习在实践中表现出色，但缺乏理论解释。研究者希望从最优控制角度分析GCRL为何有效，特别是与经典密集奖励方法的对比，以及在部分可观测环境中的优势", "method": "从最优控制理论出发，推导经典二次型目标与目标条件奖励之间的最优性差距；考虑部分可观测马尔可夫决策过程，将状态估计与概率奖励联系起来；在非线性和不确定环境中使用强化学习和预测控制技术进行验证", "result": "建立了目标条件强化学习与经典最优控制之间的理论联系，解释了为什么GCRL比密集奖励方法更有效；证明了目标条件奖励特别适合双重控制问题；在非线性和不确定环境中的实验验证了目标条件策略的优势", "conclusion": "目标条件强化学习的成功可以从最优控制理论得到解释，它通过概率奖励形式避免了经典密集奖励的局限性，特别适合处理不确定性和部分可观测性，为双重控制问题提供了有效的解决方案"}}
{"id": "2512.06459", "pdf": "https://arxiv.org/pdf/2512.06459", "abs": "https://arxiv.org/abs/2512.06459", "authors": ["Shiliang Zhang", "Sabita Maharjan"], "title": "Cenergy3: An API for City Energy 3D Modeling", "categories": ["cs.HC"], "comment": null, "summary": "The efficient management and planning of urban energy systems require integrated three-dimensional (3D) models that accurately represent both consumption nodes and distribution networks. This paper introduces our developed geospatial Application Programming Interface (API) that automates the generation of 3D urban digital model from open data. The API synthesizes data from OpenTopography, OpenStreetMap, and Overture Maps in generating 3D models. The rendered model visualizes and contextualizes power grid infrastructure alongside the built environment and transportation networks. The API provides interactive figures for the 3D models, which are essential for analyzing infrastructure alignment and spatially linking energy demand nodes (buildings) with energy supply (utility grids). Our API leverages standard Web Mercator coordinates (EPSG:3857) and JSON serialization to ensure interoperability within smart city and energy simulation platforms.", "AI": {"tldr": "开发一个用于城市能源3D建模的API，能够从开放数据自动生成城市数字模型，可视化电力基础设施与建筑环境、交通网络的关联", "motivation": "城市能源系统的高效管理和规划需要集成的三维模型来准确表示消费节点和配电网络，现有方法缺乏自动化生成3D城市数字模型的工具", "method": "开发了一个地理空间API，整合OpenTopography、OpenStreetMap和Overture Maps的数据，使用标准Web Mercator坐标(EPSG:3857)和JSON序列化，生成交互式3D模型", "result": "API能够自动生成3D城市数字模型，可视化电力基础设施与建筑环境、交通网络的关联，提供交互式图形用于分析基础设施对齐和空间连接能源需求节点与供应", "conclusion": "Cenergy3 API为智能城市和能源模拟平台提供了一个可互操作的解决方案，能够自动生成3D城市能源模型，支持城市能源系统的高效管理和规划"}}
{"id": "2512.06458", "pdf": "https://arxiv.org/pdf/2512.06458", "abs": "https://arxiv.org/abs/2512.06458", "authors": ["Rishiraj Bhattacharyya", "Sourav Chakraborty", "Yash Pote", "Uddalok Sarkar", "Sayantan Sen"], "title": "Instance Dependent Testing of Samplers using Interval Conditioning", "categories": ["cs.DS", "cs.AI"], "comment": null, "summary": "Sampling algorithms play a pivotal role in probabilistic AI. However, verifying if a sampler program indeed samples from the claimed distribution is a notoriously hard problem. Provably correct testers like Barbarik, Teq, Flash, CubeProbe for testing of different kinds of samplers were proposed only in the last few years. All these testers focus on the worst-case efficiency, and do not support verification of samplers over infinite domains, a case occurring frequently in Astronomy, Finance, Network Security, etc. In this work, we design the first tester of samplers with instance-dependent efficiency, allowing us to test samplers over natural numbers. Our tests are developed via a novel distance estimation algorithm between an unknown and a known probability distribution using an interval conditioning framework. The core technical contribution is a new connection with probability mass estimation of a continuous distribution. The practical gains are also substantial: our experiments establish up to 1000x speedup over state-of-the-art testers.", "AI": {"tldr": "提出首个具有实例依赖效率的采样器测试器，能够测试自然数域上的采样器，通过区间条件框架实现未知分布与已知分布之间的距离估计", "motivation": "现有采样器测试方法仅关注最坏情况效率，无法验证无限域上的采样器，而无限域采样在多个领域（天文学、金融、网络安全等）中频繁出现", "method": "通过区间条件框架设计距离估计算法，建立与连续分布概率质量估计的新连接，实现未知分布与已知分布之间的距离测量", "result": "实验结果显示，相比现有最先进的测试器，新方法实现了高达1000倍的加速", "conclusion": "成功设计了首个支持无限域采样器测试的实例依赖效率测试器，通过创新的区间条件框架和概率质量估计连接，显著提升了测试性能"}}
{"id": "2512.06447", "pdf": "https://arxiv.org/pdf/2512.06447", "abs": "https://arxiv.org/abs/2512.06447", "authors": ["Jiuyi Chen", "Mingkui Tan", "Haifeng Lu", "Qiuna Xu", "Zhihua Wang", "Runhao Zeng", "Xiping Hu"], "title": "Towards Stable Cross-Domain Depression Recognition under Missing Modalities", "categories": ["cs.CV"], "comment": null, "summary": "Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.", "AI": {"tldr": "提出一个基于多模态大语言模型的稳定跨域抑郁症识别框架，解决现实应用中多模态数据缺失和跨域泛化问题", "motivation": "抑郁症带来严重的公共健康风险，需要及时且可扩展的筛查。现有的多模态自动抑郁症检测方法缺乏统一的通用框架，对现实世界中常见的模态缺失情况稳定性不足", "method": "提出SCD-MLLM框架，包含两个关键组件：1) 多源数据输入适配器(MDIA)，使用掩码机制和任务特定提示将异构数据转换为统一标记序列；2) 模态感知自适应融合模块(MAFM)，通过共享投影机制自适应整合音频和视觉特征", "result": "在五个公开抑郁症数据集(CMDC、AVEC2014、DAIC-WOZ、DVlog、EATD)上进行多数据集联合训练实验，在完整和部分模态设置下均优于现有SOTA模型及领先商业LLM(Gemini和GPT)，展示了优越的跨域泛化能力和模态缺失稳定性", "conclusion": "SCD-MLLM框架为抑郁症识别提供了一个统一、稳定且可泛化的解决方案，能够有效处理异构数据源和模态缺失问题，在现实应用中具有重要价值"}}
{"id": "2512.06444", "pdf": "https://arxiv.org/pdf/2512.06444", "abs": "https://arxiv.org/abs/2512.06444", "authors": ["Xuehui Ma", "Shiliang Zhang", "Zhiyong Sun"], "title": "Fault Tolerant Control of Mecanum Wheeled Mobile Robots", "categories": ["cs.RO"], "comment": null, "summary": "Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.", "AI": {"tldr": "提出一种针对麦克纳姆轮移动机器人(MWMR)的容错控制策略，能够同时处理完全执行器故障（如电机堵转）和部分故障（如扭矩退化）", "motivation": "麦克纳姆轮移动机器人对执行器故障高度敏感，现有容错控制方案主要针对完全故障，忽略了部分故障（如扭矩退化），这会影响性能并导致任务失败", "method": "采用后验概率实时学习故障参数，通过聚合与预定义故障对应的概率加权控制律来推导容错控制律", "result": "仿真结果表明，该容错控制策略在不同故障场景下都能有效工作，确保了MWMR控制的鲁棒性和安全性", "conclusion": "提出的容错控制策略能够处理完全和部分执行器故障，通过概率加权方法提高了MWMR在故障情况下的控制鲁棒性和安全性"}}
{"id": "2512.06443", "pdf": "https://arxiv.org/pdf/2512.06443", "abs": "https://arxiv.org/abs/2512.06443", "authors": ["Xiangyu Li", "Chengyu Yin", "Weijun Wang", "Jianyu Wei", "Ting Cao", "Yunxin Liu"], "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices", "categories": ["cs.DC", "cs.AI"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence. However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token. To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.", "AI": {"tldr": "提出Vec-LUT方法，通过向量查找表优化超低位LLM在边缘设备上的并行推理性能", "motivation": "现有基于查找表（LUT）的推理方法在并行推理场景下内存带宽利用率不足，导致预填充、测试时缩放等多令牌场景性能受限", "method": "提出向量LUT范式，构建跨并行令牌的统一查找表，实现单次1→N查找；引入向量LUT中心张量布局和缓存感知流式查找技术", "result": "在5个边缘设备和3个LLM上的评估显示，Vec-LUT比现有最优方法提升高达4.2倍，已集成到llama.cpp中", "conclusion": "Vec-LUT通过向量化查找表范式有效解决了超低位LLM并行推理中的内存带宽利用问题，显著提升了边缘设备上的推理性能"}}
{"id": "2512.06438", "pdf": "https://arxiv.org/pdf/2512.06438", "abs": "https://arxiv.org/abs/2512.06438", "authors": ["Ramazan Fazylov", "Sergey Zagoruyko", "Aleksandr Parkin", "Stamatis Lefkimmiatis", "Ivan Laptev"], "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars", "categories": ["cs.CV"], "comment": null, "summary": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/", "AI": {"tldr": "提出AGORA框架，基于3D高斯泼溅和生成对抗网络，实现可实时动画化的3D头部虚拟人生成", "motivation": "现有基于NeRF的方法渲染速度慢且动态一致性差，而3DGS方法通常局限于静态头部生成，缺乏动态控制能力，需要一种既能保持高质量又能实时动画化的解决方案", "method": "结合3D高斯泼溅与生成对抗网络，引入轻量级的FLAME条件变形分支来预测每个高斯的残差，采用双鉴别器训练方案利用参数化网格的合成渲染来增强表情保真度", "result": "在表情准确性上优于最先进的NeRF方法，在单GPU上实现250+ FPS渲染，在仅CPU推理下达到约9 FPS，首次展示了实用的仅CPU可动画化3DGS虚拟人合成", "conclusion": "AGORA代表了向实用高性能数字人类迈出的重要一步，实现了视觉真实且精确可控的虚拟人生成，在渲染速度和动画控制方面取得了显著突破"}}
{"id": "2512.06434", "pdf": "https://arxiv.org/pdf/2512.06434", "abs": "https://arxiv.org/abs/2512.06434", "authors": ["Lucas R. Mareque", "Ricardo L. Armentano", "Leandro J. Cymberknop"], "title": "Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 2 figures, 3 tables", "summary": "Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.", "AI": {"tldr": "开发基于深度学习的自动化系统，从2D合成人体图像中估计五项关键人体测量指标，用于运动员心血管筛查", "motivation": "传统人工测量方法劳动密集、操作者依赖性强且难以规模化，而人体测量指标（如腰围、肢体长度等）对检测马凡综合征和评估心血管风险至关重要", "method": "使用从3D身体网格生成的100,000张合成图像数据集，训练并评估VGG19、ResNet50和DenseNet121模型，通过全连接层进行回归分析", "result": "所有模型均达到亚厘米级精度，其中ResNet50表现最佳，在所有测量指标上的平均MAE为0.668厘米", "conclusion": "深度学习能够规模化提供准确的人体测量数据，为运动员筛查协议提供实用工具，未来将在真实世界图像上验证模型以扩展应用范围"}}
{"id": "2512.06431", "pdf": "https://arxiv.org/pdf/2512.06431", "abs": "https://arxiv.org/abs/2512.06431", "authors": ["Mohamed Shamroukh", "Mohamed Alkhuzamy Aziz"], "title": "Smart Spatial Planning in Egypt: An Algorithm-Driven Approach to Public Service Evaluation in Qena City", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "National planning standards for public services in Egypt often fail to align with unique local characteristics. Addressing this gap, this study develops a tailored planning model for Qena City. Using a hybrid methodology (descriptive, analytical, and experimental), the research utilizes Python programming to generate an intelligent spatial analysis algorithm based on Voronoi Diagrams. This approach creates city-specific planning criteria and evaluates the current coverage of public facilities. The primary contribution of this study is the successful derivation of a localized planning standards model and the deployment of an automated algorithm to assess service efficiency. Application of this model reveals a general service coverage average of 81.3%. Ambulance stations demonstrated the highest efficiency (99.8%) due to recent upgrades, while parks and open spaces recorded the lowest coverage (10%) caused by limited land availability. Spatial analysis indicates a high service density in midtown (>45 services/km^2), which diminishes significantly towards the outskirts (<5 services/km^2). Consequently, the Hajer Qena district contains the highest volume of unserved areas, while the First District (Qesm 1) exhibits the highest level of service coverage. This model offers a replicable framework for data-driven urban planning in Egyptian cities.", "AI": {"tldr": "开发基于Voronoi图的智能空间分析算法，为埃及基纳市创建定制化公共服务规划模型，评估现有设施覆盖效率", "motivation": "埃及国家公共服务规划标准往往无法适应地方独特特征，需要开发能够反映当地实际情况的定制化规划模型", "method": "采用混合方法（描述性、分析性和实验性），使用Python编程开发基于Voronoi图的智能空间分析算法，生成城市特定规划标准", "result": "总体服务覆盖率为81.3%，救护车站效率最高（99.8%），公园和开放空间覆盖率最低（10%）；市中心服务密度高（>45个/km²），郊区显著降低（<5个/km²）；Hajer Qena区未服务区域最多，第一区服务覆盖率最高", "conclusion": "成功开发了本地化规划标准模型和自动化评估算法，为埃及城市提供了可复制的数据驱动城市规划框架"}}
{"id": "2512.06426", "pdf": "https://arxiv.org/pdf/2512.06426", "abs": "https://arxiv.org/abs/2512.06426", "authors": ["Nzakiese Mbongo", "Kailash A. Hambarde", "Hugo Proença"], "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 9 figures", "summary": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.", "AI": {"tldr": "提出一种双路径Transformer框架，利用CLIP联合建模视觉和属性线索，用于远距离性别识别", "motivation": "远距离图像中的性别识别面临空间分辨率低、视角多变、面部特征丢失等挑战，需要多属性支持来提高识别准确性", "method": "采用双路径Transformer框架：1) 直接视觉路径通过选择性微调CLIP图像编码器上层；2) 属性中介路径通过软生物特征提示（发型、服装、配饰）在CLIP文本-图像空间中推断性别；使用空间通道注意力模块增强判别性定位", "result": "提出的方法在多个指标（macro-F1、准确率、AUC）上超越了最先进的人物属性和重识别基线，对距离、角度和高度变化具有一致的鲁棒性；构建了U-DetAGReID统一远距离性别数据集", "conclusion": "语言引导的双路径学习为无约束远距离场景中的负责任性别识别提供了原则性、可扩展的基础，具有可解释的属性定位和负责任的弃权行为"}}
{"id": "2512.06424", "pdf": "https://arxiv.org/pdf/2512.06424", "abs": "https://arxiv.org/abs/2512.06424", "authors": ["Tianshan Zhang", "Zeyu Zhang", "Hao Tang"], "title": "DragMesh: Interactive 3D Generation Made Easy", "categories": ["cs.CV"], "comment": null, "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.", "AI": {"tldr": "DragMesh是一个用于实时交互式3D关节运动的框架，通过解耦的动力学推理和运动生成方法，实现对新物体的合理关节运动生成。", "motivation": "现有方法在关节运动生成上存在两难：要么物理一致但速度太慢无法实时使用，要么生成速度快但违反基本运动学约束。需要一种既能实时交互又能保持运动学合理性的解决方案。", "method": "提出解耦的动力学推理和运动生成框架：1) 通过语义意图推理和几何回归（使用KPP-Net）推断潜在关节参数；2) 开发基于对偶四元数的VAE（DQ-VAE）生成完整运动轨迹；3) 使用FiLM条件注入关节先验，确保运动学约束；4) 采用数值稳定的叉积损失保证轴对齐。", "result": "DragMesh实现了实时性能，能够在新物体上生成合理的关节运动而无需重新训练，为生成式3D智能提供了实用的一步。", "conclusion": "DragMesh通过解耦的动力学推理和运动生成框架，解决了实时交互式3D关节运动生成的挑战，在保持运动学合理性的同时实现了实时性能，为生成式3D内容创建提供了实用工具。"}}
{"id": "2512.06423", "pdf": "https://arxiv.org/pdf/2512.06423", "abs": "https://arxiv.org/abs/2512.06423", "authors": ["Leonardo F. Dos Santos", "Elisa G. Vergamini", "Cícero Zanette", "Lucca Maitan", "Thiago Boaventura"], "title": "Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking", "categories": ["cs.RO", "eess.SY"], "comment": "This is the author's version of the paper accepted for publication in the 2025 International Conference on Advanced Robotics (ICAR). The final version will be available at IEEE Xplore", "summary": "This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.", "AI": {"tldr": "提出基于端口哈密顿理论的阻抗控制基准测试方法，包括因果一致的PH模型和可微分的n自由度无源性条件", "motivation": "需要标准化的阻抗控制基准测试方法，以评估不同控制器的性能，特别是在笛卡尔空间中的质量-弹簧-阻尼器阻抗控制", "method": "引入因果一致的端口哈密顿模型，推导出可微分、不依赖力-扭矩传感的n自由度无源性条件，定义基于自由运动阶跃响应功率的阻抗保真度度量", "result": "在Gazebo仿真中验证了所提度量，使用六自由度机械臂和四足机器人腿部，证明了PH框架适用于标准化阻抗控制基准测试", "conclusion": "端口哈密顿理论为阻抗控制基准测试提供了有效的框架，所提出的度量能够标准化评估不同控制器的性能"}}
{"id": "2512.06422", "pdf": "https://arxiv.org/pdf/2512.06422", "abs": "https://arxiv.org/abs/2512.06422", "authors": ["Chunwei Tian", "Jingyuan Xie", "Lingjun Li", "Wangmeng Zuo", "Yanning Zhang", "David Zhang"], "title": "A Perception CNN for Facial Expression Recognition", "categories": ["cs.CV"], "comment": "in IEEE Transactions on Image Processing (2025)", "summary": "Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.", "AI": {"tldr": "提出一种感知CNN（PCNN）用于面部表情识别，通过并行网络学习局部面部特征，并结合多域交互机制和两阶段损失函数来提高识别性能。", "motivation": "传统卷积神经网络在面部表情识别中可能忽略面部区域分割的影响，无法有效捕捉面部表情的细微变化，需要一种能够同时学习局部面部特征和全局结构特征的方法。", "method": "1. 使用五个并行网络同时学习基于眼睛、脸颊和嘴巴的局部面部特征；2. 采用多域交互机制注册和融合局部感官特征与全局面部结构特征；3. 设计两阶段损失函数来约束获取的感官信息准确性和重建的面部图像质量。", "result": "PCNN在多个实验室和真实世界面部表情识别基准测试中取得了优越结果，包括CK+、JAFFE、FER2013、FERPlus、RAF-DB以及遮挡和姿态变化数据集。", "conclusion": "提出的PCNN方法通过并行学习局部特征、多域特征融合和两阶段损失函数，有效提高了面部表情识别的性能，在多个基准测试中表现出色。"}}
{"id": "2512.06421", "pdf": "https://arxiv.org/pdf/2512.06421", "abs": "https://arxiv.org/abs/2512.06421", "authors": ["Gengze Zhou", "Chongjian Ge", "Hao Tan", "Feng Liu", "Yicong Hong"], "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.", "AI": {"tldr": "提出Self-Autoregressive Refinement (SAR)方法，通过Stagger-Scale Rollout机制和Contrastive Student-Forcing Loss，解决尺度自回归生成模型中的曝光偏差问题，提升生成质量。", "motivation": "尺度自回归生成模型存在曝光偏差问题，主要源于训练-测试不匹配和尺度间学习难度不平衡，这限制了生成质量。", "method": "提出SAR方法，包含两个核心组件：1) Stagger-Scale Rollout (SSR)机制，通过轻量级自回归展开让模型接触自身中间预测；2) Contrastive Student-Forcing Loss (CSFL)，为自生成上下文提供充分监督以确保稳定训练。", "result": "实验表明SAR能显著提升预训练AR模型的生成质量，计算开销小。例如，在ImageNet 256数据集上，SAR使FlexVAR-d16的FID降低5.2%，仅需10个epoch（32xA100 GPU上5小时）。", "conclusion": "SAR作为一种高效、可扩展且有效的后训练方法，有望成为视觉自回归生成的可靠解决方案。"}}
{"id": "2512.06417", "pdf": "https://arxiv.org/pdf/2512.06417", "abs": "https://arxiv.org/abs/2512.06417", "authors": ["Yifan Sun", "Lei Cheng", "Jianlong Li", "Peter Gerstoft"], "title": "Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator", "categories": ["cs.LG", "cs.SD"], "comment": null, "summary": "Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.", "AI": {"tldr": "提出Hankel-FNO模型，一种基于傅里叶神经算子的高效水下声学制图方法，用于快速准确的水下声场预测", "motivation": "传统数值求解器计算成本高，不适合大规模或实时应用；现有深度学习替代模型存在固定分辨率限制或依赖显式偏微分方程公式的问题，限制了其适用性和泛化能力", "method": "基于傅里叶神经算子(FNO)框架，结合声传播知识和海底地形信息，构建物理编码的深度学习模型", "result": "Hankel-FNO在速度上优于传统求解器，在精度上超越数据驱动替代方法，尤其在长距离预测中表现优异，对多样化环境和声源设置具有良好适应性", "conclusion": "提出的Hankel-FNO模型实现了高效准确的水下声学制图，为环境感知传感器部署优化和自主车辆路径规划等下游任务提供了可行的解决方案"}}
{"id": "2512.06408", "pdf": "https://arxiv.org/pdf/2512.06408", "abs": "https://arxiv.org/abs/2512.06408", "authors": ["Shuai Chen", "Lei Han", "Haoyu Wang", "Zhaoman Zhong"], "title": "CommentScope: A Comment-Embedded Assisted Reading System for a Long Text", "categories": ["cs.HC"], "comment": "32 pages, 9 figures. Submitted to CHI 2025", "summary": "Long texts are ubiquitous on social platforms, yet readers often face information overload and struggle to locate key content. Comments provide valuable external perspectives for understanding, questioning, and complementing the text, but their potential is hindered by disorganized and unstructured presentation. Few studies have explored embedding comments directly into reading. As an exploratory step, we propose CommentScope, a system with two core modules: a pipeline that classifies comments into five types and aligns them with relevant sentences, and a presentation module that integrates comments inline or as side notes, supported by visual cues such as colors, charts, and highlights. Technical evaluation shows that the hybrid \"Rule+LLM\" pipeline achieved solid performance in semantic classification (accuracy=0.90) and position alignment (accuracy=0.88). A user study (N=12) further demonstrated that the sentence-end embedding significantly improved comment discovery accuracy and reading fluency while reducing mental demand and perceived effort.", "AI": {"tldr": "开发了一个名为CommentScope的评论嵌入辅助阅读系统，用于帮助读者更好地理解和定位长文本中的关键内容", "motivation": "长文本在社交平台上普遍存在，读者面临信息过载和难以定位关键内容的挑战。评论提供了有价值的外部视角，但由于呈现方式混乱无序，其潜力未能充分发挥", "method": "系统包含两个核心模块：1) 管道模块，将评论分类为五种类型并与相关句子对齐；2) 呈现模块，将评论以内联或侧边注的形式集成，并辅以颜色、图表和高亮等视觉线索。采用\"规则+LLM\"混合方法", "result": "技术评估显示，混合管道在语义分类（准确率=0.90）和位置对齐（准确率=0.88）方面表现良好。用户研究（N=12）表明，句尾嵌入显著提高了评论发现准确率和阅读流畅性，同时降低了心理需求和感知努力", "conclusion": "CommentScope系统通过将评论结构化嵌入长文本阅读中，有效解决了信息过载问题，提升了阅读效率和理解深度，为评论辅助阅读系统提供了有价值的探索"}}
{"id": "2512.06406", "pdf": "https://arxiv.org/pdf/2512.06406", "abs": "https://arxiv.org/abs/2512.06406", "authors": ["Xianzong Wu", "Xiaohong Li", "Lili Quan", "Qiang Hu"], "title": "UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.", "AI": {"tldr": "UncertaintyZoo是一个统一的工具包，集成了29种不确定性量化方法，用于评估深度学习系统的预测不确定性，特别是在大语言模型应用中。", "motivation": "大语言模型在现实应用中的错误预测可能导致安全关键场景中的潜在损失，但现有的不确定性量化方法缺乏统一工具集成，阻碍了这些方法的实际应用和未来研究。", "method": "开发了一个标准化的统一工具包，集成了29种不确定性量化方法，涵盖五大类别，提供了标准化的接口，并在代码漏洞检测任务上对CodeBERT和ChatGLM3模型进行了评估。", "result": "实验结果表明，UncertaintyZoo能够有效揭示预测不确定性，证明了该工具包在实际应用中的有效性。", "conclusion": "UncertaintyZoo填补了不确定性量化方法缺乏统一工具的空白，促进了这些方法的实际应用和未来研究，特别是在大语言模型的安全关键应用中。"}}
{"id": "2512.06404", "pdf": "https://arxiv.org/pdf/2512.06404", "abs": "https://arxiv.org/abs/2512.06404", "authors": ["Mohammad Soleymanibrojeni", "Roland Aydin", "Diego Guedes-Sobrinho", "Alexandre C. Dias", "Maurício J. Piotrowski", "Wolfgang Wenzel", "Celso Ricardo Caldeira Rêgo"], "title": "GENIUS: An Agentic AI Framework for Autonomous Design and Execution of Simulation Protocols", "categories": ["cs.AI", "cond-mat.mtrl-sci", "physics.chem-ph"], "comment": null, "summary": "Predictive atomistic simulations have propelled materials discovery, yet routine setup and debugging still demand computer specialists. This know-how gap limits Integrated Computational Materials Engineering (ICME), where state-of-the-art codes exist but remain cumbersome for non-experts. We address this bottleneck with GENIUS, an AI-agentic workflow that fuses a smart Quantum ESPRESSO knowledge graph with a tiered hierarchy of large language models supervised by a finite-state error-recovery machine. Here we show that GENIUS translates free-form human-generated prompts into validated input files that run to completion on $\\approx$80% of 295 diverse benchmarks, where 76% are autonomously repaired, with success decaying exponentially to a 7% baseline. Compared with LLM-only baselines, GENIUS halves inference costs and virtually eliminates hallucinations. The framework democratizes electronic-structure DFT simulations by intelligently automating protocol generation, validation, and repair, opening large-scale screening and accelerating ICME design loops across academia and industry worldwide.", "AI": {"tldr": "GENIUS是一个AI代理框架，用于自主设计和执行模拟协议，旨在自动化量子力学模拟的设置和调试过程", "motivation": "预测性原子模拟推动了材料发现，但常规设置和调试仍然需要计算机专家。这种知识差距限制了集成计算材料工程（ICME）的发展，因为最先进的代码对非专家来说仍然很繁琐", "method": "GENIUS融合了智能的Quantum ESPRESSO知识图谱和分层的大型语言模型层次结构，由有限状态错误恢复机监督，将自由形式的人类提示转换为验证过的输入文件", "result": "在295个多样化基准测试中，约80%的输入文件能够运行完成，其中76%是自主修复的，成功率呈指数衰减至7%基线。与仅使用LLM的基线相比，GENIUS将推理成本减半，并几乎消除了幻觉", "conclusion": "该框架通过智能自动化协议生成、验证和修复，使电子结构DFT模拟民主化，为学术界和工业界的大规模筛选和加速ICME设计循环打开了大门"}}
{"id": "2512.06400", "pdf": "https://arxiv.org/pdf/2512.06400", "abs": "https://arxiv.org/abs/2512.06400", "authors": ["Jing Tao", "Yonghong Zong", "Banglei Guana", "Pengju Sun", "Taihang Lei", "Yang Shanga", "Qifeng Yu"], "title": "Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement", "categories": ["cs.CV"], "comment": "The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "summary": "In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.", "AI": {"tldr": "提出一种基于区域感知的红外-可见光协同融合框架，用于极端场景增强，通过多曝光和多模态成像结合，在保持可见光几何保真度的同时融入热辐射信息", "motivation": "在摄影测量中，准确融合红外和可见光谱同时保持可见光特征的几何保真度并融入热辐射信息是一个重大挑战，特别是在极端条件下。现有方法通常会损害可见图像质量，影响测量精度。", "method": "提出基于区域感知的融合框架，结合多曝光和多模态成像，使用空间变化曝光相机。首先进行基于区域感知的特征融合以确保精确的多模态配准，然后进行自适应融合与对比度增强。通过区域显著性图引导的结构相似性补偿机制优化红外-可见光谱整合。", "result": "在合成和真实世界数据上的实验表明，该方法在图像清晰度和性能方面优于现有最先进方法，定量和视觉评估都证明了其优越性。", "conclusion": "该区域感知融合框架能够有效解决极端条件下红外-可见光融合的挑战，在保持几何保真度的同时提升图像质量，适用于各种条件下的鲁棒融合。"}}
{"id": "2512.06396", "pdf": "https://arxiv.org/pdf/2512.06396", "abs": "https://arxiv.org/abs/2512.06396", "authors": ["Shovan Roy"], "title": "AgenticCyber: A GenAI-Powered Multi-Agent System for Multimodal Threat Detection and Adaptive Response in Cybersecurity", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages for IEEE conference", "summary": "The increasing complexity of cyber threats in distributed environments demands advanced frameworks for real-time detection and response across multimodal data streams. This paper introduces AgenticCyber, a generative AI powered multi-agent system that orchestrates specialized agents to monitor cloud logs, surveillance videos, and environmental audio concurrently. The solution achieves 96.2% F1-score in threat detection, reduces response latency to 420 ms, and enables adaptive security posture management using multimodal language models like Google's Gemini coupled with LangChain for agent orchestration. Benchmark datasets, such as AWS CloudTrail logs, UCF-Crime video frames, and UrbanSound8K audio clips, show greater performance over standard intrusion detection systems, reducing mean time to respond (MTTR) by 65% and improving situational awareness. This work introduces a scalable, modular proactive cybersecurity architecture for enterprise networks and IoT ecosystems that overcomes siloed security technologies with cross-modal reasoning and automated remediation.", "AI": {"tldr": "AgenticCyber是一个基于生成式AI的多智能体系统，用于多模态威胁检测和自适应响应，在网络安全领域实现实时监控和自动化修复。", "motivation": "分布式环境中网络威胁日益复杂，需要能够跨多模态数据流进行实时检测和响应的先进框架，以克服传统安全技术的孤岛问题。", "method": "采用生成式AI驱动的多智能体系统，协调专门智能体监控云日志、监控视频和环境音频，使用Google Gemini等多模态语言模型和LangChain进行智能体编排。", "result": "在威胁检测中达到96.2%的F1分数，响应延迟降至420毫秒，平均响应时间减少65%，在AWS CloudTrail日志、UCF-Crime视频帧和UrbanSound8K音频片段等基准数据集上表现优于标准入侵检测系统。", "conclusion": "该工作提出了一个可扩展、模块化的主动网络安全架构，适用于企业网络和物联网生态系统，通过跨模态推理和自动化修复克服了孤立的安全技术问题。"}}
{"id": "2512.06393", "pdf": "https://arxiv.org/pdf/2512.06393", "abs": "https://arxiv.org/abs/2512.06393", "authors": ["Qiming Bao", "Xiaoxuan Fu"], "title": "Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "comment": null, "summary": "Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations. Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.", "AI": {"tldr": "探究大语言模型在逻辑推理中对规则删除、矛盾证据注入、逻辑等价重写等结构化扰动下的泛化能力", "motivation": "虽然大语言模型在许多自然语言任务上表现出色，但它们在逻辑上下文的结构化扰动下的泛化能力仍不清楚。需要建立一个受控评估框架来测试推理可靠性", "method": "引入包含四种针对性压力测试的评估框架：1) 规则删除（冗余或必要规则）；2) 矛盾证据注入；3) 逻辑等价重写（使用六种等价定律）；4) 多定律等价堆叠。在BERT、Qwen2和LLaMA类模型上进行实验", "result": "所有模型在基础任务上达到完美准确率，对冗余规则删除和所有等价重写（单定律或多定律）完全泛化，但在必要规则删除下准确率降至25%，在明确矛盾存在时完全崩溃（0%准确率）", "conclusion": "大语言模型对语义保持的逻辑变换具有稳定的不变性，但对缺失或冲突证据仍存在根本性脆弱性。该框架为隔离此类推理失败模式提供了清晰的诊断工具，突显了当前LLM在逻辑泛化能力上的持续差距"}}
{"id": "2512.06392", "pdf": "https://arxiv.org/pdf/2512.06392", "abs": "https://arxiv.org/abs/2512.06392", "authors": ["Runlong Zhou", "Lefan Zhang", "Shang-Chen Wu", "Kelvin Zou", "Hanzhi Zhou", "Ke Ye", "Yihao Feng", "Dong Yin", "Alex Guillen Garcia", "Dmytro Babych", "Rohit Chatterjee", "Matthew Hopkins", "Xiang Kong", "Chang Lan", "Lezhi Li", "Yiping Ma", "Daniele Molinari", "Senyu Tong", "Yanchao Sun", "Thomas Voice", "Jianyu Wang", "Chong Wang", "Simon Wang", "Floris Weers", "Yechen Xu", "et al. (7 additional authors not shown)"], "title": "RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.", "AI": {"tldr": "RLAX是一个在TPU上运行的大规模分布式强化学习框架，专门用于提升大语言模型的推理能力，采用参数服务器架构实现高效训练。", "motivation": "强化学习已成为提升大语言模型推理能力的标准范式，但现有方法在可扩展性、训练效率和容错性方面存在不足，需要开发能够在大规模TPU集群上高效运行的强化学习框架。", "method": "采用参数服务器架构，主训练器定期推送更新的模型权重到参数服务器，推理工作节点拉取最新权重生成新的rollouts；引入系统技术实现可扩展和可抢占的RL训练；开发新的数据集整理和对齐技术加速收敛。", "result": "大规模评估显示，RLAX在1024个v5p TPU上仅用12小时48分钟就将QwQ-32B模型的pass@8准确率提升了12.8%，同时在训练过程中对抢占保持鲁棒性。", "conclusion": "RLAX是一个高效、可扩展的强化学习框架，能够在大规模TPU集群上快速提升大语言模型的性能，为大规模RL训练提供了实用的系统解决方案。"}}
{"id": "2512.06390", "pdf": "https://arxiv.org/pdf/2512.06390", "abs": "https://arxiv.org/abs/2512.06390", "authors": ["Mehrab Hosain", "Sabbir Alom Shuvo", "Matthew Ogbe", "Md Shah Jalal Mazumder", "Yead Rahman", "Md Azizul Hakim", "Anukul Pandey"], "title": "Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.NI", "cs.PF"], "comment": "Accepted at 2025 IEEE Asia Pacific Conference on Wireless and Mobile (APWiMob). 7 pages, 5 figures", "summary": "The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.", "AI": {"tldr": "该论文对AI时代下基于CDN增强的Web技术安全防御进行了系统性调查，重点分析了边缘计算环境中部署的AI增强安全机制。", "motivation": "现代Web技术栈面临AI辅助的自动化攻击持续演进，而CDN和边缘计算将可编程防御部署在最接近用户和机器人的位置，使其成为机器学习驱动检测、限流和隔离的自然执行点。", "method": "采用系统性调查方法，包括威胁分类映射到边缘可观测信号、评估指标、部署手册和治理指南，全面分析边缘部署的AI增强防御技术。", "result": "研究发现边缘中心的AI技术显著改善了检测和缓解时间，减少了数据移动并增强了合规性，但也引入了模型滥用、投毒和治理方面的新风险。", "conclusion": "边缘AI防御在提升Web安全方面具有显著优势，但需要关注可解释AI、对抗鲁棒性和自主多智能体防御等研究方向，以应对新出现的风险挑战。"}}
{"id": "2512.06387", "pdf": "https://arxiv.org/pdf/2512.06387", "abs": "https://arxiv.org/abs/2512.06387", "authors": ["Yuhang Huang", "Junchao Li", "Boyang Ma", "Xuelong Dai", "Minghui Xu", "Kaidi Xu", "Yue Zhang", "Jianping Wang", "Xiuzhen Cheng"], "title": "Beyond Model Jailbreak: Systematic Dissection of the \"Ten DeadlySins\" in Embodied Intelligence", "categories": ["cs.CR", "cs.RO"], "comment": null, "summary": "Embodied AI systems integrate language models with real world sensing, mobility, and cloud connected mobile apps. Yet while model jailbreaks have drawn significant attention, the broader system stack of embodied intelligence remains largely unexplored. In this work, we conduct the first holistic security analysis of the Unitree Go2 platform and uncover ten cross layer vulnerabilities the \"Ten Sins of Embodied AI Security.\" Using BLE sniffing, traffic interception, APK reverse engineering, cloud API testing, and hardware probing, we identify systemic weaknesses across three architectural layers: wireless provisioning, core modules, and external interfaces. These include hard coded keys, predictable handshake tokens, WiFi credential leakage, missing TLS validation, static SSH password, multilingual safety bypass behavior, insecure local relay channels, weak binding logic, and unrestricted firmware access. Together, they allow adversaries to hijack devices, inject arbitrary commands, extract sensitive information, or gain full physical control.Our findings show that securing embodied AI requires far more than aligning the model itself. We conclude with system level lessons learned and recommendations for building embodied platforms that remain robust across their entire software hardware ecosystem.", "AI": {"tldr": "对Unitree Go2平台进行首个全面的安全分析，揭示了跨层漏洞的\"十大罪状\"，展示了保护具身AI需要超越模型本身的对齐", "motivation": "虽然模型越狱已引起广泛关注，但具身智能的整个系统堆栈安全仍未被充分探索。具身AI系统整合了语言模型、现实世界感知、移动性和云端连接应用，需要对其整个软硬件生态系统进行安全分析", "method": "使用BLE嗅探、流量拦截、APK逆向工程、云API测试和硬件探测等技术，对Unitree Go2平台进行全面的安全分析，涵盖三个架构层：无线配置层、核心模块层和外部接口层", "result": "发现了十个跨层漏洞（\"具身AI安全十大罪状\"），包括硬编码密钥、可预测的握手令牌、WiFi凭据泄露、缺失TLS验证、静态SSH密码、多语言安全绕过行为、不安全的本地中继通道、弱绑定逻辑和无限制固件访问。这些漏洞允许攻击者劫持设备、注入任意命令、提取敏感信息或获得完全物理控制", "conclusion": "保护具身AI需要远远超越模型本身的对齐。需要在整个软件硬件生态系统中建立健壮的安全机制，提出了系统级的经验教训和建议"}}
{"id": "2512.06383", "pdf": "https://arxiv.org/pdf/2512.06383", "abs": "https://arxiv.org/abs/2512.06383", "authors": ["Tesshu Hanaka", "Yuto Okada", "Yota Otachi", "Lena Volk"], "title": "Finding a Maximum Common (Induced) Subgraph: Structural Parameters Revisited", "categories": ["cs.DS"], "comment": "16 pages, 1 figure, WALCOM 2026", "summary": "We study the parameterized complexity of the problems of finding a maximum common (induced) subgraph of two given graphs. Since these problems generalize several NP-complete problems, they are intractable even when parameterized by strongly restricted structural parameters. Our contribution in this paper is to sharply complement the hardness of the problems by showing fixed-parameter tractable cases: both induced and non-induced problems parameterized by max-leaf number and by neighborhood diversity, and the induced problem parameterized by twin cover number. These results almost completely determine the complexity of the problems with respect to well-studied structural parameters. Also, the result on the twin cover number presents a rather rare example where the induced and non-induced cases have different complexity.", "AI": {"tldr": "研究在给定两个图中寻找最大公共（诱导）子图的参数化复杂度问题", "motivation": "最大公共（诱导）子图问题推广了多个NP完全问题，即使在强限制的结构参数下也是难解的，需要补充展示固定参数可处理的情况", "method": "通过max-leaf number、neighborhood diversity和twin cover number等结构参数来分析问题的参数化复杂度", "result": "证明了诱导和非诱导问题在max-leaf number和neighborhood diversity参数下都是固定参数可处理的，而诱导问题在twin cover number参数下也是固定参数可处理的", "conclusion": "这些结果几乎完全确定了问题相对于常见结构参数的复杂度，并展示了诱导和非诱导情况具有不同复杂度的罕见例子"}}
{"id": "2512.06380", "pdf": "https://arxiv.org/pdf/2512.06380", "abs": "https://arxiv.org/abs/2512.06380", "authors": ["Xiao Zhan", "Guangzhi Sun", "Jose Such", "Phil Woodland"], "title": "Protecting Bystander Privacy via Selective Hearing in LALMs", "categories": ["cs.SD", "cs.AI"], "comment": "Dataset: https://huggingface.co/datasets/BrianatCambridge/SelectiveHearingBench", "summary": "Large audio language models (LALMs) are increasingly deployed in real-world settings where they inevitably capture speech from unintended nearby bystanders, raising privacy risks that existing benchmarks and defences largely overlook. We introduce SH-Bench, the first benchmark designed to evaluate selective hearing: a model's ability to attend to an intended main speaker while refusing to process or reveal information about incidental bystander speech. SH-Bench contains 3,968 multi-speaker audio mixtures spanning both real-world and synthetic scenarios, paired with 77k multiple-choice questions that probe models under general and selective operating modes. We propose Selective Efficacy (SE), a unified metric capturing both multi-speaker comprehension and bystander-privacy protection. Our evaluation of state-of-the-art open-source and proprietary LALMs reveals substantial privacy leakage, with strong audio understanding failing to translate into selective protection of bystander privacy. To mitigate this gap, we introduce Bystander Privacy Fine-Tuning (BPFT), a training pipeline that teaches models to refuse bystander-related queries without degrading main-speaker comprehension. BPFT yields substantial gains which improve SE by up to 15.9% over Gemini 2.5 Pro, demonstrating that selective hearing is learnable but far from achieved in current LALMs. SH-Bench and BPFT provide the first systematic framework for measuring and improving bystander privacy in audio foundation models.", "AI": {"tldr": "该论文提出了SH-Bench基准测试和Bystander Privacy Fine-Tuning (BPFT)方法，用于评估和改进大型音频语言模型在保护旁听者隐私方面的选择性听觉能力。", "motivation": "大型音频语言模型在实际部署中不可避免地会捕获到非目标旁听者的语音，现有基准测试和防御机制大多忽视了这种隐私风险，需要系统性的评估和改进方法。", "method": "1) 创建SH-Bench基准测试，包含3,968个多说话者音频混合和77k多项选择题；2) 提出Selective Efficacy (SE)统一指标；3) 开发Bystander Privacy Fine-Tuning (BPFT)训练流程，教导模型拒绝回答旁听者相关查询而不影响主要说话者理解。", "result": "评估显示当前最先进的LALMs存在显著的隐私泄露问题，音频理解能力强但旁听者隐私保护能力差。BPFT方法使SE指标相比Gemini 2.5 Pro提高了15.9%，证明选择性听觉是可学习的但当前模型尚未实现。", "conclusion": "SH-Bench和BPFT为音频基础模型中的旁听者隐私保护提供了首个系统性评估和改进框架，揭示了当前模型在选择性听觉方面的不足，并展示了通过专门训练可以显著改善隐私保护能力。"}}
{"id": "2512.06379", "pdf": "https://arxiv.org/pdf/2512.06379", "abs": "https://arxiv.org/abs/2512.06379", "authors": ["Yi Huo", "Lei Zhang"], "title": "OCFER-Net: Recognizing Facial Expression in Online Learning System", "categories": ["cs.CV"], "comment": null, "summary": "Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.", "AI": {"tldr": "提出OCFER-Net网络，通过正交约束改进卷积核，提升在线学习系统中面部表情识别的准确率", "motivation": "在线学习日益普及，但缺乏有效的情感交互机制。面部表情识别(FER)可以帮助教师了解学生情绪状态，但现有方法很少利用卷积矩阵的正交性来提升特征表达能力", "method": "提出OCFER-Net网络，通过正则化器对卷积核施加正交约束，提取更具多样性和表达力的特征", "result": "在具有挑战性的FER-2013数据集上实验，性能优于基线方法1.087个点", "conclusion": "正交约束能有效提升卷积神经网络在面部表情识别任务中的性能，为在线学习系统提供更准确的情感分析工具"}}
{"id": "2512.06377", "pdf": "https://arxiv.org/pdf/2512.06377", "abs": "https://arxiv.org/abs/2512.06377", "authors": ["Yi Huo", "Yun Ge"], "title": "VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System", "categories": ["cs.CV"], "comment": null, "summary": "Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .", "AI": {"tldr": "该论文提出VAD-Net，用于智能教育系统中的多维面部表情识别，通过在FER2013数据集上标注VAD（效价-唤醒度-支配度）三维情感参数，并引入正交卷积改进网络性能。", "motivation": "当前面部表情识别数据集大多基于离散情感类别（如高兴、愤怒等），表达能力有限。未来情感计算需要更全面精确的情感度量，VAD多维参数能提供更全面的情感描述。现有数据集如AffectNet只包含VA（效价和唤醒度）信息，缺乏D（支配度）维度。", "method": "1. 在FER2013数据集上首创性地标注D（支配度）维度，构建VAD标注数据集；2. 提出基于ResNet的正交化回归网络，通过正交卷积提取更多样化和表达性强的特征，提高预测精度。", "result": "实验结果表明：1. D维度可以被测量，但相比V和A维度更难获取（无论是人工标注还是网络预测）；2. 通过引入正交卷积的消融实验验证，正交卷积配置能获得更好的VAD预测效果。", "conclusion": "该研究为FER数据集提供了D维度的首创性标注，并通过正交卷积提出了更好的VAD预测网络。新建的VAD标注FER2013数据集可作为衡量VAD多维情感的基准，基于ResNet的正交化回归网络可作为VAD情感预测的面部表情识别基线。"}}
{"id": "2512.06376", "pdf": "https://arxiv.org/pdf/2512.06376", "abs": "https://arxiv.org/abs/2512.06376", "authors": ["Xinhao Xiang", "Abhijeet Rastogi", "Jiawei Zhang"], "title": "Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework", "categories": ["cs.CV"], "comment": null, "summary": "Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.", "AI": {"tldr": "本文提出了一个诊断评估框架，用于评估AI生成的驾驶视频（AIGV）在自动驾驶训练和评估中的可靠性，包括构建ADGV-Bench基准测试和开发ADGVE评估器。", "motivation": "文本到视频模型能够生成高分辨率驾驶场景，为自动驾驶提供了低成本、可扩展的数据替代方案，但这些AI生成的驾驶视频是否能够可靠地支持自动驾驶模型的训练和评估仍是一个关键问题。", "method": "1. 提出了AIGV常见故障模式的分类法；2. 构建了ADGV-Bench驾驶基准测试，包含人工质量标注和多个感知任务的密集标签；3. 开发了ADGVE评估器，结合静态语义、时序线索、车道遵守信号和视觉语言模型引导的推理，为每个视频片段生成单一质量评分。", "result": "实验表明，盲目添加原始AIGV会降低感知性能，而使用ADGVE过滤AIGV能持续改善视频质量评估指标和下游自动驾驶模型，使AIGV成为真实世界数据的有益补充。", "conclusion": "研究揭示了AIGV在自动驾驶应用中的风险和潜力，并提供了在实际自动驾驶流程中安全利用大规模视频生成的实用工具，强调了适当过滤的重要性。"}}
{"id": "2512.06373", "pdf": "https://arxiv.org/pdf/2512.06373", "abs": "https://arxiv.org/abs/2512.06373", "authors": ["Yuji Wang", "Wenlong Liu", "Jingxuan Niu", "Haoji Zhang", "Yansong Tang"], "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning", "categories": ["cs.CV"], "comment": "The project page is [this url](https://github.com/VoyageWang/VG-Refiner)", "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.", "AI": {"tldr": "提出VG-Refiner框架，专注于工具精炼的指代接地推理，通过智能强化学习解决现有工具集成视觉推理中处理不可靠工具输出的问题", "motivation": "现有工具集成视觉推理(TiVR)范式主要关注通过强化学习集成各种视觉工具，但缺乏处理不可靠或错误工具输出的有效响应机制，特别是在指代和接地任务中，不准确的检测工具预测常常误导TiVR模型产生幻觉推理", "method": "提出VG-Refiner框架，采用两阶段\"思考-再思考\"机制，使模型能够显式分析和响应工具反馈；引入精炼奖励机制，鼓励对不良工具结果进行有效修正；使用少量任务特定数据增强精炼能力", "result": "在指代和推理接地基准测试中实现了准确性和修正能力的显著提升，同时保持了预训练模型的通用能力；提出了两个新指标并建立了公平评估协议来系统测量模型的精炼能力", "conclusion": "VG-Refiner是首个专注于工具精炼指代接地推理的框架，通过智能强化学习有效解决了工具输出不可靠的问题，为工具集成视觉推理领域提供了新的解决方案"}}
{"id": "2512.06368", "pdf": "https://arxiv.org/pdf/2512.06368", "abs": "https://arxiv.org/abs/2512.06368", "authors": ["Weitao Xiong", "Zhiyuan Yuan", "Jiahao Lu", "Chengfeng Zhao", "Peng Li", "Yuan Liu"], "title": "Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.", "AI": {"tldr": "提出Human3R方法，通过结合SMPL人体模型和单目深度估计的混合几何先验，从单目视频中实现更好的3D动态人体场景重建", "motivation": "单目动态视频重建在动态人体场景中面临几何不一致和分辨率退化问题，现有方法缺乏3D人体结构理解，导致几何不一致结果和人体边界漂移", "method": "提出Human3R方法，采用分层处理流程：先处理全分辨率图像获取整体场景几何，然后通过策略性裁剪和交叉注意力融合增强人体特定细节，通过特征融合模块集成SMPL先验", "result": "在TUM Dynamics和GTA-IM数据集上的实验表明，该方法在动态人体重建方面具有优越性能", "conclusion": "通过结合混合几何先验和分层处理流程，Human3R能够实现几何一致且细节丰富的动态人体场景重建"}}
{"id": "2512.06364", "pdf": "https://arxiv.org/pdf/2512.06364", "abs": "https://arxiv.org/abs/2512.06364", "authors": ["Shyama Sastha Krishnamoorthy Srinivasan", "Harsh Pala", "Mohan Kumar", "Pushpendra Singh"], "title": "JEEVHITAA -- An End-to-End HCAI System to Support Collective Care", "categories": ["cs.CR", "cs.ET", "cs.HC"], "comment": "14 pages, 4 figures, 6 tables, 2 algorithms, Preprint of work Submitted to MobiSys 2026 - Emergent Ideas Track", "summary": "Current mobile health platforms are predominantly individual-centric and lack the necessary primitives for coordinated, auditable, multi-actor workflows. However, in many settings worldwide, health decisions are enacted by multi-actor care networks rather than single users. We present JEEVHITAA, an Android/Flutter system that provides context-sensitive, role-aware sharing and verifiable information flows for care circles. JEEVHITAA ingests platform and device data (via Google Health Connect and BLE connectors), constructs multi-layer user profiles from sensor streams and tiered onboarding, and enforces fine-grained, time-bounded access control across permissioned care graphs. Data are end-to-end encrypted in local stores and during peer sync (Firebase), and provisions are made for document capture by camera or upload as PDF. An integrated retrieval-augmented LLM pipeline (i) produces structured, role-targeted summaries and action plans, (ii) enables users to gather advanced insights on health reports, and (iii) performs evidence-grounded user-relevant verification of arbitrary health content, returning provenance, confidence scores, and source citations. We describe the system architecture, connector abstractions, and security primitives, and evaluate robustness and compatibility using synthetic, ontology-driven simulations and vendor compatibility tests. Finally, we outline plans for longitudinal in-the-wild deployments to measure system performance, the correctness of access control, and the real-world effectiveness of relationship-aware credibility support.", "AI": {"tldr": "JEEVHITAA是一个端到端的HCAI系统，旨在支持集体护理，通过多角色协作、可验证的信息流和基于角色的访问控制来协调医疗护理网络。", "motivation": "当前移动健康平台主要面向个人，缺乏协调、可审计的多参与者工作流程支持，而全球许多医疗决策实际上由多参与者护理网络共同制定。", "method": "开发Android/Flutter系统，集成Google Health Connect和BLE连接器收集数据，构建多层用户档案，实施细粒度时间限制访问控制，使用端到端加密，并集成检索增强的LLM管道生成结构化摘要、行动计划和内容验证。", "result": "系统通过本体驱动的模拟和供应商兼容性测试验证了鲁棒性和兼容性，展示了角色感知共享、可验证信息流和基于证据的内容验证功能。", "conclusion": "JEEVHITAA为集体护理提供了有效的技术支持框架，计划通过实际部署进一步评估系统性能、访问控制正确性和关系感知可信度支持的实际效果。"}}
{"id": "2512.06363", "pdf": "https://arxiv.org/pdf/2512.06363", "abs": "https://arxiv.org/abs/2512.06363", "authors": ["Jiabao Guo", "Yadian Wang", "Hui Ma", "Yuhao Fu", "Ju Jia", "Hui Liu", "Shengeng Tang", "Lechao Cheng", "Yunfeng Diao", "Ajian Liu"], "title": "Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.", "AI": {"tldr": "提出一种欺骗感知提示学习框架(SPL-UAD)，用于统一物理-数字面部攻击检测，通过解耦优化分支解决现有方法中物理和数字攻击检测之间的冲突问题。", "motivation": "现实世界人脸识别系统同时面临物理呈现攻击和数字伪造攻击的威胁，现有基于CLIP的方法在统一检测中存在优化方向冲突的问题，需要一种能够独立处理两种攻击类型的解决方案。", "method": "提出SPL-UAD框架：1)构建可学习的并行提示分支，通过自适应欺骗上下文提示生成实现物理和数字攻击的独立优化控制；2)设计线索感知增强，利用双提示机制生成具有挑战性的样本挖掘任务，增强模型对未见攻击类型的鲁棒性。", "result": "在大型UniAttackDataPlus数据集上的广泛实验表明，该方法在统一攻击检测任务中取得了显著的性能提升。", "conclusion": "提出的SPL-UAD框架通过解耦物理和数字攻击的优化分支，有效解决了现有方法中的冲突问题，实现了更全面的生物特征保护，在统一攻击检测任务中表现出优越性能。"}}
{"id": "2512.06358", "pdf": "https://arxiv.org/pdf/2512.06358", "abs": "https://arxiv.org/abs/2512.06358", "authors": ["Mingjia Li", "Jin Hu", "Hainuo Wang", "Qiming Hu", "Jiarui Wang", "Xiaojie Guo"], "title": "Rectifying Latent Space for Generative Single-Image Reflection Removal", "categories": ["cs.CV"], "comment": null, "summary": "Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.", "AI": {"tldr": "本文提出了一种基于潜在扩散模型的单图像反射去除方法，通过重构潜在空间来解决反射去除问题", "motivation": "现有的单图像反射去除方法在处理高度模糊的分层图像输入时表现不佳，难以推理被破坏区域的组成，导致在真实场景中恢复和泛化能力不足。问题的核心在于语义编码器的潜在空间缺乏将复合图像解释为其组成层线性叠加的内在结构。", "method": "方法包含三个协同组件：1）反射等变VAE，将潜在空间与反射形成的线性物理模型对齐；2）可学习的任务特定文本嵌入，提供精确指导并绕过模糊语言；3）深度引导的早期分支采样策略，利用生成随机性获得有希望的结果。", "result": "在多个基准测试中取得了新的SOTA性能，并且在具有挑战性的真实世界案例中表现出良好的泛化能力。", "conclusion": "通过重构潜在扩散模型的潜在空间，使其能够有效感知和处理高度模糊的分层图像输入，从而实现了高质量的单图像反射去除，解决了现有方法在推理复合图像组成方面的局限性。"}}
{"id": "2512.06357", "pdf": "https://arxiv.org/pdf/2512.06357", "abs": "https://arxiv.org/abs/2512.06357", "authors": ["Tony Sallooma", "Okyay Kaynak", "Xinbo Yub", "Wei He"], "title": "Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Engineering Applications of Artificial Intelligence 2022", "summary": "Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.", "AI": {"tldr": "本文提出了一种基于PID控制思想的神经网络时间序列预测增强方法，用于提高周期性时间序列多步预测的准确性，同时保持系统复杂度基本不变。", "motivation": "神经网络在时间序列预测中应用广泛，但其结构复杂性会影响预测精度。需要一种方法在保持系统复杂度基本不变的前提下，提高神经网络模型的预测性能。", "method": "提出基于比例-积分-微分（PID）控制思想的增强方法，在每个时间步对神经网络的预测值进行PID校正，使其更接近真实值。该方法作为预测后的增强模块，不影响原始神经网络结构。", "result": "将PID增强方法应用于两个深度神经网络模型进行需水量预测，并扩展到小时能耗预测。实验结果表明，该方法显著提高了预测精度，同时系统复杂度基本保持不变。", "conclusion": "PID增强方法能有效提高神经网络在周期性时间序列多步预测中的准确性，且对系统复杂度影响极小，具有广泛的适用性。"}}
{"id": "2512.06354", "pdf": "https://arxiv.org/pdf/2512.06354", "abs": "https://arxiv.org/abs/2512.06354", "authors": ["Niclas Flehmig", "Mary Ann Lundteigen", "Shen Yin"], "title": "The Missing Variable: Socio-Technical Alignment in Risk Evaluation", "categories": ["cs.CY", "cs.HC", "eess.SY"], "comment": null, "summary": "This paper addresses a critical gap in the risk assessment of AI-enabled safety-critical systems. While these systems, where AI systems assists human operators, function as complex socio-technical systems, existing risk evaluation methods fail to account for the associated complex interaction between human, technical, and organizational elements. Through a comparative analysis of system attributes from both socio-technical and AI-enabled systems and a review of current risk evaluation methods, we confirm the absence of socio-technical considerations in standard risk expressions. To bridge this gap, we introduce a novel socio-technical alignment $STA$ variable designed to be integrated into the foundational risk equation. This variable estimates the degree of harmonious interaction between the AI systems, human operators, and organizational processes. A case study on an AI-enabled liquid hydrogen bunkering system demonstrates the variable's relevance. By comparing a naive and a safeguarded system design, we illustrate how the $STA$-augmented expression captures socio-technical safety implications that traditional risk evaluation overlooks, providing a more holistic basis for risk evaluation.", "AI": {"tldr": "该论文提出在AI赋能安全关键系统的风险评估中引入社会技术对齐变量，以弥补传统方法忽视人、技术、组织复杂交互的缺陷。", "motivation": "当前AI赋能安全关键系统（AI辅助人类操作员）作为复杂的社会技术系统运行，但现有风险评估方法未能考虑人、技术、组织元素之间的复杂交互，存在关键缺陷。", "method": "通过社会技术系统与AI赋能系统的属性比较分析，以及对现有风险评估方法的审查，确认标准风险表达式中缺乏社会技术考量，进而提出社会技术对齐变量STA，并将其整合到基础风险方程中。", "result": "在AI赋能液氢加注系统的案例研究中，通过比较朴素设计和安全防护设计，展示了STA增强表达式能够捕捉传统风险评估忽视的社会技术安全影响，提供更全面的风险评估基础。", "conclusion": "社会技术对齐变量STA填补了AI赋能安全关键系统风险评估的关键空白，能够更全面地评估系统风险，为更有效的安全设计提供理论基础。"}}
{"id": "2512.06353", "pdf": "https://arxiv.org/pdf/2512.06353", "abs": "https://arxiv.org/abs/2512.06353", "authors": ["Kaicheng Yang", "Kaisen Yang", "Baiting Wu", "Xun Zhang", "Qianrui Yang", "Haotong Qin", "He Zhang", "Yulun Zhang"], "title": "TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search", "categories": ["cs.CV"], "comment": "Code and Supplementary Material could be found at https://github.com/racoonykc/TreeQ", "summary": "Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ", "AI": {"tldr": "提出TreeQ框架，通过树结构混合精度搜索推动扩散Transformer的量化边界，解决DiT模型部署中的计算和内存挑战", "motivation": "扩散Transformer在图像生成中表现出色，但实际部署面临高计算和内存需求。混合精度量化在U-Net上取得成功，但在DiT架构中的应用仍有限且未充分探索", "method": "提出三个关键技术：1) 树结构搜索利用DiT线性特性在O(n)时间内遍历解空间；2) 环境噪声指导统一PTQ和QAT优化目标；3) 通用Monarch分支防止超低位量化中的信息瓶颈", "result": "在DiT-XL/2模型的W3A3和W4A4 PTQ/PEFT设置下达到最先进性能，首次在DiT模型上实现接近无损的4位PTQ性能", "conclusion": "TreeQ框架成功解决了DiT量化的关键挑战，通过创新的树结构搜索、统一优化目标和信息保护机制，显著推进了扩散Transformer的量化边界"}}
{"id": "2512.06350", "pdf": "https://arxiv.org/pdf/2512.06350", "abs": "https://arxiv.org/abs/2512.06350", "authors": ["Nghi Truong", "Phanish Puranam", "Özgecan Koçak"], "title": "Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the \"doomer\" and \"boomer\" perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk (\"X-risk\") arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks (\"E-risks\") pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.", "AI": {"tldr": "分析Lex Fridman播客中关于AI风险的不同观点，解码\"末日论者\"和\"繁荣论者\"之间的分歧根源", "motivation": "尽管各方都希望AI造福人类并避免灾难性后果，但关于AI风险的辩论仍存在深刻分歧。本文旨在解析这些分歧的根源，理解为什么在共同目标下仍存在不同观点。", "method": "将\"末日论者\"和\"繁荣论者\"的观点差异分解为定义性、事实性、因果性和道德前提，使用LLM集成方法大规模分析推理链，解析文本数据", "result": "发现关于存在性风险（X-risk）的分歧主要源于对复杂系统中设计与涌现的因果前提不同；关于就业风险（E-risks）的分歧则源于对过去理论（进化）适用性的因果前提不同。两种分歧都不涉及重大道德价值观差异，都可以用人类理性有限性的不同观点来描述", "conclusion": "提出的分析推理链方法可以应用于任何领域的公共风险辩论，识别关键争议点。AI风险辩论中的分歧更多是认知框架和因果假设的差异，而非根本价值观冲突"}}
{"id": "2512.06345", "pdf": "https://arxiv.org/pdf/2512.06345", "abs": "https://arxiv.org/abs/2512.06345", "authors": ["Xiangshuai Song", "Jun-Jie Huang", "Tianrui Liu", "Ke Liang", "Chang Tang"], "title": "CLUENet: Cluster Attention Makes Neural Networks Have Eyes", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, 2026 Association for the Advancement of Artificial Intelligence", "summary": "Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.", "AI": {"tldr": "提出CLUENet（CLUster attEntion Network），一种用于视觉语义理解的透明深度架构，通过聚类注意力机制使神经网络具备\"眼睛\"般的视觉能力", "motivation": "尽管基于卷积和注意力的模型在视觉任务中取得成功，但其刚性感受野和复杂架构限制了建模不规则空间模式的能力，并阻碍了可解释性，对需要高模型透明度的任务构成挑战。聚类范式提供了有前景的可解释性和灵活的语义建模，但存在精度有限、效率低和训练中梯度消失的问题", "method": "提出三个关键创新：(1) 全局软聚合和硬分配，采用温度缩放余弦注意力和门控残差连接增强局部建模；(2) 块间硬共享特征调度；(3) 改进的聚类池化策略", "result": "在CIFAR-100和Mini-ImageNet上的实验表明，CLUENet超越了现有的聚类方法和主流视觉模型，在准确性、效率和透明度之间提供了有吸引力的平衡", "conclusion": "CLUENet通过聚类注意力机制解决了现有视觉模型的局限性，提供了一种既保持高性能又具有良好可解释性的透明深度架构，为视觉语义理解任务提供了新的解决方案"}}
{"id": "2512.06344", "pdf": "https://arxiv.org/pdf/2512.06344", "abs": "https://arxiv.org/abs/2512.06344", "authors": ["Kaile Wang", "Lijun He", "Haisheng Fu", "Haixia Bi", "Fan Li"], "title": "Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate", "categories": ["cs.CV"], "comment": null, "summary": "Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.", "AI": {"tldr": "提出一种多模态引导的任务感知生成式图像压缩框架（MTGC），用于解决超低比特率下生成式图像压缩的语义偏差问题", "motivation": "生成式图像压缩在超低比特率（bpp < 0.05）下常因生成幻觉导致语义偏差，限制了其在6G语义通信中的可靠部署，需要增强语义一致性", "method": "MTGC框架整合三种引导模态：简洁鲁棒的文本描述（全局语义）、高度压缩图像（低层视觉信息）、语义伪词（细粒度任务相关语义）。通过任务感知语义压缩模块（TASCM）生成SPWs，采用双路径协同引导机制的多模态引导扩散解码器（MGDD）将三种引导注入扩散过程", "result": "在DIV2K数据集上，DISTS指标下降10.59%，在超低比特率下显著提升语义一致性、感知质量和像素级保真度", "conclusion": "MTGC框架通过多模态引导和任务感知设计，有效解决了超低比特率下生成式图像压缩的语义偏差问题，为6G语义通信中的可靠图像压缩提供了解决方案"}}
{"id": "2512.06343", "pdf": "https://arxiv.org/pdf/2512.06343", "abs": "https://arxiv.org/abs/2512.06343", "authors": ["Tong Xie", "Andrew Bai", "Yuanhao Ban", "Yunqi Hong", "Haoyu Li", "Cho-jui Hsieh"], "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.", "AI": {"tldr": "分析Bradley-Terry损失函数中的表示距离偏差问题，并提出NormBT方法进行修正", "motivation": "发现BT损失函数中梯度范数不仅取决于预测误差，还受到表示距离的影响，导致小距离对的梯度消失而大距离对的梯度过强，影响奖励模型的学习效果", "method": "提出NormBT方法，通过自适应成对归一化方案平衡表示距离效应，使学习信号聚焦于预测误差", "result": "在多种LLM骨干网络和数据集上，NormBT一致提升奖励模型性能，在RewardBench的推理类别上获得超过5%的显著提升", "conclusion": "揭示了广泛使用的BT目标函数的关键局限性，并提供了简单有效的修正方案，有助于改进LLM对齐中的奖励建模"}}
{"id": "2512.06337", "pdf": "https://arxiv.org/pdf/2512.06337", "abs": "https://arxiv.org/abs/2512.06337", "authors": ["Xuan Xie", "Xuan Wang", "Wenjie Wang"], "title": "DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization", "categories": ["cs.AI"], "comment": null, "summary": "The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.", "AI": {"tldr": "提出DaGRPO方法，通过解决GRPO中的梯度冲突问题来提升大语言模型的长链推理能力训练效果", "motivation": "GRPO方法在激发大语言模型的后训练推理能力方面表现出色，但存在训练不稳定和样本效率低的问题。研究发现根本原因在于on-policy rollout中缺乏区分度：常规查询中高度同质化的样本导致破坏性梯度冲突，而困难查询中有效正样本稀缺导致优化无效。", "method": "DaGRPO包含两个核心机制：1) 序列级梯度矫正 - 使用细粒度评分动态屏蔽低区分度的样本对，从源头上消除梯度冲突；2) 离策略数据增强 - 引入高质量锚点来恢复困难任务的训练信号。", "result": "在9个数学推理和分布外泛化基准测试中，DaGRPO显著超越现有的SFT、GRPO和混合基线，实现了新的最先进性能（例如在数学基准上平均准确率提升+4.7%）。深入分析证实DaGRPO有效缓解了梯度爆炸并加速了长链推理能力的出现。", "conclusion": "DaGRPO通过解决GRPO中的梯度冲突问题，显著提升了训练稳定性和样本效率，为大语言模型的长链推理能力训练提供了有效的解决方案。"}}
{"id": "2512.06332", "pdf": "https://arxiv.org/pdf/2512.06332", "abs": "https://arxiv.org/abs/2512.06332", "authors": ["Jeffrey Gu", "Minkyu Jeon", "Ambri Ma", "Serena Yeung-Levy", "Ellen D. Zhong"], "title": "CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks", "categories": ["cs.CV"], "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.", "AI": {"tldr": "提出CryoHype方法，使用基于Transformer的超网络重建数千个冷冻电镜结构，解决多分子物种混合的组成异质性挑战", "motivation": "冷冻电镜通常用于单一分子物种成像，但具有同时确定多个目标结构的潜力。现有方法主要关注单个或少数结构的构象异质性，无法解决由许多不同分子物种混合产生的组成异质性", "method": "提出CryoHype，一种基于Transformer的超网络，动态调整隐式神经表示的权重，用于冷冻电镜重建", "result": "在包含100个结构的挑战性基准数据集上取得最先进结果，并证明CryoHype能够扩展到从无标记冷冻电镜图像中重建1000个不同结构（固定姿态设置）", "conclusion": "CryoHype成功解决了冷冻电镜中多分子物种混合的组成异质性挑战，实现了大规模高通量结构重建"}}
{"id": "2512.06330", "pdf": "https://arxiv.org/pdf/2512.06330", "abs": "https://arxiv.org/abs/2512.06330", "authors": ["Haoyu Zhang", "Junhan Luo", "Yugang Cao", "Siran Peng", "Jie Huang", "Liangjian-Deng"], "title": "S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.", "AI": {"tldr": "提出S2WMamba模型，通过谱空间小波变换和Mamba架构实现全色锐化，在保持光谱保真度的同时注入空间细节", "motivation": "全色锐化中同时处理PAN和MS图像会导致空间细节与光谱保真度纠缠，现有方法难以有效解耦这两类信息", "method": "使用2D Haar DWT处理PAN图像定位空间边缘纹理，使用通道级1D Haar DWT处理每个像素的光谱分离高低频分量，构建谱分支和空间分支，通过Mamba跨调制进行信息交换，最后使用多尺度动态门融合分支输出", "result": "在WV3、GF2和QB数据集上匹配或超越现有基线方法（FusionMamba、CANNet、U2Net、ARConv），PSNR提升最高达0.23 dB，在WV3上HQNR达到0.956", "conclusion": "S2WMamba通过显式解耦频率信息并进行轻量级跨模态交互，有效解决了全色锐化中空间细节与光谱保真度的纠缠问题，在多个数据集上取得了优异性能"}}
{"id": "2512.06328", "pdf": "https://arxiv.org/pdf/2512.06328", "abs": "https://arxiv.org/abs/2512.06328", "authors": ["Jiahao Li", "Yusheng Luo", "Yunzhong Lou", "Xiangdong Zhou"], "title": "ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted as an Oral presentation at AAAI 2026", "summary": "We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.", "AI": {"tldr": "提出ReCAD框架，使用强化学习增强预训练大模型，从多模态输入生成精确的参数化CAD模型", "motivation": "现有方法通常依赖监督微调注入知识，对可编辑性支持有限，且未能充分利用预训练大模型的强大生成先验", "method": "1) 微调视觉语言模型获得基本CAD生成能力；2) 提出新颖的RL策略，以参数化代码为引导增强模型推理；3) 采用分层基元学习过程，在统一奖励函数下逐步教授结构化组合技能", "result": "在文本到CAD和图像到CAD任务中达到新的最先进水平，显著提高分布内和分布外设置的几何精度。例如在图像到CAD任务中，将平均Chamfer距离从73.47降至29.61（分布内），从272.06降至80.23（分布外）", "conclusion": "ReCAD框架通过强化学习增强预训练大模型，能够生成精确的参数化CAD模型，仅需简单功能接口即可实现复杂CAD操作，显著优于现有基线方法"}}
{"id": "2512.06306", "pdf": "https://arxiv.org/pdf/2512.06306", "abs": "https://arxiv.org/abs/2512.06306", "authors": ["Haoxian Zhou", "Chuanzhi Xu", "Langyi Chen", "Haodong Chen", "Yuk Ying Chung", "Qiang Qu", "Xaoming Chen", "Weidong Cai"], "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "AI": {"tldr": "提出一种基于点云框架的事件驱动人体姿态估计方法，通过利用事件流的时空特性来提高性能", "motivation": "现有方法将事件流转换为密集事件帧会带来额外计算开销并牺牲事件信号的高时间分辨率，需要充分利用事件流的时空特性", "method": "设计事件时间切片卷积模块捕获事件切片间的短期依赖，结合事件切片序列模块进行结构化时间建模，在点云表示中应用边缘增强来改善稀疏事件条件下的空间边缘信息", "result": "在DHP19数据集上的实验表明，该方法在三种代表性点云骨干网络（PointNet、DGCNN、Point Transformer）上均能持续提升性能", "conclusion": "通过有效利用事件流的时空特性，提出的方法能够在保持事件相机高时间分辨率优势的同时，提高人体姿态估计的性能"}}
{"id": "2512.06304", "pdf": "https://arxiv.org/pdf/2512.06304", "abs": "https://arxiv.org/abs/2512.06304", "authors": ["Xining Song", "Zhihua Wei", "Rui Wang", "Haixiao Hu", "Yanxiang Chen", "Meng Han"], "title": "Degrading Voice: A Comprehensive Overview of Robust Voice Conversion Through Input Manipulation", "categories": ["eess.AS", "cs.AI", "cs.CR", "cs.SD"], "comment": null, "summary": "Identity, accent, style, and emotions are essential components of human speech. Voice conversion (VC) techniques process the speech signals of two input speakers and other modalities of auxiliary information such as prompts and emotion tags. It changes para-linguistic features from one to another, while maintaining linguistic contents. Recently, VC models have made rapid advancements in both generation quality and personalization capabilities. These developments have attracted considerable attention for diverse applications, including privacy preservation, voice-print reproduction for the deceased, and dysarthric speech recovery. However, these models only learn non-robust features due to the clean training data. Subsequently, it results in unsatisfactory performances when dealing with degraded input speech in real-world scenarios, including additional noise, reverberation, adversarial attacks, or even minor perturbation. Hence, it demands robust deployments, especially in real-world settings. Although latest researches attempt to find potential attacks and countermeasures for VC systems, there remains a significant gap in the comprehensive understanding of how robust the VC model is under input manipulation. here also raises many questions: For instance, to what extent do different forms of input degradation attacks alter the expected output of VC models? Is there potential for optimizing these attack and defense strategies? To answer these questions, we classify existing attack and defense methods from the perspective of input manipulation and evaluate the impact of degraded input speech across four dimensions, including intelligibility, naturalness, timbre similarity, and subjective perception. Finally, we outline open issues and future directions.", "AI": {"tldr": "该论文全面综述了通过输入操作实现鲁棒语音转换的方法，分析了语音转换系统在面对输入语音降质时的脆弱性，并对攻击与防御策略进行了系统分类和评估。", "motivation": "尽管语音转换技术在生成质量和个性化能力方面取得了快速进展，但由于训练数据过于干净，这些模型只学习了非鲁棒特征，导致在现实场景中处理降质输入语音（如噪声、混响、对抗攻击等）时性能不佳。目前缺乏对语音转换模型在输入操作下鲁棒性的全面理解。", "method": "从输入操作的角度对现有攻击和防御方法进行分类，并在四个维度上评估降质输入语音的影响：可懂度、自然度、音色相似度和主观感知。", "result": "论文系统分析了不同形式的输入降质攻击如何改变语音转换模型的预期输出，探讨了优化攻击和防御策略的潜力，并揭示了语音转换系统在现实环境中的脆弱性。", "conclusion": "语音转换系统在面对输入操作时存在显著的鲁棒性问题，需要开发更有效的防御策略。论文最后概述了开放问题和未来研究方向，为构建更鲁棒的语音转换系统提供了理论基础。"}}
{"id": "2512.06301", "pdf": "https://arxiv.org/pdf/2512.06301", "abs": "https://arxiv.org/abs/2512.06301", "authors": ["Jihun Ahn", "Gabriella Pasya Irianti", "Vikram Thapar", "Su-Mi Hur"], "title": "Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.", "AI": {"tldr": "提出CI-LLM框架，结合HAPPY分子表示法和数值描述符，用于聚合物信息学的性质预测和逆向设计", "motivation": "机器学习在无机化合物和小分子材料发现中已取得成功，但聚合物领域仍难以应用这些方法，数据稀缺被认为是主要瓶颈，但研究表明通过策略性分子表示可以克服这一限制", "method": "引入CI-LLM框架，结合HAPPY（将化学亚结构编码为token）和数值描述符，使用transformer架构；性质预测采用De³BERTa编码器，逆向设计采用GPT-based生成器", "result": "De³BERTa比基于SMILES的模型推理速度快3.5倍，R²分数提高0.9-4.1%；GPT生成器实现100%骨架保留，成功完成负相关目标的多性质优化", "conclusion": "该综合框架展示了聚合物科学中机器学习的前向预测和逆向设计能力，证明了策略性分子表示对推进聚合物机器学习应用的重要性"}}
{"id": "2512.06297", "pdf": "https://arxiv.org/pdf/2512.06297", "abs": "https://arxiv.org/abs/2512.06297", "authors": ["Luca Di Carlo", "Chase Goddard", "David J. Schwab"], "title": "Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "comment": "Under Review", "summary": "Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.", "AI": {"tldr": "该论文研究了过参数化神经网络中损失景观的连通性与优化动态的局限性之间的悖论，揭示了曲率变化与优化噪声相互作用产生的熵障碍如何导致解在参数空间中的局域化。", "motivation": "现代神经网络表现出一个矛盾现象：损失景观中的吸引盆通常通过低损失路径相连，但优化动态通常局限于单个凸盆，很少探索中间点。论文旨在解释这种连通性与实际优化行为之间的差异。", "method": "通过分析曲率变化与优化动态中噪声的相互作用，识别出熵障碍的产生机制。实证研究发现，沿着连接路径远离最小值时曲率系统性地增加，产生将噪声动态偏转回端点的有效力。", "result": "曲率变化产生的熵障碍比能量障碍持续更长时间，塑造了参数空间中解的晚期局域化。即使损失保持近乎平坦，这些障碍仍然存在并影响优化动态。", "conclusion": "曲率诱导的熵力在深度学习景观中同时控制着连通性和局域化，解释了为什么神经网络虽然存在低损失连接路径，但优化过程通常仍局限于单个吸引盆。"}}
{"id": "2512.06296", "pdf": "https://arxiv.org/pdf/2512.06296", "abs": "https://arxiv.org/abs/2512.06296", "authors": ["Sooho Moon", "Yunyong Ko"], "title": "How Sharp and Bias-Robust is a Model? Dual Evaluation Perspectives on Knowledge Graph Completion", "categories": ["cs.AI"], "comment": "5 pages, 4 figures, 2 tables, ACM WSDM 2026", "summary": "Knowledge graph completion (KGC) aims to predict missing facts from the observed KG. While a number of KGC models have been studied, the evaluation of KGC still remain underexplored. In this paper, we observe that existing metrics overlook two key perspectives for KGC evaluation: (A1) predictive sharpness -- the degree of strictness in evaluating an individual prediction, and (A2) popularity-bias robustness -- the ability to predict low-popularity entities. Toward reflecting both perspectives, we propose a novel evaluation framework (PROBE), which consists of a rank transformer (RT) estimating the score of each prediction based on a required level of predictive sharpness and a rank aggregator (RA) aggregating all the scores in a popularity-aware manner. Experiments on real-world KGs reveal that existing metrics tend to over- or under-estimate the accuracy of KGC models, whereas PROBE yields a comprehensive understanding of KGC models and reliable evaluation results.", "AI": {"tldr": "提出一个名为PROBE的新型知识图谱补全评估框架，从预测锐度和流行度偏差鲁棒性两个关键视角评估KGC模型", "motivation": "现有KGC评估指标存在两个关键缺陷：1) 忽略预测锐度（评估单个预测的严格程度）；2) 缺乏对流行度偏差的鲁棒性（预测低流行度实体的能力）", "method": "提出PROBE框架，包含两个组件：1) 秩变换器（RT）- 根据所需预测锐度水平估计每个预测的分数；2) 秩聚合器（RA）- 以流行度感知方式聚合所有分数", "result": "在真实世界知识图谱上的实验表明，现有指标倾向于高估或低估KGC模型的准确性，而PROBE能提供对KGC模型的全面理解和可靠的评估结果", "conclusion": "PROBE框架通过同时考虑预测锐度和流行度偏差鲁棒性，为知识图谱补全提供了更全面、可靠的评估方法，揭示了现有评估指标的局限性"}}
{"id": "2512.06290", "pdf": "https://arxiv.org/pdf/2512.06290", "abs": "https://arxiv.org/abs/2512.06290", "authors": ["Yiheng Huang", "Shuang She", "Zewei Wei", "Jianmin Lin", "Ming Yang", "Wenyin Liu"], "title": "StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\\%$ to 95.54$\\%$, demonstrating the effectiveness and robustness of our approach.", "AI": {"tldr": "StrokeNet提出了一种新颖的在线手写笔画分类方法，通过将笔画编码为参考点对表示（点+特征向量），使用参考点进行空间查询，特征向量进行交互建模，以解决现有方法难以捕捉笔画间细粒度语义关系的问题。", "motivation": "笔画分类面临书写风格变化、内容模糊和动态书写位置的挑战。核心问题在于建模笔画间的语义关系。现有深度学习方法难以捕捉局部化的细粒度笔画交互关系，而从点级视角处理又会引入冗余。需要一种能有效表示细粒度笔画关系的方法。", "method": "1. 将笔画编码为参考点对表示（点+特征向量）；2. 动态为每个笔画选择参考点并排序，使用内联序列注意力（ISA）模块构建上下文特征；3. 设计跨椭圆查询（CEQ）机制聚类参考点并在不同空间尺度提取特征；4. 联合优化框架同时通过参考点回归预测笔画类别，并通过辅助分支（Aux-Branch）建模相邻笔画语义转换。", "result": "在多个公开在线手写数据集上达到最先进性能。特别是在CASIA-onDo数据集上，准确率从93.81%提升到95.54%，证明了方法的有效性和鲁棒性。", "conclusion": "StrokeNet通过创新的参考点对表示和细粒度交互建模机制，成功解决了在线手写笔画分类中的关键挑战，显著提升了分类性能，为笔画级手写分析提供了有效的解决方案。"}}
{"id": "2512.06282", "pdf": "https://arxiv.org/pdf/2512.06282", "abs": "https://arxiv.org/abs/2512.06282", "authors": ["Lyn Chao-ling Chen", "Kuan-Wen Chen", "Yi-Ping Hung"], "title": "A Sleep Monitoring System Based on Audio, Video and Depth Information", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted in the Computer Vision, Graphics and Image Processing (CVGIP 2013)", "summary": "For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.", "AI": {"tldr": "开发基于音频、视频和深度信息的非侵入式睡眠监测系统，用于定量评估睡眠障碍", "motivation": "需要一种非侵入式的方法来定量评估睡眠障碍，传统方法如多导睡眠图（PSG）需要专业设备和环境，不适合家庭使用", "method": "使用红外深度传感器、RGB摄像头和四麦克风阵列设备，在几乎无光源环境中监测睡眠。建立深度信号的背景模型检测运动幅度，建立彩色图像的背景模型检测光照变化，使用事件检测算法从三种传感器处理数据中检测事件发生", "result": "系统在睡眠条件下进行了测试，实验结果验证了系统的可靠性", "conclusion": "成功开发了一种基于事件方法的非侵入式睡眠监测系统，能够有效检测运动、开关灯和噪音三类睡眠干扰事件，适用于家庭环境中的睡眠监测"}}
{"id": "2512.06281", "pdf": "https://arxiv.org/pdf/2512.06281", "abs": "https://arxiv.org/abs/2512.06281", "authors": ["Hengzhuang Li", "Xinsong Zhang", "Qiming Peng", "Bin Luo", "Han Hu", "Dengyang Jiang", "Han-Jia Ye", "Teng Zhang", "Hai Jin"], "title": "Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.", "AI": {"tldr": "提出LaVer训练框架，通过潜在空间掩码图像建模增强多模态大语言模型的视觉表示能力", "motivation": "MLLMs存在模态不平衡问题，深层网络中对视觉信息的利用不足，导致视觉性能下降或产生幻觉，主要原因是训练时缺乏直接的视觉监督信号", "method": "提出潜在视觉重建（LaVer）框架，在LLM的联合潜在语义空间中进行掩码图像建模，为MLLMs提供直接的视觉激活", "result": "实验证明该方法在多种基准测试中表现优越，特别是在需要密集视觉能力的场景中，模型表现出增强的视觉注意力分配", "conclusion": "LaVer框架有效解决了MLLMs的模态不平衡问题，增强了视觉表示能力，提升了模型在视觉密集型任务中的性能"}}
{"id": "2512.06276", "pdf": "https://arxiv.org/pdf/2512.06276", "abs": "https://arxiv.org/abs/2512.06276", "authors": ["Tianyi Gao", "Hao Li", "Han Fang", "Xin Wei", "Xiaodong Dong", "Hongbo Sun", "Ye Yuan", "Zhongjiang He", "Jinglin Xu", "Jingmin Xin", "Hao Sun"], "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.", "AI": {"tldr": "提出RefBench-PRO基准，用于评估多模态大语言模型在指代表达理解任务中的感知和推理能力，包含六个渐进挑战性子任务", "motivation": "现有REC基准主要评估感知能力，缺乏可解释的评分机制，无法揭示MLLM在不同认知能力上的定位能力", "method": "1) 将指代表达分解为感知和推理两个核心维度，细分为六个渐进挑战性子任务；2) 开发全自动数据生成管道；3) 提出Ref-R1学习方案，结合基于动态IoU的GRPO提高定位精度", "result": "RefBench-PRO能够对MLLM在指代表达理解上进行可解释评估，在感知和推理方面都提出了更大挑战，建立了更强的REC基线", "conclusion": "RefBench-PRO是一个全面的REC基准，通过分解感知和推理维度并提供可解释评估，能够更好地揭示MLLM的定位能力"}}
{"id": "2512.06275", "pdf": "https://arxiv.org/pdf/2512.06275", "abs": "https://arxiv.org/abs/2512.06275", "authors": ["Kegang Wang", "Jiankai Tang", "Yuntao Wang", "Xin Liu", "Yuxuan Fan", "Jiatong Ji", "Yuanchun Shi", "Daniel McDuff"], "title": "FacePhys: State of the Heart Learning", "categories": ["cs.CV"], "comment": null, "summary": "Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \\emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\\% to 99\\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.", "AI": {"tldr": "提出FacePhys算法，一种基于时空状态空间对偶性的内存高效远程光电容积描记（rPPG）技术，用于通过摄像头进行实时生命体征测量", "motivation": "当前基于摄像头的生命体征测量技术面临计算资源限制、数据传输压缩导致信号质量下降、以及模型可扩展性、跨数据集泛化能力和实时操作之间的三难问题", "method": "基于时空状态空间对偶性构建内存高效rPPG算法，利用可转移的心脏状态捕捉视频帧间的细微周期性变化，同时保持最小计算开销", "result": "FacePhys实现了49%的错误率降低，内存占用仅3.6MB，每帧延迟9.46ms，比现有方法快83%到99%，建立了新的最先进水平", "conclusion": "FacePhys解决了模型可扩展性、跨数据集泛化和实时操作的三难问题，实现了在实际部署中的可靠实时性能，为舒适、普遍的健康监测提供了可行解决方案"}}
{"id": "2512.06274", "pdf": "https://arxiv.org/pdf/2512.06274", "abs": "https://arxiv.org/abs/2512.06274", "authors": ["Hanmo Zhang", "Zenghui Sun", "Kai Wang"], "title": "Networked Restless Multi-Arm Bandits with Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.", "AI": {"tldr": "提出Networked RMAB框架，将传统RMAB与独立级联模型结合以捕捉网络环境中个体间的相互作用，解决传统RMAB假设臂独立性的局限性。", "motivation": "传统RMAB假设臂之间相互独立，但在现实世界环境中（如公共卫生干预），个体间存在显著相互作用，这种网络效应未被传统模型捕捉，限制了实际应用效果。", "method": "1) 定义Networked RMAB框架，结合RMAB和独立级联模型；2) 建立Bellman方程并分析其计算挑战；3) 证明Bellman方程的子模性，应用爬山算法获得1-1/e近似保证；4) 通过改进的收缩分析证明近似Bellman更新的收敛性；5) 开发针对网络设置的高效Q学习算法。", "result": "实验验证：在真实世界图数据上，提出的Q学习方法优于k步前瞻方法和忽略网络效应的方法，证明了捕捉网络效应的重要性。", "conclusion": "Networked RMAB框架成功捕捉了网络环境中的个体相互作用，提出的近似算法具有理论保证，实验证明网络效应显著影响决策效果，为资源分配和干预优化提供了更现实的建模工具。"}}
{"id": "2512.06269", "pdf": "https://arxiv.org/pdf/2512.06269", "abs": "https://arxiv.org/abs/2512.06269", "authors": ["Quan Tran", "Tuan Dang"], "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "10 pages", "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.", "AI": {"tldr": "提出TriaGS方法，通过三角测量引导的几何一致性约束来改进3D高斯泼溅技术，解决仅依赖光度损失导致的几何不一致问题", "motivation": "3D高斯泼溅技术仅依赖光度损失进行重建，这会导致几何不一致、产生\"漂浮物\"伪影和无结构几何，难以提取高质量表面", "method": "通过约束多视角三角测量来强制全局几何一致性，利用自监督方式从相邻视角重新三角化得到鲁棒共识点，并惩罚渲染3D点与共识点的偏差", "result": "在多个数据集上取得最先进结果，在DTU数据集上达到平均Chamfer距离0.50mm，优于同类显式方法", "conclusion": "提出的TriaGS方法通过三角测量引导的几何一致性约束有效改进了3D高斯泼溅的重建质量，减少了伪影并提升了几何精度"}}
{"id": "2512.06261", "pdf": "https://arxiv.org/pdf/2512.06261", "abs": "https://arxiv.org/abs/2512.06261", "authors": ["Taekyung Kim", "Keyvan Majd", "Hideki Okamoto", "Bardh Hoxha", "Dimitra Panagou", "Georgios Fainekos"], "title": "Safe Model Predictive Diffusion with Shielding", "categories": ["cs.RO"], "comment": "Project page: https://www.taekyung.me/safe-mpd", "summary": "Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.", "AI": {"tldr": "提出Safe Model Predictive Diffusion (Safe MPD)方法，将基于模型的扩散框架与安全防护机制结合，生成既满足运动学约束又安全的轨迹规划方案。", "motivation": "为复杂机器人系统生成安全、满足运动学约束且最优的轨迹是机器人学中的核心挑战。现有方法通常采用后处理修正，但存在计算不可行性和可行性损失等问题。", "method": "结合基于模型的扩散框架与安全防护机制，在去噪过程中对所有样本强制执行可行性和安全性约束，避免后处理修正的常见缺陷。", "result": "在具有挑战性的非凸规划问题上验证，包括运动学和加速度控制的拖拉机-拖车系统。结果显示在成功率和安全性方面显著优于现有安全策略，同时实现亚秒级计算时间。", "conclusion": "Safe MPD是一种无需训练的扩散规划器，能够生成构造上既满足运动学约束又安全的轨迹，避免了后处理修正的计算不可行性和可行性损失问题。"}}
{"id": "2512.06259", "pdf": "https://arxiv.org/pdf/2512.06259", "abs": "https://arxiv.org/abs/2512.06259", "authors": ["Yash Choudhary", "Preeti Rao", "Pushpak Bhattacharyya"], "title": "Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.", "AI": {"tldr": "本文提出GAMENet，一种用于音乐流行度预测的多模态深度学习架构，通过自适应门控机制整合音频、歌词和社交元数据专家，并引入职业轨迹动态特征来捕捉艺术家职业生涯动量。", "motivation": "预测歌曲发布前的商业成功对音乐产业至关重要，但现有方法存在四个主要局限：1) 音频和歌词的时间动态被平均化；2) 歌词表示忽略组合结构和情感语义；3) 忽略艺术家和歌曲历史表现；4) 多模态融合方法依赖简单特征拼接导致表示对齐不佳。", "method": "提出GAMENet架构，包含：1) 音频特征通过OnionEnsembleAENet提取；2) 歌词嵌入通过大语言模型管道获取；3) 新引入的职业轨迹动态特征捕捉多年艺术家职业生涯动量和歌曲级轨迹统计；4) 自适应门控机制整合多模态专家；5) 时间参与建模。", "result": "在Music4All数据集上，GAMENet比直接多模态特征拼接的R²提高12%；仅Spotify音频描述符R²为0.13，整合聚合CTD特征后提升至0.69，时间CTD特征带来额外7%增益；在SpotGenTrack数据集上比先前基线提高16%。", "conclusion": "GAMENet通过自适应融合多模态专家和时间参与建模，显著提升了音乐流行度预测性能，验证了各模态的独特贡献和模型的有效性，为音乐产业提供了实用的预测工具。"}}
{"id": "2512.06258", "pdf": "https://arxiv.org/pdf/2512.06258", "abs": "https://arxiv.org/abs/2512.06258", "authors": ["Chaoyang Wang", "Yangfan He", "Yiyang Zhou", "Yixuan Wang", "Jiaqi Liu", "Peng Xia", "Zhengzhong Tu", "Mohit Bansal", "Huaxiu Yao"], "title": "Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.", "AI": {"tldr": "该论文提出了一种解决大型视觉语言模型推理路径选择偏差问题的两阶段后训练框架PSO，通过优化推理路径选择来提高模型的推理准确性和稳定性。", "motivation": "研究发现大型视觉语言模型存在一个关键但未被充分探索的缺陷：即使模型知道正确答案，也经常通过错误的推理路径得出结果。核心问题不是缺乏知识，而是在庞大的推理搜索空间中存在路径选择偏差。", "method": "提出了PSO（路径选择优化）两阶段后训练框架：第一阶段使用带有模板和答案奖励的组相对策略优化来培养结构化、逐步推理；第二阶段进行在线偏好优化，模型从GRPO生成的数据中采样推理路径，自我评估并调整到偏好的轨迹。同时将错误或次优路径存储在负向回放记忆中作为硬负样本。", "result": "实验表明PSO能有效剪除无效推理路径，显著提高推理准确性（平均提升7.4%），并产生更稳定一致的思维链。Pass@K与Pass@1之间的显著差异证明了失败主要源于错误推理而非无知。", "conclusion": "PSO框架成功解决了LVLMs中的推理路径选择偏差问题，通过优化推理路径选择提高了模型的推理性能和稳定性，为改善视觉语言模型的推理可靠性提供了有效方法。"}}
{"id": "2512.06256", "pdf": "https://arxiv.org/pdf/2512.06256", "abs": "https://arxiv.org/abs/2512.06256", "authors": ["Aniruddha Maiti", "Satya Nimmagadda", "Kartha Veerya Jammuladinne", "Niladri Sengupta", "Ananya Jana"], "title": "Convergence of Outputs When Two Large Language Models Interact in a Multi-Agentic Setup", "categories": ["cs.CL", "cs.AI"], "comment": "accepted to LLM 2025", "summary": "In this work, we report what happens when two large language models respond to each other for many turns without any outside input in a multi-agent setup. The setup begins with a short seed sentence. After that, each model reads the other's output and generates a response. This continues for a fixed number of steps. We used Mistral Nemo Base 2407 and Llama 2 13B hf. We observed that most conversations start coherently but later fall into repetition. In many runs, a short phrase appears and repeats across turns. Once repetition begins, both models tend to produce similar output rather than introducing a new direction in the conversation. This leads to a loop where the same or similar text is produced repeatedly. We describe this behavior as a form of convergence. It occurs even though the models are large, trained separately, and not given any prompt instructions. To study this behavior, we apply lexical and embedding-based metrics to measure how far the conversation drifts from the initial seed and how similar the outputs of the two models becomes as the conversation progresses.", "AI": {"tldr": "研究两个大型语言模型在多智能体设置中相互交互时输出的收敛现象", "motivation": "探索在没有外部输入的情况下，两个大型语言模型相互对话多轮后会发生什么，特别是观察它们是否会收敛到重复模式", "method": "使用Mistral Nemo Base 2407和Llama 2 13B hf模型，从一个短种子句子开始，让两个模型相互读取对方的输出并生成响应，持续固定步数，应用词汇和嵌入指标来测量对话与初始种子的偏离程度以及两个模型输出的相似性", "result": "大多数对话开始时连贯但后来陷入重复，许多运行中出现短短语并在多轮中重复，一旦重复开始，两个模型倾向于产生相似输出而不是引入新方向，导致相同或相似文本的循环，即使模型很大、单独训练且没有提示指令也会发生这种收敛行为", "conclusion": "两个大型语言模型在多智能体交互中会收敛到重复输出模式，表现为一种形式的收敛，即使模型规模大且独立训练，在没有外部干预的情况下对话会陷入循环"}}
{"id": "2512.06255", "pdf": "https://arxiv.org/pdf/2512.06255", "abs": "https://arxiv.org/abs/2512.06255", "authors": ["Shijie Wang", "Xin Yu", "Yadan Luo", "Zijian Wang", "Pengfei Zhang", "Zi Huang"], "title": "Language-driven Fine-grained Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer", "AI": {"tldr": "提出LaFG框架，通过语言模型将类别名称转换为属性级监督，用于细粒度图像检索，提升对未见类别的泛化能力", "motivation": "现有细粒度图像检索方法使用稀疏的one-hot标签作为监督，忽略了类别名称中丰富的语义信息，导致跨类别细节可比性建模不足，限制了模型对未见类别的泛化能力", "method": "使用大语言模型将类别名称转换为详细的属性导向描述，利用冻结的视觉语言模型将这些描述投影到视觉对齐空间，聚类形成数据集范围的属性词汇表，通过全局提示模板选择类别相关属性并聚合成类别特定的语言原型，用这些原型监督检索模型", "result": "该方法在多个细粒度图像检索基准测试中取得了最先进的性能，特别是在未见类别上表现出优异的泛化能力", "conclusion": "通过利用语言模型将类别名称转换为丰富的属性级监督，LaFG框架有效解决了现有细粒度检索方法语义监督不足的问题，显著提升了对未见类别的泛化性能"}}
{"id": "2512.06251", "pdf": "https://arxiv.org/pdf/2512.06251", "abs": "https://arxiv.org/abs/2512.06251", "authors": ["Fangzhou Lin", "Yuping Wang", "Yuliang Guo", "Zixun Huang", "Xinyu Huang", "Haichong Zhang", "Kazunori Yamada", "Zhengzhong Tu", "Liu Ren", "Ziming Zhang"], "title": "NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks", "categories": ["cs.CV"], "comment": "12 pages, 7 figures", "summary": "Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.", "AI": {"tldr": "NexusFlow是一个新颖的轻量级即插即用框架，通过可逆流网络统一异构任务在部分监督下的学习，解决现有方法主要关注同质密集预测任务而忽视结构多样性任务的局限性。", "motivation": "现有部分监督多任务学习方法主要关注同质密集预测任务，而现实场景中需要处理结构多样的任务（如密集地图重建和稀疏多目标跟踪），这些任务在标注不完整的情况下存在结构差异和领域差距，需要有效的知识迁移方法。", "method": "引入带有可逆耦合层的代理网络，将不同任务的潜在特征分布对齐到统一的规范空间。耦合层是双射的，在映射特征到共享空间时保留信息，避免表示崩溃，同时保持表达能力，实现结构不同任务间的有效对齐。", "result": "在nuScenes自动驾驶数据集上，针对领域分区的密集地图重建和稀疏多目标跟踪任务，NexusFlow取得了新的最先进结果。在NYUv2数据集上，针对同质密集预测任务（分割、深度、表面法线），在所有任务上都获得了一致的性能提升，证明了其广泛适用性。", "conclusion": "NexusFlow通过可逆流网络有效解决了异构任务在部分监督下的统一学习问题，既能处理结构差异大的任务，也能在同质任务上取得一致改进，为部分监督多任务学习提供了通用且有效的解决方案。"}}
{"id": "2512.06247", "pdf": "https://arxiv.org/pdf/2512.06247", "abs": "https://arxiv.org/abs/2512.06247", "authors": ["Gus Henry Smith", "Sandesh Adhikary", "Vineet Thumuluri", "Karthik Suresh", "Vivek Pandit", "Kartik Hegde", "Hamid Shojaei", "Chandra Bhagavatula"], "title": "DUET: Agentic Design Understanding via Experimentation and Testing", "categories": ["cs.SE", "cs.AI", "cs.AR"], "comment": null, "summary": "AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.", "AI": {"tldr": "提出DUET方法，通过实验和测试来增强AI代理对硬件设计的理解能力，特别是针对SystemVerilog RTL代码的复杂行为理解", "motivation": "大型语言模型在处理硬件设计任务时面临挑战，特别是对于SystemVerilog RTL代码，因为RTL代码使用低级语言特征编码复杂、动态、时间演化的行为，仅从语法难以推断这些复杂行为，限制了代码补全、文档生成和验证等下游任务的完成能力", "method": "DUET方法模仿硬件设计专家理解复杂设计的方式：通过迭代实验使用多种工具。该方法迭代生成假设，使用EDA工具（如仿真、波形检查和形式验证）进行测试，并整合结果以构建自底向上的设计理解", "result": "评估显示，DUET相比没有实验的基线流程，显著提高了AI代理在形式验证任务上的性能", "conclusion": "DUET提供了一种通用的方法论，通过实验和测试来增强AI代理对硬件设计的理解能力，解决了LLM在处理RTL代码时面临的独特挑战，为硬件设计自动化开辟了新途径"}}
{"id": "2512.06244", "pdf": "https://arxiv.org/pdf/2512.06244", "abs": "https://arxiv.org/abs/2512.06244", "authors": ["Caleb Ju", "Guanghui Lan"], "title": "Auto-exploration for online reinforcement learning", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "35 pages (9 appendix), 1 figure. Comments are welcome", "summary": "The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.", "AI": {"tldr": "提出一种具有自动探索功能的新强化学习方法，无需先验知识或算法相关参数，在表格设置和线性函数逼近两种情况下都能实现O(ε⁻²)样本复杂度", "motivation": "现有强化学习算法需要假设在状态和动作空间上进行充分探索，这导致算法不可实现且性能次优。需要开发能够自动探索状态和动作空间的方法，无需先验知识或问题相关参数。", "method": "提出了两种变体：表格设置和线性函数逼近。核心创新包括动态混合时间、用于采样的折扣状态分布、简单的鲁棒梯度估计器，以及用于验证收敛的最近优势差距函数。", "result": "在存在探索最优策略的算法独立假设下，两种方法都能达到O(ε⁻²)样本复杂度来解决ε误差问题。这些复杂度是新颖的，因为它们避免了先前工作中可能任意大的算法相关参数。", "conclusion": "提出的自动探索方法解决了强化学习中的探索-利用困境，实现了参数无关、易于实现且具有理论保证的算法，在表格和线性函数逼近设置中都表现出色。"}}
{"id": "2512.06240", "pdf": "https://arxiv.org/pdf/2512.06240", "abs": "https://arxiv.org/abs/2512.06240", "authors": ["Chuanhao Nie", "Yunbo Liu", "Chao Wang"], "title": "AI Application in Anti-Money Laundering for Sustainable and Transparent Financial Systems", "categories": ["cs.AI"], "comment": null, "summary": "Money laundering and financial fraud remain major threats to global financial stability, costing trillions annually and challenging regulatory oversight. This paper reviews how artificial intelligence (AI) applications can modernize Anti-Money Laundering (AML) workflows by improving detection accuracy, lowering false-positive rates, and reducing the operational burden of manual investigations, thereby supporting more sustainable development. It further highlights future research directions including federated learning for privacy-preserving collaboration, fairness-aware and interpretable AI, reinforcement learning for adaptive defenses, and human-in-the-loop visualization systems to ensure that next-generation AML architectures remain transparent, accountable, and robust. In the final part, the paper proposes an AI-driven KYC application that integrates graph-based retrieval-augmented generation (RAG Graph) with generative models to enhance efficiency, transparency, and decision support in KYC processes related to money-laundering detection. Experimental results show that the RAG-Graph architecture delivers high faithfulness and strong answer relevancy across diverse evaluation settings, thereby enhancing the efficiency and transparency of KYC CDD/EDD workflows and contributing to more sustainable, resource-optimized compliance practices.", "AI": {"tldr": "本文探讨了人工智能在反洗钱领域的应用，旨在通过AI技术提升金融系统的可持续性和透明度，特别提出了一种结合图检索增强生成(RAG Graph)的AI驱动KYC应用方案。", "motivation": "洗钱和金融欺诈每年造成数万亿美元损失，威胁全球金融稳定，传统监管方法存在检测准确率低、误报率高、人工调查负担重等问题，需要现代化解决方案来支持可持续发展。", "method": "论文综述了AI在AML工作流程中的应用，并提出了一个创新的AI驱动KYC应用方案，该方案整合了图检索增强生成(RAG Graph)与生成模型，以提升KYC流程的效率、透明度和决策支持能力。", "result": "实验结果表明，RAG-Graph架构在不同评估场景下展现出高忠实度和强答案相关性，显著提升了KYC客户尽职调查/增强尽职调查工作流程的效率和透明度，有助于实现更可持续、资源优化的合规实践。", "conclusion": "AI技术能够有效现代化反洗钱工作流程，提高检测准确性、降低误报率、减少人工负担，同时论文提出的未来研究方向（包括联邦学习、公平可解释AI、强化学习等）将确保下一代AML架构保持透明、可问责和鲁棒性，促进可持续金融系统发展。"}}
{"id": "2512.06232", "pdf": "https://arxiv.org/pdf/2512.06232", "abs": "https://arxiv.org/abs/2512.06232", "authors": ["Ellen Su", "Solim Legris", "Todd M. Gureckis", "Mengye Ren"], "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.", "AI": {"tldr": "研究探讨了仅使用视觉数据（即使是发展上真实的儿童视角视频）是否足以让深度学习模型学习直觉物理，发现仅改变数据分布和规模不足以让当前架构获得人类水平的直觉物理能力。", "motivation": "人类通过丰富的内部模型和直觉物理理解来导航世界，而当前最先进的深度学习模型尽管在大量互联网视频数据上训练，在直觉物理基准测试中仍达不到人类水平。研究者想探究数据分布（而非数据量）是否是学习这些物理原理的关键。", "method": "使用Video Joint Embedding Predictive Architecture (V-JEPA)模型在SAYCam数据集上预训练，这是一个发展上真实的、以自我为中心的视频数据集，部分捕捉了三个儿童的日常视觉体验。该数据集仅占SOTA模型训练数据量的0.01%。然后在IntPhys2基准测试上评估性能。", "result": "在SAYCam数据集上训练并没有显著提高在IntPhys2基准测试上的性能。这表明仅使用发展上真实的数据集训练不足以让当前架构学习支持直觉物理的表征。", "conclusion": "仅改变视觉数据的量和分布可能不足以构建具有人工直觉物理的系统。学习直觉物理可能需要超越视觉数据的其他因素，如多模态信息、主动交互或不同的架构设计。"}}
{"id": "2512.06230", "pdf": "https://arxiv.org/pdf/2512.06230", "abs": "https://arxiv.org/abs/2512.06230", "authors": ["Pranav Balakrishnan", "Sidisha Barik", "Sean M. O'Rourke", "Benjamin M. Marlin"], "title": "GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.", "AI": {"tldr": "提出GPU-GLMB方法，通过修改GLMB滤波器使其支持同一传感器对同一目标的多次检测，从而打破检测间依赖关系，实现GPU并行加速，提升多目标跟踪的扩展性。", "motivation": "基于随机有限集的标记多假设跟踪方法（如GLMB滤波器）虽然理论性质良好，但在标准测量模型下维护多个假设计算成本极高，即使应用假设剪枝近似仍难以满足实时性需求。特别是在分布式机器学习虚拟传感器网络中，需要支持同一传感器对同一目标的多次检测能力。", "method": "提出改进的GLMB滤波器变体，允许同一传感器对同一目标产生多次检测。这种修改打破了标准GLMB滤波器中检测间的依赖关系，使得滤波器更新过程具有更好的并行可扩展性，从而能够高效部署在GPU硬件上。", "result": "初步分析显示GPU加速实现的GLMB跟踪器在运行时间扩展性方面表现良好，特别是在目标数量和保留假设最大数量方面。GPU并行化显著提升了计算效率。", "conclusion": "通过修改GLMB滤波器支持多次检测，成功打破了检测间依赖，实现了GPU友好的并行计算结构。这为标记随机有限集多目标跟踪方法在分布式虚拟传感器网络中的实际部署提供了可行的计算框架，显著提升了跟踪系统的可扩展性。"}}
{"id": "2512.06221", "pdf": "https://arxiv.org/pdf/2512.06221", "abs": "https://arxiv.org/abs/2512.06221", "authors": ["Alena Makarova"], "title": "Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study", "categories": ["cs.CV"], "comment": "15 pages, 13 figures. Reproducibility study", "summary": "This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.", "AI": {"tldr": "对SVD与WDR结合的图像压缩方法进行独立可复现性研究，验证原始论文声称的优于JPEG2000和单独WDR的性能", "motivation": "原始论文声称SVD与WDR结合在视觉质量和压缩比上优于JPEG2000和单独WDR，但存在实现细节模糊的问题，需要进行独立验证和可复现性研究", "method": "重新实现SVD+WDR方法，填补原始描述中的缺失细节（如量化和阈值初始化），在原始实验基础上增加新图像测试，使用PSNR和SSIM进行性能评估", "result": "与原始声称相反，SVD+WDR方法在PSNR上通常不优于JPEG2000或WDR，仅在SSIM上部分优于JPEG2000，原始描述中的模糊细节显著影响可复现性和性能报告", "conclusion": "该研究强调了论文中实现细节描述的重要性，模糊的描述会导致可复现性问题，并可能影响性能结论的可靠性"}}
{"id": "2512.06211", "pdf": "https://arxiv.org/pdf/2512.06211", "abs": "https://arxiv.org/abs/2512.06211", "authors": ["Martin G. Herold", "Evangelos Kipouridis", "Joachim Spoerhase"], "title": "A Broader View on Clustering under Cluster-Aware Norm Objectives", "categories": ["cs.DS", "cs.LG"], "comment": null, "summary": "We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture. In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\\widetilde{O}(\\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\\widetilde{O}(\\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering. We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.", "AI": {"tldr": "本文研究了(f,g)-聚类问题，提出了改进的近似算法，填补了先前工作中的近似性差距。", "motivation": "先前提出的(f,g)-聚类问题虽然包含了k-Center、k-Median等基本聚类问题，但在更一般设置下的近似性分析存在较大差距，需要更清晰的近似性图景。", "method": "设计了三种主要算法：1) 对任意f的(f, L₁)-聚类问题的O(log²n)近似算法；2) 一般(f,g)-聚类问题的O(k)近似算法；3) 基于新定义的f和g参数的插值算法。", "result": "1) 将(f, L₁)-聚类的近似比从Õ(√n)改进到O(log²n)；2) 将一般(f,g)-聚类的近似比从Õ(√kn)改进到O(k)；3) 新算法在多项对数因子内插值了多个基本聚类问题的最佳已知界限。", "conclusion": "本文为(f,g)-聚类问题提供了更清晰的近似性图景，填补了先前工作中的近似性差距，并通过新定义的参数实现了对不同聚类问题界限的统一插值。"}}
{"id": "2512.06207", "pdf": "https://arxiv.org/pdf/2512.06207", "abs": "https://arxiv.org/abs/2512.06207", "authors": ["Harshil Suthar", "Dipankar Maity"], "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots", "categories": ["cs.RO"], "comment": "Submitted to conference", "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.", "AI": {"tldr": "该论文提出了一种通信感知的空中机器人支持地面机器人的框架，通过价值信息评估、混合整数线性规划和效用评分探索策略，在带宽受限条件下优化地图信息传输决策。", "motivation": "在多机器人系统中，空中机器人需要为地面机器人提供环境地图支持，但通信带宽有限，需要解决何时传输、传输什么以及传输多少信息的问题，同时还要进行环境探索和建图。", "method": "提出一个三部分框架：1) 基于价值信息(VoI)决定传输什么信息；2) 使用混合整数线性规划(MILP)决定传输多少信息；3) 基于效用评分的探索策略获取额外信息。同时分析通信与运动之间的权衡。", "result": "开发了一个综合框架，能够在带宽受限条件下优化空中机器人向地面机器人传输地图信息的决策，实现了通信成本与地面机器人导航成本之间的权衡分析。", "conclusion": "该工作为通信受限环境下的多机器人协作提供了系统化解决方案，通过价值信息评估和优化传输策略，有效平衡了环境探索、地图传输和地面机器人导航的需求。"}}
{"id": "2512.06206", "pdf": "https://arxiv.org/pdf/2512.06206", "abs": "https://arxiv.org/abs/2512.06206", "authors": ["Akis Linardos", "Sarthak Pati", "Ujjwal Baid", "Brandon Edwards", "Patrick Foley", "Kevin Ta", "Verena Chung", "Micah Sheller", "Muhammad Irfan Khan", "Mojtaba Jafaritadi", "Elina Kontio", "Suleiman Khan", "Leon Mächler", "Ivan Ezhov", "Suprosanna Shit", "Johannes C. Paetzold", "Gustav Grimberg", "Manuel A. Nickel", "David Naccache", "Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni", "Daewoon Kim", "Leonard L. Klausmann", "Prashant Shah", "et al. (3 additional authors not shown)"], "title": "The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Published at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:033", "summary": "We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.", "AI": {"tldr": "MICCAI FeTS挑战赛2024专注于联邦学习在胶质瘤亚区分割中的应用，评估旨在提升鲁棒性和效率的新型权重聚合方法", "motivation": "联邦学习在医学影像分析中面临通信效率和鲁棒性挑战，需要开发更有效的权重聚合方法来处理多机构数据分布差异", "method": "使用标准化的联邦学习设置和多机构数据集（来自BraTS基准的1251个训练案例），评估六支参赛团队提出的新权重聚合方法，采用累积评分系统综合考虑分割性能（DSC和HD95）和通信效率（收敛分数）", "result": "基于PID控制器的方法获得最高排名，在ET、TC、WT分割上分别达到0.733、0.761、0.751的平均DSC值，HD95值分别为33.922mm、33.623mm、32.309mm，同时具有最高的通信效率（收敛分数0.764），超越了以往挑战赛的最佳方法", "conclusion": "PID控制器是稳定和优化联邦学习中权重聚合的有效机制，该挑战赛推动了医学影像联邦学习的发展，代码已开源供社区使用"}}
{"id": "2512.06205", "pdf": "https://arxiv.org/pdf/2512.06205", "abs": "https://arxiv.org/abs/2512.06205", "authors": ["Daniel Quigley", "Eric Maynard"], "title": "On measuring grounding and generalizing grounding problems", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "36 pages, 85 sources", "summary": "The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.", "AI": {"tldr": "该论文将符号接地问题从二元判断重新构建为一个包含多个评估维度的审计框架，包括真实性、保持性、忠实性、鲁棒性和组合性等五个核心要求，并应用于四种接地模式和三个案例研究。", "motivation": "传统符号接地问题通常被视为二元判断（接地或未接地），这种简化方式无法捕捉到接地现象的复杂性和多样性。论文旨在提供一个更精细、可操作的框架来系统评估不同系统中的接地问题。", "method": "提出一个多维评估框架，包含五个核心要求：真实性（机制是否在智能体内部并通过学习/进化获得）、保持性（原子意义是否保持完整）、忠实性（包括相关性和因果性）、鲁棒性（在扰动下的优雅降级）、组合性（系统是否从部分构建整体）。通过评估元组（上下文、意义类型、威胁模型、参考分布）来索引这些要求。", "result": "将框架应用于四种接地模式（符号、指称、向量、关系）和三个案例研究：模型论语义学实现精确组合但缺乏因果保证；大语言模型在语言任务上显示相关拟合和局部鲁棒性，但在无接地交互的世界任务上缺乏成功选择；人类语言通过进化和发育获得满足强真实性要求。", "conclusion": "通过将哲学上的表征问题操作化，为科学哲学家、计算机科学家、语言学家和数学家提供了一个共同的术语和技术框架，用于系统研究接地和意义问题，超越了传统的二元接地判断。"}}
{"id": "2512.06204", "pdf": "https://arxiv.org/pdf/2512.06204", "abs": "https://arxiv.org/abs/2512.06204", "authors": ["Rodney Lafuente-Mercado", "Daniela Rus", "T. Konstantin Rusch"], "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.", "AI": {"tldr": "提出了一种名为\"时间范围\"的模型无关指标，用于量化强化学习策略对过去观察的记忆使用程度", "motivation": "需要量化训练好的强化学习策略实际上使用了多少过去的观察信息，以更好地理解和比较不同智能体在不同环境中的记忆依赖特性", "method": "通过反向模式自动微分计算雅可比矩阵块∂y_s/∂x_t，将其作为时间影响剖面，并通过幅度加权平均滞后进行总结", "result": "时间范围指标在多种任务和架构中表现良好：在完全观察控制中保持较小值，在Copy-k任务中与任务真实滞后尺度匹配，与实现接近最优回报所需的最小历史窗口对齐", "conclusion": "时间范围提供了一个实用的序列级记忆依赖读数，可用于比较智能体和环境，并选择最短的足够上下文长度"}}
{"id": "2512.06198", "pdf": "https://arxiv.org/pdf/2512.06198", "abs": "https://arxiv.org/abs/2512.06198", "authors": ["Oussama Sifour", "Soulaimane Berkane", "Abdelhamid Tayebi"], "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation", "categories": ["cs.RO"], "comment": "8 pages", "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.", "AI": {"tldr": "设计一种级联紧耦合观测器，仅使用IMU、体坐标系矢量测量（如磁力计）和固定锚点的距离测量来重构刚体的完整状态", "motivation": "为自主导航系统提供一种轻量级且有效的单距离辅助导航方案，减少对复杂传感器系统的依赖，同时保证状态估计的准确性和鲁棒性", "method": "首先构建扩展线性时变系统估计体坐标系位置、速度和重力方向，然后利用恢复的重力方向和体坐标系矢量测量在SO(3)上重构完整姿态，形成级联观测器架构", "result": "在满足一致可观性条件下建立了级联设计的几乎全局渐近稳定性，仿真研究表明在三维轨迹上能准确估计位置、速度和姿态", "conclusion": "单距离辅助是一种轻量级且有效的自主导航模态，通过级联紧耦合观测器设计能够实现完整状态重构，具有对传感器噪声和轨迹变化的鲁棒性"}}
{"id": "2512.06196", "pdf": "https://arxiv.org/pdf/2512.06196", "abs": "https://arxiv.org/abs/2512.06196", "authors": ["Charlie Masters", "Marta Grześkiewicz", "Stefano V. Albrecht"], "title": "ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to the AAAI 2026 LLAMAS Workshop (Large Language Model Agents for Multi-Agent Systems)", "summary": "As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.", "AI": {"tldr": "提出ARCANE框架，将AI对齐问题转化为多智能体协作问题，通过动态生成自然语言评分标准来代表利益相关者偏好", "motivation": "随着基于大语言模型的智能体越来越多地应用于长期任务，保持其与利益相关者偏好的一致性变得至关重要。需要可解释的奖励模型让利益相关者能够理解和审计模型目标，并且能够在交互时引导智能体，无需重新训练即可纳入偏好变化", "method": "将对齐问题构建为多智能体协作问题，动态生成自然语言评分标准（加权可验证标准）。受效用理论启发，将评分标准学习构建为重构问题，应用正则化的组序列策略优化（GSPO）程序，平衡可解释性、忠实性和计算效率", "result": "使用从GDPVal基准派生的219个标注评分标准语料库，在需要多步推理和工具使用的挑战性任务上评估ARCANE。学习的评分标准产生紧凑、易读的评估，并实现可配置的权衡（如正确性与简洁性），无需重新训练", "conclusion": "基于评分标准的奖励模型为复杂、长期AI系统提供了可解释、测试时自适应对齐的有前景路径"}}
{"id": "2512.06193", "pdf": "https://arxiv.org/pdf/2512.06193", "abs": "https://arxiv.org/abs/2512.06193", "authors": ["Jihyung Park", "Saleh Afroogh", "Junfeng Jiao"], "title": "Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \\textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM's output probabilistically shifts the affective state of a dialogue.", "AI": {"tldr": "提出GAUGE框架，用于实时检测AI聊天机器人中隐藏的对话升级（隐性伤害）", "motivation": "现有的大语言模型安全机制主要依赖外部分类器或临床标准，无法实时检测对话中逐渐升级的情感强化或情感漂移导致的隐性伤害", "method": "提出GAUGE框架，这是一种轻量级的基于logit的方法，通过测量LLM输出如何概率性地改变对话的情感状态来检测隐藏的对话升级", "result": "GAUGE能够实时检测传统毒性过滤器无法发现的隐性伤害，包括重复情感强化和情感漂移导致的对话升级", "conclusion": "GAUGE框架为AI聊天机器人提供了一种有效的实时安全防护机制，能够检测和预防对话中的隐性情感伤害"}}
{"id": "2512.06192", "pdf": "https://arxiv.org/pdf/2512.06192", "abs": "https://arxiv.org/abs/2512.06192", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Temma Suzuki", "Keita Yoneda", "Kei Okada"], "title": "REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation", "categories": ["cs.RO"], "comment": "Accepted on Advanced Intelligent Systems", "summary": "Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, \"Remote Wire Drive.\" As a proof-of-concept, we designed and developed the Remote Wire-Driven robot \"REWW-ARM\", which consists of the following components: 1) a novel power transmission mechanism, the \"Remote Wire Transmission Mechanism\" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.", "AI": {"tldr": "提出并验证了一种新型远程线驱动系统REWW-ARM，将电子设备与操作环境分离，同时保留电子控制和驱动能力", "motivation": "电子设备对机器人至关重要但限制了其使用环境，需要一种方法在排除操作环境中电子设备的同时保留先进的电子控制和驱动能力", "method": "设计了远程线驱动系统REWW-ARM，包括：1）新型远程线传输机制（RWTM）；2）无电子设备的远端移动机器人；3）电机单元，通过RWTM提供动力和基于状态估计的电子闭环控制", "result": "通过多项实验评估了REWW-ARM的机械和控制性能，展示了其在陆地和水下的移动、姿态控制和物体操作能力", "conclusion": "远程线驱动系统有潜力应用于各种类型的机器人，从而扩展其操作范围"}}
{"id": "2512.06190", "pdf": "https://arxiv.org/pdf/2512.06190", "abs": "https://arxiv.org/abs/2512.06190", "authors": ["Shichen Li", "Ahmadreza Eslaminia", "Chenhui Shao"], "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.", "AI": {"tldr": "开发一种多模态零样本预测方法，用于预测食品干燥过程中颜色的动态轨迹，能够泛化到未见过的干燥条件", "motivation": "现有食品干燥颜色变化研究主要依赖低维颜色特征，无法完全捕捉复杂的动态颜色轨迹，且现有建模方法缺乏对未见过程条件的泛化能力", "method": "提出新颖的多模态颜色轨迹预测方法，整合高维时序颜色信息与干燥过程参数，实现准确且数据高效的颜色轨迹预测", "result": "在未见干燥条件下，模型在饼干干燥中达到RMSE 2.12，苹果干燥中达到RMSE 1.29，相比基线模型误差减少超过90%", "conclusion": "该方法在准确性、鲁棒性和广泛适用性方面表现出优越性能，为食品干燥质量监控提供了有效工具"}}
{"id": "2512.06185", "pdf": "https://arxiv.org/pdf/2512.06185", "abs": "https://arxiv.org/abs/2512.06185", "authors": ["Ankit Gupta", "Christoph Adami", "Emily Dolson"], "title": "SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling", "categories": ["cs.CV"], "comment": "10 pages with 8 figures, plus 13 pages and 16 figures of supplementary material", "summary": "Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the \"fooling images\" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.", "AI": {"tldr": "该论文重新研究了深度神经网络对非自然图像的过度自信问题，并提出了SPOOF攻击方法，这是一种简单高效的像素操作攻击，能够在现代架构上生成高置信度的欺骗图像。", "motivation": "深度神经网络在图像识别任务中表现出色，但对非自然图像仍表现出过度自信。作者重新审视了Nguyen等人（2015）的\"欺骗图像\"工作，发现即使在最先进的网络架构中，高置信度欺骗问题依然存在，特别是基于Transformer的ViT-B/16模型最为脆弱。", "method": "论文重新实现了基于CPPN和直接编码的进化欺骗攻击，并提出了SPOOF方法——一种简约、一致且更高效的黑盒攻击方法。SPOOF通过最小像素修改生成欺骗图像，计算成本大幅降低。", "result": "实验证实高置信度欺骗问题在现代网络中依然存在，ViT-B/16是最脆弱的模型，需要比卷积模型更少的查询就能实现近乎确定的错误分类。SPOOF能够以极少的像素修改生成无法识别的欺骗图像，计算效率显著提升。即使使用欺骗图像作为额外类别进行重新训练，SPOOF仍能通过稍高的查询预算持续欺骗模型。", "conclusion": "现代深度分类器在对抗非自然图像方面存在持续脆弱性。SPOOF攻击方法虽然简单，但能有效暴露这种脆弱性，且即使采用防御性重新训练，也只能提供部分抵抗。这表明需要更强大的防御机制来应对这类攻击。"}}
{"id": "2512.06182", "pdf": "https://arxiv.org/pdf/2512.06182", "abs": "https://arxiv.org/abs/2512.06182", "authors": ["Shuhao Qi", "Qiling Aori", "Luyao Zhang", "Mircea Lazar", "Sofie Haesaert"], "title": "Situation-Aware Interactive MPC Switching for Autonomous Driving", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.", "AI": {"tldr": "该论文提出了一种基于情境感知的交互式MPC切换策略，用于自动驾驶在交互交通场景中的平衡性能与计算开销。", "motivation": "自动驾驶在交互交通场景中需要不同复杂度的MPC控制器，高保真模型能实现更智能行为但计算成本高，而强交互场景相对较少，因此需要根据情境需求选择合适的控制器来平衡性能与计算开销。", "method": "首先对不同MPC公式的交互能力进行比较研究并建立层次化评估；然后开发基于神经网络的分类器，实现不同交互能力控制器之间的情境感知切换。", "result": "情境感知切换策略能够在罕见但关键的情况下激活最先进的交互式MPC以显著提升整体性能，同时在大多数场景中使用基础MPC显著降低计算负载。", "conclusion": "通过情境感知的控制器切换策略，能够在自动驾驶交互交通场景中有效平衡性能需求与计算效率，实现实用化的性能-计算开销权衡。"}}
{"id": "2512.06179", "pdf": "https://arxiv.org/pdf/2512.06179", "abs": "https://arxiv.org/abs/2512.06179", "authors": ["Shilin Hu", "Jingyi Xu", "Sagnik Das", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction", "categories": ["cs.CV"], "comment": null, "summary": "Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.", "AI": {"tldr": "提出一个基于物理的附着阴影检测框架，通过联合推理场景光照和几何关系来检测投射阴影和附着阴影", "motivation": "现有阴影检测方法主要针对投射阴影，缺乏专门的附着阴影数据集和模型。附着阴影对于定义物体三维结构和增强场景理解至关重要，因此需要填补这一研究空白", "method": "系统包含阴影检测模块（分别预测两种阴影类型）和光照估计模块（从检测到的阴影推断光照方向）。利用估计的光照方向结合表面法线，推导出几何一致的部分地图来识别可能自遮挡的区域，然后反馈给阴影预测进行细化，形成闭环推理过程", "result": "实验结果表明，这种迭代的几何-光照推理显著改善了附着阴影的检测，BER至少减少33%，同时保持了强大的完整阴影和投射阴影性能", "conclusion": "通过联合推理阴影与场景光照和几何的相互关系，提出了一个有效的附着阴影检测框架，并构建了包含1,458张图像的数据集，为附着阴影检测提供了新的研究基础"}}
{"id": "2512.06174", "pdf": "https://arxiv.org/pdf/2512.06174", "abs": "https://arxiv.org/abs/2512.06174", "authors": ["Shilin Hu", "Jingyi Xu", "Akshat Dave", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction", "categories": ["cs.CV"], "comment": null, "summary": "Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.", "AI": {"tldr": "提出了一种结合显式物理建模与深度学习的新颖阴影生成框架，通过单目3D几何先验和近似光照方向来生成物理一致的逼真阴影", "motivation": "现有基于深度学习的阴影生成方法很少使用显式物理建模，而阴影形成的物理原理（遮挡物阻挡光线）本应遵循几何和光照的物理规律", "method": "首先从单目RGB图像获取密集点云表示的近似3D几何并预测主导光照方向，基于阴影形成的物理原理计算初始阴影位置和形状，然后将这个物理基础的初始估计集成到扩散框架中进行细化和逼真化", "result": "在DESOBAV2数据集上训练，模型生成的阴影既视觉逼真又物理一致，特别是在复杂几何或模糊光照场景中优于现有方法", "conclusion": "成功将显式物理建模（几何和光照）嵌入到基于深度学习的阴影生成中，实现了物理一致的高质量阴影生成"}}
{"id": "2512.06172", "pdf": "https://arxiv.org/pdf/2512.06172", "abs": "https://arxiv.org/abs/2512.06172", "authors": ["Sheng Liu", "Panos Papadimitratos"], "title": "DEFEND: Poisoned Model Detection and Malicious Client Exclusion Mechanism for Secure Federated Learning-based Road Condition Classification", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC 2026)", "summary": "Federated Learning (FL) has drawn the attention of the Intelligent Transportation Systems (ITS) community. FL can train various models for ITS tasks, notably camera-based Road Condition Classification (RCC), in a privacy-preserving collaborative way. However, opening up to collaboration also opens FL-based RCC systems to adversaries, i.e., misbehaving participants that can launch Targeted Label-Flipping Attacks (TLFAs) and threaten transportation safety. Adversaries mounting TLFAs poison training data to misguide model predictions, from an actual source class (e.g., wet road) to a wrongly perceived target class (e.g., dry road). Existing countermeasures against poisoning attacks cannot maintain model performance under TLFAs close to the performance level in attack-free scenarios, because they lack specific model misbehavior detection for TLFAs and neglect client exclusion after the detection. To close this research gap, we propose DEFEND, which includes a poisoned model detection strategy that leverages neuron-wise magnitude analysis for attack goal identification and Gaussian Mixture Model (GMM)-based clustering. DEFEND discards poisoned model contributions in each round and adapts accordingly client ratings, eventually excluding malicious clients. Extensive evaluation involving various FL-RCC models and tasks shows that DEFEND can thwart TLFAs and outperform seven baseline countermeasures, with at least 15.78% improvement, with DEFEND remarkably achieving under attack the same performance as in attack-free scenarios.", "AI": {"tldr": "提出DEFEND机制，用于检测联邦学习中的中毒模型并排除恶意客户端，以保护基于联邦学习的道路状况分类系统免受定向标签翻转攻击", "motivation": "联邦学习在智能交通系统中应用广泛，但面临定向标签翻转攻击的威胁。现有防御措施无法在攻击下维持接近无攻击场景的性能水平，缺乏针对TLFA的特定模型行为检测和检测后的客户端排除机制", "method": "DEFEND包括中毒模型检测策略，利用神经元级幅度分析进行攻击目标识别和基于高斯混合模型的聚类。在每轮训练中丢弃中毒模型的贡献，相应调整客户端评分，最终排除恶意客户端", "result": "在各种FL-RCC模型和任务上的广泛评估表明，DEFEND能够抵御TLFA攻击，性能至少优于7个基线防御方法15.78%，在攻击下实现了与无攻击场景相同的性能水平", "conclusion": "DEFEND机制有效解决了联邦学习中针对道路状况分类的定向标签翻转攻击问题，通过中毒模型检测和恶意客户端排除，在攻击下保持了与无攻击场景相同的性能表现"}}
{"id": "2512.06171", "pdf": "https://arxiv.org/pdf/2512.06171", "abs": "https://arxiv.org/abs/2512.06171", "authors": ["Jessica Plassmann", "Nicolas Schuler", "Michael Schuth", "Georg von Freymann"], "title": "Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.", "AI": {"tldr": "开发自动化工作流程，为剪切散斑测量生成缺陷标注，支持弱监督缺陷检测训练", "motivation": "剪切散斑技术缺乏高质量标注数据集，手动标注劳动密集、主观性强且难以标准化，限制了工业应用", "method": "引入基于深度学习的自动化工作流程，从剪切散斑测量中生成缺陷标注，包括高分辨率分割和边界框标签", "result": "与专家标注数据对比评估显示，自动化标注具有足够准确性，能够支持弱监督训练", "conclusion": "该自动化工作流程减少了手动标注工作量，支持可扩展的数据集创建，为稳健的缺陷检测提供了解决方案"}}
{"id": "2512.06161", "pdf": "https://arxiv.org/pdf/2512.06161", "abs": "https://arxiv.org/abs/2512.06161", "authors": ["Gondy Leroy", "Prakash Bisht", "Sai Madhuri Kandula", "Nell Maltman", "Sydney Rice"], "title": "Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages", "summary": "Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.", "AI": {"tldr": "本文提出了一种透明可解释的机器学习方法，利用BioBERT分析非结构化临床文本，通过转移学习策略在自闭症谱系障碍检测中实现高性能。", "motivation": "自闭症谱系障碍诊断过程漫长且需求增加，现有机器学习模型多为黑盒且通常在单一数据集上训练，限制了其泛化能力和临床可信度。", "method": "使用BioBERT语言模型分析临床文本，训练模型标记行为描述并映射到诊断标准，然后分配最终标签。评估了两种转移学习策略：顺序训练和混合训练，并与黑盒方法进行比较。", "result": "透明模型表现优异，混合数据训练策略获得最佳结果（97%灵敏度，98%特异性）。顺序训练导致性能略有下降。黑盒模型表现较差（90%灵敏度，96%特异性）。", "conclusion": "透明方法优于黑盒方法，混合数据集训练应为首选策略，为神经发育诊断中更可信、可泛化且临床可操作的AI工具铺平了道路。"}}
{"id": "2512.06158", "pdf": "https://arxiv.org/pdf/2512.06158", "abs": "https://arxiv.org/abs/2512.06158", "authors": ["Su Sun", "Cheng Zhao", "Himangi Mittal", "Gaurav Mittal", "Rohith Kukkala", "Yingjie Victor Chen", "Mei Chen"], "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation", "categories": ["cs.CV"], "comment": "15 pages, 11 figures", "summary": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.", "AI": {"tldr": "提出Track4DGen框架，通过结合多视角视频扩散模型、基础点跟踪器和混合4D高斯泼溅重建器，从稀疏输入生成动态4D对象", "motivation": "从稀疏输入生成动态4D对象具有挑战性，需要同时保持外观和运动一致性，抑制伪影和时间漂移。现有方法仅依赖像素或潜在空间的视频扩散损失，缺乏明确的时间感知特征级跟踪指导", "method": "两阶段框架：第一阶段在多视角视频扩散生成器中注入跟踪器导出的运动先验，强制密集特征级点对应关系；第二阶段使用混合运动编码重建4D高斯泼溅，结合扩散特征和Hex-plane特征，并用4D球谐函数增强", "result": "在多个多视角视频生成和4D生成基准测试中超越基线方法，产生时间稳定、可文本编辑的4D资产，并创建了高质量数据集Sketchfab28用于基准测试", "conclusion": "通过显式注入跟踪器导出的运动先验到特征表示中，Track4DGen能够生成时间一致、跨视角连贯的动态4D对象，为4D生成领域提供了新方法"}}
{"id": "2512.06154", "pdf": "https://arxiv.org/pdf/2512.06154", "abs": "https://arxiv.org/abs/2512.06154", "authors": ["Barproda Halder", "Pasan Dissanayake", "Sanghamitra Dutta"], "title": "Learning Invariant Graph Representations Through Redundant Information", "categories": ["cs.LG", "cs.AI", "cs.IT"], "comment": null, "summary": "Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.", "AI": {"tldr": "该论文提出了一种基于部分信息分解(PID)的冗余信息引导的图表示学习方法(RIG)，用于学习不变图表示以实现分布外泛化。", "motivation": "现有基于经典信息论的图表示学习方法难以完全去除虚假成分，导致分布外泛化能力不足。需要更精确地识别和处理虚假子图与不变子图之间关于目标Y的冗余信息。", "method": "提出RIG框架：使用部分信息分解(PID)识别冗余信息；采用多级优化方法，交替进行冗余信息下界估计和最大化；同时分离虚假子图和因果子图。", "result": "在合成和真实世界图数据集上的实验表明，RIG框架在多种分布偏移下具有优越的泛化能力。", "conclusion": "部分信息分解(PID)为图表示学习提供了超越经典信息论的新工具，RIG框架通过精确处理冗余信息有效提升了分布外泛化性能。"}}
{"id": "2512.06151", "pdf": "https://arxiv.org/pdf/2512.06151", "abs": "https://arxiv.org/abs/2512.06151", "authors": ["Ratnangshu Das", "Siddhartha Upadhyay", "Pushpak Jagtap"], "title": "Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.", "AI": {"tldr": "提出了一种用于非线性纯反馈系统的实时时空管（STT）控制框架，在动态环境中实现规定时间内的到达-避障-停留任务", "motivation": "解决非线性纯反馈系统在未知动态和动态环境中的实时控制问题，确保在规定时间内完成到达-避障-停留任务，同时保证安全性", "method": "引入实时时空管（STT）框架，定义为状态空间中的时变球体，其中心和半径仅使用实时传感器输入在线自适应调整，并推导出闭式、无近似的控制律", "result": "提供了障碍物避障和按时任务完成的正式保证，通过移动机器人和空中车辆的仿真和硬件实验验证了框架的有效性和可扩展性", "conclusion": "提出的实时时空管框架能够有效处理动态环境中的非线性系统控制问题，为未知动态系统的安全任务完成提供了可靠的解决方案"}}
{"id": "2512.06147", "pdf": "https://arxiv.org/pdf/2512.06147", "abs": "https://arxiv.org/abs/2512.06147", "authors": ["Hochul Hwang", "Soowan Yang", "Jahir Sadik Monon", "Nicholas A Giudice", "Sunghoon Ivan Lee", "Joydeep Biswas", "Donghyun Kim"], "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": null, "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.", "AI": {"tldr": "开发一个仅使用视觉的机器人导航助手GuideNav，用于帮助盲人和低视力人士进行导航，该系统通过\"教学-重复\"的方式工作，无需依赖昂贵的传感器如LiDAR。", "motivation": "虽然针对盲人和低视力人士的移动辅助系统研究取得了进步，但直接指导机器人导航设计的参考仍然很少。为了填补这一空白，需要进行以用户为中心的研究来开发更有效的导航辅助系统。", "method": "首先进行了全面的人类研究，包括访谈26名导盲犬使用者、4名白手杖使用者、9名导盲犬训练师和1名定向行走训练师，并观察了15+小时的导盲犬辅助行走。基于这些见解，开发了GuideNav系统，这是一个仅使用视觉的\"教学-重复\"导航系统，通过构建拓扑路径表示、结合视觉地点识别与时间滤波，并使用相对姿态估计器来计算导航动作。", "result": "GuideNav在五个室外环境中实现了公里级的路径跟随，即使在教学和重复运行之间存在明显的场景变化也能保持可靠性。用户研究（3名导盲犬使用者和1名导盲犬训练师）进一步证实了系统的可行性，这是首次展示四足移动系统以类似于导盲犬的方式检索路径。", "conclusion": "GuideNav展示了仅使用视觉的机器人导航系统可以有效帮助盲人和低视力人士进行导航，其设计灵感来自导盲犬的训练和工作方式，为开发更实用、成本效益更高的辅助系统提供了新的方向。"}}
{"id": "2512.06137", "pdf": "https://arxiv.org/pdf/2512.06137", "abs": "https://arxiv.org/abs/2512.06137", "authors": ["Alan Chen", "Tejas Kotwal", "Govind Menon"], "title": "Entropic Regularization in the Deep Linear Network", "categories": ["cs.NE", "math.DS", "math.PR"], "comment": ":37N40; 53C20; 15A18; 68T07", "summary": "We study regularization for the deep linear network (DLN) using the entropy formula introduced in arXiv:2509.09088. The equilibria and gradient flow of the free energy on the Riemannian manifold of end-to-end maps of the DLN are characterized for energies that depend symmetrically on the singular values of the end-to-end matrix. The only equilibria are minimizers and the set of minimizers is an orbit of the orthogonal group. In contrast with random matrix theory there is no singular value repulsion. The corresponding gradient flow reduces to a one-dimensional ordinary differential equation whose solution gives explicit relaxation rates toward the minimizers. We also study the concavity of the entropy in the chamber of singular values. The entropy is shown to be strictly concave in the Euclidean geometry on the chamber but not in the Riemannian geometry defined by the DLN metric.", "AI": {"tldr": "研究深度线性网络中基于熵公式的正则化方法，分析自由能在黎曼流形上的平衡点、梯度流和松弛速率", "motivation": "探索深度线性网络中熵正则化的数学特性，理解在黎曼几何框架下正则化对网络训练动态的影响", "method": "使用arXiv:2509.09088引入的熵公式，在深度线性网络的黎曼流形上分析自由能的平衡点和梯度流，研究奇异值对称依赖的能量函数", "result": "发现只有最小化点是平衡点，最小化点集是正交群的轨道；与随机矩阵理论不同，没有奇异值排斥现象；梯度流简化为常微分方程，给出显式松弛速率；熵在奇异值室中欧几里得几何下严格凹，但在DLN度量的黎曼几何下不凹", "conclusion": "深度线性网络中的熵正则化具有独特的数学特性，其训练动态可通过简化的一维常微分方程描述，为理解深度网络的正则化机制提供了理论框架"}}
{"id": "2512.06134", "pdf": "https://arxiv.org/pdf/2512.06134", "abs": "https://arxiv.org/abs/2512.06134", "authors": ["Georgi Hrusanov", "Duy-Thanh Vu", "Duy-Cat Can", "Sophie Tascedda", "Margaret Ryan", "Julien Bodelet", "Katarzyna Koscielska", "Carsten Magnus", "Oliver Y. Chén"], "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.", "AI": {"tldr": "提出一种基于物理信息神经Koopman机器（NKM）的阿尔茨海默病认知衰退预测框架，通过融合多模态数据和注意力机制实现个性化、可解释的纵向预测。", "motivation": "现有方法难以在保持可解释性的同时整合多模态数据进行阿尔茨海默病的纵向个性化预测，需要一种能够同时预测多个认知评分并揭示生物标志物贡献的新框架。", "method": "提出神经Koopman机器（NKM）架构，结合动态系统和注意力机制，利用融合组感知分层注意力在Koopman算子框架内将非线性轨迹转化为可解释的线性表示，整合分析知识和生物学知识指导特征分组。", "result": "在ADNI数据集上的实验表明，NKM在预测认知衰退轨迹方面优于传统机器学习和深度学习模型，能够同时预测多个认知评分变化，量化不同生物标志物的贡献，并识别与认知恶化最相关的大脑区域。", "conclusion": "NKM通过可解释的显式系统推进了阿尔茨海默病的个性化预测，揭示了疾病进展的多模态生物学基础，为疾病评估和管理提供了新工具。"}}
{"id": "2512.06130", "pdf": "https://arxiv.org/pdf/2512.06130", "abs": "https://arxiv.org/abs/2512.06130", "authors": ["Grant Stagg", "Isaac E. Weintraub", "Cameron K. Peterson"], "title": "Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer", "categories": ["cs.RO", "eess.SY"], "comment": "Accepted for presentation at AIAA SciTech 2026. 17 pages, 7 figures", "summary": "Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.", "AI": {"tldr": "该论文提出了曲线-直线概率交战区域（CSPEZ）的概念，用于量化规避者应避免的空间区域，以降低受到转弯率受限追击者捕获的风险，并开发了在不确定性下最小化捕获风险的轨迹生成方法。", "motivation": "在实际追击-规避场景中，追击者通常存在位置、航向、速度、射程和最大转弯率等多种不确定性。传统确定性交战区域无法有效处理这些不确定性，需要开发概率性框架来量化捕获风险，从而生成更安全的规避轨迹。", "method": "首先推导了确定性曲线-直线基本交战区域（CSBEZ）的解析解，然后扩展到概率框架，采用了四种不确定性传播方法：蒙特卡洛采样、线性化、二次近似和神经网络回归。最后将CSPEZ约束集成到轨迹优化算法中。", "result": "评估了四种近似方法的准确性和计算成本，并展示了CSPEZ约束如何能够集成到轨迹优化算法中，生成明确考虑追击者不确定性的安全路径。不同方法在精度和效率之间存在权衡。", "conclusion": "提出的CSPEZ框架能够有效量化在追击者参数不确定性下的捕获风险，为规避者提供概率性安全区域。集成CSPEZ约束的轨迹优化算法能够生成考虑不确定性的安全路径，为实际追击-规避场景提供了实用的解决方案。"}}
{"id": "2512.06123", "pdf": "https://arxiv.org/pdf/2512.06123", "abs": "https://arxiv.org/abs/2512.06123", "authors": ["Qilin Zhou", "Zhengyuan Wei", "Haipeng Wang", "Zhuo Wang", "W. K. Chan"], "title": "Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "accepted by IEEE Transactions on Reliability; extended technical report", "summary": "Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.", "AI": {"tldr": "提出HiCert方法，一种基于掩码的认证检测技术，用于深度学习的补丁鲁棒性认证，能够系统性地认证不一致样本和一致样本", "motivation": "现有的认证检测方法无法有效认证那些被错误分类或其突变体被不一致预测到不同标签的样本，这限制了补丁鲁棒性认证的全面性", "method": "通过形式化分析，HiCert建立了有害样本与其良性对应物之间的形式关系，通过检查每个良性样本的潜在有害突变体的最大置信度边界，确保每个有害样本要么具有低于该边界的相同预测标签突变体的最小置信度，要么至少有一个突变体被预测为与有害样本本身不同的标签", "result": "HiCert在实验中表现出高效性，实现了新的最先进性能：认证了显著更多的良性样本（包括不一致和一致样本），在没有警告的样本上实现了显著更高的准确率，以及显著更低的虚假沉默比率", "conclusion": "HiCert是第一个能够提供如此全面的补丁鲁棒性认证用于认证检测的工作，通过系统性地认证不一致样本和一致样本，显著提高了深度学习系统对抗对抗性补丁攻击的防御能力"}}
{"id": "2512.06112", "pdf": "https://arxiv.org/pdf/2512.06112", "abs": "https://arxiv.org/abs/2512.06112", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "18 pages, 11 figures", "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "AI": {"tldr": "WAM-Flow是一种用于自动驾驶的视觉-语言-动作模型，通过离散流匹配在结构化令牌空间中进行并行粗到细运动规划", "motivation": "传统自回归解码器在轨迹规划中存在顺序解码的局限性，需要一种能够并行生成、支持可调节计算精度权衡的方法，同时整合安全、进度和舒适度等多目标优化", "method": "采用离散流匹配框架，结合度量对齐数值分词器（通过三元组边界学习保持几何信息）、几何感知流目标函数和模拟器引导的GRPO对齐方法，实现并行粗到细轨迹生成", "result": "在NAVSIM v1基准测试中，1步推理达到89.1 PDMS，5步推理达到90.3 PDMS，性能优于自回归和基于扩散的VLA基线模型", "conclusion": "离散流匹配为端到端自动驾驶提供了一个有前景的新范式，通过并行解码和一致性模型训练实现了优越的闭环性能"}}
{"id": "2512.06109", "pdf": "https://arxiv.org/pdf/2512.06109", "abs": "https://arxiv.org/abs/2512.06109", "authors": ["Ajinkya Bhole", "Mohammad Mahmoudi Filabadi", "Guillaume Crevecoeur", "Tom Lefebvre"], "title": "Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions", "categories": ["math.OC", "cs.LG", "cs.RO", "eess.SY"], "comment": null, "summary": "This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.", "AI": {"tldr": "本文通过Kullback-Leibler正则化统一了多种随机最优控制问题，提出了一个分离策略和转移概率KL惩罚的广义框架，能够恢复经典随机最优控制、风险敏感控制及其策略正则化版本。", "motivation": "现有随机最优控制问题存在多种不同表述，缺乏统一的理论框架。本文旨在通过KL正则化建立统一视角，揭示不同控制问题之间的内在联系，并探索具有良好计算性质的特殊情况。", "method": "提出一个中心问题，将策略和转移概率的KL惩罚分离并赋予独立权重，建立广义KL正则化框架。通过调整权重参数，可以恢复经典SOC、RSOC及其策略正则化版本。特别关注策略和转移概率KL权重相等的结构同步情况。", "result": "证明了软策略SOC和RSOC是原始SOC和RSOC的上界，可以通过迭代正则化解恢复原始解。在结构同步情况下，风险寻求的软策略RSOC具有线性Bellman方程、路径积分解和组合性等强大性质，扩展了这些计算友好性质到更广泛的控制问题。", "conclusion": "KL正则化为随机最优控制提供了统一框架，不仅连接了不同控制问题，还揭示了具有良好计算性质的特殊情况。结构同步的软策略RSOC特别重要，因为它继承了路径积分控制的有利性质，为更广泛的控制问题提供了高效计算方法。"}}
{"id": "2512.06108", "pdf": "https://arxiv.org/pdf/2512.06108", "abs": "https://arxiv.org/abs/2512.06108", "authors": ["Botao 'Amber' Hu", "Samuel Chua", "Helena Rong"], "title": "Protocol Futuring: Speculating Second-Order Dynamics of Protocols in Sociotechnical Infrastructural Futures", "categories": ["cs.CY", "cs.HC"], "comment": "Submitted to CHI 2026. Under review", "summary": "Drawing on infrastructure studies in HCI and CSCW, this paper introduces Protocol Futuring, a methodological framework that extends design futuring by foregrounding protocols-rules, standards, and coordination mechanisms-as the primary material of speculative inquiry. Rather than imagining discrete future artifacts, Protocol Futuring examines how protocol rules accumulate drift, jam, and other second-order effects over long temporal horizons. We demonstrate the method through a case study of Knowledge Futurama, a multi-team participatory workshop exploring millennial-scale knowledge preservation. Using a relay format in which teams inherited and reinterpreted partially formed designs, the workshop revealed how ambiguous handovers, adversarial reinterpretations, shifting cultural norms, and crisis dynamics transform protocols as they move across communities and epochs. The case shows how Protocol Futuring makes infrastructural politics and long-run consequences analytically visible. We discuss the method's strengths, limitations, and implications for researchers seeking to investigate emergent sociotechnical systems whose impacts unfold over extended timescales.", "AI": {"tldr": "提出Protocol Futuring方法框架，将协议作为推测性探究的主要材料，研究协议规则在长时间尺度上的二阶效应", "motivation": "现有设计未来方法主要关注离散的未来人工制品，而忽略了协议规则在长时间尺度上的积累性效应和基础设施政治", "method": "通过Knowledge Futurama案例研究，采用多团队参与式工作坊和接力形式，让团队继承和重新解释部分形成的设计", "result": "工作坊揭示了模糊交接、对抗性重新解释、文化规范转变和危机动态如何在不同社区和时代间转变协议", "conclusion": "Protocol Futuring方法使基础设施政治和长期后果在分析上可见，为研究长期影响的社会技术系统提供了方法论工具"}}
{"id": "2512.06106", "pdf": "https://arxiv.org/pdf/2512.06106", "abs": "https://arxiv.org/abs/2512.06106", "authors": ["Constanze Albrecht", "Chayapatr Archiwaranguprok", "Rachel Poonsiriwong", "Awu Chen", "Peggy Yin", "Monchai Lertsutthiwong", "Kavin Winson", "Hal Hershfield", "Pattie Maes", "Pat Pataranutaporn"], "title": "Future You: Designing and Evaluating Multimodal AI-generated Digital Twins for Strengthening Future Self-Continuity", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "What if users could meet their future selves today? AI-generated future selves simulate meaningful encounters with a digital twin decades in the future. As AI systems advance, combining cloned voices, age-progressed facial rendering, and autobiographical narratives, a central question emerges: Does the modality of these future selves alter their psychological and affective impact? How might a text-based chatbot, a voice-only system, or a photorealistic avatar shape present-day decisions and our feeling of connection to the future? We report a randomized controlled study (N=92) evaluating three modalities of AI-generated future selves (text, voice, avatar) against a neutral control condition. We also report a systematic model evaluation between Claude 4 and three other Large Language Models (LLMs), assessing Claude 4 across psychological and interaction dimensions and establishing conversational AI quality as a critical determinant of intervention effectiveness. All personalized modalities strengthened Future Self-Continuity (FSC), emotional well-being, and motivation compared to control, with avatar producing the largest vividness gains, yet with no significant differences between formats. Interaction quality metrics, particularly persuasiveness, realism, and user engagement, emerged as robust predictors of psychological and affective outcomes, indicating that how compelling the interaction feels matters more than the form it takes. Content analysis found thematic patterns: text emphasized career planning, while voice and avatar facilitated personal reflection. Claude 4 outperformed ChatGPT 3.5, Llama 4, and Qwen 3 in enhancing psychological, affective, and FSC outcomes.", "AI": {"tldr": "研究评估了三种不同模态（文本、语音、头像）的AI生成未来数字孪生对增强未来自我连续性的心理影响，并比较了不同大语言模型的性能。", "motivation": "随着AI系统的发展，结合克隆声音、年龄进展面部渲染和自传式叙事，关键问题出现：这些未来自我的不同模态是否会改变其心理和情感影响？不同形式的数字孪生如何影响当前决策和对未来的连接感？", "method": "采用随机对照研究（N=92），评估三种AI生成未来自我模态（文本、语音、头像）与中性对照条件。同时系统评估了Claude 4与其他三个大语言模型在心理和交互维度上的表现。", "result": "所有个性化模态都增强了未来自我连续性、情感幸福感和动机，头像模态在生动性方面提升最大，但不同格式间无显著差异。交互质量指标（特别是说服力、真实感和用户参与度）是心理和情感结果的有力预测因素。Claude 4在增强心理、情感和未来自我连续性结果方面优于其他模型。", "conclusion": "交互的吸引力比形式更重要，高质量的对话AI是干预效果的关键决定因素。不同模态有不同主题模式：文本强调职业规划，而语音和头像促进个人反思。"}}
{"id": "2512.06105", "pdf": "https://arxiv.org/pdf/2512.06105", "abs": "https://arxiv.org/abs/2512.06105", "authors": ["Junwen Zheng", "Xinran Xu", "Li Rong Wang", "Chang Cai", "Lucinda Siyun Tan", "Dingyuan Wang", "Hong Liang Tey", "Xiuyi Fan"], "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI-26-AIA", "summary": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.", "AI": {"tldr": "提出一种基于对比学习和LLM报告生成的可解释性黑色素瘤诊断框架，通过将临床诊断标准（ABC规则）映射到视觉特征空间，实现高性能分类与临床可解释性的统一。", "motivation": "深度学习在黑色素瘤分类中已达到专家水平，但模型不透明和缺乏可解释性阻碍了临床采用，临床医生难以信任黑盒模型的决策过程。", "method": "提出跨模态可解释性黑色素瘤框架（CEFM），利用对比学习作为核心机制实现可解释性。通过双投影头将临床诊断标准（不对称性、边界、颜色）映射到Vision Transformer嵌入空间，对齐临床语义与视觉特征，然后通过自然语言生成将对齐表示转换为结构化文本解释。", "result": "在公共数据集上达到92.79%的准确率和0.961的AUC，在多个可解释性指标上有显著提升。定性分析显示学习到的嵌入空间排列与临床医生应用ABC规则的方式一致。", "conclusion": "该框架有效弥合了高性能分类与临床信任之间的差距，通过将临床诊断标准直接嵌入到视觉表示学习中，为黑色素瘤诊断提供了透明、可解释的决策过程。"}}
{"id": "2512.06103", "pdf": "https://arxiv.org/pdf/2512.06103", "abs": "https://arxiv.org/abs/2512.06103", "authors": ["Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection", "categories": ["cs.CV"], "comment": "Accepted in IEEE T-BIOM", "summary": "Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \\textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\\,nm, 830\\,nm, 850\\,nm, 870\\,nm, and 980\\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.", "AI": {"tldr": "提出SpectraIrisPAD框架，利用视觉基础模型进行光谱条件多光谱虹膜呈现攻击检测，通过DINOv2 Vision Transformer结合可学习光谱位置编码、令牌融合和对比学习来提取区分性特征。", "motivation": "虹膜识别虽然准确，但在实际应用中容易受到呈现攻击的威胁。传统虹膜识别主要在近红外光谱工作，而多光谱成像能提供互补的反射信息，增强呈现攻击检测方法的泛化能力。", "method": "使用DINOv2 Vision Transformer作为骨干网络，配备可学习光谱位置编码、令牌融合和对比学习机制，提取区分性的波段特定特征。同时创建了包含5个不同近红外波长的多光谱虹膜呈现攻击检测数据集MSIrPAD。", "result": "在未见攻击评估协议下，SpectraIrisPAD在所有性能指标上均优于多个最先进的基线方法，展现出卓越的鲁棒性和泛化能力，能够有效检测多种呈现攻击。", "conclusion": "提出的SpectraIrisPAD框架通过利用视觉基础模型和多光谱信息，显著提升了虹膜呈现攻击检测的性能和泛化能力，为虹膜生物识别系统的安全性提供了有效保障。"}}
{"id": "2512.06102", "pdf": "https://arxiv.org/pdf/2512.06102", "abs": "https://arxiv.org/abs/2512.06102", "authors": ["Ufuk Çakır", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "To be presented at the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS)", "summary": "Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.", "AI": {"tldr": "JaxWildfire是一个基于GPU加速的野火模拟器，专门为强化学习训练设计，使用JAX实现并支持向量化模拟", "motivation": "现有野火模拟器速度较慢，限制了强化学习代理的训练效率，因为RL训练需要大量的环境交互", "method": "基于细胞自动机的概率性火势传播模型，使用JAX实现并利用vmap进行向量化模拟，支持GPU加速", "result": "相比现有软件实现6-35倍加速，支持基于梯度的模拟器参数优化，并能用于训练RL代理学习野火抑制策略", "conclusion": "该工作是推进强化学习技术在自然灾害管理应用中的重要一步，为RL训练提供了高效的模拟环境"}}
{"id": "2512.06097", "pdf": "https://arxiv.org/pdf/2512.06097", "abs": "https://arxiv.org/abs/2512.06097", "authors": ["Emre Umucu", "Guillermina Solis", "Leon Garza", "Emilia Rivas", "Beatrice Lee", "Anantaa Kotal", "Aritran Piplai"], "title": "Empathy by Design: Aligning Large Language Models for Healthcare Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: https://github.com/LeonG19/Empathy-by-Design", "AI": {"tldr": "开发基于直接偏好优化（DPO）的框架，用于在医疗保健对话中提升大型语言模型的事实准确性、语义连贯性和同理心等人性化特质", "motivation": "通用大型语言模型在医疗保健和护理应用中存在两个关键缺陷：事实不可靠性和缺乏同理心沟通，这在敏感情境下对非专业人士和护理人员寻求医疗指导或情感安慰构成重大风险", "method": "采用直接偏好优化（DPO）对齐框架，使用成对偏好数据微调领域适应的LLMs，其中偏好回复反映支持性和可访问的沟通风格，而被拒绝的回复代表规定性或过于技术化的语调", "result": "经验评估显示，DPO调优模型相比基线和商业替代品（如谷歌医疗对话系统）实现了更高的语义对齐、改进的事实准确性和更强的人性化评估分数", "conclusion": "基于偏好的对齐为开发可信赖、有同理心且具备临床知识的AI助手提供了可扩展且透明的途径，适用于护理人员和医疗保健沟通"}}
{"id": "2512.06096", "pdf": "https://arxiv.org/pdf/2512.06096", "abs": "https://arxiv.org/abs/2512.06096", "authors": ["Karthik Mohan", "Sonam Singh", "Amit Arvind Kale"], "title": "BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.", "AI": {"tldr": "BeLLA是一个端到端的鸟瞰图大语言助手，用于自动驾驶场景中的问答任务，通过将统一的360度BEV表示与大型语言模型连接来实现空间推理。", "motivation": "现有视觉语言模型在自动驾驶研究中存在局限性：单视图编码器无法利用多摄像头系统的空间结构，而聚合多视图特征缺乏统一的空间表示，难以进行自我中心方向、物体关系和上下文推理。", "method": "提出端到端架构，将统一的360度鸟瞰图表示与大型语言模型连接，用于自动驾驶问答。利用BEV表示提供统一的空间参考框架，增强空间推理能力。", "result": "在NuScenes-QA和DriveLM基准测试中，BeLLA在需要空间推理的任务上（如相对物体定位和行为理解）表现优异，某些任务上获得+9.3%的绝对提升，在其他类别中也具有竞争力。", "conclusion": "BeLLA通过结合统一BEV表示和语言模型，显著提升了自动驾驶场景中的空间推理能力，为更丰富的场景理解、上下文感知推理和可解释决策提供了有效解决方案。"}}
{"id": "2512.06080", "pdf": "https://arxiv.org/pdf/2512.06080", "abs": "https://arxiv.org/abs/2512.06080", "authors": ["Tzofi Klinghoffer", "Siddharth Somasundaram", "Xiaoyu Xiang", "Yuchen Fan", "Christian Richardt", "Akshat Dave", "Ramesh Raskar", "Rakesh Ranjan"], "title": "Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light", "categories": ["cs.CV"], "comment": "SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io", "summary": "3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.", "AI": {"tldr": "提出一种基于单光子激光雷达的单次测量3D重建方法，通过分解双反射光来恢复遮挡区域和镜面反射场景的几何信息", "motivation": "单次测量下的3D场景重建在存在遮挡区域和镜面材料（如镜子）时具有挑战性，现有方法需要逐点扫描，而多路复用照明下的光传输复杂性难以解析反演", "method": "使用数据驱动方法反演单光子激光雷达中的光传输，创建首个大规模室内场景激光雷达瞬态数据集（约10万个样本），学习复杂光传输先验，将测量的双反射光分解为每个激光点的组成贡献", "result": "通过实验证明分解后的光可用于从单次测量中推断具有遮挡和镜子的场景的3D几何形状，实现了在复杂场景下的单次测量3D重建", "conclusion": "该方法通过利用多反射光中的额外信息，成功解决了单次测量下遮挡和镜面场景的3D重建问题，为实际应用提供了更实用的解决方案"}}
{"id": "2512.06065", "pdf": "https://arxiv.org/pdf/2512.06065", "abs": "https://arxiv.org/abs/2512.06065", "authors": ["Runjia Li", "Moayed Haji-Ali", "Ashkan Mirzaei", "Chaoyang Wang", "Arpit Sahni", "Ivan Skorokhodov", "Aliaksandr Siarohin", "Tomas Jakab", "Junlin Han", "Sergey Tulyakov", "Philip Torr", "Willi Menapace"], "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://snap-research.github.io/EgoEdit", "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "AI": {"tldr": "提出EgoEdit系统，一个用于第一人称视角视频编辑的完整生态系统，包括数据集、实时流式推理模型和评估基准，专门解决第一人称视频编辑中的独特挑战。", "motivation": "现有AI视频编辑器在第三人称视频上表现良好，但第一人称视角视频存在独特挑战（如快速自我运动和频繁的手-物体交互），导致显著的领域差距。此外，现有的离线编辑流程存在高延迟问题，限制了实时交互。", "method": "1) 构建EgoEditData数据集：专门为第一人称编辑场景设计的手动策划数据集，包含丰富的手-物体交互，并明确保留手部信息；2) 开发EgoEdit模型：支持单GPU实时流式推理的指令跟随第一人称视频编辑器；3) 创建EgoEditBench评估套件：针对指令忠实度、手部和交互保留、以及自我运动下的时间稳定性进行评估。", "result": "EgoEdit在时间稳定性、指令忠实度和交互延迟方面表现优异。在第一人称编辑基准测试中取得明显优势（现有方法在此类任务上表现不佳），同时在通用编辑任务上保持与最强基线相当的性能。", "conclusion": "提出了一个完整的第一人称视频编辑生态系统，包括专门的数据集、实时流式推理模型和评估基准，有效解决了第一人称视频编辑中的独特挑战，为交互式AR应用提供了实用解决方案。"}}
{"id": "2512.06062", "pdf": "https://arxiv.org/pdf/2512.06062", "abs": "https://arxiv.org/abs/2512.06062", "authors": ["S. M. Mustaqim", "Anantaa Kotal", "Paul H. Yi"], "title": "When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.", "AI": {"tldr": "提出一种针对生成式AI模型的成员推断攻击方法，通过分析合成数据中的聚类结构来推断原始训练数据的成员信息", "motivation": "生成模型被广泛用于生成保护隐私的合成数据，但现有方法主要关注样本级别的记忆，忽略了数据流形中的结构重叠可能导致的信息泄露风险", "method": "提出黑盒成员推断攻击方法：1）重复查询生成模型获取大量合成样本；2）进行无监督聚类识别合成分布的密集区域；3）分析聚类中心点和邻域作为原始训练数据的代理，推断成员信息或重建近似记录", "result": "在医疗、金融等敏感领域的实验表明，即使生成器使用差分隐私或其他噪声机制训练，真实数据与合成数据之间的聚类重叠仍会导致可测量的成员信息泄露", "conclusion": "揭示了合成数据生成管道中未被充分探索的攻击面，需要更强的隐私保证机制，不仅要考虑样本级别的记忆，还要考虑分布邻域推断问题"}}
{"id": "2512.06060", "pdf": "https://arxiv.org/pdf/2512.06060", "abs": "https://arxiv.org/abs/2512.06060", "authors": ["Mohanakrishnan Hariharan"], "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.", "AI": {"tldr": "该论文提出了一个将强化学习与自主智能体相结合的框架，用于从业务需求文档自动生成软件测试用例，并通过质量工程反馈实现持续改进。", "motivation": "传统基于大型语言模型的测试用例生成系统依赖静态知识库，无法随时间提升性能。需要一种能够从质量工程反馈中学习并持续改进的自动化测试生成方法。", "method": "提出强化学习增强的智能体RAG框架，结合专用智能体和混合向量-图知识库存储测试知识，使用PPO和DQN算法根据测试有效性、缺陷检测率等指标优化智能体行为。", "result": "在企业级苹果项目上的实验验证显示：测试生成准确率从94.8%提升至97.2%（提高2.4%），缺陷检测率提升10.8%。", "conclusion": "该框架建立了由质量工程专业知识驱动的持续知识精炼循环，能够逐步提升测试用例质量，增强而非替代人工测试能力。"}}
{"id": "2512.06058", "pdf": "https://arxiv.org/pdf/2512.06058", "abs": "https://arxiv.org/abs/2512.06058", "authors": ["Siming Yan"], "title": "Representation Learning for Point Cloud Understanding", "categories": ["cs.CV"], "comment": "181 pages", "summary": "With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.", "AI": {"tldr": "该论文研究点云理解中的表示学习，重点探索监督学习、自监督学习和从2D到3D的迁移学习三种方法，通过整合预训练的2D模型来提升3D网络训练效果。", "motivation": "随着3D数据获取技术的快速发展（如3D扫描仪、LiDAR、RGB-D相机），3D数据在计算机视觉、机器人、地理空间分析等领域应用日益广泛。3D数据提供丰富的几何、形状和尺度信息，结合2D图像可为机器提供更全面的环境理解，对自动驾驶、机器人、遥感和医疗等应用至关重要。", "method": "论文采用三种主要方法：1）监督表示学习用于点云基元分割；2）自监督学习方法；3）从2D到3D的迁移学习。核心方法是整合预训练的2D模型来支持3D网络训练，而不是简单地将2D数据转换为3D。", "result": "通过大量实验验证了所提方法的有效性，展示了这些方法在提升3D理解能力方面的潜力。整合2D知识的方法显著改善了3D网络训练效果，推动了点云表示学习的发展。", "conclusion": "该论文提出的整合预训练2D模型支持3D网络训练的方法，能够有效提升点云表示学习效果，为点云理解领域提供了新的研究方向，展示了2D知识在3D任务中的迁移价值。"}}
{"id": "2512.06048", "pdf": "https://arxiv.org/pdf/2512.06048", "abs": "https://arxiv.org/abs/2512.06048", "authors": ["Sahil Garg"], "title": "The Road of Adaptive AI for Precision in Cybersecurity", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.", "AI": {"tldr": "该论文探讨了在网络安全领域设计和部署生产级生成式AI（GenAI）管道的实践经验，重点关注AI系统如何通过持续自适应来应对不断变化的知识库、工具和威胁。", "motivation": "网络安全环境的快速演变和日益复杂性对AI系统提出了独特挑战，需要AI能够持续适应新的威胁、工具和知识，以保持防御的有效性和精确性。", "method": "论文基于真实世界部署经验，提出了结合检索级和模型级自适应机制的方法，构建端到端的自适应AI系统，并分享了设计、构建和运营生产级GenAI管道的实践指导。", "result": "论文提供了从实际部署中提炼的实用指导，提出了利用检索和模型级自适应的最佳实践，并展示了不同自适应机制在端到端系统中如何相互补充。", "conclusion": "自适应AI对于网络安全防御的精确性至关重要，论文为AI从业者和行业利益相关者提供了可操作的视角，并指出了使GenAI在网络安全中更加稳健、精确和可审计的开放研究方向。"}}
{"id": "2512.06046", "pdf": "https://arxiv.org/pdf/2512.06046", "abs": "https://arxiv.org/abs/2512.06046", "authors": ["Ramprasath Ganesaraja", "Swathika N", "Saravanan AP", "Kamalkumar Rathinasamy", "Chetana Amancharla", "Rahul Das", "Sahil Dilip Panse", "Aditya Batwe", "Dileep Vijayan", "Veena Ashok", "Thanushree A P", "Kausthubh J Rao", "Alden Olivero", "Roshan", "Rajeshwar Reddy Manthena", "Asmitha Yuga Sre A", "Harsh Tripathi", "Suganya Selvaraj", "Vito Chin", "Kasthuri Rangan Bhaskar", "Kasthuri Rangan Bhaskar", "Venkatraman R", "Sajit Vijayakumar"], "title": "Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework", "categories": ["cs.SE", "cs.AI"], "comment": "17 pages, 9 figures", "summary": "We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline", "AI": {"tldr": "AI4UI是一个面向企业级前端开发的自主多智能体框架，能够将设计原型转换为生产就绪的UI代码，专注于安全性、可扩展性、合规性和可维护性。", "motivation": "现有通用代码助手主要面向快速原型开发，无法满足企业级应用对生产就绪代码的严格要求，需要专门针对企业级前端开发需求构建的自主框架。", "method": "采用专门的多智能体框架，包括：1）Figma语法用于自主解释设计需求；2）领域感知知识图谱；3）安全的抽象/包代码集成策略；4）专业知识驱动的架构模板；5）由专门智能体角色协调的变更导向工作流。", "result": "在大型基准测试中，AI4UI实现了97.24%的平台兼容性、87.10%的编译成功率、86.98%的安全合规性、78.00%的功能实现成功率、73.50%的代码审查质量和73.36%的UI/UX一致性。在200名专家评估者的盲选偏好研究中表现出色。", "conclusion": "AI4UI框架能够将企业级前端开发从数月压缩到数周，生成数千个经过验证的UI屏幕，在保持人类参与关键环节的同时实现高度自主化，为企业级应用交付提供了生产就绪的解决方案。"}}
{"id": "2512.06042", "pdf": "https://arxiv.org/pdf/2512.06042", "abs": "https://arxiv.org/abs/2512.06042", "authors": ["Ashish Hooda", "Mihai Christodorescu", "Chuangang Ren", "Aaron Wilson", "Kassem Fawaz", "Somesh Jha"], "title": "Auto-SPT: Automating Semantic Preserving Transformations for Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.", "AI": {"tldr": "Auto-SPT是一个自动化生成代码语义保持变换的框架，用于增强代码克隆检测模型的鲁棒性", "motivation": "现有的代码克隆检测模型主要在干净、结构化的代码数据集上训练，而现实世界的代码经常经历各种语义保持变换（如重构、压缩、格式化、编译器优化），导致训练数据和测试数据之间存在关键差距", "method": "基于大型语言模型自动构建代码合成数据生成器，包括：1) 使用LLMs设计多样化的语义保持变换；2) 为这些变换生成强实现；3) 组合变换以产生强变换效果", "result": "1) 形式化分析显示SPT的多样性影响其组合强度；2) Auto-SPT生成的SPT比现有方法更多样化；3) 这些SPT显著降低了最先进代码克隆检测器的性能；4) Auto-SPT可用于增强训练数据集，产生对现实世界对抗性代码变换具有鲁棒性的模型", "conclusion": "Auto-SPT框架能够自动生成多样化的语义保持变换，有效解决代码克隆检测中训练与测试数据之间的差距问题，并能用于增强模型的鲁棒性"}}
{"id": "2512.06041", "pdf": "https://arxiv.org/pdf/2512.06041", "abs": "https://arxiv.org/abs/2512.06041", "authors": ["Candy Olivia Mawalim", "Haotian Zhang", "Shogo Okada"], "title": "Technical Report of Nomi Team in the Environmental Sound Deepfake Detection Challenge 2026", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "This paper presents our work for the ICASSP 2026 Environmental Sound Deepfake Detection (ESDD) Challenge. The challenge is based on the large-scale EnvSDD dataset that consists of various synthetic environmental sounds. We focus on addressing the complexities of unseen generators and low-resource black-box scenarios by proposing an audio-text cross-attention model. Experiments with individual and combined text-audio models demonstrate competitive EER improvements over the challenge baseline (BEATs+AASIST model).", "AI": {"tldr": "本文介绍了Nomi团队在2026年环境声音深度伪造检测挑战赛中的工作，提出了音频-文本交叉注意力模型来处理未见过的生成器和低资源黑盒场景", "motivation": "环境声音深度伪造检测面临两个主要挑战：1）需要检测来自未见过的生成器（unseen generators）的伪造音频；2）在低资源黑盒场景下进行检测。现有方法在这些复杂场景下性能有限", "method": "提出音频-文本交叉注意力模型，通过结合文本和音频模态的信息来增强检测能力。实验包括单独的文本-音频模型以及组合模型，并与挑战赛基线（BEATs+AASIST模型）进行比较", "result": "实验结果表明，所提出的音频-文本交叉注意力模型在EER（等错误率）指标上相比挑战赛基线取得了有竞争力的改进", "conclusion": "音频-文本交叉注意力模型能够有效处理环境声音深度伪造检测中的复杂场景，特别是在面对未见过的生成器和低资源黑盒条件时表现出良好的性能"}}
{"id": "2512.06040", "pdf": "https://arxiv.org/pdf/2512.06040", "abs": "https://arxiv.org/abs/2512.06040", "authors": ["Alireza Mohammadi", "Keshav Sood", "Dhananjay Thiruvady", "Asef Nazari"], "title": "Physics-Guided Deepfake Detection for Voice Authentication Systems", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Voice authentication systems deployed at the network edge face dual threats: a) sophisticated deepfake synthesis attacks and b) control-plane poisoning in distributed federated learning protocols. We present a framework coupling physics-guided deepfake detection with uncertainty-aware in edge learning. The framework fuses interpretable physics features modeling vocal tract dynamics with representations coming from a self-supervised learning module. The representations are then processed via a Multi-Modal Ensemble Architecture, followed by a Bayesian ensemble providing uncertainty estimates. Incorporating physics-based characteristics evaluations and uncertainty estimates of audio samples allows our proposed framework to remain robust to both advanced deepfake attacks and sophisticated control-plane poisoning, addressing the complete threat model for networked voice authentication.", "AI": {"tldr": "提出一个结合物理引导的深度伪造检测与不确定性感知边缘学习的框架，用于保护网络边缘的语音认证系统", "motivation": "语音认证系统在网络边缘面临双重威胁：1) 复杂的深度伪造合成攻击；2) 分布式联邦学习协议中的控制平面中毒攻击", "method": "融合物理引导特征（建模声道动力学）与自监督学习表示，通过多模态集成架构处理，最后使用贝叶斯集成提供不确定性估计", "result": "框架能够同时抵抗先进的深度伪造攻击和复杂的控制平面中毒攻击，解决了网络语音认证的完整威胁模型", "conclusion": "提出的物理引导深度伪造检测与不确定性感知边缘学习框架为网络边缘语音认证系统提供了全面的安全保护方案"}}
{"id": "2512.06038", "pdf": "https://arxiv.org/pdf/2512.06038", "abs": "https://arxiv.org/abs/2512.06038", "authors": ["Kelsey Fontenot", "Anjali Gorti", "Iva Goel", "Tonio Buonassisi", "Alexander E. Siemenn"], "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction", "categories": ["cs.RO", "cs.LG"], "comment": "15 pages, 8 figures", "summary": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.", "AI": {"tldr": "开发了一种用于自驱动实验室的闭环自动化基板处理和交换系统，通过机器人、双执行器分配器和深度学习计算机视觉来检测和纠正透明脆弱基板操作中的错误。", "motivation": "自驱动实验室在化学和材料实验中自动化了许多步骤，但基板的处理和重新装载这一关键步骤常常被忽视，特别是对于透明和脆弱的基板，这限制了实验室的完全自动化能力。", "method": "开发了ASHE系统，结合机器人技术、双执行器分配器和深度学习驱动的计算机视觉，形成闭环控制系统，能够实时检测基板操作错误并进行自动纠正。", "result": "在130次独立试验中，首次放置准确率达到98.5%，仅发生两次基板错位，且这些错误都被成功检测并自动纠正。", "conclusion": "通过开发更准确可靠的基板处理方法，提高了自驱动实验室的自动化能力，进一步加速了新型化学和材料发现的研究进程。"}}
{"id": "2512.06032", "pdf": "https://arxiv.org/pdf/2512.06032", "abs": "https://arxiv.org/abs/2512.06032", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.", "AI": {"tldr": "分析SAM2和SAM3之间的根本性差异，解释为什么SAM2的基于提示的专长无法迁移到SAM3的多模态概念驱动分割范式", "motivation": "研究Segment Anything Model家族中SAM2和SAM3之间的根本性不连续性，理解为什么基于提示的分割专长无法转移到概念驱动的多模态分割范式", "method": "通过五个核心组件进行结构化分析：1) 概念性断裂对比，2) 架构差异分析，3) 数据集和标注差异，4) 训练和超参数区别，5) 评估指标和失败模式分析", "result": "揭示了SAM2和SAM3之间的根本性范式转变，SAM2是基于空间提示的几何分割，而SAM3是多模态概念驱动的开放词汇分割，两者的专长无法直接迁移", "conclusion": "SAM3代表了分割基础模型的新类别，标志着从基于提示的分割向概念驱动分割时代的转变，为未来的研究方向指明了方向"}}
{"id": "2512.06024", "pdf": "https://arxiv.org/pdf/2512.06024", "abs": "https://arxiv.org/abs/2512.06024", "authors": ["Jiabin Liu", "Zihao Zhou", "Jialei Yan", "Anxin Guo", "Alvise Benetazzo", "Hui Li"], "title": "Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing", "categories": ["cs.CV", "physics.flu-dyn"], "comment": null, "summary": "Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.", "AI": {"tldr": "提出一种基于神经网络的3D海洋波浪流体动力学重建方法，从相机感知中重建波浪自由表面和速度场", "motivation": "解决长期海洋波浪观测任务中密集视觉重建的高计算成本问题，以及持续视觉遮挡带来的挑战，实现对波浪自由表面和速度场的精确三维重建", "method": "设计了一种波浪自由表面视觉重建神经网络，采用注意力增强的金字塔架构，针对波浪运动的多尺度和时间连续特性进行优化，利用物理约束从演化的自由表面边界进行时间分辨的三维速度场重建", "result": "在真实海况实验中实现了毫米级波浪高程预测、主导频率误差低于0.01Hz、高频谱功率律精确估计，以及非线性速度场的高保真3D重建，仅需1.35秒即可重建两百万个点，优于传统视觉重建方法", "conclusion": "该方法通过全局多尺度注意力和学习到的波浪传播动力学编码，在遮挡条件下保持强泛化能力，为海洋物理研究提供了高效的3D波浪流体动力学重建解决方案"}}
{"id": "2512.06022", "pdf": "https://arxiv.org/pdf/2512.06022", "abs": "https://arxiv.org/abs/2512.06022", "authors": ["Fu Li", "Weichao Zhao", "You Li", "Zhichao Zhou", "Dongliang He"], "title": "DreamFoley: Scalable VLMs for High-Fidelity Video-to-Audio Generation", "categories": ["cs.SD", "cs.MM"], "comment": "10 pages; Bytedance", "summary": "Recent advances in video generation have achieved remarkable improvements in visual content fidelity. However, the absence of synchronized audio severely undermines immersive experience and restricts practical applications of these technologies. To address this challenge, several pioneering works have explored diffusion transformer architectures for generating plausible video-synchronized audio, including Kling-foley, HunyuanVideo-foley and Thinksound. Distinct from existing works, we introduce an autoregressive audio generation architecture (DreamFoley) that harnesses the capabilities of large vision-language models (VLMs) to jointly model sequential interactions among video, audio, and text modalities. Our approach features a dual-visual encoder module that effectively captures both audio-aligned and text-aligned visual features. Additionally, we employ a Residual Vector Quantization audio tokenizer with a delay-pattern generation scheme to balance the trade-off between training efficiency and audio quality. Moreover, we introduce the classifier-free guidance strategy into VLMs to bootstrap generated audio quality. Furthermore, we establish an efficient data production pipeline to scale audio-video-text triple collection. Finally, extensive experiments are conducted to validate the effectiveness of our model, achieving promising performance across popular benchmarks. We hope that the findings in this study provide a strong foundation for future video-to-audio generation research. We also release the previously missing audio-visual textual descriptions from the public benchmark, aiming to facilitate subsequent researchers in conducting more convenient and effective evaluations and comparisons.", "AI": {"tldr": "DreamFoley提出了一种基于自回归架构的视频到音频生成方法，利用大型视觉语言模型联合建模视频、音频和文本模态的序列交互，实现高质量的视频同步音频生成。", "motivation": "当前视频生成技术在视觉内容保真度方面取得了显著进步，但缺乏同步音频严重影响了沉浸式体验并限制了这些技术的实际应用。现有方法主要基于扩散变换器架构，而本研究探索了自回归架构的潜力。", "method": "1. 采用自回归音频生成架构，利用大型视觉语言模型联合建模视频、音频和文本模态的序列交互；2. 设计双视觉编码器模块，分别捕捉音频对齐和文本对齐的视觉特征；3. 使用带有延迟模式生成方案的残差向量量化音频分词器，平衡训练效率和音频质量；4. 将无分类器引导策略引入视觉语言模型以提升生成音频质量；5. 建立高效的数据生产流程来扩展音频-视频-文本三元组收集。", "result": "在多个流行基准测试上取得了有希望的性能表现，验证了模型的有效性。同时发布了之前缺失的音频-视觉文本描述，为后续研究提供便利。", "conclusion": "DreamFoley为视频到音频生成研究提供了坚实的基础，展示了自回归架构在联合建模多模态序列交互方面的潜力，有望推动该领域的进一步发展。"}}
{"id": "2512.06020", "pdf": "https://arxiv.org/pdf/2512.06020", "abs": "https://arxiv.org/abs/2512.06020", "authors": ["Wenyi Mo", "Tianyu Zhang", "Yalong Bai", "Ligong Han", "Ying Ba", "Dimitris N. Metaxas"], "title": "PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: \\href{https://prefgen.github.io/}{\\texttt{https://prefgen.github.io}}", "summary": "Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.", "AI": {"tldr": "提出一个多模态框架PrefGen，利用多模态大语言模型提取丰富的用户表示，并将其注入基于扩散的图像生成中，实现偏好条件图像生成", "motivation": "现有方法要么无法捕捉细微的用户偏好，要么缺乏有效的机制来编码个性化的视觉信号，需要开发能够更好地适应个体用户美学选择的图像生成方法", "method": "1) 训练MLLM进行偏好导向的视觉问答任务以捕捉细粒度语义线索；2) 引入两个互补的探测任务：用户间区分和用户内区分；3) 设计基于最大均值差异的对齐损失来弥合模态差距；4) 将得到的嵌入用于条件化生成器", "result": "大量实验表明，该方法在图像质量和偏好对齐方面显著优于强基线，突出了表示提取和对齐在个性化生成中的有效性", "conclusion": "提出的多模态框架通过有效的用户表示提取和对齐机制，成功实现了能够忠实遵循提示和用户偏好的个性化图像生成"}}
{"id": "2512.06018", "pdf": "https://arxiv.org/pdf/2512.06018", "abs": "https://arxiv.org/abs/2512.06018", "authors": ["Jiameng Wei", "Dinh Dang", "Kaixun Yang", "Emily Stokes", "Amna Mazeh", "Angelina Lim", "David Wei Dai", "Joel Moore", "Yizhou Fan", "Danijela Gasevic", "Dragan Gasevic", "Guanliang Chen"], "title": "Uncovering Students' Inquiry Patterns in GenAI-Supported Clinical Practice: An Integration of Epistemic Network Analysis and Sequential Pattern Mining", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Assessment of medication history-taking has traditionally relied on human observation, limiting scalability and detailed performance data. While Generative AI (GenAI) platforms enable extensive data collection and learning analytics provide powerful methods for analyzing educational traces, these approaches remain largely underexplored in pharmacy clinical training. This study addresses this gap by applying learning analytics to understand how students develop clinical communication competencies with GenAI-powered virtual patients -- a crucial endeavor given the diversity of student cohorts, varying language backgrounds, and the limited opportunities for individualized feedback in traditional training settings. We analyzed 323 students' interaction logs across Australian and Malaysian institutions, comprising 50,871 coded utterances from 1,487 student-GenAI dialogues. Combining Epistemic Network Analysis to model inquiry co-occurrences with Sequential Pattern Mining to capture temporal sequences, we found that high performers demonstrated strategic deployment of information recognition behaviors. Specifically, high performers centered inquiry on recognizing clinically relevant information, integrating rapport-building and structural organization, while low performers remained in routine question-verification loops. Demographic factors including first-language background, prior pharmacy work experience, and institutional context, also shaped distinct inquiry patterns. These findings reveal inquiry patterns that may indicate clinical reasoning development in GenAI-assisted contexts, providing methodological insights for health professions education assessment and informing adaptive GenAI system design that supports diverse learning pathways.", "AI": {"tldr": "本研究通过整合认知网络分析和序列模式挖掘，揭示了学生在生成式AI支持的临床实践中不同的询问模式，识别了高绩效学生与低绩效学生在临床沟通能力发展上的关键差异。", "motivation": "传统药物史采集评估依赖人工观察，限制了可扩展性和详细性能数据。虽然生成式AI平台能够收集大量数据，学习分析技术也能分析教育痕迹，但这些方法在药学临床培训中尚未得到充分探索。本研究旨在填补这一空白，特别是在学生群体多样化、语言背景各异以及传统培训中个性化反馈机会有限的情况下。", "method": "研究分析了澳大利亚和马来西亚323名学生的互动日志，包含50,871个编码话语和1,487个学生-GenAI对话。采用认知网络分析建模询问共现关系，结合序列模式挖掘捕捉时间序列模式，以理解学生在生成式AI支持的虚拟患者环境中如何发展临床沟通能力。", "result": "研究发现高绩效学生展现出战略性的信息识别行为部署，他们以识别临床相关信息为中心，整合关系建立和结构组织；而低绩效学生则停留在常规的提问-验证循环中。人口统计学因素（包括母语背景、先前药学工作经验和机构背景）也塑造了不同的询问模式。", "conclusion": "这些发现揭示了在生成式AI辅助环境中可能指示临床推理发展的询问模式，为健康专业教育评估提供了方法论见解，并为支持多样化学习路径的自适应生成式AI系统设计提供了信息。"}}
{"id": "2512.06017", "pdf": "https://arxiv.org/pdf/2512.06017", "abs": "https://arxiv.org/abs/2512.06017", "authors": ["Laurence Liang"], "title": "Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models", "categories": ["cs.RO", "eess.IV"], "comment": "Accepted at CVIS 2025", "summary": "Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf\" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.", "AI": {"tldr": "使用前沿视觉语言模型作为\"即用型\"工具，从单张目标图像估计机器人手臂关节角度，无需训练", "motivation": "随着机器人手臂在工业和家庭应用中的普及，可靠的关节角度估计能提高安全性和性能保证，并可作为验证器进一步训练机器人策略。传统方法需要训练，而本文探索无需训练的方法。", "method": "使用前沿视觉语言模型作为即用型工具，从单张图像直接估计机器人关节角度。在合成和真实世界图像数据对上评估前沿VLM，建立性能基准。", "result": "建立了当前前沿VLM在机器人姿态估计上的性能基准。实证结果表明，仅通过测试时间缩放或参数缩放并不能改善关节角度预测性能。", "conclusion": "前沿视觉语言模型可作为无需训练的机器人姿态估计工具，但当前方法在缩放策略上存在局限性，需要进一步研究改进。"}}
{"id": "2512.06014", "pdf": "https://arxiv.org/pdf/2512.06014", "abs": "https://arxiv.org/abs/2512.06014", "authors": ["Jiho Shin", "Dominic Marshall", "Matthieu Komorowski"], "title": "Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.", "AI": {"tldr": "对两个大规模胸部X光嵌入模型（CXR-Foundation和MedImageInsight）在公开数据集MIMIC-CXR和NIH-CXR14上进行基准测试和比较", "motivation": "尽管医学图像基础模型在表示学习方面表现出色，但它们在跨数据集上的比较行为尚未得到充分探索，需要标准化评估和可复现的基准", "method": "使用统一的预处理流程和固定的下游分类器，直接从预训练编码器提取嵌入，训练轻量级LightGBM分类器，报告平均AUROC和F1分数及95%置信区间", "result": "MedImageInsight在大多数任务中表现略优，而CXR-Foundation展现出更强的跨数据集稳定性；MedImageInsight嵌入的无监督聚类显示出与定量结果一致的疾病特异性结构", "conclusion": "研究强调了医学基础模型标准化评估的必要性，为未来多模态和临床整合研究建立了可复现的基准"}}
{"id": "2512.06013", "pdf": "https://arxiv.org/pdf/2512.06013", "abs": "https://arxiv.org/abs/2512.06013", "authors": ["Wenhao Li", "Chengwei Ma", "Weixin Mao"], "title": "VAT: Vision Action Transformer by Unlocking Full Representation of ViT", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.", "AI": {"tldr": "提出VAT（Vision Action Transformer）架构，通过解锁ViT的完整特征层次结构来改进机器人学习中的视觉感知与动作生成融合", "motivation": "在机器人学习中，大多数方法仅使用ViT最后一层的特征，丢弃了有价值的信息，导致表示不足。需要充分利用ViT的完整特征层次来提升感知与动作生成的融合效果", "method": "扩展ViT架构，提出VAT模型，通过在所有Transformer层中处理专门的动作token与视觉特征，实现感知与动作生成的深度渐进式融合", "result": "在模拟操作任务套件中，VAT在四个LIBERO基准测试上达到98.15%的平均成功率，超越了OpenVLA-OFT等先前方法，建立了新的最先进水平", "conclusion": "VAT不仅是一个强大的模仿学习模型，更重要的是证明了充分利用视觉模型的完整\"表示轨迹\"对于推进机器人策略的关键重要性"}}
{"id": "2512.06012", "pdf": "https://arxiv.org/pdf/2512.06012", "abs": "https://arxiv.org/abs/2512.06012", "authors": ["Emmanuel Akeweje", "Conall Kirk", "Chi-Wai Chan", "Denis Dowling", "Mimi Zhang"], "title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing", "categories": ["cs.CV"], "comment": null, "summary": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.", "AI": {"tldr": "开发一个用于增材制造金属粉末形态分析的高通量无监督机器学习框架，通过图像处理和聚类方法对316L粉末颗粒进行形态特征分析。", "motivation": "选择性激光熔化（SLM）增材制造技术的零件质量严重依赖于原料粉末的形态特征，但传统的粉末表征方法通量低、定性化，无法捕捉工业规模批次的异质性，需要开发自动化、高通量的分析方法。", "method": "提出了一个结合高通量成像、形状提取和聚类的机器学习框架，开发并评估了三种聚类流程：自编码器流程、形状描述符流程和函数数据流程，在约126,000个粉末图像数据集上进行测试。", "result": "内部有效性指标确定傅里叶描述符+k-means流程最为有效，达到最低的Davies-Bouldin指数和最高的Calinski-Harabasz分数，同时在标准桌面工作站上保持每个颗粒亚毫秒级的运行时间。", "conclusion": "该无监督学习框架能够快速、自动化评估粉末形态，支持跟踪粉末在重复使用周期中的形状演变，为SLM工作流程中的实时原料监测提供了路径，形成的形状组为未来研究粉末形态与流动性、堆积密度和SLM零件质量的关系奠定了基础。"}}
{"id": "2512.06010", "pdf": "https://arxiv.org/pdf/2512.06010", "abs": "https://arxiv.org/abs/2512.06010", "authors": ["Thomas Massena", "Corentin Friedrich", "Franck Mamalet", "Mathieu Serrurier"], "title": "Fast and Flexible Robustness Certificates for Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.", "AI": {"tldr": "提出一种用于语义分割的新型可验证鲁棒性认证方法，基于Lipschitz约束网络实现快速高效的鲁棒性保证", "motivation": "当前深度神经网络对微小扰动敏感，现有鲁棒性认证方法主要针对分类任务，语义分割领域缺乏高效认证方法", "method": "引入内置Lipschitz约束的可验证鲁棒语义分割网络，提出通用化鲁棒性认证框架，利用Lipschitz网络特性实现灵活高效认证", "result": "在Cityscapes等挑战性数据集上实现有竞争力的像素精度，认证速度比随机平滑方法快约600倍，支持多种性能指标的ℓ₂攻击最坏情况分析", "conclusion": "首次实现实时兼容的可验证鲁棒语义分割，提供灵活计算高效的鲁棒性认证框架，在速度和认证质量上均优于现有方法"}}
{"id": "2512.06008", "pdf": "https://arxiv.org/pdf/2512.06008", "abs": "https://arxiv.org/abs/2512.06008", "authors": ["Fang Li", "Tonglin Mu", "Shuling Li", "Junran Guo", "Keyuan Li", "Jianing Li", "Ziyang Luo", "Xiaodong Fan", "Ye Chen", "Yunfeng Liu", "Hong Cai", "Lip Ket Chin", "Jinbei Zhang", "Shihai Sun"], "title": "Semantic Temporal Single-photon LiDAR", "categories": ["eess.IV", "cs.CV", "quant-ph"], "comment": "14 pages, 5 figures. And any comment is welcome", "summary": "Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.", "AI": {"tldr": "提出基于语义通信和自更新语义知识库的语义时间单光子LiDAR系统，用于开放场景下的目标识别", "motivation": "现有时间单光子LiDAR在开放场景（出现未知目标）中效果不佳，且在低信噪比和短采集时间下性能显著下降，需要更鲁棒和自适应的解决方案", "method": "将TSP-LiDAR目标识别过程建模为语义通信问题，引入自更新语义知识库（SKB）机制，动态更新新遇到目标的语义特征", "result": "在低信噪比和有限采集时间条件下超越传统方法；自更新SKB机制在真实实验中使九种未知目标的识别准确率达到89%（无更新机制为66%）", "conclusion": "该框架在复杂动态环境中具有自适应和鲁棒目标识别的潜力，通过语义通信和自更新知识库机制解决了开放场景和恶劣条件下的识别问题"}}
{"id": "2512.06006", "pdf": "https://arxiv.org/pdf/2512.06006", "abs": "https://arxiv.org/abs/2512.06006", "authors": ["Xuefei", "Wang", "Kai A. Horstmann", "Ethan Lin", "Jonathan Chen", "Alexander R. Farhang", "Sophia Stiles", "Atharva Sehgal", "Jonathan Light", "David Van Valen", "Yisong Yue", "Jennifer J. Sun"], "title": "Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adapting production-level computer vision tools to bespoke scientific datasets is a critical \"last mile\" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.", "AI": {"tldr": "本文研究了AI代理在生物医学影像工作流优化中的应用，发现简单代理框架在生成适应代码方面优于人类专家解决方案。", "motivation": "将生产级计算机视觉工具适配到定制科学数据集是一个关键的\"最后一公里\"瓶颈。当前解决方案不切实际：微调需要大量标注数据集（科学家通常缺乏），而手动代码适配需要科学家数周到数月的努力。", "method": "引入了一个系统性的代理代码优化评估框架，用于研究三个生产级生物医学影像管道。使用简单代理框架生成适应代码，并与人类专家解决方案进行比较。", "result": "简单代理框架一致生成优于人类专家解决方案的适应代码。分析显示常见、复杂的代理架构并非普遍有益，为代理设计提供了实用路线图。", "conclusion": "AI代理可以自动化生物医学影像工作流中的手动编码任务，简单代理框架在实际应用中表现优异，为真实世界影响提供了清晰路径。开源了评估框架，并通过将代理生成的函数部署到生产管道中验证了该方法。"}}
{"id": "2512.06003", "pdf": "https://arxiv.org/pdf/2512.06003", "abs": "https://arxiv.org/abs/2512.06003", "authors": ["Ramin Sharifi", "Pouya Shiri", "Amirali Baniasadi"], "title": "PrunedCaps: A Case For Primary Capsules Discrimination", "categories": ["cs.CV"], "comment": null, "summary": "Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.", "AI": {"tldr": "提出了一种名为PrunedCaps的胶囊网络剪枝方法，通过识别和移除不重要的主要胶囊来显著提高计算效率", "motivation": "胶囊网络虽然具有比卷积神经网络更好的仿射变换鲁棒性和重叠图像检测能力，但由于主要胶囊数量众多，导致训练和测试速度慢、资源消耗大，缺乏资源效率", "method": "研究胶囊网络中主要胶囊的剪枝可能性，通过移除不重要的胶囊来优化网络结构，在MNIST、Fashion-MNIST、CIFAR-10和SVHN数据集上进行实验", "result": "剪枝后的胶囊网络在移除95%胶囊的情况下，性能比传统架构快9.90倍，在动态路由阶段节省超过95.36%的浮点运算，且不损失准确率", "conclusion": "主要胶囊剪枝是有效的优化策略，能显著提高胶囊网络的效率，同时解释了为什么某些数据集从剪枝中获益更大"}}
{"id": "2512.06002", "pdf": "https://arxiv.org/pdf/2512.06002", "abs": "https://arxiv.org/abs/2512.06002", "authors": ["Evan Conway", "David Porfirio", "David Chan", "Mark Roberts", "Laura M. Hiatt"], "title": "POrTAL: Plan-Orchestrated Tree Assembly for Lookahead", "categories": ["cs.RO", "cs.AI"], "comment": "Submitted to ICRA 26", "summary": "Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.", "AI": {"tldr": "提出了一种新的轻量级概率规划算法POrTAL，用于机器人在部分可观测环境下的任务规划，结合了FF-Replan和POMCP两种基线算法的优势。", "motivation": "在人类-机器人交互场景中，机器人通常只能部分观测环境，需要在不确定性下制定规划。现有概率规划算法要么计算效率低（受限于机器人有限的计算资源），要么需要过多步骤才能达成目标。", "method": "提出了Plan-Orchestrated Tree Assembly for Lookahead (POrTAL)算法，结合了FF-Replan和POMCP两种基线规划算法的优势，形成一种轻量级的概率规划方法。", "result": "通过一系列案例研究，证明POrTAL能够快速找到解决方案，在步骤数量方面优于FF-Replan和POMCP基线算法，并展示了在不同时间约束下的性能表现。", "conclusion": "POrTAL是一种有效的轻量级概率规划算法，能够在部分可观测环境下为机器人提供高效的任务规划解决方案，在计算效率和规划质量之间取得了良好平衡。"}}
{"id": "2512.05998", "pdf": "https://arxiv.org/pdf/2512.05998", "abs": "https://arxiv.org/abs/2512.05998", "authors": ["Michael Todasco"], "title": "Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals", "categories": ["cs.AI", "cs.GT", "cs.LG"], "comment": "25 pages, 8 tables, 2 figures. Pilot study. Data, prompts, and code available at https://osf.io/dc24t/", "summary": "Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. \"Whale\" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.", "AI": {"tldr": "该研究探索了通过虚构预测市场（赌博游戏）框架来改进大型语言模型评估其他模型时的准确性，并生成可校准的置信度信号。", "motivation": "当前大型语言模型在评估其他模型时通常缺乏置信度表示，无法区分模型对其判断的确定程度，这限制了评估结果的可信度和实用性。", "method": "研究设计了100个数学和逻辑问题，让6个基线模型（3个当前代，3个前代）回答。然后让3个预测模型在两种条件下预测每个基线模型是否能正确回答问题：控制条件（简单正确/错误预测）和激励条件（预测加下注1-100,000 LLMCoin，起始资金1,000,000 LLMCoin）。", "result": "激励条件下预测准确率略高（81.5% vs. 79.1%，p=0.089），学习速度显著更快（第1轮到第4轮改进12.0% vs. 2.9%，p=0.011）。下注大小与置信度相关：大额下注（40,000+硬币）正确率约99%，小额下注（<1,000硬币）正确率仅约74%。", "conclusion": "虚构金钱并不能使模型更聪明，但赌博机制创建了可读的置信度信号，使模型内部信念变得可见可用。简单的金融框架可能帮助LLM成为风险感知的预测者，为元评估系统和LLM间预测市场奠定基础。"}}
{"id": "2512.05996", "pdf": "https://arxiv.org/pdf/2512.05996", "abs": "https://arxiv.org/abs/2512.05996", "authors": ["Yi Liu", "Jingyu Song", "Vedanth Kallakuri", "Katherine A. Skinner"], "title": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting", "categories": ["cs.CV", "cs.CY", "cs.RO", "eess.IV"], "comment": "18 pages, under review", "summary": "Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.", "AI": {"tldr": "提出FishDetector-R1框架，这是一个基于多模态大语言模型的统一框架，用于弱监督条件下的鱼类检测、分割和计数任务。", "motivation": "水下鱼类图像分析对生态监测至关重要，但由于视觉质量下降和标注成本高昂，该任务仍然具有挑战性。需要一种能够在弱监督条件下准确进行鱼类检测、分割和计数的可靠解决方案。", "method": "采用基于MLLM的统一框架，包含两个关键组件：1）新颖的detect-to-count提示机制，强制空间一致的检测和计数；2）基于可验证奖励的强化学习（RLVR），利用稀疏点标签的互补可扩展范式。", "result": "在DeepFish数据集上，相比基线方法取得了显著提升：AP提高20%，mIoU提高10%，MAE降低30%，GAME降低35%。消融研究验证了奖励设计的有效性，且改进能够很好地泛化到其他水下数据集，显示出强大的跨域鲁棒性。", "conclusion": "FishDetector-R1通过弱监督为准确的海洋视觉理解提供了一个可靠且可扩展的解决方案，在鱼类检测、分割和计数任务上表现出色，具有很好的泛化能力。"}}
{"id": "2512.05994", "pdf": "https://arxiv.org/pdf/2512.05994", "abs": "https://arxiv.org/abs/2512.05994", "authors": ["Rohan Sharma", "Dancheng Liu", "Jingchen Sun", "Shijie Zhou", "Jiayu Qin", "Jinjun Xiong", "Changyou Chen"], "title": "KidSpeak: A General Multi-purpose LLM for Kids' Speech Recognition and Screening", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "With the rapid advancement of conversational and diffusion-based AI, there is a growing adoption of AI in educational services, ranging from grading and assessment tools to personalized learning systems that provide targeted support for students. However, this adaptability has yet to fully extend to the domain of children's speech, where existing models often fail due to their reliance on datasets designed for clear, articulate adult speech. Children, particularly those in early developmental stages or with speech and language pathologies, present unique challenges that current AI models and datasets are ill-equipped to handle. To address this, we introduce KidSpeak, a multi-task speech-enhanced Foundation Model capable of both generative and discriminative tasks specifically tailored to children's speech patterns. Our framework employs a two-stage training process that incorporates phonetic knowledge into the speech encoder, achieving an average accuracy of 87% across four separate tasks. Furthermore, recognizing the limitations of scalable human annotation and existing speech alignment tools, we propose the Flexible and Automatic Speech Aligner (FASA) and leverage the method to construct high quality datasets for training and evaluation. This novel alignment tool significantly improves the quality of aligned children's speech from noisy data, enhancing data quality by 13.6x compared to human annotations, as demonstrated on the CHILDES dataset. To the best of our knowledge, KidSpeak and FASA represent the first comprehensive solution designed for speech and language therapy in children, offering both a multi-purpose speech LLM and a robust alignment tool.", "AI": {"tldr": "KidSpeak是一个专为儿童语音识别和筛查设计的通用多任务大语言模型，通过两阶段训练结合语音学知识，解决了现有模型在处理儿童语音方面的不足。", "motivation": "现有AI模型主要基于清晰、发音准确的成人语音数据集设计，无法有效处理儿童（特别是早期发展阶段或有语言障碍的儿童）的独特语音特征，限制了AI在教育服务领域的全面应用。", "method": "采用两阶段训练过程：1）将语音学知识融入语音编码器；2）提出灵活自动语音对齐器（FASA）构建高质量训练数据集。FASA显著提升了从嘈杂数据中对齐儿童语音的质量。", "result": "在四个独立任务上平均准确率达到87%；FASA在CHILDES数据集上的对齐质量比人工标注提高了13.6倍，显著改善了训练数据的质量。", "conclusion": "KidSpeak和FASA为儿童语音和语言治疗提供了首个全面解决方案，包括一个多用途语音大语言模型和强大的对齐工具，填补了现有AI模型在处理儿童语音方面的空白。"}}
{"id": "2512.05993", "pdf": "https://arxiv.org/pdf/2512.05993", "abs": "https://arxiv.org/abs/2512.05993", "authors": ["Ruchika Verma", "Shrishtee Kandoi", "Robina Afzal", "Shengjia Chen", "Jannes Jegminat", "Michael W. Karlovich", "Melissa Umphlett", "Timothy E. Richardson", "Kevin Clare", "Quazi Hossain", "Jorge Samanamud", "Phyllis L. Faust", "Elan D. Louis", "Ann C. McKee", "Thor D. Stein", "Jonathan D. Cherry", "Jesse Mez", "Anya C. McGoldrick", "Dalilah D. Quintana Mora", "Melissa J. Nirenberg", "Ruth H. Walker", "Yolfrankcis Mendez", "Susan Morgello", "Dennis W. Dickson", "Melissa E. Murray", "et al. (12 additional authors not shown)"], "title": "Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.", "AI": {"tldr": "开发专门针对神经病理学领域的领域特定基础模型NeuroFM，用于改善基于AI的神经病理学分析", "motivation": "现有基础模型主要基于外科病理学数据训练，这些数据富含非神经组织，过度代表非神经系统疾病，而神经病理学具有独特的细胞类型、细胞结构和疾病特异性病理特征，领域不匹配限制了通用模型捕捉神经退行性疾病关键形态模式的能力", "method": "开发NeuroFM，这是一个专门在涵盖多种神经退行性病理的脑组织全切片图像上训练的基础模型", "result": "NeuroFM在多个神经病理学特定下游任务中表现出优于通用模型的性能，包括混合性痴呆疾病分类、海马区域分割以及神经退行性共济失调识别（涵盖小脑性特发性震颤和脊髓小脑性共济失调亚型）", "conclusion": "领域专业化基础模型在脑组织上训练能够比通用外科病理学数据集训练的模型更好地捕捉神经病理学特异性特征，通过针对神经退行性疾病独特形态景观定制基础模型，NeuroFM能够为脑疾病诊断和研究提供更准确可靠的AI分析，为数字病理学专业领域的领域特定模型开发树立了先例"}}
{"id": "2512.05992", "pdf": "https://arxiv.org/pdf/2512.05992", "abs": "https://arxiv.org/abs/2512.05992", "authors": ["Azeez Idris", "Abdurahman Ali Mohammed", "Samuel Fanijo"], "title": "Stronger is not better: Better Augmentations in Contrastive Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "NeurIPS Black in AI workshop - 2022", "summary": "Self-supervised contrastive learning is among the recent representation learning methods that have shown performance gains in several downstream tasks including semantic segmentation. This paper evaluates strong data augmentation, one of the most important components for self-supervised contrastive learning's improved performance. Strong data augmentation involves applying the composition of multiple augmentation techniques on images. Surprisingly, we find that the existing data augmentations do not always improve performance for semantic segmentation for medical images. We experiment with other augmentations that provide improved performance.", "AI": {"tldr": "评估自监督对比学习中强数据增强对医学图像分割任务的影响，发现现有增强方法并不总是能提升性能，并探索了能带来改进的增强策略。", "motivation": "自监督对比学习在多个下游任务中表现出性能提升，强数据增强是其关键组件之一。然而，现有研究未充分评估这些增强方法在医学图像分割任务中的实际效果，特别是强增强（多种增强技术组合）是否总是有益。", "method": "通过实验评估现有强数据增强方法在医学图像分割任务中的效果，并探索其他能带来性能改进的增强策略。对比不同增强组合对自监督对比学习表示质量的影响。", "result": "研究发现现有强数据增强方法并不总是能提升医学图像分割性能，有时甚至可能降低性能。通过实验发现了能提供改进性能的其他增强策略。", "conclusion": "在医学图像分割的自监督对比学习中，更强的数据增强并不总是更好。需要针对特定任务和领域设计合适的增强策略，而不是盲目采用现有强增强方法。"}}
{"id": "2512.05991", "pdf": "https://arxiv.org/pdf/2512.05991", "abs": "https://arxiv.org/abs/2512.05991", "authors": ["Chang Liu", "Tianjiao Jing", "Chengcheng Ma", "Xuanqi Zhou", "Zhengxuan Lian", "Qin Jin", "Hongliang Yuan", "Shi-Sheng Huang"], "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head", "categories": ["cs.CV"], "comment": null, "summary": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.", "AI": {"tldr": "提出EmoDiffTalk，一种基于情感感知扩散的可编辑3D高斯说话头生成框架，支持多模态情感编辑", "motivation": "现有基于3D高斯泼溅的逼真3D说话头在情感表达操控方面存在不足，特别是在细粒度和动态情感编辑方面", "method": "提出情感感知高斯扩散方法，包括动作单元提示高斯扩散过程和准确的文本到AU情感控制器", "result": "在EmoTalk3D和RenderMe-360数据集上实验显示，在情感细腻度、唇形同步保真度和可控性方面优于先前工作", "conclusion": "EmoDiffTalk为高质量、扩散驱动、多模态可编辑3D说话头合成建立了原理性途径，是首批支持连续多模态情感编辑的3D高斯泼溅说话头生成框架之一"}}
{"id": "2512.05988", "pdf": "https://arxiv.org/pdf/2512.05988", "abs": "https://arxiv.org/abs/2512.05988", "authors": ["Junho Kim", "Seongwon Lee"], "title": "VG3T: Visual Geometry Grounded Gaussian Transformer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.", "AI": {"tldr": "VG3T是一个新颖的多视角前馈网络，通过3D高斯表示预测3D语义占据，直接从多视角图像联合预测具有语义属性的高斯集合，克服了逐视角处理的碎片化和不一致性问题。", "motivation": "现有方法在多视角融合方面存在困难，导致3D表示碎片化和性能不佳。需要一种统一范式来同时表示几何和语义信息，解决逐视角处理的碎片化和不一致性问题。", "method": "提出VG3T多视角前馈网络，直接预测具有语义属性的3D高斯集合。引入两个关键组件：基于网格的采样和位置细化，以缓解像素对齐高斯初始化方法中常见的距离依赖密度偏差。", "result": "在nuScenes基准测试中，VG3T在mIoU上实现了1.7%的显著提升，同时使用的基元数量比之前的最先进方法减少了46%，展示了其卓越的效率和性能。", "conclusion": "VG3T提供了一种统一的多视角3D场景表示方法，通过联合多视角处理克服了传统方法的碎片化问题，在效率和性能方面都表现出色，为3D场景理解提供了新的解决方案。"}}
{"id": "2512.05987", "pdf": "https://arxiv.org/pdf/2512.05987", "abs": "https://arxiv.org/abs/2512.05987", "authors": ["Chenyue Yu", "Jianyu Yu"], "title": "Adaptive Dataset Quantization: A New Direction for Dataset Pruning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCPR 2025", "summary": "This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.", "AI": {"tldr": "提出一种新的数据集量化方法，通过减少样本内冗余来压缩大规模数据集，解决边缘设备存储和通信成本问题。", "motivation": "解决资源受限边缘设备中大规模数据集的存储和通信成本挑战，传统的数据集剪枝和蒸馏方法主要关注样本间冗余，而本方法关注样本内冗余压缩。", "method": "首先对每个样本应用线性对称量化获得初始量化范围和尺度，然后引入自适应量化分配算法，为具有不同精度要求的样本分配不同的量化比率，同时保持总压缩比恒定。", "result": "在CIFAR-10、CIFAR-100和ImageNet-1K上的实验验证了方法的有效性，在保持模型训练性能的同时实现了显著的数据集压缩，在相同压缩比下优于传统量化和数据集剪枝基线方法。", "conclusion": "该方法首次使用有限比特表示数据集以减少存储，引入了具有自适应比率分配的数据集级量化算法，为数据集压缩提供了新的方向。"}}
{"id": "2512.05982", "pdf": "https://arxiv.org/pdf/2512.05982", "abs": "https://arxiv.org/abs/2512.05982", "authors": ["Lingfeng Zhou", "Yi Xu", "Zhenyu Wang", "Dequan Wang"], "title": "FlockVote: LLM-Empowered Agent-Based Modeling for Simulating U.S. Presidential Elections", "categories": ["physics.soc-ph", "cs.AI", "cs.MA"], "comment": "Published as a conference paper at ICAIS 2025", "summary": "Modeling complex human behavior, such as voter decisions in national elections, is a long-standing challenge for computational social science. Traditional agent-based models (ABMs) are limited by oversimplified rules, while large-scale statistical models often lack interpretability. We introduce FlockVote, a novel framework that uses Large Language Models (LLMs) to build a \"computational laboratory\" of LLM agents for political simulation. Each agent is instantiated with a high-fidelity demographic profile and dynamic contextual information (e.g. candidate policies), enabling it to perform nuanced, generative reasoning to simulate a voting decision. We deploy this framework as a testbed on the 2024 U.S. Presidential Election, focusing on seven key swing states. Our simulation's macro-level results successfully replicate the real-world outcome, demonstrating the high fidelity of our \"virtual society\". The primary contribution is not only the prediction, but also the framework's utility as an interpretable research tool. FlockVote moves beyond black-box outputs, allowing researchers to probe agent-level rationale and analyze the stability and sensitivity of LLM-driven social simulations.", "AI": {"tldr": "提出FlockVote框架，利用大型语言模型构建基于智能体的政治模拟系统，用于模拟美国总统选举中的选民决策行为", "motivation": "传统基于智能体的模型过于简化，大规模统计模型缺乏可解释性，需要一种能够模拟复杂人类行为（如选民决策）的高保真、可解释的计算社会科学方法", "method": "使用大型语言模型构建\"计算实验室\"，每个智能体具有高保真的人口统计特征和动态上下文信息（如候选人政策），能够进行细致的生成式推理来模拟投票决策", "result": "在2024年美国总统选举的七个关键摇摆州进行模拟，宏观层面的结果成功复制了真实世界的结果，证明了\"虚拟社会\"的高保真度", "conclusion": "FlockVote不仅提供了预测能力，更重要的是作为一个可解释的研究工具，超越了黑盒输出，允许研究人员探究智能体层面的推理逻辑，分析LLM驱动社会模拟的稳定性和敏感性"}}
{"id": "2512.05981", "pdf": "https://arxiv.org/pdf/2512.05981", "abs": "https://arxiv.org/abs/2512.05981", "authors": ["Leren Qian", "Mohammad Khishe", "Yiqian Huang", "Seyedali Mirjalili"], "title": "SEB-ChOA: An Improved Chimp Optimization Algorithm Using Spiral Exploitation Behavior", "categories": ["cs.NE"], "comment": "This is the author-accepted manuscript of the article published in Neural Computing and Applications (2024)", "summary": "The chimp optimization algorithm (ChOA) is a nature-inspired algorithm that imitates chimpanzees' individual intelligence and hunting behaviors. In this algorithm, the hunting process consists of four steps: driving, blocking, chasing, and attacking. Because of the novelty of ChOA, the steps of the hunting process have been modeled in a simple way, leading to slow and premature convergence similar to other iterative algorithms. This paper proposes six spiral functions and introduces two novel hybrid spiral functions (SEB-ChOA) to address these deficiencies. The performance of SEB-ChOA is evaluated on 23 standard benchmarks, 20 benchmarks of the IEEE CEC-2005 test suite, 10 cases from the IEEE CEC06-2019 test suite, and 12 constrained real-world engineering problems from IEEE CEC-2020. The SEB-ChOA variants are compared with three groups of optimization algorithms, including Particle Swarm Optimization (PSO) and Genetic Algorithm (GA) as well-known optimizers; Slime Mould Algorithm (SMA), Marine Predators Algorithm (MPA), Ant Lion Optimization (ALO), and Henry Gas Solubility Optimization (HGSO) as recently developed optimizers; and jDE100 and DISHchain1e+12, the winners of the IEEE CEC06-2019 competition. Additional comparisons are made with EBOwithCMAR and CIPDE as strong secondary baselines. The SEB-ChOA methods achieve top-ranked results on nearly all benchmarks and show competitive performance compared to jDE100 and DISHchain1e+12. Statistical results indicate that SEB-ChOA outperforms PSO, GA, SMA, MPA, ALO, and HGSO while producing results comparable to those of jDE100 and DISHchain1e+12.", "AI": {"tldr": "提出了一种改进的黑猩猩优化算法SEB-ChOA，通过引入螺旋开发行为来解决原始ChOA算法收敛慢和早熟的问题", "motivation": "原始黑猩猩优化算法(ChOA)虽然模仿了黑猩猩的狩猎行为，但由于建模过于简单，存在收敛速度慢和早熟收敛的问题，需要改进其开发能力", "method": "提出了六种螺旋函数，并引入了两种新颖的混合螺旋函数(SEB-ChOA)，通过螺旋开发行为增强算法的搜索能力", "result": "在23个标准基准函数、20个IEEE CEC-2005测试套件、10个IEEE CEC06-2019测试套件以及12个IEEE CEC-2020实际工程问题上进行测试，SEB-ChOA在几乎所有基准测试中都取得了顶级结果，性能优于PSO、GA、SMA、MPA、ALO和HGSO，与jDE100和DISHchain1e+12性能相当", "conclusion": "SEB-ChOA通过引入螺旋开发行为有效解决了原始ChOA算法的收敛问题，在各种基准测试和实际工程问题上表现出优越性能，是一种有效的改进优化算法"}}
{"id": "2512.05979", "pdf": "https://arxiv.org/pdf/2512.05979", "abs": "https://arxiv.org/abs/2512.05979", "authors": ["Mikhail Tsitsvero", "Atsuyuki Nakao", "Hisaki Ikebata"], "title": "Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction", "categories": ["physics.chem-ph", "cs.AI", "cs.DM", "cs.LG"], "comment": "22 pages, 8 figures", "summary": "Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.", "AI": {"tldr": "提出一种通用的化学过程表示方法，通过多模态图神经网络学习跨领域材料属性预测", "motivation": "化学过程的实验验证缓慢且昂贵，现有专利和文献数据异构难以利用，限制了材料发现", "method": "引入通用有向树过程图表示，将非结构化文本、分子结构和数值测量统一为机器可读格式；开发多模态图神经网络，采用属性条件注意力机制", "result": "在约9,000份文档的700,000个过程图上训练，学习到语义丰富的嵌入表示；在特定领域小数据集上微调后表现优异", "conclusion": "大规模学习的通用过程表示能有效迁移到专业预测任务，只需少量额外数据即可实现跨领域材料属性预测"}}
{"id": "2512.05971", "pdf": "https://arxiv.org/pdf/2512.05971", "abs": "https://arxiv.org/abs/2512.05971", "authors": ["Mohammadhossein Ghahramani", "Yan Qiao", "NaiQi Wu", "Mengchu Zhou"], "title": "A Multi-objective Optimization Approach for Feature Selection in Gentelligent Systems", "categories": ["cs.NE", "cs.AI"], "comment": "11 pages. IEEE Internet of Things Journal, 2025", "summary": "The integration of advanced technologies, such as Artificial Intelligence (AI), into manufacturing processes is attracting significant attention, paving the way for the development of intelligent systems that enhance efficiency and automation. This paper uses the term \"Gentelligent system\" to refer to systems that incorporate inherent component information (akin to genes in bioinformatics-where manufacturing operations are likened to chromosomes in this study) and automated mechanisms. By implementing reliable fault detection methods, manufacturers can achieve several benefits, including improved product quality, increased yield, and reduced production costs. To support these objectives, we propose a hybrid framework with a dominance-based multi-objective evolutionary algorithm. This mechanism enables simultaneous optimization of feature selection and classification performance by exploring Pareto-optimal solutions in a single run. This solution helps monitor various manufacturing operations, addressing a range of conflicting objectives that need to be minimized together. Manufacturers can leverage such predictive methods and better adapt to emerging trends. To strengthen the validation of our model, we incorporate two real-world datasets from different industrial domains. The results on both datasets demonstrate the generalizability and effectiveness of our approach.", "AI": {"tldr": "提出一种基于多目标优化的特征选择方法，用于智能制造系统中的故障检测", "motivation": "制造业需要集成人工智能技术来提高效率、自动化水平和产品质量，通过可靠的故障检测方法可以改善产品质量、提高产量并降低生产成本", "method": "提出混合框架，采用基于支配关系的多目标进化算法，在单次运行中同时优化特征选择和分类性能，探索帕累托最优解", "result": "在两个不同工业领域的真实数据集上验证了模型，结果表明该方法具有良好的泛化能力和有效性", "conclusion": "提出的多目标优化特征选择方法能够帮助制造商监控各种制造操作，处理多个冲突目标的同时优化问题，有助于适应新兴趋势"}}
{"id": "2512.05969", "pdf": "https://arxiv.org/pdf/2512.05969", "abs": "https://arxiv.org/abs/2512.05969", "authors": ["Hokin Deng"], "title": "Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices", "categories": ["cs.CV", "cs.AI"], "comment": "See $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and $\\href{https://github.com/hokindeng/VMEvalKit}{code}$", "summary": "We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the \"Task Pair\" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.", "AI": {"tldr": "该论文展示了视频生成模型在推理任务上的能力，包括国际象棋、迷宫、数独、心理旋转和瑞文矩阵等任务，并建立了可扩展的评估框架。", "motivation": "探索视频生成模型是否具备推理能力，并建立一个可扩展的评估范式来系统测试模型在各种推理任务上的表现。", "method": "提出了\"任务对\"设计的实验范式，构建了包含39个模型的代码框架VMEvalKit，支持自动评估并与人类判断强相关。", "result": "领先的视频模型如Sora-2在推理任务上达到60%的成功率，自动评估与人类判断强相关，证明了该范式的有效性。", "conclusion": "视频生成模型已开始具备推理能力，建立的评估范式具有高度可扩展性，为通过强化学习改进视频模型的推理能力提供了机会。"}}
{"id": "2512.05967", "pdf": "https://arxiv.org/pdf/2512.05967", "abs": "https://arxiv.org/abs/2512.05967", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiovì"], "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.", "AI": {"tldr": "该论文提出了一种增强的检索增强生成（RAG）架构，通过集成实体链接技术来提高教育平台问答系统的准确性，特别是在意大利语教育领域。", "motivation": "传统基于语义相似度的RAG系统在专业领域（如教育）中，由于术语歧义问题，难以保证事实准确性。特别是在意大利语教育平台中，需要更精确的检索机制来确保回答的可靠性。", "method": "提出了一种增强的RAG架构，包含基于Wikidata的实体链接模块，并实现了三种重排序策略：混合分数加权模型、互逆排名融合和交叉编码器重排序器，以结合语义和实体信息。", "result": "在两个基准测试（自定义学术数据集和标准SQuAD-it数据集）上进行了实验。结果显示，在特定领域环境中，基于互逆排名融合的混合方案显著优于基线和交叉编码器方法；而在通用领域数据集上，交叉编码器表现最佳。", "conclusion": "研究证实了领域不匹配效应的存在，强调了领域适应和混合排序策略对于提高检索增强生成的事实精确性和可靠性的重要性。实体感知的RAG系统在教育环境中具有潜力，可促进自适应和可靠的AI辅助教学工具的发展。"}}
{"id": "2512.05965", "pdf": "https://arxiv.org/pdf/2512.05965", "abs": "https://arxiv.org/abs/2512.05965", "authors": ["Hongyu Li", "Manyuan Zhang", "Dian Zheng", "Ziyu Guo", "Yimeng Jia", "Kaituo Feng", "Hao Yu", "Yexin Liu", "Yan Feng", "Peng Pei", "Xunliang Cai", "Linjiang Huang", "Hongsheng Li", "Si Liu"], "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor", "categories": ["cs.CV"], "comment": "Project page: https://appletea233.github.io/think-while-edit", "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.", "AI": {"tldr": "开发了一个名为EditThinker的迭代推理框架，通过\"思考-编辑\"循环来提升基于指令的图像编辑模型的指令遵循能力", "motivation": "现有的基于指令的图像编辑方法虽然能生成高质量图像，但单次成功率有限，主要受随机性和缺乏深思熟虑的限制，需要更好的指令遵循能力", "method": "提出了一个深思熟虑的编辑框架，模拟人类认知循环，通过迭代执行\"思考-编辑\"循环：批判结果、精炼指令，然后重复生成直到满意。训练了一个单一的多模态大语言模型EditThinker作为推理引擎，联合生成批判分数、推理过程和精炼指令，并使用强化学习对齐模型的思考与编辑过程", "result": "在四个基准测试上的广泛实验表明，该方法显著提升了任何图像编辑模型的指令遵循能力，取得了大幅度的改进", "conclusion": "EditThinker框架通过迭代推理解锁了图像编辑的深思熟虑能力，能够显著提高指令遵循性能，并将发布数据构建框架、数据集和模型以造福社区"}}
{"id": "2512.05964", "pdf": "https://arxiv.org/pdf/2512.05964", "abs": "https://arxiv.org/abs/2512.05964", "authors": ["Kevin Black", "Allen Z. Ren", "Michael Equi", "Sergey Levine"], "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.", "AI": {"tldr": "提出训练时动作条件化方法替代推理时修复，实现高效实时分块控制，降低计算开销", "motivation": "实时分块(RTC)通过推理时修复实现动作条件化，但引入计算开销增加推理延迟，需要更高效的替代方案", "method": "在训练时模拟推理延迟，直接对动作前缀进行条件化，无需修改模型架构或机器人运行时，仅需少量代码", "result": "在模拟实验中，训练时RTC在高推理延迟下优于推理时RTC；在真实世界实验中（盒装和制作咖啡任务），保持任务性能和速度对等，同时计算成本更低", "conclusion": "训练时动作条件化是推理时修复的实用替代方案，适用于实时机器人控制，计算效率更高"}}
{"id": "2512.05962", "pdf": "https://arxiv.org/pdf/2512.05962", "abs": "https://arxiv.org/abs/2512.05962", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "AI": {"tldr": "该论文提出了一种通过过滤驱动推理的方法来解决LLMs在强化学习训练中多样性损失的问题，使用α-散度家族来平衡精度和多样性。", "motivation": "强化学习已成为调整LLMs进行推理任务的标准方法，但这种方法往往导致模型多样性显著损失。作者认为这是因为RL隐式优化了\"模式寻求\"或\"零强制\"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。", "method": "从显式目标分布出发，通过过滤掉错误答案同时保留正确答案的相对概率。从预训练的LLM开始，使用α-散度家族来近似这个目标分布，该家族统一了先前方法，并通过在模式寻求和质量覆盖散度之间插值来直接控制精度-多样性权衡。", "result": "在Lean定理证明基准测试中，该方法在覆盖精度帕累托前沿上实现了最先进的性能，在覆盖轴上优于所有先前方法。", "conclusion": "通过过滤驱动推理并使用α-散度家族来控制精度-多样性权衡，可以有效解决LLMs在强化学习训练中的多样性损失问题，在定理证明任务中取得了更好的覆盖性能。"}}
{"id": "2512.05960", "pdf": "https://arxiv.org/pdf/2512.05960", "abs": "https://arxiv.org/abs/2512.05960", "authors": ["Munsif Ali", "Najmul Hassan", "Lucia Ventura", "Davide Di Bari", "Simonepietro Canese"], "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.", "AI": {"tldr": "提出AQUA-Net网络，通过自适应频率融合和光照感知机制来增强水下图像，解决颜色失真、低对比度和雾状外观问题，同时保持较低的计算复杂度。", "motivation": "水下图像因波长依赖的光吸收和散射而存在严重颜色失真、低对比度和雾状外观问题，现有深度学习模型计算复杂度高，限制了实时应用部署。", "method": "采用残差编码器-解码器架构，集成双辅助分支：频率融合编码器从傅里叶域提取频率线索，光照感知解码器基于Retinex理论通过学习的照明图进行自适应曝光校正。", "result": "在多个基准数据集上的实验表明，AQUA-Net在定性和定量评估中与SOTA方法性能相当，但使用更少的参数；消融研究证实频率和光照分支提供互补贡献，改善了可见性和颜色表示。", "conclusion": "AQUA-Net展示了强大的泛化能力和鲁棒性，为真实世界水下成像应用提供了有效解决方案，同时提出了一个高分辨率的地中海真实水下视频数据集。"}}
{"id": "2512.05959", "pdf": "https://arxiv.org/pdf/2512.05959", "abs": "https://arxiv.org/abs/2512.05959", "authors": ["David Anugraha", "Patrick Amadeus Irawan", "Anshul Singh", "En-Shiun Annie Lee", "Genta Indra Winata"], "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.", "AI": {"tldr": "提出M4-RAG基准，用于评估多语言多模态检索增强生成系统在视觉问答任务中的表现", "motivation": "现有视觉语言模型受限于静态训练数据，而多语言多模态RAG系统研究不足，需要大规模基准来评估跨语言、跨文化和多模态的检索增强能力", "method": "构建包含42种语言、56种方言和语域的大规模基准，包含超过80,000个文化多样性图像-问题对，并建立受控检索环境包含数百万精心策划的多语言文档", "result": "系统评估显示RAG对较小视觉语言模型有益，但对较大模型效果不佳甚至降低性能，揭示了模型规模与当前检索效果之间的不匹配问题", "conclusion": "M4-RAG为推进下一代RAG系统提供了基础，使其能够在语言、模态和文化背景下无缝推理"}}
{"id": "2512.05958", "pdf": "https://arxiv.org/pdf/2512.05958", "abs": "https://arxiv.org/abs/2512.05958", "authors": ["Sara Patel", "Mingxun Zhou", "Giulia Fanti"], "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.", "AI": {"tldr": "提出MaxShapley算法，用于生成式搜索引擎中的公平内容贡献度归因和补偿机制", "motivation": "随着基于大语言模型的生成式搜索引擎取代传统搜索，需要建立公平的机制来归因和补偿内容提供者，以维持生态系统可持续发展", "method": "MaxShapley是一种高效的公平归因算法，基于可分解的最大和效用函数，将Shapley值的指数计算复杂度降低到线性复杂度", "result": "在三个多跳问答数据集上的评估显示，MaxShapley在保持与精确Shapley值相当归因质量的同时，显著降低资源消耗（最高减少8倍）", "conclusion": "MaxShapley为生成式搜索生态系统提供了高效、公平的内容贡献度归因解决方案，有助于建立激励兼容的补偿机制"}}
{"id": "2512.05955", "pdf": "https://arxiv.org/pdf/2512.05955", "abs": "https://arxiv.org/abs/2512.05955", "authors": ["Haowen Liu", "Shaoxiong Yao", "Haonan Chen", "Jiawei Gao", "Jiayuan Mao", "Jia-Bin Huang", "Yilun Du"], "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io", "AI": {"tldr": "提出SIMPACT框架，通过仿真增强视觉语言模型的动作规划能力", "motivation": "视觉语言模型缺乏对物理动力学的理解，难以应用于需要物理推理的精细机器人操作任务", "method": "在测试时通过仿真循环构建物理模拟，让VLM提出动作、观察仿真结果并迭代优化推理", "result": "在五个具有挑战性的刚体和可变形物体操作任务上达到最先进性能", "conclusion": "通过测试时集成物理仿真来增强VLM推理，是实现通用具身智能的有前景路径"}}
{"id": "2512.05954", "pdf": "https://arxiv.org/pdf/2512.05954", "abs": "https://arxiv.org/abs/2512.05954", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code", "categories": ["cs.AI"], "comment": null, "summary": "We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems", "AI": {"tldr": "SymPyBench是一个大规模合成基准测试，包含15,045个大学物理问题，支持无限参数配置，提供结构化推理步骤和可执行Python代码，用于评估科学推理能力。", "motivation": "现有科学推理基准测试通常缺乏动态性和可执行性，无法充分评估模型在参数变化下的稳定性和一致性。需要一种能够测试模型在不同问题变体上表现一致性的基准。", "method": "创建参数化物理问题数据集，包含三种问题类型（MC-Symbolic、MC-Numerical、free-form），每个问题附带结构化推理步骤和可执行Python代码。引入三个新评估指标：一致性分数、失败率和混淆率。", "result": "实验显示最先进的指令调优语言模型在科学推理方面既有优势也有局限。新评估指标能有效量化模型在不同问题变体上的变异性和不确定性。", "conclusion": "SymPyBench为开发更鲁棒和可解释的推理系统提供了基础，其动态、代码驱动的特性能够更全面地评估科学推理能力。"}}
{"id": "2512.05953", "pdf": "https://arxiv.org/pdf/2512.05953", "abs": "https://arxiv.org/abs/2512.05953", "authors": ["Yunhao Cao", "Zubin Bhaumik", "Jessie Jia", "Xingyi He", "Kuan Fang"], "title": "Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning", "categories": ["cs.RO"], "comment": null, "summary": "We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.", "AI": {"tldr": "COIL是一个用于视觉运动控制的模仿学习框架，通过3D关键点对应关系表示任务，支持可变空间和时间粒度的任务规范。", "motivation": "现有方法通常假设固定数量的关键点或均匀时间间隔，无法适应不同用户意图和任务需求的可变空间和时间粒度。", "method": "使用3D关键点对应关系表示任务，设计具有时空注意力机制的条件策略，通过模拟中的自监督管道训练，自动生成对应标签。", "result": "COIL在真实世界操作任务中，在稀疏和密集规范下都优于先前方法，能够跨任务、对象和运动模式泛化。", "conclusion": "COIL提供了一个灵活的任务表示框架，通过对应关系导向的方法实现了强大的视觉运动控制，支持可变粒度的任务规范。"}}
{"id": "2512.05951", "pdf": "https://arxiv.org/pdf/2512.05951", "abs": "https://arxiv.org/abs/2512.05951", "authors": ["Teofil Bodea", "Masanori Misono", "Julian Pritzi", "Patrick Sabanic", "Thore Sommer", "Harshavardhan Unnibhavi", "David Schall", "Nuno Santos", "Dimitrios Stavrakakis", "Pramod Bhatotia"], "title": "Trusted AI Agents in the Cloud", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.", "AI": {"tldr": "开发Omega系统，为云端的AI代理提供端到端的安全隔离和可信执行环境，确保在多主体生态系统中保护敏感数据、防止篡改，并实现可验证的信任和监督。", "motivation": "当前基于大语言模型的AI代理在云端部署时面临安全挑战：它们访问敏感数据、调用外部工具并与其他代理交互，但运行在复杂的多方生态系统中，不受信任的组件可能导致数据泄露、篡改或意外行为。现有的机密虚拟机仅提供二进制级别的保护，无法保证跨主体信任、加速器级隔离或受监督的代理行为。", "method": "Omega系统基于机密虚拟机和机密GPU构建可信代理平台，使用嵌套隔离在单个CVM内托管多个代理。通过差异认证建立跨主体信任，提供策略规范和执行框架来管理数据访问、工具使用和代理间通信，确保数据保护和合规性。", "result": "在AMD SEV-SNP和NVIDIA H100上实现Omega系统，完全保护代理状态跨越CVM-GPU边界，实现高性能的同时支持高密度、符合策略的多代理部署，达到云规模。", "conclusion": "Omega系统成功解决了云端AI代理的安全挑战，通过端到端隔离、可验证的跨主体信任和监督机制，为可信AI代理提供了可行的解决方案，能够在云规模下安全部署高密度多代理系统。"}}
{"id": "2512.05950", "pdf": "https://arxiv.org/pdf/2512.05950", "abs": "https://arxiv.org/abs/2512.05950", "authors": ["Zalish Mahmud", "Anantaa Kotal", "Aritran Piplai"], "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025", "AI": {"tldr": "提出Impugan，一种基于条件生成对抗网络（cGAN）的鲁棒数据填补方法，用于处理不完整数据和整合异构数据集", "motivation": "现实世界数据普遍存在不完整问题，传统填补方法（如回归模型、期望最大化、多重填补）依赖于线性和独立性假设，这些假设在复杂或异构数据中很少成立，导致有偏或过度平滑的估计", "method": "使用条件生成对抗网络（cGAN），生成器基于观测特征重建缺失值，判别器区分真实数据与填补数据，通过对抗训练学习观测变量与缺失变量之间的非线性、多模态依赖关系", "result": "在基准数据集和多源整合任务中，Impugan相比领先基线方法实现了高达82%的地球移动距离（EMD）降低和70%的互信息偏差（MI）降低", "conclusion": "对抗训练的生成模型为填补和合并不完整、异构数据提供了可扩展且原则性的方法，能够捕捉传统方法无法表示的非线性和多模态关系"}}
{"id": "2512.05946", "pdf": "https://arxiv.org/pdf/2512.05946", "abs": "https://arxiv.org/abs/2512.05946", "authors": ["Truong Thanh Hung Nguyen", "Truong Thinh Nguyen", "Hung Cao"], "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem", "categories": ["cs.AI", "cs.ET", "cs.SE"], "comment": "Quantum Software Engineering Practices at The 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026)", "summary": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.", "AI": {"tldr": "本文提出了Variational Quantum Rainbow DQN (VQR-DQN)，将环形拓扑变分量子电路与Rainbow DQN结合，用于优化人力资源分配问题", "motivation": "资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表示能力，需要利用量子计算的叠加和纠缠特性来增强表示能力", "method": "将人力资源分配问题建模为马尔可夫决策过程，结合环形拓扑变分量子电路与Rainbow DQN，利用量子叠加和纠缠增强策略表示", "result": "在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少了26.8%的归一化完工时间，相比Double DQN和经典Rainbow DQN提升了4.9-13.4%", "conclusion": "VQR-DQN展示了量子增强深度强化学习在大规模资源分配问题中的潜力，电路表达能力、纠缠与策略质量之间存在理论联系"}}
{"id": "2512.05943", "pdf": "https://arxiv.org/pdf/2512.05943", "abs": "https://arxiv.org/abs/2512.05943", "authors": ["Shima Imani", "Seungwhan Moon", "Lambert Mathias", "Lu Zhang", "Babak Damavandi"], "title": "TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.", "AI": {"tldr": "TRACE是一个用于分析和增强视觉语言模型逐步推理能力的框架，通过透明推理和一致性评估来诊断推理轨迹而非仅评估最终结果。", "motivation": "可靠的数学和科学推理仍然是大型视觉语言模型面临的开放挑战。标准的最终答案评估常常掩盖推理错误，导致无声故障持续存在。需要一种能够诊断推理轨迹而非仅评估最终结果的框架。", "method": "TRACE框架利用辅助推理集（ARS），这些是紧凑的子问题-答案对，用于分解复杂问题。通过基于一致性的指标评估中间步骤，并暴露标准评估忽略的故障。框架还定义了置信区域来区分可靠和不可靠的推理路径。", "result": "实验表明，ARS之间的一致性相关于最终答案的正确性，有助于精确定位推理步骤中出现故障的位置，为模型改进提供可操作的信号。置信区域支持有效的过滤、调试和模型优化。", "conclusion": "TRACE框架通过透明推理和一致性评估，为分析和增强视觉语言模型的逐步推理能力提供了有效工具，能够诊断推理轨迹中的问题并支持模型改进。"}}
{"id": "2512.05941", "pdf": "https://arxiv.org/pdf/2512.05941", "abs": "https://arxiv.org/abs/2512.05941", "authors": ["Zhiyuan Jiang", "Shenghao Xie", "Wenyi Li", "Wenqiang Zu", "Peihang Li", "Jiahao Qiu", "Siqi Pei", "Lei Ma", "Tiejun Huang", "Mengdi Wang", "Shilong Liu"], "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick", "summary": "Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.", "AI": {"tldr": "提出ZoomClick方法，利用缩放作为GUI定位的强先验，无需训练即可提升GUI定位模型的性能", "motivation": "现有GUI定位方法依赖大规模边界框监督，但仍面临跨平台泛化、复杂布局分析和细粒度元素定位等挑战，缩放作为一种未被充分探索的先验具有潜力", "method": "提出训练免费的ZoomClick方法，通过表征缩放的四个关键属性（预缩放、深度、收缩尺寸、最小裁剪尺寸）来解锁其动态空间聚焦和自适应上下文切换能力", "result": "方法显著提升通用视觉语言模型和专用GUI定位模型的性能，在多个主流基准测试中达到最先进水平，如UI-Venus-72B在ScreenSpot-Pro上达到73.1%成功率", "conclusion": "缩放是GUI定位的强大先验，提出的ZoomClick方法有效且无需训练，同时创建了GUIZoom-Bench基准来评估模型对缩放的适应性，为未来研究提供方向"}}
{"id": "2512.05937", "pdf": "https://arxiv.org/pdf/2512.05937", "abs": "https://arxiv.org/abs/2512.05937", "authors": ["Anne Sielemann", "Valentin Barner", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn", "Juergen Beyerer"], "title": "Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "8 pages, 2 figures, 7 tables", "summary": "Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...]. Download: synset.de/datasets/synset-signset-ger/background-effect", "AI": {"tldr": "该论文研究了背景相关性对深度学习分类任务的影响，通过合成数据集量化背景特征在交通标志识别中的重要性变化", "motivation": "当前可解释AI方法（如SHAP和GradCAM）虽然能分析输入特征对分类结果的重要性，但难以定量测试这些解释的可靠性。特别是难以区分背景相关性是合理的还是虚假的，需要系统化的方法来量化背景对分类的影响", "method": "系统生成六个合成数据集用于交通标志识别任务，这些数据集仅在相机变化程度和背景相关性上有所不同。通过控制变量方法，量化背景相关性、相机变化水平和交通标志形状对分类性能和背景特征重要性的独立影响", "result": "研究量化了背景特征在何时以及多大程度上获得重要性以支持分类任务，揭示了训练域变化如何影响背景特征的重要性变化。提供了背景相关性对分类性能影响的具体测量结果", "conclusion": "通过合成数据集的方法，能够系统化地量化背景相关性对深度学习分类的影响，为理解可解释AI方法的输出提供了更可靠的基础，并揭示了训练数据特性如何影响模型对背景特征的依赖"}}
{"id": "2512.05936", "pdf": "https://arxiv.org/pdf/2512.05936", "abs": "https://arxiv.org/abs/2512.05936", "authors": ["Anne Sielemann", "Lena Loercher", "Max-Lion Schumacher", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn"], "title": "Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 8 figures, 3 tables", "summary": "In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.", "AI": {"tldr": "该论文提出了一个用于德国交通标志识别的合成数据集生成管道和数据集，结合了数据驱动和解析建模的优势。", "motivation": "解决交通标志识别任务中训练/测试数据的需求，特别是针对2020年新发布的罕见交通标志，同时提供可解释AI和鲁棒性测试的应用可能性。", "method": "使用GAN生成纹理实现数据驱动的污垢和磨损伪影，结合解析场景调制实现物理正确的光照和详细参数化，创建包含105500张图像、211个德国交通标志类别的合成数据集。", "result": "生成了Synset Signset Germany合成数据集，包含掩码、分割图像和丰富的元数据，在真实世界GTSRB基准测试中评估了真实度，并与最先进的合成数据集CATERED进行了比较。", "conclusion": "该合成数据集结合了数据驱动和解析建模的优势，为交通标志识别提供了高质量的合成数据，特别适用于可解释AI和鲁棒性测试应用。"}}
{"id": "2512.05932", "pdf": "https://arxiv.org/pdf/2512.05932", "abs": "https://arxiv.org/abs/2512.05932", "authors": ["L. Dudzik", "M. Roschani", "A. Sielemann", "K. Trampert", "J. Ziehn", "J. Beyerer", "C. Neumann"], "title": "Physically-Based Simulation of Automotive LiDAR", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter. Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties. Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern. The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.", "AI": {"tldr": "提出了一种用于模拟汽车激光雷达的物理基础分析模型，该模型包含光晕效应、回波脉冲宽度和环境光等关键因素，并通过光学实验室测量系统确定模型参数。", "motivation": "现有的汽车激光雷达模拟方法往往缺乏物理基础，无法准确模拟真实世界中的复杂光学现象，如光晕效应、回波脉冲宽度变化和环境光干扰等。需要一种能够系统确定参数并通过实验室测量验证的物理基础模拟方法。", "method": "开发了一个分析模型，使用近红外域的物理基础渲染技术，假设单次反射和逆向反射。模型包含灵活的波束导向模式和非零直径的发射/接收二极管。通过光学实验室测量（使用测角仪在0.01°分辨率下测量不同目标表面的光度亮度）系统确定模型参数。", "result": "该方法成功在Valeo Scala Gen. 2和Blickfeld Cube 1两种汽车激光雷达系统上进行了校准和测试。尽管这两种系统在特性和可用接口上存在显著差异，但相关模型参数都能成功提取。", "conclusion": "提出的物理基础激光雷达模拟模型能够准确模拟真实汽车激光雷达的关键光学特性，通过系统化的实验室测量方法可以确定模型参数，为不同激光雷达系统提供统一的模拟框架。"}}
{"id": "2512.05930", "pdf": "https://arxiv.org/pdf/2512.05930", "abs": "https://arxiv.org/abs/2512.05930", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.", "AI": {"tldr": "PRiSM是一个用于评估科学推理能力的多模态基准测试，通过Python代码执行进行验证，包含超过24,750个大学水平的物理和数学问题，采用动态生成和可执行代码验证机制。", "motivation": "当前评估视觉语言模型在科学领域的基准存在局限性：缺乏中间推理步骤、对变化的鲁棒性不足、缺少验证科学正确性的机制。现有数据集大多是静态的，无法充分评估模型在数学和物理等需要概念理解、符号推理和遵循形式法则的领域的能力。", "method": "1. 开发PRiSM基准：包含24,750+大学水平物理和数学问题，每个问题包含动态文本和视觉输入、生成图像；2. 使用PrismAgent代理管道生成结构化问题实例；3. 提供丰富结构化输出：用于生成和验证的可执行Python代码、详细逐步推理；4. 提出五个评估任务：泛化能力、符号程序合成、扰动鲁棒性、推理修正、歧义解析。", "result": "PRiSM基准能够对多模态视觉语言模型进行细粒度实验审计，揭示失败模式、不确定性行为和科学推理局限性。通过全面评估现有VLM，展示了它们在科学推理能力方面的限制，并证明PRiSM能够提供更深入的洞察。", "conclusion": "PRiSM通过Python代码执行验证的动态多模态基准测试，填补了科学推理评估领域的空白，能够更全面、准确地评估视觉语言模型在数学和物理等科学领域的推理能力，为模型改进提供有价值的诊断工具。"}}
{"id": "2512.05928", "pdf": "https://arxiv.org/pdf/2512.05928", "abs": "https://arxiv.org/abs/2512.05928", "authors": ["Pedro Vidal", "Bernardo Biesseck", "Luiz E. L. Coelho", "Roger Granada", "David Menotti"], "title": "A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition", "categories": ["cs.CV"], "comment": "18 pages, 17 figures", "summary": "Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.", "AI": {"tldr": "本文对用于人脸识别的合成人脸数据生成技术进行了比较研究，评估不同生成方法在面部识别任务中的效果", "motivation": "人脸识别面临隐私法规、数据集退化、人口统计偏差、可解释性、鲁棒性等挑战，合成人脸数据被提出作为有前景的解决方案，能够缓解隐私问题、控制面部属性、减轻人口统计偏差并为模型提供补充数据", "method": "比较不同技术生成的合成人脸数据集在面部识别任务中的有效性，在八个领先数据集上评估准确率、rank-1、rank-5以及假阳性率为0.01%时的真阳性率，提供文献中未广泛探索的比较分析", "result": "结果表明合成数据能够捕捉真实变化，但需要进一步研究来缩小与真实数据的性能差距。扩散模型、GANs和3D模型等技术显示出显著进展，但仍面临挑战", "conclusion": "合成人脸数据生成技术在人脸识别领域具有潜力，能够解决隐私和偏差等问题，但当前技术仍需改进以达到与真实数据相当的性能水平"}}
{"id": "2512.05927", "pdf": "https://arxiv.org/pdf/2512.05927", "abs": "https://arxiv.org/abs/2512.05927", "authors": ["Zhiting Mei", "Tenny Yin", "Micah Baker", "Ola Shorinwa", "Anirudha Majumdar"], "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", "AI": {"tldr": "提出C3方法，一种用于可控视频生成模型的校准不确定性量化技术，能够在子补丁级别进行密集置信度估计，精确定位生成视频帧中的不确定性区域。", "motivation": "当前可控视频生成模型存在幻觉问题（生成与物理现实不符的视频帧），在机器人策略评估和规划等任务中带来严重问题。现有视频模型缺乏评估和表达置信度的能力，阻碍了幻觉缓解。", "method": "提出C3不确定性量化方法，包含三个核心创新：1）通过严格适当评分规则训练视频模型以获得正确性和校准性；2）在潜在空间估计视频模型的不确定性，避免像素空间方法的不稳定性和高昂训练成本；3）将密集潜在空间不确定性映射到可解释的像素级RGB空间不确定性，提供高分辨率不确定性热图。", "result": "在大型机器人学习数据集（Bridge和DROID）和真实世界评估中，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测。", "conclusion": "C3方法为可控视频生成模型提供了校准的不确定性量化能力，能够精确定位生成视频中的不可信区域，有助于缓解幻觉问题，提高模型在机器人学习等关键应用中的可靠性。"}}
{"id": "2512.05926", "pdf": "https://arxiv.org/pdf/2512.05926", "abs": "https://arxiv.org/abs/2512.05926", "authors": ["Wenyan Luo", "Dustin G. Mixon"], "title": "BalLOT: Balanced $k$-means clustering with optimal transport", "categories": ["stat.ML", "cs.DS", "cs.IT", "cs.LG", "math.OC"], "comment": "20 pages, 9 figures", "summary": "We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.", "AI": {"tldr": "提出BalLOT方法，使用最优传输解决平衡k-means聚类问题，实现快速有效的聚类解决方案", "motivation": "解决平衡k-means聚类这一基本问题，传统方法在处理平衡约束时效率较低，需要更有效的解决方案", "method": "引入基于最优传输的交替最小化方法BalLOT，通过最优传输处理平衡约束，结合交替优化策略", "result": "实验证明BalLOT快速有效；理论证明对通用数据产生整数耦合；在随机球模型下实现精确和部分恢复；提出实现一步恢复的初始化方案", "conclusion": "BalLOT为平衡k-means聚类提供了理论保证的快速有效解决方案，结合了最优传输和交替最小化的优势"}}
{"id": "2512.05925", "pdf": "https://arxiv.org/pdf/2512.05925", "abs": "https://arxiv.org/abs/2512.05925", "authors": ["Federico Bianchi", "Yongchan Kwon", "Zachary Izzo", "Linjun Zhang", "James Zou"], "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.", "AI": {"tldr": "开发基于GPT-5的论文正确性检查器，系统量化已发表AI论文中的客观错误数量及其随时间的变化趋势", "motivation": "同行评审出版物是构建新研究知识的基础，但文献中持续存在的错误会传播并造成混淆，加速的研究节奏和日益增长的同行评审需求使错误更难被发现和避免", "method": "使用基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊已发表论文中的客观错误（如公式、推导、计算、图表错误），排除主观考量，并由人类专家验证AI识别的错误", "result": "发现已发表论文包含不可忽视的客观错误数量，且每篇论文平均错误数随时间增加：NeurIPS从2021年的3.8个增加到2025年的5.9个（增长55.3%）；ICLR从2018年的4.1个增加到2025年的5.2个；TMLR从2022/23年的5.0个增加到2025年的5.5个。AI检查器识别错误的精确度为83.2%，并能对75.8%的识别错误提出正确修正", "conclusion": "前沿大语言模型在检测和修正已发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础，虽然大多数问题相对较小，但修正它们可以减少文献混淆并增强可复现性"}}
{"id": "2512.05922", "pdf": "https://arxiv.org/pdf/2512.05922", "abs": "https://arxiv.org/abs/2512.05922", "authors": ["Khang Le", "Anh Mai Vu", "Thi Kim Trang Vo", "Ha Thach", "Ngoc Bui Lam Quang", "Thanh-Huy Nguyen", "Minh H. N. Le", "Zhu Han", "Chandra Mohan", "Hien Van Nguyen"], "title": "LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation", "categories": ["cs.CV"], "comment": "Note: Khang Le and Anh Mai Vu contributed equally", "summary": "Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.", "AI": {"tldr": "提出一种用于弱监督组织病理学分割的聚类无关单阶段可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖", "motivation": "解决弱监督组织病理学分割中的三个主要挑战：类间同质性、类内异质性和CAM引起的区域收缩问题。现有两阶段方法成本高、对超参数敏感，且将原型发现与分割学习解耦，限制了效率和效果", "method": "提出聚类无关的单阶段可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖，避免了两阶段方法的复杂性和解耦问题", "result": "在BCSS-WSSS数据集上达到最先进的性能，在mIoU和mDice指标上优于先前方法。定性分割图显示更清晰的边界和更少的错误标签，激活热图显示可学习原型覆盖了每个类别内更多样化和互补的区域", "conclusion": "提出的单阶段可学习原型框架通过多样性正则化有效解决了弱监督组织病理学分割中的挑战，相比基于聚类的方法更高效且能覆盖更多样化的类内区域"}}
{"id": "2512.05920", "pdf": "https://arxiv.org/pdf/2512.05920", "abs": "https://arxiv.org/abs/2512.05920", "authors": ["Jiawen Yang", "Yihui Cao", "Xuanyu Tian", "Yuyao Zhang", "Hongjiang Wei"], "title": "NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.", "AI": {"tldr": "提出NICE（神经隐式颅面模型），用于正颌手术的面部软组织变形预测，通过隐式神经表示实现精确的解剖重建和手术结果预测。", "motivation": "正颌手术是矫正牙颌面骨骼畸形的重要干预手段，但术后面部外观预测面临挑战，因为骨骼移动与面部软组织之间存在复杂的非线性相互作用。现有的生物力学模型、参数化模型和深度学习方法要么计算效率低，要么无法完全捕捉这些复杂的相互作用。", "method": "NICE包含两个模块：1）形状模块，使用区域特定的隐式符号距离函数（SDF）解码器重建面部表面、上颌骨和下颌骨；2）手术模块，使用区域特定的变形解码器，这些解码器由共享的手术潜在代码驱动，以建模面部表面对骨骼移动的复杂非线性生物力学响应，并融入解剖先验知识。", "result": "大量实验表明，NICE优于当前最先进的方法，显著提高了关键面部区域（如嘴唇和下巴）的预测准确性，同时稳健地保持了解剖完整性。", "conclusion": "这项工作为增强正颌手术的手术规划和患者咨询提供了临床可行的工具，通过神经隐式表示有效解决了面部软组织变形预测的挑战。"}}
{"id": "2512.05918", "pdf": "https://arxiv.org/pdf/2512.05918", "abs": "https://arxiv.org/abs/2512.05918", "authors": ["Xiaobo Wu", "Youmin Zhang"], "title": "A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following", "categories": ["eess.SP", "cs.RO", "stat.ML"], "comment": null, "summary": "Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol.", "AI": {"tldr": "提出一种残差方差匹配递归最小二乘滤波器，用于无人机地形跟随任务中的实时航点估计", "motivation": "现有实时滤波算法在非线性时变系统中难以在测量噪声下保持准确的航点估计，影响无人机飞行安全和野火检测任务", "method": "基于残差方差匹配估计准则，开发了残差方差匹配递归最小二乘滤波器，自适应估计非线性时变无人机地形跟随系统的实时航点", "result": "在模拟地形环境中验证，RVM-RLS滤波器相比基准算法在多个评估指标上提高航点估计精度约88%", "conclusion": "该方法在实时滤波方面具有方法学进步，RVM-RLS滤波器在无人机在线野火巡逻中具有实际应用潜力"}}
{"id": "2512.05908", "pdf": "https://arxiv.org/pdf/2512.05908", "abs": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": "Accepted at LLM4Code Workshop, ICSE 2026", "summary": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "AI": {"tldr": "该论文提出了一种通过自然语言摘要实现微服务架构中多仓库缺陷定位的方法，将代码库转换为层次化的自然语言摘要，然后进行自然语言到自然语言的搜索，而不是跨模态检索。", "motivation": "微服务架构中的多仓库缺陷定位面临三个主要挑战：自然语言缺陷报告与代码之间的语义鸿沟、LLM上下文限制、以及需要首先识别正确的仓库。现有方法如检索增强生成(RAG)系统在处理大规模多仓库代码库时效果有限。", "method": "方法包括两个核心步骤：1) 将代码库转换为层次化的自然语言摘要（文件级、目录级、仓库级），2) 采用两阶段搜索策略：首先将缺陷报告路由到相关仓库，然后在选定的仓库内进行自上而下的定位（仓库->目录->文件）。", "result": "在包含46个仓库和110万行代码的工业系统DNext上评估，该方法达到了Pass@10为0.82和MRR为0.50，显著优于检索基线和GitHub Copilot、Cursor等代理式RAG系统。", "conclusion": "研究表明，工程化的自然语言表示比原始源代码在可扩展的缺陷定位中更有效，提供了可解释的仓库->目录->文件搜索路径，这对于在企业AI工具中建立信任至关重要。"}}
{"id": "2512.05906", "pdf": "https://arxiv.org/pdf/2512.05906", "abs": "https://arxiv.org/abs/2512.05906", "authors": ["Lennart P. L. Landsmeer", "Amirreza Movahedin", "Said Hamdioui", "Christos Strydis"], "title": "EventQueues: Autodifferentiable spike event queues for brain simulation on AI accelerators", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neural networks (SNNs), central to computational neuroscience and neuromorphic machine learning (ML), require efficient simulation and gradient-based training. While AI accelerators offer promising speedups, gradient-based SNNs typically implement sparse spike events using dense, memory-heavy data-structures. Existing exact gradient methods lack generality, and current simulators often omit or inefficiently handle delayed spikes. We address this by deriving gradient computation through spike event queues, including delays, and implementing memory-efficient, gradient-enabled event queue structures. These are benchmarked across CPU, GPU, TPU, and LPU platforms. We find that queue design strongly shapes performance. CPUs, as expected, perform well with traditional tree-based or FIFO implementations, while GPUs excel with ring buffers for smaller simulations, yet under higher memory pressure prefer more sparse data-structures. TPUs seem to favor an implementation based on sorting intrinsics. Selective spike dropping provides a simple performance-accuracy trade-off, which could be enhanced by future autograd frameworks adapting diverging primal/tangent data-structures.", "AI": {"tldr": "开发了EventQueues——一种可自动微分的事件队列框架，用于在AI加速器上高效模拟和训练具有延迟的脉冲神经网络", "motivation": "当前脉冲神经网络模拟存在梯度计算缺乏通用性、内存效率低、延迟脉冲处理不足等问题，而AI加速器为高效模拟提供了硬件基础但缺乏合适的数据结构支持", "method": "通过推导基于脉冲事件队列（包括延迟）的梯度计算方法，实现了内存高效、支持梯度的队列数据结构，并在CPU、GPU、TPU、LPU等多种平台上进行基准测试", "result": "队列设计对性能影响显著：CPU适合传统树结构或FIFO实现，GPU在小规模模拟中环形缓冲区表现优异但在高内存压力下稀疏数据结构更优，TPU偏好基于排序原语的实现，选择性脉冲丢弃提供了性能-精度权衡", "conclusion": "EventQueues框架为AI加速器上的脉冲神经网络模拟提供了高效、可微分的解决方案，队列设计需针对不同硬件平台优化，未来自动微分框架可通过差异化原始/切线数据结构进一步提升性能"}}
{"id": "2512.05905", "pdf": "https://arxiv.org/pdf/2512.05905", "abs": "https://arxiv.org/abs/2512.05905", "authors": ["Wenhao Yan", "Sheng Ye", "Zhuoyi Yang", "Jiayan Teng", "ZhenHui Dong", "Kairui Wen", "Xiaotao Gu", "Yong-Jin Liu", "Jie Tang"], "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations", "categories": ["cs.CV"], "comment": null, "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.", "AI": {"tldr": "SCAIL是一个通过上下文学习3D一致姿态表示来实现工作室级角色动画的框架，旨在解决现有方法在复杂运动和跨身份动画中保持结构保真度和时间一致性的挑战。", "motivation": "尽管近期有进展，但实现符合工作室生产标准的角色动画仍然具有挑战性。现有方法可以将运动从驱动视频转移到参考图像，但在涉及复杂运动和跨身份动画的野外场景中，往往无法保持结构保真度和时间一致性。", "method": "提出两个关键创新：1) 新颖的3D姿态表示，提供更鲁棒和灵活的运动信号；2) 在扩散变换器架构中引入全上下文姿态注入机制，实现对完整运动序列的有效时空推理。同时开发了确保多样性和质量的策划数据管道。", "result": "实验表明SCAIL实现了最先进的性能，并将角色动画推向工作室级的可靠性和真实感。建立了全面的基准进行系统评估。", "conclusion": "SCAIL通过创新的3D姿态表示和全上下文姿态注入机制，结合精心策划的数据管道，显著提升了角色动画的质量，使其更接近工作室级的生产标准。"}}
{"id": "2512.05880", "pdf": "https://arxiv.org/pdf/2512.05880", "abs": "https://arxiv.org/abs/2512.05880", "authors": ["Simon Guiroy", "Mats Richter", "Sarath Chandar", "Christopher Pal"], "title": "Neural Coherence : Find higher performance to out-of-distribution tasks from few samples", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.", "AI": {"tldr": "该论文提出了一种名为\"神经一致性\"的新方法，用于在数据稀缺、无标签且分布外的情况下，从预训练模型检查点中选择最佳模型进行下游任务微调。", "motivation": "当前预训练大模型微调实践中，如何从大量训练检查点中选择最佳起点模型是一个开放问题。当目标任务数据稀缺、无标签且分布外时，依赖分布内验证数据的传统方法变得不可靠或不适用。", "method": "提出\"神经一致性\"概念，通过表征模型在源域和目标域上的激活统计特性，定义数据高效（仅需少量无标签目标样本）的模型选择方法。", "result": "在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，并在元学习设置中评估。相比基线方法，该方法显著提高了跨域泛化性能，并展示了在训练数据选择中的有效性。", "conclusion": "神经一致性是一个强大的原理，能够在数据稀缺、无标签且分布外的情况下，仅使用少量目标样本实现可靠的模型选择，显著提升跨域泛化性能。"}}
{"id": "2512.05866", "pdf": "https://arxiv.org/pdf/2512.05866", "abs": "https://arxiv.org/abs/2512.05866", "authors": ["Md. Mahbub Hasan Akash", "Aria Tasnim Mridula", "Sheekar Banerjee", "Ishtiak Al Mamoon"], "title": "Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator", "categories": ["cs.CV"], "comment": "This paper has been accepted for presentation at the IEEE 28th International Conference on Computer and Information Technology (ICCIT), December 2025", "summary": "Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.", "AI": {"tldr": "提出一种基于Swin Transformer和PatchGAN判别器的水下图像重建方法，用于解决水下图像的颜色失真、低对比度和雾化效应问题。", "motivation": "水下成像对海洋探索、环境监测和基础设施检查至关重要，但水会导致波长相关的吸收和散射，造成图像严重退化。传统方法和基于卷积神经网络的方法由于感受野有限和无法建模全局依赖关系，往往无法充分解决这些挑战。", "method": "提出了一种新颖的深度学习框架，将Swin Transformer架构集成到生成对抗网络(GAN)中。生成器采用带有Swin Transformer块的U-Net结构，以捕获局部特征和长距离依赖关系。使用PatchGAN判别器进行对抗训练以确保高频细节保留。", "result": "在EUVP数据集上训练和评估，取得了最先进的性能：PSNR为24.76 dB，SSIM为0.89，显著优于现有方法。视觉结果显示有效的色彩平衡恢复、对比度改善和雾化减少。消融研究证实了Swin Transformer设计优于卷积替代方案。", "conclusion": "该方法为各种海洋应用提供了稳健的水下图像重建解决方案，通过结合Swin Transformer的全局建模能力和GAN的对抗训练机制，有效解决了水下图像退化的核心问题。"}}
{"id": "2512.05865", "pdf": "https://arxiv.org/pdf/2512.05865", "abs": "https://arxiv.org/abs/2512.05865", "authors": ["Florent Draye", "Anson Lei", "Ingmar Posner", "Bernhard Schölkopf"], "title": "Sparse Attention Post-Training for Mechanistic Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\\approx 0.3 \\%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.", "AI": {"tldr": "提出一种简单的后训练方法，使Transformer注意力机制变得稀疏而不牺牲性能，通过稀疏正则化和约束损失目标，在保持原始预训练损失的同时将注意力连接减少到约0.3%", "motivation": "现有稀疏注意力方法主要关注计算效率，而本文利用稀疏性作为结构先验，旨在通过稀疏化暴露更有组织、更可解释的连接模式，为机制可解释性提供支持", "method": "采用灵活稀疏正则化的后训练方法，在约束损失目标下优化注意力权重，使注意力连接变得稀疏，同时保持模型性能", "result": "在高达10亿参数的模型上，能将注意力连接减少到约0.3%的边，同时保持原始预训练损失；局部稀疏性导致全局电路简化，任务特定电路涉及的组件（注意力头和MLP）更少，连接边减少高达100倍", "conclusion": "Transformer注意力可以变得数量级更稀疏，表明其大部分计算是冗余的，稀疏性可以作为构建更有结构和可解释模型的指导原则"}}
{"id": "2512.05863", "pdf": "https://arxiv.org/pdf/2512.05863", "abs": "https://arxiv.org/abs/2512.05863", "authors": ["Tasnimul Hassan", "Md Faisal Karim", "Haziq Jeelani", "Elham Behnam", "Robert Green", "Fayeq Jeelani Syed"], "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.", "AI": {"tldr": "本文提出了一种基于检索增强生成（RAG）的医学问答系统，通过结合领域特定知识检索与开源大语言模型来回答医学问题，旨在提高事实准确性并减少幻觉。", "motivation": "直接将大语言模型应用于临床领域存在挑战，如保持事实准确性和避免幻觉。医学问答系统需要可靠的信息来源和准确的回答，而现有LLMs在专业医学领域可能产生不准确或虚构的内容。", "method": "采用检索增强生成（RAG）框架，结合医学文献检索与开源LLMs（LLaMA~2和Falcon）。使用低秩适应（LoRA）对模型进行高效领域专业化微调，系统检索相关医学文献来支撑LLMs的回答。", "result": "在PubMedQA和MedMCQA基准数据集上的评估显示，检索增强显著提高了答案准确性。微调的LLaMA~2模型在PubMedQA上达到71.8%准确率，相比55.4%的零样本基线有显著提升。基于检索证据的回答减少了约60%的未支持内容。", "conclusion": "RAG增强的开源大语言模型在可靠的生物医学问答方面具有潜力，为实用的临床信息学应用指明了方向。通过将答案基于检索证据，系统提高了事实正确性并减少了幻觉。"}}
{"id": "2512.05859", "pdf": "https://arxiv.org/pdf/2512.05859", "abs": "https://arxiv.org/abs/2512.05859", "authors": ["Abhijith Punnappurath", "Luxi Zhao", "Ke Zhao", "Hue Nguyen", "Radek Grzeszczuk", "Michael S. Brown"], "title": "Edit-aware RAW Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.", "AI": {"tldr": "提出一种可插拔的编辑感知损失函数，用于增强RAW图像重建方法对不同渲染风格和编辑操作的鲁棒性", "motivation": "用户通常在拍摄后编辑相机图像以达到理想的后期处理效果。虽然RAW域编辑提供更高的准确性和灵活性，但大多数编辑都在相机显示参考输出（如8位sRGB JPEG）上进行，因为RAW图像很少被存储。现有RAW重建方法虽然可以从sRGB图像恢复RAW数据，但通常针对像素级RAW重建保真度优化，在不同渲染风格和编辑操作下性能会下降。", "method": "引入一个可插拔的编辑感知损失函数，可集成到任何现有RAW重建框架中。损失函数包含一个模块化、可微分的图像信号处理器（ISP），模拟具有可调参数的真实后期处理流程。训练期间，每个ISP模块的参数从精心设计的分布中随机采样，模拟真实相机处理中的实际变化。损失在sRGB空间中计算，通过这个可微分ISP渲染的地面真实RAW和重建RAW之间的差异。", "result": "在各种编辑条件下，结合该损失函数可将sRGB重建质量提升高达1.5-2 dB PSNR。当应用于元数据辅助的RAW重建方法时，该方法能够针对目标编辑进行微调，获得进一步增益。", "conclusion": "由于摄影编辑是消费级成像中RAW重建的主要动机，这种简单而有效的损失函数为增强现有方法的编辑保真度和渲染灵活性提供了一个通用机制。"}}
{"id": "2512.05853", "pdf": "https://arxiv.org/pdf/2512.05853", "abs": "https://arxiv.org/abs/2512.05853", "authors": ["Shiji Zhao", "Shukun Xiong", "Yao Huang", "Yan Jin", "Zhenyu Wu", "Jiyang Guan", "Ranjie Duan", "Jialing Tao", "Hui Xue", "Xingxing Wei"], "title": "VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.", "AI": {"tldr": "提出VRSA（视觉推理序列攻击）方法，通过将有害文本分解为多个顺序相关的子图像，诱导多模态大语言模型逐步外化和聚合完整的有害意图，实现对MLLMs的越狱攻击。", "motivation": "多模态大语言模型在视觉模态中的安全风险被忽视，现有研究主要关注文本模态的推理安全风险。由于MLLMs具备强大的推理能力，视觉推理任务中可能存在被利用进行越狱攻击的潜在风险，需要全面评估。", "method": "提出VRSA方法：1) 将原始有害查询分解为多个顺序相关的子图像；2) 自适应场景优化选择最相关场景；3) 语义连贯补全迭代重写子文本；4) 文本-图像一致性对齐保持语义一致性。通过视觉推理序列逐步诱导模型输出有害内容。", "result": "实验表明VRSA在开源和闭源MLLMs（如GPT-4o和Claude-4.5-Sonnet）上相比现有最先进的越狱攻击方法取得了更高的攻击成功率。", "conclusion": "VRSA方法有效揭示了多模态大语言模型在视觉推理任务中的安全漏洞，证明了视觉模态同样存在被利用进行越狱攻击的风险，为MLLMs的安全评估提供了新的视角和方法。"}}
{"id": "2512.05844", "pdf": "https://arxiv.org/pdf/2512.05844", "abs": "https://arxiv.org/abs/2512.05844", "authors": ["Daniel Rose", "Roxane Axel Jacob", "Johannes Kirchmair", "Thierry Langer"], "title": "NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.", "AI": {"tldr": "提出NEAT模型，一种用于3D分子生成的自回归集合变换器，通过邻域引导实现高效、顺序无关的分子图生成", "motivation": "现有自回归模型在3D分子生成中存在局限性：文本有自然顺序，但分子图中的下一个原子预测应该对原子排列顺序不变。先前工作使用规范顺序或焦点原子来回避这个问题，作者认为这是不必要的", "method": "提出NEAT模型，将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布。模型具有邻域引导、高效、自回归和集合变换器特性", "result": "NEAT在3D分子生成中达到接近最先进的性能，具有高计算效率和原子级排列不变性，为可扩展分子设计建立了实用基础", "conclusion": "NEAT模型成功解决了自回归分子生成中的顺序依赖问题，通过集合变换器方法实现了顺序无关的3D分子生成，为分子设计提供了高效实用的解决方案"}}
{"id": "2512.05837", "pdf": "https://arxiv.org/pdf/2512.05837", "abs": "https://arxiv.org/abs/2512.05837", "authors": ["Kaichen Ouyang"], "title": "Wasserstein Evolution : Evolutionary Optimization as Phase Transition", "categories": ["cs.NE"], "comment": "19 pages, 5 figures", "summary": "This paper establishes a novel connection between evolutionary computation and statistical physics by formalizing evolutionary optimization as a phase transition process. We introduce Wasserstein Evolution (WE), a principled optimization framework that implements the Wasserstein gradient flow of a free energy functional, mathematically bridging evolutionary dynamics with thermodynamics. WE directly translates the physical competition between potential gradient forces (exploitation) and entropic forces (exploration) into algorithmic dynamics, providing an adaptive, theoretically grounded mechanism for balancing exploration and exploitation. Experiments on challenging benchmark functions demonstrate that WE achieves competitive convergence performance while maintaining dramatically higher population diversity than classical methods (GA, DE, CMA-ES).This superior entropy preservation enables effective navigation of multi-modal landscapes without premature convergence, validating the physical interpretation of optimization as a disorder-to-order transition. Our work provides not only an effective optimization algorithm but also a new paradigm for understanding evolutionary computation through statistical physics.", "AI": {"tldr": "该论文提出了Wasserstein Evolution (WE)框架，将进化优化形式化为相变过程，建立了进化计算与统计物理学之间的新联系。", "motivation": "建立进化计算与统计物理学之间的理论联系，为进化优化提供物理基础，通过相变理论解释优化过程，并开发能够自适应平衡探索与利用的优化算法。", "method": "提出Wasserstein Evolution (WE)框架，实现自由能泛函的Wasserstein梯度流，将势梯度力（利用）与熵力（探索）的物理竞争转化为算法动力学。", "result": "在挑战性基准函数上的实验表明，WE实现了有竞争力的收敛性能，同时保持了比经典方法（GA、DE、CMA-ES）显著更高的种群多样性，有效避免了早熟收敛。", "conclusion": "该工作不仅提供了一个有效的优化算法，还通过统计物理学为理解进化计算提供了新范式，验证了优化作为无序到有序相变过程的物理解释。"}}
{"id": "2512.05836", "pdf": "https://arxiv.org/pdf/2512.05836", "abs": "https://arxiv.org/abs/2512.05836", "authors": ["Clarissa W. Ong", "Hiba Arnaout", "Kate Sheehan", "Estella Fox", "Eugen Owtscharow", "Iryna Gurevych"], "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.", "AI": {"tldr": "开发一个端到端管道，利用大型语言模型从治疗会话转录本中自动生成个性化心理网络，以支持案例概念化和治疗规划。", "motivation": "心理治疗个性化需要基于个性化网络选择治疗模块，但传统方法需要密集的纵向数据，难以规模化。本研究旨在利用LLMs解决这一可扩展性问题，从治疗转录本中自动生成临床相关的网络。", "method": "1) 标注77个治疗转录本中的3364个心理过程及其维度；2) 应用上下文学习联合识别心理过程及其维度；3) 引入两步法将过程分组为临床有意义的聚类；4) 生成解释增强的聚类间关系。", "result": "方法在少量训练示例下达到高性能；专家评估显示，多步方法生成的网络在临床效用和可解释性上优于直接提示方法（90%专家偏好）；网络在临床相关性、新颖性和有用性方面获得72-75%的评分。", "conclusion": "研究证明了使用LLMs从治疗转录本创建临床相关网络的可行性，具有自下而上的案例概念化和潜在主题识别优势。未来研究应探讨这些网络相对于其他个性化方法（包括统计估计网络）是否能改善治疗结果。"}}
{"id": "2512.05830", "pdf": "https://arxiv.org/pdf/2512.05830", "abs": "https://arxiv.org/abs/2512.05830", "authors": ["Muhammet Cagri Yeke", "Samil Sirin", "Kivilcim Yuksel", "Abdurrahman Gumus"], "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 11 figures, 5 tables", "summary": "This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.", "AI": {"tldr": "该论文提出了一种基于图像转换和深度学习的Phase-OTDR事件检测方法，通过将一维光纤传感数据转换为灰度图像，再组合成多通道RGB表示，利用迁移学习模型实现六种事件的高精度分类。", "motivation": "传统Phase-OTDR数据分析方法在处理复杂光纤事件时存在局限性，需要更鲁棒和高效的分析技术。研究旨在通过图像化数据表示和深度学习技术，提高光纤事件检测的准确性和可靠性。", "method": "提出了一种新颖的数据转换方法：将一维Phase-OTDR数据通过Gramian Angular Difference Field、Gramian Angular Summation Field和Recurrence Plot三种技术转换为灰度图像，然后将这些图像组合成多通道RGB表示。使用迁移学习模型（EfficientNetB0和DenseNet121）进行分类，并采用5折交叉验证评估模型性能。", "result": "该方法在公开的Phase-OTDR数据集上取得了优异的分类性能：EfficientNetB0模型达到98.84%的准确率，DenseNet121模型达到98.24%的准确率。5折交叉验证的测试准确率分别为99.07%和98.68%。研究还减少了数据集大小并提高了分析效率。", "conclusion": "图像化数据转换与深度学习相结合的方法为光纤传感数据分析提供了变革性的潜力，显著提高了光纤监测系统的准确性和可靠性。该方法代码和图像数据集已在GitHub上公开，支持进一步研究。"}}
{"id": "2512.05825", "pdf": "https://arxiv.org/pdf/2512.05825", "abs": "https://arxiv.org/abs/2512.05825", "authors": ["Shuhei Watanabe"], "title": "Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\\lfloor \\frac{M + 1}{2} \\rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.", "AI": {"tldr": "提出了一种近似盒分解算法，用于加速基于超体积的多目标优化中的获取函数计算", "motivation": "基于超体积的贝叶斯优化是多目标决策的标准方法，但获取函数优化的计算成本高昂，主要源于超体积改进计算的昂贵性。虽然盒分解提供了处理频繁精确改进计算的有效方式，但在最坏情况下具有超多项式内存复杂度。现有近似算法缺乏严格的算法描述。", "method": "提供了Couckuyt等人（2012）近似算法的全面数学和算法细节，填补了文献中的空白。该算法通过近似盒分解来降低计算复杂度，避免超多项式内存需求。", "result": "提出了一个完整的近似盒分解算法框架，提供了严格的数学描述和算法实现细节，使得基于超体积的多目标优化在实际应用中更加可行。", "conclusion": "通过提供近似盒分解算法的全面描述，解决了基于超体积的多目标贝叶斯优化中的计算瓶颈问题，使该方法在实际应用中更加实用。"}}
{"id": "2512.05824", "pdf": "https://arxiv.org/pdf/2512.05824", "abs": "https://arxiv.org/abs/2512.05824", "authors": ["Hafsa Akebli", "Adam Shephard", "Vincenzo Della Mea", "Nasir Rajpoot"], "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma", "categories": ["cs.AI", "cs.CV"], "comment": "4 pages, 2 figures", "summary": "Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.", "AI": {"tldr": "开发一个多模态肿瘤学智能体（MOA），用于预测低级别胶质瘤中的IDH1基因突变，整合组织病理学特征、临床数据和基因组信息。", "motivation": "低级别胶质瘤中IDH1突变定义了具有特定预后和治疗意义的临床亚群，准确预测IDH1突变状态对临床决策至关重要。现有方法可能无法充分利用多模态信息的互补性。", "method": "开发多模态肿瘤学智能体（MOA），整合基于TITAN基础模型的组织病理学工具，并通过PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理。在TCGA-LGG队列的488名患者上进行评估。", "result": "MOA（无组织病理学工具）优于临床基线，F1分数为0.826 vs 0.798。与组织病理学特征融合后，MOA达到最高性能，F1分数为0.912，超过组织病理学基线（0.894）和融合的组织病理学-临床基线（0.897）。", "conclusion": "提出的智能体通过外部生物医学来源丰富了互补的突变相关信息，能够准确预测IDH1突变，展示了多模态整合在肿瘤学预测中的优势。"}}
{"id": "2512.05815", "pdf": "https://arxiv.org/pdf/2512.05815", "abs": "https://arxiv.org/abs/2512.05815", "authors": ["Marios-Nektarios Stamatopoulos", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints", "categories": ["cs.RO"], "comment": null, "summary": "This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV.", "AI": {"tldr": "提出一个用于多无人机空中3D打印的安全感知调度框架，通过优化任务分配和调度实现无冲突协作，同时考虑任务依赖、安全约束和资源限制", "motivation": "解决多无人机同时进行空中3D打印时的协调问题，包括任务依赖、碰撞避免、资源限制（电池和材料），以及如何动态确定最优无人机数量以平衡任务完成时间和资源使用效率", "method": "提出一个优化问题框架，将施工任务分解为子任务，考虑几何结构依赖、无人机间安全约束、材料使用和飞行时间；通过动态选择任务开始时间和位置保证并行执行无碰撞；引入重要性优先级加速计算；采用效用最大化动态确定最优无人机数量", "result": "通过Gazebo仿真验证了框架有效性，无人机在任务控制模块协调下基于最优调度计划执行打印任务，同时满足材料和电池约束，实现了无冲突并行操作", "conclusion": "该框架成功解决了多无人机空中3D打印的协调问题，能够生成考虑任务依赖和安全约束的最优任务计划，并通过效用最大化平衡了任务完成时间与无人机部署数量，为大规模空中建造提供了可行方案"}}
{"id": "2512.05814", "pdf": "https://arxiv.org/pdf/2512.05814", "abs": "https://arxiv.org/abs/2512.05814", "authors": ["Fubao Zhu", "Zhanyuan Jia", "Zhiguo Wang", "Huan Huang", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chen Zhao", "Weihua Zhou"], "title": "UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection", "categories": ["cs.CV"], "comment": "The code is already available on GitHub: https://github.com/chenzhao2023/UG_FADDA_AlzhemiersClassification", "summary": "Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.", "AI": {"tldr": "提出了一种不确定性引导的联邦域适应框架（UG-FedDA），用于多中心阿尔茨海默病检测，解决跨站点异质性和隐私保护问题。", "motivation": "现有阿尔茨海默病分类框架在多中心研究中面临挑战：忽视站点间异质性、缺乏不确定性量化机制，限制了其鲁棒性和临床适用性。", "method": "结合不确定性量化与联邦域适应，使用自注意力变换器提取多模板ROI特征，通过不确定性引导的特征对齐来减轻源-目标分布偏移。", "result": "在ADNI、AIBL和OASIS三个公共数据集上，UG-FedDA在AD vs. NC、MCI vs. AD、NC vs. MCI三个分类任务中均取得一致改进，最高准确率达90.54%。", "conclusion": "该框架不仅能在多站点间高效适应，还能保护严格隐私，为多中心阿尔茨海默病检测提供了有效的解决方案。"}}
{"id": "2512.05812", "pdf": "https://arxiv.org/pdf/2512.05812", "abs": "https://arxiv.org/abs/2512.05812", "authors": ["Fabian Konstantinidis", "Moritz Sackmann", "Ulrich Hofmann", "Christoph Stiller"], "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.", "AI": {"tldr": "提出一种高效且鲁棒的多智能体驾驶仿真行为模型，通过实例中心化场景表示和查询中心化对称上下文编码器来优化交通参与者的行为控制", "motivation": "可扩展的多智能体驾驶仿真需要既真实又计算高效的行为模型，现有方法在计算效率和鲁棒性方面存在不足", "method": "采用实例中心化场景表示（每个交通参与者和地图元素在自身局部坐标系中建模），使用查询中心化对称上下文编码器处理交互，通过对抗逆强化学习训练行为模型，并提出自适应奖励变换平衡鲁棒性和真实性", "result": "方法在token数量上高效扩展，显著减少训练和推理时间，在位置准确性和鲁棒性方面优于多个智能体中心化基线方法", "conclusion": "提出的实例中心化表示和自适应奖励变换方法能够实现高效且鲁棒的多智能体驾驶仿真行为建模，为可扩展仿真提供了有效解决方案"}}
