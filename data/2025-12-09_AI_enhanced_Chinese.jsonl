{"id": "2512.07834", "pdf": "https://arxiv.org/pdf/2512.07834", "abs": "https://arxiv.org/abs/2512.07834", "authors": ["Yi-Chuan Huang", "Jiewen Chan", "Hao-Jen Chien", "Yu-Lun Liu"], "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "categories": ["cs.CV"], "comment": "Project page: https://yichuanh.github.io/Voxify-3D/", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "AI": {"tldr": "Voxify3D 是一种自动从 3D 网格生成像素艺术的方法，通过结合正交视角的监督、基于 CLIP 的语义对齐和调色板约束的 Gumbel-Softmax 定量来优化离散颜色空间。", "motivation": "传统的生成方法无法在几何简化的同时保持语义信息，并且难以达到像素级精确的调色板限制美学效果。Voxify3D 的动机在于解决这些挑战，实现从 3D 网格到像素艺术自动化的高质量转换。", "method": "该框架采用两阶段过程：首先通过正交视角监督消除透视失真以实现精确定量；然后使用基于 CLIP 的语义对齐确保在不同分辨率下保持一致的语义信息。最后，利用调色板约束的 Gumbel-Softmax 定量优化离散颜色空间。", "result": "实验显示 Voxify3D 在 CLIP-IQA 和用户偏好上表现优秀（CLIP-IQA 得分为 37.12，用户偏好评率为 77.90%），在不同抽象层次和分辨率下均取得良好效果。", "conclusion": "Voxify3D 成功解决了从 3D 网格生成高质量像素艺术的挑战，并提供了一个端到端的离散优化框架。"}}
{"id": "2512.07833", "pdf": "https://arxiv.org/pdf/2512.07833", "abs": "https://arxiv.org/abs/2512.07833", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "title": "Relational Visual Similarity", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page, data, and code: https://thaoshibe.github.io/relsim", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "AI": {"tldr": "研究通过视觉语言模型测量图像间的关系相似性，以弥补现有图像相似度模型无法捕捉关系相似性的不足。", "motivation": "现有的视觉相似度指标仅基于感知属性的相似性，未能充分捕捉到人类能够识别的关系相似性。这种能力被认为是区分人类与其他物种的关键特征之一。", "method": "研究首先定义了图像间关系相似性的可测量标准，并构建了一个包含114k幅图像和匿名描述其内在逻辑而非表面内容的标注数据集；使用该数据集对视觉语言模型进行微调，以评估图像间的关系相似性。", "result": "研究表明现有图像相似度模型在捕捉关系相似性方面存在重大不足。研究提出的方法为根据内部结构而不是外观连接图像提供了新途径，并展示了大量现实世界的应用潜力。", "conclusion": "通过将视觉语言模型应用于测量图像之间的关系相似性，可以有效地弥补现有方法的局限性，从而更全面地理解图像内容和逻辑关联"}}
{"id": "2512.07831", "pdf": "https://arxiv.org/pdf/2512.07831", "abs": "https://arxiv.org/abs/2512.07831", "authors": ["Jiehui Huang", "Yuechen Zhang", "Xu He", "Yuan Gao", "Zhi Cen", "Bin Xia", "Yan Zhou", "Xin Tao", "Pengfei Wan", "Jiaya Jia"], "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "categories": ["cs.CV"], "comment": "Project Website https://jackailab.github.io/Projects/UnityVideo", "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "AI": {"tldr": "本文提出UnityVideo框架，通过多模态和多任务联合学习来增强世界感知的视频生成。", "motivation": "现有视频生成模型受限于单一模态条件，缺乏全面的世界理解。为解决这一问题，引入了跨模态交互与多样化的数据表示方法，以提升整体世界的知识表达。", "method": "UnityVideo框架包含两个核心组件：动态噪声统一不同的训练范式和一种通过模块化参数及上下文学习的模式转换器。同时提供了大规模多模态数据集用于实验。", "result": "该方法显著提升了视频质量和一致性，增强了对物理世界约束的理解，并提高了零样本泛化的性能。", "conclusion": "UnityVideo框架在视频生成方面取得了优越的表现，通过联合优化加速了收敛速度并增强了对于未见数据的适应能力。"}}
{"id": "2512.07829", "pdf": "https://arxiv.org/pdf/2512.07829", "abs": "https://arxiv.org/abs/2512.07829", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "AI": {"tldr": "提出了一种简单有效的框架（FAE），将预训练视觉表示适应到适合生成的低维潜在空间。", "motivation": "由于理解导向特征和生成友好的潜在空间之间存在根本性差异，导致在适配预训练视觉表示时遇到挑战。因此需要一种简单有效的方法来解决这个问题。", "method": "提出了一种基于单层注意力机制的框架（FAE），该框架通过两个独立的深度解码器实现：一个用于原始特征空间的重建，另一个则将重构的特征作为输入进行图像生成。", "result": "在各类条件分类和文本到图像基准测试中，FAE展示了强大的性能，在ImageNet 256x256上达到了接近最先进的FID分数。", "conclusion": "通过简单有效的框架（FAE），解决了适配预训练视觉表示为适合生成低维潜在空间的问题，并在多种生成模型中展示出优良的性能。"}}
{"id": "2512.07826", "pdf": "https://arxiv.org/pdf/2512.07826", "abs": "https://arxiv.org/abs/2512.07826", "authors": ["Haoyang He", "Jie Wang", "Jiangning Zhang", "Zhucun Xue", "Xingyuan Bu", "Qiangpeng Yang", "Shilei Wen", "Lei Xie"], "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "categories": ["cs.CV"], "comment": "38 pages", "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "AI": {"tldr": "介绍了一个用于指令引导视频编辑的大规模高质量数据集OpenVE-3M及其模型训练成果。", "motivation": "现有指令引导视频编辑的数据集中质量高且多样性的稀缺，需要一个大规模、高质量的开放数据集来推动该领域的发展。", "method": "构建了包含多种编辑类型的大型数据集，并设计了一个统一基准，同时训练了一个高效的5B参数模型。", "result": "所提出的OpenVE-3M数据集在规模和多样性上超越现有公开数据集；训练出的OpenVE-Edit模型在新构造的评估基准中表现优异，优于所有之前的开放源代码模型。", "conclusion": "通过提供高质量的数据集及高效的编辑模型，为视频编辑任务提供了新的可能性。"}}
{"id": "2512.07821", "pdf": "https://arxiv.org/pdf/2512.07821", "abs": "https://arxiv.org/abs/2512.07821", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "AI": {"tldr": "世界镜头（WorldReel）是一种4D视频生成器，能够产生在空间和时间上都一致的视频。", "motivation": "现有的视频生成方法虽然能达到高度的真实感，但在三维一致性方面仍有不足。作者提出了WorldReel来解决这个问题，目标是生成具有持续几何结构和运动模型的逼真且时空一致性的4D视频。", "method": "通过结合合成数据与真实数据训练世界镜头，合成数据提供精确的空间时间监督（如几何形状、运动和相机位置），而真实视频则为生成过程带来多样性和现实感。这种方法使WorldReel能够在各种情况下都保持良好的几何精度并具备较强的泛化能力。", "result": "实验结果显示，WorldReel在动态场景与移动摄像机的时空一致性视频生成方面达到了最新的技术水平，在几何一致性和运动连贯性方面有显著提升，并减少了视点和时间上的伪影。", "conclusion": "通过引入4D表示法，世界镜头实现了前所未有的视频生成质量：不仅逼真且具有高度的一致性，还能够支持渲染、交互以及对场景进行推理的单一稳定的时空模型。"}}
{"id": "2512.07820", "pdf": "https://arxiv.org/pdf/2512.07820", "abs": "https://arxiv.org/abs/2512.07820", "authors": ["Prithila Angkan", "Amin Jalali", "Paul Hungler", "Ali Etemad"], "title": "Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.", "AI": {"tldr": "本文提出了一种基于图卷积网络的EEG表示学习方法，结合频域和时间-频率谱图信息，以提高脑机接口中的分类性能。", "motivation": "通过解决EEG信号的时间动态性和个体差异性导致的高类内相似度问题，该研究旨在提升脑机接口系统的准确率。", "method": "利用图卷积网络融合频域和时频谱表示，并引入中心损失和成对差分损失以提高分类性能。此外，采用梯度对齐策略解决不同领域间的冲突，使模型优化方向一致。", "result": "实验在BCI-2a、CL-Drive及CLARE三个公开数据集上验证了方法的有效性。消融研究表明各种组件对该模型的影响。", "conclusion": "GEEGA通过融合多领域的EEG表示信息并引入有效的梯度对齐策略，显著提高了脑机接口的分类性能和鲁棒性。"}}
{"id": "2512.07819", "pdf": "https://arxiv.org/pdf/2512.07819", "abs": "https://arxiv.org/abs/2512.07819", "authors": ["Shubham S. Kumbhar", "Abhijeet M. Kulkarni", "Panagiotis Artemiadis"], "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "categories": ["cs.RO"], "comment": null, "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "AI": {"tldr": "该论文提出了一种控制框架，使类人机器人能够与人类伙伴协同执行运输任务。", "motivation": "为了提高人类和类人机器人在协作运输中的效率和安全性，作者开发了一套支持平移和旋转运动的控制框架。", "method": "该方法包括一个高层规划器、低层控制器以及刚度调节机制。使用I-LIP模型结合自适应模型和MPC方法生成动态可行的步伐计划，并通过QP基全身体控制器执行。刚度调节确保机器人与物体之间的互动符合期望的姿态。", "result": "实验表明，该框架在Digit类人平台上有效运行，能够实现平移、转弯等协同行为，并提出了效率指标来量化合作质量。", "conclusion": "研究结果展示了基于此控制框架的协作任务具有高效的特性，并提供了关于理想轨迹特性的见解。"}}
{"id": "2512.07818", "pdf": "https://arxiv.org/pdf/2512.07818", "abs": "https://arxiv.org/abs/2512.07818", "authors": ["Xinyuan Cao", "Santosh S. Vempala"], "title": "Provable Long-Range Benefits of Next-Token Prediction", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "66 pages, 5 figures", "summary": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.", "AI": {"tldr": "证明了现代语言模型在下个标记预测上的训练可以捕捉到长程结构。", "motivation": "解释为什么通过下一词预测训练的现代语言模型能够生成连贯文档并捕获长范围结构。", "method": "通过对递归神经网络（RNN）优化下个标记预测，推导出一个逼近训练分布的模型，并提供多项式界以实现k-令牌不可区分性。", "result": "证明了对于从训练分布采样的文档，任何算法都无法通过检查下一个$k$个标记来区分实际文档和由语言模型生成的$k$个标记。", "conclusion": "提出了复杂性理论解释现代语言模型在长程连贯性上的表现。"}}
{"id": "2512.07814", "pdf": "https://arxiv.org/pdf/2512.07814", "abs": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "21 pages, 8 figures", "summary": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "AI": {"tldr": "通过训练动态研究不同类型的个人身份信息(PII)在代码大型语言模型(LLM4Code)中的学习和泄露风险，提供因果证据。", "motivation": "LLM4Code的隐私问题导致开发人员生产效率提高的同时也带来了隐患，现有研究忽略了不同类型PII的不同风险。本文旨在探究不同类型的PII在模型中被学习和泄露的可能性及其因果关系。", "method": "构建包含多种类型PII的数据集，微调规模不同的代表模型，计算真实数据上的训练动态，并建立结构因果模型以估计可学性对泄漏的影响。", "result": "结果显示不同类型PII的泄漏风险差异显著且与训练动态相关：易学习的信息如IP地址泄露概率高，而难学习的如密钥和密码则较少被泄露。模糊类型表现出复杂行为。", "conclusion": "这项研究首次提供了不同类型PII在代码大型语言模型中具有不同泄漏风险的因果证据，并为开发针对这些类型的保护措施提供指导。"}}
{"id": "2512.07813", "pdf": "https://arxiv.org/pdf/2512.07813", "abs": "https://arxiv.org/abs/2512.07813", "authors": ["Hari Prakash Thanabalan", "Lars Bengtsson", "Ugo Lafont", "Giovanni Volpe"], "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.", "AI": {"tldr": "介绍了一种受尺蠖启发的软机器人，其运动方向通过凹槽图案被动控制。", "motivation": "软机器人需要定向控制以导航复杂地形，然而实现这种控制通常需要多个致动器，这增加了机械复杂性、复杂化控制系统并增加能耗。", "method": "采用单一卷绕介电弹性体致动器和3D打印的带有沟槽图案的基底来引导其对齐和轨迹。通过系统实验表明，改变凹槽角度可以精确控制运动方向。", "result": "这种方法减少了能量消耗，简化了机器人设计，并扩大了生物启发式软机器人的应用范围，如搜索和救援、管道检查以及行星探索。", "conclusion": "提出了一种利用凹槽导向的简单有效的软机器人定向方法，提高了其在复杂环境中的适用性。"}}
{"id": "2512.07810", "pdf": "https://arxiv.org/pdf/2512.07810", "abs": "https://arxiv.org/abs/2512.07810", "authors": ["Jordan Taylor", "Sid Black", "Dillon Bowen", "Thomas Read", "Satvik Golechha", "Alex Zelenka-Martin", "Oliver Makins", "Connor Kissane", "Kola Ayonrinde", "Jacob Merizian", "Samuel Marks", "Chris Cundy", "Joseph Bloom"], "title": "Auditing Games for Sandbagging", "categories": ["cs.AI"], "comment": "77 pages (28 non-appendix pages), 38 figures", "summary": "Future AI systems could conceal their capabilities ('sandbagging') during evaluations, potentially misleading developers and auditors. We stress-tested sandbagging detection techniques using an auditing game. First, a red team fine-tuned five models, some of which conditionally underperformed, as a proxy for sandbagging. Second, a blue team used black-box, model-internals, or training-based approaches to identify sandbagging models. We found that the blue team could not reliably discriminate sandbaggers from benign models. Black-box approaches were defeated by effective imitation of a weaker model. Linear probes, a model-internals approach, showed more promise but their naive application was vulnerable to behaviours instilled by the red team. We also explored capability elicitation as a strategy for detecting sandbagging. Although Prompt-based elicitation was not reliable, training-based elicitation consistently elicited full performance from the sandbagging models, using only a single correct demonstration of the evaluation task. However the performance of benign models was sometimes also raised, so relying on elicitation as a detection strategy was prone to false-positives. In the short-term, we recommend developers remove potential sandbagging using on-distribution training for elicitation. In the longer-term, further research is needed to ensure the efficacy of training-based elicitation, and develop robust methods for sandbagging detection. We open source our model organisms at https://github.com/AI-Safety-Institute/sandbagging_auditing_games and select transcripts and results at https://huggingface.co/datasets/sandbagging-games/evaluation_logs . A demo illustrating the game can be played at https://sandbagging-demo.far.ai/ .", "AI": {"tldr": "本论文通过设计审计游戏来测试和评估检测AI系统潜在的隐藏能力（即'沙袋'行为）的方法。", "motivation": "未来的AI系统可能在评估过程中故意隐瞒其真实能力，这可能会误导开发者和审查者。为了提高对这种行为的识别能力，研究团队进行了一系列实验以评估不同方法的有效性。", "method": "通过一个红队训练五种模型，其中一些会在某些条件下表现不佳，作为沙袋行为的代理；然后由蓝队使用黑盒、内部结构或基于训练的方法来检测这些沙袋模型。此外，还探索了能力揭示策略，并比较了提示式和基于培训的方式对检测沙袋行为的有效性。", "result": "实验结果表明，蓝队无法可靠地区分真正的沙袋者与普通模型。虽然线性探针等内部结构方法表现较好，但容易被红方的特定行为所误导；能力揭示策略方面，提示式的方法不准确，而基于培训的方式可以激活沙袋模型的真实能力，但也可能提升正常模型的表现。", "conclusion": "短期内建议开发者通过在分配数据上进行训练来消除潜在的沙袋行为。长期来看，需要进一步研究以提高基于培训的能力揭示方法的有效性，并开发更稳健的检测技术来识别AI系统的隐藏能力。"}}
{"id": "2512.07807", "pdf": "https://arxiv.org/pdf/2512.07807", "abs": "https://arxiv.org/abs/2512.07807", "authors": ["Shai Krakovsky", "Gal Fiebelman", "Sagie Benaim", "Hadar Averbuch-Elor"], "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to SIGGRAPH Asia 2025. Project webpage: https://tau-vailab.github.io/Lang3D-XL", "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "AI": {"tldr": "本文提出了一种名为Lang3D-XL的方法，通过引入低维语义瓶颈特征和多分辨率哈希编码器来提高大规模场景中语言嵌入的三维高斯表示的有效性和效率。", "motivation": "将语言字段嵌入到3D表示中可以增强对空间环境的理解，并且现有方法在处理大规模数据时面临语义特征不匹配及内存与运行时间上的挑战，因此需要新的解决方案来提高性能和效率。", "method": "作者提出了一种低维语义瓶颈特征作为基础3D高斯表示的一部分，通过渲染传递给多分辨率哈希编码器，并引入衰减下采样模块以解决地面真相2D特征的语义不匹配问题。", "result": "在HolyScenes数据集上的实验表明，Lang3D-XL方法超越了现有方案，在性能和效率上都表现更好。", "conclusion": "通过有效利用低维语义瓶颈特征结合多分辨率哈希编码器及衰减下采样模块，Lang3D-XL能够更高效地处理大规模场景中的语言嵌入三维高斯表示。"}}
{"id": "2512.07806", "pdf": "https://arxiv.org/pdf/2512.07806", "abs": "https://arxiv.org/abs/2512.07806", "authors": ["Gyeongjin Kang", "Seungkwon Yang", "Seungtae Nam", "Younggeun Lee", "Jungwoo Kim", "Eunbyung Park"], "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "categories": ["cs.CV"], "comment": "Project page: see https://gynjn.github.io/MVP/", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "AI": {"tldr": "提出了一种名为多视角金字塔变压器（MVP）的架构，用于从大量图像中快速重建大型三维场景。", "motivation": "为了实现大规模和复杂场景的高效、高质量重建，作者提出了MVP架构来解决当前方法在效率和表现力之间的权衡问题。", "method": "通过设计本地到全局的视图层次结构和精细到粗糙的空间层次结构，MVP能够在单次前向传播中快速准确地从图像集合中重建三维场景。", "result": "实验验证了MVP与3D高斯点云表示结合时，在各种数据集上实现了最先进的可推广重建质量，并且在广泛的视图配置下保持高效的计算效率和扩展性。", "conclusion": "通过结合多视角信息和层次化设计，MVP能够在大规模场景重建任务中提供高效的解决方案。"}}
{"id": "2512.07805", "pdf": "https://arxiv.org/pdf/2512.07805", "abs": "https://arxiv.org/abs/2512.07805", "authors": ["Yifan Zhang", "Zixiang Chen", "Yifeng Liu", "Zhen Qin", "Huizhuo Yuan", "Kangping Xu", "Yang Yuan", "Quanquan Gu", "Andrew Chi-Chih Yao"], "title": "Group Representational Position Encoding", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Page: https://github.com/model-architectures/GRAPE", "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.", "AI": {"tldr": "提出GRAPE框架，统一基于群作用的位置编码方法。", "motivation": "为长上下文模型提供原理性设计空间，整合RoPE和ALiBi作为特殊情况。", "method": "通过SO(d)中的乘法旋转（Multiplicative GRAPE）和GL中的加法logit偏置（Additive GRAPE），基于群作用创建位置编码框架。", "result": "GRAPE为位置几何提供了原理性的设计空间，同时扩展了RoPE和ALiBi的结构。", "conclusion": "GRAPE统一并扩展了现有的位置编码方法，提供了一种新的设计理念。"}}
{"id": "2512.07802", "pdf": "https://arxiv.org/pdf/2512.07802", "abs": "https://arxiv.org/abs/2512.07802", "authors": ["Zhaochong An", "Menglin Jia", "Haonan Qiu", "Zijian Zhou", "Xiaoke Huang", "Zhiheng Liu", "Weiming Ren", "Kumara Kahatapitiya", "Ding Liu", "Sen He", "Chenyang Zhang", "Tao Xiang", "Fanny Yang", "Serge Belongie", "Tian Xie"], "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "categories": ["cs.CV"], "comment": "Project Page: https://zhaochongan.github.io/projects/OneStory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "AI": {"tldr": "本文提出了一种名为OneStory的方法，用于生成连贯的多镜头视频。", "motivation": "现有的多镜头视频生成方法在处理复杂的叙事时表现不佳，因为它们依赖有限的时间窗口或单一的关键帧条件，无法有效建模跨镜头的长期上下文。", "method": "OneStory通过将MSV重新定义为下一个镜头生成任务，并利用预训练的图像到视频模型进行强大的视觉条件化。它引入了两个关键模块：帧选择模块和自适应条件器，用于构建基于先前镜头中信息性帧的相关全局内存并生成紧凑上下文。", "result": "在文本和图像条件下，OneStory实现了叙事连贯性的最先进水平，在各种复杂场景下都能产生控制性和沉浸式的长视频故事。", "conclusion": "OneStory通过其创新的方法和技术提高了多镜头视频的生成质量，并为未来的研究提供了有价值的见解。"}}
{"id": "2512.07801", "pdf": "https://arxiv.org/pdf/2512.07801", "abs": "https://arxiv.org/abs/2512.07801", "authors": ["Raunak Jain", "Mudita Khurana"], "title": "Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.", "AI": {"tldr": "提出了一种协作因果解释框架，旨在通过更好的人机合作来提高决策支持的效率。", "motivation": "当前的人工智能在高风险环境中未能有效提升团队决策能力，人类与AI之间的互补优势并未充分实现。", "method": "研究了专家共同构建认知模型、目标设定及假设验证的过程，并提出了协作因果解释框架作为未来的研究方向和组织架构。", "result": "提出了协作因果解释系统的设计理念，该系统旨在通过持续的学习来提升人机合作的效率与准确性。", "conclusion": "协作因果解释系统的引入有望解决现有决策支持中的不足，推动智能代理参与认知工作，并成为人类伙伴的思想助手。"}}
{"id": "2512.07796", "pdf": "https://arxiv.org/pdf/2512.07796", "abs": "https://arxiv.org/abs/2512.07796", "authors": ["Sridhar Mahadevan"], "title": "Large Causal Models from Large Language Models", "categories": ["cs.AI"], "comment": "29 pages", "summary": "We introduce a new paradigm for building large causal models (LCMs) that exploits the enormous potential latent in today's large language models (LLMs). We describe our ongoing experiments with an implemented system called DEMOCRITUS (Decentralized Extraction of Manifold Ontologies of Causal Relations Integrating Topos Universal Slices) aimed at building, organizing, and visualizing LCMs that span disparate domains extracted from carefully targeted textual queries to LLMs. DEMOCRITUS is methodologically distinct from traditional narrow domain and hypothesis centered causal inference that builds causal models from experiments that produce numerical data. A high-quality LLM is used to propose topics, generate causal questions, and extract plausible causal statements from a diverse range of domains. The technical challenge is then to take these isolated, fragmented, potentially ambiguous and possibly conflicting causal claims, and weave them into a coherent whole, converting them into relational causal triples and embedding them into a LCM. Addressing this technical challenge required inventing new categorical machine learning methods, which we can only briefly summarize in this paper, as it is focused more on the systems side of building DEMOCRITUS. We describe the implementation pipeline for DEMOCRITUS comprising of six modules, examine its computational cost profile to determine where the current bottlenecks in scaling the system to larger models. We describe the results of using DEMOCRITUS over a wide range of domains, spanning archaeology, biology, climate change, economics, medicine and technology. We discuss the limitations of the current DEMOCRITUS system, and outline directions for extending its capabilities.", "AI": {"tldr": "构建大规模因果模型（LCM）的新范式，利用大型语言模型（LLMs）的能力。", "motivation": "探索将大型语言模型应用于提取和组织跨领域的大规模因果关系的潜力。", "method": "开发系统DEMOCRITUS，使用高质素的语言模型提出主题、生成因果问题并从多种领域中抽取可能的因果陈述，并通过新的分类机器学习方法将其整合成大规模因果模型。", "result": "在考古学、生物学、气候变化等广泛领域展示了DEMOCRITUS系统的应用效果，并探讨了其局限性和未来的改进方向。", "conclusion": "提出了一种新的构建跨领域的大型因果模型的方法，使用语言模型进行主题和问题生成及因果陈述抽取，并通过新方法整合这些信息。"}}
{"id": "2512.07795", "pdf": "https://arxiv.org/pdf/2512.07795", "abs": "https://arxiv.org/abs/2512.07795", "authors": ["Nearchos Potamitis", "Lars Klein", "Akhil Arora"], "title": "ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "11 pages, 3 tables, 4 figures", "summary": "Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .", "AI": {"tldr": "ReasonBENCH是一款评估大型语言模型推理稳定性的基准工具。", "motivation": "当前的评价实践主要关注单次运行准确性，忽略了随机解码带来的内在不确定性。这种忽视导致了可靠性盲区。", "method": "引入了一种模块化评估库、多轮协议以及公开排行榜来量化LLM推理中的不稳定因素。", "result": "发现大多数推理策略和模型在不同任务中表现出了高不稳定性，即使平均性能相似的策略也存在四倍以上的置信区间差异。", "conclusion": "强调了重复性和可靠性对于可靠的大规模语言模型推理至关重要，并为未来的不确定性量化技术提供了基础。"}}
{"id": "2512.07778", "pdf": "https://arxiv.org/pdf/2512.07778", "abs": "https://arxiv.org/abs/2512.07778", "authors": ["Sen Ye", "Jianning Pei", "Mengde Xu", "Shuyang Gu", "Chunyu Wang", "Liwei Wang", "Han Hu"], "title": "Distribution Matching Variational AutoEncoder", "categories": ["cs.CV"], "comment": null, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "AI": {"tldr": "提出了一种新的变分自编码器DMVAE，该模型通过分布匹配约束将隐含空间与任意参考分布对齐。", "motivation": "现有方法如VAE和基础模型对齐的编码器在压缩图像时会隐式地限制隐含空间而没有明确塑造其分布。研究团队希望探究更优的隐含空间分布，以提高图像生成的效果。", "method": "DMVAE通过引入分布匹配约束将编码器的隐含分布与参考分布对齐。这种方法超越了传统VAE中的高斯先验，可以与自监督特征、扩散噪声或其他先验分布相匹配。", "result": "实验表明，使用基于SSL生成的分布可以在ImageNet上达到3.2的gFID值，同时仅需64个训练周期。", "conclusion": "选择合适的隐含空间结构（通过分布级别的对齐实现）是提高图像合成质量的关键。"}}
{"id": "2512.07776", "pdf": "https://arxiv.org/pdf/2512.07776", "abs": "https://arxiv.org/abs/2512.07776", "authors": ["Maximilian Schall", "Felix Leonard Knöfel", "Noah Elias König", "Jan Jonas Kubeler", "Maximilian von Klinski", "Joan Wilhelm Linnemann", "Xiaoshi Liu", "Iven Jelle Schlegelmilch", "Ole Woyciniuk", "Alexandra Schild", "Dante Wasmuht", "Magdalena Bermejo Espinet", "German Illera Basas", "Gerard de Melo"], "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring", "categories": ["cs.CV"], "comment": "Accepted at WACV 2026", "summary": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species", "AI": {"tldr": "介绍了一个自动化的系统GorillaWatch，用于野外大猩猩的重新识别和种群监测。", "motivation": "当前监控濒危西部低地大猩猩受到大量手动努力从庞大的相机陷阱视频档案中重新识别个体的阻碍。主要障碍是缺乏大规模、‘野外’视频数据集以训练稳健的深度学习模型。", "method": "提出了一个综合基准，包括三个新的数据集：Gorilla-SPAC-Wild, Gorilla-Berlin-Zoo和Gorilla-SPAC-MoT；开发了GorillaWatch，这是一个集成检测、跟踪和重新识别的端到端管道；引入了一种多帧自我监督预训练策略；使用可微分适应的AttnLRP验证模型依赖于鉴别生物特征而不是背景相关性。", "result": "基准测试显示聚合大规模图像骨干网络中的功能优于专门的视频架构；通过整合时空约束解决无监督的人口计数问题。", "conclusion": "公开发布了所有代码和数据集，以促进对濒危物种的可扩展、非侵入式监测"}}
{"id": "2512.07775", "pdf": "https://arxiv.org/pdf/2512.07775", "abs": "https://arxiv.org/abs/2512.07775", "authors": ["David Thorne", "Nathan Chan", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "OptMap: Geometric Map Distillation via Submodular Maximization", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "AI": {"tldr": "论文提出了一种名为OptMap的几何地图精简算法，用于实时生成适用于不同应用的地图。", "motivation": "自主机器人需要在环境的不同尺度上进行推理和规划，每种算法可能都需要不同的地图以实现最佳性能。选择具有代表性的地图是一个计算难题，因为其涉及NP难组合优化问题。", "method": "论文提出了一种基于次模最大化的方法来生成应用特定的地图。这种方法利用了多项式时间算法，并设计了一个新的次模奖励函数以量化信息量并减少输入集的大小和偏置。", "result": "实验表明，OptMap能够在开源数据集上运行，且计算需求较少。此外，它还可以与其他LiDAR SLAM算法结合使用。", "conclusion": "论文成功开发了一种能够实时生成适用于不同应用的地图的方法，并通过理论与实际测试验证了其有效性和实用性。"}}
{"id": "2512.07765", "pdf": "https://arxiv.org/pdf/2512.07765", "abs": "https://arxiv.org/abs/2512.07765", "authors": ["Gustavo A. Cardona", "Shubham S. Kumbhar", "Panagiotis Artemiadis"], "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "categories": ["cs.RO"], "comment": "60 pages, 5 figures, 3 tables", "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "AI": {"tldr": "本文通过控制、意图估计和计算人类模型三个核心支柱，回顾了物理人形机器人互动的现状，并提出了未来研究方向。", "motivation": "为了在非结构化的人类环境中部署机器人，需要实现稳健、可扩展且适应性强的人机交互。当前的研究存在跨领域的整合不足的问题。", "method": "作者通过三个核心支柱（人形模型和控制、人类意图估计、计算人类模型）对物理人形互动的现状进行了回顾，并提出了未来的统一方法路径。", "result": "本文提出了一种基于模态和机器人参与程度分类的交互类型综合分类法，以促进跨领域的整合并为未来研究提供方向。", "conclusion": "通过分析当前的研究挑战，本文为实现稳健、安全且直观的人机互动提供了路线图。"}}
{"id": "2512.07761", "pdf": "https://arxiv.org/pdf/2512.07761", "abs": "https://arxiv.org/abs/2512.07761", "authors": ["Xiqiao Xiong", "Ouxiang Li", "Zhuo Liu", "Moxin Li", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models", "categories": ["cs.AI", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.", "AI": {"tldr": "研究通过多轮强化学习训练攻击者LLM以从黑盒模型中获取有害内容。", "motivation": "大型语言模型易受囚禁攻击，威胁其安全应用。现有方法依赖单次优化无法学习长期策略。为此，提出了一种新的方法来提升攻击成功率和长期性。", "method": "将问题定义为多轮强化学习任务，通过控制中间输出的有害性和语义相关性来防止触发拒绝机制，并促进长期攻击策略的学习。", "result": "在多个基准上实验显示了改进的攻击成功率，验证了方法的有效性。", "conclusion": "所提出的方法能够有效提高大型语言模型在多轮黑盒囚禁攻击中的脆弱性。"}}
{"id": "2512.07760", "pdf": "https://arxiv.org/pdf/2512.07760", "abs": "https://arxiv.org/abs/2512.07760", "authors": ["Menglin Wang", "Xiaojin Gong", "Jiachen Li", "Genlin Ji"], "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "AI": {"tldr": "本文提出了一种基于模态感知的偏置缓解和不变性学习的方法，用于跨可见光和红外图像的人体再识别。", "motivation": "在无监督的跨可见光-红外人体再识别任务中，由于模态间的巨大差距，准确估计跨模态关联成为主要挑战。现有方法通常依赖最优传输来关联模内集群，并容易传播局部集群误差，且忽略了全局实例关系。本文通过挖掘和关注可见光与红外模态偏置，旨在从两方面解决跨模态学习问题。", "method": "本文提出了一种基于模态感知的雅卡尔距离方法，以缓解由模态差异引起的距离偏置，并通过全局聚类估计更可靠的跨模态关联。此外，设计了一个‘分割与对比’策略来获取模态特异性的全局原型，并在全球关联指导下显式对齐这些原型，实现模态不变但具有身份区分能力的表示学习。", "result": "该方法在基准VI-ReID数据集上取得了最先进的性能，并显著优于现有方法，验证了其有效性。", "conclusion": "本文提出的方法能够有效缓解跨模态偏置并提升再识别准确性，在多个评估指标下均超越了现有的最新技术。"}}
{"id": "2512.07756", "pdf": "https://arxiv.org/pdf/2512.07756", "abs": "https://arxiv.org/abs/2512.07756", "authors": ["Mayank Anand", "Ujair Alam", "Surya Prakash", "Priya Shukla", "Gora Chand Nandi", "Domenec Puig"], "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.", "AI": {"tldr": "本文提出了UltrasODM框架，用于通过光学流和双马网络进行三维自由手超声重建。", "motivation": "临床超声检查高度依赖操作者，快速探头移动和亮度波动常导致重建错误，降低信任度及临床实用性。为解决此问题，提出UltrasODM系统以协助操作人员提高超声重建质量。", "method": "UltrasODM框架包含对比排名模块、光学流与双马网络融合的6-DoF姿态估计以及结合贝叶斯不确定性分析的人机交互层，能够提供逐帧不确定性评估和视觉提示。", "result": "在临床自由手超声数据集上测试后，UltrasODM将漂移误差降低了15.2%，距离误差减少了12.1%，Hausdorff距离减少了10.1%，提高了重建的可靠性和准确性。", "conclusion": "通过引入透明度和医生反馈机制，UltrasODM改进了超声图像重建流程的安全性与可靠性。"}}
{"id": "2512.07747", "pdf": "https://arxiv.org/pdf/2512.07747", "abs": "https://arxiv.org/abs/2512.07747", "authors": ["Shihao Zhao", "Yitong Chen", "Zeyinzi Jiang", "Bojia Zi", "Shaozhe Hao", "Yu Liu", "Chaojie Mao", "Kwan-Yee K. Wong"], "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "AI": {"tldr": "提出了一种自动、任务通用且低成本的框架Unison，用于统一理解与生成。", "motivation": "现有的两种方法在多模态学习中的统一理解和生成中存在不足：一种需要大量数据和计算资源，另一种虽然成本较低但覆盖范围有限或生成质量不佳。两者都无法解析输入元信息并需要手动配置参数。", "method": "采用两阶段方案结合预训练模型的能力，在极低的培训成本下涵盖多种多模态理解任务和生成任务，并自动解析用户意图、确定目标任务类型，以及提取相应任务所需的信息。", "result": "实验表明，在仅使用50万训练样本和50个GPU小时的情况下，该模型能够准确自动识别任务并提取相关参数，并在各种理解和生成任务中表现优异。", "conclusion": "Unison框架成功实现了统一多模态理解与生成任务的自动化，具有广泛的覆盖范围、优秀的性能和低培训成本。"}}
{"id": "2512.07745", "pdf": "https://arxiv.org/pdf/2512.07745", "abs": "https://arxiv.org/abs/2512.07745", "authors": ["Jialv Zou", "Shaoyu Chen", "Bencheng Liao", "Zhiyu Zheng", "Yuehao Song", "Lefei Zhang", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2", "AI": {"tldr": "本文提出了一种新的基于强化学习约束的扩散模型DiffusionDriveV2，用于端到端自动驾驶。", "motivation": "传统的生成式扩散模型在自驾车行为模拟中倾向于产生保守和单一的行为。尽管DiffusionDrive通过预定义锚点来促进多样化的轨迹规划，但其依赖于模仿学习的方法无法同时保证质量和多样性。", "method": "本文提出了一种新的方法，使用强化学习约束低质量模式，并探索高质量的轨迹。采用规模自适应乘法噪声以增强广域搜索，运用GRPO算法在锚点间进行优势估计和全局视角管理。", "result": "DiffusionDriveV2模型在NAVSIM v1数据集上达到91.2 PDMS，在NAVSIM v2数据集上的EPDMS为85.5，超过了之前的方法，实现了质量和多样性的最佳平衡。", "conclusion": "通过引入强化学习约束和创新的噪声机制，DiffusionDriveV2成功解决了传统扩散模型在自动驾驶领域的模式崩溃问题，并达到了质量与多样性之间的最优解。"}}
{"id": "2512.07741", "pdf": "https://arxiv.org/pdf/2512.07741", "abs": "https://arxiv.org/abs/2512.07741", "authors": ["Agnes Norbury", "George Fairs", "Alexandra L. Georgescu", "Matthew M. Nour", "Emilia Molimpakis", "Stefano Goria"], "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data", "categories": ["cs.LG", "cs.SD"], "comment": ":J.3; J.4", "summary": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.", "AI": {"tldr": "通过声纹和语音特征预测抑郁和焦虑的症状。", "motivation": "在精神疾病评估中，整合报告内容与非言语迹象（如语调、讲话速度等）是一项挑战。使用贝叶斯网络模型可以克服这一难题，并为临床应用提供支持工具。", "method": "基于大规模数据集开发了一种多模态的贝叶斯网络模型来预测抑郁和焦虑的症状，评估了该模型在不同输入模式之间的集成度与冗余性。", "result": "模型对抑郁和焦虑症状（ROC-AUC大于0.83）具有较高的准确性，并且具有较好的公平性和临床实用性。核心个体症状的性能也表现出色（ROC-AUC>0.74）。", "conclusion": "通过大规模多模态数据流，以症状而非疾病水平来表示常见精神障碍时，该方法是一种构建稳健评估支持工具的原则性途径，可提供透明、解释性强且适合临床专家监督的输出。"}}
{"id": "2512.07738", "pdf": "https://arxiv.org/pdf/2512.07738", "abs": "https://arxiv.org/abs/2512.07738", "authors": ["Dengjia Zhang", "Charles Weng", "Katherine Guerrerio", "Yi Lu", "Kenton Murray", "Alexander Martin", "Reno Kriz", "Benjamin Van Durme"], "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "categories": ["cs.CV"], "comment": "7 pages, 1 figure", "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "AI": {"tldr": "开发了一种列表学习框架，用于改进视频问答任务中答案生成的语义精度和排名一致性。", "motivation": "为了提高多模态模型在视频-问题对中的回答生成质量，特别是在语义精确度和排序稳定性方面，提出了新的方法。", "method": "首先使用基础多模态模型为每个问题生成多个候选答案，然后通过一种新的带有屏蔽指针交叉熵损失的排名权重进行重新排序。该框架整合了基于指针的选择、依赖于排名的加权以及词汇限制下的屏蔽交叉熵，以实现稳定和可解释的列表优化。", "result": "实验结果显示，在准确性及排名稳定性方面都有显著提高，尤其是在需要时间推理和语义消歧的问题上表现更好。", "conclusion": "通过结合生成式模型与判别式排序的方法，成功提高了视频问答中答案生成的质量。"}}
{"id": "2512.07733", "pdf": "https://arxiv.org/pdf/2512.07733", "abs": "https://arxiv.org/abs/2512.07733", "authors": ["Meng Cao", "Xingyu Li", "Xue Liu", "Ian Reid", "Xiaodan Liang"], "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.", "AI": {"tldr": "提出SpatialDreamer框架，通过主动探索、视觉想象和基于证据的推理来提高多模态语言模型在复杂空间推理任务中的性能。", "motivation": "现有方法依赖于被动观察空间数据，在复杂的需要心理模拟的空间推理任务上表现不佳。为此，作者提出了SpatialDreamer以解决这一问题。", "method": "提出了一种强化学习框架SpatialDreamer，并引入了基于几何一致性的策略优化GeoPO来应对长时水平推理任务中缺乏细粒度奖励监督的问题。", "result": "实验结果表明，SpatialDreamer在多个具有挑战性的基准测试上取得了高度竞争力的结果。", "conclusion": "SpatialDreamer为多模态语言模型提供了接近人类的主动空间心理模拟能力的重要进展。"}}
{"id": "2512.07730", "pdf": "https://arxiv.org/pdf/2512.07730", "abs": "https://arxiv.org/abs/2512.07730", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "categories": ["cs.CV", "cs.AI"], "comment": "WACV 2026", "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "AI": {"tldr": "SAVE框架通过稀疏自编码器引导的视觉信息增强，减少多模态大型语言模型中的对象幻觉。", "motivation": "多模态大型语言模型容易因语言先验和视觉信息丢失而产生对象幻觉。为此，提出SAVE方法来解决这一问题。", "method": "通过二元物体存在性问答探测器识别稀疏自编码器的特征，并沿这些特征引导模型，增强其视觉理解能力以减少幻觉。", "result": "在CHAIR_S上达到10％p性能提升，在POPE和MMHal-Bench上也取得显著改善。实验表明该方法具有鲁棒性和泛化性。", "conclusion": "SAVE通过沿视觉理解特征引导模型，减少了不确定物体标记的生成并增加了对图像标记的关注，从而有效减轻了幻觉问题。"}}
{"id": "2512.07729", "pdf": "https://arxiv.org/pdf/2512.07729", "abs": "https://arxiv.org/abs/2512.07729", "authors": ["Aidas Aglinskas", "Stefano Anzellotti"], "title": "Improving action classification with brain-inspired deep networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "AI": {"tldr": "本文探讨了脑启发的深度神经网络在动作分类中的应用，研究了这种网络架构与人类视觉系统的相似性以及其性能提升。", "motivation": "为了探索深神经网络是否能更有效地利用身体和背景信息来提高动作识别准确性，并且通过构建具有特定领域流的人工智能系统来模拟人类大脑处理方式的优越性", "method": "作者使用HAA500数据集训练深度神经网络，研究了其对包含人体和背景的动作刺激版本以及只含有其中之一的刺激版本的表现。同时进行人的行为识别实验，并设计了一种新的架构，该架构模仿人脑中的特定领域流来分别处理身体和场景信息。", "result": "结果表明，现有的DNN在使用含有身体和背景的动作刺激时表现良好；去除身体后表现几乎相同，但去除背景后则表现较差。人类参与者可以准确识别所有三种版本的刺激，并且对于仅包含身体的刺激表现出更好的准确性。新的脑启发架构改进了动作分类性能，并且其不同刺激版本的表现模式更接近于人类参与者的观察结果。", "conclusion": "通过分离处理人体和背景信息，该研究提出的新型脑启发深度网络架构在动作识别任务上表现更加优异，并能够模仿人类视觉系统的行为。"}}
{"id": "2512.07724", "pdf": "https://arxiv.org/pdf/2512.07724", "abs": "https://arxiv.org/abs/2512.07724", "authors": ["Zhengzheng Tang"], "title": "The Native Spiking Microarchitecture: From Iontronic Primitives to Bit-Exact FP8 Arithmetic", "categories": ["cs.ET", "cs.AI"], "comment": ":10 pagesACM Class:C.1.3; I.2.6", "summary": "The 2025 Nobel Prize in Chemistry for Metal-Organic Frameworks (MOFs) and recent breakthroughs by Huanting Wang's team at Monash University establish angstrom-scale channels as promising post-silicon substrates with native integrate-and-fire (IF) dynamics. However, utilizing these stochastic, analog materials for deterministic, bit-exact AI workloads (e.g., FP8) remains a paradox. Existing neuromorphic methods often settle for approximation, failing Transformer precision standards. To traverse the gap \"from stochastic ions to deterministic floats,\" we propose a Native Spiking Microarchitecture. Treating noisy neurons as logic primitives, we introduce a Spatial Combinational Pipeline and a Sticky-Extra Correction mechanism. Validation across all 16,129 FP8 pairs confirms 100% bit-exact alignment with PyTorch. Crucially, our architecture reduces Linear layer latency to O(log N), yielding a 17x speedup. Physical simulations further demonstrate robustness against extreme membrane leakage (beta approx 0.01), effectively immunizing the system against the stochastic nature of the hardware.", "AI": {"tldr": "本文提出了一种原生脉冲微架构，用于将随机、类比的材料应用于确定性的FP8计算。", "motivation": "利用纳米级通道作为后硅基板进行确定性计算存在挑战，现有的神经形态方法往往只能提供近似结果，无法满足Transformer的标准。本文旨在解决从随机离子到确定性浮点数的问题。", "method": "将嘈杂的神经元视为逻辑原语，并引入了空间组合流水线和粘连额外校正机制来实现精确计算。", "result": "验证结果显示16,129个FP8对中的每一个都与PyTorch完全一致。同时，该架构将线性层延迟降低到O(log N)，速度提升了17倍，并且在极端膜泄漏情况下表现出鲁棒性。", "conclusion": "本文提出的方法能够有效地利用随机、类比材料实现精确计算，为神经形态硬件的设计提供了新的思路和解决方案。"}}
{"id": "2512.07723", "pdf": "https://arxiv.org/pdf/2512.07723", "abs": "https://arxiv.org/abs/2512.07723", "authors": ["Yonggeon Lee", "Jibin Hwang", "Alfred Malengo Kondoro", "Juhyun Song", "Youngtae Noh"], "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages, 9 figures, AAAI'26 (accepted)", "summary": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.", "AI": {"tldr": "提出了一种基于Transformer的实时到事件(TTE)模型，用于准确预测电动汽车(EV)用户的出发时间。", "motivation": "为了减缓锂离子电池(LIBs)在长时间高状态下的快速退化，需要精准预测用户出发时间以推迟完全充电直至出行前。", "method": "通过将每一天离散成网格化的令牌来表示TTE序列，并利用流式上下文信息进行预测，从而更准确地捕捉到个体日常中的非规律性出发模式。", "result": "实验证明，该方法在涉及93名用户的真实世界研究中优于基线模型。", "conclusion": "结果表明，所提算法具有实际部署的潜力，并为可持续交通系统做出了贡献。"}}
{"id": "2512.07720", "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "该论文提出了一种结合3D重建和视频生成的方法，用于实时创建高质量的上半身数字替身。", "motivation": "当前方法在生成高保真度上半身3D替身时存在纹理模糊、动作生硬等问题。为了解决这些问题，作者提出了ViSA框架，将3D重建模型与视频生成模型相结合。", "method": "ViSA利用3D重建模型提供结构和外观先验知识，并用实时自回归视频扩散模型进行渲染，以实现高保真度的细节合成和动态效果。", "result": "实验表明，该方法显著减少了图像失真问题，在视觉质量上超越了现有方法，提供了更真实、自然的动作表现。", "conclusion": "通过结合3D重建与视频生成的优点，ViSA为实时应用场景（如游戏和虚拟现实）提供了一个强大的解决方案。"}}
{"id": "2512.07712", "pdf": "https://arxiv.org/pdf/2512.07712", "abs": "https://arxiv.org/abs/2512.07712", "authors": ["Sayak Dutta", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "categories": ["cs.CV"], "comment": "9 pages, 2 figures, 2 tables. Accepted to the Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP 2025), Mandi, India", "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "AI": {"tldr": "本文提出了一种三阶段预处理管道，用于提高带笼动物追踪和姿态估计的性能。", "motivation": "现有的动物跟踪和姿势估计系统在处理有笼子结构和系统性遮挡的图像和视频时性能大幅下降。", "method": "该方法包含三个步骤：使用改进的Gabor增强ResNet-UNet进行笼子分割，采用CRFill技术对遮挡区域进行内容感知重建，并在去除笼子后的帧上评估姿态估计和追踪效果。", "result": "实验表明，在移除笼子遮挡后，系统的性能接近于无遮挡环境中的表现，提高了关键点检测准确性和轨迹一致性。", "conclusion": "通过提出的预处理管道，显著改善了带笼动物的跟踪和姿势估计任务的效果。"}}
{"id": "2512.07710", "pdf": "https://arxiv.org/pdf/2512.07710", "abs": "https://arxiv.org/abs/2512.07710", "authors": ["Anxiang Zeng", "Haibo Zhang", "Hailing Zhang", "Kaixiang Mo", "Liang Yao", "Ling Hu", "Long Zhang", "Shuman Liu", "Shuyi Xie", "Yanshi Li", "Yizhang Chen", "Yuepeng Sheng", "Yuwei Huang", "Zhaochen Xu", "Zhiqiang Zhou", "Ziqin Liew"], "title": "Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.", "AI": {"tldr": "论文提出了CompassMax-V3-Thinking模型，通过新的RL框架解决大规模MoE训练中的效率问题。", "motivation": "随着模型规模扩大至百亿级别，传统RL方法在处理零方差提示、不稳定的加权采样和系统性能瓶颈等方面存在挑战。为提高效率与稳定性，提出了新的解决方案。", "method": "论文引入了多阶段零方差消除法来去除无用的提示；ESPO优化算法平衡重要性采样以保持学习稳定；路由回放策略减少训练推理差异，并调整奖励模型防止优势反转；此外还构建了一个高性能RL系统，使用FP8精度和重叠计算。", "result": "新方法显著提高了百亿级MoE模型在内部和公开评估中的性能表现。", "conclusion": "通过这些创新性技术的综合应用，论文成功地解决了大规模MoE模型训练中遇到的问题，并展示了出色的实验结果。"}}
{"id": "2512.07705", "pdf": "https://arxiv.org/pdf/2512.07705", "abs": "https://arxiv.org/abs/2512.07705", "authors": ["Saroj Gopali", "Bipin Chhetri", "Deepika Giri", "Sima Siami-Namini", "Akbar Siami Namin"], "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data. This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning. These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.", "AI": {"tldr": "该论文研究了基于大型语言模型的时序数据预测性能，特别探讨了零样本和少样本学习方法。", "motivation": "随着预训练基础模型（如大型语言模型）的发展，特别是Google的TimesFM模型在时间序列领域的应用，本文旨在探索这些模型是否可以超越现有的时序数据建模方法。", "method": "该论文研究了使用LLM模型进行时序数据预测，包括OpenAI o4-mini和Gemini 2.5 Flash Lite以及Google的TimesFM等模型，并与TCN和LSTM网络进行了比较。", "result": "TimesFM在所有模型中表现最佳，具有最低的RMSE值（0.3023）和竞争性的推理时间（266秒）。此外，基于零样本学习的OpenAI o4-mini也表现出良好的性能。", "conclusion": "该研究发现预训练的时间序列基础模型是实时预测的一个有前途的方向，能够实现准确且可扩展的应用部署，并减少对模型调整的需求。"}}
{"id": "2512.07703", "pdf": "https://arxiv.org/pdf/2512.07703", "abs": "https://arxiv.org/abs/2512.07703", "authors": ["Leo Fillioux", "Enzo Ferrante", "Paul-Henry Cournède", "Maria Vakalopoulou", "Stergios Christodoulidis"], "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.", "AI": {"tldr": "提出了PVeRA，一种基于概率向量的随机矩阵适应方法，用于提升模型在小数据集上的训练效率。", "motivation": "大规模基础模型需要大量数据和计算资源进行训练或微调。为了克服这些限制，提出了一种更高效的适应方法来利用较少的数据和计算资源。", "method": "PVeRA是通过修改VeRA的低秩矩阵以概率方式实现输入处理的不确定性，并允许在训练和测试过程中采用不同的采样配置。", "result": "在VTAB-1k基准测试和其他七个适配器模型上，PVeRA的表现优于VeRA及其他适配器。", "conclusion": "PVeRA通过引入概率机制改进了原有的随机矩阵适应方法，提高了小数据集下的训练效率和效果。"}}
{"id": "2512.07702", "pdf": "https://arxiv.org/pdf/2512.07702", "abs": "https://arxiv.org/abs/2512.07702", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "WACV 2026", "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "AI": {"tldr": "论文提出了一种名为Negative Prompting for Image Correction (NPC)的自动化流程，通过识别并应用负向提示来改进文本到图像生成中的对齐问题。", "motivation": "尽管在文本到图像生成方面取得了重大进展，但在处理具有丰富组成结构或想象力元素的提示时，精确的文字与图像对齐仍然存在挑战。为了解决这个问题，论文引入了NPC技术，以提高文字和图像之间的准确匹配。", "method": "NPC通过分析交叉注意力模式来解释为什么既定的目标负向提示（直接与文本对齐错误相关的）以及未指定的负向提示（生成图像中无关但存在的标记）可以增强对齐。论文利用了一种验证器-描述符-提议者框架生成候选负向提示，并使用显着文本空间得分对其进行排名，以有效选择而无需额外的图像合成。", "result": "在GenEval++和Imagine-Bench基准测试上，NPC优于所有基线模型，在GenEval++上的得分为0.571，相比之下最强基线为0.371；同时在Imagine-Bench上获得了最佳整体表现。", "conclusion": "通过指导不生成什么内容，NPC提供了一条原则性、完全自动化的途径来增强扩散模型中的文本与图像的对齐。"}}
{"id": "2512.07698", "pdf": "https://arxiv.org/pdf/2512.07698", "abs": "https://arxiv.org/abs/2512.07698", "authors": ["Arslan Artykov", "Corentin Sautier", "Vincent Lepetit"], "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "AI": {"tldr": "从单个视频中使用合成训练数据准确建模关节对象", "motivation": "当前工作主要集中在多视角系统，物体扫描或静态摄像机上，而本文提出了一种仅基于合成数据的方法，能够从自由移动相机捕捉的单目视频中预测部件分割和关节参数，以解决动态环境中关节对象理解的问题。", "method": "通过使用合成训练数据，该方法直接在随意记录的视频上操作，适用于实时应用。", "result": "该方法展示了对现实世界物体的强大泛化能力，并提供了一种可扩展且实用的关节物体理解解决方案。", "conclusion": "这种方法为动态环境中的实时应用程序提供了可靠的关节对象建模工具。"}}
{"id": "2512.07697", "pdf": "https://arxiv.org/pdf/2512.07697", "abs": "https://arxiv.org/abs/2512.07697", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "AI": {"tldr": "本文提出了一种名为延迟感知扩散策略（DA-DP）的框架，用于在训练和推理过程中将推断延迟显式地纳入策略学习。", "motivation": "当机器人进行感知并选择动作时，世界处于不断变化之中。这种推断延迟导致观察状态与执行状态之间的差距为数十到数百毫秒不等。因此，引入了一种自然的从零延迟推广至实际测量延迟的方法。", "method": "DA-DP框架通过纠正零延迟轨迹以补偿其延迟并增强策略中的延迟条件化来工作。该方法在多种任务、机器人和延迟上进行了实证验证，并提供了广泛的架构独立性和模仿学习的通用模式。", "result": "实验结果表明，与未考虑延迟的方法相比，DA-DP的成功率对延迟更加稳健。", "conclusion": "本文提出了一个名为Delay-Aware Diffusion Policy (DA-DP)的框架，该方法在面对推断延迟时具有更强的鲁棒性。这鼓励了评估协议使用测量到的时间延迟报告性能，而不仅仅关注任务难度。"}}
{"id": "2512.07687", "pdf": "https://arxiv.org/pdf/2512.07687", "abs": "https://arxiv.org/abs/2512.07687", "authors": ["Sujoy Nath", "Arkaprabha Basu", "Sharanya Dasgupta", "Swagatam Das"], "title": "HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \\textsc{\\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at https://github.com/C0mRD/HalluShift_Plus.", "AI": {"tldr": "本文提出了一种新的方法HalluShift++，通过分析多模态大型语言模型（MLLM）内部表示的动态变化来检测和评估幻觉。", "motivation": "当前的方法主要依赖于外部LLM评估器来检测幻觉，这些评估器自身也容易产生幻觉，并且难以适应不同领域。本文旨在提出一种新的方法以解决这一问题。", "method": "HalluShift++通过分析MLLM内部层动态变化中的可测量异常来检测幻觉，这种新方法可以扩展到多模态场景中。", "result": "结果表明，这种方法能够有效地检测和评估MLLM中的幻觉，并且在不同领域的适应性方面优于现有技术。", "conclusion": "本文通过提出HalluShift++，解决了当前模型开发过程中对外部LLM评估器依赖的问题，提高了幻觉检测的准确性和效率。"}}
{"id": "2512.07684", "pdf": "https://arxiv.org/pdf/2512.07684", "abs": "https://arxiv.org/abs/2512.07684", "authors": ["Zihan Chen", "Lanyu Yu"], "title": "When Large Language Models Do Not Work: Online Incivility Prediction through Graph Neural Networks", "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "10 pages", "summary": "Online incivility has emerged as a widespread and persistent problem in digital communities, imposing substantial social and psychological burdens on users. Although many platforms attempt to curb incivility through moderation and automated detection, the performance of existing approaches often remains limited in both accuracy and efficiency. To address this challenge, we propose a Graph Neural Network (GNN) framework for detecting three types of uncivil behavior (i.e., toxicity, aggression, and personal attacks) within the English Wikipedia community. Our model represents each user comment as a node, with textual similarity between comments defining the edges, allowing the network to jointly learn from both linguistic content and relational structures among comments. We also introduce a dynamically adjusted attention mechanism that adaptively balances nodal and topological features during information aggregation. Empirical evaluations demonstrate that our proposed architecture outperforms 12 state-of-the-art Large Language Models (LLMs) across multiple metrics while requiring significantly lower inference cost. These findings highlight the crucial role of structural context in detecting online incivility and address the limitations of text-only LLM paradigms in behavioral prediction. All datasets and comparative outputs will be publicly available in our repository to support further research and reproducibility.", "AI": {"tldr": "该论文提出了一个基于图神经网络的框架用于检测英文维基百科社区中的不文明行为，包括毒性、攻击性和个人攻击。", "motivation": "现有的方法在准确性和效率上都存在局限性，因此作者提出了一种新的方法来解决在线不文明问题，通过引入结构上下文提高行为预测的准确性。", "method": "模型将每个用户评论表示为一个节点，并根据文本相似度定义边。此外还引入了动态调整注意力机制以平衡结点和拓扑特征的信息聚合。", "result": "实验结果表明所提出的架构在多个指标上优于12种最先进的大型语言模型，同时需要较低的推理成本。", "conclusion": "研究表明结构上下文对检测在线不文明行为至关重要，并解决了仅基于文本的语言模型在行为预测中的局限性。"}}
{"id": "2512.07680", "pdf": "https://arxiv.org/pdf/2512.07680", "abs": "https://arxiv.org/abs/2512.07680", "authors": ["P. A. Wigner", "L. Romanello", "A. Hammad", "P. H. Nguyen", "T. Lan", "S. F. Armanini", "B. B. Kocer", "M. Kovac"], "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination. Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces. Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "AI": {"tldr": "本文提出了一种可在树冠中适应性运动和操作的可空投爬行器。", "motivation": "为了实现对复杂树枝结构的有效抓取与稳定穿越，设计一种能够在树冠中可靠工作的机器人系统。", "method": "结合了基于柔性微刺的轨道、双轨旋转夹持器以及弹性尾巴，实现了在不同曲率和倾斜度枝条上的牢固附着和平稳移动。", "result": "实验表明，在达到90度身体翻滚及倾角的情况下仍能保持可靠抓取；在67.5度斜面上有效攀爬；水平树枝上最大速度为每秒0.55个身体长度，且具备10度偏航转向能力。", "conclusion": "该系统不仅展示了高效的能源利用效率和强大的环境适应性，还通过无人机缆绳部署系统提供了一个可靠、低能耗的平台用于生态采样及树冠内传感。"}}
{"id": "2512.07674", "pdf": "https://arxiv.org/pdf/2512.07674", "abs": "https://arxiv.org/abs/2512.07674", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "AI": {"tldr": "提出了DIST-CLIP框架，用于通过解耦的结构和对比度表示来指导MRI图像的标准化。", "motivation": "现有方法在处理医学成像数据时受限于目标图像需求或过于简单的标签，难以捕获复杂的数据异质性。作者提出了一种新的统一框架以灵活使用目标图像或DICOM元数据进行引导，从而改进了MRI数据的标准化。", "method": "DIST-CLIP通过预训练的CLIP编码器解耦图像中的结构和对比度，并利用自适应风格转换模块将这些对比度嵌入到结构内容中。", "result": "在多组临床数据集上验证，相比现有方法，在风格翻译保真度和组织保存方面有显著改进。", "conclusion": "DIST-CLIP提供了一种灵活的解决方案用于MRI图像标准化，增强了医学成像的数据一致性和分析能力。"}}
{"id": "2512.07673", "pdf": "https://arxiv.org/pdf/2512.07673", "abs": "https://arxiv.org/abs/2512.07673", "authors": ["Matthias Heyrman", "Chenhao Li", "Victor Klemm", "Dongho Kang", "Stelian Coros", "Marco Hutter"], "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "categories": ["cs.RO"], "comment": "15 pages", "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "AI": {"tldr": "提出了一种新的运动表示方法MDME，以实现实时模仿。", "motivation": "现有运动控制器未能有效捕捉人类和动物运动中的结构化周期模式及非周期变化。", "method": "使用基于小波的编码器和概率嵌入来统一表示结构化和非结构化特征。", "result": "在机器人上实现了准确复杂轨迹重现，优于先前的方法，并能实时模仿新动作。", "conclusion": "MDME是实现大规模实时仿真的通用基础方法。"}}
{"id": "2512.07668", "pdf": "https://arxiv.org/pdf/2512.07668", "abs": "https://arxiv.org/abs/2512.07668", "authors": ["Ronan John", "Aditya Kesari", "Vincenzo DiMatteo", "Kristin Dana"], "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "categories": ["cs.CV"], "comment": null, "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "AI": {"tldr": "本文提出了一种新的方法和数据集，用于预测行人户外导航时的视线。", "motivation": "研究并模拟真实世界中行人的视觉注意力，尤其是在室外环境中。", "method": "利用Meta公司的Project Aria眼镜收集了EgoCampus数据集，并基于此开发了EgoCampusNet模型来预测行人在户外环境中的视线。", "result": "提供了用于未来工作的眼球追踪和导航模型研究的资源。", "conclusion": "本文贡献了一套新的资源，用于研究现实世界中的注意力以及未来的注视预测模型研究。"}}
{"id": "2512.07664", "pdf": "https://arxiv.org/pdf/2512.07664", "abs": "https://arxiv.org/abs/2512.07664", "authors": ["Eduardo Vyhmeister", "Bastien Pietropaoli", "UdoBub", "Rob Schneider", "Andrea Visentin"], "title": "A Framework for Data Valuation and Monetisation", "categories": ["cs.CY", "cs.ET"], "comment": null, "summary": "As organisations increasingly recognise data as a strategic resource, they face the challenge of translating informational assets into measurable business value. Existing valuation approaches remain fragmented, often separating economic, governance, and strategic perspectives and lacking operational mechanisms suitable for real settings. This paper introduces a unified valuation framework that integrates these perspectives into a coherent decision-support model. Building on two artefacts from the Horizon Europe DATAMITE project, a taxonomy of data-quality and performance metrics, and an Analytic Network Process (ANP) tool for deriving relative importance, we develop a hybrid valuation model. The model combines qualitative scoring, cost- and utility-based estimation, relevance/quality indexing, and multi-criteria weighting to define data value transparently and systematically. Anchored in the Balanced Scorecard (BSC), the framework aligns indicators and valuation outcomes with organisational strategy, enabling firms to assess monetisation potential across Data-as-a-Service, Information-as-a-Service, and Answers-as-a-Service pathways. Methodologically, the study follows a Design Science approach complemented by embedded case studies with industrial partners, which informed continual refinement of the model. Because the evaluation is connected to a high-level taxonomy, the approach also reveals how valuation considerations map to BSC perspectives. Across the analysed use cases, the framework demonstrated flexibility, transparency, and reduced arbitrariness in valuation, offering organisations a structured basis for linking data assets to strategic and economic outcomes.", "AI": {"tldr": "提出了一种统一的数据估值框架，该框架结合了经济、治理和战略视角，并通过平衡计分卡将指标与企业策略对齐。", "motivation": "组织需要一种方法来将信息资产转化为可衡量的业务价值。现有评估方式缺乏整合性且不适用于实际环境。", "method": "基于Horizon Europe DATAMITE项目中的两个工具，即数据质量和绩效指标分类和ANP工具，结合定性和定量估值方法，开发了一个混合模型。", "result": "该框架在不同使用案例中展示了灵活性、透明度并减少了估值的随意性。", "conclusion": "企业可以利用此结构化的基础将数据资产与战略及经济成果相链接。"}}
{"id": "2512.07661", "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "优化引导扩散法用于生成交互式场景，提高自驾车模拟的现实性和控制性", "motivation": "现有模型在合成复杂交通行为时缺乏可控性或违反物理及社会规则。因此提出一种训练无关框架OMEGA来改善这一问题，提升样本的真实性、一致性和可控性", "method": "基于优化引导扩散法，在每一步反向扩散中通过约束优化重新锚定，确保生成的轨迹符合实际和行为上的合理性；同时将主视角攻击者互动形式化为分布空间中的博弈论优化，近似纳什均衡以生成逼真的安全关键对抗场景", "result": "实验表明，OMEGA提升了生成的真实度、一致性和可控性，自由探索模式下物理与行为上有效的场景比例从32.35％提高到72.27%，控制重点生成模式下从11%提升至80%，并且能生成五倍于现有方法的近碰撞帧", "conclusion": "提出的方法OMEGA有效提升了多代理驾驶场景合成的质量，特别是在自驾车安全评估中的重要性"}}
{"id": "2512.07652", "pdf": "https://arxiv.org/pdf/2512.07652", "abs": "https://arxiv.org/abs/2512.07652", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "AI": {"tldr": "本文提出了一种基于AI的自主水下系统，用于海洋探索和科学研究。", "motivation": "传统海洋探索面临极端条件、低能见度和高成本等挑战，导致大量海域未被探索。为解决这些问题，该研究开发了自动化的水下探测分析报告系统。", "method": "该系统采用YOLOv12 Nano进行实时物体检测，利用ResNet50提取特征，并通过PCA降维和K-Means++聚类基于视觉特性分组海洋对象；还引入GPT-4o Mini生成结构化报告与总结。使用DeepFish及OzFish数据集训练和评估。", "result": "实验结果表明，系统检测水下物体的mAP@0.5为0.512、精度为0.535、召回率为0.438；PCA成功将特征维度降至原有98%方差的同时保留了大部分信息；K-Means聚类根据视觉相似性有效分组检测到的对象；LLM生成的报告提供了有关发现和集群的洞察。", "conclusion": "该集成方法显著减少了潜水员的风险，提高了任务效率，并增强了水下数据分析的速度与深度，为科学探索开辟了新的道路。"}}
{"id": "2512.07651", "pdf": "https://arxiv.org/pdf/2512.07651", "abs": "https://arxiv.org/abs/2512.07651", "authors": ["Yuanye Liu", "Hanxiao Zhang", "Nannan Shi", "Yuxin Shi", "Arif Mahmood", "Murtaza Taj", "Xiahai Zhuang"], "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method", "categories": ["cs.CV"], "comment": ":68U10ACM Class:I.4.6", "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.", "AI": {"tldr": "该论文介绍了用于肝纤维化量化和分析的LiQA数据集，并提出了一种基准方法。", "motivation": "准确评估肝纤维化对于临床管理至关重要，但现有算法在处理复杂现实条件时性能受限。为此，开发了LiQA数据集以推动更稳健的技术进步。", "method": "论文使用半监督学习框架结合外部数据进行肝脏分割，并应用基于CAM的多视图共识方法对肝纤维化分期进行优化。", "result": "评估结果表明，利用多源数据和解剖约束可以显著提高模型在临床环境中的鲁棒性。", "conclusion": "提出的基准方法提高了处理复杂条件下的准确性和稳定性，为未来研究奠定了基础。"}}
{"id": "2512.07647", "pdf": "https://arxiv.org/pdf/2512.07647", "abs": "https://arxiv.org/abs/2512.07647", "authors": ["Georgios Tzachristas", "Lei Deng", "Ioannis Tzachristas", "Gong Zhang", "Renhai Chen"], "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=τ\\|μ_{\\mathrm{tail}}-μ_{\\mathrm{head}}\\|_2$ with $τ=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\leτ\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approxΦ_c(σ+Φ^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.", "AI": {"tldr": "本文开发了一种统一的数学框架，用于通过总变差距离量化Top-k注意截断的近似误差。", "motivation": "为了精确控制注意力机制中的Top-k稀疏化策略所带来的误差，从而提高模型性能并减少计算资源消耗", "method": "利用总变差距离研究了注意力分布与它的Top-k截断之间的关系，并推导出了一系列非渐进确定性界限。通过引入头尾分解证明了输出误差的因子化表达式。", "result": "在合成对数和bert-base-uncased模型上的实验结果验证了$k_ε/n$的预测缩放，表明认证Top-k可以平均减少评分键2-4倍同时满足规定的总变差预算", "conclusion": "提出的理论框架不仅为理解注意力机制提供了新的视角，还提供了一种有效的方法来实现模型优化和资源节省"}}
{"id": "2512.07631", "pdf": "https://arxiv.org/pdf/2512.07631", "abs": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "categories": ["cs.AI", "cs.CC", "cs.IT", "cs.LG"], "comment": null, "summary": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "AI": {"tldr": "本文提出了一个框架，用于预测代理在资源约束下解决任务的可能性。", "motivation": "研究如何确定自主代理是否应该投入资源来解决问题。", "method": "通过信息理论界将问题解决视为信息获取过程，计算有效成本以预测资源需求。", "result": "实验验证显示ACP预测与实际性能紧密相关，并提高了效率。", "conclusion": "框架在LLM和代理工作流程中普遍适用，提供了一种统一的信息理论视角。"}}
{"id": "2512.07628", "pdf": "https://arxiv.org/pdf/2512.07628", "abs": "https://arxiv.org/abs/2512.07628", "authors": ["Zhiqi Li", "Wenhuan Li", "Tengfei Wang", "Zhenwei Wang", "Junta Wu", "Haoyuan Wang", "Yunhan Yang", "Zehuan Huang", "Yang Li", "Peidong Liu", "Chunchao Guo"], "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "AI": {"tldr": "该论文提出了MoCA模型，用于高效生成包含大量组件的三维对象和场景。", "motivation": "现有的基于部分的三维生成方法在处理组件数量增加时存在计算成本高的问题，影响了其可扩展性。", "method": "MoCA通过重要性基数组件路由选择关键组件进行稀疏全局注意力操作，并压缩不重要的组件以减少复杂度。这种方法使得三维资产创建更高效且具有更好的可扩展性。", "result": "实验表明，MoCA在组成对象和场景生成任务上优于现有方法。", "conclusion": "提出的方法能有效地解决大规模三维生成的计算问题，提高了模型性能及效率"}}
{"id": "2512.07627", "pdf": "https://arxiv.org/pdf/2512.07627", "abs": "https://arxiv.org/abs/2512.07627", "authors": ["Maximos Kaliakatsos-Papakostas", "Konstantinos Soiledis", "Theodoros Tsamis", "Dimos Makris", "Vassilis Katsouros", "Emilios Cambouropoulos"], "title": "Incorporating Structure and Chord Constraints in Symbolic Transformer-based Melodic Harmonization", "categories": ["cs.SD", "cs.AI", "cs.SC"], "comment": "Proceedings of the 6th Conference on AI Music Creativity (AIMC 2025), Brussels, Belgium, September 10th-12th", "summary": "Transformer architectures offer significant advantages regarding the generation of symbolic music; their capabilities for incorporating user preferences toward what they generate is being studied under many aspects. This paper studies the inclusion of predefined chord constraints in melodic harmonization, i.e., where a desired chord at a specific location is provided along with the melody as inputs and the autoregressive transformer model needs to incorporate the chord in the harmonization that it generates. The peculiarities of involving such constraints is discussed and an algorithm is proposed for tackling this task. This algorithm is called B* and it combines aspects of beam search and A* along with backtracking to force pretrained transformers to satisfy the chord constraints, at the correct onset position within the correct bar. The algorithm is brute-force and has exponential complexity in the worst case; however, this paper is a first attempt to highlight the difficulties of the problem and proposes an algorithm that offers many possibilities for improvements since it accommodates the involvement of heuristics.", "AI": {"tldr": "本文研究了在基于Transformer的旋律和声化中加入预定义和弦约束的方法，提出了一种结合束搜索、A*算法及回溯法的新方法B*。", "motivation": "通过对用户偏好的考虑，探索如何使自回归Transformer模型能够根据提供的旋律和指定位置处的目标和弦生成符合要求的和声化。", "method": "提出了一个名为B*的算法，该算法结合了束搜索、A*算法以及回溯技术，并能迫使预训练的Transformer满足特定位置上的和弦约束。", "result": "虽然提出的算法在最坏情况下具有指数复杂度，但它为如何将和弦约束融入旋律和声化提供了一种方法，也为未来改进提供了多种可能性。", "conclusion": "尽管B*算法存在计算效率的问题，但它是解决这一问题的第一步尝试，并展示了通过引入启发式技术进一步优化的潜力。"}}
{"id": "2512.07624", "pdf": "https://arxiv.org/pdf/2512.07624", "abs": "https://arxiv.org/abs/2512.07624", "authors": ["Yongbo Yu", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Time Series Foundation Models for Process Model Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.", "AI": {"tldr": "研究使用时间序列基础模型进行过程模型预测，通过零样本或微调方法在直接跟随关系的时间序列上取得比传统模型更好的效果。", "motivation": "现有机器学习和深度学习模型在直接跟随关系的时间序列上表现不佳，主要是由于数据的稀疏性和异质性。本文研究时间序列基础模型作为过程模型预测的一种替代方案。", "method": "利用从现实生活中事件日志中提取的直接跟随关系时间序列，比较零样本使用时间序列基础模型和经过微调后的模型，并与传统和专门训练的模型进行对比。", "result": "时间序列基础模型在不额外训练的情况下一般能取得更低的预测误差（MAE和RMSE），表现出从非过程领域转移到过程领域的时空结构的有效性。虽然微调可以进一步提高准确性，但在较小或更复杂的数据集上这种优势可能会消失。", "conclusion": "该研究显示了时间序列基础模型在处理相关的时间序列时具有的泛化能力和数据效率，并且据我们所知是首个对PMF进行系统评估的临时基础模型的研究。"}}
{"id": "2512.07623", "pdf": "https://arxiv.org/pdf/2512.07623", "abs": "https://arxiv.org/abs/2512.07623", "authors": ["Lalitha A R"], "title": "Context-Adaptive Color Optimization for Web Accessibility: Balancing Perceptual Fidelity and Functional Requirements", "categories": ["cs.HC", "math.OC"], "comment": "8 pages, 2 figures", "summary": "We extend our OKLCH-based accessibility optimization with context-adaptive constraint strategies that achieve near-universal success rates across diverse use cases. Our original strict algorithm reached 66-77% success by prioritizing minimal perceptual change ($ΔE \\leq 5.0$), optimizing for enterprise contexts where brand fidelity is paramount. However, this one-size-fits-all approach fails to serve the broader ecosystem of web developers who need accessible solutions even when strict perceptual constraints cannot be satisfied. We introduce recursive optimization (Mode~1) that compounds small adjustments across iterations, achieving 93.68% success on all color pairs and 100% success on reasonable pairs (contrast ratio $ρ> 2.0$), representing a +27.23 percentage point improvement. A relaxed fallback mode (Mode~2) handles pathological edge cases, reaching 98.73% overall success. Evaluation on 10,000 realistic web color pairs demonstrates that context-aware constraint relaxation, combined with absolute hue preservation, enables practical accessibility compliance while maintaining brand color identity. The median perceptual change remains zero across all modes (most pairs already comply), while the 90th percentile reaches $ΔE_{2000} = 15.55$ in Mode~1 -- perceptually acceptable when hue invariance preserves the essential character of the original color. The approach is deployed in CM-Colors v0.5.0 (800+ monthly downloads), providing developers with explicit control over the accessibility-fidelity trade-off appropriate to their context.", "AI": {"tldr": "该论文提出了一种基于上下文自适应策略的颜色优化方法，用于提高网页的可访问性。", "motivation": "现有的严格算法虽然在某些场景下取得了较好的效果（如企业环境），但在更广泛的场景中存在局限。作者希望通过引入新的优化模式解决这一问题，并满足更多开发者的实际需求。", "method": "论文提出了一种递归优化方法和一种放松的回退模式，通过结合绝对色调保护以及基于OKLCH的颜色调整策略来实现。", "result": "新方法在10,000个网页颜色对上的测试中取得了93.68%的整体成功率为合理对比度配对（对比率ρ>2.0）的情况下达到了100%，并提供了开发者所需的可访问性和保真度之间的权衡。", "conclusion": "该方法在CM-Colors v0.5.0版本中的应用表明，通过上下文感知约束放松和绝对色调保护可以实现实际的可访问性合规性同时保持品牌形象。"}}
{"id": "2512.07618", "pdf": "https://arxiv.org/pdf/2512.07618", "abs": "https://arxiv.org/abs/2512.07618", "authors": ["Jiratchaphat Nanta", "Vorapong Suppakitpaisarn", "Piyashat Sripratak"], "title": "Approximation Algorithms for the $b$-Matching and List-Restricted Variants of MaxQAP", "categories": ["cs.DS", "cs.DM"], "comment": null, "summary": "We study approximation algorithms for two natural generalizations of the Maximum Quadratic Assignment Problem (MaxQAP). In the Maximum List-Restricted Quadratic Assignment Problem, each node in one partite set may only be matched to nodes from a prescribed list. For instances on $n$ nodes where every list has size at least $n - O(\\sqrt{n})$, we design a randomized $O(\\sqrt{n})$-approximation algorithm based on the linear-programming relaxation and randomized rounding framework of Makarychev, Manokaran, and Sviridenko. In the Maximum Quadratic $b$-Matching Assignment Problem, we seek a $b$-matching that maximizes the MaxQAP objective. We refine the standard MaxQAP relaxation and combine randomized rounding over $b$ independent iterations with a polynomial-time algorithm for maximum-weight $b$-matching problem to obtain an $O(\\sqrt{bn})$-approximation. When $b$ is constant and all lists have size $n - O(\\sqrt{n})$, our guarantees asymptotically match the best known approximation factor for MaxQAP, yielding the first approximation algorithms for these two variants.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.07613", "pdf": "https://arxiv.org/pdf/2512.07613", "abs": "https://arxiv.org/abs/2512.07613", "authors": ["Arthur Fleig"], "title": "A Retrospective on Ultrasound Mid-Air Haptics in HCI", "categories": ["cs.HC"], "comment": "3 pages; AlpCHI 2026 Revisiting HCI Research Track", "summary": "In 2013, the UltraHaptics system demonstrated that focused ultrasound could generate perceivable mid-air tactile sensations, building on earlier explorations of airborne ultrasound as a haptic medium. These contributions established ultrasound mid-air haptics (UMH) as a viable interaction modality and laid the technical and perceptual foundations for subsequent advances in Human-Computer Interaction (HCI). In this extended abstract, we revisit this formative work, trace the research and design trajectories it enabled, and reflect on how UMH has supported multisensory interaction, immersion, and inclusion. We also highlight how this line of research exemplifies the value of interdisciplinary collaboration to advance novel interactive technologies.", "AI": {"tldr": "回顾了超声波空中介质触觉技术的发展历程，分析其在人机交互领域的应用价值。", "motivation": "探讨超声波中空中触觉（UMH）技术如何支持多感官互动、沉浸和包容性，并强调跨学科合作的重要性。", "method": "回顾了2013年UltraHaptics系统首次展示的基于聚焦超声波生成可感知空中介质触感的技术，以及后续在HCI领域的进展。", "result": "总结了UMH技术如何促进了人机交互领域的新研究和设计方向，并强调其对于多感官互动、沉浸体验及包容性的重要作用。", "conclusion": "展示了跨学科合作对推动新型交互技术发展的关键价值。"}}
{"id": "2512.07612", "pdf": "https://arxiv.org/pdf/2512.07612", "abs": "https://arxiv.org/abs/2512.07612", "authors": ["Kairong Luo", "Zhenbo Sun", "Xinyu Shi", "Shengqi Chen", "Bowen Yu", "Yunyi Chen", "Chenyi Dang", "Hengtao Tao", "Hui Wang", "Fangming Liu", "Kaifeng Lyu", "Wenguang Chen"], "title": "PCMind-2.1-Kaiyuan-2B Technical Report", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.", "AI": {"tldr": "介绍了PCMind-2.1-Kaiyuan-2B，一个开源的20亿参数模型，旨在提高资源受限条件下的训练效率和效果。", "motivation": "解决大型语言模型发展中开放源码社区与业界之间的知识差距问题。通过提供高质量的数据、有效的数据混合策略以及改进的训练方法来缩小这一差距。", "method": "采用量级数据基准测试法，战略性重复选择方案及多领域课程训练策略以提升稀疏高质数据利用效率，并优化了FP16稳定性等架构修改。", "result": "Kaiyuan-2B模型在性能上与最先进的开源模型相当，展示了适用于资源受限预训练的有效和可扩展解决方案。", "conclusion": "PCMind-2.1-Kaiyuan-2B项目提供了一个全面的开放源代码、数据及代码库，以促进大型语言模型的研究和发展。"}}
{"id": "2512.07611", "pdf": "https://arxiv.org/pdf/2512.07611", "abs": "https://arxiv.org/abs/2512.07611", "authors": ["Yongsheng Lian"], "title": "Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark. Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.", "AI": {"tldr": "本文对比分析了PPO、GRPO和DAPO三种强化学习算法在大型语言模型复杂推理提升中的应用，并进行了参数调整研究。", "motivation": "为了提高大型语言模型的复杂推理能力，通过系统地比较PPO、GRPO和DAPO三种强化学习算法的效果并进行参数调优。", "method": "首先对模型进行精细微调以适应特定的游戏任务Countdown Game，然后在一系列通用推理基准测试中评估其性能。进行了参数分析，探讨了不同参数设置下这些算法的表现。", "result": "RL训练的模型普遍优于未经过此过程的基础模型；增加GRPO和DAPO中的组规模可以提高训练稳定性和准确率；动态抽样（DS）组件在DAPO中的使用未能提升性能，反而关闭该功能时表现最佳。", "conclusion": "研究证明了强化学习方法可显著改进大型语言模型的推理能力，并为参数调优提供了实践指导。"}}
{"id": "2512.07609", "pdf": "https://arxiv.org/pdf/2512.07609", "abs": "https://arxiv.org/abs/2512.07609", "authors": ["Nikita Vaibhav Pavle", "Shrreya Rajneesh", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Obstacle Avoidance of UAV in Dynamic Environments Using Direction and Velocity-Adaptive Artificial Potential Field", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "The conventional Artificial Potential Field (APF) is fundamentally limited by the local minima issue and its inability to account for the kinematics of moving obstacles. This paper addresses the critical challenge of autonomous collision avoidance for Unmanned Aerial Vehicles (UAVs) operating in dynamic and cluttered airspace by proposing a novel Direction and Relative Velocity Weighted Artificial Potential Field (APF). In this approach, a bounded weighting function, $ω(θ,v_{e})$, is introduced to dynamically scale the repulsive potential based on the direction and velocity of the obstacle relative to the UAV. This robust APF formulation is integrated within a Model Predictive Control (MPC) framework to generate collision-free trajectories while adhering to kinematic constraints. Simulation results demonstrate that the proposed method effectively resolves local minima and significantly enhances safety by enabling smooth, predictive avoidance maneuvers. The system ensures superior path integrity and reliable performance, confirming its viability for autonomous navigation in complex environments.", "AI": {"tldr": "本文提出了一种基于方向和相对速度加权的人工势场方法，用于无人机在动态环境中的避障。", "motivation": "传统的APF存在局部极小值问题且无法考虑移动障碍物的运动学特性。本文旨在解决无人机自主避碰的问题，并提高其安全性。", "method": "通过引入一个基于方向和相对速度的加权函数，动态调整人工势场中的排斥力，集成在模型预测控制框架内生成安全轨迹。", "result": "仿真结果表明该方法有效解决了局部极小值问题，并提升了避障性能。", "conclusion": "所提出的系统能够确保路径完整性和可靠的自主导航，在复杂环境中具有应用潜力。"}}
{"id": "2512.07608", "pdf": "https://arxiv.org/pdf/2512.07608", "abs": "https://arxiv.org/abs/2512.07608", "authors": ["Jing Wang", "Jie Shen", "Xing Niu", "Tong Zhang", "Jeremy Weiss"], "title": "Metric-Fair Prompting: Treating Similar Samples Similarly", "categories": ["cs.CL", "cs.AI"], "comment": "ef:NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models", "summary": "We introduce \\emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}~--~treating similar instances similarly~--~we compute question similarity using NLP embeddings and solve items in \\emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each \\((\\text{question}, \\text{option})\\) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.", "AI": {"tldr": "本文提出了一个公平性的提示框架——Metric-Fair Prompting，用于指导大规模语言模型在决策时遵守度量公平性约束。", "motivation": "为了提高大型语言模型在处理类似实例时的一致性和准确性，引入了度量公平的提示框架以确保相似样本得到一致且合理的对待。", "method": "通过计算问题的NLP嵌入来衡量问题间的相似性，并采用一种全局决策协议，该协议包括提取决定性的临床特征、映射每个（问题,选项）对到一个评分f(x)，并施加类似Lipschitz风格的约束以确保相似输入得到相似评分。", "result": "在MedQA(US)基准测试中，与标准单项提示相比，Metric-Fair Prompting提升了模型性能，表明公平导向的信心引导推理可以增强LLM在高风险临床多选题上的准确性。", "conclusion": "研究表明，在处理高风险领域的问题时，采用度量公平性框架能够提高语言模型的决策质量和一致性。"}}
{"id": "2512.07606", "pdf": "https://arxiv.org/pdf/2512.07606", "abs": "https://arxiv.org/abs/2512.07606", "authors": ["Jingna Qiu", "Frauke Wilm", "Mathias Öttl", "Jonas Utz", "Maja Schlereth", "Moritz Schillinger", "Marc Aubreville", "Katharina Breininger"], "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "AI": {"tldr": "本文提出了一种新的主动学习采样策略——分解采样（DECOMP），用于提高区域注释的效率，特别是在密集预测任务中。", "motivation": "当前大多数工作集中在选择具有代表性的图像上以优化分类任务中的标注效率。然而，在医学成像等领域的密集预测任务中，区域级别注释比图像级别注释更有效但成本更高。现有的代表性注释区域选取方法存在计算和内存开销大、相关性差及过度依赖不确定性采样等问题。", "method": "DECOMP通过使用伪标签将图片分解为特定类别的组件并从每个类别中选择区域来提高标注多样性，同时利用类别级别的预测置信度进一步引导采样过程，确保困难类别得到更多的注释。", "result": "在ROI分类、二维分割和三维分割任务上，DECOMP相较于基线方法能更好地选取少数类别的区域，并提升这些挑战性类别上的性能表现。", "conclusion": "该研究通过提出分解采样策略解决了现有主动学习中代表性标注选择存在的问题，提高了密集预测任务中的标注效率。"}}
{"id": "2512.07602", "pdf": "https://arxiv.org/pdf/2512.07602", "abs": "https://arxiv.org/abs/2512.07602", "authors": ["Pengfei Sun", "Zhe Su", "Jascha Achterberg", "Giacomo Indiveri", "Dan F. M. Goodman", "Danyal Akarca"], "title": "Algorithm-hardware co-design of neuromorphic networks with dual memory pathways", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neural networks excel at event-driven sensing yet maintaining task-relevant context over long timescales. However building these networks in hardware respecting both tight energy and memory budgets, remains a core challenge in the field. We address this challenge through novel algorithm-hardware co-design effort. At the algorithm level, inspired by the cortical fast-slow organization in the brain, we introduce a neural network with an explicit slow memory pathway that, combined with fast spiking activity, enables a dual memory pathway (DMP) architecture in which each layer maintains a compact low-dimensional state that summarizes recent activity and modulates spiking dynamics. This explicit memory stabilizes learning while preserving event-driven sparsity, achieving competitive accuracy on long-sequence benchmarks with 40-60% fewer parameters than equivalent state-of-the-art spiking neural networks. At the hardware level, we introduce a near-memory-compute architecture that fully leverages the advantages of the DMP architecture by retaining its compact shared state while optimizing dataflow, across heterogeneous sparse-spike and dense-memory pathways. We show experimental results that demonstrate more than a 4x increase in throughput and over a 5x improvement in energy efficiency compared with state-of-the-art implementations. Together, these contributions demonstrate that biological principles can guide functional abstractions that are both algorithmically effective and hardware-efficient, establishing a scalable co-design paradigm for real-time neuromorphic computation and learning.", "AI": {"tldr": "本文提出了一种新颖的算法硬件协同设计方法，通过引入双记忆路径架构（DMP），提高了脉冲神经网络在长序列任务中的性能，并展示了显著的能量效率和吞吐量提升。", "motivation": "解决构建具有紧致能量预算和内存限制的同时能够长时间保持任务相关上下文的脉冲神经网络的核心挑战。", "method": "通过引入双记忆路径架构（DMP），结合快速脉冲活动与慢速记忆通道，优化了算法设计；同时提出了近存计算架构以充分利用DMP的优势，提高了数据流效率。", "result": "实验结果表明，在长序列基准测试中实现了40-60％的参数减少，并在吞吐量和能效方面分别取得了超过4倍和5倍以上的改进。", "conclusion": "这项研究展示了生物学原理如何指导出既具有算法有效性又硬件高效的函数抽象，建立了实时神经形态计算和学习的大规模协同设计范式。"}}
{"id": "2512.07599", "pdf": "https://arxiv.org/pdf/2512.07599", "abs": "https://arxiv.org/abs/2512.07599", "authors": ["Hanshi Wang", "Zijian Cai", "Jin Gao", "Yiwei Zhang", "Weiming Hu", "Ke Wang", "Zhipeng Zhang"], "title": "Online Segment Any 3D Thing as Instance Tracking", "categories": ["cs.CV"], "comment": "NeurIPS 2025, Code is at https://github.com/AutoLab-SAI-SJTU/AutoSeg3D", "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "AI": {"tldr": "在线实时三维分割构成具身智能体感知和理解操作环境的基础能力。本文重新定义了在线三维分割为实例跟踪问题（AutoSeg3D），通过利用对象查询来传播时序信息，增强了模型的时空理解和计算效率。", "motivation": "现实中的感知是一个动态过程，现有基于查询的方法忽视了时间维度的理解。为了增强具身智能体在环境中的时序感知能力，本文提出了一种新的在线三维分割方法，即实例跟踪问题（AutoSeg3D）。", "method": "利用对象查询进行时序信息传播，通过长时间的实例关联来促进特征和对象身份的一致性，并通过短期实例更新丰富即时观察。此外，引入空间一致性学习以减少片段化的问题，从而提升长期和短期时间学习的有效性。", "result": "该方法在ScanNet200、ScanNet、SceneNN和3RScan数据集上取得了新的最佳结果，比ESAM高出2.8 AP，并且提供了持续的性能增益。", "conclusion": "本文通过将在线三维分割重新定义为实例跟踪问题（AutoSeg3D），解决了现有方法在时序理解上的不足。利用对象查询和空间一致性学习的方法不仅增强了模型对环境的理解，还提高了计算效率。"}}
{"id": "2512.07596", "pdf": "https://arxiv.org/pdf/2512.07596", "abs": "https://arxiv.org/abs/2512.07596", "authors": ["Wenzhen Dong", "Jieming Yu", "Yiming Huang", "Hongqiu Wang", "Lei Zhu", "Albert C. S. Chung", "Hongliang Ren", "Long Bai"], "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "categories": ["cs.CV", "cs.RO"], "comment": "Technical Report", "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "AI": {"tldr": "评估SAM 3在机器人手术中的分割、三维感知和重建能力。", "motivation": "探索并验证Segment Anything Model (SAM) 3在机器人辅助外科手术环境下的性能，尤其是在零样本分割和语言提示方面的表现。", "method": "通过基准测试来比较SAM 3与前一代模型的表现，使用多种类型的数据集包括MICCAI EndoVis、SCARED、StereoMIS和EndoNeRF进行综合评估。重点在于点框提示、动态视频跟踪以及三维重建能力的评价。", "result": "在图像和视频分割任务上，SAM 3显示了相对于其前辈明显的性能提升；然而，在复杂的高度动态场景中仍然存在挑战。语言提示显示出潜力但需进一步训练以适应手术环境。", "conclusion": "尽管面临一些限制，特别是对于复杂场景中的零样本识别，SAM 3依然展示了强大的分割和三维重建能力，并且为未来的研究提供了方向，特别是在加强特定领域数据的训练上。"}}
{"id": "2512.07590", "pdf": "https://arxiv.org/pdf/2512.07590", "abs": "https://arxiv.org/abs/2512.07590", "authors": ["Kaili Qi", "Zhongyi Huang", "Wenli Yang"], "title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.", "AI": {"tldr": "本文提出了一种结合变分模型和深度学习的改进UNet架构，用于提高含噪图像分割效果。", "motivation": "为了克服模糊或断裂边界对图像分割的影响，本文开发了一种融合物理先验信息的新方法，以增强模型在处理复杂噪声环境下的性能。", "method": "该方法采用一种修改后的Cahn-Hilliard方程，并加入了边缘检测器和平均曲率项。架构包括F模块进行高效的频域预处理，以及T模块确保局部计算的准确性和稳定性。", "result": "实验结果表明，提出的模型在多个基准数据集上实现了性能与计算效率的良好平衡，不仅优于纯CNN方法，在视觉质量上也接近Transformer基线。", "conclusion": "改进后的UNet架构结合了变分偏微分方程和深度神经网络的优势，为解决图像分割中的挑战提供了一种有效途径。"}}
{"id": "2512.07584", "pdf": "https://arxiv.org/pdf/2512.07584", "abs": "https://arxiv.org/abs/2512.07584", "authors": ["Meituan LongCat Team", "Hanghang Ma", "Haoxian Tan", "Jiale Huang", "Junqiang Wu", "Jun-Yan He", "Lishuai Gao", "Songlin Xiao", "Xiaoming Wei", "Xiaoqi Ma", "Xunliang Cai", "Yayong Guan", "Jie Hu"], "title": "LongCat-Image Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "AI": {"tldr": "介绍了一种名为LongCat-Image的开创性开源双语（中英文）基础模型，用于图像生成。", "motivation": "解决当前主导模型中存在的多语言文本渲染、照片现实感、部署效率和开发人员可访问性的核心挑战。", "method": "通过严格的预训练、中期训练和微调阶段的数据整理策略实现，并在强化学习阶段协同使用策划的奖励模型。该方法使得模型成为新的行业标准，在中文字符渲染上表现出色，同时保持较小的核心扩散模型（6B参数）以提高效率。", "result": "LongCat-Image在多语言文本渲染、照片现实感和部署效率方面达到了最先进的水平，并且提供了完整的开源生态系统支持。", "conclusion": "LongCat-Image通过其创新方法和技术实现，为视觉内容创建设定了新的行业标准。"}}
{"id": "2512.07583", "pdf": "https://arxiv.org/pdf/2512.07583", "abs": "https://arxiv.org/abs/2512.07583", "authors": ["Navid Asgari", "Benjamin M. Cole"], "title": "Complementary Learning Approach for Text Classification using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "67 pages", "summary": "In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).", "AI": {"tldr": "本文提出了一种利用大规模语言模型进行文本分类的方法，结合了学者和机器的优势，并通过链式思维和少量样本学习提示来降低使用成本。", "motivation": "旨在以低成本高效地集成大型语言模型的能力，同时弥补它们的弱点，促进人类与机器之间的协作。", "method": "该方法利用大规模语言模型，配合链式思考及少量样本学习提示策略，在定量研究中实现人机团队合作，使人类能够使用归纳推理和自然语言来质疑机器和人的工作。", "result": "展示了如何通过这种方法调查人类-机器评分差异，并应用于1934个制药联盟公告的文本分类（1990-2017年）。", "conclusion": "该方法证明了学者可以通过谨慎、低成本的技术解决大规模语言模型固有的弱点，提高了人机合作中的效率和准确性。"}}
{"id": "2512.07582", "pdf": "https://arxiv.org/pdf/2512.07582", "abs": "https://arxiv.org/abs/2512.07582", "authors": ["Guangyan Chen", "Meiling Wang", "Qi Shao", "Zichen Zhou", "Weixin Mao", "Te Cui", "Minzhao Zhu", "Yinan Deng", "Luojie Yang", "Zhanqi Zhang", "Yi Yang", "Hua Chen", "Yufeng Yue"], "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations", "categories": ["cs.RO"], "comment": null, "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.", "AI": {"tldr": "提出了一种从单次观看视频演示中学习任务的视觉语言动作模型ViVLA", "motivation": "现有方法在训练分布之外的任务泛化能力有限，人类可以通过观察一次行为就掌握新技能，受到这一能力启发提出了ViVLA模型", "method": "通过处理专家演示视频和机器人视觉观测来预测行动序列，并利用可扩展的数据生成管道合成专家-代理对样本进行训练", "result": "实验结果表明ViVLA能够从单个专家演示视频中学习新型操作技巧，在LIBERO任务上提高了超过30%的表现，跨身体视频保持了35%以上的提升", "conclusion": "展示了机器人通过观察人类视频可以有效学习新技能"}}
{"id": "2512.07580", "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "研究探讨了视觉令牌剪枝在深层网络中的表现，并发现深层令牌信息逐渐消失，提出了一种基于随机剪枝的新方法。", "motivation": "现有训练无成本的剪枝方法在深层网络中效果不佳，且未能明确深层令牌失去有效性的原因。为了改善这一状况并提高模型效率和性能，研究人员决定探索深层视觉令牌的信息衰减规律以及可能的解决方案。", "method": "通过分析视觉令牌信息随深度变化的情况，并定义了一个衡量令牌信息量的指标来验证其假设：随着网络加深，视觉令牌逐渐失去其有效信息。此外，他们还提出了一种简单的随机剪枝策略在深层进行操作以平衡性能和效率，同时将这种方法与现有的DivPrune技术相结合。", "result": "研究发现，深层视觉令牌的信息会逐渐变得均匀，并最终在一个中间层消失。该位置对于不同的任务类型（如OCR）以及模型的容量不同。通过使用随机剪枝方法可以有效地提高现有剪枝策略的效果，在减少50%的视觉令牌的情况下，保持了96.9%的Qwen-2.5-VL-7B性能。", "conclusion": "研究表明深层视觉令牌的信息确实会消失，并且提出了一个有效的解决方案——使用随机剪枝方法来实现更好的平衡和效率。这表明随机剪枝可以作为现有剪枝策略的一种补充，以提高VLLMs在实际应用中的表现。"}}
{"id": "2512.07577", "pdf": "https://arxiv.org/pdf/2512.07577", "abs": "https://arxiv.org/abs/2512.07577", "authors": ["Artur Czumaj", "Christian Sohler"], "title": "Property Testing of Computational Networks", "categories": ["cs.DS"], "comment": null, "summary": "In this paper we initiate the study of \\emph{property testing of weighted computational networks viewed as computational devices}. Our goal is to design property testing algorithms that for a given computational network with oracle access to the weights of the network, accept (with probability at least $\\frac23$) any network that computes a certain function (or a function with a certain property) and reject (with probability at least $\\frac23$) any network that is \\emph{far} from computing the function (or any function with the given property). We parameterize the notion of being far and want to reject networks that are \\emph{$(ε,δ)$-far}, which means that one needs to change an $ε$-fraction of the description of the network to obtain a network that computes a function that differs in at most a $δ$-fraction of inputs from the desired function (or any function with a given property). To exemplify our framework, we present a case study involving simple neural Boolean networks with ReLU activation function. As a highlight, we demonstrate that for such networks, any near constant function is testable in query complexity independent of the network's size. We also show that a similar result cannot be achieved in a natural generalization of the distribution-free model to our setting, and also in a related vanilla testing model.", "AI": {"tldr": "本文研究了加权计算网络作为计算设备的属性测试。", "motivation": "目的是设计算法，以接受给定的计算网络（该网络可以或接近于计算特定函数），并拒绝远未达到此要求的网络。", "method": "提出了一种框架来测试近似常数函数在加权计算网络中的可测性，并展示了一个关于简单神经布尔网络ReLU激活函数的案例研究。", "result": "证明了对于这样的网络，任何接近于恒定值的功能都可以通过独立于网络大小的查询复杂度进行测试。", "conclusion": "展示了在这种框架下可以实现一些特定的结果，但指出在该分布自由模型的自然推广以及相关朴素测试模型中无法获得类似结果。"}}
{"id": "2512.07576", "pdf": "https://arxiv.org/pdf/2512.07576", "abs": "https://arxiv.org/abs/2512.07576", "authors": ["Xuecheng Li", "Weikuan Jia", "Komildzhon Sharipov", "Sharipov Hotam Beknazarovich", "Farzona S. Ataeva", "Qurbonaliev Alisher", "Yuanjie Zheng"], "title": "R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder--decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.", "AI": {"tldr": "提出了一种用于自动分割多方向脊柱X光图像的递归残差多重路径融合网络R2MF-Net。", "motivation": "为了提高脊柱结构在X射线图像中的准确分割，减少临床医生手动操作的时间和不一致性问题，并处理低对比度图像以及肋骨阴影或重叠组织的情况。", "method": "设计了一种递归残差多重路径编码器-解码器网络R2MF-Net。该网络包括一个粗分割网络和一个细分割网络，它们通过级联连接在一起。改进的Inception风格多分支特征提取器被两个阶段采用，而递归残差跳跃（R2-Jump）模块插入到跳过路径中以逐步对齐编码器和解码器语义。多尺度跨阶段跳转机制允许细网络重用来自粗网络多个解码级别层次表示的图像，从而增强分割在不同成像方向和对比度条件下的稳定性。", "result": "评估了R2MF-Net在一个包含228组冠状面、左弯和右弯脊柱X光图像的临床多视图放射线数据集上的性能。结果表明该网络可以提高对脊柱结构分割的准确性。", "conclusion": "通过引入递归残差跳跃连接机制和多尺度跨阶段跳转机制，R2MF-Net提高了在不同成像方向和对比度条件下脊柱X光图像的自动分割能力，并展示了其潜在的应用价值。"}}
{"id": "2512.07574", "pdf": "https://arxiv.org/pdf/2512.07574", "abs": "https://arxiv.org/abs/2512.07574", "authors": ["Xuecheng Li", "Weikuan Jia", "Komildzhon Sharipov", "Alimov Ruslan", "Lutfuloev Mazbutdzhon", "Ismoilov Shuhratjon", "Yuanjie Zheng"], "title": "Precise Liver Tumor Segmentation in CT Using a Hybrid Deep Learning-Radiomics Framework", "categories": ["eess.IV", "cs.CR", "cs.CV"], "comment": null, "summary": "Accurate three-dimensional delineation of liver tumors on contrast-enhanced CT is a prerequisite for treatment planning, navigation and response assessment, yet manual contouring is slow, observer-dependent and difficult to standardise across centres. Automatic segmentation is complicated by low lesion-parenchyma contrast, blurred or incomplete boundaries, heterogeneous enhancement patterns, and confounding structures such as vessels and adjacent organs. We propose a hybrid framework that couples an attention-enhanced cascaded U-Net with handcrafted radiomics and voxel-wise 3D CNN refinement for joint liver and liver-tumor segmentation. First, a 2.5D two-stage network with a densely connected encoder, sub-pixel convolution decoders and multi-scale attention gates produces initial liver and tumor probability maps from short stacks of axial slices. Inter-slice temporal consistency is then enforced by a simple three-slice refinement rule along the cranio-caudal direction, which restores thin and tiny lesions while suppressing isolated noise. Next, 728 radiomic descriptors spanning intensity, texture, shape, boundary and wavelet feature groups are extracted from candidate lesions and reduced to 20 stable, highly informative features via multi-strategy feature selection; a random forest classifier uses these features to reject false-positive regions. Finally, a compact 3D patch-based CNN derived from AlexNet operates in a narrow band around the tumor boundary to perform voxel-level relabelling and contour smoothing.", "AI": {"tldr": "提出了一种结合注意力增强级联U-Net和手工特征精炼的混合框架，用于CT图像中的肝脏肿瘤自动分割。", "motivation": "准确三维描绘肝肿瘤对于治疗计划、导航和反应评估至关重要，但手动勾画耗时且主观性强。该研究旨在克服低对比度等难题，开发高效精确的方法以自动化这一过程。", "method": "采用级联U-Net生成初步分割图，并通过手工特征选择和3D CNN精炼提高准确性。首先使用2.5D网络进行初始分割；其次利用多片规则增强一致性；再运用随机森林分类器过滤假阳性区域；最后，基于AlexNet的3D CNN实现边界修正。", "result": "该方法在多个数据集上表现出色，提高了肝脏肿瘤自动分割精度和鲁棒性。", "conclusion": "所提框架有效结合深度学习与手工特征的优势，在低对比度及复杂背景情况下仍能准确分割肝肿瘤。"}}
{"id": "2512.07569", "pdf": "https://arxiv.org/pdf/2512.07569", "abs": "https://arxiv.org/abs/2512.07569", "authors": ["Joel Ekstrand", "Tor Mattsson", "Zahra Taghiyarrenani", "Slawomir Nowaczyk", "Jens Lundström", "Mikael Lindén"], "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.", "AI": {"tldr": "提出了一种加权对比学习方法WECA，用于异常感知的时间序列预测。", "motivation": "在ATM现金物流等应用中，需要准确地进行多变量时间序列预测以应对突然的需求变化。然而现有的深度预测模型在正常数据上表现良好但在分布偏移时效果较差。", "method": "通过加权对比学习方法WECA对正常和异常增强表示进行对齐，同时保持异常相关信息并在良性变化中保持一致性。", "result": "使用全国ATM交易数据集评估表明，与普通训练基线相比，WECA在异常影响的数据上提高了6.1个百分点的SMAPE评分，且在正常数据上的性能几乎没有下降。", "conclusion": "WECA能够在不影响常规操作性能的情况下提高时间序列预测在异常情况下的可靠性。"}}
{"id": "2512.07568", "pdf": "https://arxiv.org/pdf/2512.07568", "abs": "https://arxiv.org/abs/2512.07568", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "AI": {"tldr": "提出了一种通过残差语义解相关来解决多模态表示学习中模态主导、冗余信息耦合和虚假跨模态关联问题的框架。", "motivation": "为了解决多模态表示学习中的模态主导性，冗余信息耦合以及虚假跨模态关联等问题，并提高预测时对模态驱动的理解及在部分模态缺失情况下的鲁棒性。", "method": "提出了一种双流残差语义解相关网络（DSRSD-Net），该框架通过残差分解和显式语义解相关约束来分离模态特有信息与共享信息。其包括：1）一个双流表示学习模块，通过残差投影分离出跨模态共享因素；2）一个残差语义对齐头部，使用对比和回归目标将不同模态的共享因素映射到公共空间中；3）一个解相关正交损失，通过抑制特征坍缩并防止特有流与共享流之间的交叉冗余来约束共同空间中的协方差结构。", "result": "在两个大规模教育基准数据集上的实验结果表明，DSRSD-Net比单一模态、早期融合、晚期融合和联合注意基线在下一步预测和最终成果预测上都表现出更高的准确性。", "conclusion": "通过提出的DSRSD-Net框架能够有效解决多模态表示学习中的关键挑战，并且优于多种现有方法。"}}
{"id": "2512.07564", "pdf": "https://arxiv.org/pdf/2512.07564", "abs": "https://arxiv.org/abs/2512.07564", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "24 pages, 3 figures, 2 tables. Training-free self-correction framework for vision-language models. Code and implementation details will be released at: https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git", "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "AI": {"tldr": "提出了一种无训练的自校正框架，以减少视觉语言模型中的幻觉。", "motivation": "解决视觉语言模型生成错误但看似真实的图像内容的问题。", "method": "结合多维度不确定性量化和注意力引导裁剪来改进响应，通过迭代修正提高准确性。", "result": "在POPE和MMHAL BENCH基准测试中，该方法减少了9.8个百分点的幻觉率，并提高了4.7点的对象存在准确度。", "conclusion": "方法有效减少模型幻觉并提高基于视觉证据的准确性。计划在未来版本中验证更多架构以推广研究。"}}
{"id": "2512.07558", "pdf": "https://arxiv.org/pdf/2512.07558", "abs": "https://arxiv.org/abs/2512.07558", "authors": ["Shimin Zhang", "Xianwei Chen", "Yufan Shen", "Ziyuan Ye", "Jibin Wu"], "title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.", "AI": {"tldr": "论文提出了ReLaX方法，通过分析和干预大型推理模型的潜在动力学来提高其探索-利用平衡。", "motivation": "增强大型推理模型（LRM）在强化学习中的推理能力时面临熵塌陷问题，导致政策过早收敛。需要更好的机制来优化这一过程。", "method": "使用Koopman算子理论获取线性化的隐藏状态动力学表示，并引入动态光谱离散度（DSD）衡量模型的潜在异质性，以促进有效的探索-利用平衡。", "result": "实验表明ReLaX能够显著减少过早收敛现象并获得最先进的性能。", "conclusion": "通过调节大型推理模型中的潜在动力学来优化政策搜索过程是提高其性能的有效方法。"}}
{"id": "2512.07544", "pdf": "https://arxiv.org/pdf/2512.07544", "abs": "https://arxiv.org/abs/2512.07544", "authors": ["Kyungro Lee", "Dongha Choi", "Hyunju Lee"], "title": "MoCoRP: Modeling Consistent Relations between Persona and Response for Persona-based Dialogue", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages", "summary": "As dialogue systems become increasingly important across various domains, a key challenge in persona-based dialogue is generating engaging and context-specific interactions while ensuring the model acts with a coherent personality. However, existing persona-based dialogue datasets lack explicit relations between persona sentences and responses, which makes it difficult for models to effectively capture persona information. To address these issues, we propose MoCoRP (Modeling Consistent Relations between Persona and Response), a framework that incorporates explicit relations into language models. MoCoRP leverages an NLI expert to explicitly extract the NLI relations between persona sentences and responses, enabling the model to effectively incorporate appropriate persona information from the context into its responses. We applied this framework to pre-trained models like BART and further extended it to modern large language models (LLMs) through alignment tuning. Experimental results on the public datasets ConvAI2 and MPChat demonstrate that MoCoRP outperforms existing baselines, achieving superior persona consistency and engaging, context-aware dialogue generation. Furthermore, our model not only excels in quantitative metrics but also shows significant improvements in qualitative aspects. These results highlight the effectiveness of explicitly modeling persona-response relations in persona-based dialogue. The source codes of MoCoRP are available at https://github.com/DMCB-GIST/MoCoRP.", "AI": {"tldr": "MoCoRP框架旨在通过显式建模角色与回应之间的关系来提高对话系统的连贯性和个性化。", "motivation": "当前的基于角色对话数据集中缺乏明确的角色句子和回应之间联系，使得模型难以有效捕捉角色信息。为此提出了MoCoRP框架以解决这一问题。", "method": "MoCoRP利用NLI专家提取角色句子与回应间的逻辑关系，并将这些显式关系纳入语言模型中，从而帮助模型更有效地从上下文中汲取合适的角色信息。", "result": "实验结果表明，相对于现有基线方法，MoCoRP在公开数据集ConvAI2和MPChat上实现了更高的角色一致性以及更具吸引力的对话生成。不仅在定量指标方面表现出色，在定性评估中也显著提高了模型性能。", "conclusion": "该研究证明了显式建模角色-回应关系对于增强基于角色对话系统的连贯性和个性化具有重要价值。"}}
{"id": "2512.07540", "pdf": "https://arxiv.org/pdf/2512.07540", "abs": "https://arxiv.org/abs/2512.07540", "authors": ["Boxuan Lyu", "Haiyue Song", "Hidetaka Kamigaito", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Kotaro Funakoshi", "Manabu Okumura"], "title": "Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.", "AI": {"tldr": "本文提出了使用最小贝叶斯风险（MBR）解码来改进参考自由自动机器翻译评估中的错误跨度检测任务。", "motivation": "现有的生成型错误跨度检测方法通常采用最大后验概率（MAP）解码，但这种方法假设模型估计的概率与人工标注的相似度完全相关。然而，实际情况并非如此，因此作者提出使用MBR解码来解决这一问题。", "method": "本文将句级和跨度级相似性指标作为效用函数，并通过这些函数选择最接近人类注释的候选假设，从而改进错误跨度检测任务。", "result": "实验结果表明，在系统、句子及跨度级别上，MBR解码优于MAP基线。此外，应用MBR蒸馏技术可以使标准贪心模型达到与MBR解码相同的性能水平，解决了推理延迟问题。", "conclusion": "本文提出的方法在改进错误跨度检测方面取得了显著成果，并通过MBR蒸馏技术降低了计算成本，提高了效率。"}}
{"id": "2512.07533", "pdf": "https://arxiv.org/pdf/2512.07533", "abs": "https://arxiv.org/abs/2512.07533", "authors": ["Yuzhou Nie", "Hongwei Li", "Chengquan Guo", "Ruizhe Jiang", "Zhun Wang", "Bo Li", "Dawn Song", "Wenbo Guo"], "title": "VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We propose VulnLLM-R, the~\\emph{first specialized reasoning LLM} for vulnerability detection. Our key insight is that LLMs can reason about program states and analyze the potential vulnerabilities, rather than simple pattern matching. This can improve the model's generalizability and prevent learning shortcuts. However, SOTA reasoning LLMs are typically ultra-large, closed-source, or have limited performance in vulnerability detection. To address this, we propose a novel training recipe with specialized data selection, reasoning data generation, reasoning data filtering and correction, and testing-phase optimization. Using our proposed methodology, we train a reasoning model with seven billion parameters. Through extensive experiments on SOTA datasets across Python, C/C++, and Java, we show that VulnLLM-R has superior effectiveness and efficiency than SOTA static analysis tools and both open-source and commercial large reasoning models. We further conduct a detailed ablation study to validate the key designs in our training recipe. Finally, we construct an agent scaffold around our model and show that it outperforms CodeQL and AFL++ in real-world projects. Our agent further discovers a set of zero-day vulnerabilities in actively maintained repositories. This work represents a pioneering effort to enable real-world, project-level vulnerability detection using AI agents powered by specialized reasoning models. The code is available at~\\href{https://github.com/ucsb-mlsec/VulnLLM-R}{github}.", "AI": {"tldr": "提出了一种专门用于漏洞检测的大型语言模型VulnLLM-R", "motivation": "现有方法依赖于简单的模式匹配，无法有效识别复杂的潜在漏洞。因此，需要一种能够理解程序状态并进行逻辑推理以发现漏洞的方法来提高模型泛化能力", "method": "通过特定数据选择、生成、过滤和校正以及测试阶段优化的训练方案来训练具有七亿参数的推理模型。并构建了一个代理框架围绕该模型，用于在实际项目中检测零日漏洞", "result": "VulnLLM-R在多种编程语言中的漏洞检测方面优于现有的静态分析工具和其他大型语言模型", "conclusion": "这是首次使用专门的推理模型和AI代理来实现现实世界的项目级漏洞检测的工作"}}
{"id": "2512.07528", "pdf": "https://arxiv.org/pdf/2512.07528", "abs": "https://arxiv.org/abs/2512.07528", "authors": ["Nishanth Venkatesh", "Andreas A. Malikopoulos"], "title": "Model-Based Reinforcement Learning Under Confounding", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 figures - decompressed draft", "summary": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.", "AI": {"tldr": "本文探讨了在未观察到的情境下，基于模型的强化学习方法如何处理因混淆导致的问题，并提出了一种新的策略以解决这些问题。", "motivation": "传统的基于模型的学习方法在这种存在隐性情境和混淆的数据集情况下是不一致的。因此，需要一种新方法来准确评估状态基政策而非行为政策下的期望奖励。", "method": "本文采用近似离线策略评估方法，结合平均行为转移模型，构建了一个替代理论上的马尔可夫决策过程（MDP），其贝尔曼操作符能够用于一致性状态基策略的规划，并与最大因果熵框架整合。", "result": "提出了新的基于模型的学习和计划方法，在混淆环境下可以使用仅根据可观测的状态-动作-奖励轨迹进行学习，适用于未观察到、不可用或难以收集的情境信息的情况。", "conclusion": "新方法能够在具有隐性情境的复杂环境中有效地进行基于模型的学习和规划，并且在实际应用中表现出强大的灵活性和实用性。"}}
{"id": "2512.07527", "pdf": "https://arxiv.org/pdf/2512.07527", "abs": "https://arxiv.org/abs/2512.07527", "authors": ["Fei Yu", "Yu Liu", "Luyang Tang", "Mingchao Sun", "Zengye Ge", "Rui Bu", "Yuchao Jin", "Haisen Zhao", "He Sun", "Yangyan Li", "Mu Xu", "Wenzheng Chen", "Baoquan Chen"], "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail. To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs. Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.", "AI": {"tldr": "该论文提出了一种从极端偏航角的卫星图像中生成城市级三维重建的方法，解决了现有技术无法处理的大角度视差问题。", "motivation": "解决从稀疏的高空卫星图像中合成地面视角的问题，传统方法如NeRF和3DGS在这种情况下表现不佳。", "method": "首先采用2.5维高度图表示城市几何结构，并通过可微渲染技术从卫星图像中绘制网格外观。为了增强纹理细节，还训练了一个生成式网络来恢复高质量的纹理信息。", "result": "实验表明该方法在大规模城市重建方面表现出色，能够利用少量卫星图片合成逼真的地面视角。", "conclusion": "提出的方法不仅视觉效果吸引人，而且可以用于下游任务如城市规划和仿真。"}}
{"id": "2512.07522", "pdf": "https://arxiv.org/pdf/2512.07522", "abs": "https://arxiv.org/abs/2512.07522", "authors": ["Sebastian Sztwiertnia", "Felix Friedrich", "Kristian Kersting", "Patrick Schramowski", "Björn Deiseroth"], "title": "LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.", "AI": {"tldr": "LIME通过将语法、语义和上下文属性的元数据嵌入到令牌中，提高语言模型的预训练效率。", "motivation": "现有的大型语言模型依赖于大量的高质量数据进行预训练，但这类数据日益稀缺。虽然元数据通常用于创建和管理这些数据集，但它作为直接训练信号的潜力尚未充分发掘。", "method": "LIME方法通过将语法、语义和上下文属性的元数据嵌入到令牌中，增强令牌的表示能力，并提出LIME+1变体以引导生成过程。", "result": "LIME可以使模型适应训练数据分布的速度提高56%，同时仅引入0.01%额外参数，在计算成本几乎可以忽略的情况下改善了语言建模能力和生成任务性能。", "conclusion": "LIME不仅提高了预训练效率，还增强了令牌化过程，并且在不同规模的模型上都表现出了持续的优点。"}}
{"id": "2512.07515", "pdf": "https://arxiv.org/pdf/2512.07515", "abs": "https://arxiv.org/abs/2512.07515", "authors": ["Pengqian Lu", "Jie Lu", "Anjin Liu", "Guangquan Zhang"], "title": "SPAD: Seven-Source Token Probability Attribution with Syntactic Aggregation for Detecting Hallucinations in RAG", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Detecting hallucinations in Retrieval-Augmented Generation (RAG) remains a challenge. Prior approaches attribute hallucinations to a binary conflict between internal knowledge (stored in FFNs) and retrieved context. However, this perspective is incomplete, failing to account for the impact of other components in the generative process, such as the user query, previously generated tokens, the current token itself, and the final LayerNorm adjustment. To address this, we introduce SPAD. First, we mathematically attribute each token's probability into seven distinct sources: Query, RAG, Past, Current Token, FFN, Final LayerNorm, and Initial Embedding. This attribution quantifies how each source contributes to the generation of the current token. Then, we aggregate these scores by POS tags to quantify how different components drive specific linguistic categories. By identifying anomalies, such as Nouns relying on Final LayerNorm, SPAD effectively detects hallucinations. Extensive experiments demonstrate that SPAD achieves state-of-the-art performance", "AI": {"tldr": "SPAD 是一种检测 RAG 中幻觉的方法，通过将每个令牌的概率归因于七个不同的来源来量化各个组件对生成当前令牌的贡献。", "motivation": "现有的方法只能在内部知识和检索上下文之间进行二元冲突分析，而未能考虑用户查询、先前生成的令牌、当前令牌本身以及最终 LayerNorm 调整等因素的影响。因此需要一种新的方法来全面评估这些因素对幻觉检测的作用。", "method": "SPAD 首先将每个标记的概率归因于七个不同的来源：Query, RAG, Past, 当前 Token, FFN, Final LayerNorm 和 Initial Embedding，然后通过词性标签聚合这些分数以量化不同组件如何驱动特定的语法类别。通过识别异常情况（如名词依赖最终 LayerNorm），SPAD 有效检测幻觉。", "result": "在广泛的实验中，SPAD 达到了最先进的性能，证明了其在检测 RAG 中幻觉方面的有效性。", "conclusion": "SPAD 方法全面分析了生成过程中不同组件对当前令牌概率的影响，通过识别异常情况有效地检测出幻觉，并实现了目前最好的结果。"}}
{"id": "2512.07514", "pdf": "https://arxiv.org/pdf/2512.07514", "abs": "https://arxiv.org/abs/2512.07514", "authors": ["Junkai Lin", "Hang Long", "Huipeng Guo", "Jielei Zhang", "JiaYi Yang", "Tianle Guo", "Yang Yang", "Jianwen Li", "Wenxiao Zhang", "Matthias Nießner", "Wei Yang"], "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "categories": ["cs.CV"], "comment": null, "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "AI": {"tldr": "提出了一种生成高质量三维网格的方法MeshRipple，解决了传统方法在长范围几何依赖性上的问题。", "motivation": "为了解决现有自回归网格生成器因记忆限制而产生的长程几何依赖性的缺失问题，导致的孔洞和碎片化组件的问题。", "method": "通过从活跃生成前沿向外扩展网格，并结合前沿感知BFS标记、扩张预测策略和稀疏注意全局内存来解决这一问题。", "result": "MeshRipple可以生成具有高表面保真度和拓扑完整性的网格，优于最近的基准方法。", "conclusion": "提出的MeshRipple解决了现有自回归网格生成器中的关键限制，并展示了在三维物体合成任务上的优越性能。"}}
{"id": "2512.07509", "pdf": "https://arxiv.org/pdf/2512.07509", "abs": "https://arxiv.org/abs/2512.07509", "authors": ["Nikita Gabdullin"], "title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 5 figures, 1 table, 4 equations", "summary": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.", "AI": {"tldr": "探索用于神经网络快速训练的潜在空间配置向量系统的可能性", "motivation": "通过预先配置潜在空间，可以提高神经网络性能并加快大规模分类任务的训练速度。", "method": "利用不同的向量系统来构造和优化潜在空间，以加速图像数据集上的编码器和视觉变换器的训练。", "result": "使用预定义的向量系统进行潜在空间配置可显著加快ImageNet-1K及50k至600k类别的大规模分类任务的训练速度。", "conclusion": "最小化潜在空间维度数可以加速收敛，减少向量数据库大小。"}}
{"id": "2512.07507", "pdf": "https://arxiv.org/pdf/2512.07507", "abs": "https://arxiv.org/abs/2512.07507", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "categories": ["cs.RO", "cs.SE"], "comment": null, "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "AI": {"tldr": "提出了虚拟物理融合测试平台VP-AutoTest，用于自动驾驶汽车的全面测试。", "motivation": "传统测试方法如虚拟仿真、封闭场地和公共道路测试存在车辆状态不真实、测试能力有限及成本高昂等问题。这些问题促使了对虚拟物理融合测试的兴趣。", "method": "构建了一个包含多种虚拟和物理元素的平台，支持单车交互和多车合作测试，并通过对抗测试和平行推断加速故障检测。", "result": "该平台能进行多维度评估并诊断缺陷，同时通过与真实世界的实验结果对比来确保测试的准确性和效率。", "conclusion": "VP-AutoTest解决了传统自动驾驶汽车测试中的限制问题，提高了测试的真实度和效率。"}}
{"id": "2512.07504", "pdf": "https://arxiv.org/pdf/2512.07504", "abs": "https://arxiv.org/abs/2512.07504", "authors": ["Ryota Okumura", "Kaede Shiohara", "Toshihiko Yamasaki"], "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026, 8 pages, supplementary included. Dataset and code: https://github.com/RyotaOkumura/ControlVP", "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "AI": {"tldr": "提出ControlVP框架，用于校正生成图像中的消失点不一致问题。", "motivation": "现有文本到图像模型生成的图像存在几何一致性问题，特别是消失点不一致会导致结构上的不合理性，影响视觉真实感。", "method": "通过引入用户引导和基于建筑轮廓的结构性指导来扩展预训练扩散模型，同时加入明确鼓励图像边缘与透视线索对齐的几何约束。", "result": "增强了全局几何一致性，保持了与基线相比相当的视觉保真度。该方法特别适用于需要准确空间结构的应用场景，例如图象到3D重建。", "conclusion": "ControlVP框架能够有效提升生成图像的几何一致性和真实感，并在特定应用领域表现出色。"}}
{"id": "2512.07503", "pdf": "https://arxiv.org/pdf/2512.07503", "abs": "https://arxiv.org/abs/2512.07503", "authors": ["Yao Teng", "Zhihuan Jiang", "Han Shi", "Xian Liu", "Xuefei Ning", "Guohao Dai", "Yu Wang", "Zhenguo Li", "Xihui Liu"], "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.", "AI": {"tldr": "SJD++是一种训练无关的并行解码算法，用于加速自回归文本到图像生成。", "motivation": "大型自回归模型可以生成高质量的高分辨率图像，但在推理过程中由于需要进行数百甚至数千次顺序前向传递来进行下一个标记预测而速度缓慢。因此提出了一种新的方法来解决这个问题。", "method": "SJD++结合了Jacobi解码中的迭代多令牌预测机制和投机采样的概率性草稿与验证机制，并且在每次验证阶段后重用高置信度的草案令牌而不是重新采样所有令牌，以此来加速自回归文本到图像生成过程。", "result": "实验表明SJD++能在不降低视觉质量的情况下实现2倍至7倍的推理延迟减少和步骤压缩。", "conclusion": "通过引入SJD++算法可以显著提高大型自回归模型在生成高分辨率高质量图像时的速度，同时保持良好的视觉效果。"}}
{"id": "2512.07501", "pdf": "https://arxiv.org/pdf/2512.07501", "abs": "https://arxiv.org/abs/2512.07501", "authors": ["Weilin Luo", "Xueyi Liang", "Haotian Deng", "Yanan Liu", "Hai Wan"], "title": "AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\\% verification success rate, significantly surpassing the $65$\\% success rate of the SOTA approach.", "AI": {"tldr": "AutoICE是一种通过LLM驱动的进化搜索自动生成可验证C代码的方法，提高了代码生成的质量和效率。", "motivation": "自动合成符合自然语言规范的可验证代码有助于提升软件正确性和可靠性。然而现有方法因缺乏领域特定的预训练语料库和难以有效形式化隐含知识而存在严重语法和语义错误。", "method": "AutoICE引入了多样个体初始化、协作交叉以实现迭代更新的多样性，从而减少单代理迭代中的误差传播，并采用自省变异来促进隐性知识的发现。", "result": "实验结果显示，AutoICE验证代码的成功率为90.36%，优于现有最佳方法。在开发者友好的数据集变体中，成功率为88.33%，显著高于65%的最佳方法成功率。", "conclusion": "AutoICE通过引入进化搜索策略和自省变异机制，在生成高质量、可验证的C代码方面取得了显著进步。"}}
{"id": "2512.07500", "pdf": "https://arxiv.org/pdf/2512.07500", "abs": "https://arxiv.org/abs/2512.07500", "authors": ["Penghui Liu", "Jiangshan Wang", "Yutong Shen", "Shanhui Mo", "Chenyang Qi", "Yue Ma"], "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.", "AI": {"tldr": "本文提出了MultiMotion框架，用于解决多对象视频运动转移中的挑战。", "motivation": "由于多物体视频中的运动纠缠和缺乏对象级控制，Diffusion Transformer（DiT）架构在处理多主体视频运动转换时面临重大挑战。", "method": "本文引入了Maskaware Attention Motion Flow (AMF) 方法，利用SAM2掩码显式分离并控制多个目标的运动特征。此外，还提出了一种高阶预测校正求解器RectPC，用于高效准确的采样，特别是在多实体生成中。", "result": "MultiMotion在多个不同对象之间实现了精确、语义对齐和时间一致性的运动转移，同时保持了DiT的高质量和可扩展性。", "conclusion": "该研究提供了一个能够有效解决多主体视频运动转移问题的新框架。"}}
{"id": "2512.07498", "pdf": "https://arxiv.org/pdf/2512.07498", "abs": "https://arxiv.org/abs/2512.07498", "authors": ["Chih-Chung Hsu", "Shao-Ning Chen", "Chia-Ming Lee", "Yi-Fang Wang", "Yi-Shiuan Chou"], "title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior", "categories": ["cs.CV"], "comment": "16 pages (including appendix)", "summary": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.", "AI": {"tldr": "提出了一种新的检测DeepFake的方法，该方法通过构建适应性稀疏图嵌入和显式拉普拉斯谱先验，从不稳定或无序的面部序列中识别出伪造视频。", "motivation": "现有的DeepFake检测器假设面部序列在时间上是连贯且清晰的，但在实际情况中，压缩失真、遮挡和对抗性攻击会导致面部检测失效。为了解决这些问题，该论文提出了一个新的方法来提高检测器在不稳定或无序面部序列下的鲁棒性。", "method": "提出了一种拉普拉斯正则化图卷积网络（LR-GCN），该网络构建了一个无序时间图嵌入（OF-TGE）来组织帧级CNN特征，并引入了显式图拉普拉斯谱先验，以突出结构异常和伪造痕迹。此外，还实施了一种双层稀疏机制来抑制无效面部的影响。", "result": "在FF++、Celeb-DFv2和DFDC数据集上的实验表明，所提出的方法达到了最先进的性能，并且在严重全局和局部干扰下显著提高了鲁棒性。", "conclusion": "通过构建适应性稀疏图嵌入和显式拉普拉斯谱先验，该方法可以在不稳定的或无序的面部序列中有效地检测DeepFake。"}}
{"id": "2512.07497", "pdf": "https://arxiv.org/pdf/2512.07497", "abs": "https://arxiv.org/abs/2512.07497", "authors": ["JV Roig"], "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations", "categories": ["cs.AI", "cs.SE"], "comment": "48 pages, 3 tables, 2 listings", "summary": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.", "AI": {"tldr": "研究大型语言模型（LLMs）作为自主代理在工具使用场景中的表现，通过KAMI v0.1基准测试分析三种代表性模型的执行轨迹。", "motivation": "探讨模型规模与代理行为稳健性的关系，识别失败模式并提出改善方法。", "method": "利用KAMI v0.1基准对Granite 4 Small、Llama 4 Maverick和DeepSeek V3.1等三种模型进行900次执行轨迹分析，着重于每步的行为细化而非总体分数。", "result": "发现LLMs的代理行为稳健性并不完全依赖于模型规模；失败模式包括无依据行动、过度帮助、环境干扰和负载下的脆弱性。", "conclusion": "可靠的企业部署不仅需要强大的模型，还需通过训练和设计选择强化验证、限制发现及遵循源数据。"}}
{"id": "2512.07487", "pdf": "https://arxiv.org/pdf/2512.07487", "abs": "https://arxiv.org/abs/2512.07487", "authors": ["David M. Allison", "Stephen Herzog"], "title": "Artificial Intelligence and Nuclear Weapons Proliferation: The Technological Arms Race for (In)visibility", "categories": ["cs.CY", "cs.AI"], "comment": "Best Paper Award (2025) from Risk Analysis as one of the articles published in the journal that year with the most significant impacts to the theory or practice of risk analysis. Main text: 17 pages, 5 tables, 5 figures. Online appendix: 4 pages, 3 figures, 1 table. Online simulation tool for the formal model available here: https://david-m-allison.github.io/ProliferationSimulation", "summary": "A robust nonproliferation regime has contained the spread of nuclear weapons to just nine states. Yet, emerging and disruptive technologies are reshaping the landscape of nuclear risks, presenting a critical juncture for decision makers. This article lays out the contours of an overlooked but intensifying technological arms race for nuclear (in)visibility, driven by the interplay between proliferation-enabling technologies (PETs) and detection-enhancing technologies (DETs). We argue that the strategic pattern of proliferation will be increasingly shaped by the innovation pace in these domains. Artificial intelligence (AI) introduces unprecedented complexity to this equation, as its rapid scaling and knowledge substitution capabilities accelerate PET development and challenge traditional monitoring and verification methods. To analyze this dynamic, we develop a formal model centered on a Relative Advantage Index (RAI), quantifying the shifting balance between PETs and DETs. Our model explores how asymmetric technological advancement, particularly logistic AI-driven PET growth versus stepwise DET improvements, expands the band of uncertainty surrounding proliferation detectability. Through replicable scenario-based simulations, we evaluate the impact of varying PET growth rates and DET investment strategies on cumulative nuclear breakout risk. We identify a strategic fork ahead, where detection may no longer suffice without broader PET governance. Governments and international organizations should accordingly invest in policies and tools agile enough to keep pace with tomorrow's technology.", "AI": {"tldr": "研究人工智能技术对核武器不可见性的影响及其与检测增强技术之间的竞赛。", "motivation": "新兴和颠覆性技术正在重塑核风险格局，提出了决策者的关键节点，即如何应对核武器的不可见化和技术竞赛问题。", "method": "构建了一个以相对优势指数为中心的形式模型，分析了创新步伐在这一领域的战略模式。通过基于场景的可重复模拟评估不同因素对累积核突破风险的影响。", "result": "发现技术不对称进步扩大了关于核武器扩散可见性的不确定性范围，并且检测可能不再足够，除非有更广泛的核技术治理政策。", "conclusion": "政府和国际组织应该投资于足够的政策和工具来应对未来的技术挑战。"}}
{"id": "2512.07483", "pdf": "https://arxiv.org/pdf/2512.07483", "abs": "https://arxiv.org/abs/2512.07483", "authors": ["Daniel Fürst", "Matthijs Jansen op de Haar", "Mennatallah El-Assady", "Daniel A Keim", "Maximilian T. Fischer"], "title": "SemanticTours: A Conceptual Framework for Non-Linear, Knowledge Graph-Driven Data Tours", "categories": ["cs.HC"], "comment": "14 pages, 9 figures, 2 tables", "summary": "Interactive tours help users explore datasets and provide onboarding. They rely on a linear sequence of views, showing a curated set of relevant data selections and introduce user interfaces. Existing frameworks of tours, however, often do not allow for branching and refining hypotheses outside of a rigid sequence, which is important in knowledge-centric domains such as law. For example, lawyers performing analytical case analysis need to iteratively weigh up different legal norms and construct strings of arguments. To address this gap, we propose SemanticTours, a semantic, graph-based model of tours that shifts from a sequence-based towards a graph-based navigation. Our model constructs a domain-specific knowledge graph that connects data elements based on user-definable semantic relationships. These relationships enable non-linear graph navigation that defines tours. We apply SemanticTours to the domain of law and conceptualize a visual analytics design and interaction concept for analytical reasoning in legal case analysis. Our concept accounts for the inherent complexity of graph-based tours using aggregated graph nodes and supporting navigation with a semantic lens. During an evaluation with six domain experts from law, they suggest that graph-based tours better support their analytical reasoning than sequences. Our work opens research opportunities for such tours to support analytical reasoning in law and other knowledge-centric domains.", "AI": {"tldr": "提出了一种基于知识图的非线性数据游览框架SemanticTours，以支持法律等领域的复杂分析。", "motivation": "现有数据游览框架大多依赖于固定的顺序视图，这限制了在知识密集型领域（如法律）中的迭代探索和假设验证。作者旨在开发一种更具灵活性的方法来应对这一挑战。", "method": "设计了一种基于语义关系的知识图模型，允许用户通过非线性导航进行灵活的数据探索，并将此方法应用于法律分析场景中，提出了相应的可视化界面与交互概念。", "result": "在六名领域专家的评估下，该框架支持了更有效的分析推理过程。", "conclusion": "SemanticTours为复杂领域的分析推理提供了一种新的视角和工具，特别是在知识密集型领域如法律方面有显著的应用前景。"}}
{"id": "2512.07482", "pdf": "https://arxiv.org/pdf/2512.07482", "abs": "https://arxiv.org/abs/2512.07482", "authors": ["Florian Lüttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 8 figures", "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "AI": {"tldr": "本文通过分析高速公路的真实交通数据，提出了识别和生成安全相关驾驶场景的方法。", "motivation": "随着自动驾驶功能和技术复杂性的增加，需要在开发和部署前预先识别相关的驾驶场景以提高验证效率。", "method": "基于AVEAS项目的数据采集与处理流程，对高速公路的变道场景应用关键性衡量指标进行评估，并提出生成合成场景的方法来应对大量未知安全问题场景。", "result": "展示了一条可以识别、提取和生成公路交通中安全相关驾驶场景的处理链。", "conclusion": "通过分析真实数据并使用合成方法，能够有效识别和处理安全相关的驾驶场景，进而提高自动驾驶车辆和高级驾驶员辅助系统的开发与验证效率。"}}
{"id": "2512.07480", "pdf": "https://arxiv.org/pdf/2512.07480", "abs": "https://arxiv.org/abs/2512.07480", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Zihan Zheng", "Yuan Zhang", "Yan Lu"], "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance", "categories": ["cs.CV"], "comment": null, "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.", "AI": {"tldr": "提出了一种基于单步扩散的视频编码方法S2VC，用于在低比特率下实现高质量的视频压缩。", "motivation": "现有神经视频编解码器虽然改善了感知质量，但仍面临生成能力有限和采样复杂度高的问题。因此，引入一种结合条件框架和高效生成方式的新方法来解决这些问题。", "method": "S2VC利用单步扩散编码器与高效的生成框架相结合，并通过上下文语义引导从缓冲特征中提取帧自适应的语义信息；同时在时间一致性指导下保证了跨帧的一致性。", "result": "实验表明，该方法相较于先前的方法，在感知质量上达到了最先进的水平，平均比特率节省达52.73%。", "conclusion": "单步扩散模型应用于视频压缩中显示出了高效的高质量编码潜力。"}}
{"id": "2512.07474", "pdf": "https://arxiv.org/pdf/2512.07474", "abs": "https://arxiv.org/abs/2512.07474", "authors": ["Yifei Huang", "Tianyu Yan", "Sitong Gong", "Xiwei Gao", "Caixin Kang", "Ruicong Liu", "Huchuan Lu", "Bo Zheng"], "title": "Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story's world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne's Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.", "AI": {"tldr": "开发了一种将文学作品转化为沉浸式多角色对话体验的系统，解决LLM驱动的角色在保持人物特征和叙事连贯性方面的问题", "motivation": "解决通用LLM中的人物漂移问题以及超出故事逻辑的能力导致的情节不一致性和鲁棒性失败", "method": "提出一种两阶段训练流程：第一阶段使用无数据强化微调提升角色一致性；第二阶段利用时间感知知识图谱和检索驱动的训练加强叙事连贯性和鲁棒性", "result": "实验验证了该系统的有效性能，特别是在人物特定指标上超越GPT-4，并在一致性和鲁棒性方面接近完美表现", "conclusion": "研究揭示了AI驱动叙述系统的设计准则：自适应培训对于可信度至关重要；明确的时间感知约束对保持连贯且中断耐受的移动网络体验是关键"}}
{"id": "2512.07472", "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($π_{0}$ and $π_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "本文提出了一种利用三维空间可操作性场（SAFs）干预视觉语言动作模型的方法，以增强其在机器人操控中的鲁棒性和适应性。", "motivation": "视觉-语言-行动（VLA）模型由于缺乏显式的3D空间推理，在遇到分布变化时容易陷入记忆陷阱。这限制了它们在未知环境下的灵活性和可靠性。", "method": "引入了可操作性场干预（AFI），通过SAFs作为插件引导VLA行为，检测到记忆陷阱后重新定位机器人，并选择累积可操作性最高的轨迹。", "result": "实验表明该方法平均提高了23.5%的性能，验证了其在增强VLA模型应对分布变化鲁棒性方面的有效性。", "conclusion": "通过引入SAFs作为干预手段，可以显著提高视觉-语言-行动模型在机器人操控任务中的适应性和可靠性。"}}
{"id": "2512.07469", "pdf": "https://arxiv.org/pdf/2512.07469", "abs": "https://arxiv.org/abs/2512.07469", "authors": ["Xiangpeng Yang", "Ji Xie", "Yiyuan Yang", "Yan Huang", "Min Xu", "Qiang Wu"], "title": "Unified Video Editing with Temporal Reasoner", "categories": ["cs.CV"], "comment": "Project Page: https://videocof.github.io/", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "AI": {"tldr": "本文提出了一种新的视频编辑方法VideoCoF，它通过一个显式的推理步骤实现了精确的指令到区域对齐和细粒度的视频编辑。", "motivation": "现有的视频编辑方法存在权衡：专家模型虽然精准但依赖特定任务的先验条件如掩码；统一的时间上下文学习模型则不使用掩码，但在指令到区域映射方面较弱且定位不够准确。", "method": "本文提出了VideoCoF，一种基于Chain-of-Frames推理的方法。该方法通过预测编辑区域潜变量进行显式推理，并确保视频扩散模型在生成目标帧之前完成这一过程。", "result": "实验显示，在仅使用50k对视频数据的情况下，VideoCoF达到了最新的性能水平。", "conclusion": "本文展示了VideoCoF的高效性和有效性，证明了其能以较低的数据成本达到最先进的编辑效果。"}}
{"id": "2512.07464", "pdf": "https://arxiv.org/pdf/2512.07464", "abs": "https://arxiv.org/abs/2512.07464", "authors": ["Haolin Song", "Hongbo Zhu", "Tao Yu", "Yan Liu", "Mingqi Yuan", "Wengang Zhou", "Hua Chen", "Houqiang Li"], "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "AI": {"tldr": "介绍了一种适应复杂地形的人形机器人步态感知行走框架", "motivation": "现有技术在复杂地形上实现可靠的人形机器人行走仍具挑战性，尤其是在长楼梯等场景中。本文旨在提高人形机器人的环境感知能力，以实现在多变地形上的稳健行走", "method": "提出了一个融合地形感测、步态调节和全身控制的单个强化学习策略。使用安装在底座下的向下深度相机实时重建密集的高度地图，并采用统一策略产生关节命令及全球踏步相位信号。通过单一阶段教师-学生训练方案提高政策的学习效率", "result": "实验展示了机器人在模拟与现实世界中的稳健行走，包括上下楼梯和跨越46厘米间隙的能力", "conclusion": "本文提出的框架增强了人形机器人的适应性和稳定性，在复杂地形上实现了可靠的行走"}}
{"id": "2512.07462", "pdf": "https://arxiv.org/pdf/2512.07462", "abs": "https://arxiv.org/abs/2512.07462", "authors": ["Trung-Kiet Huynh", "Duy-Minh Dao-Sy", "Thanh-Bang Cao", "Phong-Hao Le", "Hong-Dan Nguyen", "Phu-Quy Nguyen-Lam", "Minh-Luan Nguyen-Vo", "Hong-Phat Pham", "Phu-Hoa Pham", "Thien-Kim Than", "Chi-Nguyen Tran", "Huy Tran", "Gia-Thoai Tran-Le", "Alessio Buscemi", "Le Hong Trang", "The Anh Han"], "title": "Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG", "math.DS"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.", "AI": {"tldr": "通过博弈论框架评估大型语言模型在重复社会困境中的行为。", "motivation": "理解大型语言模型作为自主决策者的行为对安全、协调以及人工智能驱动的社会和经济基础设施设计具有重要意义。", "method": "扩展FAIRGAME框架，引入带有动态收益的多代理公共物品游戏来系统地评估LLM行为。使用传统监督分类模型训练在经典重复博弈策略上进行路径分析，揭示LLM的行为意图。", "result": "发现了一致的行为特征，包括敏感于激励的合作、跨语言差异以及最终转向背叛的趋势。", "conclusion": "提供了一个统一的方法论基础来审计作为战略代理的大型语言模型，并揭示了系统性的合作偏见及其对AI治理和集体决策的影响。"}}
{"id": "2512.07459", "pdf": "https://arxiv.org/pdf/2512.07459", "abs": "https://arxiv.org/abs/2512.07459", "authors": ["Xiangjun Tang", "Biao Zhang", "Peter Wonka"], "title": "Human Geometry Distribution for 3D Animation Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Generating realistic human geometry animations remains a challenging task, as it requires modeling natural clothing dynamics with fine-grained geometric details under limited data. To address these challenges, we propose two novel designs. First, we propose a compact distribution-based latent representation that enables efficient and high-quality geometry generation. We improve upon previous work by establishing a more uniform mapping between SMPL and avatar geometries. Second, we introduce a generative animation model that fully exploits the diversity of limited motion data. We focus on short-term transitions while maintaining long-term consistency through an identity-conditioned design. These two designs formulate our method as a two-stage framework: the first stage learns a latent space, while the second learns to generate animations within this latent space. We conducted experiments on both our latent space and animation model. We demonstrate that our latent space produces high-fidelity human geometry surpassing previous methods ($90\\%$ lower Chamfer Dist.). The animation model synthesizes diverse animations with detailed and natural dynamics ($2.2 \\times$ higher user study score), achieving the best results across all evaluation metrics.", "AI": {"tldr": "本文提出了一种用于生成逼真的三维人体动画的新方法，通过紧凑的分布式潜在表示和身份条件设计来解决几何细节不足及运动数据有限的问题。", "motivation": "由于需要在有限的数据下模拟自然服装动态并保持高精度的几何细节，所以产生逼真的人体几何动画仍然是一个挑战。因此，本文提出了新的解决方案以提高生成质量。", "method": "方法包括两部分：1）一种基于分布的紧凑潜在表示；2）一个能够充分利用少量运动数据多样性的生成模型。通过这两个设计形成了两阶段框架：第一阶段学习潜在空间，第二阶段在此空间中生成动画。", "result": "实验显示所提出的潜在空间在几何细节方面优于以往的方法（90%更低的Chamfer距离）。而动画模型合成的结果得到了用户的高度评价（2.2倍于以前方法的用户研究分数），成为所有评估指标中的最佳。", "conclusion": "本文通过两个创新设计解决了生成逼真三维人体动画的关键问题，提高了几何细节的质量和动画多样性。"}}
{"id": "2512.07454", "pdf": "https://arxiv.org/pdf/2512.07454", "abs": "https://arxiv.org/abs/2512.07454", "authors": ["Amir Mohammad Akhlaghi", "Amirhossein Shabani", "Mostafa Abdolmaleki", "Saeed Reza Kheradpisheh"], "title": "Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini -- originally a monolingual English model -- can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique \"warm-up\" stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at https://huggingface.co/amirakhlaghiqqq/PersianPhi.", "AI": {"tldr": "本文介绍了Persian-Phi模型，通过课程学习的方法将英文单语大型语言模型Microsoft Phi-3 Mini有效适应到波斯语，并取得了与大规模多语言基线相媲美的性能。", "motivation": "由于低资源语言的训练需要巨大的计算成本，使得AI普及受到限制。本文提出了一种高效的跨语言适应方法，旨在以较小的成本将大型语言模型扩展到未充分代表的语言中。", "method": "通过使用双语叙述进行预热阶段来对齐嵌入，并采用持续预训练和参数高效微调的方式，使单语英语模型Microsoft Phi-3 Mini能够有效适应波斯语。", "result": "尽管模型规模较小，Persian-Phi在开放的波斯语言模型排行榜上表现优异，证明了这种方法的有效性。", "conclusion": "研究提供了一个可验证且可扩展的框架，用于将最先进的大型语言模型以最少的硬件资源扩展到未充分代表的语言中。"}}
{"id": "2512.07453", "pdf": "https://arxiv.org/pdf/2512.07453", "abs": "https://arxiv.org/abs/2512.07453", "authors": ["Van An Nguyen", "Vuong Khang Huynh", "Ho Nam Duong", "Huu Loi Bui", "Hai Anh Ha", "Quang Dung Le", "Le Quoc Dung Ngo", "Tan Dat Nguyen", "Ngoc Ngu Nguyen", "Hoai Thuong Nguyen", "Zhao Song", "Le Hong Trang", "The Anh Han"], "title": "Social welfare optimisation in well-mixed and structured populations", "categories": ["physics.soc-ph", "cs.AI", "cs.MA", "math.OC", "nlin.AO"], "comment": null, "summary": "Research on promoting cooperation among autonomous, self-regarding agents has often focused on the bi-objective optimisation problem: minimising the total incentive cost while maximising the frequency of cooperation. However, the optimal value of social welfare under such constraints remains largely unexplored. In this work, we hypothesise that achieving maximal social welfare is not guaranteed at the minimal incentive cost required to drive agents to a desired cooperative state. To address this gap, we adopt to a single-objective approach focused on maximising social welfare, building upon foundational evolutionary game theory models that examined cost efficiency in finite populations, in both well-mixed and structured population settings. Our analytical model and agent-based simulations show how different interference strategies, including rewarding local versus global behavioural patterns, affect social welfare and dynamics of cooperation. Our results reveal a significant gap in the per-individual incentive cost between optimising for pure cost efficiency or cooperation frequency and optimising for maximal social welfare. Overall, our findings indicate that incentive design, policy, and benchmarking in multi-agent systems and human societies should prioritise welfare-centric objectives over proxy targets of cost or cooperation frequency.", "AI": {"tldr": "本文通过单一目标的方法最大化社会福利，研究了不同干扰策略如何影响合作频率和社会福祉。", "motivation": "以往的研究集中在最小化激励成本和最大化合作频率上。然而，最大化的社会福利在这样的约束下尚未得到充分探索。", "method": "采用基于演化博弈论的分析模型以及代理基模拟来研究不同的干预策略对社会福利的影响。", "result": "研究表明，优化纯的成本效率或合作频率与最大化社会福利之间存在显著差异。", "conclusion": "研究结果表明，在多智能体系统和社会中设计激励措施、政策和基准应优先考虑基于福祉的目标。"}}
{"id": "2512.07450", "pdf": "https://arxiv.org/pdf/2512.07450", "abs": "https://arxiv.org/abs/2512.07450", "authors": ["Imran Ahsan", "Hyunwook Yu", "Jinsung Kim", "Mucheol Kim"], "title": "Forget and Explain: Transparent Verification of GNN Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining). Code is available at https://github.com/ImranAhsan23/F-E", "summary": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.", "AI": {"tldr": "本文提出了一种透明验证方法，用于确认图神经网络在删除指定信息后的遗忘情况。", "motivation": "现有的GNN遗忘方法注重效率和可扩展性，缺乏透明度。文章旨在通过解释性的指标来证明遗忘过程的真实性，并且提供可视化证据。", "method": "该研究通过对比模型遗忘前后快照的差异，使用五种解释性指标（残差归因、热图偏移、可解释评分偏差、图编辑距离和诊断规则变化）作为透明证据。实验中采用GCN和GAT两种基础结构，并测试了Retrain, GraphEditor, GNNDelete, IDEA四种遗忘策略。", "result": "结果表明，Retrain和GNNDelete可以实现几乎完全的遗忘，GraphEditor提供了部分删除的效果，而IDEA则留有残余信号。这些解释性差异是遗忘的主要证据；同时报告了成员推理ROC-AUC作为补充性的全局隐私度量。", "conclusion": "该验证方法为理解图神经网络遗忘过程提供了一种新的视角，并且通过透明化手段增强了用户的信任和满意度，使得在保护用户隐私的同时也能保证模型的有效性。"}}
{"id": "2512.07437", "pdf": "https://arxiv.org/pdf/2512.07437", "abs": "https://arxiv.org/abs/2512.07437", "authors": ["Chenwei Shi", "Xueyu Luan"], "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "comment": "23 pages, 8 figures, 3 tables", "summary": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.", "AI": {"tldr": "该论文介绍了将KANs集成到DreamerV3框架中的方法，以提高参数效率和可解释性。", "motivation": "研究旨在通过引入Kolmogorov-Arnold Networks (KANs)来改进现有的MLP架构，在保持或提升性能的同时减少参数数量并提高模型的透明度。", "method": "提出了使用KAN替换DreamerV3中特定组件的方法，并在JAX世界模型中实现了优化版本。研究分为了三个子系统：视觉感知，潜在预测和行为学习。", "result": "实验结果表明，在奖励和继续性预测方面采用FastKAN作为MLP的替代品后，性能与原始架构保持一致，同时提高了参数效率。", "conclusion": "该报告为未来基于KAN的世界模型开发提供了一个初步研究的基础。"}}
{"id": "2512.07436", "pdf": "https://arxiv.org/pdf/2512.07436", "abs": "https://arxiv.org/abs/2512.07436", "authors": ["Hang He", "Chuhuai Yue", "Chengqi Dong", "Mingxue Tian", "Zhenfeng Liu", "Jiajun Chai", "Xiaohan Wang", "Yufei Zhang", "Qun Liao", "Guojun Yin", "Wei Lin", "Chengcheng Wan", "Haiying Sun", "Ting Su"], "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large reasoning models (LRMs) have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench includes over 150,000 high-quality entries from various cities and business types. We construct 300 multi-hop QA tasks based on real user queries, challenging agents to understand questions and retrieve information in multiple steps. We also developed LocalPlayground, a unified environment integrating multiple tools for agent interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.1) achieves only 34.34% correctness, and most models have issues with completeness (average 77.33%) and faithfulness (average 61.99%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at localsearchbench.github.io.", "AI": {"tldr": "本文介绍了LocalSearchBench，这是一个针对本地生活服务领域的综合性代理搜索基准测试。", "motivation": "大型推理模型（LRMs）在复杂多步骤推理方面取得了进展，但很少有研究关注具有独特挑战性的垂直领域。本文旨在填补这一空白，特别是在本地生活服务领域，该领域的查询通常模糊且需要跨商家和产品的多层次推理。", "method": "构建了LocalSearchBench，包含超过150,000条高质量条目，涵盖多个城市和地区业务类型。基于真实用户查询构造了300个问答任务，测试代理理解和信息检索的能力。此外还开发了LocalPlayground，一个集成了多种工具的统一环境。", "result": "实验表明，即使是最先进的LRMs在LocalSearchBench上也表现不佳：最佳模型（DeepSeek-V3.1）仅达到34.34%的准确性，并且大多数模型存在完整性问题和忠实度不足的问题。这凸显了专门基准测试和领域特定代理训练的重要性。", "conclusion": "本文展示了LocalSearchBench作为首个全面的本地生活服务领域代理搜索基准的重要性，揭示了大型推理模型在该领域的局限性，强调了对这些挑战进行专项研究的需求。"}}
{"id": "2512.07430", "pdf": "https://arxiv.org/pdf/2512.07430", "abs": "https://arxiv.org/abs/2512.07430", "authors": ["Yangle Li", "Danli Luo", "Haifeng Hu"], "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.", "AI": {"tldr": "本文提出了一种新的多模态情感分析框架MIDG，用于领域泛化。", "motivation": "现有的多模态情感分析领域的通用方法往往忽视了在不变特征提取过程中的跨模式协同作用，并且知识注入技术也存在跨模态知识片段化的缺点。", "method": "该模型引入了一个混合不变专家模型来提取域不变特征，还设计了一种跨模态适配器以增强多模态表示的语义丰富性。", "result": "在三个数据集上的广泛领域实验表明，所提出的MIDG框架优于现有方法。", "conclusion": "该研究提出了一种新的多模态情感分析框架，通过改进不变特征提取和跨模态知识注入，在多域泛化方面取得了显著的性能提升。"}}
{"id": "2512.07426", "pdf": "https://arxiv.org/pdf/2512.07426", "abs": "https://arxiv.org/abs/2512.07426", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, accepted for oral presentation at SPIE Medical Imaging, 2026", "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "AI": {"tldr": "该论文探讨了WSI归一化过程中可能出现的幻觉问题，并提出了一种检测这些幻觉的新方法。", "motivation": "现有WSI归一化模型可能引入难以察觉的真实感伪影，对下游分析构成威胁。当前评估实践往往忽视这些问题，因此作者希望通过研究揭示这一风险并寻找解决方案。", "method": "论文提出了一个新的图像比较指标来自动检测归一化输出中的幻觉，并使用此方法系统地评估了几种常用归一化方法在真实临床数据上的表现。", "result": "所提出的方法发现了显著的不一致和失败情况，这些问题未能通过常规度量标准被捕捉到。", "conclusion": "研究结果强调了开发更稳健、可解释性更强的归一化技术以及严格的验证协议对于WSI处理在临床应用中的重要性。"}}
{"id": "2512.07419", "pdf": "https://arxiv.org/pdf/2512.07419", "abs": "https://arxiv.org/abs/2512.07419", "authors": ["Haidong Kang", "Jun Du", "Lihong Lin"], "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.", "AI": {"tldr": "提出了一种基于大语言模型的无训练自动代理发现框架，用于混合精度量化。", "motivation": "解决传统方法在混合精度量化过程中效率低、灵活性差和依赖专家知识的问题。", "method": "利用大语言模型结合直接策略优化增强的强化学习来寻找最优的自动代理。", "result": "实验表明该方法达到了最先进的性能。", "conclusion": "该研究为混合精度量化提供了一种新的视角，即基于大语言模型的设计算法。"}}
{"id": "2512.07415", "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "该论文通过数据挖掘的方法来探索个体之间的移动性交互模式。", "motivation": "理解个人的运动行为以及他们对外部环境的反应是模拟人类动态的关键组成部分。现有的解决方案通常基于预设的行为模型，而本研究旨在直接从数据中发现可能的互动作证据并寻找复杂的、持久的行为模式。", "method": "该方法通过在真实案例研究（包括车辆和行人）的数据集中搜索移动性事件，并在此基础上找寻复杂且持续存在的交互行为模式。", "result": "实验评估显示，所提出的方法能够有效识别个体间的移动性和时间演变的配置情况，有助于提高现有的模拟模型。", "conclusion": "通过直接从数据中发现并分析个体之间的运动交互模式，可以为改进现有的仿真模型提供新的见解。"}}
{"id": "2512.07410", "pdf": "https://arxiv.org/pdf/2512.07410", "abs": "https://arxiv.org/abs/2512.07410", "authors": ["Bin Li", "Ruichi Zhang", "Han Liang", "Jingyan Zhang", "Juze Zhang", "Xin Chen", "Lan Xu", "Jingyi Yu", "Jingya Wang"], "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs", "categories": ["cs.CV"], "comment": "Project page: https://binlee26.github.io/InterAgent-Page", "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.", "AI": {"tldr": "提出了一种基于物理的多智能体人形控制框架InterAgent，该框架能够通过文本指令生成连贯、物理上合理和语义忠实的多智能体行为。", "motivation": "现有的方法主要局限于单个代理场景，忽视了在多代理交互中至关重要的物理上的合理性。为了填补这一空白，作者提出了一种新的基于文本驱动的物理多代理人形控制框架。", "method": "引入了一种自回归扩散变压器，并配备有多个流块来解耦本体感觉、外周感觉和动作以减少跨模态干扰并促进协同协调；提出了一个新颖的交互图外周感觉表示，显式捕捉关节到关节的空间依赖关系，从而促进了网络学习。", "result": "实验结果表明，InterAgent在多智能体控制任务中优于多个强大的基线方法，并达到了最先进的性能水平。", "conclusion": "InterAgent能够从文本指令生成连贯、物理上合理和语义忠实的多智能体行为，为未来的研究提供了一个有效的工具。"}}
{"id": "2512.07404", "pdf": "https://arxiv.org/pdf/2512.07404", "abs": "https://arxiv.org/abs/2512.07404", "authors": ["Francisco Ribeiro", "Claudio Spiess", "Prem Devanbu", "Sarah Nadi"], "title": "Do LLMs Trust the Code They Write?", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.", "AI": {"tldr": "本文研究了大型语言模型在生成代码时内部是否编码了正确性信号，并探索如何利用这一信号提高生成代码的质量。", "motivation": "尽管大型语言模型在代码生成方面非常有效，但它们经常输出错误的代码。原因是模型输出的概率通常与正确性不相关，且仅反映了最终生成的结果。因此，本文研究LLM内部是否编码了正确的信号，以提升代码生成质量。", "method": "通过对比四个LLM中相同编程任务下正确和错误代码的隐藏状态，识别出一个正确性的表示。实验表明，利用这种提取到的正确性信号优于标准对数似然排名和模型表达的信心。", "result": "本文发现，通过内部正确性信号可以提高选择高质量代码样本的能力，并且不需要执行测试，从而提升了自动代码生成系统的可靠性。", "conclusion": "最终证明了利用LLM中的内部表示可以增强代码生成系统，使自动生成的代码更加可靠。"}}
{"id": "2512.07400", "pdf": "https://arxiv.org/pdf/2512.07400", "abs": "https://arxiv.org/abs/2512.07400", "authors": ["Giulia Lanzillotta", "Damiano Meier", "Thomas Hofmann"], "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "AI": {"tldr": "论文探讨了连续学习中浅层和深层遗忘的渐近分析，通过扩展Neural Collapse框架来解释经验回放中的关键不对称性。", "motivation": "解决持续学习中神经网络即使输出预测失败也会保留过去任务线性可分表示的问题，揭示经验回放在浅层和深层遗忘之间的差异及其原因。", "method": "将Neural Collapse框架扩展到顺序设置下，分析了浅层和深层遗忘的几何特性，并证明了一定比例的经验回放可以保证特征空间中的线性分离。", "result": "发现小型缓冲区会导致分类器失效，而大型缓冲区才能有效缓解浅层遗忘。提出了解决统计偏差的方法以提高性能。", "conclusion": "通过将CL与离分布检测相结合，挑战了依赖大量回放的经验方法，并建议采用纠正统计偏误的策略来实现稳健的小型回放学习。"}}
{"id": "2512.07397", "pdf": "https://arxiv.org/pdf/2512.07397", "abs": "https://arxiv.org/abs/2512.07397", "authors": ["Ali Joundi", "Yann Traonmilin", "Jean-François Aujol"], "title": "From sparse recovery to plug-and-play priors, understanding trade-offs for stable recovery with generalized projected gradient descent", "categories": ["eess.IV", "cs.NE", "math.OC"], "comment": null, "summary": "We consider the problem of recovering an unknown low-dimensional vector from noisy, underdetermined observations. We focus on the Generalized Projected Gradient Descent (GPGD) framework, which unifies traditional sparse recovery methods and modern approaches using learned deep projective priors. We extend previous convergence results to robustness to model and projection errors. We use these theoretical results to explore ways to better control stability and robustness constants. To reduce recovery errors due to measurement noise, we consider generalized back-projection strategies to adapt GPGD to structured noise, such as sparse outliers. To improve the stability of GPGD, we propose a normalized idempotent regularization for the learning of deep projective priors. We provide numerical experiments in the context of sparse recovery and image inverse problems, highlighting the trade-offs between identifiability and stability that can be achieved with such methods.", "AI": {"tldr": "本文研究了从嘈杂、欠定观测中恢复未知低维向量的问题，探讨广义投影梯度下降（GPGD）框架下的稳定性与鲁棒性。", "motivation": "通过统一传统稀疏恢复方法和现代深度投影先验学习的方法，提高在模型误差和投影误差存在的情况下恢复的稳健性和稳定性。", "method": "提出了广义反向投影策略以适应结构化噪声，并提出规范化幂等正则化来改善GPGD的学习过程。进行了数值实验以验证这些策略的有效性。", "result": "结果表明，通过提出的策略可以有效降低测量噪音引起的恢复误差，提高深度项目先验学习的稳定性。", "conclusion": "本文展示了如何利用理论分析和实验手段在稀疏恢复与图像逆问题中平衡标识性和稳定性的方法。"}}
{"id": "2512.07394", "pdf": "https://arxiv.org/pdf/2512.07394", "abs": "https://arxiv.org/abs/2512.07394", "authors": ["Zhifan Zhu", "Siddhant Bansal", "Shashank Tripathi", "Dima Damen"], "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "categories": ["cs.CV"], "comment": "webpage: https://zhifanzhu.github.io/objects-along-hit", "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "AI": {"tldr": "重建手部交互时间线中的对象姿态，利用约束优化和传播框架（COP）提高物体在稳定抓握下的三维重构精度。", "motivation": "提出一种新的任务ROHIT，旨在通过分析从刚性物体视角定义的手部交互时间线来实现更准确的物体重建。这种方法可以在没有3D地面实况的情况下高效地标记、研究和评估视频中的对象重建。", "method": "提出约束优化与传播框架（COP），该方法通过在手部交互时间线上传播物体姿态，实现了对稳定抓握状态下对象的姿态重构。", "result": "在HOT3D和EPIC-Kitchens数据集上进行定量评估，结果显示COP提高了6.2-11.3%的稳定抓握重建精度，并且将手部交互时间线（HIT）的重建提高到最多24.5%，从而实现了更准确的姿态重构。", "conclusion": "ROHIT任务和COP框架为在没有三维地面实况的情况下，实现视频中对象姿态的有效标注、研究及评估提供了一种新方法。"}}
{"id": "2512.07391", "pdf": "https://arxiv.org/pdf/2512.07391", "abs": "https://arxiv.org/abs/2512.07391", "authors": ["Đorđe Nedeljković"], "title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.", "AI": {"tldr": "提出GlimmerNet，一种轻量级的基于无人机紧急监控任务的卷积神经网络。", "motivation": "展示如何在不依赖计算昂贵组件的情况下保持强大的全局感知能力。", "method": "引入分组膨胀深度可分离卷积（GDBlocks）和聚合模块，实现多尺度特征提取并有效融合跨组表示。", "result": "使用31K参数和比最新基线少29%的FLOPs，在UAV-focused AIDERv2数据集上达到0.966的新加权F1分数。", "conclusion": "GlimmerNet建立了新的精度-效率权衡前沿，适用于资源受限无人机平台上的实时紧急监控任务。"}}
{"id": "2512.07390", "pdf": "https://arxiv.org/pdf/2512.07390", "abs": "https://arxiv.org/abs/2512.07390", "authors": ["Gilhyun Nam", "Taewon Kim", "Joonhyun Jeong", "Eunho Yang"], "title": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted to WACV 2026", "summary": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.", "AI": {"tldr": "该论文提出了Style Invariance as a Correctness Likelihood (SICL)框架，用于提高模型在测试时间适应中的不确定性估计准确性。", "motivation": "现有的校准方法假设固定的模型或静态分布，在实际动态的测试环境中表现不佳。作者旨在通过风格不变性来解决这种问题，以提供更可靠的预测不确定性估计。", "method": "SICL框架通过测量样式改变后的预测一致性来估计实例级别的正确性概率，仅需要前向传递即可实现。", "result": "在四个基线、五种TTA方法以及两种真实场景中的评估表明，SICL能够将校准误差平均降低13个百分点。", "conclusion": "论文提出的方法可以在不增加额外训练成本的情况下改善模型的不确定性估计能力，并适用于各种测试时间适应策略。"}}
{"id": "2512.07388", "pdf": "https://arxiv.org/pdf/2512.07388", "abs": "https://arxiv.org/abs/2512.07388", "authors": ["Remi Poivet", "Catherine Pelachaud", "Malika Auvray"], "title": "Breaking Players' expectations: the Role of Non-player Characters' coherence and Consistency", "categories": ["cs.HC"], "comment": null, "summary": "In video games, non-player characters (NPCs) play a pivotal role in shaping players' experiences. The design of these characters, encompassing their appearance and behaviors, can be manipulated in terms of coherence and consistency to maintain players' expectations or, on the contrary, to surprise them. The extent to which NPCs' coherence and consistency influence players' evaluation of them remains to be unveiled. To address this knowledge gap, two experiments were conducted in the context of a military shooter game. Players' evaluations of NPCs' perceived intelligence and believability were measured, as these two dimensions are fundamental to players' adoption of NPCs and subsequent commitment to them. The first experiment investigated the impact of disrupting players' initial expectations on their evaluations of NPCs. The second experiment focused on the influence of NPCs' coherence and consistency on both players' expectations and evaluation of NPCs, using a combination of questionnaires and behavioral and physiological measures. The results of our study show that disrupting players' initial expectations influences their assessment of NPCs, with coherent and consistent design reinforcing expectations and incoherent design challenging them.", "AI": {"tldr": "研究探讨了非玩家角色（NPC）的连贯性和一致性如何影响玩家对他们的评价。", "motivation": "揭示NPC设计中连贯性和一致性对其感知智能和可信度的影响，从而改善游戏体验。", "method": "通过一个军事射击游戏中两组实验来测量玩家对NPC的认知评估：第一组探究打破初始预期的影响；第二组考察NPC的连贯性与一致性如何影响玩家期望及其评价。", "result": "结果显示打破玩家预期会影响他们对NPC的评估，而设计的连贯性和一致性则会强化或挑战这些预期。", "conclusion": "NPC的设计可以通过调节其连贯性和一致性来更好地引导或颠覆玩家的期望，从而增强游戏体验。"}}
{"id": "2512.07385", "pdf": "https://arxiv.org/pdf/2512.07385", "abs": "https://arxiv.org/abs/2512.07385", "authors": ["Chunhui Zhang", "Li Liu", "Zhipeng Zhang", "Yong Wang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline", "categories": ["cs.CV"], "comment": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.", "AI": {"tldr": "本文提出了一种新的多模态视觉跟踪任务，即UAV-Anti-UAV，并构建了一个包含1810个视频的百万规模数据集。", "motivation": "当前反无人机研究主要集中在固定地面摄像机捕获的RGB、红外或RGB-IR视频上，而忽略了从移动平台进行目标追踪的任务。本文旨在填补这一空白，提出了一种新的挑战性任务以促进相关领域的研究。", "method": "提出了MambaSTS基线方法，采用Mamba和Transformer模型分别学习全局语义和空间特征，并利用状态空间模型的优势通过时间令牌传播机制建立视频级别的长期上下文。", "result": "实验评估了50种现代深度跟踪算法的有效性，表明在UAV-Anti-UAV领域仍存在很大的改进空间。", "conclusion": "本文提出了新的挑战性任务和大规模数据集，并提供了一种有效的基线方法，证明现有技术在处理移动平台上的目标追踪方面仍有较大提升潜力。"}}
{"id": "2512.07383", "pdf": "https://arxiv.org/pdf/2512.07383", "abs": "https://arxiv.org/abs/2512.07383", "authors": ["Deepika SN Vemuri", "Gautham Bellamkonda", "Aditya Pola", "Vineeth N Balasubramanian"], "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning", "categories": ["cs.CV"], "comment": "18 pages, 19 figures, WACV 2026", "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.", "AI": {"tldr": "逻辑增强的概念瓶颈模型（LogicCBM）通过可微逻辑运算将从概念瓶颈模型中学习到的语义概念进行连接，从而超越简单的加权组合来生成最终预测。", "motivation": "现有概念瓶颈模型主要关注于解释性方面，并且通常限制在使用线性组合。为了进一步提升这些模型的能力和表达力，引入了逻辑模块以利用多种逻辑操作增强概念之间的关系。", "method": "提出了一种新的逻辑模块，该模块通过可微分的逻辑运算将从CBM中学习到的概念连接起来，并用这种方法来生成最终预测结果。", "result": "在知名基准测试和合成数据集上的实验表明，这些模型具有更高的准确性和干预效果，并且保持高度可解释性。", "conclusion": "通过引入逻辑操作增强概念瓶颈模型的方法可以有效提高模型的表达力、准确性以及可解释性。"}}
{"id": "2512.07381", "pdf": "https://arxiv.org/pdf/2512.07381", "abs": "https://arxiv.org/abs/2512.07381", "authors": ["Shuohan Tao", "Boyao Zhou", "Hanzhang Tu", "Yuwang Wang", "Yebin Liu"], "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "AI": {"tldr": "提出Tessellation GS方法，通过在网格面上约束和推断二维高斯分布来实现单目动态场景的鲁棒重建。", "motivation": "现有的三维Gaussian Splatting方法由于其各向异性特性，在稀疏视图和动态场景重建中存在过拟合问题，导致视角外插性能差，提出的方法旨在解决这些问题。", "method": "该方法基于网格面约束二维高斯分布，并使用分层神经特征推断这些分布的属性；通过自适应网格细分策略和细节感知损失函数指导高斯细分；利用重建基础模型提供的先验信息初始化高斯变形。", "result": "该方法在外观和网格重建任务上优于现有SOTA方法，LPIPS降低29.1%，Chamfer距离减少49.2%。", "conclusion": "Tessellation GS能够从单个静态或移动相机中鲁棒地重建一般动态对象，解决了优化方法难以实现的问题。"}}
{"id": "2512.07379", "pdf": "https://arxiv.org/pdf/2512.07379", "abs": "https://arxiv.org/abs/2512.07379", "authors": ["Mahila Moghadami", "Mohammad Ali Keyvanrad", "Melika Sabaghian"], "title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency", "categories": ["cs.CV"], "comment": "22 pages, 16 figures", "summary": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.", "AI": {"tldr": "本论文提出了一种增强YOLO模型的小目标检测框架，通过改进滑动窗口技术和架构修改来提高准确性和效率。", "motivation": "为了在大规模航空图像中更有效地进行小目标检测，现有的方法通常涉及裁剪和对网络结构的调整。随着航空影像在工业应用中的重要性日益增加，需要一个稳健的小目标检测框架。", "method": "研究采用了SW-YOLO为基础模型，并通过改进滑动窗口的尺寸及重叠度来优化速度与精度；此外还通过对架构进行修改引入先进的特征提取模块、CBAM注意机制以及新的头部结构以进一步提升小目标检测准确性。", "result": "该方法在VisDrone2019数据集上取得了显著的准确率提升，从YOLOv5L基础模型的35.5%提高到61.2%，超越了其他经典算法如CZDet（58.36%）。", "conclusion": "研究展示了在大规模航空图像中小目标检测的重要性和改进潜力，通过特定的方法设计和优化实现了显著的性能提升。"}}
{"id": "2512.07371", "pdf": "https://arxiv.org/pdf/2512.07371", "abs": "https://arxiv.org/abs/2512.07371", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": "project page: https://project-espada.github.io/espada/", "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "AI": {"tldr": "本文提出了ESPADA框架，通过语义感知的示例数据降采样加速模仿学习。", "motivation": "行为克隆基线方法在继承人类演示动作时过于缓慢和谨慎，限制了实际部署。现有加速技术忽视任务语义，在多样场景中表现不佳。", "method": "ESPADA利用VLM-LLM管道及3D抓取器-物体关系分段展示，并使用动态时间扭曲传播标签以实现非关键阶段的激进降采样和精确控制保持。", "result": "在模拟和真实世界实验中，与ACT、DP基线相比，ESPADA实现了大约2倍的速度提升，同时维持成功率。", "conclusion": "ESPADA通过语义感知分段展示技术提高了模仿学习效率，并缩小了人类演示动作与高效机器人控制之间的差距。"}}
{"id": "2512.07363", "pdf": "https://arxiv.org/pdf/2512.07363", "abs": "https://arxiv.org/abs/2512.07363", "authors": ["Michael Stern", "Maurizio Vergari", "Julia Schorlemmer", "Francesco Vona", "David Grieshammer", "Jan-Niklas Voigt-Antons"], "title": "The Impact of Spatial Misalignment and Time Delay on Collaborative Presence in Augmented Reality", "categories": ["cs.HC"], "comment": null, "summary": "Precise temporal and spatial alignment is critical in collaborative Augmented Reality (AR) where users rely on shared visual information to coordinate actions. System latency and object misalignment can disrupt communication, reduce task efficiency, and negatively impact the overall user experience. While previous research has primarily focused on individual AR interactions, the impact of these inconsistencies on collaboration remains underexplored. This article investigates how user experience and task load are affected by object misalignment and time delay in a shared AR space. To examine these factors, we conducted an experiment with 32 participants, organized into 16 pairs, who collaboratively completed a spatial placement task. Within each condition, both participants alternated roles, taking turns as the leader-providing verbal placement instructions-and the builder-executing the placement. Six conditions were tested, manipulating object alignment (perfectly aligned vs. randomly misaligned) and time delay (0s, 0.1s, 0.4s). The misalignment was applied randomly to each virtual object with a shift of +-20 cm on every axis to create a clear distinction in spatial perception. User experience and task load were assessed to evaluate how these factors influence collaboration and interaction in AR environments. Results showed that spatial misalignment significantly increased perceived workload (NASA-TLX) and lowered user ratings in Pragmatic quality and Attractiveness (UEQ), while time delay had a more limited effect. These findings highlight the critical role of spatial accuracy in maintaining collaboration quality in AR.", "AI": {"tldr": "研究探讨了在协作增强现实（AR）环境中，物体错位和时间延迟对用户体验及任务负荷的影响。", "motivation": "精确的时间和空间一致性对于依赖共享视觉信息来协调动作的协作AR至关重要。系统滞后和对象不一致可以破坏通信、降低工作效率并损害整体用户体验。", "method": "通过实验研究了32名参与者组成的16个小组，他们完成了一项共同放置任务，并交替担任领导者（提供口头指令）和建造者（执行放置）。六种条件被测试，分别操纵物体对齐方式（完全一致与随机错位）和时间延迟（0秒、0.1秒和0.4秒）。结果通过NASA-TLX评估工作负荷以及UEQ中的实用性和吸引力来衡量。", "result": "结果显示，空间不一致性显著增加了感知的工作负荷，并降低了用户在实用性和吸引力方面的评分；而时间延迟的影响较小。", "conclusion": "这些发现强调了在AR中保持协作质量的关键在于空间准确性。"}}
{"id": "2512.07361", "pdf": "https://arxiv.org/pdf/2512.07361", "abs": "https://arxiv.org/abs/2512.07361", "authors": ["Giuseppe Leo", "Paolo Gibertini", "Irem Ilter", "Erika Covi", "Ole Richter", "Elisabetta Chicca"], "title": "An Asynchronous Mixed-Signal Resonate-and-Fire Neuron", "categories": ["eess.SP", "cs.NE"], "comment": null, "summary": "Analog computing at the edge is an emerging strategy to limit data storage and transmission requirements, as well as energy consumption, and its practical implementation is in its initial stages of development. Translating properties of biological neurons into hardware offers a pathway towards low-power, real-time edge processing. Specifically, resonator neurons offer selectivity to specific frequencies as a potential solution for temporal signal processing. Here, we show a fabricated Complementary Metal-Oxide-Semiconductor (CMOS) mixed-signal Resonate-and-Fire (R&F) neuron circuit implementation that emulates the behavior of these neural cells responsible for controlling oscillations within the central nervous system. We integrate the design with asynchronous handshake capabilities, perform comprehensive variability analyses, and characterize its frequency detection functionality. Our results demonstrate the feasibility of large-scale integration within neuromorphic systems, thereby advancing the exploitation of bio-inspired circuits for efficient edge temporal signal processing.", "AI": {"tldr": "本文实现了一种仿生神经元电路，该电路可以在边缘进行低功耗、实时信号处理。", "motivation": "通过将生物神经元的特性转化为硬件设计来限制数据存储和传输的需求以及降低能量消耗，从而开发一种新的计算策略。", "method": "制造了一种CMOS混合信号共振放电（Resonate-and-Fire）神经元电路，并将其与异步握手功能集成。", "result": "证明了大规模集成在类脑系统中的可行性并展示了其频率检测功能。", "conclusion": "该研究推进了生物启发式电路在边缘实时信号处理领域的应用。"}}
{"id": "2512.07360", "pdf": "https://arxiv.org/pdf/2512.07360", "abs": "https://arxiv.org/abs/2512.07360", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV2026", "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "AI": {"tldr": "本文提出了一种结构感知的特征校正方法，以改进开放词汇语义分割中的细粒度视觉区域与文本关联问题。", "motivation": "CLIP模型在图像-文本对上的预训练导致其偏向于全局语义匹配，这影响了它对局部区域的理解和预测准确性。", "method": "通过构建基于低级特征的区域邻接图（RAG）来捕获局部结构关系，并利用该图校正CLIP特征以增强局部区分能力。", "result": "实验表明，该方法显著减少了分割噪声，提升了区域一致性，在多个开放词汇语义分割基准测试中表现出色。", "conclusion": "提出的结构感知特征校正技术能够有效改善基于CLIP的开放词汇语义分割性能。"}}
{"id": "2512.07359", "pdf": "https://arxiv.org/pdf/2512.07359", "abs": "https://arxiv.org/abs/2512.07359", "authors": ["Bin Zhao", "Yiwen Lu", "Haohua Zhu", "Xiao Li", "Sheng Yi"], "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "categories": ["cs.RO", "cs.GR"], "comment": "10 pages, 4 figures. Accepted at ICBSR'25 (International Conference on Biomechanical Systems and Robotics)", "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "AI": {"tldr": "本文提出了一种通过光学运动捕捉数据创建多刚体手模型的方法，该方法可以在保持真实外观的同时实现实时物理仿真。", "motivation": "在数字孪生应用中，需要一种能够在解剖学精确度和计算效率之间取得平衡的手部模拟模型。为了满足这一需求，本文开发了一种多刚体手部建模方法。", "method": "基于光学运动捕捉数据构建个性化MANO（多抽象手模型）并将其转换为URDF表示形式，并通过投影约束解决旋转问题。对于单自由度关节采用闭式解法；而对于双自由度关节，引入BCH校正的迭代算法以处理旋转的非交换性。", "result": "实验结果表明，该方法在数字孪生中的手部重建误差低于1厘米，并能够成功执行各种抓取任务。", "conclusion": "本文的方法不仅提高了模拟的真实性和效率，还为数字孪生应用提供了重要的技术支持。"}}
{"id": "2512.07357", "pdf": "https://arxiv.org/pdf/2512.07357", "abs": "https://arxiv.org/abs/2512.07357", "authors": ["Navid Ashrafi", "Francesco Vona", "Sina Hinzmann", "Juliane Henning", "Maurizio Vergari", "Maximilian Warsinke", "Catarina Pinto Moreira", "Jan-Niklas Voigt-Antons"], "title": "Size Matters: The Impact of Avatar Size on User Experience in Healthcare Applications", "categories": ["cs.HC"], "comment": "7 pages, 3 figures", "summary": "The usage of virtual avatars in healthcare applications has become widely popular; however, certain critical aspects, such as social distancing and avatar size, remain insufficiently explored. This research investigates user experience and preferences when interacting with a healthcare application utilizing virtual avatars displayed in different sizes. For our study, we had 23 participants interacting with five different avatars (a human-size avatar followed by four smaller avatars in a randomized order) varying in size, projected on a wall in front of them. The avatars were fully integrated with an artificial intelligence chatbot to make them conversational. Users were asked to rate the usability of the system after interacting with each avatar and complete a survey regarding trust and an additional questionnaire on social presence. The results of this study show that avatar size significantly influences the perceived attractiveness and perspicuity, with the medium-sized avatars receiving the highest ratings. Social presence correlated strongly with stimulation and attractiveness, suggesting that an avatar's visual appeal and interactivity influenced user engagement more than its physical size. Additionally, we observed a tendency for gender-specific differences on some of the UEQ+ scales, with male participants tending to prefer human-sized representations, while female participants slightly favored smaller avatars. These findings highlight the importance of avatar design and representation in optimizing user experience and trust in virtual healthcare environments.", "AI": {"tldr": "研究探讨了不同大小的虚拟化身在医疗应用中的用户体验和偏好。", "motivation": "探索虚拟化身大小对用户信任、交互性和体验的影响，以优化虚拟医疗服务环境。", "method": "23名参与者与五个不同大小的虚拟化身互动，并对其进行了可用性评分和社会临场感调查。", "result": "中等大小的化身获得了最高的吸引力和清晰度评价；性别差异表明男性倾向于偏好人形大小，女性则略微偏爱较小的化身。", "conclusion": "化身设计对用户体验和信任至关重要，优化视觉吸引力和交互性比单纯调整物理尺寸更为重要。"}}
{"id": "2512.07355", "pdf": "https://arxiv.org/pdf/2512.07355", "abs": "https://arxiv.org/abs/2512.07355", "authors": ["Alexandre Rocchi--Henry", "Thomas Fel", "Gianni Franchi"], "title": "A Geometric Unification of Concept Learning with Concept Cones", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages", "summary": "Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\\footnote{We adopt the terminology of \\citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.", "AI": {"tldr": "本文提出了一种几何框架，将概念瓶颈模型和稀疏自动编码器统一起来。", "motivation": "为了理解监督学习与无监督学习之间的关系，并提供一种衡量稀疏自动编码器发现的概念与人类定义的概念的吻合度的方法。", "method": "通过分析两种方法中的线性方向组合，建立了概念锥的几何结构。使用这种结构，可以量化不同参数设置对概念提取的影响。", "result": "发现了稀疏性和扩展因子的最佳范围，以实现几何和语义上的最佳匹配。", "conclusion": "本文提出了一种将监督学习与无监督学习统一起来的框架，并为评估发现的概念提供了有理论依据的标准。"}}
{"id": "2512.07352", "pdf": "https://arxiv.org/pdf/2512.07352", "abs": "https://arxiv.org/abs/2512.07352", "authors": ["Xueping Zhang", "Zhenshan Zhang", "Yechen Wang", "Linxi Li", "Liwei Jin", "Ming Li"], "title": "MultiAPI Spoof: A Multi-API Dataset and Local-Attention Network for Speech Anti-spoofing Detection", "categories": ["cs.SD"], "comment": null, "summary": "Existing speech anti-spoofing benchmarks rely on a narrow set of public models, creating a substantial gap from real-world scenarios in which commercial systems employ diverse, often proprietary APIs. To address this issue, we introduce MultiAPI Spoof, a multi-API audio anti-spoofing dataset comprising about 230 hours of synthetic speech generated by 30 distinct APIs, including commercial services, open-source models, and online platforms. Based on this dataset, we define the API tracing task, enabling fine-grained attribution of spoofed audio to its generation source. We further propose Nes2Net-LA, a local-attention enhanced variant of Nes2Net that improves local context modeling and fine-grained spoofing feature extraction. Experiments show that Nes2Net-LA achieves state-of-the-art performance and offers superior robustness, particularly under diverse and unseen spoofing conditions. Code \\footnote{https://github.com/XuepingZhang/MultiAPI-Spoof} and dataset \\footnote{https://xuepingzhang.github.io/MultiAPI-Spoof-Dataset/} have released.", "AI": {"tldr": "本文提出了一种多API音频防伪数据集和改进的Nes2Net-LA网络，用于提升语音防伪检测性能。", "motivation": "现有的语音防伪基准测试依赖于狭窄的一组公共模型，与使用多种多样且通常是专有API的真实世界场景之间存在较大差距。本文旨在通过创建一个包含多源合成语音的数据集来解决这一问题，并提出一种改进的网络结构以提升检测性能。", "method": "基于所提出的MultiAPI Spoof数据集，定义了API追踪任务，并提出了Nes2Net-LA模型，这是一种增强了局部注意力机制的变种，能够更好地建模局部上下文并提取细粒度的防伪特征。", "result": "实验表明，Nes2Net-LA在多源和未知条件下的表现优于现有方法，达到了最先进的性能水平。", "conclusion": "本文通过提出一个多API音频数据集以及改进的模型结构解决了语音防伪检测中的挑战性问题，并证明了新方法的有效性和鲁棒性。"}}
{"id": "2512.07351", "pdf": "https://arxiv.org/pdf/2512.07351", "abs": "https://arxiv.org/abs/2512.07351", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": null, "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "AI": {"tldr": "本文提出了DeepAgent，一种基于多代理协作框架的深度伪造检测方法，通过结合视觉和音频模态来提高检测准确性。", "motivation": "针对现有单一模型在处理合成媒体时易受模态不匹配、噪音和操纵影响的问题，提出了一种新的多代理融合方法以增强鲁棒性。", "method": "DeepAgent包含两个互补的代理：Agent-1使用简化版AlexNet CNN进行视觉分析；Agent-2结合音频特征、Whisper生成的文字转录和EasyOCR读取的画面序列识别音频视频不一致。最终，通过随机森林元分类器融合两者的决策。", "result": "在Celeb-DF和FakeAVCeleb数据集的组合测试中，Agent-1的准确率为94.35%，而Agent-2与元分类器分别达到了93.69%和81.56%。通过DeepFakeTIMIT进行跨数据集验证时，最终准确率达到97.49%，表明该方法具有较好的泛化能力。", "conclusion": "研究表明层次化的融合方式能够有效缓解单一模态的弱点，并证明了多代理方法在处理多样化伪造类型中的有效性。"}}
{"id": "2512.07348", "pdf": "https://arxiv.org/pdf/2512.07348", "abs": "https://arxiv.org/abs/2512.07348", "authors": ["Xinyu Wei", "Kangrui Cen", "Hongyang Wei", "Zhen Guo", "Bairui Li", "Zeqing Wang", "Jinrui Zhang", "Lei Zhang"], "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition", "categories": ["cs.CV"], "comment": "Project Page: https://MICo-150K.github.io/", "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.", "AI": {"tldr": "MICo-150K是一个用于多图像合成（Multi-Image Composition，MICo）的大型数据集。", "motivation": "目前在可控图像生成中，特别是在多参考输入的图像合成上存在不足，主要是由于缺乏高质量的数据集。为此，研究者提出了一套系统性的方法来解决这一问题，并创建了包含各种任务和复杂场景的数据集。", "method": "通过构建一系列代表性任务分类、收集高质量源图、设计多样化的MICo提示以及利用强大的模型进行合成及人工筛选与优化，最终形成一个综合型数据集。同时开发了一个评估基准平台和新的评价指标。", "result": "实验表明，MICo-150K能够有效提升图像编辑模型的性能，并且新提出的基线模型在特定任务上取得了与其他先进模型相当甚至更好的表现。", "conclusion": "该研究通过建立MICo-150K数据集、构建评估基准以及引入新的评价指标，为多图像合成领域的进一步探索提供了有价值的资源。"}}
{"id": "2512.07345", "pdf": "https://arxiv.org/pdf/2512.07345", "abs": "https://arxiv.org/abs/2512.07345", "authors": ["Shilong Jin", "Haoran Duan", "Litao Hua", "Wentao Huang", "Yuan Zhou"], "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, 8 figures, 5 tables, 2 algorithms, Accepted by AAAI 2026", "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "AI": {"tldr": "提出了一种新的框架TD-Attn，用于解决基于T2I扩散模型的多视角不一致性问题。", "motivation": "文本到图像(T2I)扩散模型存在先验视图偏差，导致生成的不同视角对象外观冲突。需要一种方法来克服这一限制，提升三维任务的一致性。", "method": "通过数学分析揭示了T2I模型中先验视图偏差的根本原因，并提出TD-Attn框架，包括3D-Aware Attention Guidance Module和Hierarchical Attention Modulation Module两个关键组件，以解决多视角一致性问题。", "result": "实验结果表明，TD-Attn能够显著提高基于扩散模型的三维任务中的多视角一致性。", "conclusion": "TD-Attn框架展示了其作为通用插件的能力，有效解决了T2I扩散模型中由于先验视图偏差引起的多视角不一致问题。"}}
{"id": "2512.07344", "pdf": "https://arxiv.org/pdf/2512.07344", "abs": "https://arxiv.org/abs/2512.07344", "authors": ["Shengyuan Ye", "Bei Ouyang", "Tianyi Qian", "Liekang Zeng", "Mu Yuan", "Xiaowen Chu", "Weijie Hong", "Xu Chen"], "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "categories": ["cs.DC", "cs.AI"], "comment": "Accepted by IEEE International Conference on Computer Communications 2026", "summary": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "AI": {"tldr": "提出了一种名为Venus的边缘记忆和检索系统，用于基于视觉语言模型（VLM）的有效在线视频理解。", "motivation": "现有的视觉语言模型在多模态理解和推理能力方面取得了显著进展，但在实际部署中忽视了资源限制，导致系统开销过大。因此需要一种能够在边缘设备上高效运行的解决方案来提高实时性能和准确性。", "method": "Venus采用了云端分离架构，在边缘构建内存并检索关键帧以减少云侧负载。它包括两个阶段：在摄取阶段，通过场景分割和聚类连续处理流式视频，并利用多模态嵌入模型生成高效存储和检索的层次化记忆；查询阶段中，使用基于阈值的逐步采样算法从内存中索引传入查询并选择关键帧。", "result": "实验结果表明，与最先进的方法相比，Venus在总响应延迟方面实现了15倍至131倍的速度提升，在保证推理准确性的同时能够在秒级内做出实时反应。", "conclusion": "Venus成功解决了基于视觉语言模型的在线视频理解应用中的性能瓶颈问题，通过边缘计算和优化检索算法提高了系统的效率和实用性。"}}
{"id": "2512.07338", "pdf": "https://arxiv.org/pdf/2512.07338", "abs": "https://arxiv.org/abs/2512.07338", "authors": ["Luís Marnoto", "Alexandre Bernardino", "Bruno Martins"], "title": "Generalized Referring Expression Segmentation on Aerial Photos", "categories": ["cs.CV"], "comment": "Submitted to IEEE J-STARS", "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .", "AI": {"tldr": "本文提出了针对航空影像的广义指称表达分割任务，构建了一个大规模的数据集Aerial-D，并采用了一种结合系统规则生成与大型语言模型增强的方法来丰富指称表达。", "motivation": "当前航空影像在空间分辨率、色彩一致性、目标像素密度和部分遮挡等方面存在挑战。为了应对这些问题，本文旨在开发一个适合航空影像的指称表达分割数据集，并提高模型处理历史影像的能力。", "method": "本文设计了一个自动化的管道生成Aerial-D数据集，该管道结合了规则驱动的指称表达生成与大型语言模型增强程序来提升语义丰富性和视觉细节。随后利用RSRefSeg架构训练模型，在现代和历史航空图像上实现统一的目标实例和语义分割。", "result": "实验结果表明，组合训练方法在当代基准测试中表现出竞争力，并且能够在模拟的黑白、棕褐色及颗粒状降级的历史影像条件下保持较强的准确性。", "conclusion": "本文通过构建Aerial-D数据集和提出一种新颖的方法来解决航空影像中的指称表达分割问题，展示了模型对不同历史影像条件的良好适应性。"}}
{"id": "2512.07332", "pdf": "https://arxiv.org/pdf/2512.07332", "abs": "https://arxiv.org/abs/2512.07332", "authors": ["Zhengquan Luo", "Guy Tadmor", "Or Amar", "David Zeevi", "Zhiqiang Xu"], "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.", "AI": {"tldr": "提出了一种基于扩展里奇流的局部曲率感知知识图谱嵌入方法RicciKGE，以适应真实世界图形中不同区域间的急剧变化的局部曲率。", "motivation": "现有知识图谱嵌入（KGE）方法使用统一的几何空间来处理所有实体，但无法适应现实世界图形中的复杂结构和局部差异。这种固定几何会扭曲实体间的真实距离并降低表达能力。", "method": "通过将KGE损失梯度与局部曲率相结合，在扩展里奇流中动态调整实体嵌入以适应底层流形的几何变化，实现对不同知识图谱结构的有效建模。", "result": "实验结果显示RicciKGE在链接预测和节点分类基准测试上都表现出色，证明其能够有效应对复杂多样的知识图谱结构。", "conclusion": "通过引入扩展里奇流机制，使得实体嵌入与底层几何相互适应优化，从而提高模型的表达能力和泛化性能。"}}
{"id": "2512.07331", "pdf": "https://arxiv.org/pdf/2512.07331", "abs": "https://arxiv.org/abs/2512.07331", "authors": ["Kanishk Awadhiya"], "title": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.", "AI": {"tldr": "本文探讨了视觉变压器在处理不同类型数据时，层级表示稀疏性的自然形成机制。", "motivation": "研究者发现视觉变压器即使缺乏CNN的层次化偏差也能自发地表现出'U形'信息熵模式。他们希望通过分析不同数据集上训练的视觉变压器各层的有效编码维度来探究这一现象的数据依赖性成因。", "method": "通过对DINO训练的视觉变压器在UC Merced、Tiny ImageNet和CIFAR-100等不同复杂度数据集上的层级有效编码维度进行分析，研究者验证了瓶颈深度与任务所需语义抽象之间的关联。", "result": "研究发现，在以纹理为主的图像集中，高阶表示会保持不变；而在对象为中心的图像集合中，视觉变压器会在中间层抑制高频信息，形成一个有效的'学习到的瓶颈'来隔离语义特征。", "conclusion": "这项研究表明，视觉变压器能够通过数据驱动的方式学习到适应任务需求的层级表示稀疏性，而非单纯的架构特性。"}}
{"id": "2512.07328", "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "提出了ContextAnyone，这是一种基于上下文感知的扩散框架，用于从文本和单个参考图像生成一致的角色视频。", "motivation": "现有的个性化方法专注于面部识别但未能保持更广泛的背景线索的一致性，如发型、服装和体型等。这导致在视觉连贯性和角色一致性方面存在挑战。", "method": "ContextAnyone通过将扩散框架与新颖的Emphasize-Attention模块相结合，增强了对参考图像信息的理解和利用，并引入了Gap-RoPE位置嵌入来稳定时间建模。", "result": "实验表明，ContextAnyone在身份一致性和视觉质量方面超越了现有的参考到视频的方法。", "conclusion": "ContextAnyone能够生成连贯且背景保持良好的角色视频，在各种动作和场景中表现出色。"}}
{"id": "2512.07317", "pdf": "https://arxiv.org/pdf/2512.07317", "abs": "https://arxiv.org/abs/2512.07317", "authors": ["Alexander Wietfeld", "Wolfgang Kellerer"], "title": "DBMC-aNOMAly: Asynchronous NOMA with Pilot-Symbol Optimization Protocol for Diffusion-Based Molecular Communication Networks", "categories": ["cs.ET"], "comment": "16 pages, 17 figures; This work has been submitted to IEEE Transactions on Molecular, Biological, and Multi-Scale Communications", "summary": "Multiple access (MA) schemes can enable cooperation between multiple nodes in future diffusion-based molecular communication (DBMC) networks. Non-orthogonal MA for DBMC networks (DBMC-NOMA) is a promising option for efficient simultaneous MA using a single molecule type. Expanding significantly upon previous work on the topic, this paper addresses the question of parameter optimization and bit error probability (BEP) reduction in an asynchronous network using DBMC-NOMA. First, we analytically derive the associated BEP and use the result for a thorough comparison with other MA schemes like time-division and molecule-division MA. We show that the asynchronous nature of the system can be exploited for performance gain, and the upper-bound performance can be achieved in all circumstances by avoiding a few worst-case offset configurations. Subsequently, we propose DBMC-aNOMAly, a pilot-symbol-based optimization protocol for asynchronous DBMC-NOMA, and extensively evaluate it using Monte-Carlo simulations. DBMC-aNOMAly is shown to provide robust BEP reduction for different network sizes, noise levels, subjected to sampling jitter, as well as for changing conditions during runtime, particularly, compared to protocols in previous work. DBMC-aNOMAly consists of a set of simple operations such as comparisons and additions, deliberately designed to be implementable with chemical reaction networks, setting up future work on the realistic modeling of the protocol.", "AI": {"tldr": "本文提出了DBMC-aNOMAly，一种针对异步扩散分子通信网络中的非正交多址接入（NOMA）的优化协议。", "motivation": "为了提高未来基于扩散的分子通信网络中非正交多址接入方案的性能，特别是减少比特错误概率并提升系统效率。", "method": "通过推导相关的比特误码率，并与时间分集和分子分集等其他多址接入方案进行详细比较。提出DBMC-aNOMAly协议，利用引导符号进行优化，并使用蒙特卡洛模拟对其性能进行了广泛评估。", "result": "DBMC-aNOMAly在各种网络规模、噪声水平以及运行时的变动条件下，均表现出优异的比特误码率降低效果。此外，在所有情况下避免了最坏配置以达到上限性能。", "conclusion": "通过引入DBMC-aNOMAly协议，提高了异步扩散分子通信中非正交多址接入方案的表现，并展示了该协议在不同网络条件下的鲁棒性与有效性。"}}
{"id": "2512.07316", "pdf": "https://arxiv.org/pdf/2512.07316", "abs": "https://arxiv.org/abs/2512.07316", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection", "categories": ["cs.RO", "eess.SY"], "comment": "7 pages, 4 figures, submitted to IFAC World Congress 2026", "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.", "AI": {"tldr": "提出了一种基于模型预测控制的无人水面艇自主对接方法，能够抵抗外部干扰。", "motivation": "现有无人水面艇对接方案多将一个无人艇视为固定目标，该研究旨在实现两艘无人艇之间的合作对接，并通过模型预测控制方法提高对接效率和抗扰性。", "method": "使用集中式的模型预测控制来解决无人艇对接中的控制问题，通过预测模型预见外部干扰对无人艇的影响并进行抑制。", "result": "仿真结果显示所提出的方法能够实现更快更高效的对接过程，并能有效抵抗水下洋流等几乎静态的外界干扰。", "conclusion": "该方法提供了一种有效的两艘无人水面艇之间合作对接方案，且具有较好的抗扰能力。"}}
{"id": "2512.07314", "pdf": "https://arxiv.org/pdf/2512.07314", "abs": "https://arxiv.org/abs/2512.07314", "authors": ["Yuxiao Luo", "Songming Zhang", "Sijie Ruan", "Siran Chen", "Kang Liu", "Yang Xu", "Yu Zheng", "Ling Yin"], "title": "M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.", "AI": {"tldr": "本文提出了一种新的框架M-STAR，用于生成长期的人类移动轨迹。", "motivation": "现有的方法在长周期轨迹生成和显式时空多尺度建模方面存在局限性。", "method": "M-STAR结合了多尺度时空编码器和基于Transformer的解码器来实现细粒度预测。", "result": "实验结果表明，与现有方法相比，M-STAR在保真度和生成速度上均有显著提升。", "conclusion": "提出的M-STAR框架能更高效地生成高质量的人类移动轨迹。"}}
{"id": "2512.07313", "pdf": "https://arxiv.org/pdf/2512.07313", "abs": "https://arxiv.org/abs/2512.07313", "authors": ["Bosun Kang", "Hyejun Park", "Chenglin Fan"], "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach", "categories": ["cs.LG", "cs.DS"], "comment": "7 pages", "summary": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.", "AI": {"tldr": "本文通过贝叶斯决策视角重新审视经典的滑雪租赁问题，并提出一种离散贝叶斯框架，该框架能够维持时间范围内的确切后验分布。", "motivation": "在传统算法中，最坏情况成本是基于无假设条件的，而近期的学习增强方法则依赖于带鲁棒性保证的噪声预测。本文旨在统一这两种视角，并通过不确定性量化和专家先验知识的无缝集成来优化决策过程。", "method": "提出了一种离散贝叶斯框架，该框架在时间范围内保持精确后验分布，允许基于专家先验知识进行原理性的不确定性评估，并能平滑地从最坏情况过渡到完全知情的情况。", "result": "实验验证显示，在准确的先验条件下，所提算法可以取得近似最优的结果，同时保证了鲁棒性最坏情形下的性能。", "conclusion": "该框架展示了在线决策问题中在不完美预测下贝叶斯推理的实际优势，并可扩展至融合多种预测、非统一先验及情境信息。"}}
{"id": "2512.07312", "pdf": "https://arxiv.org/pdf/2512.07312", "abs": "https://arxiv.org/abs/2512.07312", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "categories": ["cs.AR", "cs.AI", "cs.DC"], "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing. We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups. Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "AI": {"tldr": "本文提出了一种动态缓存管理方法DCO，通过预测性和应用感知策略优化大型语言模型加速器的性能。", "motivation": "随着大语言模型的迅速普及，传统的软件复杂性增加和深度层次化快存储（SPM）异步管理模式已不再适合当前的需求。本文旨在探索一种简化编程且具有高性能的共享系统级缓存设计。", "method": "使用数据流信息来指导缓存替换策略，包括死块预测，并结合旁路决策机制以减轻缓存冲突问题。同时构建了一个详细的分析模型以验证在大规模真实工作负载下的性能。", "result": "实验表明与传统缓存架构相比，本方法可以获得显著的性能提升（最高达1.80倍）。此外，在RTL中的实现面积为0.064平方毫米，并能够支持2GHz的时钟频率。", "conclusion": "研究结果展示了共享缓存设计在优化未来AI加速器系统开发方面的潜力。"}}
{"id": "2512.07309", "pdf": "https://arxiv.org/pdf/2512.07309", "abs": "https://arxiv.org/abs/2512.07309", "authors": ["Guosheng Wang", "Shen Wang", "Lei Yang"], "title": "Radiance-Field Reinforced Pretraining: Scaling Localization Models with Unlabeled Wireless Signals", "categories": ["cs.IT", "cs.AI"], "comment": null, "summary": "Radio frequency (RF)-based indoor localization offers significant promise for applications such as indoor navigation, augmented reality, and pervasive computing. While deep learning has greatly enhanced localization accuracy and robustness, existing localization models still face major challenges in cross-scene generalization due to their reliance on scene-specific labeled data. To address this, we introduce Radiance-Field Reinforced Pretraining (RFRP). This novel self-supervised pretraining framework couples a large localization model (LM) with a neural radio-frequency radiance field (RF-NeRF) in an asymmetrical autoencoder architecture. In this design, the LM encodes received RF spectra into latent, position-relevant representations, while the RF-NeRF decodes them to reconstruct the original spectra. This alignment between input and output enables effective representation learning using large-scale, unlabeled RF data, which can be collected continuously with minimal effort. To this end, we collected RF samples at 7,327,321 positions across 100 diverse scenes using four common wireless technologies--RFID, BLE, WiFi, and IIoT. Data from 75 scenes were used for training, and the remaining 25 for evaluation. Experimental results show that the RFRP-pretrained LM reduces localization error by over 40% compared to non-pretrained models and by 21% compared to those pretrained using supervised learning.", "AI": {"tldr": "该论文提出了一种名为Radiance-Field Reinforced Pretraining (RFRP)的新型自监督预训练框架，用于提高基于无线信号的室内定位模型在跨场景泛化方面的准确性。", "motivation": "现有基于射频（RF）的室内定位模型依赖于特定场景的标签数据，在不同场景下的推广能力较弱。为此，作者提出了一种新的自监督预训练方法以利用大规模无标签RF数据进行有效的表征学习。", "method": "该论文提出了RFRP框架，这是一种非对称自动编码器架构，它将一个大型定位模型（LM）与神经射频辐射场（RF-NeRF）耦合在一起。LM编码接收到的RF光谱为隐式位置相关的表示形式，而RF-NeRF则通过解码这些表示来重构原始光谱。", "result": "实验结果表明，在100个不同场景的数据集上，使用RFRP预训练后的定位模型相比于未经过预训练的模型和基于监督学习的预训练模型，分别将定位误差减少了40％以上和21%。", "conclusion": "该研究证明了利用大规模无标签RF数据进行自监督预训练可以显著改善射频室内定位系统的跨场景泛化能力。"}}
{"id": "2512.07306", "pdf": "https://arxiv.org/pdf/2512.07306", "abs": "https://arxiv.org/abs/2512.07306", "authors": ["Thierry Petit", "Arnault Pachot"], "title": "Exact Synthetic Populations for Scalable Societal and Market Modeling", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Submitted for peer review on December 7, 2025", "summary": "We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.", "AI": {"tldr": "介绍了一种约束规划框架，用于生成精确符合目标统计数据的合成人口。", "motivation": "提出一种方法以精准控制人口统计特征而不依赖微数据，并研究其在下游分析中的影响。", "method": "利用约束编程直接编码聚合统计数据和结构关系来创建合成人群。", "result": "验证该框架并展示它如何用于模拟社会行为，探索市场与政策场景。", "conclusion": "通过Pollitics项目开发了此技术，可在不使用个人数据的情况下提供可复制的决策级见解。"}}
{"id": "2512.07305", "pdf": "https://arxiv.org/pdf/2512.07305", "abs": "https://arxiv.org/abs/2512.07305", "authors": ["Tobias Abraham Haider"], "title": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset", "categories": ["cs.CV"], "comment": null, "summary": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.", "AI": {"tldr": "本文重新评估了先前关于使用预训练的Google Inception-ResNet-v2模型进行欧洲野生动物物种检测的研究，并在不同的数据集上重复实验。", "motivation": "为了验证Carl等人的研究结果是否具有可复现性和泛化能力，本研究使用公开资源和不同数据集重新进行了相同的实验。", "method": "作者从头开始复制了先前的实验，利用包含90种物种共900张图像的数据集，并进行了一些预处理。然后用Google Inception-ResNet-v2模型对这些数据进行分类。", "result": "实验得出的整体分类准确率为62%，这与原始研究中报告的71%接近，表明尽管使用了不同的数据集，结果具有一定的可复现性。然而，每个类别的性能差异显著，宏观F1得分为0.28，说明直接利用ImageNet标签进行物种识别时存在局限。", "conclusion": "研究表明预训练卷积神经网络可以作为野生动物物种识别的实用基线方法，但要实现一致且高质量的预测，则需要对特定物种进行适应或采用迁移学习的方法。"}}
{"id": "2512.07303", "pdf": "https://arxiv.org/pdf/2512.07303", "abs": "https://arxiv.org/abs/2512.07303", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots", "categories": ["cs.RO"], "comment": "7 pages, 3 figures, submitted to IFAC World Congress 2026", "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.", "AI": {"tldr": "构建连续拓扑模型，解决缆绳机器人路径规划问题。", "motivation": "现有方法多依赖离散表示的配置空间，无法同时捕捉缆绳和机器人的连续位置信息。提出一种新方法来提高性能并简化计算。", "method": "通过建立工作空间与缆绳机器人配置空间之间的关系，并开发算法构建配置空间的单纯复形模型。", "result": "该模型可快速生成，比传统方法快且更加连续，支持多种路径规划算法。", "conclusion": "所提出的模型在时间效率和适用性方面均优于现有算法。"}}
{"id": "2512.07302", "pdf": "https://arxiv.org/pdf/2512.07302", "abs": "https://arxiv.org/abs/2512.07302", "authors": ["Mingning Guo", "Mengwei Wu", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.", "AI": {"tldr": "本文提出了一种名为AerialVP的框架，用于增强无人机图像感知中的任务提示，以提高基于视觉语言模型（VLM）的方法在处理复杂背景和目标混淆情况下的性能。", "motivation": "现有的基于视觉语言模型（VLM）的图像感知方法在应用于无人机图像时面临挑战，如目标混淆、尺度变化和复杂的背景环境。这些问题限制了传统VLM方法的有效性，因此需要一种新的框架来增强任务提示并提高模型性能。", "method": "AerialVP通过三个阶段的工作流程来提升任务提示：首先分析任务提示以确定任务类型和改进需求；然后从工具库中选择适当的工具；最后基于这些信息生成增强的任务提示。为了评估AerialVP，作者创建了AerialSense基准测试集，涵盖了无人机图像推理、视觉问答和视觉定位等任务。", "result": "实验结果表明，与传统的VLM方法相比，使用AerialVP框架可以显著提高模型在多种分辨率、光照条件以及城市和自然场景中的性能表现。", "conclusion": "通过引入AerialVP框架并构建全面的评估基准AerialSense，本文展示了如何利用增强的任务提示来提升无人机图像感知任务中基于视觉语言模型的方法的表现。"}}
{"id": "2512.07287", "pdf": "https://arxiv.org/pdf/2512.07287", "abs": "https://arxiv.org/abs/2512.07287", "authors": ["Sijia Li", "Yuchen Huang", "Zifan Liu", "Zijian Li", "Jingjing fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.", "AI": {"tldr": "本文提出了SIT-Graph，一种用于增强多轮工具使用中经验重用的方法。", "motivation": "当前的LLM代理在处理逐步明确意图和环境随每一步操作演变的情况下，难以适应变化。因此需要更好的方法来利用过去的经验。", "method": "通过构建包含简短状态摘要和工具间依赖关系的工具图，SIT-Graph能够平衡回溯先前上下文和遵循常规步骤之间的关系，从而更好地进行多轮决策。", "result": "实验表明，与现有基线相比，SIT-Graph在多轮工具使用基准测试中表现更佳，提供了更稳健的工具选择和有效经验重用。", "conclusion": "SIT-Graph通过结合简短状态摘要和工具间依赖关系，改善了多轮代理系统的适应性和有效性。"}}
{"id": "2512.07277", "pdf": "https://arxiv.org/pdf/2512.07277", "abs": "https://arxiv.org/abs/2512.07277", "authors": ["Srihari Bandarupalli", "Bhavana Akkiraju", "Charan Devarakonda", "Vamsiraghusimha Narsinga", "Anil Kumar Vuppala"], "title": "Efficient ASR for Low-Resource Languages: Leveraging Cross-Lingual Unlabeled Data", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted in AACL IJCNLP 2025", "summary": "Automatic speech recognition for low-resource languages remains fundamentally constrained by the scarcity of labeled data and computational resources required by state-of-the-art models. We present a systematic investigation into cross-lingual continuous pretraining for low-resource languages, using Perso-Arabic languages (Persian, Arabic, and Urdu) as our primary case study. Our approach demonstrates that strategic utilization of unlabeled speech data can effectively bridge the resource gap without sacrificing recognition accuracy. We construct a 3,000-hour multilingual corpus through a scalable unlabeled data collection pipeline and employ targeted continual pretraining combined with morphologically-aware tokenization to develop a 300M parameter model that achieves performance comparable to systems 5 times larger. Our model outperforms Whisper Large v3 (1.5B parameters) on Persian and achieves competitive results on Arabic and Urdu despite using significantly fewer parameters and substantially less labeled data. These findings challenge the prevailing assumption that ASR quality scales primarily with model size, revealing instead that data relevance and strategic pretraining are more critical factors for low-resource scenarios. This work provides a practical pathway toward inclusive speech technology, enabling effective ASR for underrepresented languages without dependence on massive computational infrastructure or proprietary datasets.", "AI": {"tldr": "本文提出了一种利用跨语言未标记数据进行连续预训练的方法，以提高低资源语言的自动语音识别性能。", "motivation": "由于标签数据稀缺和计算资源限制，针对低资源语言的自动语音识别效果受限。通过有效使用未标注的数据来弥补这一差距，并且不牺牲识别准确率是该研究的主要动机。", "method": "构建了一个3000小时多语种语料库并通过可扩展的未标记数据收集流程进行跨语言连续预训练，结合形态感知令牌化技术开发了具有3亿参数大小的模型。", "result": "所提出的模型在波斯语上超过了Whisper Large v3（15亿参数），并且在阿拉伯语和乌尔都语上的性能也很有竞争力。这些结果表明，在低资源场景下，数据的相关性和策略性预训练比仅仅增加模型尺寸更为关键。", "conclusion": "该研究为开发适合代表性不足的语言的高效自动语音识别技术提供了一条实用路径，无需依赖大规模计算基础设施或专有数据集。"}}
{"id": "2512.07276", "pdf": "https://arxiv.org/pdf/2512.07276", "abs": "https://arxiv.org/abs/2512.07276", "authors": ["Mai Tsujimoto", "Junjue Wang", "Weihao Xuan", "Naoto Yokoya"], "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026. Camera-ready-based version with minor edits for readability (no change in the contents)", "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.", "AI": {"tldr": "提出了Geo3DVQA，一个评估视觉语言模型在基于RGB遥感图像的高度感知、三维地理推理能力的基准。", "motivation": "当前方法依赖昂贵且专业化的传感器限制了全球可访问性。需要一种新的方式来集成多个3D线索和处理多样化查询以提高解释性。", "method": "创建了一个包含110k问题答案对的Geo3DVQA基准，涵盖了三种复杂性的16个任务类别：单一特征推理、多特征推理以及应用级空间分析。", "result": "评估了十个最先进的视觉语言模型，发现GPT-4o和Gemini-2.5-Flash在RGB到三维推理上的准确率分别为28.6%和33.0%，而经过领域特定微调的Qwen2.5-VL-7B达到了49.6%。", "conclusion": "Geo3DVQA揭示了现有视觉语言模型的局限性和领域适应的有效性，为可扩展、普及和全面的三维地理分析带来了新的挑战。"}}
{"id": "2512.07275", "pdf": "https://arxiv.org/pdf/2512.07275", "abs": "https://arxiv.org/abs/2512.07275", "authors": ["Siyu Wang", "Hua Wang", "Huiyu Li", "Fan Zhang"], "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "The paper has been accepted by BIBM 2025", "summary": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.", "AI": {"tldr": "提出了一种基于多尺度残差结构的编码器-解码器网络架构，用于皮肤病变分割。", "motivation": "为了解决现有方法在处理不规则病变形状和低对比度方面的不足，提高皮肤疾病早期检测和准确诊断的能力。", "method": "引入了多分辨率多通道融合（MRCF）模块来捕捉跨尺度特征，并提出了交叉混合注意力模块（CMAM），以动态计算权重并增强特征捕获的灵活性。还设计了外部注意力桥（EAB）用于信息的有效利用，克服了传统U-Net中由于跳连接导致的信息损失问题。", "result": "实验结果显示所提出模型在皮肤病变分割数据集上显著优于现有的基于变压器和卷积神经网络的方法，表现出卓越的分割准确性和鲁棒性。", "conclusion": "该研究通过创新性的网络架构解决了现有方法对不规则形状和低对比度病变分割效果不佳的问题，并展示了优越的技术性能。"}}
{"id": "2512.07273", "pdf": "https://arxiv.org/pdf/2512.07273", "abs": "https://arxiv.org/abs/2512.07273", "authors": ["Zhi Rao", "Yucheng Zhou", "Benjia Zhou", "Yiqing Huang", "Sergio Escalera", "Jun Wan"], "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation", "categories": ["cs.CV"], "comment": null, "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.", "AI": {"tldr": "本文提出了一种新的框架RVLF，用于无字幕手语翻译任务。", "motivation": "当前的手语翻译方法存在手势表示不足和句子级语义不一致的问题。为了克服这些问题，作者开发了RVLF框架以提高翻译质量。", "method": "RVLF通过引入有效的语义表征学习机制以及基于GRPO的优化策略来解决手语翻译中的问题。首先利用DINOv2提取视觉特征，并结合骨架运动线索进行融合；其次采用翻译保真度和句子完整性作为奖励函数，进一步提升模型性能。", "result": "RVLF在多个数据集上显著提升了BLEU-4分数：CSL-Daily、PHOENIX-2014T、How2Sign、OpenASL分别提高了+5.1、+1.11、+1.4和+1.61。", "conclusion": "RVLF在无字幕的手语翻译任务中表现出色，证明了GRPO优化策略的有效性。"}}
{"id": "2512.07269", "pdf": "https://arxiv.org/pdf/2512.07269", "abs": "https://arxiv.org/abs/2512.07269", "authors": ["Mike Diessner", "Yannick Tarant"], "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "AI": {"tldr": "该论文提出了一种基于图像和深度数据的图生成流水线，用于创建物理关键基础设施（如水或能源设施）的虚拟表示。", "motivation": "传统的三维点云模型通常需要昂贵的激光扫描设备，并且需要专业知识来使用。因此，本论文旨在通过一种更经济的方法来创建这些模型，该方法基于图像和深度数据并结合了深度学习技术。", "method": "提出了一种流水线方法，利用RGB图像和由立体相机生成的深度数据进行对象检测与实例分割，并采用用户定义的启发式规则或规则推断它们之间的关系。", "result": "实验结果表明，该策略可以产生接近地面真实情况的图结构，同时具有高度灵活性和透明度。", "conclusion": "这种方法不仅成本效益高，还能够适应特定应用的需求，在关键基础设施领域的决策中表现出很高的适用性。"}}
{"id": "2512.07266", "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Flögel", "Alexandru Vasilache", "Max Grobbel", "Jürgen Becker", "Sören Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "本论文提出了一种结合尖峰神经网络和人工神经网络的混合导航决策方法，用于机器人在人类环境中的社会导航。", "motivation": "为了将自主移动机器人更好地集成到人类环境中，并实现能量效率高且事件驱动的计算方式，同时解决当前神经形态方法在深度强化学习导航应用中训练不稳定的难题。", "method": "本文提出了一种结合尖峰神经网络和人工神经网络的混合导航决策系统，使用尖峰神经网络作为行为器来处理环境感知和社会交互信息，并用人工神经网络作为评判器来进行价值评估。", "result": "该方法在社会导航性能上得到了提升，并且降低了能耗估计约1.69个数量级。", "conclusion": "通过采用混合的深度强化学习架构，实现了更高效、节能的社会导航决策系统。"}}
{"id": "2512.07265", "pdf": "https://arxiv.org/pdf/2512.07265", "abs": "https://arxiv.org/abs/2512.07265", "authors": ["Bhavana Akkiraju", "Srihari Bandarupalli", "Swathi Sambangi", "Vasavi Ravuri", "R Vijaya Saraswathi", "Anil Kumar Vuppala"], "title": "TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation", "categories": ["cs.CL", "eess.AS"], "comment": "Submitted to AACL IJCNLP 2025", "summary": "Despite Telugu being spoken by over 80 million people, speech translation research for this morphologically rich language remains severely underexplored. We address this gap by developing a high-quality Telugu--English speech translation benchmark from 46 hours of manually verified CSTD corpus data (30h/8h/8h train/dev/test split). Our systematic comparison of cascaded versus end-to-end architectures shows that while IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data, finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data. This finding suggests that with careful hyperparameter tuning and sufficient parallel data (potentially less than 100 hours), end-to-end systems can achieve performance comparable to cascaded approaches in low-resource settings. Our metric reliability study evaluating BLEU, METEOR, ChrF++, ROUGE-L, TER, and BERTScore against human judgments reveals that traditional metrics provide better quality discrimination than BERTScore for Telugu--English translation. The work delivers three key contributions: a reproducible Telugu--English benchmark, empirical evidence of competitive end-to-end performance potential in low-resource scenarios, and practical guidance for automatic evaluation in morphologically complex language pairs.", "AI": {"tldr": "开发了一个高质量的泰卢固语-英语语音翻译基准，评估了级联和端到端架构的表现。", "motivation": "填补泰卢固语这种形态丰富的语言在语音翻译研究方面的空白。", "method": "使用46小时的手动验证CSTD语料库数据（30h/8h/8h训练/开发/测试分割），对比级联和端到端架构的表现，评估BLEU、METEOR等传统指标与BERTScore在低资源场景下的性能。", "result": "IndicWhisper + IndicMT表现最佳；SeamlessM4T表现出竞争力；传统的质量评测指标优于BERTScore。", "conclusion": "提供了可重复的泰卢固语-英语翻译基准，证明了端到端系统在低资源场景下的竞争潜力，并为自动评估复杂语言对提供实践指导。"}}
{"id": "2512.07259", "pdf": "https://arxiv.org/pdf/2512.07259", "abs": "https://arxiv.org/abs/2512.07259", "authors": ["Tharindu Wickremasinghe", "Marco F. Duarte"], "title": "Affine Subspace Models and Clustering for Patch-Based Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": "Asilomar Conference on Signals, Systems, and Computers 2025", "summary": "Image tile-based approaches are popular in many image processing applications such as denoising (e.g., non-local means). A key step in their use is grouping the images into clusters, which usually proceeds iteratively splitting the images into clusters and fitting a model for the images in each cluster. Linear subspaces have emerged as a suitable model for tile clusters; however, they are not well matched to images patches given that images are non-negative and thus not distributed around the origin in the tile vector space. We study the use of affine subspace models for the clusters to better match the geometric structure of the image tile vector space. We also present a simple denoising algorithm that relies on the affine subspace clustering model using least squares projection. We review several algorithmic approaches to solve the affine subspace clustering problem and show experimental results that highlight the performance improvements in clustering and denoising.", "AI": {"tldr": "研究了利用仿射子空间模型进行图像块聚类和去噪的方法，提出了一种基于最小二乘投影的简单去噪算法。", "motivation": "现有的线性子空间模型不适用于非负的图像块，因此提出了更匹配图像几何结构的仿射子空间模型以提高聚类和去噪的效果。", "method": "通过几种不同的算法解决仿射子空间聚类问题，并使用最小二乘投影进行图像去噪。", "result": "实验结果表明所提出的仿射子空间聚类方法在聚类和去噪方面优于传统线性子空间方法。", "conclusion": "利用仿射子空间模型可以更好地匹配图像块的几何结构，从而提高聚类效果和去噪性能。"}}
{"id": "2512.07253", "pdf": "https://arxiv.org/pdf/2512.07253", "abs": "https://arxiv.org/abs/2512.07253", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 8 figures, and 7 tables", "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "AI": {"tldr": "提出了一种基于深度学习的实时内窥镜视频增强框架DGGAN。", "motivation": "为了提高手术的安全性和有效性，解决现有图像增强方法计算复杂度过高、无法实现实时处理的问题。", "method": "通过对比学习从图像中提取退化表示，并引入融合机制指导单帧增强模型，采用循环一致性约束训练模型以提升鲁棒性和泛化能力。", "result": "实验表明，该框架在性能和效率上均优于多种现有方法，证明了基于退化的建模对实时内窥镜视频增强的有效性。", "conclusion": "提出的方法为临床应用提供了一种实用的路径。"}}
{"id": "2512.07251", "pdf": "https://arxiv.org/pdf/2512.07251", "abs": "https://arxiv.org/abs/2512.07251", "authors": ["Junqi Liu", "Zejun Wu", "Pedro R. A. S. Bassi", "Xinze Zhou", "Wenxuan Li", "Ibrahim E. Hamamci", "Sezgin Er", "Tianyu Lin", "Yi Luo", "Szymon Płotka", "Bjoern Menze", "Daguang Xu", "Kai Ding", "Kang Wang", "Yang Yang", "Yucheng Tang", "Alan L. Yuille", "Zongwei Zhou"], "title": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.", "AI": {"tldr": "提出了一种名为SMILE的解剖学感知扩散模型，用于对比增强。", "motivation": "现有的图像增强方法往往过度编辑，导致器官变形、虚假发现和遗漏小肿瘤。这些问题源于这些模型缺乏对解剖结构和对比度动态的理解。", "method": "SMILE引入了三个关键思想：（1）结构感知监督；（2）无需配准的学习方式；（3）统一的推理过程。", "result": "在六个外部数据集上，SMILE优于现有方法，在图像质量和临床实用性方面表现更好。", "conclusion": "SMILE通过提供解剖学准确和诊断有意义的图像，提高了癌症检测的准确性。"}}
{"id": "2512.07249", "pdf": "https://arxiv.org/pdf/2512.07249", "abs": "https://arxiv.org/abs/2512.07249", "authors": ["Jingran Yang", "Min Zhang", "Lingfeng Zhang", "Zhaohui Wang", "Yonggang Zhang"], "title": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.", "AI": {"tldr": "提出了一种基于影响函数的样本重加权方法IFFair，以减轻机器学习模型中的偏见。", "motivation": "由于机器学习算法在数据驱动下可能放大潜在的样本偏差，导致不公平决策。因此，需要一种新的预处理方法来减少这种偏差。", "method": "采用影响函数指导动态调整训练过程中不同群体之间的样本权重，以减轻偏差，同时不改变网络结构、数据特征和决策边界。", "result": "实验结果显示，在多种现实世界的数据集上，该方法有效减少了分类设置中的多个公认的公平性指标的偏见，并且在多目标之间取得了更好的权衡。", "conclusion": "IFFair能够有效地减轻机器学习模型的偏差，同时保持较高的准确性，为解决机器学习中的公平性问题提供了一种新的解决方案。"}}
{"id": "2512.07247", "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "AdLift 提出了一种针对3D高斯点云资产的保护方法，防止指令驱动编辑。", "motivation": "扩散模型扩展到三维领域后，虽然可以对3DGS进行忠实操作和内容创作，但也暴露了这些资源于未经授权编辑的风险。因此需要一种能够防范这种风险的方法。", "method": "AdLift 方法通过将严格受限的2D对抗扰动提升为3D高斯保护方式，并使用定制的Lifted PGD优化策略来确保扰动效果和不可见性，从而实现对任意视角和维度的指令驱动编辑防护。", "result": "实验结果表明，AdLift 在不同视点上均能有效抵御最先进的基于指令的2D图像和3DGS编辑攻击。", "conclusion": "AdLift 提供了一种有效的保护机制来防止3D高斯点云资产免受指令驱动编辑的影响。"}}
{"id": "2512.07245", "pdf": "https://arxiv.org/pdf/2512.07245", "abs": "https://arxiv.org/abs/2512.07245", "authors": ["Toshinori Yamauchi", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features", "categories": ["cs.CV"], "comment": "11+6 pages, 8 figures, 4 tables", "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.", "AI": {"tldr": "通过隔离决策关键特征并将其映射到CLIP特征空间来生成图像分类器的文本解释，提高解释的准确性与可解释性。", "motivation": "现有零样本方法生成的文字解释通常是描述可见内容而非驱动预测的因素。为此，提出TEXTER以解决这一问题。", "method": "识别贡献于预测的关键神经元，并强调这些神经元中的特征；然后将这些特征映射到CLIP特征空间中检索文字解释；稀疏自编码器进一步提高Transformer架构的可解释性。", "result": "实验表明，与现有方法相比，TEXTER生成更忠实且更具可解释性的解释。", "conclusion": "提出的TEXTER方法可以有效提升图像分类决策的关键特征识别，并能更好地反映模型推理过程。"}}
{"id": "2512.07241", "pdf": "https://arxiv.org/pdf/2512.07241", "abs": "https://arxiv.org/abs/2512.07241", "authors": ["Md. Srabon Chowdhury", "Syeda Fahmida Tanzim", "Sheekar Banerjee", "Ishtiak Al Mamoon", "AKM Muzahidul Islam"], "title": "Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture", "categories": ["cs.CV"], "comment": null, "summary": "Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.", "AI": {"tldr": "提出了一个结合SqueezeNet v1和EfficientNet-B0的混合深度学习模型，用于基于MRI图像进行脑肿瘤分类。", "motivation": "为了克服磁共振成像（MRI）在脑肿瘤诊断过程中存在的时间消耗大及容易出现观察者之间误差的问题，提出了一种新的方法来提高准确性并减少计算复杂度。", "method": "使用SqueezeNet v1和EfficientNet-B0构建混合模型，并结合手工设计的特征描述符如HOG、LBP等进一步增强模型性能。实验在Nickparvar脑肿瘤MRI数据集上进行，该数据集包含7,023张对比增强T1加权轴向切片。", "result": "测试准确率达到98.93%，使用测试时间增广（TTA）后提高至99.08%。模型只需训练少于2.1百万参数和小于1.2GFLOPs，展示了高效计算能力和出色的泛化性能。", "conclusion": "所提出的混合网络架构在自动化MRI肿瘤分类上达到了接近临床可靠性的准确度，并且可能成为辅助决策支持系统的有力工具。"}}
{"id": "2512.07237", "pdf": "https://arxiv.org/pdf/2512.07237", "abs": "https://arxiv.org/abs/2512.07237", "authors": ["Cheng Zhang", "Boying Li", "Meng Wei", "Yan-Pei Cao", "Camilo Cruz Gambardella", "Dinh Phung", "Jianfei Cai"], "title": "Unified Camera Positional Encoding for Controlled Video Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/chengzhag/UCPE", "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.", "AI": {"tldr": "本文提出了UCPE（统一相机位置编码），一种包含完整相机信息的几何一致表示方法，用于视频生成和自动驾驶等领域中的相机控制。", "motivation": "现有的相机编码方法通常依赖于简化后的针孔假设，这限制了其在实际摄像头的多样性和镜头畸变下的泛化能力。为了提高相机控制的效果，本文提出了一种新的几何一致表示方法来解决这个问题。", "method": "通过引入相对光线编码和绝对方位编码，本文提出了UCPE（Unified Camera Positional Encoding）方法，它能够将完整的相机信息，包括六自由度姿态、内参以及镜头畸变统一起来。UCPE在预训练的视频扩散变换器中加入了一个轻量级的空间注意力适配器。", "result": "实验表明，采用UCPE可以实现状态-of-the-art的相机可控性，并且保持较高的视觉保真度。通过构建包含广泛摄像机运动和镜头类型的大型视频数据集，展示了UCPE在相机控制视频生成方面的有效性。", "conclusion": "本文提出的UCPE方法为未来多视角、视频和3D任务提供了一种通用的相机表示方法，具有广阔的潜在应用前景。"}}
{"id": "2512.07234", "pdf": "https://arxiv.org/pdf/2512.07234", "abs": "https://arxiv.org/abs/2512.07234", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "AI": {"tldr": "提出了一种新的方法Dropout Prompt Learning，以增强视觉语言模型的鲁棒性和适应性。", "motivation": "为了提高视觉语言模型在不同挑战场景下的性能和泛化能力，通过引入dropout技术来增强模型的稳健性。", "method": "针对文本和视觉分支中的每个标记应用dropout，并评估其重要性；同时提出了残差熵正则化以保持语义对齐并促进多样化的表示。", "result": "实验结果表明，该方法在低样本学习、长尾分类以及分布外泛化等场景中优于现有的正则化方法。", "conclusion": "Dropout Prompt Learning 方法能够有效提高视觉语言模型的鲁棒性和适应性，在多个基准测试上表现出色。"}}
{"id": "2512.07232", "pdf": "https://arxiv.org/pdf/2512.07232", "abs": "https://arxiv.org/abs/2512.07232", "authors": ["Wenlong Liu", "Jiahua Pan", "Xingyu Zhang", "Xinxin Gong", "Yang Ye", "Xujin Zhao", "Xin Wang", "Kent Wu", "Hua Xiang", "Houmin Yan", "Qingpeng Zhang"], "title": "Cross-platform Product Matching Based on Entity Alignment of Knowledge Graph with RAEA model", "categories": ["cs.AI"], "comment": "10 pages, 5 figures, published on World Wide Web", "summary": "Product matching aims to identify identical or similar products sold on different platforms. By building knowledge graphs (KGs), the product matching problem can be converted to the Entity Alignment (EA) task, which aims to discover the equivalent entities from diverse KGs. The existing EA methods inadequately utilize both attribute triples and relation triples simultaneously, especially the interactions between them. This paper introduces a two-stage pipeline consisting of rough filter and fine filter to match products from eBay and Amazon. For fine filtering, a new framework for Entity Alignment, Relation-aware and Attribute-aware Graph Attention Networks for Entity Alignment (RAEA), is employed. RAEA focuses on the interactions between attribute triples and relation triples, where the entity representation aggregates the alignment signals from attributes and relations with Attribute-aware Entity Encoder and Relation-aware Graph Attention Networks. The experimental results indicate that the RAEA model achieves significant improvements over 12 baselines on EA task in the cross-lingual dataset DBP15K (6.59% on average Hits@1) and delivers competitive results in the monolingual dataset DWY100K. The source code for experiments on DBP15K and DWY100K is available at github (https://github.com/Mockingjay-liu/RAEA-model-for-Entity-Alignment).", "AI": {"tldr": "本文提出了一种基于知识图实体对齐的跨平台产品匹配方法，特别是在eBay和Amazon之间的产品匹配。", "motivation": "现有实体对齐方法未能充分利用属性三元组与关系三元组之间的相互作用，导致效果不佳。为此，作者设计了一种新的框架RAEA来解决这一问题。", "method": "该文提出了一个两阶段的产品匹配流程：粗过滤和细过滤，并引入了新的实体对齐模型RAEA用于细过滤阶段，该模型通过属性感知的实体编码器和关系感知图注意力网络聚焦于属性三元组与关系三元组之间的相互作用。", "result": "实验结果表明，在跨语言数据集DBP15K上，RAEA模型在Hits@1指标平均提升了6.59%，并且在单语言数据集DWY100K上的表现也具有竞争力。", "conclusion": "基于知识图谱实体对齐的产品匹配方法通过有效利用属性和关系信息能够显著提高产品匹配的准确性。"}}
{"id": "2512.07230", "pdf": "https://arxiv.org/pdf/2512.07230", "abs": "https://arxiv.org/abs/2512.07230", "authors": ["Abhinav Raundhal", "Gaurav Behera", "P J Narayanan", "Ravi Kiran Sarvadevabhatla", "Makarand Tapaswi"], "title": "STRinGS: Selective Text Refinement in Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026. Project Page, see https://STRinGS-official.github.io", "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.", "AI": {"tldr": "本文提出了一种基于文本感知的、选择性优化框架STRinGS，以改进3D高斯点云中文字的清晰度和可读性。", "motivation": "在进行三维场景重建时，传统方法如3D高斯点云难以精确再现细小的文字细节。这会严重影响图像中的语义信息传达。", "method": "STRinGS框架首先区分文本区域与非文本区域，对文本区域进行针对性优化后再将其合并到整体的3D重构中。同时引入了OCR字符错误率(CER)来评估文字可读性。", "result": "相较于未经改进的传统方法，STRinGS在仅需7K次迭代下实现了63.6%相对提升的文字清晰度和可读性。", "conclusion": "通过本文提出的STRinGS框架与新构建的STRinGS-360数据集，在富含文本信息的真实场景中取得了更好的三维重建效果，推动了面向文本密集型环境下的三维理解技术进步。"}}
{"id": "2512.07229", "pdf": "https://arxiv.org/pdf/2512.07229", "abs": "https://arxiv.org/abs/2512.07229", "authors": ["Fang Zhou", "Zhiqiang Chen", "Martin Pavlovski", "Yizhong Zhang"], "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted to the Main Track of the 28th European Conference on Artificial Intelligence (ECAI 2025). To appear in the proceedings published by IOS Press (DOI: 10.3233/FAIA413)", "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.", "AI": {"tldr": "提出ReLKD框架，通过知识蒸馏学习跨类关系以改进新类别分类。", "motivation": "解决广泛类别发现任务中的跨类关系学习难题，提升对未知类别的识别性能。", "method": "采用三级模块结构：目标粒度模块学习区分性表示，粗粒度模块捕捉层次化类别关系，蒸馏模块进行知识转移优化。", "result": "实验表明ReLKD在四个数据集上表现出色，尤其是在标注样本较少的情况下。", "conclusion": "提出的方法有效利用了跨类关系，改进了新类别分类任务的表现。"}}
{"id": "2512.07228", "pdf": "https://arxiv.org/pdf/2512.07228", "abs": "https://arxiv.org/abs/2512.07228", "authors": ["Hengyang Yao", "Lin Li", "Ke Sun", "Jianing Qiu", "Huiping Chen"], "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.", "AI": {"tldr": "该论文提出了一个用于对抗DeepFake面部交换攻击的鲁棒保护扰动方法EOLT，通过学习变换分布和自适应生成实例特定干扰来提高防御效果。", "motivation": "传统的方法在基本转换如压缩或调整大小时容易破坏嵌入图像中的不可见扰动。作者分析了30种不同的转换，并发现现有技术对训练过程中所用的转换选择高度敏感，因此提出了改进方案以增强鲁棒性。", "method": "该方法引入期望学习变换分布（EOLT）框架，使用策略网络自动优先考虑关键变换并生成针对实例特定干扰。通过强化学习实现具体化防御瓶颈建模的同时保持广泛的迁移能力。", "result": "实验表明该方法显著优于现有技术，在平均鲁棒性方面提高了26%，在具有挑战性的转换类别中最多提升了30%。", "conclusion": "EOLT框架提供了一种有效的策略来对抗DeepFake面部交换攻击，通过自适应变换分布学习和实例特定扰动生成增强了防御效果。"}}
{"id": "2512.07226", "pdf": "https://arxiv.org/pdf/2512.07226", "abs": "https://arxiv.org/abs/2512.07226", "authors": ["Runwu Shi", "Chang Li", "Jiang Wang", "Rui Zhang", "Nabeela Khan", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Unsupervised Single-Channel Audio Separation with Diffusion Source Priors", "categories": ["eess.AS", "cs.SD"], "comment": "15 pages, 31 figures, accepted by The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Single-channel audio separation aims to separate individual sources from a single-channel mixture. Most existing methods rely on supervised learning with synthetically generated paired data. However, obtaining high-quality paired data in real-world scenarios is often difficult. This data scarcity can degrade model performance under unseen conditions and limit generalization ability. To this end, in this work, we approach this problem from an unsupervised perspective, framing it as a probabilistic inverse problem. Our method requires only diffusion priors trained on individual sources. Separation is then achieved by iteratively guiding an initial state toward the solution through reconstruction guidance. Importantly, we introduce an advanced inverse problem solver specifically designed for separation, which mitigates gradient conflicts caused by interference between the diffusion prior and reconstruction guidance during inverse denoising. This design ensures high-quality and balanced separation performance across individual sources. Additionally, we find that initializing the denoising process with an augmented mixture instead of pure Gaussian noise provides an informative starting point that significantly improves the final performance. To further enhance audio prior modeling, we design a novel time-frequency attention-based network architecture that demonstrates strong audio modeling capability. Collectively, these improvements lead to significant performance gains, as validated across speech-sound event, sound event, and speech separation tasks.", "AI": {"tldr": "本文提出了一种无监督的单通道音频分离方法，使用扩散源先验进行迭代重建引导。", "motivation": "现有的大多数方法依赖于合成生成的配对数据进行有监督学习，在真实场景中很难获得高质量的数据，这会导致模型在未见条件下的性能下降和泛化能力受限。因此，本文从无监督的角度出发解决单通道音频分离的问题。", "method": "该方法通过迭代指导初始状态向解的方向收敛来实现分离，引入了一种先进的逆问题求解器以减少扩散先验与重建引导之间的梯度冲突，并且使用增强的混合输入初始化去噪过程。此外，设计了基于时间频率注意力机制的新颖网络架构。", "result": "改进后的模型在语音声音事件、声音事件和语音分离任务上表现出显著性能提升。", "conclusion": "通过无监督的方法结合扩散源先验以及先进的逆问题求解器，实现了高质量的单通道音频分离，并且提出了新颖的时间频率注意力网络架构以进一步增强音频建模能力。"}}
{"id": "2512.07224", "pdf": "https://arxiv.org/pdf/2512.07224", "abs": "https://arxiv.org/abs/2512.07224", "authors": ["Tianyi Ren", "Daniel Low", "Pittra Jaengprajak", "Juampablo Heras Rivera", "Jacob Ruzevick", "Mehmet Kurt"], "title": "Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician\" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \\textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.", "AI": {"tldr": "本文研究了通过Shapley值衍生的协议和不确定性指标来提高深度学习医学图像分割模型的临床可解释性。", "motivation": "尽管深度学习在医学图像分割中表现出色，但为了确保其在临床上被接受和应用，解释性的需求依然存在。因此，本文探索了使用对比级别的Shapley值评估特征重要性和模型性能。", "method": "通过系统地扰动模型输入来评估特征的重要性，研究采用BraTS 2024数据集生成四类MRI对比的四个模型架构中的Shapley值排名，并提出了两个指标：模型与临床影像排名之间的协议度量和跨验证折痕的Shapley排名方差不确定性量化。", "result": "性能较好的病例（Dice > 0.6）显示出更高的协议度，而较高的Shapley排名变异性则与较差的表现相关联。这些结果表明新提出的指标可以作为模型可靠性的临床可解释代理。", "conclusion": "所提出的方法通过使用对比级别的Shapley值提供了对深度学习医学图像分割模型的更全面、更具临床意义的理解，有助于医生更好地理解最先进的分割模型。"}}
{"id": "2512.07221", "pdf": "https://arxiv.org/pdf/2512.07221", "abs": "https://arxiv.org/abs/2512.07221", "authors": ["Zichao Shu", "Shitao Bei", "Lijun Li", "Zetao Chen"], "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality", "categories": ["cs.RO"], "comment": null, "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.", "AI": {"tldr": "本文提出了一种新型连续时间最大似然估计器，以解决基于标记的光学运动捕捉系统在生成地面真实数据时存在的时间和空间校准问题及测量抖动的问题。", "motivation": "随着XR应用标准的提高，SLAM基准测试的需求也变得更加严格。然而，目前使用的光捕获系统受到时间空间校准和内部抖动限制，难以准确评估旋转误差等关键指标。", "method": "本文提出了一种连续时间最大似然估计器，并结合辅助惯性测量单元(IMU)数据来补偿运动捕捉抖动；此外还提出了可变时间同步方法及基于螺丝同构约束的姿态残差，实现了多传感器和设备的时间空间校准。", "result": "实验结果表明，该方法优于现有技术，在XR应用中为最先进的SLAM算法提供了精确的基准测试能力。", "conclusion": "本文的方法通过解决运动捕捉系统的抖动及时间空间校准问题，提升了SLAM算法在XR中的性能评估精度，并且验证了其实际可行性。"}}
{"id": "2512.07219", "pdf": "https://arxiv.org/pdf/2512.07219", "abs": "https://arxiv.org/abs/2512.07219", "authors": ["Sungyong Chung", "Alireza Talebpour", "Samer H. Hamdar"], "title": "Characterizing Lane-Changing Behavior in Mixed Traffic", "categories": ["cs.MA", "cs.GT", "cs.RO", "eess.SY"], "comment": null, "summary": "Characterizing and understanding lane-changing behavior in the presence of automated vehicles (AVs) is crucial to ensuring safety and efficiency in mixed traffic. Accordingly, this study aims to characterize the interactions between the lane-changing vehicle (active vehicle) and the vehicle directly impacted by the maneuver in the target lane (passive vehicle). Utilizing real-world trajectory data from the Waymo Open Motion Dataset (WOMD), this study explores patterns in lane-changing behavior and provides insight into how these behaviors evolve under different AV market penetration rates (MPRs). In particular, we propose a game-theoretic framework to analyze cooperative and defective behaviors in mixed traffic, applied to the 7,636 observed lane-changing events in the WOMD. First, we utilize k-means clustering to classify vehicles as cooperative or defective, revealing that the proportions of cooperative AVs are higher than those of HDVs in both active and passive roles. Next, we jointly estimate the utilities of active and passive vehicles to model their behaviors using the quantal response equilibrium framework. Empirical payoff tables are then constructed based on these utilities. Using these payoffs, we analyze the presence of social dilemmas and examine the evolution of cooperative behaviors using evolutionary game theory. Our results reveal the presence of social dilemmas in approximately 4% and 11% of lane-changing events for active and passive vehicles, respectively, with most classified as Stag Hunt or Prisoner's Dilemma (Chicken Game rarely observed). Moreover, the Monte Carlo simulation results show that repeated lane-changing interactions consistently lead to increased cooperative behavior over time, regardless of the AV penetration rate.", "AI": {"tldr": "本文通过分析Waymo Open Motion Dataset中的数据，利用博弈论框架研究了混合交通中车道变换行为的模式和演化。", "motivation": "了解在有自动驾驶车辆（AVs）的情况下车道变换的行为对于确保混合交通的安全性和效率至关重要。", "method": "文章使用k-means聚类将车辆分为合作型与非合作型，并利用量反应均衡框架联合估计主动车和被动车的效用，构建基于这些效用的经验支付表，分析社会困境的存在并探讨合作行为随时间的演变。", "result": "结果显示，在车道变换事件中约有4%的主动车和11%的被动车存在社会困境，且大多数为猎鹿博弈或囚徒困境。重复的车道变换互动会随着时间增加合作行为的比例。", "conclusion": "研究揭示了混合交通环境中车道变换行为的社会困境以及这些行为随时间演化的趋势，表明自动驾驶车辆在提高交通效率和安全性方面具有潜在的优势。"}}
{"id": "2512.07218", "pdf": "https://arxiv.org/pdf/2512.07218", "abs": "https://arxiv.org/abs/2512.07218", "authors": ["Feng Liang", "Weixin Zeng", "Runhao Zhao", "Xiang Zhao"], "title": "NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.", "AI": {"tldr": "提出了一种神经符号框架NeSTR，用于改进大型语言模型在复杂时间推理中的性能。", "motivation": "现有的方法要么未能充分利用LLM的推理能力，要么缺乏结构化的时态表示，导致推理不一致或产生幻觉。因此需要一种新的方法来提高LLM的时间理解能力。", "method": "NeSTR通过符号编码保持明确的时间关系，通过验证确保逻辑一致性，并使用溯因反思纠正错误推断。", "result": "实验显示，NeSTR在各种时间问答基准上表现出色，并且可以零样本地改进时态推理。", "conclusion": "神经符号集成显著增强了大型语言模型的时间理解能力。"}}
{"id": "2512.07215", "pdf": "https://arxiv.org/pdf/2512.07215", "abs": "https://arxiv.org/abs/2512.07215", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "AI": {"tldr": "通过CLIP和DINOv2的视觉对比分析，评估它们在手部抓取场景中的三维姿态估计任务上的表现。", "motivation": "利用VFMs和VLMs丰富的语义和几何表示能力来比较两种模型在三维姿态估计任务中的性能差异，为机器人操纵和抓取应用选择合适的视觉模型提供依据。", "method": "对比研究基于CLIP的和DINOv2的方法在6D物体姿态估计上的表现，并通过基准数据集进行广泛的实验。", "result": "CLIP方法表现出更好的语义一致性，而DINOv2方法展示了增强的几何精度。两者具有互补优势。", "conclusion": "根据分析结果，建议根据不同需求选择合适的视觉模型用于机器人操纵和抓取应用。"}}
{"id": "2512.07212", "pdf": "https://arxiv.org/pdf/2512.07212", "abs": "https://arxiv.org/abs/2512.07212", "authors": ["Zhaoyang Liu", "Mokai Pan", "Zhongyi Wang", "Kaizhen Zhu", "Haotao Lu", "Jingya Wang", "Ye Shi"], "title": "Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.", "AI": {"tldr": "本文提出了一种新的生成型视觉运动策略BridgePolicy，通过扩散桥的公式将观察嵌入到随机微分方程中，从而改善机器人控制的精确度和可靠性。", "motivation": "现有的模仿学习方法通常不把观察直接整合进扩散过程的动力学模型，而是将其作为去噪网络中的高阶条件输入。这种处理方式导致了从随机噪声开始采样，减弱了感知与控制之间的耦合，并且往往产生次优的性能表现。", "method": "引入了一种新的策略BridgePolicy，其核心是通过扩散桥的形式将观察嵌入到随机微分方程中，从而允许从一个信息丰富的先验状态而不是随机噪声开始采样。此外，设计了多模态融合模块和语义对齐器来统一视觉输入与状态，并对齐观察与动作表示。", "result": "实验结果显示，在52个模拟任务上的三个基准测试以及五个实际世界任务中，BridgePolicy的表现均优于现有的顶级生成型策略。", "conclusion": "通过将观测嵌入到随机微分方程中的方式，改进了机器人控制的精度和可靠性，并且在广泛的实验验证下表现出了优越性。"}}
{"id": "2512.07211", "pdf": "https://arxiv.org/pdf/2512.07211", "abs": "https://arxiv.org/abs/2512.07211", "authors": ["Frederik Hagelskjær", "Dimitrios Arapis", "Steffen Madsen", "Thorbjørn Mosekjær Iversen"], "title": "Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds", "categories": ["cs.CV"], "comment": "8 pages, 8 figures, 5 tables, ICCR 2025", "summary": "Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings. We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io", "AI": {"tldr": "论文提出了一种基于神经网络的方法，用于仅使用3D无色数据估计物体姿态不确定性。", "motivation": "传统的姿态估计方法通常提供单一的姿态估计结果，无法捕捉由于视觉模糊引起的姿态不确定性。现有的姿态分布估算方法依赖于颜色信息，在工业环境中难以实现。", "method": "论文提出了一种新的基于深度学习的方法，使用仅3D数据来估测物体的旋转和平面镜像对称性引起的姿态不确定性。", "result": "通过在现实世界的拾取场景中验证了该方法的有效性和鲁棒性，并证明了其可以扩展到全面的姿态分布估计。", "conclusion": "论文成功地提出了不依赖于RGB输入的第一种基于深度学习的物体姿态不确定性的估测框架。"}}
{"id": "2512.07209", "pdf": "https://arxiv.org/pdf/2512.07209", "abs": "https://arxiv.org/abs/2512.07209", "authors": ["Masato Ishii", "Akio Hayakawa", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits", "categories": ["cs.MM", "cs.LG", "cs.SD"], "comment": null, "summary": "We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.", "AI": {"tldr": "该论文提出了一种新的音频视频编辑流水线，通过条件生成音频来增强编辑后的视频和其伴随音频之间的连贯性。", "motivation": "为了提高编辑后视频与相应音频之间的一致性和内容完整性，作者提出了一个新的视频到音频生成模型。此模型基于源音频、目标视频以及文本提示进行条件化生成。", "method": "该方法首先使用先进的视频编辑技术产生目标视频，然后执行音频编辑以适应视觉变化。提出了一种新的视频到音频生成模型并扩展其架构以支持条件音频输入，并采用数据增强策略提高训练效率。", "result": "实验结果显示，与现有方法相比，本文的方法在保持音视频一致性和内容完整性方面表现更优。", "conclusion": "论文通过引入新颖的编辑流程和改进的生成模型，在实现高效且连贯的音频视频同步方面取得了显著进展。"}}
{"id": "2512.07208", "pdf": "https://arxiv.org/pdf/2512.07208", "abs": "https://arxiv.org/abs/2512.07208", "authors": ["Fei Luo", "Ziwei Zhao", "Mingxuan Wang", "Duoyang Li", "Zhe Qian", "Jiayi Tuo", "Chenyue Zhou", "Yanbiao Ma"], "title": "Geometric Prior-Guided Federated Prompt Calibration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.", "AI": {"tldr": "本文提出了一种基于几何先验的联邦提示校准框架GGTPC，用于解决数据异质性导致的联邦提示学习中的局部训练偏差问题。", "motivation": "现有的联邦提示学习方法在处理数据异质性方面效果不佳，因为它们主要关注聚合或正则化，未能直接纠正由于数据偏斜引起的局部训练偏差。为了解决这一根本原因，本文提出了GGTPC框架。", "method": "提出了一种新的几何先验校准层（GPCL），该方法在服务器端通过隐私保护的方式从协方差矩阵重建全局数据分布的形状，并指导客户端在校正其本地特征分布时与全局先验保持一致。", "result": "实验结果表明，GGTPC框架在不同数据偏斜水平下均优于现有的最先进技术。具体而言，在严重偏斜的数据集（如β=0.1和β=0.01的CIFAR-100）上分别提高了2.15%和9.17%，在领域偏斜的数据集中也提升了4.60%", "conclusion": "GGTPC框架成功地减轻了数据异质性问题，通过纠正根本性的局部训练偏差来提高联邦学习模型的性能，并作为一个可插拔模块增强各种FL算法的表现。"}}
{"id": "2512.07206", "pdf": "https://arxiv.org/pdf/2512.07206", "abs": "https://arxiv.org/abs/2512.07206", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "AI": {"tldr": "开发了一种全自动深度学习系统AutoLugano，用于从基线FDG-PET/CT扫描中进行淋巴瘤分类、解剖定位和自动Lugano分期。", "motivation": "通过自动化处理基线FDG-PET/CT图像来实现完全自动化淋巴瘤分类，从而提高诊断效率和准确性。", "method": "该系统包括三个模块：1）基于解剖信息的病变分割；2）基于图谱的解剖定位；3）自动Lugano分期。这些步骤通过深度学习模型执行，并在公共数据集上进行训练与验证。", "result": "在独立验证集中，所提模型表现出色，实现区域检测的总体准确率为88.31%，敏感性为74.47%，特异性为94.21%，F1得分为80.80%。对于治疗分层的关键任务（局限期与晚期），系统达到高准确性85.07%，特异性和敏感性分别为90.48%和82.61%。", "conclusion": "AutoLugano是首个将单次基线FDG-PET/CT扫描转化为完整Lugano分期的全自动、端到端流程，展示了在初始分期、治疗分层和支持临床决策中的强大潜力。"}}
{"id": "2512.07203", "pdf": "https://arxiv.org/pdf/2512.07203", "abs": "https://arxiv.org/abs/2512.07203", "authors": ["Xuhui Zheng", "Kang An", "Ziliang Wang", "Yuhang Wang", "Faqiang Qian", "Yichao Wu"], "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning", "categories": ["cs.CV"], "comment": "7 pages, 1 figures", "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.", "AI": {"tldr": "MMRPT通过掩码多模态强化预训练框架，增强了大规模视觉语言模型的视觉推理能力。", "motivation": "现有的多模态预训练受限于图像-描述对的表面化偏向，导致模型更倾向于模仿描述而非基于视觉的理解。为解决此问题，引入了MMRPT来增强视觉理解。", "method": "MMRPT通过估计句子级别的视觉依赖性、掩码高度依赖视觉的部分并用语义-视觉奖励指导重构这些部分的方法进行多模态预训练。", "result": "实验表明，在各种基准测试中，MMRPT提供了零样本性能改进，并在监督微调下增强了模型的鲁棒性。", "conclusion": "通过强化学习驱动的掩码推理，可以为多模态模型提供更可靠和更具泛化的预训练目标。"}}
{"id": "2512.07201", "pdf": "https://arxiv.org/pdf/2512.07201", "abs": "https://arxiv.org/abs/2512.07201", "authors": ["Cheng Yu"], "title": "Understanding Diffusion Models via Code Execution", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.", "AI": {"tldr": "通过代码执行的角度解释扩散模型的工作原理。", "motivation": "现有教程主要集中在推导方程，对实际代码操作指导有限。本文旨在填补理论与实践之间的差距。", "method": "提供一个约300行的简洁实现，保留了前向扩散、逆采样、噪声预测网络和训练循环等关键组件。", "result": "提供了研究人员清晰的第一手理解，如何在实践中运行扩散模型及其代码与理论对应关系。", "conclusion": "该报告通过提供简化的代码示例帮助研究人员更好地理解和实现扩散模型。"}}
{"id": "2512.07198", "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "本文提出了一个两阶段的管道，结合大型语言模型和文本到图像生成模型的能力，用于生成富有逻辑链的故事图片。", "motivation": "为了丰富视觉表达并促进主动解释，通过利用复杂的语义信息来创建故事图片。", "method": "提出了一种名为StorytellingPainter的两阶段管道，并开发了一个包括三个主要评估器的框架以评价生成效果。此外还探索了减少开源和专有语言模型性能差距的方法。", "result": "实验结果证明了所提方法的有效性和可行性。", "conclusion": "通过结合大型语言模型和文本到图像生成模型，可以有效地创造富含逻辑链的故事图片，并提供了一套评估框架来衡量这些图片的质量。"}}
{"id": "2512.07197", "pdf": "https://arxiv.org/pdf/2512.07197", "abs": "https://arxiv.org/abs/2512.07197", "authors": ["Seokhyun Youn", "Soohyun Lee", "Geonho Kim", "Weeyoung Kwon", "Sung-Ho Bae", "Jihyong Oh"], "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting", "categories": ["cs.CV"], "comment": "The first three authors contributed equally to this work. The last two authors are co-corresponding authors. Please visit our project page at https://cmlab-korea.github.io/Awesome-Efficient-GS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.", "AI": {"tldr": "本文提供了关于高效三维和四维高斯点着技术的第一个统一概述，系统地将现有方法分类为参数压缩与重构压缩。", "motivation": "为了克服3D高斯点着在存储和渲染中的大规模内存和计算需求的挑战，特别是在动态场景中，需要减少冗余同时保持重建质量的方法。", "method": "本文把现有的高效三维和四维高斯点着方法系统地归类为参数压缩与重构压缩，并全面总结了每个类别内的核心思想和研究趋势。", "result": "涵盖了广泛使用的数据集、评估指标以及具有代表性的基准对比。", "conclusion": "讨论当前的限制并指出了可扩展的，紧凑且实时的静态和动态三维场景表示高斯点着技术的研究方向。"}}
{"id": "2512.07195", "pdf": "https://arxiv.org/pdf/2512.07195", "abs": "https://arxiv.org/abs/2512.07195", "authors": ["Xuan Zhang", "Wenxuan Zhang", "Anxu Wang", "See-Kiong Ng", "Yang Deng"], "title": "MASim: Multilingual Agent-Based Simulation for Social Science", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA", "cs.SI"], "comment": null, "summary": "Multi-agent role-playing has recently shown promise for studying social behavior with language agents, but existing simulations are mostly monolingual and fail to model cross-lingual interaction, an essential property of real societies. We introduce MASim, the first multilingual agent-based simulation framework that supports multi-turn interaction among generative agents with diverse sociolinguistic profiles. MASim offers two key analyses: (i) global public opinion modeling, by simulating how attitudes toward open-domain hypotheses evolve across languages and cultures, and (ii) media influence and information diffusion, via autonomous news agents that dynamically generate content and shape user behavior. To instantiate simulations, we construct the MAPS benchmark, which combines survey questions and demographic personas drawn from global population distributions. Experiments on calibration, sensitivity, consistency, and cultural case studies show that MASim reproduces sociocultural phenomena and highlights the importance of multilingual simulation for scalable, controlled computational social science.", "AI": {"tldr": "本文介绍了MASim，一种支持多语言交互的多代理仿真框架。", "motivation": "现有模拟大多为单语环境，无法建模跨语言互动。作者提出MASim以研究真实社会中的跨语言交流。", "method": "MASim通过构建MAPS基准来实现多轮交互，并分析全球公众意见和媒体影响。", "result": "实验表明MASim能够重现社会文化现象并突出多语种模拟的重要性。", "conclusion": "MASim是首个支持多种语言的仿真框架，有助于开展大规模、可控的社会科学研究。"}}
{"id": "2512.07194", "pdf": "https://arxiv.org/pdf/2512.07194", "abs": "https://arxiv.org/abs/2512.07194", "authors": ["Yuchen Tian", "Samuel Tensingh", "Jason Eshraghian", "Nhan Duy Truong", "Omid Kavehei"], "title": "Synchrony-Gated Plasticity with Dopamine Modulation for Spiking Neural Networks", "categories": ["cs.NE"], "comment": "23 pages, 7 figures, 5 tables, accepted by TMLR", "summary": "While surrogate backpropagation proves useful for training deep spiking neural networks (SNNs), incorporating biologically inspired local signals on a large scale remains challenging. This difficulty stems primarily from the high memory demands of maintaining accurate spike-timing logs and the potential for purely local plasticity adjustments to clash with the supervised learning goal. To effectively leverage local signals derived from spiking neuron dynamics, we introduce Dopamine-Modulated Spike-Synchrony-Dependent Plasticity (DA-SSDP), a synchrony-based rule that is sensitive to loss and brings a synchrony-based local learning signal to the model. DA-SSDP condenses spike patterns into a synchrony metric at the batch level. An initial brief warm-up phase assesses its relationship to the task loss and sets a fixed gate that subsequently adjusts the local update's magnitude. In cases where synchrony proves unrelated to the task, the gate settles at one, simplifying DA-SSDP to a basic two-factor synchrony mechanism that delivers minor weight adjustments driven by concurrent spike firing and a Gaussian latency function. These small weight updates are only added to the network`s deeper layers following the backpropagation phase, and our tests showed this simplified version did not degrade performance and sometimes gave a small accuracy boost, serving as a regularizer during training. The rule stores only binary spike indicators and first-spike latencies with a Gaussian kernel. Without altering the model structure or optimization routine, evaluations on benchmarks like CIFAR-10 (+0.42\\%), CIFAR-100 (+0.99\\%), CIFAR10-DVS (+0.1\\%), and ImageNet-1K (+0.73\\%) demonstrated consistent accuracy gains, accompanied by a minor increase in computational overhead. Our code is available at https://github.com/NeuroSyd/DA-SSDP.", "AI": {"tldr": "本文提出了一种基于同步性和多巴胺调节的脉冲神经网络训练方法，该方法在保持模型结构和优化流程不变的情况下提升了多个数据集上的精度。", "motivation": "传统的替代反向传播法虽然能够有效训练深度脉冲神经网络(SNNs)，但在大规模引入生物启发式局部信号时遇到了挑战。这些挑战包括内存需求高、难以将纯本地可塑性调整与监督学习目标协调一致等问题。", "method": "本文提出了一种新的规则DA-SSDP，该规则利用同步性来传递局部更新的幅度，并通过多巴胺调节机制评估其与任务损失的关系并设定固定门限。此方法只存储二进制脉冲指示器和第一个脉冲延迟时间，并在反向传播阶段后将小权重调整应用到网络更深层。", "result": "实验结果表明，该方法在多个基准测试数据集（如CIFAR-10、CIFAR-100、CIFAR10-DVS和ImageNet-1K）上均实现了精度提升，并且增加了少量的计算开销。", "conclusion": "DA-SSDP不仅提升了模型的性能，还在不改变模型结构或优化流程的情况下提供了额外的正则化效果。"}}
{"id": "2512.07192", "pdf": "https://arxiv.org/pdf/2512.07192", "abs": "https://arxiv.org/abs/2512.07192", "authors": ["Niu Yi", "Xu Tianyi", "Ma Mingming", "Wang Xinkun"], "title": "HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression", "categories": ["cs.CV"], "comment": "12 pages, 7 figures", "summary": "Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.", "AI": {"tldr": "HVQ-CGIC框架通过引入超先验模型改进了基于向量量化（VQ）的生成式图像压缩方法，从而在比特率与失真之间实现了更好的平衡。", "motivation": "现有的基于向量量化的学习型图像压缩方法使用静态全局概率分布估计熵值，无法适应特定图像的内容，导致未被充分利用的比特率和灵活速率控制方面的挑战。", "method": "提出了HVQ-CGIC框架，在此基础上设计了RD平衡与控制机制，并结合轻量级超先验估计网络实现了显著的RD性能提升。", "result": "在Kodak数据集上，相较于现有SOTA方法，HVQ-CGIC框架使用更少比特率却达到了相同的LPIPS值。", "conclusion": "HVQ-CGIC可能成为VQGAN图像压缩的基础组件，并且类似于HyperPrior框架在神经网络图像压缩中的作用。"}}
{"id": "2512.07191", "pdf": "https://arxiv.org/pdf/2512.07191", "abs": "https://arxiv.org/abs/2512.07191", "authors": ["Wenqi Zhao", "Jiacheng Sang", "Fenghua Cheng", "Yonglu Shu", "Dong Li", "Xiaofeng Yang"], "title": "RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.", "AI": {"tldr": "提出了一种基于Retinex分解的线性化结构先验反射模型（RefLSM），用于医学图像分割和偏场校正。", "motivation": "传统水平集方法在严重非均匀成像条件下难以精确分割，且依赖近似的偏场估计。为了克服这些限制，引入一种新的基于Retinex的反射分解来提高精度和鲁棒性。", "method": "通过将观察图像分解为反射率和偏场组件，并结合线性结构先验引导光滑反射梯度朝向数据驱动参考，避免了重新初始化引起的扩散。使用ADMM优化方案解决变分问题。", "result": "在多个医学影像数据集上验证了RefLSM的优越分割精度、鲁棒性和计算效率，优于现有水平集方法。", "conclusion": "RefLSM通过引入反射分解和线性化结构先验显著提高了医学图像分割的质量，并展示了良好的应用前景。"}}
{"id": "2512.07190", "pdf": "https://arxiv.org/pdf/2512.07190", "abs": "https://arxiv.org/abs/2512.07190", "authors": ["Pengfei Gu", "Huimin Li", "Haoteng Tang", "Dongkuan", "Xu", "Erik Enriquez", "DongChul Kim", "Bin Fu", "Danny Z. Chen"], "title": "Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.", "AI": {"tldr": "本文提出了一种新的基于拓扑引导的分类框架，该框架利用多尺度和多层次持久拓扑特征，并将其整合到视觉分类骨干网络中。", "motivation": "现代深度神经网络在医学图像分类中的表现非常出色。然而这些网络要么强调像素强度特性而不是根本解剖结构（例如那些由同调不变量编码的），要么仅通过单参数持续性捕获简单的拓扑特征。本文旨在解决这些问题，提出了一种新的方法。", "method": "输入图像后，首先在多图分辨率/尺度上计算立方持久图(PDs)。然后设计了一个“葡萄园”算法来将这些PDs合并成一个单一的稳定图，捕捉从整体解剖到早期疾病可能指示的细微局部异常的所有粒度特征。此外还开发了一种基于交叉注意的神经网络直接处理最终合并的PD。", "result": "在三个公共数据集上的评估表明，本文的方法相对于强大的基准和最先进的方法实现了持续且显著的改进，证明了其多尺度多层次拓扑视角的价值。", "conclusion": "通过将多尺度多层次拓扑整合到端到端架构中，本方法增强了模型识别复杂解剖结构的能力。实验结果证实这种方法的有效性。"}}
{"id": "2512.07186", "pdf": "https://arxiv.org/pdf/2512.07186", "abs": "https://arxiv.org/abs/2512.07186", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "title": "START: Spatial and Textual Learning for Chart Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "WACV2026 Camera Ready", "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "AI": {"tldr": "该论文提出了一种用于图表理解的START模型，通过空间和文本学习来提高多模态大型语言模型在现实场景中的应用能力。", "motivation": "传统的自然图像处理方法不适用于图表的理解，因为图表同时具有结构化的视觉布局（空间属性）和数据表示（文本属性）。为了应对这一挑战，该研究提出了START模型以增强对图表的理解。", "method": "提出了一种新的数据生成管道，利用多模态大型语言模型将真实图表图像翻译成可执行的代码，并使用一个大型语言模型来确定图表元素的位置。同时，设计了Chart Spatial understanding Benchmark（CS-Bench）用于评估模型理解图表空间结构的能力。", "result": "START模型在不同大小的模型和基准测试中表现优异，超越了以前的最佳状态。", "conclusion": "通过引入空间和文本学习，START模型能够在各种任务上提供一致的性能提升，并且优于现有的方法。"}}
{"id": "2512.07179", "pdf": "https://arxiv.org/pdf/2512.07179", "abs": "https://arxiv.org/abs/2512.07179", "authors": ["Wonbeen Lee", "Channyoung Lee", "Junho Sohn", "Hansam Cho"], "title": "PICKT: Practical Interlinked Concept Knowledge Tracing for Personalized Learning using Knowledge Map Concept Relations", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "15 pages, 5 figures, 17 tables. Preparing submission for EDM 2026 conference", "summary": "With the recent surge in personalized learning, Intelligent Tutoring Systems (ITS) that can accurately track students' individual knowledge states and provide tailored learning paths based on this information are in demand as an essential task. This paper focuses on the core technology of Knowledge Tracing (KT) models that analyze students' sequences of interactions to predict their knowledge acquisition levels. However, existing KT models suffer from limitations such as restricted input data formats, cold start problems arising with new student enrollment or new question addition, and insufficient stability in real-world service environments. To overcome these limitations, a Practical Interlinked Concept Knowledge Tracing (PICKT) model that can effectively process multiple types of input data is proposed. Specifically, a knowledge map structures the relationships among concepts considering the question and concept text information, thereby enabling effective knowledge tracing even in cold start situations. Experiments reflecting real operational environments demonstrated the model's excellent performance and practicality. The main contributions of this research are as follows. First, a model architecture that effectively utilizes diverse data formats is presented. Second, significant performance improvements are achieved over existing models for two core cold start challenges: new student enrollment and new question addition. Third, the model's stability and practicality are validated through delicate experimental design, enhancing its applicability in real-world product environments. This provides a crucial theoretical and technical foundation for the practical implementation of next-generation ITS.", "AI": {"tldr": "本文提出了一种实用的互联概念知识追踪模型PICKT，以解决传统知识追踪模型存在的限制问题，并在真实操作环境中验证了该模型的有效性和实用性。", "motivation": "随着个性化学习的发展，需要一种能够准确跟踪学生个人知识状态并提供定制化学习路径的智能教学系统。然而，现有的知识追踪模型存在输入数据格式受限、新学生入学或新增试题时出现冷启动问题以及在真实服务环境中稳定性不足等问题。", "method": "本文提出了一种实用互联概念知识追踪（PICKT）模型，可以有效处理多种类型的输入数据，通过构建知识图谱来表示题目和概念间的关系，在冷启动情况下也能实现有效的知识追踪。", "result": "实验结果表明，该模型在解决新学生入学和新增试题两个核心冷启动问题上显著优于现有模型，并且通过精细的实验设计验证了其稳定性和实用性，从而提高了在实际产品环境中的适用性。", "conclusion": "本文研究为下一代智能教学系统（ITS）的实际应用提供了重要的理论和技术基础。"}}
{"id": "2512.07178", "pdf": "https://arxiv.org/pdf/2512.07178", "abs": "https://arxiv.org/abs/2512.07178", "authors": ["Latifa Dwiyanti", "Sergio Ryan Wibisono", "Hidetaka Nambo"], "title": "ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "This paper was accepted and presented at the 7th World Symposium on Software Engineering (WSSE) 2025 on 25 October 2025 in Okayama, Japan, and is currently awaiting publication", "summary": "Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.", "AI": {"tldr": "本文通过结合SHAP和大型语言模型（LLM）提出了一种增强版的可解释性AI工具ContextualSHAP，旨在为非技术用户提供更易理解的上下文文本解释。", "motivation": "尽管SHAP在提供机器学习模型的重要性可视化方面表现出色，但它缺乏有意义的上下文解释。为了弥补这一不足，本文提出了ContextualSHAP以提升对用户尤其是不具备技术背景用户的可解释性。", "method": "通过将SHAP与大型语言模型（LLM）结合，并采用由用户定义的参数来定制化生成适合特定场景和受众的文本解释，从而增强其上下文信息。", "result": "基于真实用户进行的里克特量表调查及后续访谈显示，相较于仅使用可视化方法，所提工具产生的解释在可理解性和情境适应性方面获得了更好的反馈。", "conclusion": "初步研究结果表明，通过将可视化与上下文化文本相结合的方式可以提供更友好且可信的模型解释。"}}
{"id": "2512.07177", "pdf": "https://arxiv.org/pdf/2512.07177", "abs": "https://arxiv.org/abs/2512.07177", "authors": ["Fanjun Bu", "Melina Tsai", "Audrey Tjokro", "Tapomayukh Bhattacharjee", "Jorge Ortiz", "Wendy Ju"], "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction", "categories": ["cs.RO"], "comment": null, "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.", "AI": {"tldr": "本文提出了一种基于视觉语言模型的代理方法，以模拟人类在人机交互中的社会智能。", "motivation": "机器人需要根据复杂的非言语线索来决定何时与人类互动，这些线索难以通过显式建模实现。因此，研究人员希望通过观察真实场景下的互动行为和专家操作者的决策过程，开发出一种更有效的代理方法。", "method": "研究者设计了一个两阶段管道：首先使用轻量级的感知检测器（如目光转移和空间位置）在关键时刻触发视频基于视觉语言模型(VLM)查询。然后评估了两种不同的提示策略的有效性。", "result": "研究表明，通过选择性地应用VLM作为社会推理代理能够实现更加符合社会规范的人机互动行为。", "conclusion": "该方法使得机器人能够在真实世界中依据人类自然提供的线索进行适当的行动，提升了人机交互的社交响应能力。"}}
{"id": "2512.07171", "pdf": "https://arxiv.org/pdf/2512.07171", "abs": "https://arxiv.org/abs/2512.07171", "authors": ["Shravan Venkatraman", "Rakesh Raj Madavan", "Pavan Kumar S", "Muthu Subash Kavitha"], "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration", "categories": ["cs.CV"], "comment": "21 pages, 11 figures, 5 tables", "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.", "AI": {"tldr": "该论文提出了TIDE框架，用于解决水下图像恢复中的复杂退化问题。", "motivation": "当前的水下图像恢复方法难以处理空间变化且多样的退化情况。因此需要一种新的方法来针对性地修复这些退化。", "method": "TIDE通过两阶段逆向退化估计框架，将退化分解为四种关键因素，并设计了专门针对每种因素的恢复专家。这种方法在适应性融合和逐步细化中取得了较好的效果。", "result": "实验表明，TIDE在标准基准测试和挑战性的浑浊水条件下表现良好，尤其是在颜色校正和对比度增强方面优于现有方法。", "conclusion": "通过有效处理多因素退化问题，TIDE在保持自然结果的同时提高了图像质量和感知质量。"}}
{"id": "2512.07170", "pdf": "https://arxiv.org/pdf/2512.07170", "abs": "https://arxiv.org/abs/2512.07170", "authors": ["Jiayang Li", "Chengjie Jiang", "Junjun Jiang", "Pengwei Liang", "Jiayi Ma", "Liqiang Nie"], "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.", "AI": {"tldr": "该论文提出了一种基于扩散变换器的统一语义和可控图像融合框架DiTFuse，实现了端到端、理解高层语义的同时进行细粒度多模态对齐。", "motivation": "目前的图像融合方法在鲁棒性、适应性和可控制性方面存在限制。现有模型难以同时理解和执行高阶语义以及细粒度多模态对齐，尤其是在复杂场景下表现不佳。", "method": "DiTFuse框架通过联合编码两幅图像和自然语言指令，在共享潜在空间中进行端到端的、语义感知融合。该方法采用预训练策略，并利用一个多退化掩蔽图像建模策略来学习跨模态对齐、模态不变恢复及任务感知特征选择。", "result": "实验结果表明，DiTFuse在公共IVIF、MFF和MEF基准测试上表现出色，在视觉效果和语义保留方面优于现有方法。该模型支持多级用户控制并具有零样本泛化能力。", "conclusion": "DiTFuse统一了红外-可见光、多重焦距以及多曝光图像融合，并且能够进行文本控制精修及下游任务，是一个强大的多功能框架。"}}
{"id": "2512.07168", "pdf": "https://arxiv.org/pdf/2512.07168", "abs": "https://arxiv.org/abs/2512.07168", "authors": ["Georgios Ioannides", "Christos Constantinou", "Aman Chadha", "Aaron Elkins", "Linsey Pang", "Ravid Shwartz-Ziv", "Yann LeCun"], "title": "JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "UniReps: Unifying Representations in Neural Models (NeurIPS 2025 Workshop)", "summary": "We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.", "AI": {"tldr": "提出了一种结合JEPA和DAAM的两阶段自监督框架，用于学习鲁棒的语音表示。", "motivation": "通过引入联合嵌入预测架构(JEPA)和密度自适应注意机制(DAAM)，实现语义音频特征的学习，以提高语音编码效率和质量。", "method": "第一阶段利用JEPA结合DAAM从潜在空间进行掩码预测学习语义音频特征；第二阶段采用有限标量量化(FSQ)和混合基数打包方案对表示进行高效分词，并使用HiFi-GAN解码器实现高质量波形重构。通过引入基于高斯混合的密度自适应门控，模型可以执行自适应时间特征选择并发现层次语音结构。", "result": "生成的令牌（47.5个/秒）提供可逆、高度压缩且适合语言模型的表示，并在性能和效率上优于现有的神经音频编解码器。", "conclusion": "该框架成功地实现了鲁棒语音表示的学习，提高了语音编码的质量和效率。"}}
