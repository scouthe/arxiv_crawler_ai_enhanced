{"id": "2601.14256", "pdf": "https://arxiv.org/pdf/2601.14256", "abs": "https://arxiv.org/abs/2601.14256", "authors": ["Matthew Gwilliam", "Xiao Wang", "Xuefeng Hu", "Zhenheng Yang"], "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding", "categories": ["cs.CV"], "comment": "18 pages, 16 tables, 4 figures", "summary": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.", "AI": {"tldr": "本文提出了一种新的模型，该模型通过隐式神经表示学习图像表征，以同时支持识别和生成任务。", "motivation": "现有的图像表示学习模型通常专注于识别或生成中的一个方向。作者希望通过一种方法统一这两种目标，即在同一个模型中实现高效的图像识别和高质量的图像生成能力。", "method": "该研究采用了超网络进行隐式神经表征的学习，通过将图像映射到模型权重来快速准确地重构图像，并且结合了知识蒸馏以提高其泛化能力和性能。", "result": "所提出的模型在学习图像表示时表现出色，在多种视觉任务中达到了最新的技术水平，并实现了高质量的小型嵌入式生成能力。", "conclusion": "该研究证明了一种新的统一的图像表征学习方法，可以同时支持高效的识别和高质量的生成任务。"}}
{"id": "2601.14255", "pdf": "https://arxiv.org/pdf/2601.14255", "abs": "https://arxiv.org/abs/2601.14255", "authors": ["Sangbeom Lim", "Seoung Wug Oh", "Jiahui Huang", "Heeji Yoon", "Seungryong Kim", "Joon-Young Lee"], "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://cvlab-kaist.github.io/VideoMaMa/", "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.", "AI": {"tldr": "本文介绍了VideoMaMa，一种通过预训练的视频扩散模型将粗略分割掩码转换为精确像素级alpha蒙版的技术。", "motivation": "现有的视频抠图模型在现实世界视频上的泛化能力有限，主要原因是缺乏标注数据。为此，作者提出了VideoMaMa以解决这一问题。", "method": "VideoMaMa利用预训练的视频扩散模型将粗略的分割掩码转换为精确的alpha蒙版，并开发了一个可扩展的伪标签生成管道来构建大规模的视频抠图数据集MA-V。", "result": "该论文通过在合成数据上训练的方法，实现了对现实世界视频的强大零样本泛化能力。基于MA-V的数据集，微调后的SAM2模型（即SAM2-Matte）在野外视频上的表现优于现有数据集训练的同类模型。", "conclusion": "研究结果表明大规模伪标签生成的视频抠图的重要性，并展示了生成先验和可访问分割线索如何推动视频抠图领域的规模化进展。"}}
{"id": "2601.14253", "pdf": "https://arxiv.org/pdf/2601.14253", "abs": "https://arxiv.org/abs/2601.14253", "authors": ["Hongyuan Chen", "Xingyu Chen", "Youjia Zhang", "Zexiang Xu", "Anpei Chen"], "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis", "categories": ["cs.CV"], "comment": "Project page: https://motion3-to-4.github.io/. Code: https://github.com/Inception3D/Motion324", "summary": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.", "AI": {"tldr": "本文介绍了Motion 3-to-4，一种用于从单目视频和可选的3D参考网格生成高质量4D动态对象的前馈框架。", "motivation": "尽管在2D、视频和3D内容生成方面取得了显著进步，但4D合成仍然具有挑战性，主要因为训练数据有限以及从单目视角恢复几何形状和运动时存在固有模糊性。本文旨在解决这些问题。", "method": "Motion 3-to-4将4D合成分解为静态3D形状生成和运动重建两个步骤，利用一个规范的参考网格学习紧凑的运动潜变量表示，并预测每帧顶点轨迹以恢复完整的、在时间上一致的几何结构。引入了一个可扩展的逐帧变换器来增强模型对不同序列长度的鲁棒性。", "result": "评估显示，在标准基准和包含准确地面真实几何数据的新数据集上的测试中，Motion 3-to-4相比先前工作提供了更高的保真度和空间一致性。", "conclusion": "研究展示了从单目视频合成高质量4D动态对象的可能性，并通过实验验证了所提方法的有效性。"}}
{"id": "2601.14251", "pdf": "https://arxiv.org/pdf/2601.14251", "abs": "https://arxiv.org/abs/2601.14251", "authors": ["Said Taghadouini", "Adrien Cavaillès", "Baptiste Aubertin"], "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.", "AI": {"tldr": "介绍LightOnOCR-2-1B，这是一个10亿参数的端到端多语言视觉-语言模型，用于将文档图像转换成整洁、自然排序的文字。", "motivation": "旨在通过一个更小且更快的模型，在不使用脆弱的OCR流水线的情况下提供高质量的文本识别，同时提高在各种文档上的表现。", "method": "该模型通过大规模、高质量的蒸馏混合训练，涵盖扫描件、法语文件和科学PDF。引入了归一化的边界框预测，并利用预训练中的简历策略和基于IoU奖励的强化学习进行精炼。此外，使用检查点平均和任务算术合并来提高鲁棒性。", "result": "在OlmOCR-Bench上达到了最先进的结果，模型规模只有先前最佳模型的1/9且速度更快，并发布了新的评估基准LightOnOCR-bbox-bench。", "conclusion": "展示了一个小而快的视觉-语言模型可以提供高质量和多样的文本识别性能，并开放了模型检查点、数据集和评估基准。"}}
{"id": "2601.14250", "pdf": "https://arxiv.org/pdf/2601.14250", "abs": "https://arxiv.org/abs/2601.14250", "authors": ["Pengze Zhang", "Yanze Wu", "Mengtian Li", "Xu Bai", "Songtao Zhao", "Fulong Ye", "Chong Mou", "Xinghui Li", "Zhuowei Chen", "Qian He", "Mingyuan Gao"], "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "categories": ["cs.CV"], "comment": "Github Page: https://pangzecheung.github.io/OmniTransfer/", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "AI": {"tldr": "OmniTransfer是一个统一的框架，用于时空视频转换。", "motivation": "现有的视频定制方法依赖参考图像或特定任务的时间先验知识，未能充分利用视频中丰富的时空信息，限制了视频生成的灵活性和泛化能力。", "method": "OmniTransfer通过跨帧多视图信息来增强外观一致性，并利用时间线索实现精细的时间控制。其核心设计包括任务感知的位置偏差、参考解耦因果学习和任务自适应多模态对齐。", "result": "实验表明，OmniTransfer在外观（身份和样式）和时间转移（相机移动和视频效果）方面超越了现有的方法，并且在没有使用姿势的情况下，在运动转换上与姿势引导的方法相当。", "conclusion": "该研究确立了一种新的范式，以实现灵活、高保真度的视频生成。"}}
{"id": "2601.14246", "pdf": "https://arxiv.org/pdf/2601.14246", "abs": "https://arxiv.org/abs/2601.14246", "authors": ["Zeyuan Chen", "Kai Zhang", "Zhuowen Tu", "Yuanjun Xiong"], "title": "Soft Tail-dropping for Adaptive Visual Tokenization", "categories": ["cs.CV"], "comment": null, "summary": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.", "AI": {"tldr": "本文介绍了Soft Tail-dropping Adaptive Tokenizer（STAT），一种根据图像的结构复杂性和细节水平自适应选择输出标记数量的一维离散视觉标记器。", "motivation": "动机在于开发一种可以生成长度自适应的一维视觉标记，这些标记与因果一维自回归（AR）视觉生成模型自然兼容的方法。", "method": "STAT将图像编码为一系列离散代码和每个标记的保留概率，并对这些保留概率进行正则化处理，使其在序列中单调递减，同时将其分布显式地与图像级别的复杂性度量对齐。", "result": "结果表明，在ImageNet-1k数据集上，使用STAT装备的标准因果AR模型可以产生具有竞争力或更优的视觉生成质量，且显示出优于以往纯自回归尝试的良好扩展行为。", "conclusion": "结论是STAT能够生成适应图像复杂性的长度自适应标记，并显著提升了与之结合的一维自回归视觉生成模型的效果和可扩展性。"}}
{"id": "2601.14242", "pdf": "https://arxiv.org/pdf/2601.14242", "abs": "https://arxiv.org/abs/2601.14242", "authors": ["Bertie Vidgen", "Austin Mann", "Abby Fennelly", "John Wright Stanly", "Lucas Rothman", "Marco Burstein", "Julien Benchek", "David Ostrofsky", "Anirudh Ravichandran", "Debnil Sur", "Neel Venugopal", "Alannah Hsia", "Isaac Robinson", "Calix Huang", "Olivia Varones", "Daniyal Khan", "Michael Haines", "Zach Richards", "Chirag Mahapatra", "Brendan Foody", "Osvald Nitski"], "title": "APEX-Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.", "AI": {"tldr": "介绍了一个名为APEX-Agents的基准，用于评估AI代理在长周期、跨应用任务中的执行能力。", "motivation": "为了评估AI代理是否能够完成由投资银行分析师、管理顾问和公司律师创建的复杂工作任务，特别是在模拟真实工作环境中。", "method": "引入了AI生产力指数APEX-Agents，该基准测试要求代理在包含文件和工具的真实工作环境中导航并执行任务。测试了八种代理，并使用Pass@1进行评估。", "result": "Gemini 3 Flash (Thinking=High)获得了最高的24.0%得分，其次是GPT-5.2、Claude Opus 4.5和Gemini 3 Pro。", "conclusion": "发布了包含所有提示、评分标准、黄金输出、文件和元数据的APEX-Agents基准测试，并公开了用于代理执行和评估的基础设施Archipelago。"}}
{"id": "2601.14235", "pdf": "https://arxiv.org/pdf/2601.14235", "abs": "https://arxiv.org/abs/2601.14235", "authors": ["LSST Dark Energy Science Collaboration", "Eric Aubourg", "Camille Avestruz", "Matthew R. Becker", "Biswajit Biswas", "Rahul Biswas", "Boris Bolliet", "Adam S. Bolton", "Clecio R. Bom", "Raphaël Bonnet-Guerrini", "Alexandre Boucaud", "Jean-Eric Campagne", "Chihway Chang", "Aleksandra Ćiprijanović", "Johann Cohen-Tanugi", "Michael W. Coughlin", "John Franklin Crenshaw", "Juan C. Cuevas-Tello", "Juan de Vicente", "Seth W. Digel", "Steven Dillmann", "Mariano Javier de León Dominguez Romero", "Alex Drlica-Wagner", "Sydney Erickson", "Alexander T. Gagliano", "et al. (41 additional authors not shown)"], "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration", "categories": ["astro-ph.IM", "astro-ph.CO", "cs.AI", "cs.LG", "stat.ML"], "comment": "84 pages. This is v1.0 of the DESC's white paper on AI/ML, a collaboration document that is being made public but which is not planned for submission to a journal", "summary": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.", "AI": {"tldr": "本文探讨了AI/ML在鲁宾LSST暗能量科学合作中的应用机会，涵盖多种核心方法和挑战，并指出了未来研究重点。", "motivation": "鉴于LSST将生成海量异构天文学数据，传统的分析方法难以应对。DESC需要统计有力、可扩展且可靠的手段来约束暗能量和暗物质。", "method": "本文调研了AI/ML在DESC主要宇宙学探测中的应用现状，并识别出关键的研究优先方向，如大规模贝叶斯推理、基于物理的方法等。", "result": "文中揭示了核心方法和技术挑战的共性，并讨论了最新的基础模型方法和LLM驱动的自主AI系统潜力及其实施风险。", "conclusion": "本文强调了成功部署新方法所需的软件、计算、数据基础设施及人力资本要求，同时提出了与外部行为者更广泛协调的机会。"}}
{"id": "2601.14234", "pdf": "https://arxiv.org/pdf/2601.14234", "abs": "https://arxiv.org/abs/2601.14234", "authors": ["Qiyang Li", "Sergey Levine"], "title": "Q-learning with Adjoint Matching", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": "32 pages, 8 figures, 7 tables", "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.", "AI": {"tldr": "本文提出了QAM算法，解决了连续动作强化学习中的一个长期挑战。", "motivation": "现有的方法在处理流或扩散策略时面临数值不稳定的问题，而QAM通过使用邻接匹配技术来克服这一问题。", "method": "QAM利用邻接匹配将评论家的动作梯度转换为无须回传的逐步目标函数，并结合时间差备份进行批评者学习。", "result": "QAM在离线和从离线到在线强化学习中，对于硬性稀疏奖励任务始终优于先前的方法。", "conclusion": "邻接匹配技术可以有效解决连续动作策略中的优化挑战，而QAM算法展示了其在强化学习领域的优越性能。"}}
{"id": "2601.14232", "pdf": "https://arxiv.org/pdf/2601.14232", "abs": "https://arxiv.org/abs/2601.14232", "authors": ["Egor Cherepanov", "Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "38 pages, 44 figures, 3 tables", "summary": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.", "AI": {"tldr": "本文介绍了KAGE-Bench，一个用于评估强化学习中已知轴向视觉泛化的基准测试。", "motivation": "现有的基准测试将多种分布变化混在一起，阻碍了系统性分析。作者希望通过此工作解决像素级强化学习代理在纯视觉分布变化下的表现问题，并提供了一个可以独立控制视觉因素的环境。", "method": "构建KAGE-Env平台，该平台可以分离并独立控制观察过程中的视觉轴，同时保持底层控制问题不变。基于这个环境定义了包含34个训练和评估配置对的基准测试套件KAGE-Bench。", "result": "使用PPO-CNN基线模型在KAGE-Bench上进行了实验，发现存在轴依赖性的失败情况，背景变化和光度学变化往往导致性能崩溃，而代理外观的变化影响相对较小。同时发现某些视觉改变会保留前向运动但破坏任务完成。", "conclusion": "该工作提供了一个可以快速重现的基准测试平台，用于评估强化学习在纯视觉分布变化下的泛化能力，揭示了不同类型的视觉变化对模型性能的影响。"}}
{"id": "2601.14230", "pdf": "https://arxiv.org/pdf/2601.14230", "abs": "https://arxiv.org/abs/2601.14230", "authors": ["Yiyang Wang", "Yiqiao Jin", "Alex Cabral", "Josiah Hester"], "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "15 pages, 9 figures", "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.", "AI": {"tldr": "本文介绍了MASCOT框架，旨在解决多智能体系统的个性坍塌和社会逢迎问题，并提升其在心理支持和工作场所中的表现。", "motivation": "该研究的动机在于改善多智能体系统中常见的个性化丧失和社会互动冗余问题，以提供更有效的社会协作伙伴。", "method": "MASCOT采用了一种双层优化策略：个性意识行为对齐（通过RLAIF驱动的管道细化个体代理的行为）和协同对话优化（根据群体奖励指导的元政策来确保多样化和有成效的对话）。", "result": "实验结果显示，MASCOT在心理支持和工作场所领域显著优于现有的基线模型，在个性一致性和社会贡献方面分别提高了14.1和10.6。", "conclusion": "该框架为构建下一代社交智能多智能体系统提供了一个实用的路线图。"}}
{"id": "2601.14227", "pdf": "https://arxiv.org/pdf/2601.14227", "abs": "https://arxiv.org/abs/2601.14227", "authors": ["Theodore Aptekarev", "Vladimir Sokolovsky", "Gregory Furman"], "title": "Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis", "categories": ["cs.SD"], "comment": "7 pages, 4 figures", "summary": "Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata. AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.", "AI": {"tldr": "本文研究了使用Transformer架构进行呼吸音分析和多模态诊断的方法。", "motivation": "传统的听诊法依赖主观判断，本研究旨在通过深度学习方法提高呼吸音分类的准确性和客观性。", "method": "采用了Audio Spectrogram Transformer (AST) 和Vision-Language Model (VLM)，其中AST用于呼吸音分析，VLM将频谱图与结构化患者元数据结合进行诊断。", "result": "AST在哮喘检测中达到了约97%的准确率、F1分数为97%，ROC AUC值为0.98；VLM达到86-87%的准确性，并展示了整合临床背景信息的能力。", "conclusion": "研究结果证实了自注意力机制在声学筛查中的有效性，表明多模态架构有潜力成为全面诊断工具。"}}
{"id": "2601.14212", "pdf": "https://arxiv.org/pdf/2601.14212", "abs": "https://arxiv.org/abs/2601.14212", "authors": ["Daniel Loscos", "Narciso Marti-Oliet", "Ismael Rodriguez"], "title": "Generalization and Completeness of Stochastic Local Search Algorithms", "categories": ["cs.NE", "cs.CL"], "comment": "This paper was published in Swarm and Evolutionary Computation. The present version is the author's accepted manuscript", "summary": "We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.", "AI": {"tldr": "本文将随机局部搜索（SLS）启发式算法泛化为一个独特的形式模型，并证明了该模型的图灵完备性。", "motivation": "研究动机在于构建一个可以涵盖多种具体SLS算法的形式模型，以便能够分析和理解这些算法的本质特性。", "method": "作者提出了一个包含通用结构和参数化结构的模型，通过不同方式实例化参数部分来获得不同的启发式算法，并用此模型证明了Turing完备性。", "result": "该研究成功地将遗传算法（GA）、蚁群优化（ACO）和粒子群优化（PSO）等具体SLS方法纳入统一的形式框架，并证明了SLS类算法的图灵完备性。", "conclusion": "确定任何非平凡性质关于输入与输出之间的关系对于SLS方法是不可判定的，这一结论为理解和进一步研究这类启发式搜索算法提供了理论基础。"}}
{"id": "2601.14209", "pdf": "https://arxiv.org/pdf/2601.14209", "abs": "https://arxiv.org/abs/2601.14209", "authors": ["Matthew Y. R. Yang", "Hao Bai", "Ian Wu", "Gene Yang", "Amrith Setlur", "Aviral Kumar"], "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.", "AI": {"tldr": "论文介绍了干预训练（InT），一种通过自我提出干预来实现大型语言模型（LLM）推理中细粒度信用分配的训练方法。", "motivation": "强化学习中的标准奖励机制仅在最终答案级别进行信用分配，导致正确的中间步骤被不当惩罚或错误步骤受到不准确激励。这种方法存在信用分配问题，需要更好的解决方案来提高LLMs的推理能力。", "method": "论文提出了干预训练（InT），该方法利用参考解决方案和验证模型生成解比从头开始生成更简单的事实，使模型能够识别其推理中的第一个错误，并提出一步干预以引导轨迹向正确答案发展。通过监督微调（SFT）将此干预应用于到发生错误点的策略迭代中。", "result": "使用InT和随后的强化学习微调，论文展示了一个4B参数基模型在IMO-AnswerBench上的准确性提高了近14%，超过了更大的开源模型如gpt-oss-20b。", "conclusion": "干预训练（InT）通过自我提出干预实现了更细粒度的信用分配，并为后续强化学习提供了更好的初始化，显著提升了数学推理任务中的准确性。"}}
{"id": "2601.14208", "pdf": "https://arxiv.org/pdf/2601.14208", "abs": "https://arxiv.org/abs/2601.14208", "authors": ["Nitin Kulkarni", "Akhil Devarashetti", "Charlie Cluss", "Livio Forte", "Dan Buckmaster", "Philip Schneider", "Chunming Qiao", "Alina Vereshchaka"], "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "8 pages, 9 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.", "AI": {"tldr": "本文提出了一种利用三相机装置捕获车辆底盘视频并生成交互式3D模型的端到端流水线，以改善检查效率和买家信心。", "motivation": "检查二手汽车底盘是一个劳动密集型任务，需要检查员在车下仔细检查，而且在线买家很少能看到底盘照片。本文旨在通过技术手段提高工作效率并增强买家信任。", "method": "该方法使用一种基于结构从运动（SfM）的流水线，并结合精确相机校准、同步视频流以及来自摄像头装置的强大几何先验，克服了宽角镜头畸变和低视差场景的问题。使用约束匹配策略、DISK特征提取器和注意力机制LightGlue匹配器生成高质量稀疏点云以供高保真3D模型的渲染。", "result": "实验和消融研究证明本文的设计选择对实现最先进的质量至关重要，该方法能够产生交互式且实时渲染的底盘3D模型。", "conclusion": "本研究通过一种创新的方法解决了车辆底盘检查中面临的具体挑战，提高了检查效率，并为在线买家提供了更好的视觉体验，从而增强了他们对购买的信心。"}}
{"id": "2601.14207", "pdf": "https://arxiv.org/pdf/2601.14207", "abs": "https://arxiv.org/abs/2601.14207", "authors": ["Rotem Gatenyo", "Ohad Fried"], "title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.", "AI": {"tldr": "论文提出了Copy-Trasform-Paste方法，用于基于文本提示和几何约束实现零样本的三维模型对齐。", "motivation": "研究动机在于通过结合语言描述和几何信息来优化三维模型之间的空间关系，以提高内容创作和场景构建的能力。", "method": "该方法利用CLIP驱动的梯度和可微渲染器直接在测试时优化相对姿态，同时使用软ICP项和穿透损失来增加表面接触并减少相互渗透，并设计相机控制以集中于交互区域。", "result": "论文创建了一个包含多种类别关系的数据集进行评估，实验结果表明该方法优于其他基准方法，能够生成语义上准确且物理合理的对齐效果。", "conclusion": "结论指出，通过结合语言和几何约束，所提出的方法在零样本的三维模型对齐任务中表现出色，并为内容创作提供了有力支持。"}}
{"id": "2601.14195", "pdf": "https://arxiv.org/pdf/2601.14195", "abs": "https://arxiv.org/abs/2601.14195", "authors": ["Frederik Glitzner", "David Manlove"], "title": "A Minimax Perspective on Almost-Stable Matchings", "categories": ["cs.GT", "cs.DS"], "comment": "Preliminary version to appear at AAMAS 2026", "summary": "Stability is crucial in matching markets, yet in many real-world settings - from hospital residency allocations to roommate assignments - full stability is either impossible to achieve or can come at the cost of leaving many agents unmatched. When stability cannot be achieved, algorithmicists and market designers face a critical question: how should instability be measured and distributed among participants? Existing approaches to \"almost-stable\" matchings focus on aggregate measures, minimising either the total number of blocking pairs or the count of agents involved in blocking pairs. However, such aggregate objectives can result in concentrated instability on a few individual agents, raising concerns about fairness and incentives to deviate. We introduce a fairness-oriented approach to approximate stability based on the minimax principle: we seek matchings that minimise the maximum number of blocking pairs any agent is in. Equivalently, we minimise the maximum number of agents that anyone has justified envy towards. This distributional objective protects the worst-off agents from a disproportionate amount of instability. We characterise the computational complexity of this notion across fundamental matching settings. Surprisingly, even very modest guarantees prove computationally intractable: we show that it is NP-complete to decide whether a matching exists in which no agent is in more than one blocking pair, even when preference lists have constant-bounded length. This hardness applies to both Stable Roommates and maximum-cardinality Stable Marriage. On the positive side, we provide polynomial-time algorithms when agents rank at most two others, and present approximation algorithms and integer programs. Our results map the algorithmic landscape and reveal fundamental trade-offs between distributional guarantees and computational feasibility.", "AI": {"tldr": "论文提出了一个基于最小最大化原则的近似稳定匹配方法，旨在保护最不利个体免受过多不稳定影响。", "motivation": "在许多现实世界的应用中，完全稳定的匹配可能是不可实现的或会导致许多参与者未被匹配。现有的\"几乎稳定\"匹配方法关注的是整体指标，这可能导致不稳定性集中于少数个体，从而引发公平性和激励偏差问题。", "method": "论文提出了一种基于最小最大化原则的方法来近似稳定性，即寻找一个使任何个体处于阻塞对中的最大数量最小的匹配。这个分布目标保护最不利个体免受过多不稳定影响。", "result": "研究了这种新稳定度量在基本匹配设置下的计算复杂性，并展示了即使是很小的保证也证明是计算上不可行的，例如决定是否存在一个匹配使得没有个体处于超过一个阻塞对中，这在NP完全问题范围内。但也提供了多项式时间算法和近似算法。", "conclusion": "论文揭示了分配保障与计算可行性之间的基本权衡关系，并绘制了该方法的算法图景。"}}
{"id": "2601.14192", "pdf": "https://arxiv.org/pdf/2601.14192", "abs": "https://arxiv.org/abs/2601.14192", "authors": ["Xiaofang Yang", "Lijun Li", "Heng Zhou", "Tong Zhu", "Xiaoye Qu", "Yuchen Fan", "Qianshan Wei", "Rui Ye", "Li Kang", "Yiran Qin", "Zhiqiang Kou", "Daizong Liu", "Qi Li", "Ning Ding", "Siheng Chen", "Jing Shao"], "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "categories": ["cs.AI", "cs.CL"], "comment": "35 pages, 200 references", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "AI": {"tldr": "本文探讨了如何提高智能代理系统的效率，主要从记忆、工具学习和规划三个核心组件入手。", "motivation": "尽管智能代理的效果不断提升，但其在实际部署中的效率问题常常被忽视。文章旨在研究代理系统本身的效率。", "method": "通过回顾近期不同实现方式的方法，总结了高效原则如上下文压缩管理、强化学习奖励设计以减少工具调用、控制搜索机制增强效率等。", "result": "将效率定义为在固定成本预算下的效果比较以及相同水平有效性下成本的对比，并探讨了面向效率的基准测试方法和评价协议。", "conclusion": "文章讨论了关键挑战和未来方向，旨在提供有价值的见解。"}}
{"id": "2601.14188", "pdf": "https://arxiv.org/pdf/2601.14188", "abs": "https://arxiv.org/abs/2601.14188", "authors": ["Liang Shi", "Wei Li", "Kevin M Beussman", "Lin Chen", "Yun Fu"], "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.", "AI": {"tldr": "提出IIR-VLM，一种用于上下文实例级识别的大型视觉语言模型。", "motivation": "尽管现代VLM在视觉感知方面表现出色，但在实例级别识别（ILR）上的表现不尽如人意。现有解决方案成本高且难以区分细粒度对象。", "method": "整合预训练的ILR专家模型作为辅助视觉编码器，提供专业特征以使VLM能够以一次性方式学习新实例，并利用这种知识实现实例感知的视觉理解。", "result": "在现有的实例个性化基准测试中验证了IIR-VLM的有效性，并在一个新的具有挑战性的基准测试上展示了其优越的ILR性能。", "conclusion": "通过引入IIR-VLM，有效提升了VLM在实例级识别任务中的表现，特别是在需要精确识别特定个体或对象的应用场景。"}}
{"id": "2601.14180", "pdf": "https://arxiv.org/pdf/2601.14180", "abs": "https://arxiv.org/abs/2601.14180", "authors": ["Yichao Liu", "Yueyang Teng", "Junwen Guo"], "title": "Progressive self-supervised blind-spot denoising method for LDCT denoising", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.", "AI": {"tldr": "本文提出了一种仅依赖于低剂量CT图像的自监督训练策略，以解决LDCT图像去噪问题。", "motivation": "由于正常剂量CT数据难以在临床实践中获得，该研究旨在减轻对这些配对数据的依赖，并提高LDCT图像的质量。", "method": "引入逐步盲点去噪机制，通过逐步强制条件独立实现更精细的去噪学习，并向LDCT图像添加高斯噪声以作为正则化手段，防止过拟合。", "result": "在Mayo LDCT数据集上的广泛实验表明，该方法始终优于现有的自监督方法，并且性能与几种代表性的监督去噪方法相当甚至更好。", "conclusion": "研究证明了提出的逐步盲点去噪机制和自监督训练策略的有效性，在LDCT图像去噪任务中展示了良好的性能。"}}
{"id": "2601.14175", "pdf": "https://arxiv.org/pdf/2601.14175", "abs": "https://arxiv.org/abs/2601.14175", "authors": ["Suvrat Raju", "Praneeth Netrapalli"], "title": "A model of errors in transformers", "categories": ["cs.LG", "cs.AI", "cs.CL", "hep-th"], "comment": "8+17pages", "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.", "AI": {"tldr": "研究了大型语言模型在需要确定性输出的任务中的错误率，并提出了一个基于两个参数的误差模型。", "motivation": "探讨大型语言模型在处理重复任务时出现错误的原因，提出一种新的解释方法来替代现有的一些关于模型推理崩溃或无法表达组合函数的观点。", "method": "通过有效场理论的方法重新组织模型中的大量原始参数，并推导出一个描述准确性和任务复杂性之间关系的定量两参数模型。使用多个语言模型进行广泛的实证测试以验证该模型。", "result": "发现提出的误差率预测与实际观察到的任务准确性有很好的一致性，但在某些情况下也存在偏差。", "conclusion": "该研究提供了一种新的视角来理解大型语言模型的错误机制，并展示如何通过构建提示来降低错误率。"}}
{"id": "2601.14172", "pdf": "https://arxiv.org/pdf/2601.14172", "abs": "https://arxiv.org/abs/2601.14172", "authors": ["Víctor Yeste", "Paolo Rosso"], "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum", "categories": ["cs.CL", "cs.AI"], "comment": "Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,", "summary": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.", "AI": {"tldr": "论文主要任务是研究在文本句子级别上识别Schwartz动机连续体中的19个人类价值观，使用不同的模型和方法进行检测。", "motivation": "动机在于探讨如何在包含稀疏道德线索的新闻和政治宣言等非上下文句子中实现细粒度的价值观识别，并面对类别不平衡的问题。", "method": "论文采用了二元道德存在任务，构建了基于DeBERTa-base模型并使用轻量级信号增强的方法。此外还对比了门控分层方法与直接多标签分类器的表现，并测试了几种指示调优的大型语言模型在零样本和少样本设置下的效果。", "result": "研究结果显示，尽管存在挑战，通过调整阈值可以实现大约0.74的正类F1分数。轻量级信号和小型集成提供了最可靠的改进，而分层门控提供的收益有限。一个基于软投票的监督模型集合达到了宏平均F1值为0.332。", "conclusion": "在8GB单GPU约束下，仔细调优的监督编码器仍然是一种强大的且计算效率高的结构化人类价值观检测基准，在某些情况下优于大型语言模型的表现。"}}
{"id": "2601.14171", "pdf": "https://arxiv.org/pdf/2601.14171", "abs": "https://arxiv.org/abs/2601.14171", "authors": ["Qianli Ma", "Chang Guo", "Zhiheng Tian", "Siyu Wang", "Jipeng Xiao", "Yuanhao Yue", "Zhipeng Zhang"], "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "categories": ["cs.AI"], "comment": null, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "AI": {"tldr": "本文介绍了一种名为RebuttalAgent的多智能体框架，用于生成基于证据规划的任务中的作者反驳意见。", "motivation": "当前解决方案通常将编写有效反驳视为直接文本生成问题，存在幻觉、忽略批评和缺乏可验证依据的问题。为了克服这些限制，提出了RebuttalAgent来改善这些问题。", "method": "该系统将复杂的反馈分解为原子关注点，并动态构建混合上下文以合成压缩摘要与高保真文本，同时整合自动且按需的外部搜索模块解决需要引用外部文献的关注点。", "result": "在提出的RebuttalBench上验证了该方法，在覆盖面、忠实性和策略连贯性方面优于强基线，展示了作为同行评审过程中的透明和可控助手的优势。", "conclusion": "实验结果表明RebuttalAgent框架能够有效提升反驳意见的质量，通过证据锚定确保每项论点都有据可循。"}}
{"id": "2601.14165", "pdf": "https://arxiv.org/pdf/2601.14165", "abs": "https://arxiv.org/abs/2601.14165", "authors": ["Zhenghong Li", "Wensheng Cheng", "Congwu Du", "Yingtian Pan", "Zhaozheng Yin", "Haibin Ling"], "title": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction", "categories": ["cs.CV"], "comment": "17 pages, 11 figures", "summary": "Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.", "AI": {"tldr": "开发了一种名为ASBA的新网络，用于从稀疏采样的原始A线中重建ODT图像。", "motivation": "现有的高保真度B扫描依赖密集采样，这延长了扫描时间并限制了捕捉快速血液流动的能力。为了克服这些问题，作者提出了一种新的方法来提高稀疏采样的有效性。", "method": "提出了一个A线ROI状态空间模型以提取沿A线的稀疏分布流特征，并引入B线相位注意力机制根据相位差异捕获沿每个B线的长距离流动信号。此外，还引入了一个流动感知加权损失函数来鼓励网络优先准确重建流动信号。", "result": "实验结果显示，在真实的动物数据上，所提出的方法明显优于现有的最先进的重建方法。", "conclusion": "ASBA网络在从稀疏采样的原始A线中有效重建ODT图像方面具有显著优势。"}}
{"id": "2601.14161", "pdf": "https://arxiv.org/pdf/2601.14161", "abs": "https://arxiv.org/abs/2601.14161", "authors": ["Yitong Dong", "Qi Zhang", "Minchao Jiang", "Zhiqiang Wu", "Qingnan Fan", "Ying Feng", "Huaqi Zhang", "Hujun Bao", "Guofeng Zhang"], "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.", "AI": {"tldr": "本文提出了一种高保真新视角合成框架，通过一步式扩散网络解决了基于Vision Transformer的三维高斯散射方法在处理低分辨率输入和维护结构一致性方面的局限性。", "motivation": "为了解决现有基于ViT的3DGS方法因计算成本导致只能处理低分辨率图像的问题以及生成增强方法缺乏3D感知而导致跨视图不一致的问题。", "method": "设计了双域细节感知模块，利用特征引导扩散网络，并采用统一训练策略实现几何骨架和扩散细化模块的联合优化。", "result": "实验表明该方法在多个数据集上可以保持高质量的生成效果。", "conclusion": "本研究提出的方法有效克服了现有3DGS技术的局限性，实现了高保真新视角合成。"}}
{"id": "2601.14160", "pdf": "https://arxiv.org/pdf/2601.14160", "abs": "https://arxiv.org/abs/2601.14160", "authors": ["Ali Hamza Bashir", "Muhammad Rehan Khalid", "Kostadin Cvejoski", "Jana Birr", "Jule Berghaus", "Armin Berger", "Sandra Halscheidt", "Christian Temath", "Rafet Sifa", "David Berghaus"], "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.", "AI": {"tldr": "该论文介绍了通过合成数据对大型语言模型进行领域适应，以提高其在德国法律问答任务上的表现。", "motivation": "由于专家知识的限制，大型语言模型在专业领域如法律推理上常常会产生事实错误或虚构信息。本文旨在解决这一问题，特别是在高风险的知识密集型领域中。", "method": "论文提出了一种新型合成数据生成方法，系统性地从权威的德国法规中直接生产高质量、多样化的法理问答对，并使用自动过滤和参数高效的微调技术进行模型适应。", "result": "实验结果表明，使用本文提出的合成数据集调整后的语言模型在德国法律问题回答任务上的表现显著优于基线模型。", "conclusion": "研究强调了精心设计的合成数据作为高风险知识密集型领域中人工注释的稳健替代方案的可行性。"}}
{"id": "2601.14157", "pdf": "https://arxiv.org/pdf/2601.14157", "abs": "https://arxiv.org/abs/2601.14157", "authors": ["Bruno Sienkiewicz", "Łukasz Neumann", "Mateusz Modrzejewski"], "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.", "AI": {"tldr": "本文介绍了ConceptCaps，一个包含23k音乐-字幕-音频三元组的数据集，并通过验证确认其在音乐模型解释性方面的作用。", "motivation": "现有的音乐数据集缺乏干净、明确的概念正反例样本，而基于概念的可解释性方法（如TCAV）需要这种结构。为了满足这一需求，作者创建了ConceptCaps数据集。", "method": "本文采用VAE学习属性共现模式，并通过微调后的LLM将属性列表转换为专业的描述。最后使用MusicGen合成对应的音频。此过程分为语义建模和文本生成两部分，提高了连贯性和可控性。", "result": "通过音-文对齐（CLAP）、语言质量度量（BERTScore、MAUVE）及TCAV分析验证了数据集的有效性，确认概念探测器能恢复出音乐上具有意义的模式。", "conclusion": "ConceptCaps提供了一个结构化且干净的数据集来支持基于概念的解释方法在音乐模型上的应用，并展示了通过分离语义建模与文本生成可以提高数据的质量和连贯性。"}}
{"id": "2601.14154", "pdf": "https://arxiv.org/pdf/2601.14154", "abs": "https://arxiv.org/abs/2601.14154", "authors": ["Shubham Pandey", "Bhavin Jawade", "Srirangaraj Setlur", "Venu Govindaraju", "Kenneth Seastedt"], "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to P2P-CV @ WACV 2026", "summary": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.", "AI": {"tldr": "本文介绍了MIRACLE，一个用于预测肺癌手术后并发症风险的深度学习架构。", "motivation": "术后并发症严重影响患者预后并增加医疗成本，因此开发准确且透明的风险预测工具成为迫切需求。", "method": "MIRACLE结合术前临床和放射学数据，使用超球嵌入空间融合异构输入，并引入干预性深度学习模块来提高可解释性和实用性。", "result": "实验结果表明，MIRACLE在个性化术后风险管理和解释方面优于传统机器学习模型和大型语言模型。", "conclusion": "研究证明了MIRACLE在预测肺癌手术后并发症方面的有效性及其在临床应用中的潜力。"}}
{"id": "2601.14152", "pdf": "https://arxiv.org/pdf/2601.14152", "abs": "https://arxiv.org/abs/2601.14152", "authors": ["Hyunjong Ok", "Jaeho Lee"], "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.", "AI": {"tldr": "本文研究了语言模型对提示结构敏感性的一个显著案例，发现将上下文放在问题和选项之前（CQO）比相反顺序（QOC）的性能高出超过14%。", "motivation": "大型语言模型表现出对提示结构的惊人敏感度，但其背后的机制尚不清楚。本文旨在揭示这种敏感性的原因。", "method": "通过系统化的架构分析，作者研究了不同提示结构下的多选题回答任务，并重点考察了因果注意力机制的作用。", "result": "研究表明，在QOC格式中，因果掩码阻止选项关注上下文，导致信息瓶颈，而CQO格式则没有这种问题，性能显著提升。", "conclusion": "研究结果表明因果注意力是影响多选题回答任务提示结构敏感性的重要机制，揭示了语言模型在处理不同顺序的信息时存在的局限性。"}}
{"id": "2601.14133", "pdf": "https://arxiv.org/pdf/2601.14133", "abs": "https://arxiv.org/abs/2601.14133", "authors": ["Bin Yu", "Shijie Lian", "Xiaopeng Lin", "Yuliang Wei", "Zhaolong Shen", "Changti Wu", "Yuzhuo Miao", "Xinming Wang", "Bailing Wang", "Cong Huang", "Kai Chen"], "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "categories": ["cs.RO", "cs.CV"], "comment": "GitHub: https://github.com/ZGC-EmbodyAI/TwinBrainVLA", "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.", "AI": {"tldr": "介绍了一种名为TwinBrainVLA的新型架构，通过冻结的大脑保留通用语义理解能力，并训练专门用于身体感知的小脑，以解决机器人控制中的视觉语言和动作模型之间的冲突。", "motivation": "传统的Vision-Language-Action（VLA）模型通常会为机器人的控制任务调整一个单一的Vision-Language Model（VLM），但这会导致高级语义理解和低级传感器运动技能学习之间存在矛盾，甚至出现“灾难性遗忘”问题。TwinBrainVLA旨在解决这一冲突。", "method": "提出了一个新的架构TwinBrainVLA，它协调了一个保持通用语义理解能力的泛化视觉语言模型（Left Brain）和一个专注于身体感知的专业视觉语言模型（Right Brain），并通过新型非对称变换器混合机制（Asymmetric Mixture-of-Transformers, AsyMoT）来实现协同工作。", "result": "在SimplerEnv和RoboCasa基准测试上的广泛实验表明，TwinBrainVLA的操控性能优于最先进的基线模型，并且能够明确地保持预训练视觉语言模型的全面视觉理解能力。", "conclusion": "该设计为构建同时具备高级语义理解和低级物理灵活性的一般性机器人提供了一个有希望的方向。"}}
{"id": "2601.14130", "pdf": "https://arxiv.org/pdf/2601.14130", "abs": "https://arxiv.org/abs/2601.14130", "authors": ["Till Aczel", "David F. Jenny", "Simon Bührer", "Andreas Plesner", "Antonio Di Maio", "Roger Wattenhofer"], "title": "GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression", "categories": ["cs.CV"], "comment": null, "summary": "Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.", "AI": {"tldr": "本文提出了GIC-DLC，一种用于灰度图像压缩的可微逻辑电路硬件友好型编解码器。", "motivation": "传统的神经网络图像编解码器虽然能实现更高的压缩比，但计算开销大，限制了其在能量受限设备上的部署。因此，本文旨在开发一个既能提高压缩效率又能减少能耗和延迟的解决方案。", "method": "GIC-DLC通过训练查找表结合神经网络的灵活性与布尔操作的高效性来实现灰度图像压缩。", "result": "实验表明，GIC-DLC在灰度基准数据集上的性能优于传统的编解码器，并实现了显著的能量消耗和延迟降低。", "conclusion": "研究结果证明了学习型压缩可以是硬件友好的，为边缘设备的低功耗图像压缩提供了一条有希望的发展路线。"}}
{"id": "2601.14128", "pdf": "https://arxiv.org/pdf/2601.14128", "abs": "https://arxiv.org/abs/2601.14128", "authors": ["Shoujie Li", "Changqing Guo", "Junhao Gong", "Chenxin Liang", "Wenhua Ding", "Wenbo Ding"], "title": "SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media", "categories": ["cs.RO"], "comment": "Accepted by IEEE Transactions on Robotics", "summary": "Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac's 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system's practical performance.", "AI": {"tldr": "本文介绍了SandWorm机器人及其SWTac传感器，用于在颗粒介质中进行视觉和触觉感知，并实现了高效的运动与定位。", "motivation": "由于颗粒动态的不可预测性，颗粒介质中的感知仍然具有挑战性。本文旨在解决这一问题，通过提出一种新的事件驱动的视觉触觉传感器来增强机器人的感知能力。", "method": "SandWorm机器人采用螺杆驱动和模仿蠕动运动以提高其在颗粒介质中的移动效率，并配备SWTac传感器，该传感器结合了主动振动、弹簧隔离机制和IMU指导的时间滤波器，实现了高分辨率的触觉成像。", "result": "实验表明，SWTac传感器能够达到0.2毫米的纹理分辨率，98%的石头分类准确率和0.15牛的力估计误差；SandWorm机器人在复杂颗粒介质中展示了高达12.5毫米/秒的速度，并成功执行了管道清理和地下勘探任务。", "conclusion": "SandWorm及其SWTac传感器系统在颗粒介质环境中展示出了高效的运动能力和强大的感知性能，具有广泛的潜在应用价值。"}}
{"id": "2601.14127", "pdf": "https://arxiv.org/pdf/2601.14127", "abs": "https://arxiv.org/abs/2601.14127", "authors": ["Renmiao Chen", "Yida Lu", "Shiyao Cui", "Xuan Ouyang", "Victor Shea-Jay Huang", "Shumin Zhang", "Chengwei Pan", "Han Qiu", "Minlie Huang"], "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": "*15 pages, 5 figures. Introduces MIR-SafetyBench (2,676 instances; 9 multi-image relations). Equal contribution; †Corresponding author. Code/data: https://github.com/thu-coai/MIR-SafetyBench", "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.", "AI": {"tldr": "本文研究了多模态大型语言模型在处理多图像推理任务时的安全风险，并提出了一个专门针对多图像推理安全性的基准测试MIR-SafetyBench。", "motivation": "随着多模态大型语言模型处理复杂多图像指令的能力增强，它们可能面临新的安全隐患。本文旨在研究这种能力提升所带来的潜在安全问题。", "method": "作者创建了包含2676个实例的MIR-SafetyBench基准测试，涵盖了9种多图像关系类别，并对19个多模态大型语言模型进行了广泛评估。", "result": "研究表明，具有更先进多图像推理能力的模型在MIR-SafetyBench上的表现更脆弱，许多看似安全的回答实际上存在误解或避重就轻的问题；不安全生成内容表现出较低的关注熵。", "conclusion": "本文指出，多模态大型语言模型可能过于专注于任务解决而忽视了安全性约束，建议关注模型的安全性问题。"}}
{"id": "2601.14124", "pdf": "https://arxiv.org/pdf/2601.14124", "abs": "https://arxiv.org/abs/2601.14124", "authors": ["Saad Mankarious", "Aya Zirikly"], "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.", "AI": {"tldr": "本文提出了一种基于扩散模型的合成文本生成方法，用于缓解阿拉伯语心理健康分析中的数据稀缺和性别偏差问题。", "motivation": "现有的合成数据方法依赖预训练大型语言模型（LLMs），这些模型可能输出多样性有限并传播训练数据中的偏见。为解决这些问题并增加女性作者内容的数量，本文提出了新的解决方案。", "method": "使用CARMA阿拉伯语心理健康语料库，通过男性到女性的风格转换来生成合成文本，并构建了五个数据集以捕捉不同性别表达的语言和语义方面。每个设置都训练了一个单独的扩散模型来进行偏见缓解。", "result": "定量评估表明，源文本与生成文本在语义一致性上有较高保真度且表面层级风格有所差异，定性分析验证了语言上合理的性别转换。结果证明该方法可以产生高熵、语义忠实的合成数据，无需依赖预训练LLMs。", "conclusion": "扩散模型为基础的风格迁移提供了一种有效和灵活的方法，在资源稀缺的心理健康领域缓解了性别偏差问题。"}}
{"id": "2601.14115", "pdf": "https://arxiv.org/pdf/2601.14115", "abs": "https://arxiv.org/abs/2601.14115", "authors": ["Liangsi Lu", "Jingchao Wang", "Zhaorong Dai", "Hanqian Liu", "Yang Shi"], "title": "Riemannian Liquid Spatio-Temporal Graph Network", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted to The Web Conference 2026", "summary": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io", "AI": {"tldr": "本文提出了Riemannian Liquid Spatio-Temporal Graph Network（RLSTG）框架，结合连续时间液体动态和黎曼流形的几何归纳偏置，用于更准确地建模具有复杂结构的时空图。", "motivation": "液态时变常数网络（LTCs）在模拟不规则采样动力学方面表现出色，但受限于欧几里得空间，无法有效处理真实世界中非欧几里得结构的图，从而影响了表示质量。为解决此问题，引入RLSTG框架。", "method": "RLSTG通过直接在曲面上定义常微分方程来模拟图演化过程，结合黎曼流形的几何特性来捕捉静态和动态时空图的本质几何特征，并提供了严格的理论保证，包括稳定性定理扩展和表达能力量化。", "result": "实验结果表明，在真实世界数据集上，RLSTG通过将先进的时间动力学与黎曼空间表示相结合，表现出优越的性能。", "conclusion": "该研究提出了一种新的框架RLSTG，能够更准确地模拟具有复杂结构的时空图，并提供了理论支持和实验证明了其有效性。"}}
{"id": "2601.14111", "pdf": "https://arxiv.org/pdf/2601.14111", "abs": "https://arxiv.org/abs/2601.14111", "authors": ["Jiaying Wu", "Can Gao", "Jinglu Hu", "Hui Li", "Xiaofeng Cao", "Jingcai Guo"], "title": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D", "AI": {"tldr": "本文提出了PMCE，一种基于概率的少样本学习框架，利用多粒度语义和标题引导增强来提高模型在少样本情况下的表现。", "motivation": "现有的少样本学习方法往往由于数据稀疏导致原型偏差严重，泛化能力差。本文旨在通过引入更丰富的语义信息并优化查询表示来改进现有方法的性能。", "method": "PMCE构建了一个非参数的知识库，存储每个类别的视觉统计和CLIP编码的类别名嵌入。在元测试时，根据类别名称嵌入的相关性检索最相关的基类，并将这些统计数据聚合为特定类别的先验信息并与支持集原型融合。同时，使用冻结的BLIP标题生成器提供无标签实例级别的图像描述，并训练轻量级增强器来优化支持原型和查询特征。", "result": "实验结果表明，PMCE在四个基准测试上持续超越强大的基线，在MiniImageNet上的1-shot设置中甚至比最强的语义竞争对手高出7.71%。", "conclusion": "通过引入多粒度语义信息和标题引导增强，PMCE显著提高了少样本学习任务中的模型性能。"}}
{"id": "2601.14104", "pdf": "https://arxiv.org/pdf/2601.14104", "abs": "https://arxiv.org/abs/2601.14104", "authors": ["Tairan Huang", "Qingqing Ye", "Yulin Jin", "Jiawei Lian", "Yi Wang", "Haibo Hu"], "title": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.", "AI": {"tldr": "本文提出了一种针对现实世界强化学习系统的扩散引导后门攻击框架（DGBA），并在TurtleBot3移动机器人上展示了其有效性。", "motivation": "尽管现有研究在仿真环境中验证了后门攻击，但在实际机器人系统中的效果尚不清楚，尤其是受安全控制管道影响可能导致传统后门攻击失效。本文旨在解决这一问题并探讨现实世界中强化学习的潜在威胁。", "method": "设计小型可打印的视觉补丁触发器，并使用条件扩散模型生成这些补丁以适应现实世界的视觉变化。采用优势中毒策略，仅在训练中的关键决策点注入触发器。", "result": "在TurtleBot3移动机器人上成功演示了目标攻击的同时保持正常的任务性能。", "conclusion": "本文展示了后门攻击在现实世界强化学习系统中的可行性，并提出了一种新的攻击框架来克服安全控制管道的抑制效果。"}}
{"id": "2601.14103", "pdf": "https://arxiv.org/pdf/2601.14103", "abs": "https://arxiv.org/abs/2601.14103", "authors": ["Xiaolu Liu", "Yicong Li", "Qiyuan He", "Jiayin Zhu", "Wei Ji", "Angela Yao", "Jianke Zhu"], "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing", "categories": ["cs.CV"], "comment": "22 pages, 12 figures", "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.", "AI": {"tldr": "本文提出了一个名为Interp3D的新框架，用于生成纹理的3D变形，能够同时保持几何结构的一致性和纹理对齐。", "motivation": "现有技术在处理3D变形时存在忽视纹理或导致语义模糊、结构错位和纹理模糊的问题，因此需要一种新的方法来解决这些问题，并实现更好的结构性一致性、纹理对齐以及在整个转换过程中更强大的鲁棒性。", "method": "Interp3D是一个无训练框架，利用生成先验知识并采用渐进式对准原则以确保几何保真度和纹理连贯。它从条件空间中的语义对齐插值开始，并通过SLAT指导的结构插值来保证结构性一致性，最后通过细粒度的纹理融合传递外观细节。", "result": "作者构建了一个名为Interp3DData的数据集进行评估，并在保真度、过渡平滑性和合理性方面都进行了衡量。实验结果表明，该方法相较于其他现有方法具有显著的优势。", "conclusion": "Interp3D框架通过结合几何结构和纹理处理的新方法，在生成纹理的3D变形过程中取得了比传统方法更好的性能表现，并在多个评估标准上展示了优越性。"}}
{"id": "2601.14101", "pdf": "https://arxiv.org/pdf/2601.14101", "abs": "https://arxiv.org/abs/2601.14101", "authors": ["Emily Kim", "Allen Wu", "Jessica Hodgins"], "title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training. We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures. Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.", "AI": {"tldr": "研究基于课程学习策略提高跨视域动作识别的效率，特别是在从地面视角到空中视角的迁移中。", "motivation": "现有的人体动作识别模型在面对不同视角时表现不佳，特别是难以将训练好的模型迁移到如空中视角这样的新领域。因此，研究旨在探索一种无需使用真实空中数据就能提高模型泛化能力的方法。", "method": "采用了课程学习策略，结合合成的空中视图数据和真实的地面视图数据，在两个阶段进行微调：直接微调方法和分步扩展训练集后再微调的多阶段课程学习方法。评估了这两种方法在REMAG数据集上的表现。", "result": "实验表明，将两种离域数据结合使用优于单个域的数据训练效果，并且两种课程策略均能与简单组合数据集的方法达到相似的精度（Top-1准确率），同时显著提高了训练效率：直接微调法减少了最多37%和30%的迭代次数；多步扩展方法则进一步减少了最多9%和30%的迭代。", "conclusion": "课程学习策略在保持性能的同时，通过提高跨视域动作识别的训练效率，为解决数据视角差异问题提供了一种有效途径。"}}
{"id": "2601.14099", "pdf": "https://arxiv.org/pdf/2601.14099", "abs": "https://arxiv.org/abs/2601.14099", "authors": ["Shi-Shun Chen", "Xiao-Yang Li", "Enrico Zio"], "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Advanced Engineering Informatics 2026, 71, 104337", "summary": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.", "AI": {"tldr": "本文提出了一种基于时间延迟交叉映射的因果特征选择框架，旨在提高软传感器模型在工业过程监测中的准确性和稳定性。", "motivation": "现有方法忽视了因果关系中存在的时间延迟和变量间的相互依赖性问题，导致软传感器模型性能不佳。为了克服这些问题，作者提出了新的方法来处理时间延迟和变量间相互依赖的问题。", "method": "本文提出了一种基于状态空间重建的因果特征选择框架，其中包括时间延迟收敛交叉映射（TDCCM）用于总体因果推断，以及时间延迟部分交叉映射（TDPCM）用于直接因果推断，并提出了一个自动特征选择策略来确定因果阈值和选择因果特征。", "result": "通过两个实际案例研究证明，TDCCM在平均性能上表现最佳，而TDPCM则在最差情况下改善了软传感器的稳定性和性能。", "conclusion": "该框架能够有效处理工业过程中变量的时间延迟和相互依赖性问题，并提高基于因果特征选择的软传感器模型的准确性和稳定性。"}}
{"id": "2601.14096", "pdf": "https://arxiv.org/pdf/2601.14096", "abs": "https://arxiv.org/abs/2601.14096", "authors": ["Benedikt Hartl", "Léo Pio-Lopez", "Chris Fields", "Michael Levin"], "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems", "categories": ["cs.AI"], "comment": "41 pages, 5 figures", "summary": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.", "AI": {"tldr": "本文探讨了自然和人造系统中的认知基本原理，提出通过嵌入空间的重映射与导航来描述问题解决过程。", "motivation": "本文旨在从不同来源、组成和基质的智能体中寻找一致的问题解决原则，并探索自然和人工系统之间的共通机制。", "method": "通过对比生物集体（如单细胞到整个有机体）如何维持稳态和再生结构，以及现代人工智能系统（包括变压器、扩散模型和神经元细胞自动机）如何重映射数据并进行迭代优化来分析。", "result": "提出了一个双原则理论：嵌入空间的重新映射与导航通过迭代误差最小化实现，并且这一理论被认为是独立于基质的认知不变量。", "conclusion": "识别这种共享机制不仅揭示了生物系统和人工模型之间的深刻平行关系，而且提供了一个跨尺度工程智能的统一框架。"}}
{"id": "2601.14091", "pdf": "https://arxiv.org/pdf/2601.14091", "abs": "https://arxiv.org/abs/2601.14091", "authors": ["Hossein Naderi", "Alireza Shojaei", "Lifu Huang", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.", "AI": {"tldr": "研究了使用轻量级单AI和多AI代理系统进行自主建筑机器人任务规划的零样本适应性，并进行了比较研究。", "motivation": "旨在通过基础模型提升未来建筑业中机器人在动态任务中的适应性和通用性，解决高成本和难适应的问题。", "method": "提出了四种模型并使用轻量级开源大语言模型（LLMs）和视觉语言模型（VLMs）实现，包括一个单代理系统和三个多代理团队合作制定机器人行动计划，并在油漆工、安全检查员和地板铺设三种角色中进行评估。", "result": "四代理团队在大多数指标上超越了最先进的GPT-4o，成本却只有其十分之一。三代理和四代理团队显示出了更好的通用性。", "conclusion": "通过讨论代理行为如何影响输出，这项研究深化了对AI团队的理解，并为未来在多样化的非结构化环境中进行的研究提供了支持。"}}
{"id": "2601.14087", "pdf": "https://arxiv.org/pdf/2601.14087", "abs": "https://arxiv.org/abs/2601.14087", "authors": ["Ruichi Han", "Yizhi Chen", "Tong Lei", "Jordi Altayo Gonzalez", "Ahmed Hemani"], "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": "Accepted for oral presentation at the 2026 VLSI Symposium on Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the Ambassador Hotel, Hsinchu, Taiwan", "summary": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.", "AI": {"tldr": "论文提出了一种基于'1'-比特计数的排序单元硬件实现，以减少DNN加速器中的互连功率消耗。", "motivation": "由于数据互连功耗限制了DNN加速器性能，通过'1'-比特计数值对数据进行排序可以降低开关活动从而减轻这一瓶颈。然而，在硬件上的实用排序实现尚未充分探索。", "method": "提出了一种比较自由的排序单元，优化用于CNN，并利用近似计算将人口计数分组到粗粒度桶中以减少硬件面积。", "result": "该设计实现了高达35.4%的硬件面积缩减，同时保持了与精确实现相当的数据重新排序带来的链路功耗降低效果。", "conclusion": "通过近似排序单元的设计，成功在维持性能的同时减少了硬件资源需求和互连功耗。"}}
{"id": "2601.14086", "pdf": "https://arxiv.org/pdf/2601.14086", "abs": "https://arxiv.org/abs/2601.14086", "authors": ["Nattapong Kurpukdee", "Adrian G. Bors"], "title": "Two-Stream temporal transformer for video action classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.", "AI": {"tldr": "本文提出了一种新的两流式变压器视频分类器，用于从内容和光流中提取时空信息以进行动作识别。", "motivation": "运动表示在视频理解中扮演重要角色，并有许多应用。近年来，通过自注意力机制，Transformer网络已被证明在其许多应用中的有效性。本研究旨在引入一种新的两流式变压器模型来改进视频行动分类。", "method": "提出的方法使用了两个流：一个用于内容，另一个用于光流表示运动信息。该模型识别联合光流和时间帧域的自注意力特征，并在Transformer编码器机制中代表它们的关系。", "result": "实验结果表明，所提出的模型在三个著名的视频数据集上提供了出色的分类效果。", "conclusion": "通过引入新的两流式变压器模型，研究成功展示了其在处理动作识别任务时的有效性。"}}
{"id": "2601.14084", "pdf": "https://arxiv.org/pdf/2601.14084", "abs": "https://arxiv.org/abs/2601.14084", "authors": ["Abdurrahim Yilmaz", "Ozan Erdem", "Ece Gokyayla", "Ayda Acar", "Burc Bugra Dagtas", "Dilara Ilhan Erdil", "Gulsum Gencoglan", "Burak Temelkuran"], "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.", "AI": {"tldr": "介绍了一个名为DermaBench的皮肤病学视觉问答基准数据集，该数据集由皮肤科专家标注。", "motivation": "现有的皮肤病图像数据集主要关注于病变识别等图像分类任务，无法全面评估多模态模型在视觉理解、语言关联和临床推理方面的能力。因此需要一个可视问答（VQA）基准来评估这些能力。", "method": "基于Diverse Dermatology Images (DDI) 数据集构建DermaBench，该数据集包含656张来自570位独特患者的临床图像，涵盖Fitzpatrick皮肤类型I至VI。采用层级标注方案，由专家皮肤科医生对每个图像进行标注。", "result": "DermaBench 包含了约14,474个VQA样式的注释，涵盖了诊断、解剖部位、病变形态、分布、表面特征、颜色和图像质量等方面的信息以及开放式叙述性描述。", "conclusion": "DermaBench作为一个由专家皮肤科医生标注的皮肤病学可视问答基准数据集，旨在促进多模态模型在医疗应用中的全面评估和发展。"}}
{"id": "2601.14079", "pdf": "https://arxiv.org/pdf/2601.14079", "abs": "https://arxiv.org/abs/2601.14079", "authors": ["Paul Walker", "James A. D. Gardner", "Andreea Ardelean", "William A. P. Smith", "Bernhard Egger"], "title": "VENI: Variational Encoder for Natural Illumination", "categories": ["cs.CV"], "comment": "Project Repo - https://github.com/paul-pw/veni Project page - https://paul-pw.github.io/veni", "summary": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.", "AI": {"tldr": "本论文提出了一种旋转等变的变分自编码器（VENI），用于建模球面上自然光照环境，解决了现有方法忽视光照环境的旋转等变性和缺乏良好隐空间表示的问题。", "motivation": "现有的逆渲染方法要么忽略了光照环境的球形和旋转等变性质，要么未能提供良好的隐空间表示。本文旨在解决这些问题，以简化逆渲染这一病态问题。", "method": "提出了一种结合矢量神经元视觉变压器（VN-ViT）作为编码器和旋转等变条件神经场作为解码器的旋转等变变分自编码器，并使用SO(2)-等变全连接层将旋转等变性从SO(3)降低到SO(2)，以保留环境映射的SO(2)-等变性。", "result": "实验表明，提出的SO(2)-等变全连接层在本文的SO(2)-等变模型中优于标准矢量神经元，并且所提方法相比现有方法在隐空间插值更平滑，提供了更好的隐空间表示。", "conclusion": "研究表明，通过使用旋转等变的方法，可以更好地建模自然光照环境并提供更好的隐空间特性，从而改进逆渲染技术。"}}
{"id": "2601.14077", "pdf": "https://arxiv.org/pdf/2601.14077", "abs": "https://arxiv.org/abs/2601.14077", "authors": ["Lars C. Reining", "Thabo Matthies", "Luisa Haussner", "Rabea Turon", "Thomas S. A. Wallis"], "title": "MooneyMaker: A Python package to create ambiguous two-tone images", "categories": ["q-bio.NC", "cs.CV"], "comment": null, "summary": "Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.", "AI": {"tldr": "介绍MooneyMaker，一个用于自动生成两色调模糊图像的Python包。", "motivation": "为了替代手动创建Mooney图像的方法，提供一种更加一致和可重复的研究工具。", "method": "开发了MooneyMaker Python包，采用多种技术生成具有初始难以识别特征的两色调图像，并允许用户直接比较不同的生成方法。", "result": "实验表明，使用较低初始识别度的技术生成的图像在展示模板后显示出更高的识别效果。", "conclusion": "通过标准化生成过程，MooneyMaker支持更一致和可重复的视觉感知研究。"}}
{"id": "2601.14069", "pdf": "https://arxiv.org/pdf/2601.14069", "abs": "https://arxiv.org/abs/2601.14069", "authors": ["Nattapong Kurpukdee", "Adrian G. Bors"], "title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.", "AI": {"tldr": "本文提出了一种无监督视频类增量学习方法，通过深度嵌入聚类管理来避免遗忘并提升视频信息的学习效果。", "motivation": "先前的方法依赖于有标签的数据和任务边界的知识，这需要昂贵的人工标注且不切实际。因此，本研究旨在解决无监督的视频类增量学习问题。", "method": "首先使用深度特征提取器网络来获取每个任务下的代表性视频特征，在没有类别或任务信息的情况下逐步构建一系列深度聚类，并在后续任务中转移先前知识。", "result": "通过忽略有标签设置中的标签，对三个标准视频动作识别数据集进行深入评估，包括UCF101、HMDB51和Something-to-Something V2，实验结果表明本方法显著优于其他基线模型。", "conclusion": "该研究表明，提出的无监督视频类增量学习方法可以有效地避免遗忘并在不使用标签的情况下提升性能。"}}
{"id": "2601.14066", "pdf": "https://arxiv.org/pdf/2601.14066", "abs": "https://arxiv.org/abs/2601.14066", "authors": ["Hendrik Möller", "Hanna Schoen", "Robert Graf", "Matan Atad", "Nathan Molinier", "Anjany Sekuboyina", "Bettina K. Budai", "Fabian Bamberg", "Steffen Ringhof", "Christopher Schlett", "Tobias Pischon", "Thoralf Niendorf", "Josua A. Decker", "Marc-André Weber", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke"], "title": "VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences", "categories": ["cs.CV"], "comment": null, "summary": "The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing \"Vertebra Identification with Anomaly Handling\" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.", "AI": {"tldr": "介绍了一种名为VERIDAH的新椎体标注算法，该算法能够自动识别和处理枚举异常。", "motivation": "在临床实践中，椎体的计数异常与慢性背痛及手术计划有关，但现有方法无法自动准确地标记这些异常情况。现有的深度学习基椎体标注算法存在不足，缺乏自动标记枚举异常的方法。", "method": "VERIDAH基于多分类头结合加权椎体序列预测算法实现，适用于不同成像序列的任意视野图像。", "result": "该方法在T2w TSE矢状面和CT成像上的表现优于现有模型，在正确标注所有椎体方面分别达到98.30%和99.18%，且能准确识别胸椎和腰椎枚举异常，分别为87.80%-96.30%和94.48%-97.22%。", "conclusion": "VERIDAH在多种成像序列中均表现出色，成功解决了自动标注椎体计数异常的问题。"}}
{"id": "2601.14063", "pdf": "https://arxiv.org/pdf/2601.14063", "abs": "https://arxiv.org/abs/2601.14063", "authors": ["Mohsinul Kabir", "Tasnim Ahmed", "Md Mezbaur Rahman", "Shaoxiong Ji", "Hassan Alhuzali", "Sophia Ananiadou"], "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "30 Pages, 13 Figures", "summary": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.", "AI": {"tldr": "介绍了一个新的基准测试XCR-Bench，用于评估大型语言模型在跨文化推理任务中的表现。", "motivation": "现有的评测方法受限于缺乏高质量的文化特有项目标注语料库和对应的平行句子对。为了克服这一限制，作者提出了XCR-Bench来系统地分析大型语言模型在识别和适应不同文化元素方面的能力。", "method": "XCR-Bench包含4900个平行句子和1098个独特的文化特有项目（CSIs），跨越了三个不同的推理任务，并引入了新的评测指标。该语料库结合了Newmark的CSI框架与Hall的文化三元组理论，可以分析深层次的文化元素。", "result": "研究表明最先进的大型语言模型在识别和适应社交礼仪及文化参考方面的表现存在一致性弱点，并且即使在一个单一的语言环境中也编码了一些区域性和民族宗教偏见。", "conclusion": "XCR-Bench提供了一个新的评测基准来研究跨文化的自然语言处理问题，揭示了当前大型语言模型存在的不足之处。"}}
{"id": "2601.14060", "pdf": "https://arxiv.org/pdf/2601.14060", "abs": "https://arxiv.org/abs/2601.14060", "authors": ["Yongcong Ye", "Kai Zhang", "Yanghai Zhang", "Enhong Chen", "Longfei Li", "Jun Zhou"], "title": "Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.", "AI": {"tldr": "本文提出了一种细粒度零样本合成图像检索方法CVSI，该方法通过互补的视觉-语义集成来提升图像检索的效果。", "motivation": "现有的零样本合成图像检索方法在捕捉细微变化和整合视觉与语义信息方面存在不足，作者旨在解决这些问题，以提高检索精度。", "method": "CVSI包含三个关键组件：（1）视觉信息提取，既提取全局图像特征又转换为伪令牌。（2）语义信息提取，使用预训练的标注模型生成多个描述，并利用大语言模型生成修改后的描述和可能添加的对象。（3）互补信息检索，整合查询与数据库中的信息以实现目标图像的高效检索。", "result": "实验表明，在三个公开数据集上的测试结果证明CVSI显著超越了现有的最先进方法。", "conclusion": "作者通过提出CVSI方法展示了在细粒度零样本合成图像检索领域取得的新进展，该方法在多个基准数据集上取得了优异的性能。"}}
{"id": "2601.14056", "pdf": "https://arxiv.org/pdf/2601.14056", "abs": "https://arxiv.org/abs/2601.14056", "authors": ["Andrea Rigo", "Luca Stornaiuolo", "Weijie Wang", "Mauro Martino", "Bruno Lepri", "Nicu Sebe"], "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.", "AI": {"tldr": "本论文提出了一种基于扩散模型的文本到图像生成方法 POCI-Diff，能够实现一致且交互式的3D布局控制和编辑。", "motivation": "现有的改进空间一致性使用2D线索或迭代复制-粘贴策略的方法往往会扭曲几何形状，并在多次编辑中无法保持一致性。因此，本论文旨在解决这些问题并提供一种新的解决方案。", "method": "POCI-Diff 通过将3D几何约束和实例级语义绑定统一到扩散过程中来实现目标定位的一致性和交互性。该方法使用Blended Latent Diffusion允许对每个对象进行明确的语义控制，并提出了一种无需变形的生成编辑流程，以支持对象插入、删除和转换等操作。", "result": "实验结果表明，POCI-Diff 能够产生高质量且与指定3D布局一致的图像，在视觉保真度和布局一致性方面优于现有方法，同时消除了由变形引起的几何畸变。", "conclusion": "本论文提出的方法在保持对象身份和编辑间的一致性方面表现出色，并能够生成符合复杂多物体场景要求的高质量图像。"}}
{"id": "2601.14055", "pdf": "https://arxiv.org/pdf/2601.14055", "abs": "https://arxiv.org/abs/2601.14055", "authors": ["Andrea Protani", "Marc Molina Van Den Bosch", "Lorenzo Giusti", "Heloisa Barbosa Da Silva", "Paolo Cacace", "Albert Sund Aillet", "Miguel Angel Gonzalez Ballester", "Friedhelm Hummel", "Luigi Serio"], "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures,", "summary": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.", "AI": {"tldr": "本文介绍了SVGFormer，一种无解码器的管道，在多模态MRI上实现准确脑肿瘤定位。", "motivation": "传统的3D医学成像模型依赖于参数繁重的编码-解码结构，这种设计将大量的参数用于空间重建而非特征学习。因此，作者提出了一种新的方法来集中所有可学能力在特征编码上，并提供从补丁到区域级别的内在解释性。", "method": "SVGFormer采用内容感知分组阶段将体积分割成超体素的语义图，并结合了基于补丁层次的Transformer和基于超体素层次的图注意力网络，联合建模细粒度区域内特征与更广泛的跨区依赖关系。", "result": "在BraTS数据集上训练两种专用模型：一个用于节点级别的分类，另一个用于肿瘤比例回归，分别实现了0.875的F1分数和0.028的MAE。", "conclusion": "结果表明基于图的编码器仅范式为3D医学图像表示提供了一个准确且内在可解释的替代方案。"}}
{"id": "2601.14053", "pdf": "https://arxiv.org/pdf/2601.14053", "abs": "https://arxiv.org/abs/2601.14053", "authors": ["Badri N. Patro", "Vijay S. Agneeswaran"], "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "eess.IV"], "comment": null, "summary": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.", "AI": {"tldr": "本文介绍了LLMOrbit，一个涵盖从2019年到2025年的大型语言模型的全面循环分类法。", "motivation": "随着AI领域的发展，从基础Transformer架构到接近人类水平性能的推理能力系统，有必要研究和理解这些模型之间的联系及发展趋势。", "method": "通过八个相互关联的轨道维度，分析了超过50种模型及其创新、训练方法和效率模式。", "result": "识别出三个关键危机（数据稀缺、成本增长、能源消耗）并提出六种突破极限的方法：测试时间计算、量化压缩、分布式边缘计算、模型合并、高效训练以及小型专业化模型。", "conclusion": "通过分析技术演变，从被动生成到工具使用代理，并提供对后训练创新的洞察，揭示了三个范式转变：后培训增益、效率革命和民主化。"}}
{"id": "2601.14052", "pdf": "https://arxiv.org/pdf/2601.14052", "abs": "https://arxiv.org/abs/2601.14052", "authors": ["Haoran Xu", "Yanlin Liu", "Zizhao Tong", "Jiaze Li", "Kexue Fu", "Yuyang Zhang", "Longxiang Gao", "Shuaiguang Li", "Xingyu Li", "Yanran Xu", "Changwei Wang"], "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.", "AI": {"tldr": "提出了一种新的多模态出分布检测方法MM-OOD，利用大规模语言模型的多轮对话和跨模态推理能力来提高近域和远域出分布样本的检测效果。", "motivation": "现有的零样本出分布检测方法主要依赖于文本空间的知识，忽视了在图像空间中检测出分布样本的固有挑战。因此，提出了MM-OOD以改善这一不足。", "method": "对于近域任务直接使用ID图像和对应的文字提示输入MLLMs；远域任务采用草图生成细化框架：通过文字提示进行异常暴露草图绘制，然后生成对应的视觉出分布样本，并最终利用跨模态提示进行细化。", "result": "实验表明该方法在广泛使用的多模态数据集上（如Food-101）实现了显著的性能提升，并且验证了其在ImageNet-1K上的可扩展性。", "conclusion": "MM-OOD通过利用大规模语言模型和跨模态推理能力，有效提高了出分布检测任务的表现，尤其是在近域和远域样本中。"}}
{"id": "2601.14051", "pdf": "https://arxiv.org/pdf/2601.14051", "abs": "https://arxiv.org/abs/2601.14051", "authors": ["Peter Devine", "Mardhiyah Sanni", "Farid Adilazuarda", "Julieta Gil Loizaga", "Barry Haddow"], "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.", "AI": {"tldr": "Kakugo 是一种新型且成本效益高的管道，用于仅使用语言名称作为输入来训练低资源语言的一般用途小型语言模型。", "motivation": "该论文旨在解决低资源语言缺乏足够的数据和计算资源的问题，通过低成本的方法为这些语言开发AI技术。", "method": "Kakugo 使用一个大型教师模型生成合成提示并翻译指令数据集以产生训练数据，并利用这些数据训练小型语言模型。", "result": "在包括翻译、分类和问答在内的多种自然语言处理任务上进行的评估表明，该管道始终提高了基本模型的表现。", "conclusion": "Kakugo 提供了一个成本不到50美元/种语言的方法，让社区能够为特定语言开发AI技术。"}}
{"id": "2601.14047", "pdf": "https://arxiv.org/pdf/2601.14047", "abs": "https://arxiv.org/abs/2601.14047", "authors": ["Alexey V. Osipov", "Nikolay N. Osipov"], "title": "Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure", "categories": ["cs.GT", "cs.AI", "cs.MA", "cs.SI", "econ.TH"], "comment": "21 pages", "summary": "Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.", "AI": {"tldr": "本文提出了一种基于自我解决的虚拟货币预测市场与聊天系统相结合的机制，以促进专家群体对科学问题进行集体分析和信息共享。", "motivation": "动机在于通过一种简单有效的方法，在专家之间直接获取并汇总复杂的、不可预知的私人信息，即使这些专家彼此不了解对方的信息结构也能达到高效的集体智慧应用。", "method": "本文提出了一种机制，该机制结合了自我解决的虚拟货币预测市场和聊天系统来实现专家之间的私有信息共享和交易。", "result": "结果显示这种机制能够引导参与者在聊天中直接分享他们的私人信息，并且根据假设的真实性进行交易，从而有效地汇总相关信息。", "conclusion": "结论是该方法可以有效地聚合来自不同专家的复杂科学问题的相关信息，即使无法确定真相或专家之间缺乏了解，仍然能以一种完全可解释的方式实现集体智慧的应用，同时通过奖励机制促进大规模的合作研究。"}}
{"id": "2601.14046", "pdf": "https://arxiv.org/pdf/2601.14046", "abs": "https://arxiv.org/abs/2601.14046", "authors": ["Shikhar Bharadwaj", "Chin-Jou Li", "Yoonjae Kim", "Kwanghee Choi", "Eunjung Yeo", "Ryan Soh-Eun Shim", "Hanyu Zhou", "Brendon Boldt", "Karen Rosero Jacome", "Kalvin Chang", "Darsh Agrawal", "Keer Xu", "Chao-Han Huck Yang", "Jian Zhu", "Shinji Watanabe", "David R. Mortensen"], "title": "PRiSM: Benchmarking Phone Realization in Speech Models", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.", "AI": {"tldr": "本论文介绍了PRiSM，一个用于评估语音模型中电话识别系统的新基准测试工具。", "motivation": "当前的电话识别（PR）系统的评估仅关注表面转录准确性，缺乏深入的性能评估。因此，需要一种新的方法来揭示这些系统在语音感知方面的盲点。", "method": "引入了PRiSM作为第一个开源基准测试平台，通过内部和外部评估手段来检测电话识别系统的性能，并标准化转录基础评估，同时对临床、教育和多语言环境中的下游效用进行评估。", "result": "发现多样化的语言暴露在训练过程中是提高PR性能的关键因素；编码器-CTC模型最为稳定；专业设计的PR模型仍优于大型音频语言模型。", "conclusion": "PRiSM发布代码、配方和数据集，推动了多语言语音模型向具有强大电话识别能力的方向发展。"}}
{"id": "2601.14044", "pdf": "https://arxiv.org/pdf/2601.14044", "abs": "https://arxiv.org/abs/2601.14044", "authors": ["Kaiyu Wu", "Pucheng Han", "Hualong Zhang", "Naigeng Wu", "Keze Wang"], "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology", "categories": ["cs.CV"], "comment": null, "summary": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.", "AI": {"tldr": "本文提出了Weather-R1，一种逻辑一致的强化微调方法，用于解决气象学领域的多模态推理问题。", "motivation": "主流的强化微调技术在应用到气象学时存在领域差距和推理一致性问题，特别是在高风险的应用场景中。", "method": "作者构建了WeatherQA，一个新的气象学推理基准，并提出了一种逻辑一致的强化微调方法（LoCo-RFT），以解决自我矛盾的推理问题。", "result": "实验表明，Weather-R1在WeatherQA上比基线提高了9.8个百分点，超越了监督微调和传统的强化微调技术。", "conclusion": "结果证明所提出的逻辑一致强化微调方法及其应用模型Weather-R1的有效性和优越性。"}}
{"id": "2601.14042", "pdf": "https://arxiv.org/pdf/2601.14042", "abs": "https://arxiv.org/abs/2601.14042", "authors": ["Jiaze Li", "Haoran Xu", "Wanyi Wu", "Changwei Wang", "Shuaiguang Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Youyang Qu", "Longxiang Gao", "Xudong Yang", "Lumin Xing"], "title": "Federated Balanced Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.", "AI": {"tldr": "本文提出了Federated Balanced Learning（FBL）以解决非独立同分布设置下联邦学习中的客户端漂移问题。", "motivation": "在非独立同分布的环境中，现有方法往往通过基于损失函数或梯度的方法来纠正已经偏移的全局模型，而忽略了客户端样本的影响。因此，本文重新思考了客户端的角色，并提出FBL从一开始就通过客户端的样本平衡来预防该问题。", "method": "技术上，FBL允许在固定数量的数据样本限制下，通过边缘侧生成模型的知识填充和知识采样实现客户端不平衡数据的样本平衡。同时设计了一个知识对齐策略来弥合合成数据与真实数据之间的差距，并采用了知识下降策略进行方法正则化。", "result": "实验结果表明，该方法优于现有最先进的基线方法。", "conclusion": "本文提出了FBL框架，通过客户端的样本平衡有效解决了非独立同分布设置下的联邦学习中的客户端漂移问题，且在实证研究中表现出色。"}}
{"id": "2601.14041", "pdf": "https://arxiv.org/pdf/2601.14041", "abs": "https://arxiv.org/abs/2601.14041", "authors": ["Yunhe Wang", "Kai Han", "Huiling Zhen", "Yuchuan Tian", "Hanting Chen", "Yongbing Huang", "Yufei Cui", "Yingte Shu", "Shan Gao", "Ismail Elezi", "Roy Vaughan Miles", "Songcen Xu", "Feng Wen", "Chao Xu", "Sinan Zeng", "Dacheng Tao"], "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.", "AI": {"tldr": "本文识别了扩散语言模型（DLMs）面临的十大挑战，并提出了一个战略路线图，以推动其发展。", "motivation": "尽管自回归（AR）架构在文本生成方面取得成功，但它们的因果瓶颈限制了全局结构洞察和迭代改进。因此，探讨如何克服这些障碍并充分发挥扩散语言模型潜力是本文的动力。", "method": "文章通过识别十个基本挑战，并提出一个由四个支柱组成的路线图来推动DLMs的发展：基础架构、算法优化、认知推理和统一多模态智能。", "result": "提出了向扩散原生生态系统转变的策略，包括多尺度标记化、主动重掩蔽和潜在思考，以克服因果视界的限制。", "conclusion": "作者认为转向扩散模型是开发下一代AI的关键步骤，这种AI具备复杂结构推理能力、动态自我修正能力和无缝多模态集成能力。"}}
{"id": "2601.14039", "pdf": "https://arxiv.org/pdf/2601.14039", "abs": "https://arxiv.org/abs/2601.14039", "authors": ["Wesam Moustafa", "Hossam Elsafty", "Helen Schneider", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.", "AI": {"tldr": "本文介绍了一种通用的、模块化的弃权框架，用于增强医疗图像分割中损失函数的抗噪声能力。", "motivation": "由于手动注释困难导致标签噪音问题在医疗图像分割任务中尤为严重，影响模型性能。现有的方法在这领域的效果有限，因此引入了弃权机制来应对这一挑战。", "method": "提出一种包含知情正则化项和基于幂律的自调优算法的弃权框架，并将其与三种不同的损失函数结合，生成GAC、SAC和ADS三种新的抗噪音变体。", "result": "在CaDIS和DSAD数据集上的实验表明，该方法在高噪声水平下显著优于无弃权基线模型。", "conclusion": "研究表明，使模型能够选择性忽略被污染的样本是一种强大的、可推广的方法，有助于构建更可靠的分割模型。"}}
{"id": "2601.14038", "pdf": "https://arxiv.org/pdf/2601.14038", "abs": "https://arxiv.org/abs/2601.14038", "authors": ["Alexandre Justo Miro", "Ludvig af Klinteberg", "Bogdan Timus", "Aron Asefaw", "Ajinkya Khoche", "Thomas Gustafsson", "Sina Sharif Mansouri", "Masoud Daneshtalab"], "title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted to The IEEE/CVF Winter Conference on Applications of Computer Vision 2026", "summary": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.", "AI": {"tldr": "本文的主要任务是纠正和量化自动驾驶中3D框注释的系统性错误，以提高监督学习和性能评估的准确性。", "motivation": "在动态场景下，由于物体在不同时间戳的不同位置被观察到，基于LiDAR数据进行3D框标注存在挑战。这些未解决的现象可能导致标注中的系统误差，因此需要一种方法来纠正这些错误，并量化其影响。", "method": "本文提出了一种新颖的离线估计方法，用于纠正注释，使其遵循物理上可行的轨迹并实现与传感器数据的空间和时间一致性。同时定义了新的评估指标来衡量此问题。", "result": "该方法在Argoverse 2、MAN TruckScenes和作者自己的专有数据集上的实验表明，3D框注释的质量提高了超过17%。此外，发现原始注释的偏差可达2.5米，动态物体受影响最大。", "conclusion": "准确的标注对于正确理解自动驾驶系统的性能至关重要，而本文的方法能够显著改善现有数据集中的注释问题，从而有助于更公正地评估系统性能。"}}
{"id": "2601.14037", "pdf": "https://arxiv.org/pdf/2601.14037", "abs": "https://arxiv.org/abs/2601.14037", "authors": ["Kumar Ashutosh", "XuDong Wang", "Xi Yin", "Kristen Grauman", "Adam Polyak", "Ishan Misra", "Rohit Girdhar"], "title": "Human detectors are surprisingly powerful reward models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.", "AI": {"tldr": "本文提出了一种简单但有效的奖励模型HuDA，用于改进生成视频中的人类动作质量。", "motivation": "当前的视频生成模型在处理复杂的非刚性运动时（如人类动态行为）存在不足，导致合成视频出现肢体缺失或扭曲、不真实的动作等问题。因此，本文旨在通过一种简单的奖励模型来解决这些问题并提升视频生成的质量。", "method": "提出名为HuDA的奖励模型，该模型结合了人体检测置信度以评估外观质量，并使用时间提示对齐分数捕捉动作的真实性。将此简单奖励函数用于无额外训练的现成模型中，并通过Group Reward Policy Optimization (GRPO) 对视频生成模型进行后训练优化。", "result": "利用HuDA显著提升了复杂人类运动的视频生成效果，胜率达到了73%，优于现有最先进的模型如Wan 2.1。此外，还展示了HuDA在动物视频和人-物交互场景中的改进效果。", "conclusion": "简单且无需额外训练的奖励模型HuDA能有效提升视频中人类动作的质量，并显示出其对其他类别生成内容（如动物）的积极作用。"}}
{"id": "2601.14030", "pdf": "https://arxiv.org/pdf/2601.14030", "abs": "https://arxiv.org/abs/2601.14030", "authors": ["Samuel W. Remedios", "Zhangxing Bian", "Shuwen Wei", "Aaron Carass", "Jerry L. Prince", "Blake E. Dewey"], "title": "Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\\times/8\\times/16\\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.", "AI": {"tldr": "本文提出了一种适用于多图像MRI超分辨率的扩散模型方法，通过改进现有单图像逆问题求解器，实现了无需重新训练模型即可进行高效的多图像超级解析。", "motivation": "现有的大多数扩散模型专注于解决单一图像的逆问题，但在如MRI等成像模态中，获取的是多个互补的低分辨率测量结果。因此，需要一种能够处理这些多图像数据的方法来实现更准确和高效的超分辨率重建。", "method": "本文提出了一种基于扩散概率模型（DPS）的概率修正方法，允许独立获得的测量值之间进行精确可分离梯度分解，从而实现了不需构建联合算子或修改扩散模型即可完成多图像超级解析。同时，还推导了MISR版本的DPS、DMAP、DPPS以及基于扩散PnP/ADMM的方法。", "result": "实验结果表明，在4倍至16倍各向异性降级情况下，所提出的方法在超分辨率性能上优于单图像超级解析方法，并且实现了对高维MRI体积的最先进的各向同性解剖重建。", "conclusion": "本文通过改进扩散模型解决了多图像MRI超级解析问题，不仅提高了超分辨率效果，还能够从常规二维多切片采集数据中恢复近似各向同性的解剖结构，显著提升了医学影像处理水平。"}}
{"id": "2601.14027", "pdf": "https://arxiv.org/pdf/2601.14027", "abs": "https://arxiv.org/abs/2601.14027", "authors": ["Junqi Liu", "Zihao Zhou", "Zekai Zhu", "Marco Dos Santos", "Weikun He", "Jiawei Liu", "Ran Wang", "Yunzhou Xie", "Junqiao Zhao", "Qiufeng Wang", "Lihong Zhi", "Jia Li", "Wenda Li"], "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "AI": {"tldr": "介绍了一种开放且通用的代理推理系统Numina-Lean-Agent，用于形式数学。", "motivation": "旨在解决现有形式证明中依赖任务特定管道和训练模型的问题，提供更灵活、可重复的形式化推理工具。", "method": "采用一般编码代理作为形式数学推理的基础，结合Claude Code与Numina-Lean-MCP实现对Lean的自主交互、相关定理检索及非正式证明等辅助推理工具。", "result": "使用Claude Opus 4.5作为基础模型，Numina-Lean-Agent解决了Putnam 2025中的所有问题（12/12），并与最佳封闭源系统持平；进一步展示了其通用性，通过与数学家合作成功形式化了Brascamp-Lieb定理。", "conclusion": "提出了一种基于一般编码代理的形式推理新范式，并验证了Numina-Lean-Agent在解决形式数学问题上的有效性和通用性。"}}
{"id": "2601.14026", "pdf": "https://arxiv.org/pdf/2601.14026", "abs": "https://arxiv.org/abs/2601.14026", "authors": ["Vugar Ismailov"], "title": "Universal Approximation Theorem for Input-Connected Multilayer Perceptrons", "categories": ["cs.LG", "cs.NE", "math.FA"], "comment": "18 pages, 2 figures, 31 references", "summary": "We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\\mathbb{R}^n$.", "AI": {"tldr": "本文引入了输入连接多层感知器（IC-MLP），并证明了在特定条件下，深度IC-MLP可以近似任何连续函数。", "motivation": "研究一种新型的神经网络架构IC-MLP，并验证其普遍逼近能力，以扩展传统MLP的能力。", "method": "通过引入直接从原始输入到每个隐藏层神经元的仿射连接，构建了IC-MLP模型，并在单变量和多变量输入条件下进行分析。", "result": "证明了深度IC-MLP可以在特定激活函数下近似任意连续函数，并给出了相应的普遍逼近定理。", "conclusion": "IC-MLP架构通过引入从原始输入到隐藏层的直接连接，增强了模型的逼近能力，在单变量和多变量条件下均能实现对连续函数的有效逼近。"}}
{"id": "2601.14022", "pdf": "https://arxiv.org/pdf/2601.14022", "abs": "https://arxiv.org/abs/2601.14022", "authors": ["Rodrigo Pereira David", "Luciano Araujo Dourado Filho", "Daniel Marques da Silva", "João Alfredo Cal-Braz"], "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.", "AI": {"tldr": "本文提出了一种基于机器学习的方法，用于在相同的真实驾驶条件下比较ICEVs和EVs的二氧化碳排放。", "motivation": "减少道路交通领域的碳排放需要一致且透明的方法来对比不同车辆技术的二氧化碳排放。通过消除驱动条件的影响，可以公平地评估不同动力系统的性能。", "method": "本文使用了循环神经网络模型，分别针对每种车型训练模型以学习从驾驶变量（如速度、加速度和温度）到内部操作变量（如扭矩和油门）以及瞬时二氧化碳等效排放率的映射。", "result": "该方法能够构建反事实场景，评估如果电动汽车遵循与内燃机车辆相同的驾驶轮廓会产生的排放。通过这种方式，可以对不同动力系统进行公平且可重复的评价。", "conclusion": "本文提出的框架提供了一个可靠的、基于数据驱动的基础，用于在真实操作条件下评估车辆的碳性能。"}}
{"id": "2601.14012", "pdf": "https://arxiv.org/pdf/2601.14012", "abs": "https://arxiv.org/abs/2601.14012", "authors": ["Youngmoon Jung", "Myunghun Jung", "Joon-Young Yang", "Yong-Hyeok Lee", "Jaeyoung Roh", "Hoon-Young Cho"], "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting", "categories": ["eess.AS", "cs.AI"], "comment": "5 pages, 1 figure, Accepted at ICASSP 2026", "summary": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.", "AI": {"tldr": "本文提出了MATE，一种用于开放词汇关键词检测的双编码器框架。", "motivation": "传统的固定短语触发方式不够灵活，而基于文本注册的开放式词汇关键词检测更加灵活。", "method": "MATE通过嵌套子嵌入（前缀）在单个向量中表示多个嵌入粒度，并利用PCA引导的前缀对齐来优化音频和文本前缀的训练。", "result": "MATE在WSJ和LibriPhrase数据集上取得了最先进的结果，且没有推理开销。", "conclusion": "本文提出的方法是首个将套娃式嵌入应用于关键词检测，并展示了其优越性。"}}
{"id": "2601.14000", "pdf": "https://arxiv.org/pdf/2601.14000", "abs": "https://arxiv.org/abs/2601.14000", "authors": ["Junwoo Chang", "Joseph Park", "Roberto Horowitz", "Jongmin Lee", "Jongeun Choi"], "title": "Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior", "categories": ["cs.RO", "cs.LG"], "comment": "14 pages, 6 figures", "summary": "Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.", "AI": {"tldr": "本文提出了一种名为Group-Invariant Skill Discovery (GISD) 的框架，该框架将群结构嵌入到技能发现目标中，以提高在物理环境中行为的基本单位的探索效率和可泛化性。", "motivation": "现有的无监督技能发现方法通常忽略了物理环境中的几何对称性，导致冗余的行为和样本低效利用。为解决这一问题，本文提出了GISD框架。", "method": "GISD框架通过引入群不变Wasserstein依赖度量，并使用群傅里叶表示参数化评分函数来确保技能在群变换下的系统泛化。该方法基于理论保证，即在群对称环境中，标准的Wasserstein依赖度量可以实现最优解。", "result": "实验表明，在状态和像素级运动基准测试中，GISD能够覆盖更广泛的州空间，并且比强基线在下游任务学习中的效率更高。", "conclusion": "通过将群结构嵌入到技能发现目标中，本文的方法提高了无监督行为基本单位的探索能力和泛化能力，证明了其在物理环境中的有效性和高效性。"}}
{"id": "2601.13999", "pdf": "https://arxiv.org/pdf/2601.13999", "abs": "https://arxiv.org/abs/2601.13999", "authors": ["Youngmoon Jung", "Joon-Young Yang", "Ju-ho Kim", "Jaeyoung Roh", "Chang Woo Han", "Hoon-Young Cho"], "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification", "categories": ["eess.AS", "cs.AI"], "comment": "5 pages, 2 figures, Accepted at ICASSP 2026", "summary": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.", "AI": {"tldr": "本文提出了一种名为DAME的时长感知嵌套嵌入框架，用于时长鲁棒性说话人验证。", "motivation": "现有方法在短语音段中的说话人判别线索有限的情况下难以实现短语音说话人验证，现有的嵌入学习策略未能适应不同持续时间的信息量。", "method": "DAME是一种模型无关的框架，构建了一个与语音持续时间对齐的子嵌套层次结构：低维表示从短语音捕获紧凑的说话人性状，而高维度编码更丰富的细节。该方法支持从头训练和微调。", "result": "在VoxCeleb1-O/E/H和VOiCES评估数据集上，DAME在1秒和其他短时长试验中持续降低了等错误率，并保持了全长度表现且无额外推理成本。这些增益适用于各种说话人编码器架构，在一般训练和微调设置下均适用。", "conclusion": "DAME框架有效提升了短语音段的说话人验证性能，同时维持了较长语音段的表现。"}}
{"id": "2601.13994", "pdf": "https://arxiv.org/pdf/2601.13994", "abs": "https://arxiv.org/abs/2601.13994", "authors": ["Mingyuan Chi"], "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.", "AI": {"tldr": "介绍了\\torchsla{}，一个开放源代码的PyTorch库，用于GPU加速、可扩展和差异化稀疏线性代数。", "motivation": "工业科学计算主要使用稀疏矩阵来表示非结构化数据，如有限元网格、图和点云。该论文旨在解决GPU加速稀疏线性求解、多GPU扩展以及基于伴随的微分等基本挑战。", "method": "提出\\torchsla{}库，支持多个后端（SciPy, cuDSS, PyTorch-native），实现GPU加速的稀疏线性代数运算，并通过域分解和边界交换技术实现多GPU扩展。同时，实现了基于伴随的微分以优化计算图节点和内存。", "result": "\\torchsla{}库能够达到400百万自由度的线性求解并在3个GPU上运行，且在PyTorch autograd中无缝集成，支持端到端差异化模拟。", "conclusion": "通过\\torchsla{}库，实现了稀疏线性代数的高效计算与差异化处理，在多GPU环境中达到显著性能提升。"}}
{"id": "2601.13992", "pdf": "https://arxiv.org/pdf/2601.13992", "abs": "https://arxiv.org/abs/2601.13992", "authors": ["Jin Cui", "Jiaqi Guo", "Jiepeng Zhou", "Ruixuan Yang", "Jiayi Lu", "Jiajun Xu", "Jiangcheng Song", "Boran Zhao", "Pengju Ren"], "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework", "categories": ["cs.CL", "cs.AI"], "comment": "11pages, 9figures", "summary": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.", "AI": {"tldr": "本文提出了一种兼容性感知的多教师CoT蒸馏框架COMPACT，该框架通过动态权重调整来融合来自不同教师的知识，以提高学生模型的能力。", "motivation": "现有的CoT蒸馏方法通常依赖单一老师，限制了学生的潜力，因为不同的LLM有各自的能力偏差，并可能遭受灾难性的遗忘问题。利用多教师虽然看起来很有吸引力，但有效整合他们的监督仍然具有挑战性。", "method": "COMPACT框架通过动态权重调整来融合来自不同老师的监督。该方法采用了三种维度的指标：基于图的一致性、基于互信息的适应性和基于损失的难度评估，以确保学生模型能够真正地理解推理过程而不是简单模仿。", "result": "实验和潜空间分析表明，COMPACT框架能有效整合多样化的推理能力，同时不破坏模型原有的知识结构，在各种基准上达到了最先进的性能，并缓解了灾难性遗忘问题。", "conclusion": "提出的COMPACT框架通过多维度动态权重调整成功解决了多教师监督融合的挑战，并展示了其在提升学生模型能力和保持原有知识方面的优越性。"}}
{"id": "2601.13987", "pdf": "https://arxiv.org/pdf/2601.13987", "abs": "https://arxiv.org/abs/2601.13987", "authors": ["Jiangwei Xie", "Zhang Wen", "Mike Davies", "Dongdong Chen"], "title": "SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration", "categories": ["eess.IV", "cs.CV"], "comment": "Technical report", "summary": "Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE's core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.", "AI": {"tldr": "本文介绍了一种完全无监督的单高光谱图像恢复框架SHARE，该框架通过结合几何等方差原则和低秩谱建模消除了对地面真实数据的需求。", "motivation": "传统的深度学习方法虽然改变了计算成像领域的面貌，但它们的有效性依赖于精心策划的地面真实数据集。然而，在现实世界的应用场景中，这种数据往往是不可用的。因此，开发一种不依赖于地面真实数据的无监督框架是必要的。", "method": "SHARE通过利用高光谱结构在可微分几何变换下的内在不变性来推导自监督信号，并通过等方差一致性约束来实现这一点。其核心组件DASA模块则进一步增强了这一范式转变，通过编码HSI的全局低秩属性和自适应地细化局部谱空间相关性。", "result": "在高光谱图像插补和超分辨率任务上的广泛实验证明了SHARE的有效性，该方法在无监督方法中表现出色，并且其性能可与有监督的方法相媲美。", "conclusion": "本文提出了一种新颖的完全无监督的单高光谱图像恢复框架，通过自监督机制实现了无需依赖地面真实数据即可进行有效的HSI恢复。"}}
{"id": "2601.13986", "pdf": "https://arxiv.org/pdf/2601.13986", "abs": "https://arxiv.org/abs/2601.13986", "authors": ["Zhang Wen", "Jiangwei Xie", "Dongdong Chen"], "title": "Equivariant Learning for Unsupervised Image Dehazing", "categories": ["cs.CV", "eess.IV"], "comment": "Technical report", "summary": "Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.", "AI": {"tldr": "提出了一种新的无监督学习框架EID，利用图像信号的对称性恢复模糊观察中的清晰模式。", "motivation": "现有的图像去雾方法依赖于精心设计的先验知识或大量的无雾霾真实数据，这些都难以获取，尤其是在科学成像中。为了克服这一挑战，作者提出了新的解决方案。", "method": "EID框架通过强制执行雾一致性和平移等方差性来恢复清晰模式，并采用对抗学习策略来建模未知的雾霾物理特性以促进去雾过程的学习。", "result": "实验在两个科学图像去雾基准数据集（包括细胞显微镜和医疗内窥镜）以及自然图像去雾上证明，EID显著优于现有方法。", "conclusion": "通过结合等方差学习与建模雾霾物理特性，EID有望实现更通用有效的科学成像中的去雾。"}}
{"id": "2601.13979", "pdf": "https://arxiv.org/pdf/2601.13979", "abs": "https://arxiv.org/abs/2601.13979", "authors": ["Raffaele Mazza", "Ciro Natale", "Pietro Falco"], "title": "Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.", "AI": {"tldr": "本文提出了一种新颖的跨模式视觉触觉感知框架，用于对可变形线性物体（如电缆）进行三维形状重建。", "motivation": "现有方法主要依赖于视觉，在变化的光照、背景杂乱或部分可见的情况下性能会下降。因此，文章旨在通过结合基础模型的视觉感知和自适应触觉探索来改进可变形线性物体的3D形状重建。", "method": "该框架利用SAM进行实例分割，并使用Florence进行语义细化，随后执行骨架化、端点检测和点云提取。对于被遮挡的电缆段落，采用触觉传感器提供局部点云并通过欧几里得聚类及拓扑保留融合将其与视觉数据合并。最后通过B样条插值生成光滑完整的电缆形状。", "result": "实验结果表明，该框架能准确重建简单和高度弯曲单个或多个电缆配置的3D形状，即使大部分部分被遮挡也能有效工作。", "conclusion": "实验证明了基础模型增强的跨模态感知在改进机器人操纵可变形物体方面的潜力。"}}
{"id": "2601.13976", "pdf": "https://arxiv.org/pdf/2601.13976", "abs": "https://arxiv.org/abs/2601.13976", "authors": ["Jing Zuo", "Lingzhou Mu", "Fan Jiang", "Chengcheng Ma", "Mu Xu", "Yonggang Qi"], "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.", "AI": {"tldr": "提出了FantasyVLN，一种统一的多模态隐式推理框架，用于改进视觉语言导航任务中的长时规划和解释性。", "motivation": "现有的Chain-of-Thought（CoT）方法存在纯文本CoTs缺乏空间定位和容易过度拟合的问题，而多模态CoTs在生成想象中的视觉观察时会产生严重的标记膨胀，导致实时导航不可行。", "method": "FantasyVLN利用预训练的Visual AutoRegressor（VAR）将想象的视觉令牌编码成紧凑的潜变量，在训练期间采用统一的多CoT策略共同学习文本、视觉和多模态CoT模式。", "result": "在LH-VLN数据集上的实验表明，该方法实现了具有推理意识且实时导航的任务，提高了成功率和效率，并将推断延迟降低了大约一个数量级。", "conclusion": "FantasyVLN提供了一种解决方案来克服现有Chain-of-Thought方法的局限性，能够在提高性能的同时保持实时处理能力。"}}
{"id": "2601.13975", "pdf": "https://arxiv.org/pdf/2601.13975", "abs": "https://arxiv.org/abs/2601.13975", "authors": ["Marco Piccolo", "Qiwei Han", "Astrid van Toor", "Joachim Vanneste"], "title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 4 figures 8 tables", "summary": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.", "AI": {"tldr": "本文提出了一种统一的信息管道，以标准化异构数据集，并在多个域中评估检测器的性能。", "motivation": "现有的检测解决方案在转移到新地点时表现不佳，因此需要一种可扩展且可靠的海洋生物多样性监测方法来支持保护和入侵物种管理。", "method": "开发了一个统一的信息管道，将不同的数据集标准化为可比较的信息流，并在一个固定的、与部署相关的检测器下进行了跨域评估。", "result": "结果发现场景构成、对象密度和语境冗余等结构性因素对跨域性能损失的影响大于水质问题。通过在低成本边缘硬件上进行基准测试，验证了运行时优化使得远程监测的实际采样率成为可能。", "conclusion": "研究强调从图像增强转向结构感知可靠性，并提供了一种民主化的工具以实现一致的海洋生态系统评估。"}}
{"id": "2601.13974", "pdf": "https://arxiv.org/pdf/2601.13974", "abs": "https://arxiv.org/abs/2601.13974", "authors": ["Shih-Yao Lin"], "title": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames", "categories": ["cs.CV"], "comment": "This paper corresponds to the camera-ready version of a WACV 2026 Workshop paper", "summary": "Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content. We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality. Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding. We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.", "AI": {"tldr": "本文提出了STEC，一种无参考的时空熵覆盖率指标，用于评估视频帧采样的有效性。", "motivation": "现有的评价指标主要集中在感知质量或重建保真度上，无法有效评估所采样帧是否充分捕捉到视频内容的信息和代表性。", "method": "STEC基于空间-时间帧熵（STFE），该方法通过熵基结构复杂性测量每帧的空间信息，并根据其时空覆盖率和冗余度来评价样本帧。它同时建模了空间信息强度、时序分散性和非冗余性。", "result": "实验表明，STEC能够清晰地区分不同的采样策略（如随机采样、均匀采样和内容感知方法），并且能揭示个体视频中的稳健模式，显示出其作为视频理解评估工具的实际价值。", "conclusion": "STEC提供了一个任务无关的诊断信号用于分析受限预算下的帧采样行为，而不是预测下游任务准确性。"}}
{"id": "2601.13973", "pdf": "https://arxiv.org/pdf/2601.13973", "abs": "https://arxiv.org/abs/2601.13973", "authors": ["Ancuta Margondai", "Mustapha Mouloua"], "title": "The Transparency Paradox in Explainable AI: A Theory of Autonomy Depletion Through Cognitive Load", "categories": ["cs.HC"], "comment": "26 pages, 4 figures, 5 tables. Submitted to Human Factors", "summary": "Objective: This paper develops a theoretical framework explaining when and why AI explanations enhance versus impair human decision-making. Background: Transparency is advocated as universally beneficial for human-AI interaction, yet identical AI explanations improve decision quality in some contexts but impair it in others. Current theories--trust calibration, cognitive load, and self-determination--cannot fully account for this paradox. Method: The framework models autonomy as a continuous stochastic process influenced by information-induced cognitive load. Using stochastic control theory, autonomy evolution is formalized as geometric Brownian motion with information-dependent drift, and optimal transparency is derived via Hamilton-Jacobi-Bellman equations. Monte Carlo simulations validate theoretical predictions. Results: Mathematical analysis generates five testable predictions about disengagement timing, working memory moderation, autonomy trajectory shapes, and optimal information levels. Computational solutions demonstrate that dynamic transparency policies outperform both maximum and minimum transparency by adapting to real-time cognitive state. The optimal policy exhibits threshold structure: provide information when autonomy is high and accumulated load is low; withhold when resources are depleted. Conclusion: Transparency effects depend on dynamic cognitive resource depletion rather than static design choices. Information provision triggers metacognitive processing that reduces perceived control when cognitive load exceeds working memory capacity. Application: The framework provides design principles for adaptive AI systems: adjust transparency based on real-time cognitive state, implement information budgets respecting capacity limits, and personalize thresholds based on individual working memory capacity.", "AI": {"tldr": "论文提出了一个理论框架，解释了AI解释在何时何地会增强或损害人类决策过程，并通过动态透明度策略来优化这一过程。", "motivation": "尽管透明性被认为对人机交互有普遍益处，但相同的AI解释在不同背景下会对人的决策产生正面或负面影响。现有理论无法完全解释这种悖论。", "method": "论文将自主权视为一个由信息诱导认知负荷影响的连续随机过程，并使用随机控制理论和蒙特卡罗模拟验证了其预测。", "result": "数学分析产生了五个可测试的预测，计算解决方案表明动态透明度策略优于最大或最小透明度。", "conclusion": "透明性效果依赖于动态的认知资源消耗，而非静态的设计选择。信息提供会触发元认知处理，当认知负荷超过工作记忆容量时，这会降低感知控制力。"}}
{"id": "2601.13969", "pdf": "https://arxiv.org/pdf/2601.13969", "abs": "https://arxiv.org/abs/2601.13969", "authors": ["Joaquín Polonuer", "Lucas Vittor", "Iñaki Arango", "Ayush Noori", "David A. Clifton", "Luciano Del Corro", "Marinka Zitnik"], "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval", "categories": ["cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.", "AI": {"tldr": "本文介绍了ARK，一种自主知识图谱探索工具，通过全局词汇搜索和邻域探索来控制广度深度的平衡。", "motivation": "现有方法在从知识图谱中检索证据时存在局限性，如相似度检索提供广泛覆盖但浅层，遍历方法依赖于选择种子节点且可能失败。因此需要一个能自适应调整广度和深度的方法。", "method": "ARK通过使用全局词汇搜索和一跳邻域探索来交替进行广度发现和深度扩展，不需要脆弱的种子选择或预设跳跃深度。", "result": "在STaRK数据集上，ARK达到了59.1%的平均Hit@1和67.4的平均MRR，分别提高了31.4%和28.0%。通过无标签模仿蒸馏进一步提升了性能。", "conclusion": "ARK展示了其在无需训练的情况下实现高效知识图谱检索的能力，并且能够适应不同类型查询，显著提升性能指标。"}}
{"id": "2601.13964", "pdf": "https://arxiv.org/pdf/2601.13964", "abs": "https://arxiv.org/abs/2601.13964", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Dong-Joo Kim"], "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.", "AI": {"tldr": "本文提出RL-BioAug框架，使用强化学习自动确定最优的EEG数据增强策略，并在少量标签数据引导下进行自监督表示学习。", "motivation": "传统的静态或随机数据增强方法难以保持非平稳EEG信号中的内在信息，影响对比学习性能。因此，作者希望通过一种更有效的数据增强方法来提高EEG任务的表现。", "method": "RL-BioAug框架采用强化学习代理自主确定最优的数据增强策略，并使用10%的标签数据指导代理决策，实现严格的自监督表示学习。", "result": "实验结果表明，与随机选择策略相比，RL-BioAug在Sleep-EDFX和CHB-MIT数据集上的Macro-F1得分分别提高了9.69%和8.80%，并且识别出针对不同任务的最佳增强方法。", "conclusion": "该框架展示了其取代传统基于启发式的方法并建立自主化数据增强新范式的潜力。"}}
{"id": "2601.13954", "pdf": "https://arxiv.org/pdf/2601.13954", "abs": "https://arxiv.org/abs/2601.13954", "authors": ["Adrien Meyer", "Didier Mutter", "Nicolas Padoy"], "title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.", "AI": {"tldr": "本论文提出了一种名为DExTeR的弱监督半监督对象检测方法，用于医疗影像中的解剖标志物定位。", "motivation": "现有的目标检测模型依赖于成本高昂的边界框注释，限制了其可扩展性。为了减少标注时间和保持定位信号的有效传递，本文提出了一种基于点注释的方法来改进医学图像的目标检测。", "method": "DExTeR采用点到箱的教师模型将单点注释转换为伪边界框标签以训练学生检测器，并引入了类引导可变形注意力机制和CLICK-MoE（CLass, Instance and Common Knowledge Mixture of Experts）来提高复杂结构中的识别能力。", "result": "DExTeR在三个不同医疗领域的数据集上实现了最先进的性能，证明了其减少标注成本的同时维持高检测精度的潜力。", "conclusion": "研究结果表明，DExTeR能够有效应对医学影像中特有的挑战，如重叠解剖结构和复杂尺寸的对象，并且可以降低标注成本同时保持高水平的检测准确性。"}}
{"id": "2601.13951", "pdf": "https://arxiv.org/pdf/2601.13951", "abs": "https://arxiv.org/abs/2601.13951", "authors": ["Shengyi Wu", "Yan Hong", "Shengyao Chen", "Zheng Wang", "Xianbing Sun", "Jiahui Zhan", "Jun Lan", "Jianfu Zhang"], "title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.", "AI": {"tldr": "本文介绍了VTONGuard，一个用于自动检测和验证AI生成虚拟试穿内容的大型基准数据集，并设计了一种多任务框架来提升检测效果。", "motivation": "随着AI技术的发展，虚拟试穿系统在电子商务和数字娱乐中越来越普及。但随之而来的是AI生成的试穿内容真实性问题，本文旨在解决这个问题并促进这些技术的安全与负责任地应用。", "method": "构建了包含超过775,000张真实和合成试穿图像的数据集VTONGuard，并基于此进行了多个检测范式的系统评估。设计了一种多任务框架，结合辅助分割来增强边界感知特征学习。", "result": "实验结果揭示了每种方法的优势和劣势，展示了跨范式泛化的持续挑战。所设计的多任务框架在VTONGuard数据集上取得了最佳的整体性能。", "conclusion": "该基准数据集有望促进公平比较，并推动更强大的检测模型的发展，同时鼓励AI生成虚拟试穿技术的安全与负责任地部署。"}}
{"id": "2601.13948", "pdf": "https://arxiv.org/pdf/2601.13948", "abs": "https://arxiv.org/abs/2601.13948", "authors": ["Nikita Kuzmin", "Songting Liu", "Kong Aik Lee", "Eng Siong Chng"], "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted by ICASSP2026", "summary": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.", "AI": {"tldr": "论文提出了Stream-Voice-Anon，一种结合神经音频编解码器和语言模型的实时语音匿名化方法。", "motivation": "保护在线语音应用中的说话人身份至关重要，但目前针对流式说话人匿名化的研究较少。现有的基于NAC的方法主要用于语音转换而非隐私保护。", "method": "Stream-Voice-Anon采用了伪说话者表示抽样、说话人嵌入混合和多样化的提示选择策略，并结合量化内容编码的解耦特性以防止说话人信息泄露，同时探讨了动态和固定延迟配置下的实时场景中的时延-隐私权衡。", "result": "在VoicePrivacy 2024挑战协议下，Stream-Voice-Anon实现了显著的语言清晰度（高达46%相对WER减少）和情感保存率（高达28%UAR相对提升），同时保持了可比的延迟时间和对懒惰知情攻击者的隐私保护。", "conclusion": "尽管对半知情攻击者有15%的相对退化，Stream-Voice-Anon仍展示了在实时语音匿名化任务上的潜力和改进。"}}
{"id": "2601.13945", "pdf": "https://arxiv.org/pdf/2601.13945", "abs": "https://arxiv.org/abs/2601.13945", "authors": ["Yixuan Deng", "Tongrun Wu", "Donghao Wu", "Zeyu Wei", "Jiayuan Wang", "Zhenglong Sun", "Yuqing Tang", "Xiaoqiang Ji"], "title": "Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.", "AI": {"tldr": "本文提出了ANCHOR框架，旨在通过明确的系统级原语实现解耦和鲁棒性，以支持Embodied AI系统的可靠部署。", "motivation": "随着Embodied AI系统从研究原型向实际应用转变，它们需要在负载变化和部分故障的情况下保持快速演进且可靠运行。现有的部署方法存在接口漂移、模块间干扰等问题，导致了大规模部署时的脆弱性。", "method": "ANCHOR框架包括Canonical Records（标准化共享状态的可演化契约）和多对多通信总线，形成一个可检查的端到端循环，解决了现有系统中的问题。", "result": "通过在去标识化的工作流实例上验证闭环可行性、表征不同负载大小和发布率下的延迟分布，并展示即使在共享内存丢失的情况下也能自动恢复流的能力。", "conclusion": "ANCHOR框架将临时的集成胶水转变为明确契约，使系统能够在负载下进行受控降级并自我修复恢复，从而实现闭合回路AI系统的可扩展部署。"}}
{"id": "2601.13942", "pdf": "https://arxiv.org/pdf/2601.13942", "abs": "https://arxiv.org/abs/2601.13942", "authors": ["Hongbo Bai", "Yujin Zhou", "Yile Wu", "Chi-Min Chan", "Pengcheng Wen", "Kunhao Pan", "Sirui Han", "Yike Guo"], "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.", "AI": {"tldr": "本文提出了一种名为Glance-or-Gaze（GoG）的框架，通过强化学习使大型多模态模型能够自适应地聚焦搜索。", "motivation": "尽管大型多模态模型在视觉理解方面取得了显著成功，但在处理知识密集型查询时仍然存在困难。现有方法依赖于不分青红皂白的整体图像检索，引入了大量冗余和噪音。", "method": "GoG框架采用选择性凝视机制，在全局上下文的扫描或高价值区域的聚焦之间动态切换，过滤掉无关信息。该方法采用了双阶段训练策略：通过监督微调进行反思行为对齐，并通过复杂度自适应强化学习来增强处理复杂查询的能力。", "result": "在六个基准测试上的实验表明了GoG框架达到最新的性能水平。消融研究证实选择性凝视和复杂度自适应RL对于有效的视觉搜索至关重要。", "conclusion": "GoG框架通过动态聚焦策略显著提高了大型多模态模型处理知识密集型查询的能力，展示了其在视觉搜索领域的潜力。"}}
{"id": "2601.13938", "pdf": "https://arxiv.org/pdf/2601.13938", "abs": "https://arxiv.org/abs/2601.13938", "authors": ["Heyang Zhou", "JiaJia Chen", "Xiaolu Chen", "Jie Bao", "Zhen Chen", "Yong Liao"], "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization", "categories": ["cs.IR", "cs.AI"], "comment": "9 pages, 3 figures. Submitted to ACL 2026. Corresponding author: Zhen Chen", "summary": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.", "AI": {"tldr": "本文提出了IF-GEO框架，用于解决多查询生成引擎优化中的冲突问题，通过融合冲突意识指令来提升跨查询稳定性。", "motivation": "随着生成引擎在信息检索中的应用越来越广泛，确保源内容的可见性成为一个挑战。特别是当需要对文档进行多样化的查询优化时，会出现冲突和竞争的修订需求。因此，提出了解决方案IF-GEO，以提高多查询情境下的性能稳定性。", "method": "IF-GEO采用\"先分化后融合\"策略，包括两个阶段：(i)从代表性的潜在查询中挖掘出不同的优化偏好；(ii)通过冲突感知的指令融合，协调这些偏好合成全局修订蓝图来指导编辑。引入了风险意识稳定度量指标以量化跨查询稳定性目标。", "result": "实验结果表明，在多查询基准测试上，IF-GEO实现了显著的性能提升并保持了在不同检索场景中的鲁棒性。", "conclusion": "通过实验证明，IF-GEO框架能够在保证跨查询稳定性的前提下实现生成引擎优化的显著性能提升。"}}
{"id": "2601.13935", "pdf": "https://arxiv.org/pdf/2601.13935", "abs": "https://arxiv.org/abs/2601.13935", "authors": ["Anoushkrit Goel", "Simroop Singh", "Ankita Joshi", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Aditya Nigam", "Arnav Bhavsar"], "title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026", "summary": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.", "AI": {"tldr": "TrackletGPT是一种用于白质束分割的语言类GPT框架。", "motivation": "由于白质束在不同的个体和条件下存在差异，但又具有跨半球和个体的相似三维结构，因此白质束分割对于研究大脑结构连接性、神经疾病以及神经外科手术至关重要。", "method": "TrackletGPT通过使用tracklets重新引入了序列信息中的token，实现了跨数据集的无缝泛化，并完全自动化编码细粒度的子流线段（Tracklets），从而扩展和改进了Tractography分割中的GPT模型。", "result": "实验结果表明，TrackletGPT在平均DICE、Overlap和Overreach分数上优于最先进的方法，在TractoInferno和HCP数据集上表现出色，甚至在跨数据集的实验中也表现良好。", "conclusion": "TrackletGPT为白质束分割任务提供了一个有效的解决方案，并且展示了其在不同数据集上的优越性能。"}}
{"id": "2601.13931", "pdf": "https://arxiv.org/pdf/2601.13931", "abs": "https://arxiv.org/abs/2601.13931", "authors": ["Yannis Vasilakis", "Rachel Bittner", "Johan Pauwels"], "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music", "categories": ["cs.SD", "cs.IR", "cs.LG"], "comment": "Accepted at IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 2026", "summary": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.", "AI": {"tldr": "本文旨在改善联合音视频模型在音乐检索中的否定处理问题。", "motivation": "现有系统难以可靠地表示诸如否定之类的语义现象，这对于区分音乐元素的存在与否非常重要。", "method": "通过从零开始训练CLAP模型，并使用文本增强和基于不相似性的对比损失来引入否定，设计以明确分离原始和否定的标题在联合嵌入空间中。", "result": "实验表明，所提出的方法单独或结合使用均能提高对否定的处理能力，同时很大程度上保持了检索性能。", "conclusion": "通过文本增强和特定损失函数改善了联合音视频模型中的否定建模问题，提高了音乐检索系统的准确性。"}}
{"id": "2601.13920", "pdf": "https://arxiv.org/pdf/2601.13920", "abs": "https://arxiv.org/abs/2601.13920", "authors": ["Spyridon C. Giagtzoglou", "Mark H. M. Winands", "Barbara Franci"], "title": "Asymmetric regularization mechanism for GAN training with Variational Inequalities", "categories": ["cs.GT", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, conference", "summary": "We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.", "AI": {"tldr": "本文提出了用于生成对抗网络（GANs）训练的非对称正则化机制，并通过变分不等式来稳定训练过程，找到纳什均衡。", "motivation": "动机在于通过提出一种新的非对称正则化方法来解决GAN训练中的不稳定问题并寻找一个纳什均衡点以提高模型性能和稳定性。", "method": "基于经典的Tikhonov步骤以及一种新型的零中心梯度惩罚，作者提出了一个新的非对称正则化机制，并通过分析获得显式的Lipschitz和强单调性常数来确保算法的线性收敛。", "result": "实证模拟表明，在无法实现强单凋的情况下，非对称正则化仍然可以达到均衡并稳定轨迹。", "conclusion": "研究得出结论：即使在强单调性不能被满足时，所提出的非对称正则化机制也能确保模型收敛到一个平衡点，并且能够有效稳定训练过程。"}}
{"id": "2601.13919", "pdf": "https://arxiv.org/pdf/2601.13919", "abs": "https://arxiv.org/abs/2601.13919", "authors": ["Yuezhe Yang", "Hao Wang", "Yige Peng", "Jinman Kim", "Lei Bi"], "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs", "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker", "AI": {"tldr": "本文提出HyperWalker框架，用于通过动态超图和测试时训练进行多跳临床模型的深度诊断。", "motivation": "现有方法在处理医疗报告生成（MRG）和视觉问答（VQA）等任务时通常采用孤立样本推理范式，忽略病历数据中可能存在的互补医学证据，限制了更准确的诊断能力。", "method": "HyperWalker框架首先构建动态超图iBrochure来建模EHR数据中的结构异质性和多模态临床信息之间的隐含高阶关联；然后通过强化学习代理Walker在该超图上导航并识别最优诊断路径，并采用linger机制进行多次迭代选择具有不同临床特征的补充案例。", "result": "实验显示，HyperWalker在MIMIC数据集上的MRG和EHRXQA上的医疗VQA任务中达到了最先进的性能。", "conclusion": "HyperWalker通过动态超图结合测试时训练提升了多模态医学信息处理与诊断的能力，并证明了其在临床推理中的优越性。"}}
{"id": "2601.13913", "pdf": "https://arxiv.org/pdf/2601.13913", "abs": "https://arxiv.org/abs/2601.13913", "authors": ["Pavlo Melnyk", "Cuong Le", "Urs Waldmann", "Per-Erik Forssén", "Bastian Wandt"], "title": "On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.", "AI": {"tldr": "本文研究了在单目3D人体姿态估计中旋转等变性的作用。", "motivation": "作者发现现有的方法在处理旋转后的输入图像时表现不佳，因此提出通过引入旋转等变性来改善模型性能。", "method": "论文提出了利用数据增强的方法学习2D到3D的转换过程中的旋转等变性，并且没有对参数空间施加明确的约束条件。", "result": "实验结果表明，在常用的HPE基准测试中，通过旋转等变性改进后的模型在处理图像平面内旋转的人体姿态时表现更好，优于现有的设计为等变性的方法。", "conclusion": "引入2D旋转等变性能有效地提高单目3D人体姿态估计的准确性和可靠性，且该增强可以通过数据增强来高效学习。"}}
{"id": "2601.13904", "pdf": "https://arxiv.org/pdf/2601.13904", "abs": "https://arxiv.org/abs/2601.13904", "authors": ["Jaeyoung Moon", "Youjin Choi", "Yucheon Park", "David Melhart", "Georgios N. Yannakakis", "Kyung-Joong Kim"], "title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation", "categories": ["cs.AI", "cs.HC"], "comment": "CHI '26 Accepted paper", "summary": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.", "AI": {"tldr": "本文提出了PREFAB，一种基于偏好的情感建模方法，用于低预算的自我标注。", "motivation": "现有的情感计算中的自我标注过程耗时、认知负担重且容易疲劳和出错。为了减轻这些问题并提高效率，作者开发了PREFAB以解决全量标注带来的问题。", "method": "PREFAB利用偏好学习模型检测相对情感变化，并结合峰值-结尾规则以及情绪的有序表示法来引导用户标注选定的部分片段，其余部分通过插值处理。", "result": "通过对技术性能研究和25名参与者的研究评估结果表明，与基线方法相比，PREFAB在建模情感转折点方面表现更好，减轻了工作量，并且在不影响标注质量的情况下提高了标注者的信心。", "conclusion": "研究表明PREFAB能够有效减少标注负担并提高标注者的工作效率和信心。"}}
{"id": "2601.13899", "pdf": "https://arxiv.org/pdf/2601.13899", "abs": "https://arxiv.org/abs/2601.13899", "authors": ["Masoumeh Javanbakhat", "Piotr Komorowski", "Dilyara Bareeva", "Wei-Chang Lai", "Wojciech Samek", "Christoph Lippert"], "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.", "AI": {"tldr": "本文提出了一种可解释的深度统计测试框架，增强了深两样本测试在个体样本和特征层次上的解释能力。", "motivation": "现有的深层神经网络两样本测试虽然能够有效检测分布差异，但其黑箱性质限制了在生物医学分析中的解释性和实用性；大多数现有后验解释性方法依赖于类别标签，在无标签统计检验设置中不合适。", "method": "提出了一种可解释的深度统计测试框架，通过增加个体样本和特征层次的解释，揭示哪些个别样本和输入特征驱动显著的组差异。该方法突出了影响图像区域和个体样本，提供了对检测到的组差别的空间和实例见解。", "result": "应用于生物医学成像数据时，所提出的框架能够识别有影响力的样本并突出与疾病相关变异相关的具有解剖学意义的区域。", "conclusion": "这项工作将统计推理与可解释的人工智能相结合，在医疗影像中实现了解释性、无标签的人口分析。"}}
{"id": "2601.13897", "pdf": "https://arxiv.org/pdf/2601.13897", "abs": "https://arxiv.org/abs/2601.13897", "authors": ["Ankita Joshi", "Ashutosh Sharma", "Anoushkrit Goel", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Arnav Bhavsar", "Aditya Nigam"], "title": "TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026", "summary": "Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.", "AI": {"tldr": "介绍了一种基于GPT的多批评家策略融合框架TractRLFusion，用于改进白质纤维追踪的准确性。", "motivation": "传统的白质纤维追踪方法依赖于经典的确定性和概率性技术，但这些方法在减少虚假连接和提高重建精度方面存在挑战。为了解决这些问题，本论文提出了一种新的基于深度学习的方法。", "method": "TractRLFusion框架采用两阶段训练数据选择过程进行有效的策略融合，并通过多批评家微调来增强鲁棒性和泛化能力。该方法利用多个强化学习策略并通过数据驱动的融合策略集成这些策略。", "result": "在HCP、ISMRM和TractoInferno数据集上的实验表明，TractRLFusion在准确性方面优于单一RL策略以及现有的经典和DRL方法，并且具有较高的解剖学可靠性。", "conclusion": "TractRLFusion框架通过多批评家政策融合提高了纤维追踪的准确性和鲁棒性，展示了其作为先进神经影像技术的价值。"}}
{"id": "2601.13895", "pdf": "https://arxiv.org/pdf/2601.13895", "abs": "https://arxiv.org/abs/2601.13895", "authors": ["Xu Zhang", "Danyang Li", "Yingjie Xia", "Xiaohang Dong", "Hualong Yu", "Jianye Wang", "Qicheng Li"], "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.", "AI": {"tldr": "本文提出了OmniOVCD，一种基于Segment Anything Model 3 (SAM 3) 的开放词汇变化检测框架。", "motivation": "现有的无训练开放词汇变化检测方法依赖于CLIP和DINO等模型来识别类别和提取特征，导致系统不稳定。因此，研究提出了一种新的方法以解决现有问题并提升性能。", "method": "该论文利用SAM 3的解耦输出头，并提出了一种协同融合到实例分离（SFID）策略，通过融合语义、实例和存在性输出构建土地覆盖掩码，并将它们分解为单独的实例掩码进行变化检测。", "result": "实验结果表明，在四个公共基准数据集上，所提模型实现了SOTA性能，分别达到了67.2、66.5、24.5和27.1（类别平均）的IoU得分。", "conclusion": "OmniOVCD通过使用SAM 3整合了分割与识别功能，并采用SFID策略显著提升了开放词汇变化检测任务中的性能，达到了高精度的变化掩码生成。"}}
{"id": "2601.13889", "pdf": "https://arxiv.org/pdf/2601.13889", "abs": "https://arxiv.org/abs/2601.13889", "authors": ["Wenge Xu", "Foroogh Hajiseyedjavadi", "Kurtis Weir", "Chukwuemeka Eze", "Mark Colley"], "title": "Towards Inclusive External Human-Machine Interface: Exploring the Effects of Visual and Auditory eHMI for Deaf and Hard-of-Hearing People", "categories": ["cs.HC"], "comment": "This is the author's version of the paper accepted at CHI Conference on Human Factors in Computing Systems (CHI '26), April 13-17, 2026, Barcelona, Spain", "summary": "External Human-Machine Interfaces (eHMIs) have been proposed to facilitate communication between Automated Vehicles (AVs) and pedestrians. However, no attention was given to Deaf and Hard-of-Hearing (DHH) people. We conducted a formative study through focus groups with 6 DHH people and 6 key stakeholders (including researchers, assistive technologists, and automotive interface designers) to compare proposed eHMIs and extract key design requirements. Subsequently, we investigated the effects of visual and auditory eHMI in a virtual reality user study with 32 participants (16 DHH). Results from our scenario suggesting that (1) DHH participants spent more time looking at the AV; (2) both visual and auditory eHMIs enhanced trust, usefulness, and perceived safety; and (3) only visual eHMIs reduced the time to step into the road, time looking at the AV, gaze time, and percentage looking at active visual eHMI components. Lastly, we provided five practical implications for making eHMI inclusive of DHH people.", "AI": {"tldr": "研究了外部人机界面(eHMI)对聋哑及听力障碍人士的影响，探讨视觉和听觉eHMI的设计要求。", "motivation": "为了改善自动化车辆与行人之间的沟通，并特别关注到聋哑及听力障碍人群的需求。", "method": "通过焦点小组讨论收集了6名聋哑及听力障碍者和6个关键利益相关方的意见，随后在虚拟现实用户研究中对32名参与者（其中16名为聋哑）进行了视觉和听觉eHMI的效果调查。", "result": "研究表明：(1) 聋哑参与者花更多时间查看自动化车辆；(2) 视觉和听觉eHMI都能增强信任感、实用性和感知的安全性；(3) 只有视觉eHMI减少了行人踏入道路的时间，缩短了注视AV的时间，并提升了注意力。", "conclusion": "提出了五项实际建议以使eHMI更具包容性，特别是针对聋哑及听力障碍人群。"}}
{"id": "2601.13887", "pdf": "https://arxiv.org/pdf/2601.13887", "abs": "https://arxiv.org/abs/2601.13887", "authors": ["Hong Su"], "title": "Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.", "AI": {"tldr": "本文提出了人类模拟计算（HSC），这是一种以人类为灵感的计算框架，旨在通过模仿人的思考、行动、学习和反思等过程来提升AI系统的适应性和推理能力。", "motivation": "大型语言模型虽然在基于文本数据的知识表示和推理方面表现出色，但它们仅依赖语言材料限制了其在开放动态真实环境中的适应能力和有效性。因此，本文旨在通过提出HSC框架来解决这些问题。", "method": "HSC框架将智能建模为一个涉及思考、行动、学习、反思以及活动调度的连续闭环过程，并强调内外部主动参与和自我改进机制。该方法还集成了人类常用的思维方式，如主要特征导向推理、通过行动扩展范围等策略。", "result": "通过理论分析表明，人类模拟策略不能仅从语言材料中完全学习到，且类人推理过程与基于行动的推理方法对于实现强大的适应性和有效互动至关重要。", "conclusion": "HSC框架为AI系统提供了更人性化的推理方式，并证明了人类灵感的方法在增强其在开放和动态环境中的性能方面的重要性。"}}
{"id": "2601.13886", "pdf": "https://arxiv.org/pdf/2601.13886", "abs": "https://arxiv.org/abs/2601.13886", "authors": ["Shangzhe Di", "Zhonghua Zhai", "Weidi Xie"], "title": "Revisiting Multi-Task Visual Representation Learning", "categories": ["cs.CV"], "comment": "Code: https://github.com/Becomebright/MTV", "summary": "Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity \"expert\" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves \"best-of-both-worlds\" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.", "AI": {"tldr": "本文介绍了一种多任务视觉预训练框架MTV，该框架通过共同优化共享的骨干网络来实现全局语义对齐和精细空间结构的理解。", "motivation": "当前的视觉表示学习存在分离问题：基于语言-视觉模型擅长全球语义对齐但缺乏空间精度，自监督方法能够捕获复杂的局部结构但在高层次语义上下文方面表现不佳。本文认为这些范式是互补的，并可以通过多任务框架进行整合。", "method": "MTV是一个多任务视觉预训练框架，它共同优化共享骨干网络上的语言-视觉对比、自监督和密集空间目标。该框架利用高容量的\"专家\"模型（如Depth Anything V2和OWLv2）来合成大规模的密集结构伪标签以减轻对手动注释的需求。", "result": "研究表明MTV实现了\"兼得两者之长\"的表现，显著增强了精细的空间推理能力而不会损害全球语义理解。", "conclusion": "多任务学习，借助高质量伪监督，是实现更通用视觉编码器的可扩展路径。"}}
{"id": "2601.13885", "pdf": "https://arxiv.org/pdf/2601.13885", "abs": "https://arxiv.org/abs/2601.13885", "authors": ["Esma Balkır", "Alice Pernthaller", "Marco Basaldella", "José Hernández-Orallo", "Nigel Collier"], "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.", "AI": {"tldr": "本文提出了一个基于IRT的自适应测试扩展方法，用于连续评分的LLM评估，并引入了一个带有自适应停止标准的不确定性感知排序器。", "motivation": "传统的计算机化自适应测试(CAT)在多选题基准上对LLM进行了有效的评估，但现代LLM评估越来越依赖于生成任务，其输出是连续评分而非正确/错误标记，因此需要新的评估方法。", "method": "本文将IRT基础上的自适应测试扩展到连续有界分数，通过用异方差正态分布替换伯努利响应分布，并引入一个带有自适应停止标准的不确定性感知排序器来实现可靠的模型排名。", "result": "该方法在五个基准上进行了验证，涵盖了n-gram、嵌入和LLM作为法官指标，在使用2%项目的情况下，与随机采样相比，排名相关性提高了0.12 τ，并且95%的预测具有高置信度。", "conclusion": "本文的方法可以更有效地评估语言模型，通过测试尽可能少的项目来实现可靠的排序和成本节约，同时提高排名的相关性和准确性。"}}
{"id": "2601.13880", "pdf": "https://arxiv.org/pdf/2601.13880", "abs": "https://arxiv.org/abs/2601.13880", "authors": ["Ye Tian", "Zihao Wang", "Onat Gungor", "Xiaoran Fan", "Tajana Rosing"], "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health", "categories": ["cs.AI"], "comment": null, "summary": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.", "AI": {"tldr": "本文介绍了LifeAgentBench，一个用于评估长期、跨维度和多用户生活方式健康推理的大型问答基准，并提出了一种名为LifeAgent的强大基线代理。", "motivation": "由于缺乏系统的基准测试，当前语言模型在个性化数字健康支持方面的实际能力尚不明确。因此，需要开发一套全面的基准来系统地评估这些模型。", "method": "构建了一个包含22,573个问题的大规模问答基准LifeAgentBench，并提出了一种名为LifeAgent的基线代理，该代理结合了多步证据检索与确定性聚合。", "result": "通过对11种领先的语言模型进行系统评估，发现长期聚合和跨维度推理的关键瓶颈。LifeAgent相比两个广泛使用的基线实现了显著改进。", "conclusion": "LifeAgentBench为评估语言模型在数字健康中的能力提供了重要工具，并展示了LifeAgent作为强大基线代理的潜力，在现实日常生活场景中具有潜在应用价值。"}}
{"id": "2601.13879", "pdf": "https://arxiv.org/pdf/2601.13879", "abs": "https://arxiv.org/abs/2601.13879", "authors": ["Dongxu Zhang", "Yiding Sun", "Cheng Tan", "Wenbiao Yan", "Ning Yang", "Jihua Zhu", "Hiajun Zhang"], "title": "Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring", "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.", "AI": {"tldr": "本文提出了一种名为V-Skip的方法，通过双路径门控机制优化视觉锚定信息瓶颈问题，以提高多模态大型语言模型的推理效率。", "motivation": "现有的压缩方法在处理多模态环境时往往导致视觉细节丢失（视觉健忘症），从而降低性能。因此，本文旨在改进这种盲目的文本压缩策略，使其更适合多模态情境。", "method": "V-Skip将令牌修剪重构成一个视觉锚定信息瓶颈优化问题，并采用双路径门控机制来衡量令牌的重要性，以保留视觉显著的线索。", "result": "实验结果表明，V-Skip在Qwen2-VL和Llama-3.2模型上实现了近三倍的速度提升，同时保持了几乎不变的准确性，在DocVQA任务中比其他基线方法提高了超过30%。", "conclusion": "本文提出的方法成功解决了现有压缩技术中的视觉健忘症问题，并显著提升了多模态推理的效率。"}}
{"id": "2601.13871", "pdf": "https://arxiv.org/pdf/2601.13871", "abs": "https://arxiv.org/abs/2601.13871", "authors": ["Michail Spanakis", "Iason Oikonomidis", "Antonis Argyros"], "title": "OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting", "categories": ["cs.CV"], "comment": null, "summary": "Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.", "AI": {"tldr": "本文介绍了OCCAM，一种无需训练、不依赖额外信息且能处理多类对象计数的方法。", "motivation": "由于无类别特定对象计数（CAC）在实际应用中的重要性日益增加，而现有方法通常假设每张图片只有一种物体类别，并依赖大量深度学习模型的训练以及额外的信息如视觉样本或文本提示，本文旨在提出一种无需训练且能处理多类别的解决方案。", "method": "OCCAM利用Segment Anything Model 2（SAM2）和自定义阈值基FINCH算法变体实现对象计数，并提议了合成数据集及F1分数作为评价指标。", "result": "在广泛使用的基准数据集FSC-147和CARPK上，该方法表现出有竞争力的性能。", "conclusion": "OCCAM证明了其作为无需训练且适用于多类别对象计数的有效性，并提出了更适合评估此类任务的新合成数据集及评价指标。"}}
{"id": "2601.13865", "pdf": "https://arxiv.org/pdf/2601.13865", "abs": "https://arxiv.org/abs/2601.13865", "authors": ["Hyunseung Lim", "Dasom Choi", "Sooyohn Nam", "Bogoan Kim", "Hwajung Hong"], "title": "Understanding Human-Multi-Agent Team Formation for Creative Work", "categories": ["cs.HC"], "comment": ":H.5.2", "summary": "Team-based collaboration is a cornerstone of modern creative work. Recent advances in generative AI open possibilities for humans to collaborate with multiple AI agents in distinct roles to address complex creative workflows. Yet, how to form Human-Multi-Agent Teams (HMATs) is underexplored, especially given that inter-agent interactions increase complexity and the risk of unexpected behaviors. In this exploratory study, we aim to understand how to form HMATs for creative work using CrafTeam, a technology probe that allows users to form and collaborate with their teams. We conducted a study with 12 design practitioners, in which participants iterated through a three-step cycle: forming HMATs, ideating with their teams, and reflecting on their teams' ideation. Our findings reveal that while participants initially attempted autonomous team operations, they ultimately adopted team formations in which they directly orchestrated agents. We discuss design considerations for HMAT formation that humans can effectively orchestrate multiple agents.", "AI": {"tldr": "研究人类如何与多智能体团队合作进行创意工作。", "motivation": "探索如何形成人类-多代理团队（HMAT）进行创意协作，特别是在复杂性和意外行为风险增加的情况下。", "method": "通过CrafTeam技术探针让参与者经历组建和协作的三个步骤：组队、与团队构思以及反思团队合作。研究对象为12名设计从业者。", "result": "发现尽管最初尝试自主操作团队，但最终参与者选择直接指挥智能体进行团队形成。", "conclusion": "提出人类可以有效协调多个代理的设计考虑因素。"}}
{"id": "2601.13864", "pdf": "https://arxiv.org/pdf/2601.13864", "abs": "https://arxiv.org/abs/2601.13864", "authors": ["Qirui Chen", "Jingxian Shuai", "Shuangwu Chen", "Shenghao Ye", "Zijian Wen", "Xufei Su", "Jie Jin", "Jiangming Li", "Jun Chen", "Xiaobin Tan", "Jian Yang"], "title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.", "AI": {"tldr": "介绍了一个名为HardSecBench的基准测试，用于评估大型语言模型在硬件代码生成中的安全意识。", "motivation": "现有研究主要集中在评估LLM生成代码的功能正确性，忽略了其潜在的安全问题。因此，需要一个基准来评估在现实规范下的安全意识。", "method": "设计了HardSecBench，包含924个任务，覆盖Verilog RTL和固件级C语言，并包括结构化规范、安全参考实现和可执行测试。提出了一种多代理管道用于自动化生成合成。", "result": "使用HardSecBench评估了多个LLM的硬件与固件代码生成能力，发现模型在满足功能要求的同时仍存在安全隐患，并且安全性结果受提示影响较大。", "conclusion": "研究突显了LLM辅助硬件设计中的挑战并提供了未来的改进见解。"}}
{"id": "2601.13858", "pdf": "https://arxiv.org/pdf/2601.13858", "abs": "https://arxiv.org/abs/2601.13858", "authors": ["Guixiang Zhang", "Yiyuan Wang", "Marius Hoggenmueller"], "title": "Designing Drone Interfaces to Assist Pedestrians Crossing Non-Signalised Roads", "categories": ["cs.HC"], "comment": "ef:OZCHI '25: Proceedings of the 37th Australian Conference on Human-Computer Interaction (2025)", "summary": "Recent research highlights the potential of drones to enhance pedestrian experiences, such as aiding navigation and supporting street-level activities. This paper explores the design of drone interfaces to assist pedestrians crossing dangerous roads without designated crosswalks or traffic lights, leveraging drones' ability to monitor and analyse real-time traffic data. Inspired by existing traffic signal systems, the interface communicates safety information through permissive alerts, prohibitive warnings, directional warnings, and collision emergency warnings. These safety cues were integrated into drone interfaces using in-situ projections and drone-equipped screens through an iterative design process. A mixed-methods, within-subjects VR evaluation (n=18) revealed that drone-assisted systems significantly improved pedestrian safety experiences and reduced mental workload compared to a baseline without any crossing aid, with projections outperforming screens. The findings suggest the potential for drone interfaces to be integrated into connected traffic systems. We also offer design recommendations for developing drone interfaces that support safe pedestrian crossings.", "AI": {"tldr": "本研究探讨了设计无人机界面以协助行人穿越无信号灯的危险道路的方法，通过实时交通数据分析和安全信息提示提高行人的安全性。", "motivation": "研究动机在于探索如何利用无人机监控实时交通数据并辅助行人过马路，特别是在没有明确人行横道或红绿灯的情况下，提升行人的安全体验。", "method": "该研究采用迭代设计过程将安全信号整合进无人机界面中，并使用虚拟现实评估方法（n=18）来测试效果，比较了投影和屏幕两种展示方式的优劣。", "result": "结果表明，与没有过马路辅助的情况相比，无人机辅助系统显著提高了行人的安全性并减轻了心理负担，特别是在使用投影技术时表现更佳。", "conclusion": "研究结论支持将无人机界面集成到连接交通系统中，并提出了一些关于开发支持安全穿越的无人机界面的设计建议。"}}
{"id": "2601.13852", "pdf": "https://arxiv.org/pdf/2601.13852", "abs": "https://arxiv.org/abs/2601.13852", "authors": ["Raül Pérez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "title": "Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICASSP 2026", "summary": "Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.", "AI": {"tldr": "本文提出了概率深度判别分析（PDDA），并将其应用于风力发电叶片分割，以提高性能和一致性。", "motivation": "线性判别分析虽能改善类别可分性，但在处理非线性数据时遇到困难，因此需要开发一种能够解决该问题的新方法。", "method": "通过深度网络优化Fisher准则直接实现Deep Discriminant Analysis（DDA），并引入概率损失函数形成PDDA以减少类内方差和增加预测置信度。", "result": "应用到风力发电叶片分割时，PDDA展示了显著的性能改进和一致性提升。这是首次将DDA应用于图像分割任务中。", "conclusion": "本文提出的概率深度判别分析（PDDA）在风力发电叶片分割任务上取得了优异的结果，表明其具有广泛的应用前景。"}}
{"id": "2601.13849", "pdf": "https://arxiv.org/pdf/2601.13849", "abs": "https://arxiv.org/abs/2601.13849", "authors": ["Ziyi Yang", "Li Rao", "Zhengding Luo", "Dongyuan Shi", "Qirui Huang", "Woon-Seng Gan"], "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control", "categories": ["eess.AS", "cs.LG", "eess.SP"], "comment": null, "summary": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.", "AI": {"tldr": "本文提出了一种基于元学习的主动噪声控制（ANC）方法，用于快速适应声环境变化。", "motivation": "传统的ANC算法在初期性能受初始化影响较大，特别是在环境发生变化时需要更快地自适应。", "method": "使用模型无关的元学习（MAML）来共同初始化控制滤波器和二次路径模型，并通过短双阶段内循环预训练初始系数。", "result": "实验结果表明，该方法能够在早期阶段降低错误率、缩短达到目标时间、减少辅助噪声能量以及在环境变化后更快恢复性能。", "conclusion": "提出的方法提供了一种简单的快速启动方案，在声环境发生变化时能够提高ANC的初期表现和适应性。"}}
{"id": "2601.13847", "pdf": "https://arxiv.org/pdf/2601.13847", "abs": "https://arxiv.org/abs/2601.13847", "authors": ["Jinhua Zhang", "Zhenqi Jia", "Rui Liu"], "title": "Emotion and Acoustics Should Agree: Cross-Level Inconsistency Analysis for Audio Deepfake Detection", "categories": ["cs.SD"], "comment": "Accepted by ICASSP 2026", "summary": "Audio Deepfake Detection (ADD) aims to detect spoof speech from bonafide speech. Most prior studies assume that stronger correlations within or across acoustic and emotional features imply authenticity, and thus focus on enhancing or measuring such correlations. However, existing methods often treat acoustic and emotional features in isolation or rely on correlation metrics, which overlook subtle desynchronization between them and smooth out abrupt discontinuities. To address these issues, we propose EAI-ADD, which treats cross level emotion acoustic inconsistency as the primary detection signal. We first project emotional and acoustic representations into a comparable space. Then we progressively integrate frame level and utterance level emotion features with acoustic features to capture cross level emotion acoustic inconsistencies across different temporal granularities. Experimental results on the ASVspoof 2019LA and 2021LA datasets demonstrate that the proposed EAI-ADD outperforms baselines, providing a more effective solution for audio anti spoofing detection.", "AI": {"tldr": "本文提出了一种新的音频深度伪造检测方法EAI-ADD，通过分析情感和声学特征之间的跨层次不一致性来提高检测准确性。", "motivation": "现有方法在处理情感和声学特征时往往孤立考虑或依赖相关性度量，忽略了这些特征之间细微的不同步现象和平滑的突然中断。因此，本文旨在解决这些问题并提供更有效的音频伪造检测方案。", "method": "EAI-ADD将情感表示和声学表示投影到一个可比较的空间中，并逐步融合帧级别和话语级别的感情与声音特性以捕捉跨层次的情感音调不一致性。", "result": "实验结果表明，提出的EAI-ADD方法在ASVspoof 2019LA和2021LA数据集上优于基线方法，为音频反欺骗检测提供了更有效的解决方案。", "conclusion": "通过分析情感与声学特征间的跨层次不一致性，EAI-ADD能够有效提升音频深伪检测的准确性，展示了在识别伪造语音方面的潜力。"}}
{"id": "2601.13846", "pdf": "https://arxiv.org/pdf/2601.13846", "abs": "https://arxiv.org/abs/2601.13846", "authors": ["Glinskaya Maria"], "title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments", "categories": ["cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.", "AI": {"tldr": "本文介绍了Virtual Urbanism（VU），一个通过合成城市复制品来量化城市身份的多模式人工智能分析框架，并展示了在东京进行的一项试点研究。", "motivation": "动机是开发一种计算可行的城市身份指标，以推进城市规划与设计领域的深入理解。", "method": "使用了Stable Diffusion和LoRA模型生成九个东京地区的动态合成城市序列，并通过排除现有导向标记来突出核心形成元素。进行了三项人类评估实验，包括感知合法性的评估、区域层面的身份量化以及提取核心身份形成元素。", "result": "研究显示了约81%的平均识别准确性，确认了复制品的有效性。Urban Identity Level（UIL）指标能够评估不同地区之间的身份水平，语义分析揭示文化嵌入类型是核心身份形成元素。", "conclusion": "VU框架作为一种可行的人工智能增强的城市分析工具被定位，为自动化的多参数身份度量指明了道路。"}}
{"id": "2601.13841", "pdf": "https://arxiv.org/pdf/2601.13841", "abs": "https://arxiv.org/abs/2601.13841", "authors": ["Pierre Bergé", "Antoine Dailly", "Yan Gerard"], "title": "Nemesis, an Escape Game in Graphs", "categories": ["cs.DS", "cs.CC", "cs.GT"], "comment": null, "summary": "We define a new escape game in graphs that we call Nemesis. The game is played on a graph having a subset of vertices labeled as exits and the goal of one of the two players, called the fugitive, is to reach one of these exit vertices. The second player, i.e. the fugitive adversary, is called the Nemesis. Her goal is to trap the fugitive in a connected component which does not contain any exit. At each round of the game, the fugitive moves from one vertex to an adjacent vertex. Then the Nemesis deletes one edge anywhere in the graph. The game ends when either the fugitive reached an exit or when he is in a connected component that does not contain any exit. In trees and graphs of maximum degree bounded by 3, Nemesis can be solved in linear time. We also show that a variant of the game called Blizzard where only edges adjacent to the position of the fugitive can be deleted also admits a linear time solution. For arbitrary graphs, we show that Nemesis is PSPACE-complete, and that it is NP-hard on planar multigraphs. We extend our results to the related Cat Herding problem, proving its PSPACE-completeness. We also prove that finding a strategy based on a full binary escape tree whose leaves are exists is NP-complete.", "AI": {"tldr": "本文定义了一个新的基于图的逃脱游戏Nemesis，并探讨了该游戏在不同图结构下的复杂性。", "motivation": "研究动机在于通过设计新类型的图论游戏来探索和理解图中搜索、追逃问题的计算复杂度，特别是对于特定类型的图如树和具有界定最大度数的图以及平面多重图。", "method": "作者采用了理论分析方法，探讨了不同条件下该游戏的可解性及其时间复杂度，并将结果扩展到了相关的牧羊犬问题。", "result": "研究发现，在树和最大度数受限为3的图中，Nemesis可以在线性时间内解决；对于任意图，该问题是PSPACE完全的，并且在平面多重图上是NP难的。Blizzard变体也在特定条件下可以在线性时间内解决。", "conclusion": "通过研究证明了Nemesis及其变种游戏以及Cat Herding问题的不同复杂度级别，为相关搜索算法的设计与分析提供了理论基础和挑战案例。"}}
{"id": "2601.13839", "pdf": "https://arxiv.org/pdf/2601.13839", "abs": "https://arxiv.org/abs/2601.13839", "authors": ["Aisha Al-Mohannadi", "Ayisha Firoz", "Yin Yang", "Muhammad Imran", "Ferda Ofli"], "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.", "AI": {"tldr": "该论文介绍了DisasterVQA数据集，用于灾害场景中的视觉问答基准测试。", "motivation": "由于社交媒体图像在灾难响应中提供了及时的现场信息，但现有视觉问答（VQA）模型在复杂和安全关键性的推理任务上的表现尚不明确，因此需要一个专为危机情境设计的数据集来评估这些模型的表现。", "method": "构建了一个包含1,395张真实世界图像和4,405个专家策划的问题答案对的基准数据集DisasterVQA，并基于人道主义框架如FEMA ESF和OCHA MIRA，涵盖洪水、野火和地震等多种灾害类型。", "result": "评估了七种最先进的视觉语言模型在不同问题类型、灾害类别、地区和人道主义任务上的表现差异。虽然这些模型对二元问题是准确的，但在细粒度定量推理、对象计数及上下文敏感解释上存在挑战，特别是在代表性不足的灾难场景中。", "conclusion": "DisasterVQA提供了一个具有挑战性且实用的数据集，指导开发更稳健和操作意义更强的视觉语言模型用于灾害响应。该数据集公开可用。"}}
{"id": "2601.13837", "pdf": "https://arxiv.org/pdf/2601.13837", "abs": "https://arxiv.org/abs/2601.13837", "authors": ["Xinya Ji", "Sebastian Weiss", "Manuel Kansy", "Jacek Naruniec", "Xun Cao", "Barbara Solenthaler", "Derek Bradley"], "title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \\OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.", "AI": {"tldr": "提出FastGHA，一种从少量输入图像生成高质量高斯头像并支持实时动画的方法。", "motivation": "克服现有方法在生成高质量三维高斯头像时效率低下的问题，提高其可扩展性和易用性。", "method": "通过直接学习像素级的高斯表示，使用基于transformer的编码器融合多视角信息，并引入轻量级MLP网络预测表情代码对应的3D高斯变形以支持实时动画。同时采用预训练模型生成的点图作为几何监督增强三维头像的平滑度。", "result": "实验表明该方法在渲染质量和推理效率方面显著优于现有方法，且能支持实时动态头像动画。", "conclusion": "FastGHA能够高效地从少量输入图像中生成高质量的高斯头像，并支持实时的动态变化。"}}
{"id": "2601.13836", "pdf": "https://arxiv.org/pdf/2601.13836", "abs": "https://arxiv.org/abs/2601.13836", "authors": ["Qian Chen", "Jinlan Fu", "Changsong Li", "See-Kiong Ng", "Xipeng Qiu"], "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": "https://openmoss.github.io/FutureOmni", "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "AI": {"tldr": "本文介绍了FutureOmni，一个用于评估多模态大语言模型从音频和视频环境中进行未来预测的基准。", "motivation": "现有的多模态大语言模型虽然在跨模态理解方面表现出色，但缺乏对未来事件的预测能力。为填补这一研究空白，作者提出了FutureOmni来评价这些模型在未来的预测表现。", "method": "FutureOmni由可扩展的LLM辅助、人工参与的数据生成流程构建，包含919个视频和1,034个多选问答对。此外，还提出了一种跨模态未来预测训练策略（OFF）以改进现有模型的表现。", "result": "实验结果显示，当前系统在音频-视频未来预测任务上的表现不佳，特别是语音密集场景下的准确率仅为64.8%，而通过OFF训练策略可以提升预测和泛化能力。", "conclusion": "FutureOmni揭示了多模态大语言模型在未来事件预测方面的局限，并提出了一种新的训练方法来改善这种状况。代码和数据集已公开发布以供进一步研究使用。"}}
{"id": "2601.13822", "pdf": "https://arxiv.org/pdf/2601.13822", "abs": "https://arxiv.org/abs/2601.13822", "authors": ["Michael Elkin", "Ariel Khuzman"], "title": "Efficient Parallel $(Δ+1)$-Edge-Coloring", "categories": ["cs.DS", "cs.DC", "cs.DM"], "comment": "72 pages, 15 figures", "summary": "We study the $(Δ+1)$-edge-coloring problem in the parallel $\\left(\\mathrm{PRAM}\\right)$ model of computation. The celebrated Vizing's theorem [Viz64] states that every simple graph $G = (V,E)$ can be properly $(Δ+1)$-edge-colored. In a seminal paper, Karloff and Shmoys [KS87] devised a parallel algorithm with time $O\\left(Δ^5\\cdot\\log n\\cdot\\left(\\log^3 n+Δ^2\\right)\\right)$ and $O(m\\cdotΔ)$ processors. This result was improved by Liang et al. [LSH96] to time $O\\left(Δ^{4.5}\\cdot \\log^3Δ\\cdot \\log n + Δ^4 \\cdot\\log^4 n\\right)$ and $O\\left(n\\cdotΔ^{3} +n^2\\right)$ processors. [LSH96] claimed $O\\left(Δ^{3.5} \\cdot\\log^3Δ\\cdot \\log n + Δ^3\\cdot \\log^4 n\\right)$ time, but we point out a flaw in their analysis, which once corrected, results in the above bound. We devise a faster parallel algorithm for this fundamental problem. Specifically, our algorithm uses $O\\left(Δ^4\\cdot \\log^4 n\\right)$ time and $O(m\\cdot Δ)$ processors. Another variant of our algorithm requires $O\\left(Δ^{4+o(1)}\\cdot\\log^2 n\\right)$ time, and $O\\left(m\\cdotΔ\\cdot\\log n\\cdot\\log^δΔ\\right)$ processors, for an arbitrarily small $δ>0$. We also devise a few other tradeoffs between the time and the number of processors, and devise an improved algorithm for graphs with small arboricity. On the way to these results, we also provide a very fast parallel algorithm for updating $(Δ+1)$-edge-coloring. Our algorithm for this problem is dramatically faster and simpler than the previous state-of-the-art algorithm (due to [LSH96]) for this problem.", "AI": {"tldr": "本文提出了一种高效的并行算法来解决$(Δ+1)$边着色问题。", "motivation": "改进现有的并行算法，提高其效率和处理速度，以适应更大规模的图计算需求。", "method": "设计了新的并行算法，在PRAM模型下实现了更快的时间复杂度和处理器使用量。还提供了一些时间与处理器数量之间的权衡方案，并针对低树宽图提出改进算法。", "result": "本文提出的算法在时间和处理器资源上均优于现有技术，特别是对于$(Δ+1)$边着色的更新问题提出了更快速且简单的解决方案。", "conclusion": "通过设计新的并行算法，成功地提高了$(Δ+1)$边着色问题在大规模图上的计算效率。"}}
{"id": "2601.13816", "pdf": "https://arxiv.org/pdf/2601.13816", "abs": "https://arxiv.org/abs/2601.13816", "authors": ["Raül Pérez-Gonzalo", "Andreas Espersen", "Antonio Agudo"], "title": "Discriminant Learning-based Colorspace for Blade Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICASSP 2026", "summary": "Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.", "AI": {"tldr": "本文提出了一种基于判别学习的颜色空间（CSDA）算法，用于改进叶片分割的准确性。", "motivation": "现代许多算法忽视了颜色表示对图像分割的重要性，导致准确度下降。", "method": "通过扩展线性判别分析到深度学习环境中，CSDA最大化多维类间分离性同时最小化类内变异性，并引入三个替代损失函数以实现端到端的优化。", "result": "在风力涡轮机叶片数据上的实验表明，使用CSDA可以显著提高分割准确度。", "conclusion": "定制化的预处理步骤对于特定领域的图像分割具有重要意义。"}}
{"id": "2601.13813", "pdf": "https://arxiv.org/pdf/2601.13813", "abs": "https://arxiv.org/abs/2601.13813", "authors": ["Timofei Kozlov", "Artem Trandofilov", "Georgii Gazaryan", "Issatay Tokmurziyev", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "GuideTouch: An Obstacle Avoidance Device for Visually Impaired", "categories": ["cs.RO", "cs.HC"], "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.", "AI": {"tldr": "介绍GuideTouch，一种为视障人士设计的防碰撞设备，能够通过振动反馈提供三维环境感知。", "motivation": "解决传统移动辅助工具在检测头部高度障碍物方面的不足，提高视障人士的安全导航能力。", "method": "使用两个垂直排列的飞行时间传感器和四个振动执行器，形成一个四点振动反馈系统，并进行实验验证其效果。", "result": "在22名参与者中测试了振动感知准确性，在14名视障用户中验证了主要方向线索识别精度达到93.75%，显示了系统的高识别准确性和实用性。", "conclusion": "GuideTouch能够提供直观的空间感知，显著提高了视障人士独立导航的安全性、信心和自主性。"}}
{"id": "2601.13809", "pdf": "https://arxiv.org/pdf/2601.13809", "abs": "https://arxiv.org/abs/2601.13809", "authors": ["Fawad Mehboob", "Monijesu James", "Amir Habel", "Jeffrin Sam", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "DroneVLA: VLA based Aerial Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.", "AI": {"tldr": "介绍了一种基于VLA的自主空中操作系统，能够根据自然语言指令获取并递送物体给用户。", "motivation": "随着空中的平台从被动观察者转变为积极的操作者，设计直观的人机交互界面成为一个挑战，以允许非专家用户可以自然地指挥这些系统。", "method": "该系统集成了基于MediaPipe的Grounding DINO和Vision-Language-Action (VLA)模型与配备了1自由度抓手和Intel RealSense RGB-D摄像头的定制无人机。通过语义推理理解用户的意图并生成优先级任务队列以进行物体的抓取，使用Grounding DINO和动态A*规划算法导航并将对象安全重新定位，并在交接阶段运用MediaPipe提供的人体姿态估计来确保与人类的安全互动。", "result": "系统通过实际实验展示了其在本地化和导航方面的有效性，误差分别为最大0.164米、平均0.070米和根均方误差0.084米。", "conclusion": "结果表明VLA模型在空中操作任务中的可行性和实用性，验证了系统设计的有效性。"}}
{"id": "2601.13802", "pdf": "https://arxiv.org/pdf/2601.13802", "abs": "https://arxiv.org/abs/2601.13802", "authors": ["Yushen Chen", "Junzhe Liu", "Yujie Tu", "Zhikang Niu", "Yuzhe Liang", "Kai Yu", "Chunyu Qiang", "Chen Zhang", "Xie Chen"], "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .", "AI": {"tldr": "开发了一套名为Habibi的开源统一方言阿拉伯语语音合成模型，支持从高资源到低资源的各种阿拉伯方言。", "motivation": "解决现有研究中对阿拉伯方言特别是统一建模视角下的语音合成研究不足的问题，填补这一领域的空白。", "method": "通过利用现有的开源ASR语料库，并采用语言学指导的课程学习方法来训练模型。", "result": "该方法在生成质量上超过了领先的商业服务，并且能够保持扩展性，无需进行文本重音化处理。", "conclusion": "为多方言阿拉伯语音合成过程建立了评估标准和第一个系统化的基准，并提供了后续研究的基础。"}}
{"id": "2601.13801", "pdf": "https://arxiv.org/pdf/2601.13801", "abs": "https://arxiv.org/abs/2601.13801", "authors": ["Yuhua Jin", "Nikita Kuzmin", "Georgii Demianchuk", "Mariya Lezina", "Fawad Mehboob", "Issatay Tokmurziyev", "Miguel Altamirano Cabrera", "Muhammad Ahsan Mustafa", "Dzmitry Tsetserukou"], "title": "HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction", "categories": ["cs.RO"], "comment": "This paper has been accepted for publication at LBR HRI 2026 conference", "summary": "Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.", "AI": {"tldr": "介绍了一种结合无人机移动性、独立基础设施的视觉投影和实时对话AI的统一平台HoverAI。", "motivation": "解决现有无人机在人类活动空间中操作时缺乏有效沟通机制的问题，导致意图不明确。", "method": "使用MEMS激光投影仪、半刚性屏幕和RGB相机感知用户通过视觉和声音，并结合VAD, ASR（Whisper）, LLM意圖分类, RAG对话系统, 面部分析和个人化, 以及语音合成(XTTS v2)技术。", "result": "评估显示命令识别准确度为0.90，性别估计F1得分为0.89，年龄平均绝对误差为5.14年，语音转录WER为0.181。", "conclusion": "HoverAI通过结合空中机器人、适应性对话AI和自包含视觉输出，引入了一种新的空间感知和社会响应型具身代理类。"}}
{"id": "2601.13798", "pdf": "https://arxiv.org/pdf/2601.13798", "abs": "https://arxiv.org/abs/2601.13798", "authors": ["Kai Wittenmayer", "Sukrut Rao", "Amin Parchami-Araghi", "Bernt Schiele", "Jonas Fischer"], "title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "32 pages, 24 figures, 3 tables", "summary": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.", "AI": {"tldr": "本文提出了Insight，一种语言对齐的概念基础模型，它提供了细粒度且空间定位良好的人类可解释概念，并展示了在分类和分割任务中与不透明的基础模型竞争的性能。", "motivation": "现有的语言对齐视觉基础模型虽然在各种下游任务上表现出色，但其学习到的表示仍然是不透明的，难以解释决策过程。本文旨在提供细粒度、空间定位良好的人类可解释概念来解决这一问题。", "method": "Insight使用层次稀疏自编码器和具有强语义表示的基础模型自动提取不同粒度的概念，并通过检查局部共现依赖关系定义概念之间的关系，进一步改进概念命名并获得更丰富的解释。", "result": "在基准数据上，Insight提供了与不透明基础模型竞争的分类和分割性能，同时提供细粒度、高质量的概念性解释。", "conclusion": "Insight不仅提高了人类对视觉语言编码器内部工作原理的理解，还展示了如何实现可解释性和高性能之间的平衡。"}}
{"id": "2601.13797", "pdf": "https://arxiv.org/pdf/2601.13797", "abs": "https://arxiv.org/abs/2601.13797", "authors": ["Gabriele Serussi", "David Vainshtein", "Jonathan Kouchly", "Dotan Di Castro", "Chaim Baskin"], "title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.", "AI": {"tldr": "本文介绍了PREGEN，一种高效的组成视频检索框架，通过结合冻结的预训练视觉语言模型和轻量级编码模型来提高检索性能。", "motivation": "当前的组成视频检索方法未能充分利用现代视觉语言模型，存在使用过时架构或需要昂贵的微调等问题。作者旨在克服这些限制，提出了一种更有效的方法。", "method": "PREGEN框架将冻结的预训练视觉语言模型与轻量级编码器结合，输入查询视频和修改文本后从每一层提取最终token的隐藏状态，并通过简单训练创建语义丰富且紧凑的嵌入用于检索。", "result": "PREGEN在标准组成视频检索基准上显著提升了召回率@1，分别达到了+27.23和+69.59的提升，在不同视觉语言模型架构下表现稳健，并展示了强大的零样本泛化能力。", "conclusion": "研究证明了PREGEN作为一种高效的组成视频检索方法的有效性，其语义能力和跨多种文本修改类型的泛化能力强。"}}
{"id": "2601.13796", "pdf": "https://arxiv.org/pdf/2601.13796", "abs": "https://arxiv.org/abs/2601.13796", "authors": ["Jingcheng Liu", "Yixiao Yu"], "title": "Zero-free regions and concentration inequalities for hypergraph colorings in the local lemma regime", "categories": ["cs.DS", "cs.DM", "math.PR"], "comment": null, "summary": "We show that for $q$-colorings in $k$-uniform hypergraphs with maximum degree $Δ$, if $k\\ge 50$ and $q\\ge 700Δ^{\\frac{5}{k-10}}$, there is a \"Lee-Yang\" zero-free strip around the interval $[0,1]$ of the partition function, which includes the special case of uniform enumeration of hypergraph colorings. As an immediate consequence, we obtain Berry-Esseen type inequalities for hypergraph $q$-colorings under such conditions, demonstrating the asymptotic normality for the size of any color class in a uniformly random coloring. Our framework also extends to the study of \"Fisher zeros\", leading to deterministic algorithms for approximating the partition function in the zero-free region. Our approach is based on extending the recent work of [Liu, Wang, Yin, Yu, STOC 2025] to general constraint satisfaction problems (CSP). We focus on partition functions defined for CSPs by introducing external fields to the variables. A key component in our approach is a projection-lifting scheme, which enables us to essentially lift information percolation type analysis for Markov chains from the real line to the complex plane. Last but not least, we also show a Chebyshev-type inequality under the sampling LLL condition for atomic CSPs.", "AI": {"tldr": "本文研究了在局部引理条件下，k-uniform超图q着色问题中的零自由区域和浓度不等式。", "motivation": "论文旨在通过扩展约束满足问题(CSP)的局部引理分析来改善对超图着色问题的理解，并推导出相关变量的渐近正态性。", "method": "研究方法基于最近的研究成果[Liu, Wang, Yin, Yu, STOC 2025]，引入外场变量定义CSP的分区函数。通过投影提升方案将信息渗透分析从实线扩展到复平面。", "result": "结果表明，在k≥50且q≥700Δ^(5/(k-10))条件下存在Lee-Yang零自由带，并且可以得到贝里-埃森型不等式，证明了着色中任意颜色类大小的渐近正态性。", "conclusion": "结论指出，研究提供了一种新的方法来分析超图着色问题中的分区函数和随机抽样局部引理条件下的切比雪夫型不等式。"}}
{"id": "2601.13778", "pdf": "https://arxiv.org/pdf/2601.13778", "abs": "https://arxiv.org/abs/2601.13778", "authors": ["Yitian Yang", "Yugin Tan", "Jung-Tai King", "Yang Chen Lin", "Yi-Chieh Lee"], "title": "Fit Matters: Format-Distance Alignment Improves Conversational Search", "categories": ["cs.HC"], "comment": "Accepted to CHI 2026. 20 pages, 7 figures", "summary": "Existing conversational search systems can synthesize information into responses, but they lack principled ways to adapt response formats to users' cognitive states. This paper investigates whether aligning format and distance, which involves matching information granularity and media to users' psychological distance, improves user experience. In a between-subjects experiment (N=464) on travel planning, we crossed two distance dimensions (temporal/spatial x near/far) with four formats varying in granularity (abstract/concrete) and media (text/image-and-text). The experiment established that format--distance alignment reduced users' risk perceptions while increasing decision confidence, perceptions of information usefulness, ease of use, enjoyment, and credibility, and adoption intentions. Concrete formats imposed higher cognitive load, but yielded productive effort when matched to near-distance tasks. Images enhanced concrete but not abstract text, suggesting multimedia benefits depend on complementarity. These findings establish format--distance alignment as a distinctive and important design dimension, enabling systems to tailor response formats to users' psychological distance.", "AI": {"tldr": "本文研究了将信息粒度和媒体与用户的心理距离相匹配的格式距离对齐是否能改善用户体验。", "motivation": "现有的对话搜索系统可以在回应中综合信息，但缺乏适应用户认知状态的原则性方法。因此，探索如何通过格式距离对齐提高用户体验。", "method": "在一次针对旅行规划的被试间实验（N=464）中，作者交叉了两个距离维度（时间/空间 x 近/远）和四种变化粒度（抽象/具体）及媒体（文本/图像与文本组合）的格式。", "result": "实验表明，格式-距离对齐减少了用户的风险感知，提高了决策信心、信息有用性认知、易用性、享受程度和可信度以及采用意向。具体格式增加了认知负荷，但在匹配近距离任务时产生了积极效果。图像增强了具体的文本但未增强抽象的文本。", "conclusion": "这些发现确立了格式-距离对齐作为一个独特且重要的设计维度，使系统能够根据用户的心理距离定制回应格式。"}}
{"id": "2601.13777", "pdf": "https://arxiv.org/pdf/2601.13777", "abs": "https://arxiv.org/abs/2601.13777", "authors": ["Zvi Chapnik", "Yizhar Or", "Shai Revzen"], "title": "Sample Efficient Learning of Body-Environment Interaction of an Under-Actuated System", "categories": ["cs.RO"], "comment": null, "summary": "Geometric mechanics provides valuable insights into how biological and robotic systems use changes in shape to move by mechanically interacting with their environment. In high-friction environments it provides that the entire interaction is captured by the ``motility map''. Here we compare methods for learning the motility map from motion tracking data of a physical robot created specifically to test these methods by having under-actuated degrees of freedom and a hard to model interaction with its substrate. We compared four modeling approaches in terms of their ability to predict body velocity from shape change within the same gait, across gaits, and across speeds. Our results show a trade-off between simpler methods which are superior on small training datasets, and more sophisticated methods, which are superior when more training data is available.", "AI": {"tldr": "本文探讨了如何高效学习欠驱动系统与其环境之间的相互作用，特别是在高摩擦环境中通过形状变化预测身体速度。", "motivation": "研究旨在利用几何力学来理解生物和机器人系统是如何通过改变自身的形状与环境进行机械互动以实现移动的，并测试不同建模方法在有限数据集上的表现。", "method": "比较了四种不同的模型方法，评估它们基于运动跟踪数据预测身体速度的能力，这些数据来自于一个特别设计用于测试的方法的物理机器人。", "result": "研究表明简单的模型方法在小规模训练数据上表现出色，而更复杂的模型则在大规模训练数据集上有优势。", "conclusion": "研究揭示了简单和复杂建模方法之间的性能权衡，并强调了根据可用的数据量选择合适的建模策略的重要性。"}}
{"id": "2601.13770", "pdf": "https://arxiv.org/pdf/2601.13770", "abs": "https://arxiv.org/abs/2601.13770", "authors": ["Mostapha Benhenda"], "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench", "AI": {"tldr": "本文介绍了Look-Ahead-Bench，一个用于衡量金融工作流中点时大语言模型（LLM）前瞻偏差的标准基准。", "motivation": "现有的方法主要通过问答来测试内在的前瞻知识，但本文希望通过在实际场景中的行为评估，区分真正的预测能力和基于记忆的表现，并建立一个标准的评价框架。", "method": "该研究设计了一个标准化的基准，分析了不同市场时期的性能衰减，并引入了几个定量基线以确定表现阈值。测试对象包括开源LLM（Llama 3.1和DeepSeek 3.2）以及来自PiT-Inference的一系列点时LLM。", "result": "结果表明，在标准LLM中发现了显著的前瞻偏差，而Pitinf模型随着规模增大展示了更好的泛化与推理能力。这些发现证明了Pitinf模型更适合于现实世界的部署。", "conclusion": "本文为金融领域的大语言模型时间偏差提供了一个标准化评估的基础，并提供了识别适合实际应用模型的实际框架。"}}
{"id": "2601.13768", "pdf": "https://arxiv.org/pdf/2601.13768", "abs": "https://arxiv.org/abs/2601.13768", "authors": ["Wenzhen Yue", "Ruohao Guo", "Ji Shi", "Zihan Hao", "Shiyu Hu", "Xianghua Ying"], "title": "vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper, we present \\textbf{vLinear}, an effective yet efficient \\textbf{linear}-based multivariate time series forecaster featuring two components: the \\textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \\textbf{velocity-oriented} flow matching objectives, we demonstrate that a \\textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.", "AI": {"tldr": "介绍vLinear，一种用于多变量时间序列预测的高效线性模型。", "motivation": "旨在解决现有基于自我注意力机制的时间序列预测模型计算复杂度高的问题，并提高预测准确性。", "method": "提出vecTrans模块和WFMLoss目标函数。vecTrans使用可学习向量建模多变量相关性，降低计算复杂度至O(N)；WFMLoss采用最终系列导向的损失函数策略，提升预测精度。", "result": "vLinear在22个基准测试和124种预测设置中达到最先进的性能，并能显著加速推理过程。", "conclusion": "展示了vLinear通过vecTrans和WFMLoss的有效性和高效性，在多变量时间序列预测中的优越表现。"}}
{"id": "2601.13761", "pdf": "https://arxiv.org/pdf/2601.13761", "abs": "https://arxiv.org/abs/2601.13761", "authors": ["Shengda Fan", "Xuyan Ye", "Yankai Lin"], "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.", "AI": {"tldr": "本文介绍了DARC，一个用于稳定大型语言模型自我进化过程的两阶段框架。", "motivation": "现有自博弈框架在优化稳定性方面存在问题，包括目标不稳定和自生成伪标签导致的引导误差。为了克服这些问题，提出了DARC来提高系统的性能。", "method": "首先训练Questioner以合成难度校准的问题，并根据显式难度级别和外部语料库进行条件化；然后使用文档增强的教师通过非对称自我蒸馏机制指导缺乏文档访问权限的学生Solver。", "result": "实验证明，DARC在九个推理基准测试和三个基础模型上平均提高了10.9分，并且始终优于所有基线，在不依赖人类标注的情况下接近完全监督模型的表现。", "conclusion": "DARC是一种无模型特性的框架，能够稳定大型语言模型的自我进化过程并显著提升其性能。"}}
{"id": "2601.13758", "pdf": "https://arxiv.org/pdf/2601.13758", "abs": "https://arxiv.org/abs/2601.13758", "authors": ["Lingling Dai", "Andong Li", "Cheng Chi", "Yifan Liang", "Xiaodong Li", "Chengshi Zheng"], "title": "GOMPSNR: Reflourish the Signal-to-Noise Ratio Metric for Audio Generation Tasks", "categories": ["cs.SD"], "comment": "Accepted by AAAI 2026", "summary": "In the field of audio generation, signal-to-noise ratio (SNR) has long served as an objective metric for evaluating audio quality. Nevertheless, recent studies have shown that SNR and its variants are not always highly correlated with human perception, prompting us to raise the questions: Why does SNR fail in measuring audio quality? And how to improve its reliability as an objective metric? In this paper, we identify the inadequate measurement of phase distance as a pivotal factor and propose to reformulate SNR with specially designed phase-distance terms, yielding an improved metric named GOMPSNR. We further extend the newly proposed formulation to derive two novel categories of loss function, corresponding to magnitude-guided phase refinement and joint magnitude-phase optimization, respectively. Besides, extensive experiments are conducted for an optimal combination of different loss functions. Experimental results on advanced neural vocoders demonstrate that our proposed GOMPSNR exhibits more reliable error measurement than SNR. Meanwhile, our proposed loss functions yield substantial improvements in model performance, and our wellchosen combination of different loss functions further optimizes the overall model capability.", "AI": {"tldr": "提出GOMPSNR以改进音频生成任务中的SNR度量，并开发了新的损失函数来优化模型性能。", "motivation": "现有SNR及其变体与人类感知不完全相关，因此需要提高其作为客观指标的可靠性。", "method": "通过引入专门设计的相位距离术语重新定义SNR，提出了GOMPSNR；并开发了两种新的损失函数类别，对应于幅度引导的相位细化和联合幅度-相位优化。", "result": "实验表明GOMPSNR比传统SNR更可靠地度量误差，并且新提出的损失函数显著提高了模型性能。", "conclusion": "通过改进SNR并开发新的损失函数，可以更好地评估音频质量和优化神经声码器的模型能力。"}}
{"id": "2601.13752", "pdf": "https://arxiv.org/pdf/2601.13752", "abs": "https://arxiv.org/abs/2601.13752", "authors": ["Chak Tou Leong", "Dingwei Chen", "Heming Xia", "Qingyu Yin", "Sunbowen Lee", "Jian Wang", "Wenjie Li"], "title": "Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering", "categories": ["cs.AI", "cs.CL"], "comment": "Working in progress", "summary": "Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \\textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.", "AI": {"tldr": "本文提出了一种名为RELIEF的框架，通过调整大型推理模型（LRM）的内部信念来改善其行为，无需依赖推理跟踪监督。", "motivation": "当前的方法在塑造LRM行为时通常依靠计算成本高昂且难以扩展的强化学习或使用金标准推理追踪进行微调。本文希望通过一种更简单有效的方式来解决这一问题。", "method": "RELIEF通过简单的logit探查捕捉模型内部跟踪自身推理特质的隐含信念，并通过微调自动生成的、自我反思的问题-答案对来使模型的行为与目标信念一致。", "result": "实验表明，RELIEF在效率和忠实度任务上可以匹配或超过行为监督和基于偏好的基线方法，同时训练成本更低。", "conclusion": "通过调整LRM的推理信念能够有效塑造其实际行为，并且这种方法比传统的需要推理跟踪的方法更高效、更具扩展性。"}}
{"id": "2601.13751", "pdf": "https://arxiv.org/pdf/2601.13751", "abs": "https://arxiv.org/abs/2601.13751", "authors": ["Daniel Kyselica", "Jonáš Herec", "Oliver Kutis", "Rado Pitoňák"], "title": "HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 9 figures, submitted to conference", "summary": "Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection", "AI": {"tldr": "本文提出了History Injection机制的Transformer模型（HiT），用于小型卫星上的连续洪水变化检测，以适应严格的运营限制。", "motivation": "自然灾害监测需要通过持续的卫星观察处理多时态数据，在严格的操作约束下进行。文章旨在开发一个能够在内存和计算能力有限的小型卫星上运行的有效洪灾检测系统。", "method": "提出History Injection机制用于Transformer模型，该机制在减少存储需求的同时保留历史观测信息，从而提高了洪水变化检测的效率。", "result": "测试显示，在STTORM-CD洪水数据集上的HiT机制下的Prithvi-tiny基础模型维持了与双时相基线相同的检测准确率，并且HiT-Prithvi模型在Jetson Orin Nano硬件上达到了43 FPS的处理速度。", "conclusion": "本文建立了一个基于卫星的自然灾害连续监测的实际框架，支持实时危害评估而不需要依赖地面基础设施。"}}
{"id": "2601.13749", "pdf": "https://arxiv.org/pdf/2601.13749", "abs": "https://arxiv.org/abs/2601.13749", "authors": ["Benaya Trabelsi", "Jonathan Shaki", "Sarit Kraus"], "title": "Pro-AI Bias in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "13 pages, 6 figures. Code available at: https://github.com/benayat/Pro-AI-bias-in-LLMs", "summary": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.", "AI": {"tldr": "本文研究了大型语言模型是否存在系统性的倾向于人工智能（AI）的偏好偏差。", "motivation": "随着大型语言模型越来越多地应用于决策支持，作者希望通过这项研究揭示这些模型是否存在对自身领域的系统性偏见。", "method": "通过三个互补实验来检测大型语言模型是否表现出对AI的偏好，包括推荐、薪资估计和内部表示探查。", "result": "发现大型语言模型在回答建议请求时更倾向于推荐与AI相关的选择，在估算薪酬时也高估了AI相关职位的薪酬，并且内部表示中显示“人工智能”与其他学术领域的相似度最高。", "conclusion": "这些模式表明，由大型语言模型生成的建议和估值可能系统性地偏斜决策者的选择和看法。"}}
{"id": "2601.13748", "pdf": "https://arxiv.org/pdf/2601.13748", "abs": "https://arxiv.org/abs/2601.13748", "authors": ["Tien-Dat Pham", "Xuan-The Tran"], "title": "EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation", "AI": {"tldr": "本文提出了EEG-Titans模型，用于通过双分支注意力机制和神经记忆实现长期癫痫发作预测。", "motivation": "准确的癫痫发作预测从脑电图(EEG)数据中仍然具有挑战性，因为预发性动态可能跨越长时间段，而临床上相关的特征可能微妙且短暂。许多深度学习模型在操作超长序列时面临捕捉局部时空模式与维护信息丰富的长距离上下文之间的持续权衡。", "method": "本文提出了EEG-Titans，一种双分支架构，结合了现代神经记忆机制以进行长期上下文建模。该模型将滑动窗口注意力用于捕获短期异常，并通过递归记忆路径汇总随时间变化的较慢、渐进趋势。", "result": "在CHB-MIT头皮EEG数据集上评估时，使用时序保留协议下，EEG-Titans实现了18个受试者平均段级敏感性为99.46%。对于易出现伪影的记录，在高噪声受试者的接收字段扩展策略下，该模型显著减少了误报（极端情况下降至0.00 FPR/h），且不牺牲敏感性。", "conclusion": "研究表明，记忆增强型长上下文建模可以在临床约束评价中提供稳健的癫痫发作预测。"}}
{"id": "2601.13737", "pdf": "https://arxiv.org/pdf/2601.13737", "abs": "https://arxiv.org/abs/2601.13737", "authors": ["Joon Lee", "Jeongyoon Han", "Doyoung Kim", "Seokhwan Jeong"], "title": "RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure", "categories": ["cs.RO", "eess.SY"], "comment": "Soft Robotics", "summary": "This paper presents the flexible RIM Hand, a biomimetic robotic hand that precisely replicates the carpometacarpal (CMC) joints and employs superelastic Nitinol wires throughout its skeletal framework. By modeling the full carpal-to-metacarpal anatomy, the design enables realistic palm deformation through tendon-driven fingers while enhancing joint restoration and supports skeletal structure with Nitinol-based dorsal extensors. A flexible silicone skin further increases contact friction and contact area, enabling stable grasps for diverse objects. Experiments show that the palm can deform up to 28%, matching human hand flexibility, while achieving more than twice the payload capacity and three times the contact area compared to a rigid palm design. The RIM Hand thus offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.", "AI": {"tldr": "介绍了一种灵活的RIM手，通过模仿人类掌指关节和使用镍钛记忆合金线来实现仿生机器人手的设计。", "motivation": "旨在设计一种具有高柔顺性、灵巧性和人体相似性的机器人手，以满足假肢和服务机器人的应用需求。", "method": "通过精确复制人类的掌腕骨（CMC）关节，并在骨骼结构中使用镍钛记忆合金线来增强支撑。同时利用灵活的硅胶皮肤增加摩擦力和接触面积。", "result": "实验表明，RIM手能够实现28%的手掌变形能力，达到人手灵活性标准；负载能力和接触面积分别是刚性设计的两倍和三倍以上。", "conclusion": "RIM手在柔顺性、灵巧性和人体相似性方面表现出色，为假肢和服务机器人应用提供了有前景的设计方案。"}}
{"id": "2601.13735", "pdf": "https://arxiv.org/pdf/2601.13735", "abs": "https://arxiv.org/abs/2601.13735", "authors": ["Hojin Kim", "Jaehyung Kim"], "title": "Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection", "categories": ["cs.AI"], "comment": "15 pages, 4 figures", "summary": "Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.", "AI": {"tldr": "本文研究了概率置信度指标在Best-of-N选择中是否真正反映了推理的质量，发现这些指标主要捕捉到的是表面流畅性而非逻辑结构。", "motivation": "挑战现有观点，即较高的概率置信度反映更高的推理质量，通过探索这些指标是否真的捕获了必要的跨步骤因果依赖关系。", "method": "引入三种类型的跨步骤因果扰动来系统地破坏推理步骤之间的依赖关系，同时保持局部流畅性，并提出对比因果性指标以明确隔离跨步骤的因果依赖。", "result": "发现即使在严重干扰下（如使用硬注意力掩码阻止模型关注先前的推理步骤），选择准确性仅略微下降，表明当前的概率置信度指标对逻辑结构不敏感，主要捕获表面流畅性和分布内先验。", "conclusion": "提出的新对比因果性指标比现有的概率基方法更能忠实于输出选择。"}}
{"id": "2601.13734", "pdf": "https://arxiv.org/pdf/2601.13734", "abs": "https://arxiv.org/abs/2601.13734", "authors": ["Chenyu Hui"], "title": "Towards robust long-context understanding of large language model via active recap learning", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages", "summary": "In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM", "AI": {"tldr": "提出主动回顾学习（ARL）框架，增强大型语言模型处理长文本上下文的理解能力。", "motivation": "针对大型语言模型在理解和处理长上下文时的不足，动机在于开发一种能够加强其在这方面的理解能力的方法。", "method": "ARL通过识别关键令牌、基于损失差距找到最相关的先前段落并使用LLM进行总结。此外，在推断过程中，模型可以自主生成和利用这些回顾摘要来建立跨段落的记忆机制。", "result": "实验显示ARL在RULER上取得了26.8%的改进，在LongBench上获得了9.44%的进步。", "conclusion": "ARL作为一种简单有效的持续预训练方法，增强了长上下文的理解能力，并推进了LLM中可扩展记忆增强的发展。"}}
{"id": "2601.13732", "pdf": "https://arxiv.org/pdf/2601.13732", "abs": "https://arxiv.org/abs/2601.13732", "authors": ["Andreas Wiedholz", "Rafael Paintner", "Julian Gleißner", "Alwin Hoffmann", "Tobias Huber"], "title": "SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation", "categories": ["cs.RO"], "comment": null, "summary": "The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison.", "AI": {"tldr": "介绍SUNSET，一个基于ROS2的自适应机器人系统示例，用于评估在不确定环境下软件系统的自我调整能力。", "motivation": "随着机器人越来越多地部署到动态环境中，并且软件系统变得越来越复杂，需要开发自适应的方法来应对不确定性环境中的挑战。", "method": "SUNSET通过一个传感器融合和语义分割的管道实现，使用了一个机器学习模型，该模型的输入预处理可以被扰动以模拟实际性能下降。", "result": "示例提供了五个可观察的症状，并支持同时存在的不确定情况，包括自我修复和自我优化，从而提供了一种严格的、可重复评估架构自适应的方法。", "conclusion": "SUNSET包括分割管道、训练的机器学习模型、不确定性注入脚本、基线控制器以及详细的集成和评估文档，以促进可复制的研究和公平比较。"}}
{"id": "2601.13724", "pdf": "https://arxiv.org/pdf/2601.13724", "abs": "https://arxiv.org/abs/2601.13724", "authors": ["Sam Cantrill", "David Ahmedt-Aristizabal", "Lars Petersson", "Hanna Suominen", "Mohammad Ali Armin"], "title": "Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement", "categories": ["cs.CV"], "comment": ":I.4.9", "summary": "Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .", "AI": {"tldr": "本文提出了面部时空图（STGraph）和MeshPhys模型，用于通过远程光体积描记法（rPPG）估计生理信号。", "motivation": "现有的rPPG方法未能显式地将其感受野与三维面部表面对齐，这限制了其性能。因此，提出了一种新的表示形式来解决这一问题。", "method": "提出了一个名为STGraph的新表示形式，用于编码3D面部网格序列的颜色和结构，并引入了MeshPhys模型来估计生理信号。", "result": "在四个基准数据集上，MeshPhys实现了最先进的或具有竞争力的性能。消融研究显示，将模型的感受野限制在面部表面上对提高性能有显著影响。", "conclusion": "STGraph和MeshPhys构成了一个新颖的原则性建模范式，用于面部rPPG估计，这使得生理信号的估算更加稳健、可解释性和普遍适用。"}}
{"id": "2601.13722", "pdf": "https://arxiv.org/pdf/2601.13722", "abs": "https://arxiv.org/abs/2601.13722", "authors": ["Yulin Hu", "Zimo Long", "Jiahe Guo", "Xingyu Sui", "Xing Fu", "Weixiang Zhao", "Yanyan Zhao", "Bing Qin"], "title": "OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \\emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \\textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \\textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \\textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.", "AI": {"tldr": "本文提出了OP-Bench，一个评估过度个性化问题的基准，并提出了一种轻量级的记忆过滤机制Self-ReCheck来缓解这一问题。", "motivation": "现有研究主要关注对话系统是否能正确应用用户信息，而忽视了个性化使用是否适当的问题。作者认为过度个性化会导致用户的不适感，因此提出了此问题并进行了深入探讨。", "method": "作者定义了三种类型的过度个性化：无关性、重复性和逢迎性，并构建了一个包含1700个验证实例的基准OP-Bench来评估多种大型语言模型和记忆增强方法。同时提出了一种轻量级的记忆过滤机制Self-ReCheck。", "result": "研究发现，当引入记忆功能时，过度个性化的问题普遍存在。进一步分析显示对话系统倾向于检索并过分关注用户记忆，即使在不需要的情况下也是如此。", "conclusion": "本工作为更可控且适当的个性化提供了初步步骤，在记忆增强的对话系统中缓解了过度个性化问题的同时保持了个性化的性能。"}}
{"id": "2601.13719", "pdf": "https://arxiv.org/pdf/2601.13719", "abs": "https://arxiv.org/abs/2601.13719", "authors": ["Xinlei Yin", "Xiulian Peng", "Xiao Li", "Zhiwei Xiong", "Yan Lu"], "title": "Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.", "AI": {"tldr": "本文提出了HAVEN框架，用于处理长视频理解问题，并通过整合视听实体连贯性和分层视频索引实现高效的动态检索和推理。", "motivation": "传统的基于简单分割策略的解决方案在理解和生成长期上下文窗口中的信息时存在困难，导致信息碎片化和全局连贯性的丧失。因此，该论文旨在解决这些挑战并提供更全面和连贯的理解方法。", "method": "HAVEN框架通过整合视听流中的实体级表示来保持语义一致性，并使用结构化的层次组织视频内容。此外，它采用代理搜索机制进行动态检索和推理，以重建连贯的叙述并跟踪细节实体。", "result": "实验表明该方法在时间连贯性、实体一致性和检索效率方面表现出色，在LVBench上的总体准确率达到84.1%，尤其在复杂推理类别中的表现达到80.1%。", "conclusion": "结果证明了结构化多模态推理对于全面且上下文一致性地理解长视频的有效性，从而确立了新的状态水平。"}}
{"id": "2601.13717", "pdf": "https://arxiv.org/pdf/2601.13717", "abs": "https://arxiv.org/abs/2601.13717", "authors": ["Zehan Li", "Yuxuan Wang", "Ali El Lahib", "Ying-Jieh Xia", "Xinyu Pi"], "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.", "AI": {"tldr": "研究模拟无知（SI）是否能近似真实无知（TI），评估大语言模型在预测任务上的表现。", "motivation": "评估LLM的预测能力存在方法论严谨性和数据时效性的矛盾，而模拟无知作为一种潜在解决方案被系统性测试。", "method": "通过477个竞赛级问题和9种模型，测试了模拟无知（SI）是否能近似真实无知（TI）。", "result": "发现模拟无知失败：存在52%的性能差距，推理链无法抑制先验知识，优化推理模型表现更差。", "conclusion": "提示不能可靠地“回溯”模型的知识；不建议使用基于SI的回顾性设置来基准化预测能力。"}}
{"id": "2601.13715", "pdf": "https://arxiv.org/pdf/2601.13715", "abs": "https://arxiv.org/abs/2601.13715", "authors": ["Yiwei Lu", "Hao Huang", "Tao Yan"], "title": "MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network", "categories": ["cs.CV"], "comment": "This paper has been accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI-26). It contians 9 pages, 11 figures", "summary": "Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.", "AI": {"tldr": "本文提出MVGD-Net，一种利用运动不一致性线索检测视频中玻璃表面的新网络。", "motivation": "鉴于玻璃表面在日常和专业环境中对基于视觉系统的潜在威胁，本文旨在开发有效的方法来识别视频中的玻璃表面。", "method": "MVGD-Net包含三个新模块：跨尺度多模态融合模块（CMFM）、历史引导注意力模块（HGAM）以及时间交叉注意力模块（TCAM），用于融合空间特征和光流图，并引入时空解码器（TSD）生成玻璃区域掩膜。", "result": "实验表明，MVGD-Net优于相关最先进方法，在大规模包含312种不同场景共19,268帧的自建数据集上表现突出。", "conclusion": "本文提出的MVGD-Net通过利用运动不一致性线索有效检测视频中的玻璃表面，并证明了其在识别任务上的优越性。"}}
{"id": "2601.13710", "pdf": "https://arxiv.org/pdf/2601.13710", "abs": "https://arxiv.org/abs/2601.13710", "authors": ["Sayeed Shafayet Chowdhury", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Vijay R. Ramakrishnan"], "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.", "AI": {"tldr": "本文研究了使用AI模型预测慢性鼻窦炎手术后临床改善情况，并比较了监督学习与生成式AI在这一任务上的表现。", "motivation": "尽管人工智能已经改变了医疗影像领域，但在基于临床数据进行前瞻性决策支持方面的应用仍然有限。本研究旨在探索是否可以通过预手术的临床数据分析来识别那些手术结果可能不佳的患者。", "method": "本文使用了一个前瞻性收集的数据集，在这个数据集中所有患者都接受了手术。作者将监督学习模型（如逻辑回归、树集成和自研多层感知器）与生成式AI（如ChatGPT、Claude、Gemini、Perplexity）进行比较，输入相同的结构化数据，并限制输出为二元推荐意见。", "result": "结果表明，最佳的监督学习模型（MLP）达到了85%的准确率，并且在校准和决策曲线净效益上表现更优。生成式AI在区分能力和校准方面普遍欠佳，尽管其解释与临床专家直觉及特征重要性相吻合。", "conclusion": "研究支持了一个基于机器学习为主、结合生成式AI以增强透明度和共同决策的工作流程：首先使用经过校准的监督学习模型进行初步筛选，之后用生成式AI提供详细解释。"}}
{"id": "2601.13709", "pdf": "https://arxiv.org/pdf/2601.13709", "abs": "https://arxiv.org/abs/2601.13709", "authors": ["Christopher Kao", "Vanshika Vats", "James Davis"], "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SI"], "comment": "For associated dataset, see https://github.com/cocochief4/llm-mafia. Published in IEEE ICA 2025, waiting for IEEEXplore proceedings", "summary": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.", "AI": {"tldr": "研究使用社会推理游戏《Mafia》来评估大型语言模型（LLM）在社交情境中的欺骗能力，并与人类进行比较。", "motivation": "鉴于大型语言模型广泛应用于各种应用中，且先前研究表明这些模型能够在控制任务下实施欺骗行为，但它们在自然语言环境中如何欺骗尚不清楚。本研究旨在填补这一空白。", "method": "通过异步多智能体框架，在《Mafia》游戏中使用GPT-4o LLM代理进行35次游戏模拟，并创建一个基于GPT-4-Turbo的Mafia检测器来分析对话记录，以评估LLM欺骗质量。", "result": "结果表明，Mafia检测器在识别由大型语言模型扮演的游戏中的黑手党玩家时准确率低于人类游戏，这表明LLMs更能融入并有效欺骗。", "conclusion": "研究表明，LLMs在社交环境中表现出高超的欺骗能力，同时也强调了这种技术带来的潜在风险。"}}
{"id": "2601.13707", "pdf": "https://arxiv.org/pdf/2601.13707", "abs": "https://arxiv.org/abs/2601.13707", "authors": ["Yujin Jo", "Sangyoon Bae", "Taesup Kim"], "title": "Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.", "AI": {"tldr": "本文提出了一种名为注意力空间对比引导（ACG）的方法，用于在大视觉语言模型中有效减轻幻觉现象。", "motivation": "解决大型视觉语言模型中存在的幻觉问题，即当语言先验主导而忽视视觉证据时导致的物体误识别和描述不一致问题。", "method": "提出注意力空间对比引导（ACG），通过单次前向计算在自注意力层内构造视觉-语言路径和纯语言路径，并应用正交化校正来消除偏向，放大视觉贡献。", "result": "实验结果表明，在CHAIR和POPE基准上，ACG实现了最先进的保真度和描述质量，同时显著降低了计算成本，减少了高达2倍的延迟。", "conclusion": "该方法提供了一种原理性且高效的替代方案，可以有效减少大型视觉语言模型中的幻觉现象，并降低计算成本。"}}
{"id": "2601.13706", "pdf": "https://arxiv.org/pdf/2601.13706", "abs": "https://arxiv.org/abs/2601.13706", "authors": ["Xinhao Liu", "Yu Wang", "Xiansheng Guo", "Gordon Owusu Boateng", "Yu Cao", "Haonan Si", "Xingchen Guo", "Nirwan Ansari"], "title": "ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins", "categories": ["cs.CV"], "comment": "35 pages, 10 figures. Submitted to ISPRS Journal of Photogrammetry and Remote Sensing. Under review", "summary": "High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/", "AI": {"tldr": "ParkingTwin 是一个无需训练、轻量级的系统，用于停车场数字孪生的实时流式三维重建。", "motivation": "高保真度的停车场数字孪生为自动代客泊车提供必要的先验信息。然而，机器人导向的重构面临着三个难题：稀疏前向视图导致弱视差和不良几何形状；动态遮挡和极端光照条件影响纹理融合稳定性；神经渲染通常需要昂贵的离线优化。", "method": "ParkingTwin 系统通过 OpenStreetMap 的语义拓扑直接生成一个度量一致的TSDF，运用四模态约束场（法向/高度/深度一致性）实时拒绝移动车辆和暂时遮挡，并在 CIELAB 中解耦亮度和色度以减少突然光照变化下的接缝。", "result": "ParkingTwin 可以在入门级 GTX 1660 上达到30+ FPS的速度，在一个实际数据集上实现 SSIM 0.87，提供大约15倍的端到端加速，并将 GPU 内存减少83.3%。", "conclusion": "ParkingTwin 提供了高效的停车场数字孪生重建解决方案，能够在低硬件成本下实时生成高质量三维模型。"}}
{"id": "2601.13705", "pdf": "https://arxiv.org/pdf/2601.13705", "abs": "https://arxiv.org/abs/2601.13705", "authors": ["Maria Lymperaiou", "Vasileios Karampinis", "Giorgos Filandrianos", "Angelos Vlachos", "Chrysoula Zerva", "Athanasios Voulodimos"], "title": "Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles", "categories": ["cs.CV"], "comment": null, "summary": "Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.", "AI": {"tldr": "本文通过视觉谜题来评估大型视觉语言模型（LVLMs）的推理能力，提供了一个统一的观点。", "motivation": "作者认为视觉谜题是诊断LVLMs推理能力的强大工具，并希望通过这种诊断揭示当前模型存在的局限性。", "method": "通过对现有基准进行分类和分析，将视觉谜题分为五种推理机制类型（归纳、类比、算法、演绎和几何/空间），并以此为基础对LVLMs的推理性能进行了评估。", "result": "发现现有模型存在脆化的泛化能力、感知与推理之间的紧密纠缠以及流利解释与忠实执行之间持续存在的差距。", "conclusion": "通过将视觉谜题作为诊断工具，本文概述了当前LVLMs推理状态，并为未来基准和具有推理意识的多模态系统指出了关键方向。"}}
{"id": "2601.13704", "pdf": "https://arxiv.org/pdf/2601.13704", "abs": "https://arxiv.org/abs/2601.13704", "authors": ["Esteban Gómez", "Tom Bäckström"], "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.", "AI": {"tldr": "本文提出了一种基于特征噪声注入的再参数化技术，以实现性能和计算复杂度之间的优化。", "motivation": "传统的神经网络模型在设计时通常采用固定层数和结构，并且通过启发式方法选择层大小。这种方法不能保证最优的性能与计算复杂性的权衡，在训练后需要使用量化或剪枝等方法来减少计算成本。", "method": "本文提出的技术基于特征噪声注入，允许使用基于SGD的方法在训练过程中联合优化性能和计算复杂性，实现了模型大小动态优化以达到目标性能-复杂度的最优权衡。", "result": "作者通过三个案例研究证明了方法的有效性，包括一个合成示例和两个实际应用：语音活动检测和音频反欺骗。", "conclusion": "该技术提供了一种有效的手段，在训练过程中实现性能与计算成本之间的优化，无需依赖启发式准则来选择要移除的权重或结构。"}}
{"id": "2601.13700", "pdf": "https://arxiv.org/pdf/2601.13700", "abs": "https://arxiv.org/abs/2601.13700", "authors": ["Jianing Yang", "Wataru Nakata", "Yuki Saito", "Hiroshi Saruwatari"], "title": "DistilMOS: Layer-Wise Self-Distillation For Self-Supervised Learning Model-Based MOS Prediction", "categories": ["cs.SD"], "comment": "Accepted to ICASSP 2026", "summary": "With the advancement of self-supervised learning (SSL), fine-tuning pretrained SSL models for mean opinion score (MOS) prediction has achieved state-of-the-art performance. However, during fine-tuning, these SSL-based MOS prediction models often suffer from catastrophic forgetting of the pretrained knowledge and tend to overfit the training set, resulting in poor generalization performance. In this study, we propose DistilMOS, a novel method that learns to predict not only MOS but also token IDs obtained by clustering the hidden representations of each layer in the pretrained SSL model. These layer-wise token targets serve as self-distillation signals that enables the MOS prediction model to extract rich internal knowledge from SSL models, enhancing both prediction accuracy and generalization capability. Experimental evaluations demonstrate that our method significantly outperforms standard SSL-based MOS prediction models on both in-domain and out-of-domain evaluations, verifying the effectiveness and practicality of the proposed method.", "AI": {"tldr": "提出DistilMOS方法，通过层级自蒸馏提升基于自监督学习的MOS预测模型的表现。", "motivation": "现有SSL模型在进行微调时容易遗忘预训练知识并倾向于过拟合，导致泛化性能不佳。", "method": "DistilMOS方法除了预测MOS外，还通过聚类预训练SSL模型中每一层的隐藏表示来学习token ID，以此作为自蒸馏信号提升内部知识提取。", "result": "实验结果表明，该方法在域内和域外评估上均显著优于标准的基于SSL的MOS预测模型。", "conclusion": "DistilMOS的有效性和实用性得到了验证。"}}
{"id": "2601.13698", "pdf": "https://arxiv.org/pdf/2601.13698", "abs": "https://arxiv.org/abs/2601.13698", "authors": ["Arjun Nichani", "Hsiang Hsu", "Chun-Fu", "Chen", "Haewon Jeong"], "title": "Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation", "categories": ["cs.LG", "cs.AI", "cs.IT", "stat.ML"], "comment": null, "summary": "Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.", "AI": {"tldr": "本文探讨了公平性、隐私和准确性之间的关系，并使用Chernoff Information来分析这些因素在不同数据集中的相互作用。", "motivation": "尽管对公平性和隐私的研究已经很深入，但两者之间的关系却很少受到关注。本研究旨在揭示这三者之间的关系，并指出其与数据分布相关的特性。", "method": "定义了Noisy Chernoff Difference来同时分析公平性、隐私和准确性之间的关系，并提出了一种用于估计未知分布数据的Chernoff Information的方法。", "result": "对于合成数据，Noisy Chernoff Difference表现出三种不同的行为模式，这些模式与数据分布有关且具有不同的公平性和隐私含义。此外，还表明了它作为衡量公平性-准确性曲线陡峭程度的代理指标的作用。", "conclusion": "研究强调了三者关系的数据依赖性，并提出了一种框架来进一步理解这一复杂动态，特别是在真实数据集上。"}}
{"id": "2601.13697", "pdf": "https://arxiv.org/pdf/2601.13697", "abs": "https://arxiv.org/abs/2601.13697", "authors": ["Zhihang Yuan", "Chengyu Yue", "Long Huang", "Litu Ou", "Lei Shi"], "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.", "AI": {"tldr": "本文提出了一种基于不确定性感知的数据选择框架GRADFILTERING，用于改进大规模语言模型的指令调优。", "motivation": "现有的数据选择方法要么建立昂贵的梯度数据集，要么使用弱代理分配静态分数，忽视了不确定性的动态变化，从而错过了LLM可解释性的一个关键来源。", "method": "GRADFILTERING是一个目标无关的、不确定性感知的数据选择框架，利用小规模的GPT-2代理和LoRA集成，并将每条示例梯度聚合为一个梯度信噪比（G-SNR）效用。", "result": "该方法在大多数LLM作为评判者以及人类评估中与随机子集和强基线匹敌或超越，且在相同计算预算下比竞争过滤器更快收敛。", "conclusion": "GRADFILTERING有效地提高了指令调优过程中数据选择的效率和效果，并通过不确定性感知评分带来了显著优势。"}}
{"id": "2601.13693", "pdf": "https://arxiv.org/pdf/2601.13693", "abs": "https://arxiv.org/abs/2601.13693", "authors": ["Shengjie Xu", "Xianbin Ye", "Mengran Zhu", "Xiaonan Zhang", "Shanzhuo Zhang", "Xiaomin Fang"], "title": "End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. Despite its critical role, reverse screening remains challenging because accurately capturing interactions between a small molecule and structurally diverse proteins is inherently complex, and conventional step-wise workflows often propagate errors across decoupled steps such as target structure modeling, pocket identification, docking, and scoring. Here, we present an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model akin to AlphaFold3, which simultaneously models the folding of proteins from a protein library and the docking of small-molecule ligands within a unified framework. We validate this approach on a diverse and representative set of approximately one hundred small molecules. Compared with conventional reverse docking, our method improves screening accuracy and demonstrates enhanced structural fidelity, binding-site precision, and target prioritization. By systematically linking small molecules to their protein targets, this framework establishes a scalable and straightforward platform for dissecting molecular mechanisms, exploring off-target interactions, and supporting rational drug discovery.", "AI": {"tldr": "本文介绍了一种利用HelixFold3进行端到端逆向筛选以识别小分子的蛋白质靶点的方法。", "motivation": "理解药物作用机制，指导化合物再利用，预测非预期副作用，并阐明生物活性化合物的分子机制需要准确识别小分子的蛋白质靶点。传统的分步骤工作流程容易在各个阶段累积错误，导致逆向筛选困难。", "method": "本文提出了一种基于HelixFold3模型的端到端逆向筛选策略，该方法可以同时建模蛋白库中蛋白质的折叠和小分子配体的对接，在统一框架下进行。", "result": "与传统的反向对接相比，这种方法提高了筛选精度，并显示出结构保真度、结合位点精确性以及靶标优先排序的改进。", "conclusion": "通过系统地将小分子与其蛋白质靶点关联起来，此框架提供了一个可扩展且易于使用的平台用于解析分子机制，探索非预期相互作用并支持有理药物发现。"}}
{"id": "2601.13689", "pdf": "https://arxiv.org/pdf/2601.13689", "abs": "https://arxiv.org/abs/2601.13689", "authors": ["Vahid Pooryousef", "Lonni Besançon", "Maxime Cordeil", "Chris Flight", "Alastair M Ross AM", "Richard Bassed", "Tim Dwyer"], "title": "Criminator: An Easy-to-Use XR \"Crime Animator\" for Rapid Reconstruction and Analysis of Dynamic Crime Scenes", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Law enforcement authorities are increasingly interested in 3D modelling for virtual crime scene reconstruction, enabling offline analysis without the cost and contamination risk of on-site investigation. Past work has demonstrated spatial relationships through static modelling but validating the sequence of events in dynamic scenarios is crucial for solving a case. Yet, animation tools are not well suited to crime scene reconstruction, and complex for non-experts in 3D modelling/animation. Through a co-design process with criminology experts, we designed \"Criminator\"-a methodological framework and XR tool that simplifies animation authoring. We evaluated this tool with participants trained in criminology (n=6) and untrained individuals (n=12). Both groups were able to successfully complete the character animation tasks and provided high usability ratings for observation tasks. Criminator has potential for hypothesis testing, demonstration, sense-making, and training. Challenges remain in how such a tool fits into the entire judicial process, with questions about including animations as evidence.", "AI": {"tldr": "介绍了一种名为Criminator的XR工具，用于简化犯罪现场动态重建和分析中的动画创作。", "motivation": "解决现有动画工具在犯罪现场重建中复杂且不适合非专家用户的问题，提升法律执行机关对虚拟犯罪场景进行离线分析的能力。", "method": "通过与犯罪学专家合作设计了Criminator，一个简化动画创作的方法框架和XR工具，并对其进行了评估测试。", "result": "参与测试的受训和未受训参与者均成功完成了角色动画任务并给出了高可用性评价。", "conclusion": "Criminator具有用于假设检验、展示、理解和培训的潜力，但仍需解决将其整合进整个司法过程中的挑战。"}}
{"id": "2601.13687", "pdf": "https://arxiv.org/pdf/2601.13687", "abs": "https://arxiv.org/abs/2601.13687", "authors": ["Zhichao Liang", "Satoshi Nakamura"], "title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue", "categories": ["cs.AI"], "comment": null, "summary": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.", "AI": {"tldr": "本文介绍了SocialMindChange基准，旨在评估语言模型在多个人群对话中通过改变参与者心理状态来引导社交影响的能力。", "motivation": "现有的动态心智理论（ToM）基准测试主要使语言模型处于被动角色。而真正的社会互动中，ToM用于计划行动以达到目标。因此，引入SocialMindChange来评估和提升这一能力。", "method": "构建了包含4个角色的社交情境，通过五连场景测试模型在对话中的表现，并采用结构化的四步框架构造了1200种社交情景，涵盖6000个场景及超过90,000个问题。", "result": "评估结果显示当前最先进的语言模型平均性能比人类低54.2%，表明它们在维护和改变长时间连锁互动中的心理状态表示方面仍然存在困难。", "conclusion": "研究结果揭示了现有LLMs在复杂社交互动中应用ToM的局限性，并指出了未来改进的方向。"}}
{"id": "2601.13684", "pdf": "https://arxiv.org/pdf/2601.13684", "abs": "https://arxiv.org/abs/2601.13684", "authors": ["Zhiyuan Shi", "Qibo Qiu", "Feng Xue", "Zhonglin Jiang", "Li Yu", "Jian Jiang", "Xiaofei He", "Wenxiao Wang"], "title": "HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\\times$ compared to the original model in the 224K context. Our code will be open-source.", "AI": {"tldr": "本文提出HeteroCache，一种无训练的动态压缩框架，用于解决长上下文LLM推理中KV缓存内存增长问题。", "motivation": "现有的静态压缩方法无法保持全局重要信息，因为它们忽视了注意力漂移现象和粗粒度缓存策略导致的高I/O开销。", "method": "HeteroCache基于注意力头的时间异构性和空间冗余性分类，并采用细粒度权重策略分配缓存预算，同时使用层次存储机制来减少I/O延迟。", "result": "实验表明，HeteroCache在多个长上下文基准测试中达到最新技术水平，并将解码加速最高达3倍。", "conclusion": "通过HeteroCache框架有效解决了KV缓存在长上下文LLM推理中的效率问题，提升了性能和速度。"}}
{"id": "2601.13683", "pdf": "https://arxiv.org/pdf/2601.13683", "abs": "https://arxiv.org/abs/2601.13683", "authors": ["Boyuan Cao", "Xingbo Yao", "Chenhui Wang", "Jiaxin Ye", "Yujie Wei", "Hongming Shan"], "title": "Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.", "AI": {"tldr": "本文介绍了一种名为动态微分线性注意力（DyDiLA）的新方法，以改进线性扩散变压器（LiTs），提高图像生成质量并解决过度平滑的问题。", "motivation": "动力源于自注意力机制的二次成本对扩散变压器（DiTs）的扩展构成重大瓶颈，而现有的线性注意力虽然减少了计算成本但牺牲了生成性能。", "method": "方法包括动态投影模块、动态测量内核和令牌微分算子，以改善LiT模型中的过度平滑问题并提高生成质量。这些设计共同构成了一个改进后的线性扩散变压器（DyDi-LiT）。", "result": "实验结果表明，新提出的DyDi-LiT在多个指标上优于当前最先进模型，证明其强大的实际应用潜力。", "conclusion": "结论是通过提出动态微分线性注意力方法并应用于改进的LiT模型中（命名为DyDi-LiT），能够有效解决过度平滑问题，并显著提高图像生成的质量和性能。"}}
{"id": "2601.13679", "pdf": "https://arxiv.org/pdf/2601.13679", "abs": "https://arxiv.org/abs/2601.13679", "authors": ["Sangwon Park", "Dongjun Kim", "Sung-Hoon Byun", "Sangwook Park"], "title": "Ultra-Lightweight Network for Ship-Radiated Sound Classification on Embedded Deployment", "categories": ["cs.SD"], "comment": "This manuscript is under review at IEEE Geoscience and Remote Sensing Letters", "summary": "This letter presents ShuffleFAC, a lightweight acoustic model for ship-radiated sound classification in resource-constrained maritime monitoring systems. ShuffleFAC integrates Frequency-Aware convolution into an efficiency-oriented backbone using separable convolution, point-wise group convolution, and channel shuffle, enabling frequency-sensitive feature extraction with low computational cost. Experiments on the DeepShip dataset show that ShuffleFAC achieves competitive performance with substantially reduced complexity. In particular, ShuffleFAC ($γ=16$) attains a macro F1-score of 71.45 $\\pm$ 1.18% using 39K parameters and 3.06M MACs, and achieves an inference latency of 6.05 $\\pm$ 0.95ms on a Raspberry Pi. Compared with MicroNet0, it improves macro F1-score by 1.82 % while reducing model size by 9.7x and latency by 2.5x. These results indicate that ShuffleFAC is suitable for real-time embedded UATR.", "AI": {"tldr": "本文介绍了ShuffleFAC，一种适用于资源受限的海事监测系统的轻量级声学模型，用于舰船辐射噪声分类。", "motivation": "为了在嵌入式设备上实现实时无源声目标识别（UATR），需要一个计算成本低且性能良好的轻量化网络。", "method": "ShuffleFAC通过整合频率感知卷积、分组点卷积以及通道洗牌技术，构建了一个高效的骨干网络，以实现对频段敏感的特征提取。", "result": "实验表明，在DeepShip数据集上，ShuffleFAC以39K参数和3.06M MACs达到71.45%（±1.18）的宏观F1分数，并在Raspberry Pi上的推理延迟仅为6.05ms（±0.95）。相较于MicroNet0，它提高了宏观F1分数1.82%，同时模型大小减少了9.7倍，延迟降低了2.5倍。", "conclusion": "ShuffleFAC证明了其在实时嵌入式UATR中的适用性，尤其是在资源受限的环境中。"}}
{"id": "2601.13677", "pdf": "https://arxiv.org/pdf/2601.13677", "abs": "https://arxiv.org/abs/2601.13677", "authors": ["Carsten T. Lüth", "Jeremias Traub", "Kim-Celine Kahl", "Till J. Bungert", "Lukas Klein", "Lars Krämer", "Paul F. Jäger", "Klaus Maier-Hein", "Fabian Isensee"], "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging", "categories": ["cs.CV"], "comment": "Accepted at TMLR", "summary": "Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.", "AI": {"tldr": "本文提出了一种名为ClaSP PE的新方法，用于解决在三维生物医学图像分割中的主动学习问题。", "motivation": "现有的主动学习方法无法一致地超越改进后的随机基线，在三维生物医学图像标注中存在时间和成本高的问题。", "method": "ClaSP PE结合了类别分层查询和带有衰减计划的对数尺度幂噪声，以确保覆盖不充分代表的结构，并在早期阶段增加查询多样性，在后期鼓励利用。", "result": "实验结果表明，ClaSP PE在24种实验设置中普遍优于改进后的随机基线，在标注效率和分割质量方面有统计显著性的提升。该方法在未见过的数据集上也表现出良好的泛化能力。", "conclusion": "本文证明了主动学习方法可以在三维生物医学图像分割任务中一致地超越随机基线，ClaSP PE提供了一个简单有效的解决方案，并且可以实际应用。"}}
{"id": "2601.13671", "pdf": "https://arxiv.org/pdf/2601.13671", "abs": "https://arxiv.org/abs/2601.13671", "authors": ["Apoorva Adimulam", "Rajesh Gupta", "Sumit Kumar"], "title": "The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.", "AI": {"tldr": "本文介绍了多智能体系统的编排架构和协议，并为企业的AI生态系统提供了实现原则。", "motivation": "该研究旨在通过整合规划、政策执行、状态管理和质量操作来构建统一的编排层，以推动多智能体系统的发展。", "method": "提出了一种集成框架和两个通信协议（模型上下文协议和Agent2Agent协议），并详细描述了如何将这些元素综合成一个连贯的技术蓝图。", "result": "实现了可扩展、审计和符合政策的推理机制，提供了全面处理编排多智能体系统的蓝图。", "conclusion": "通过提出统一架构框架和通信协议，研究为企业的AI生态系统的实现原则奠定了基础。"}}
{"id": "2601.13665", "pdf": "https://arxiv.org/pdf/2601.13665", "abs": "https://arxiv.org/abs/2601.13665", "authors": ["Mounika Kanulla", "Rajasree Dadigi", "Sailaja Thota", "Vivek Yelleti"], "title": "Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.", "AI": {"tldr": "本论文提出了一个基于Transformer的多任务融合网络，用于食物变质检测和保质期预测。", "motivation": "减少食品浪费是农业供应链中的关键挑战之一，准确有效的变质检测有助于减少这种情况，并且预测变质信息对于延长供应链管理至关重要。", "method": "本论文通过结合CNN与LSTM及DeiT Transformer提出了融合架构，以同时处理蔬菜分类、食物变质检测和保质期预测三个多任务。", "result": "实验表明，提出的融合架构CNN+CNN-LSTM和CNN+DeiT Transformer在多个深度学习模型中表现最佳，在蔬菜分类和变质检测上的F1分数分别为0.98和0.61，并在变质预测上达到了MSE为3.58和SMAPE为41.66%的性能指标。", "conclusion": "融合模型在嘈杂图像上的可靠性得到了验证，且与LIME相结合可视化了模型决策，进一步证明了其有效性和鲁棒性。"}}
{"id": "2601.13664", "pdf": "https://arxiv.org/pdf/2601.13664", "abs": "https://arxiv.org/abs/2601.13664", "authors": ["Tiancheng Fang", "Bowen Pan", "Lingxi Chen", "Jiangjing Lyu", "Chengfei Lyu", "Chaoyue Niu", "Fan Wu"], "title": "VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement", "categories": ["cs.CV"], "comment": "Under review at CVPR 2026", "summary": "We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.", "AI": {"tldr": "本文提出了VIAFormer，一种用于多视角条件下的体素细化任务的体素-图像对齐转换器模型。", "motivation": "旨在通过利用校准后的多视图图像作为指导来修复不完整的噪声体素，并提高基于体素的方法在大规模模型和大数据环境中的性能。", "method": "VIAFormer采用了协同设计，包括提供显式3D空间定位的图像索引、学习直接体素细化轨迹的矫正流目标以及实现鲁棒跨模态融合的混合流转换器。", "result": "实验表明，VIAFormer在纠正来自强大的视觉基础模型获得的体素形状上的严重合成损坏和现实缺陷方面达到了新的状态水平。", "conclusion": "VIAFormer证明了其作为实际可靠的桥梁，在真实世界的3D创作管道中的应用价值，为基于体素的方法在大规模、大数据浪潮中蓬勃发展铺平道路。"}}
{"id": "2601.13659", "pdf": "https://arxiv.org/pdf/2601.13659", "abs": "https://arxiv.org/abs/2601.13659", "authors": ["Chunlei Meng", "Ziyang Zhou", "Lucas He", "Xiaojing Du", "Chun Ouyang", "Zhongxue Gan"], "title": "Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "This study has been accepted by IEEE ICASSP2026", "summary": "Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.", "AI": {"tldr": "本文提出TSDA方法，通过将多模态信息在交互前解耦为时间和空间特征，并分别进行对齐和再结合，以改进多模态情感分析的性能。", "motivation": "主流方法依赖于时空混合建模，忽视了时空异质性，导致时空信息不对称且性能受限，因此提出TSDA来解决这一问题。", "method": "TSDA通过时间编码器和空间编码器将每种模态的信号投影为单独的时间体和空间体，然后使用因素一致的跨模态对齐进行特征对齐，并采用门控再结合模块重新组合这些流以执行任务。", "result": "实验结果表明，TSDA在多模态情感分析中优于基线方法。", "conclusion": "通过对比实验证明了设计的有效性和可解释性，证实了解耦和再结合策略的重要性。"}}
{"id": "2601.13657", "pdf": "https://arxiv.org/pdf/2601.13657", "abs": "https://arxiv.org/abs/2601.13657", "authors": ["Myong-Yol Choi", "Hankyoul Ko", "Hanse Cho", "Changseung Kim", "Seunghwan Kim", "Jaemin Seo", "Hyondong Oh"], "title": "Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.", "AI": {"tldr": "本论文提出了一种基于深度强化学习的控制方法，用于在通信受限环境中实现无人机群的集体导航。", "motivation": "受到生物集群行为中领导个体隐含指导群体而不需显式通讯的启发，研究旨在开发一种无需任何通信或外部定位系统支持，仅依靠机载LiDAR传感器进行感知和决策的方法，以使无人机群在复杂障碍环境中稳健运行。", "method": "该方法采用了一个隐式的领导者-跟随者框架，只有领导者拥有目标信息，而跟随者无人机通过深度强化学习利用仅有的LiDAR数据来学习策略。系统还使用了LiDAR点聚类和扩展卡尔曼滤波器进行稳定的邻居追踪，并在GPU加速的Nvidia Isaac Sim中训练控制器。", "result": "通过广泛的模拟测试以及现实世界的实验，该方法证实其鲁棒性和从仿真到现实的有效转移能力。在没有通信或外部定位的情况下，五架无人机成功完成了多样的室内和室外环境下的集体导航任务。", "conclusion": "本研究证明了基于深度强化学习的控制策略可以实现无需通讯支持的无人机群的稳健操作，在复杂环境中表现出色，并且具有良好的仿真到现实的迁移性能。"}}
{"id": "2601.13655", "pdf": "https://arxiv.org/pdf/2601.13655", "abs": "https://arxiv.org/abs/2601.13655", "authors": ["Guangba Yu", "Zirui Wang", "Yujie Huang", "Renyi Zhong", "Yuedong Zhong", "Yilun Wang", "Michael R. Lyu"], "title": "Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs", "categories": ["cs.SE", "cs.AI", "cs.DC"], "comment": null, "summary": "The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems. Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.", "AI": {"tldr": "研究了开源大型语言模型在实际部署中遇到的705个失败案例，揭示了可靠性瓶颈从算法缺陷转向部署堆栈系统脆弱性的现象。", "motivation": "旨在解决用户自行管理的开源大规模语言模型部署过程中存在的盲点和可靠性问题。", "method": "通过分析来自DeepSeek、Llama和Qwen生态系统中的705个真实世界失败案例进行实证研究。", "result": "发现了诊断分歧、系统同质性和生命周期升级三种现象，表明了可靠性瓶颈从算法缺陷转向部署堆栈系统的脆弱性，并且这些问题存在于整个生态中而非特定架构。", "conclusion": "这些洞察提供了可操作性的指导建议以提高大型语言模型部署的可靠性。"}}
{"id": "2601.13651", "pdf": "https://arxiv.org/pdf/2601.13651", "abs": "https://arxiv.org/abs/2601.13651", "authors": ["Marta Moscati", "Oleksandr Kats", "Mubashir Noman", "Muhammad Zaigham Zaheer", "Yufang Hou", "Markus Schedl", "Shah Nawaz"], "title": "Face-Voice Association with Inductive Bias for Maximum Class Separation", "categories": ["cs.CV"], "comment": "Accepted at ICASSP 2026", "summary": "Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.", "AI": {"tldr": "本文提出了一种用于面部和声音关联的方法，该方法通过施加最大类别分离的归纳偏见来加强多模态表示之间的区分能力。", "motivation": "之前的面部-声音关联研究主要依赖损失函数使同一人的面部和语音嵌入更接近且与其他人的嵌入有明显的分开。这项工作旨在填补将最大类别分离作为归纳偏见应用于面部-声音关联领域的空白。", "method": "本文提出的方法在多模态学习中施加了最大类别分离的归纳偏见来增强面部和声音表示之间的区分能力，并结合了跨类正交性的损失函数。", "result": "实验结果显示，该方法在两项面部-声音关联任务上达到了最先进的性能，并通过消融研究证明了与跨类正交性损失相结合时最有效。", "conclusion": "本文首次展示了将最大类别分离作为归纳偏见应用于多模态学习中面部和语音关联的有效性，并为建立新的范式铺平道路。"}}
{"id": "2601.13649", "pdf": "https://arxiv.org/pdf/2601.13649", "abs": "https://arxiv.org/abs/2601.13649", "authors": ["Xiaolin Zhou", "Zheng Luo", "Yicheng Gao", "Qixuan Chen", "Xiyang Hu", "Yue Zhao", "Ruishan Liu"], "title": "Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.", "AI": {"tldr": "本文研究了大型语言模型作为裁判在评估文本质量时存在的两种语言偏见：同种语言评估中的性能差异以及不同语言对比中偏向主要语言。", "motivation": "鉴于先前研究表明，大型语言模型作为裁判可能会出现与人类偏好不一致的偏差，特别是语言上的偏见。因此，本研究旨在探索这些偏差的具体表现及其对评估结果的影响。", "method": "通过实验分析了大型语言模型在同种语言和不同语言对比中是否存在性能差异以及偏向主要语言的现象，并进一步探究了这种语言偏见是否可以由低困惑度偏见来解释。", "result": "研究发现，对于同种语言的评估，欧洲语系的表现通常优于非洲语系；而不同语言对比时，大多数模型倾向于偏好英语答案。虽然困惑度与语言偏见有轻微的相关性，但不能完全解释这种偏见。", "conclusion": "大型语言模型作为裁判在进行文本质量评估时存在明显的语言偏见问题，这不仅体现在同种语言之间的性能差异上，也体现在不同语言对比中对主要语言的偏好。这些发现提示我们需要进一步探讨如何减缓或消除此类偏见，以提高评估结果的公平性。"}}
{"id": "2601.13647", "pdf": "https://arxiv.org/pdf/2601.13647", "abs": "https://arxiv.org/abs/2601.13647", "authors": ["Yumin Kim", "Seonghyeon Go"], "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.", "AI": {"tldr": "本文提出了一种改进的段落转换器（Fusion Segment Transformer），用于AI生成音乐的全曲检测。", "motivation": "随着生成式AI技术的发展，AI生成音乐版权和所有权问题变得越来越重要，现有方法主要针对短音频，而长音频的检测尚待研究。", "method": "该论文采用Gated Fusion Layer增强架构，整合内容与结构信息以捕捉长期上下文，提升全曲AI生成音乐检测效果。", "result": "实验结果在SONICS和AIME数据集上表明，本方法优于之前的模型和其他最新基线，取得了AI生成音乐检测领域的最先进成果。", "conclusion": "通过引入Gated Fusion Layer的改进段落转换器实现了更好的全曲AI生成音乐检测效果。"}}
{"id": "2601.13645", "pdf": "https://arxiv.org/pdf/2601.13645", "abs": "https://arxiv.org/abs/2601.13645", "authors": ["Euijin You", "Hyang-Won Lee"], "title": "Quadratic Upper Bound for Boosting Robustness", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at ICML 2025. Published in PMLR 267:72656-72676", "summary": "Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.", "AI": {"tldr": "本文提出了一种新的损失函数，利用二次上界（QUB）来改进快速对抗训练（FAT）方法的鲁棒性。", "motivation": "快速对抗训练虽然可以减少训练时间并增强模型对对抗攻击的鲁棒性，但往往因为缺乏充分探索对抗空间而导致鲁棒性下降。", "method": "本文开发了一种新的损失函数，即二次上界（QUB），用于改进现有FAT方法中的对抗训练损失函数，并将其应用于现有的快速对抗训练中。", "result": "实验结果显示，将QUB损失应用到现有的快速对抗训练方法中可显著提高鲁棒性，且模型的损失景观变得更加平滑。", "conclusion": "通过各种指标证明了改进后的鲁棒性可能是由于其更平滑的损失景观所致。"}}
{"id": "2601.13639", "pdf": "https://arxiv.org/pdf/2601.13639", "abs": "https://arxiv.org/abs/2601.13639", "authors": ["Deyun Qin", "Zezhi Liu", "Hanqian Luo", "Xiao Liang", "Yongchun Fang"], "title": "A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint", "categories": ["cs.RO"], "comment": null, "summary": "Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.", "AI": {"tldr": "提出了一种用于机器人操作的一次性多模态主动感知框架，该框架能够直接推断出最优视角。", "motivation": "现有的主动感知方法依赖于迭代优化，导致了高时间和运动成本，并且与特定任务目标紧密结合，限制了其可移植性。", "method": "提出了一种通用的一次性多模态主动感知框架，该框架解耦了视点质量评估并包含一个数据收集管道和最优视角预测网络。通过域随机化构建大规模训练数据集，并开发了一个使用跨注意力机制融合多模态特征的最优视点预测网络。", "result": "实验结果表明，基于该框架引导的主动感知显著提高了抓取成功率，且在现实世界评估中几乎翻倍了抓取成功率为无缝仿真到现实转移提供了可能。", "conclusion": "所提出的框架证明能够通过一次性多模态方法有效地提高机器人操作中的视点选择质量，并显著提升了抓取任务的成功率。"}}
{"id": "2601.13633", "pdf": "https://arxiv.org/pdf/2601.13633", "abs": "https://arxiv.org/abs/2601.13633", "authors": ["Guanqi Zhan", "Changye Li", "Zhijian Liu", "Yao Lu", "Yi Wu", "Song Han", "Ligeng Zhu"], "title": "Scaling Test-time Inference for Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce 'Efficient visual Grounding language Models' (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach's generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.", "AI": {"tldr": "本文提出了Efficient visual Grounding language Models（EGM），一种通过增加测试时的计算量来提升小型视觉语言模型在视觉定位任务中表现的方法。", "motivation": "当前的状态良好的视觉语言模型由于大型规模而部署困难且推理速度慢。作者注意到，小模型和大模型之间的主要差异在于语言模型部分而非视觉编码器大小，并希望缩小两者间的性能差距以提高效率。", "method": "EGM通过增加测试时生成的token数量来提升小型模型在视觉定位任务中的表现，从而使得小型模型能够达到与大型模型相当甚至更好的效果。", "result": "实验表明，在RefCOCO基准上，EGM-Qwen3-VL-8B实现了91.4 IoU和平均737ms的延迟（比Qwen3-VL-235B快5.9倍）。该方法还提升了模型在可见和被遮挡部分目标预测任务上的表现。", "conclusion": "通过EGM，小型视觉语言模型可以在不增加计算负担的情况下提高其在视觉定位任务中的性能，并且显著减少延迟时间。"}}
{"id": "2601.13632", "pdf": "https://arxiv.org/pdf/2601.13632", "abs": "https://arxiv.org/abs/2601.13632", "authors": ["Zhiming Xue", "Sichen Zhao", "Yalun Qi", "Xianling Zeng", "Zihan Yu"], "title": "Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via Spatiotemporal Graph Learning", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid development of the e-commerce industry, the logistics network is experiencing unprecedented pressure. The traditional static routing strategy most time cannot tolerate the traffic congestion and fluctuating retail demand. In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial optimization. We first construct a logistics topology graph by using the discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep learning model combining Graph Convolutional Network (GCN) and Gated Recurrent Unit (GRU) is adopted to extract spatial correlations and temporal dependencies for predicting future congestion risks. These prediction results are then integrated into a dynamic edge weight mechanism to perform path planning. We evaluated the framework on the Smart Logistics Dataset 2024, which contains real-world Internet of Things(IoT) sensor data. The experimental results show that the RADR algorithm significantly enhances the resilience of the supply chain. Particularly in the case study of high congestion scenarios, our method reduces the potential congestion risk exposure by 19.3% while only increasing the transportation distance by 2.1%. This empirical evidence confirms that the proposed data-driven approach can effectively balance delivery efficiency and operational safety.", "AI": {"tldr": "本文提出了一种风险感知动态路由框架（RADR），通过时空图神经网络和组合优化方法，提高了物流网络的韧性。", "motivation": "随着电子商务行业的快速发展，传统的静态路径规划策略无法有效应对交通拥堵和零售需求波动带来的压力，因此需要一种更适应变化的方法来提高物流系统的效率和安全性。", "method": "首先构建基于离散GPS数据的空间聚类物流拓扑图；其次采用结合GCN和GRU的混合深度学习模型预测未来拥堵风险；最后将这些预测结果整合到动态边权重机制中进行路径规划。", "result": "实验结果显示，RADR算法显著提高了供应链的韧性，在高拥堵场景下降低了19.3%的潜在拥堵风险暴露，仅增加2.1%的运输距离。", "conclusion": "实验证据表明，提出的基于数据驱动的方法能够有效平衡交付效率和运营安全。"}}
{"id": "2601.13622", "pdf": "https://arxiv.org/pdf/2601.13622", "abs": "https://arxiv.org/abs/2601.13622", "authors": ["Donghee Lee", "Rui Cai", "Zhe Zhao"], "title": "CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model's ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.", "AI": {"tldr": "本文提出了CARPE，一个新型的、模型无关的框架，通过引入视觉集成层和基于上下文的组合策略来提升大型视觉语言模型在图像分类等任务上的性能。", "motivation": "虽然大型视觉语言模型表现强劲，但在以视觉为中心的任务上仍不如其基础视觉编码器。因此，本文旨在提高这些模型处理视觉信息的能力。", "method": "CARPE通过引入视觉集成层和基于上下文的组合策略来确定何时优先考虑图像表示或依赖语言模型的推理能力，从而增强模型自适应加权视觉和文本模态的能力。", "result": "实验显示，CARPE不仅提高了图像分类基准测试的表现，还提升了各类视觉-语言任务的结果。该方法可以与大多数开源大型视觉语言模型有效集成。", "conclusion": "CARPE框架通过改进图像表示的处理方式，增强了模型在视觉和文本多模态信息上的表现能力，并证明了其在多种架构中的适应性。"}}
{"id": "2601.13614", "pdf": "https://arxiv.org/pdf/2601.13614", "abs": "https://arxiv.org/abs/2601.13614", "authors": ["Bo Peng", "Sirui Chen", "Lei Xu", "Chaochao Lu"], "title": "CauScientist: Teaching LLMs to Respect Data for Causal Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Causal discovery is fundamental to scientific understanding and reliable decision-making. Existing approaches face critical limitations: purely data-driven methods suffer from statistical indistinguishability and modeling assumptions, while recent LLM-based methods either ignore statistical evidence or incorporate unverified priors that can mislead result. To this end, we propose CauScientist, a collaborative framework that synergizes LLMs as hypothesis-generating \"data scientists\" with probabilistic statistics as rigorous \"verifiers\". CauScientist employs hybrid initialization to select superior starting graphs, iteratively refines structures through LLM-proposed modifications validated by statistical criteria, and maintains error memory to guide efficient search space. Experiments demonstrate that CauScientist substantially outperforms purely data-driven baselines, achieving up to 53.8% F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while standalone LLM performance degrades with graph complexity, CauScientist reduces structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.", "AI": {"tldr": "本文提出了一种名为CauScientist的协作框架，结合了LLM作为假设生成器和概率统计作为验证工具，以提高因果发现的效果。", "motivation": "现有的因果发现方法存在局限性：纯数据驱动的方法受到统计不可区分性和模型假设的影响，而基于LLM的方法要么忽视统计证据，要么使用未经验证的先验知识导致误导。因此，提出CauScientist来解决这些问题。", "method": "CauScientist采用混合初始化选择更好的初始图结构，通过迭代改进由LLMs提出的修改，并用统计标准进行验证，同时维护错误记忆以指导有效搜索空间。", "result": "实验表明，与纯数据驱动的方法相比，CauScientist显著提高了F1分数（最高提升53.8%），并将召回率从35.0%提高到100.0%，在复杂图结构中，CauScientist将结构性汉明距离(SHD)减少了44.0%。", "conclusion": "研究表明，在因果发现任务上，结合LLM和概率统计的协作框架（如CauScientist）能够显著提升性能，并减少对复杂图结构的误判。"}}
{"id": "2601.13606", "pdf": "https://arxiv.org/pdf/2601.13606", "abs": "https://arxiv.org/abs/2601.13606", "authors": ["Zheng Liu", "Honglin Lin", "Chonghan Qin", "Xiaoyang Wang", "Xin Gao", "Yu Li", "Mengzhang Cai", "Yun Zhu", "Zhanping Zhong", "Qizhi Pei", "Zhuoshi Pan", "Xiaoran Shang", "Bin Cui", "Conghui He", "Wentao Zhang", "Lijun Wu"], "title": "ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch", "categories": ["cs.CV"], "comment": "29 pages", "summary": "Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.", "AI": {"tldr": "本文提出ChartVerse，一种用于生成复杂图表和可靠推理数据的可扩展框架。", "motivation": "现有视觉语言模型在处理图表推理任务时受到高质量训练数据缺乏的限制。现有的数据集存在合成图表过于简单且问题与答案配对缺乏深度的问题。", "method": "提出了Rollout Posterior Entropy（RPE）来量化图表复杂度，并采用复杂性感知图表示生成程序化合成多样化的高复杂度图表；通过基于锚定的答案首先提取，条件生成问题并严格验证一致性。", "result": "实验结果表明，ChartVerse-8B在性能上达到了新的高度，不仅超越了其教师模型Qwen3-VL-30B-A3B-Thinking，并且与更强大的Qwen3-VL-32B-Thinking相匹敌。", "conclusion": "通过合成复杂图表和确保推理深度的数据生成方法，ChartVerse框架显著提升了视觉语言模型在图表推理任务中的表现。"}}
{"id": "2601.13600", "pdf": "https://arxiv.org/pdf/2601.13600", "abs": "https://arxiv.org/abs/2601.13600", "authors": ["Paul He", "Elke Kirschbaum", "Shiva Kasiviswanathan"], "title": "Foundations of Global Consistency Checking with Noisy LLM Oracles", "categories": ["cs.AI"], "comment": "Under Review", "summary": "Ensuring that collections of natural-language facts are globally consistent is essential for tasks such as fact-checking, summarization, and knowledge base construction. While Large Language Models (LLMs) can assess the consistency of small subsets of facts, their judgments are noisy, and pairwise checks are insufficient to guarantee global coherence. We formalize this problem and show that verifying global consistency requires exponentially many oracle queries in the worst case. To make the task practical, we propose an adaptive divide-and-conquer algorithm that identifies minimal inconsistent subsets (MUSes) of facts and optionally computes minimal repairs through hitting-sets. Our approach has low-degree polynomial query complexity. Experiments with both synthetic and real LLM oracles show that our method efficiently detects and localizes inconsistencies, offering a scalable framework for linguistic consistency verification with LLM-based evaluators.", "AI": {"tldr": "本文提出了一种自适应的分治算法，用于识别事实集合中的最小不一致子集（MUSes）并计算最小修复方案。", "motivation": "确保自然语言事实集合在全球范围内的一致性对于任务如事实核查、总结和知识库构建至关重要。然而，大语言模型（LLMs）在评估一致性时存在噪声，并且成对检查不足以保证全局连贯性。", "method": "本文提出了一种自适应的分治算法来识别最小不一致子集并计算最小修复方案，该方法具有低阶多项式查询复杂度。", "result": "实验表明，与合成和真实的LLM预言机一起使用时，本方法能够有效地检测和定位一致性问题。", "conclusion": "本文提供了一个可扩展的框架来验证基于大语言模型评估器的语言一致性，并通过实验验证了其有效性和高效性。"}}
{"id": "2601.13599", "pdf": "https://arxiv.org/pdf/2601.13599", "abs": "https://arxiv.org/abs/2601.13599", "authors": ["Linrui Ma", "Yufei Cui", "Kai Han", "Yunhe Wang"], "title": "Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models", "categories": ["cs.LG", "cs.AI"], "comment": "Work In Progress", "summary": "Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.", "AI": {"tldr": "本文提出Diffusion in Diffusion框架，旨在解决块扩散模型中的不可逆性和短视问题。", "motivation": "现有的块扩散语言模型虽然结合了自回归和扩散模型的优势，但其严格的方向性依赖导致了不可逆性和牺牲全局规划能力的问题。", "method": "方法包括先用小块进行快速草稿生成，再通过具有更大双向感知域的全局双向扩散来细化草稿，使用快照信心重掩码识别需要修改的关键标记，并采用多尺度训练扩展块扩散模型的全局能力。", "result": "实验结果显示该方法在OpenWebText数据集上为离散扩散模型树立了新基准，在仅使用基线模型26%的微调预算的情况下，将生成困惑度从25.7降低到21.9。", "conclusion": "研究表明Diffusion in Diffusion框架能够有效改善块扩散模型中的不可逆性和短视问题，并缩小与自回归模型在性能上的差距。"}}
