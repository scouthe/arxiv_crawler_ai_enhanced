{"id": "2512.08931", "pdf": "https://arxiv.org/pdf/2512.08931", "abs": "https://arxiv.org/abs/2512.08931", "authors": ["Yixuan Zhu", "Jiaqi Feng", "Wenzhao Zheng", "Yuan Gao", "Xin Tao", "Pengfei Wan", "Jie Zhou", "Jiwen Lu"], "title": "Astra: General Interactive World Model with Autoregressive Denoising", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at: https://github.com/EternalEvan/Astra", "summary": "Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.", "AI": {"tldr": "Astra是一款通用互动世界模型，能够从过去的观察和行动中预测长期未来，并支持各种形式的交互。", "motivation": "当前视频生成模型在文本或图像到高质量视频片段方面取得进展，但具备从过去观测和动作中预测长期未来的通用目的世界模型仍待探索。Astra旨在填补这一空白。", "method": "提出了一种自回归去噪架构，并利用因果时间注意力聚合过往观察，支持流式输出；采用噪声增强的历史记忆来平衡响应速度与时间一致性；引入行动感知适配器直接将行动信号注入到去噪过程中；开发了混合动作专家系统，动态路由异构动作模式。", "result": "实验表明Astra在图像真实性、长期预测和动作对齐方面优于现有最佳世界模型。", "conclusion": "Astra实现了互动性、一致性和通用的长期视频预测，并支持各种形式的交互。"}}
{"id": "2512.08930", "pdf": "https://arxiv.org/pdf/2512.08930", "abs": "https://arxiv.org/abs/2512.08930", "authors": ["Youming Deng", "Songyou Peng", "Junyi Zhang", "Kathryn Heal", "Tiancheng Sun", "John Flynn", "Steve Marschner", "Lucy Chai"], "title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://denghilbert.github.io/selfi/", "summary": "Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.", "AI": {"tldr": "本文提出了一种通过特征对齐改进三维重建的自提升框架Selfi，以提高新型视图合成和相机姿态估计任务中的性能。", "motivation": "传统的NVS依赖于具有显式三维偏差模型及事先确定的摄像机参数，而最近的视觉基础模型如VGGT可以通过训练数据和损失目标获得隐式的3D知识。然而，这些特征缺乏多视图几何一致性，因此作者提出通过特征对齐提高性能。", "method": "本文引入Selfi，一种利用自身输出作为伪真实标签进行轻量级特征适配器训练的三维重建流水线，并使用基于重投影的一致性损失将VGGT输出转换到新的几何对准特征空间中。", "result": "实验结果表明，通过这种特征对齐的方法可以在新型视图合成和相机姿态估计任务上达到最先进的性能。", "conclusion": "研究证明了特征对齐是下游三维推理中的一个关键步骤，可以显著提高模型的性能。"}}
{"id": "2512.08924", "pdf": "https://arxiv.org/pdf/2512.08924", "abs": "https://arxiv.org/abs/2512.08924", "authors": ["Chuhan Zhang", "Guillaume Le Moing", "Skanda Koppula", "Ignacio Rocco", "Liliane Momeni", "Junyu Xie", "Shuyang Sun", "Rahul Sukthankar", "Joëlle K Barral", "Raia Hadsell", "Zoubin Ghahramani", "Andrew Zisserman", "Junlin Zhang", "Mehdi SM Sajjadi"], "title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time", "categories": ["cs.CV"], "comment": "Project Page: https://d4rt-paper.github.io/", "summary": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.", "AI": {"tldr": "本文介绍了一种名为D4RT的新模型，用于高效地从视频中重建动态场景的复杂几何和运动。", "motivation": "理解并从视频中重构动态场景的复杂几何和运动是一个挑战性的任务。现有的方法通常过于计算密集或难以管理。", "method": "D4RT利用统一的变压器架构联合推断深度、时空对应关系以及完整的摄像机参数，通过一个新颖的查询机制避免了每帧解码的重计算，并提供了灵活的3D位置探查接口。", "result": "该方法在各种4D重建任务中达到了新的最先进水平，超越了以前的方法。", "conclusion": "本文提出了一种简单而强大的前馈模型，可以有效地解决从视频中重构动态场景的任务。"}}
{"id": "2512.08923", "pdf": "https://arxiv.org/pdf/2512.08923", "abs": "https://arxiv.org/abs/2512.08923", "authors": ["Angela van Sprang", "Laurens Samson", "Ana Lucic", "Erman Acar", "Sennay Ghebreab", "Yuki M. Asano"], "title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs", "categories": ["cs.AI"], "comment": "Angela van Sprang and Laurens Samson contributed equally as first authors. Preprint", "summary": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.", "AI": {"tldr": "本文介绍了两个新的基准测试REST和REST+，用于系统性地评估多模态大型语言模型（MLLM）中的跨模态不一致性。", "motivation": "研究发现MLLM在不同模态下表现出不同的推理能力，即使具有相同的语义信息。这表明需要一个新的方法来衡量这些模型的跨模态一致性和问题所在。", "method": "通过设计包含相同语义但在图像、文本和混合形式下的样本，测试了15个最先进的多模态大型语言模型（MLLM）的表现，并分析不同因素对模型性能的影响。", "result": "发现即使考虑到OCR的问题，视觉特征如颜色和分辨率会影响模型的准确性。一致性得分与文字和图片之间的模式差距相关联。", "conclusion": "研究揭示了跨模态不一致性的普遍存在以及其影响因素，并强调了解决这一问题的重要性。"}}
{"id": "2512.08922", "pdf": "https://arxiv.org/pdf/2512.08922", "abs": "https://arxiv.org/abs/2512.08922", "authors": ["Jin Hyeon Kim", "Paul Hyunbin Cho", "Claire Kim", "Jaewon Min", "Jaeeun Lee", "Jihye Park", "Yeji Choi", "Seungryong Kim"], "title": "Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Text-Aware Image Restoration (TAIR) aims to recover high- quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong gen- erative priors for general image restoration, they often pro- duce text hallucinations in text-centric tasks due to the ab- sence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that in- tegrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an it- erative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermedi- ate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine- grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.", "AI": {"tldr": "提出了一种用于高保真文本感知图像恢复的统一扩散变换器框架UniT。", "motivation": "扩散模型在通用图像恢复任务中表现强大，但在处理包含退化文本内容的任务时会产生文本幻觉。为了解决这一问题，本文提出了一个结合了视觉语言模型和OCR预测模块的新方法。", "method": "引入了一个统一的文本修复框架UniT，该框架将Diffusion Transformer、视觉-语言模型以及OCR检测模块整合在一起，在迭代过程中执行高保真度文本恢复任务。其中，视觉语言模型从退化图像中提取文本内容以提供显式的文本指导；同时，OCR预测模块在每个去噪步骤上生成中间的OCR预测结果，使视觉语言模型能够迭代地改进其指导。", "result": "实验显示，UniT能够在SA-Text和Real-Text基准测试中准确重建退化文本并显著减少幻觉，最终在高保真度文本感知图像恢复任务中的端到端F1得分上达到业界最佳水平。", "conclusion": "通过结合视觉语言模型、OCR检测模块以及扩散变换器的强表示能力，UniT能够更有效地抑制文本幻觉，并实现高质量的文本修复效果。"}}
{"id": "2512.08920", "pdf": "https://arxiv.org/pdf/2512.08920", "abs": "https://arxiv.org/abs/2512.08920", "authors": ["Jessica Yin", "Haozhi Qi", "Youngsun Wi", "Sayantan Kundu", "Mike Lambeta", "William Yang", "Changhao Wang", "Tingfan Wu", "Jitendra Malik", "Tess Hellebrekers"], "title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "categories": ["cs.RO", "cs.LG"], "comment": "Project website: https://jessicayin.github.io/osmo_tactile_glove/", "summary": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.", "AI": {"tldr": "介绍了一种开源的触觉手套OSMO，用于从人类演示中获取数据并实现人机技能转移。", "motivation": "视频无法捕捉到关键的接触信号，而这些对于掌握操纵任务至关重要。因此，开发一种能够记录触觉信息的手套来缩小视觉和触感差距。", "method": "设计了一种包含12个三轴触觉传感器的手套，该手套可与最先进的手部追踪方法结合使用以收集数据。通过仅基于人类演示的数据训练机器人策略，并且无需真实的机器人数据。", "result": "在要求持续接触压力的真实擦拭任务中，采用触觉感知的策略达到了72％的成功率，优于视觉基础线，消除了接触相关的故障模式。", "conclusion": "OSMO手套实现了从人到机器人的技能转移，提供了完整的硬件设计、固件和组装说明以支持社区采纳。"}}
{"id": "2512.08914", "pdf": "https://arxiv.org/pdf/2512.08914", "abs": "https://arxiv.org/abs/2512.08914", "authors": ["David Zenati", "Eliya Nachmani"], "title": "SAQ: Stabilizer-Aware Quantum Error Correction Decoder", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum Error Correction (QEC) decoding faces a fundamental accuracy-efficiency tradeoff. Classical methods like Minimum Weight Perfect Matching (MWPM) exhibit variable performance across noise models and suffer from polynomial complexity, while tensor network decoders achieve high accuracy but at prohibitively high computational cost. Recent neural decoders reduce complexity but lack the accuracy needed to compete with computationally expensive classical methods. We introduce SAQ-Decoder, a unified framework combining transformer-based learning with constraint aware post-processing that achieves both near Maximum Likelihood (ML) accuracy and linear computational scalability with respect to the syndrome size. Our approach combines a dual-stream transformer architecture that processes syndromes and logical information with asymmetric attention patterns, and a novel differentiable logical loss that directly optimizes Logical Error Rates (LER) through smooth approximations over finite fields. SAQ-Decoder achieves near-optimal performance, with error thresholds of 10.99% (independent noise) and 18.6% (depolarizing noise) on toric codes that approach the ML bounds of 11.0% and 18.9% while outperforming existing neural and classical baselines in accuracy, complexity, and parameter efficiency. Our findings establish that learned decoders can simultaneously achieve competitive decoding accuracy and computational efficiency, addressing key requirements for practical fault-tolerant quantum computing systems.", "AI": {"tldr": "介绍了一种结合变压器学习和约束感知后处理的量子纠错解码器SAQ-Decoder，该方法在准确性和计算效率上均表现出色。", "motivation": "解决传统量子误差校正解码器在噪声模型性能不稳定、复杂度过高或准确性不足的问题。", "method": "采用双流变压器架构结合不对称注意力模式和逻辑损失优化，实现线性复杂度的高效解码同时接近最大似然估计（ML）准确率。", "result": "SAQ-Decoder在独立噪声下达到10.99%的错误阈值，在去极化噪声下为18.6%，接近理论最优值，并且超越了现有的神经网络和经典方法的基线性能。", "conclusion": "证明了学习解码器能够同时实现竞争性的解码准确性和计算效率，满足实际容错量子计算机系统的关键需求。"}}
{"id": "2512.08912", "pdf": "https://arxiv.org/pdf/2512.08912", "abs": "https://arxiv.org/abs/2512.08912", "authors": ["Simon de Moreau", "Andrei Bursuc", "Hafid El-Idrissi", "Fabien Moutarde"], "title": "LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception", "categories": ["cs.CV", "cs.RO"], "comment": "Preprint. 12 pages, 9 figures. Project page: https://simondemoreau.github.io/LiDAS/", "summary": "Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.", "AI": {"tldr": "提出了一种名为LiDAS的照明驱动动态主动感知系统，通过自适应控制车前灯来提高夜间视觉感知性能。", "motivation": "现有基于相机的夜间感知方法依赖于场景照明而被动工作，存在重大挑战。为了克服这些限制并提升夜间驾驶的安全性，提出了LiDAS系统以优化照明效果。", "method": "通过结合现成的视觉感知模型和高清晰度车前灯，LiDAS动态预测最优照明区域来最大化下游感知性能，并减少能量消耗。", "result": "在真实环境下的零样本闭环驾驶场景中进行部署后，LiDAS相比于标准低光束提高了18.7% mAP50和5.0% mIoU，在等功率条件下还能节省40%的能耗。", "conclusion": "通过将现有的车前灯转化为主动视觉执行器，LiDAS提供了一种低成本解决方案，增强了夜间感知的鲁棒性。"}}
{"id": "2512.08905", "pdf": "https://arxiv.org/pdf/2512.08905", "abs": "https://arxiv.org/abs/2512.08905", "authors": ["Kaizhi Zheng", "Yue Fan", "Jing Gu", "Zishuo Xu", "Xuehai He", "Xin Eric Wang"], "title": "Self-Evolving 3D Scene Generation from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.", "AI": {"tldr": "本文提出了一种从单张图像自进化生成高质量、纹理化的3D场景的方法。", "motivation": "现有的图像到3D生成方法受限于物体为中心的训练，难以推广至复杂大规模场景中保持结构和纹理的一致性。", "method": "EvoScene框架结合了现有模型在几何推理和视觉知识上的优势，通过三个迭代阶段逐步改善结构与外观。", "result": "实验结果表明，EvoScene相比基线方法具有更优的几何稳定性、视角一致性的纹理以及对未见区域的良好补充。", "conclusion": "所提出的框架可以生成适合实际应用的高质量3D网格模型。"}}
{"id": "2512.08897", "pdf": "https://arxiv.org/pdf/2512.08897", "abs": "https://arxiv.org/abs/2512.08897", "authors": ["Zeyang Liu", "Le Wang", "Sanping Zhou", "Yuxuan Wu", "Xiaolong Sun", "Gang Hua", "Haoxiang Li"], "title": "UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.", "AI": {"tldr": "提出了一种统一的扩散变换器UniLayDiff，用于处理各种内容感知布局生成任务。", "motivation": "现有方法无法同时解决多种条件下的布局生成问题，需要开发一个能够统一不同子任务的模型。", "method": "将布局约束视为一种独立模态，并采用多模态扩散变换器框架来捕捉背景图像、布局元素和各种约束之间的复杂交互。通过LoRA微调预训练模型以整合关系约束。", "result": "UniLayDiff在无条件生成到不同条件生成任务上均达到当前最佳性能，是首个统一内容感知布局生成所有范围的任务的模型。", "conclusion": "提出了一个单一、端到端可训练的模型来解决各种内容感知布局生成问题。"}}
{"id": "2512.08894", "pdf": "https://arxiv.org/pdf/2512.08894", "abs": "https://arxiv.org/abs/2512.08894", "authors": ["Jakub Krajewski", "Amitis Shidani", "Dan Busbridge", "Sam Wiseman", "Jason Ramapuram"], "title": "Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.", "AI": {"tldr": "研究在大规模语言模型训练中，下游任务性能的缩放特性。", "motivation": "挑战传统观点，即直接预测下游任务表现不可靠，并提出一种新的框架来更准确地描述从训练预算到基准性能的缩放行为。", "method": "通过固定token-to-parameter比率，利用简单的幂律模型来描述多个流行下游任务的日志准确性缩放行为。引入函数形式以跨不同的token-to-parameter比率预测准确性并考虑重复采样的推理计算。", "result": "发现直接方法比先前提出的两阶段过程更好地外推，并且在高达170亿参数的模型上进行训练，验证了研究结果的有效性。", "conclusion": "提出的方法能够更准确地预测大规模语言模型训练中的下游任务表现，为未来的研究提供了支持。"}}
{"id": "2512.08892", "pdf": "https://arxiv.org/pdf/2512.08892", "abs": "https://arxiv.org/abs/2512.08892", "authors": ["Guangzhi Xiong", "Zhenghao He", "Bohan Liu", "Sanchit Sinha", "Aidong Zhang"], "title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.", "AI": {"tldr": "本文提出了一种利用稀疏自编码器（SAE）来检测检索增强生成（RAG）中不忠实输出的方法。", "motivation": "现有的RAG方法仍面临幻觉问题，即生成的内容可能违背或超出提供的证据。为解决此问题并降低依赖大规模训练数据和外部LLM的成本，作者利用稀疏自编码器进行内部表示的解耦以准确识别RAG中的幻觉特征。", "method": "通过基于信息的选择性特征选择以及加法特性建模的系统流程，引入了使用LLM内部表示检测不忠实输出的方法RAGLens。这种方法不仅提高了检测性能，还提供了可解释的理由。", "result": "实验结果表明，RAGLens在幻觉识别方面优于现有方法，并且能够提供关于其决策的有效事后缓解不忠实RAG的依据。", "conclusion": "RAGLens通过揭示LLM中与幻觉相关的信号分布的新见解来改进检索增强生成模型中的幻觉检测问题。"}}
{"id": "2512.08889", "pdf": "https://arxiv.org/pdf/2512.08889", "abs": "https://arxiv.org/abs/2512.08889", "authors": ["Damiano Marsili", "Georgia Gkioxari"], "title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers", "categories": ["cs.CV", "cs.AI"], "comment": "Project webpage: https://glab-caltech.github.io/valor/", "summary": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/", "AI": {"tldr": "本文提出了一种无需标注的训练框架，用于提升视觉推理和物体定位的能力。", "motivation": "现有方法要么需要大量图像、查询和答案的监督数据，要么依赖预训练模型而缺乏逻辑性和准确性。为解决这些问题，作者提出了一个利用AI验证器来改进语言推理和视觉定位的新框架。", "method": "该框架包括两个组件：LLM验证器通过强化学习优化LLM推理；VLM验证器则通过自动挖掘难负样本提升视觉定位能力。这种方法结合了现代AI系统的优点，并在多种空间推理任务中进行了评估。", "result": "实验结果显示，所提出的方法显著提升了视觉推理性能并超越了开源和专有模型，在改进视觉定位后进一步超过了近期的文本仅方法。", "conclusion": "该研究展示了无需标注的情况下也能有效提升视觉推理能力的新框架。"}}
{"id": "2512.08888", "pdf": "https://arxiv.org/pdf/2512.08888", "abs": "https://arxiv.org/abs/2512.08888", "authors": ["Manduhu Manduhu", "Alexander Dow", "Gerard Dooly", "James Riordan"], "title": "Accelerated Rotation-Invariant Convolution for UAV Image Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles. Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\\(\\times\\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\\(\\times\\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.", "AI": {"tldr": "本文提出了一种针对无人机图像分割的加速旋转不变卷积框架，该框架消除了传统的数据降级步骤，并通过结构化数据共享在多角度卷积中大大减少了内存流量和计算冗余。", "motivation": "传统分割架构如U-Net依赖于非旋转不变的卷积算子，在无人机航空影像中的目标可能具有任意方向且细节丰富，导致不同视角下的分割准确性下降。为了实现旋转不变性并减少计算成本和内存交通，提出了一种GPU优化的框架。", "method": "该方法通过消除传统的数据降级步骤，并利用对称滤波器之间的结构化数据共享来实现多角度卷积，在保持精度的同时大大减少了内存流量和计算冗余。进一步扩展了任意角度（非对称）旋转的角度加速卷积。", "result": "实验结果表明，提出的卷积方法比CUDNN快20-55%，能耗降低15-45%；在八方向设置下，在256×256输入上实现高达45％的速度提升和41％的能量节省，在1024×1024输入上的速度提高32％，能耗下降23%。集成到U-Net分割模型中后，框架的准确度比非旋转感知基线提高了最多6%。", "conclusion": "本文提出的加速旋转不变卷积方法为现有旋转不变CNN框架提供了一种有效且高效的替代方案，在保持精度的同时大幅提升了计算效率和能源使用率。"}}
{"id": "2512.08881", "pdf": "https://arxiv.org/pdf/2512.08881", "abs": "https://arxiv.org/abs/2512.08881", "authors": ["Aysim Toker", "Andreea-Maria Oncescu", "Roy Miles", "Ismail Elezi", "Jiankang Deng"], "title": "SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.", "AI": {"tldr": "本文提出了一种新颖的空间感知机制，用于提高视觉语言模型在卫星图像中的定位能力。", "motivation": "为了增强视觉语言模型在复杂遥感场景中对象的精确定位能力，并通过引入结构化空间推理来提升其性能。", "method": "该方法包括对预训练的视觉语言模型进行微调，使其能够完成各种指令跟随任务。同时，它利用专门设计的控制令牌与本地化模块接口，促进语言和空间信息的同时推理。", "result": "在多个遥感基准测试中实现了显著改进，特别是在视觉定位方面提高了24.8%的相对性能。", "conclusion": "通过将结构化的空间推理集成到视觉语言模型中，可以提高其对卫星数据进行可靠分析的能力。"}}
{"id": "2512.08879", "pdf": "https://arxiv.org/pdf/2512.08879", "abs": "https://arxiv.org/abs/2512.08879", "authors": ["Mohammad Abu-Shaira", "Ajita Rattani", "Weishi Shi"], "title": "DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Proc. IEEE International Conference on Big Data (BigData), 2025", "summary": "Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.", "AI": {"tldr": "本文提出了一种新的在线非线性回归模型DAO-GP，该模型具备概念漂移感知能力，能够动态调整超参数以适应数据分布的变化。", "motivation": "现实世界的许多数据集具有随时间变化的数据分布特性，这些变化称为概念漂移。传统的在线高斯过程方法缺乏对这种漂移的察觉和适应性，并且依赖于固定的超参数设置，这限制了它们的有效性和效率。", "method": "DAO-GP通过内置的概念漂移检测与调整机制，可以动态地根据数据分布的变化来修改其行为，从而更好地应对概念漂移的问题。此外，它还实现了自适应的、无超参数的方法，并且采用了基于衰减和稀疏表示的数据管理策略。", "result": "实验结果显示DAO-GP在静止条件下的表现稳定，在各种不同类型的概念漂移（突变型、渐进型等）以及不同数据特性下均表现出色。相比其他最先进的参数化模型和非参数化模型，DAO-GP展示了卓越的性能或与之相当的竞争性。", "conclusion": "通过提出DAO-GP这一概念漂移感知的在线非线性回归模型，本文提供了一种有效的解决方案来应对由数据分布变化引起的预测精度下降问题。"}}
{"id": "2512.08877", "pdf": "https://arxiv.org/pdf/2512.08877", "abs": "https://arxiv.org/abs/2512.08877", "authors": ["Ryan LeRoy", "Jack Kolb"], "title": "IPPO Learns the Game, Not the Team: A Study on Generalization in Heterogeneous Agent Teams", "categories": ["cs.RO"], "comment": "4 pages, 3 figures, appendix", "summary": "Multi-Agent Reinforcement Learning (MARL) is commonly deployed in settings where agents are trained via self-play with homogeneous teammates, often using parameter sharing and a single policy architecture. This opens the question: to what extent do self-play PPO agents learn general coordination strategies grounded in the underlying game, compared to overfitting to their training partners' behaviors? This paper investigates the question using the Heterogeneous Multi-Agent Challenge (HeMAC) environment, which features distinct Observer and Drone agents with complementary capabilities. We introduce Rotating Policy Training (RPT), an approach that rotates heterogeneous teammate policies of different learning algorithms during training, to expose the agent to a broader range of partner strategies. When playing alongside a withheld teammate policy (DDQN), we find that RPT achieves similar performance to a standard self-play baseline, IPPO, where all agents were trained sharing a single PPO policy. This result indicates that in this heterogeneous multi-agent setting, the IPPO baseline generalizes to novel teammate algorithms despite not experiencing teammate diversity during training. This shows that a simple IPPO baseline may possess the level of generalization to novel teammates that a diverse training regimen was designed to achieve.", "AI": {"tldr": "研究探讨了在异质多智能体环境中，通过自博弈训练的PPO代理能否学习到通用协调策略，并引入了一种新的旋转政策培训方法来测试这种能力。", "motivation": "质疑在同质代理团队中使用单一共享策略进行自我游戏训练是否会导致过度拟合于特定同伴行为的问题。研究旨在验证智能体是否能学习基于底层游戏的协调策略，而不是仅仅适应其训练伙伴的行为。", "method": "提出了一种新的培训方法——旋转政策培训（RPT），通过在训练过程中轮流使用不同学习算法的异质团队伙伴来暴露代理于更广泛的合作策略。研究利用了具有观察者和无人机两种不同角色的HeMAC环境进行测试。", "result": "实验证明，尽管没有经历同伴多样化训练，但IPPO基准模型仍能适应新的队友算法，在与保留的队友政策（DDQN）合作时表现出了类似的成绩。", "conclusion": "研究表明，简单的一个共享策略PPO基线可能具有适应新伙伴的能力，并且在异质多智能体环境中表现出了一定程度的一般化能力。"}}
{"id": "2512.08875", "pdf": "https://arxiv.org/pdf/2512.08875", "abs": "https://arxiv.org/abs/2512.08875", "authors": ["Joshua Ward", "Bochao Gu", "Chi-Hua Wang", "Guang Cheng"], "title": "When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.", "AI": {"tldr": "本文展示了大型语言模型在生成表格数据时存在泄露隐私的风险，并提出了一种新的攻击方法和防御策略。", "motivation": "研究揭示了基于大型语言模型的表格数据生成过程中存在的隐私泄露风险，旨在提高对这类问题的理解并寻找有效的解决方案。", "method": "通过引入一种名为LevAtt的新式无框成员推断攻击（MIA），该论文评估了不同模型和数据集下的隐私泄漏情况，并提出了两种防御措施，包括一种新的采样策略以在生成过程中扰乱数字。", "result": "实验显示所提出的攻击方法能成功揭示出大量模型和数据集中存在的隐私风险，在某些情况下甚至可以成为完美的成员分类器。同时，提出的防御机制能够有效抵御这些攻击且仅对合成数据的效用造成轻微影响。", "conclusion": "研究证明了基于大型语言模型生成表格数据存在独特的隐私漏洞，并强调需要采取有效的防护措施来保护敏感信息。"}}
{"id": "2512.08873", "pdf": "https://arxiv.org/pdf/2512.08873", "abs": "https://arxiv.org/abs/2512.08873", "authors": ["Jing Jie Tan", "Anissa Mokraoui", "Ban-Hoe Kwan", "Danny Wee-Kiat Ng", "Yan-Chai Hum"], "title": "Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "6 pages", "summary": "Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.", "AI": {"tldr": "本文提出了一种名为SOLI的算法，旨在优化低分辨率图像的潜在嵌入以提高图像描述的质量。", "motivation": "针对低分辨率图像在图像描述中的挑战和使用大型模型带来的计算资源限制问题，提出了一个轻量级且高效的解决方案。", "method": "采用Siamese网络架构来优化低分辨率图像的潜在嵌入，并通过双路径神经网络结构提高效率与准确性。", "result": "实验结果表明SOLI方法能够在减少计算开销的同时保持性能水平。", "conclusion": "SOLI是一种针对资源受限场景下轻量级、高效处理低分辨率图像描述的有效技术。"}}
{"id": "2512.08870", "pdf": "https://arxiv.org/pdf/2512.08870", "abs": "https://arxiv.org/abs/2512.08870", "authors": ["Xiang Chen", "Yuling Shi", "Qizhen Lan", "Yuchao Qiu", "Xiaodong Gu"], "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.", "AI": {"tldr": "提出了Fed-SE框架，用于在隐私限制下多环境LLM代理的联邦自进化。", "motivation": "在复杂交互任务中部署的LLM代理受限于隐私约束，无法进行中央优化和跨动态环境的共同进化。传统的FL方法难以直接应用于此类场景。", "method": "提出Fed-SE框架，通过局部演化与全局聚合范式实现代理参数有效微调并过滤高回报轨迹，在低秩子空间中聚合更新以减少负面传递。", "result": "在五种异构环境中实验结果表明Fed-SE相较于联邦基准提高了约18%的任务成功率。", "conclusion": "验证了Fed-SE框架在隐私限制下实现稳健的跨环境知识转移的有效性。"}}
{"id": "2512.08869", "pdf": "https://arxiv.org/pdf/2512.08869", "abs": "https://arxiv.org/abs/2512.08869", "authors": ["Anantaa Kotal", "Anupam Joshi"], "title": "Differentially Private Synthetic Data Generation Using Context-Aware GANs", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.", "AI": {"tldr": "使用上下文感知GAN生成符合领域规则的差异隐私合成数据。", "motivation": "传统方法在生成合成数据时难以捕捉复杂的隐含规则，可能导致不符合现实或实用的数据。为此，该研究提出ContextGAN以解决这些问题，并确保数据既满足显式模式又遵循隐含规则的同时保持隐私保护。", "method": "提出了一个结合领域特定规则的上下文感知差异隐私生成对抗网络（ContextGAN）。通过约束矩阵编码明确和隐含知识，约束感知判别器评估合成数据是否遵守这些规则。此外，差分隐私技术确保了原始数据中的敏感信息得到保护。", "result": "实验结果表明，ContextGAN能够在医疗、安全和金融领域产生高质量的符合领域规范并保持隐私的数据。与传统方法相比，它显著提高了现实性和实用性。", "conclusion": "ContextGAN能够生成既尊重显式模式又遵循隐含规则的高质量合成数据，并在严格的隐私保证下提高其应用价值。"}}
{"id": "2512.08868", "pdf": "https://arxiv.org/pdf/2512.08868", "abs": "https://arxiv.org/abs/2512.08868", "authors": ["Rui Min", "Zile Qiao", "Ze Xu", "Jiawen Zhai", "Wenyu Gao", "Xuanzhong Chen", "Haozhen Sun", "Zhen Zhang", "Xinyu Wang", "Hong Zhou", "Wenbiao Yin", "Xuan Zhou", "Yong Jiang", "Haicheng Liu", "Liang Ding", "Ling Zou", "Yi R.", "Fung", "Yalong Li", "Pengjun Xie"], "title": "EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce", "categories": ["cs.AI"], "comment": null, "summary": "Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.", "AI": {"tldr": "EcomBench是一个全面的电子商务基准，用于评估代理在现实世界中的表现。", "motivation": "为了填补现有的性能评估方法集中在学术环境或人工设计场景上的不足，弥补对实际应用挑战忽视的问题，论文提出了一个基于真实用户需求和全球领先电商平台的真实电子商务环境下的评估框架。", "method": "EcomBench从真实的用户需求出发，在顶级的全球电商平台中构建，并通过人类专家进行仔细的整理和标注。它覆盖了多个电商场景中的任务类别，并定义了三个难度级别，以评估代理的核心能力，如深度信息检索、多步推理和跨源知识整合。", "result": "EcomBench提供了一个严格的动态测试平台，用于衡量现代电子商务中代理的实际能力。", "conclusion": "通过在真实的电子商务环境中进行基准测试，EcomBench为评估基础代理提供了重要的工具。"}}
{"id": "2512.08860", "pdf": "https://arxiv.org/pdf/2512.08860", "abs": "https://arxiv.org/abs/2512.08860", "authors": ["Amit Bendkhale"], "title": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference", "categories": ["cs.CV"], "comment": "6 pages, 3 figures. Code and data: https://github.com/Amiton7/Tri-Bench. Accepted to the AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)", "summary": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.", "AI": {"tldr": "本文提出了Tri-Bench，一个用于测试视觉语言模型在倾斜相机视角和物体干扰下的空间推理能力的基准。", "motivation": "验证几何推理对于可信赖和可控的代理人工智能至关重要。尽管有令人印象深刻的能力，但视觉语言模型在现实场景变化中经常失败。因此，本文旨在研究这些模型在特定条件下的可靠性。", "method": "作者设计了一组平面三角问题来测试VLMs的空间推理能力，并引入了两个关键因素：相机姿态（平面与倾斜）和物体干扰（10种日常物品）。使用单个固定提示进行评估，该提示描述了一个周围正方形边框，从而可以通过同构映射提供正确的答案。", "result": "在六项简单任务中观察到的总体准确率相对于3D地面真实值较低，平均约为69％（最佳75%，最差64%）。所有四款VLM模型在识别少数形状类别时均失败，准确率为0%。此外，在相机倾斜下整体准确率下降了约4.1%。", "conclusion": "实验表明，尽管提示中提供了明确的框架参考指导，但这些模型仍然依赖于2D图像平面线索而不是利用该提示。物体干扰对VLM准确性没有显著影响。"}}
{"id": "2512.08854", "pdf": "https://arxiv.org/pdf/2512.08854", "abs": "https://arxiv.org/abs/2512.08854", "authors": ["Jack Brady", "Bernhard Schölkopf", "Thomas Kipf", "Simon Buchholz", "Wieland Brendel"], "title": "Generation is Required for Data-Efficient Perception", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.", "AI": {"tldr": "探讨生成方法对于实现数据高效感知的必要性，特别是在组合泛化方面。", "motivation": "质疑当前非生成视觉模型是否足以达到人类水平的视觉感知，并研究生成与非生成方法在组合泛化中的表现差异。", "method": "通过理论分析和实验验证生成方法如何利用合适的归纳偏差实现高效感知，并对比不同训练策略下的性能。", "result": "发现未经适当调整的非生成模型往往难以实现组合泛化，而生成方法能显著提升这一能力。", "conclusion": "提出并证明了对于数据高效的视觉感知而言，生成过程是必要的，并展示了通过解码器和搜索、回放可以有效提高性能。"}}
{"id": "2512.08839", "pdf": "https://arxiv.org/pdf/2512.08839", "abs": "https://arxiv.org/abs/2512.08839", "authors": ["Katherine Atwell", "Saki Imai", "Danielle Bragg", "Malihe Alikhani"], "title": "\"Nothing about us without us\": Perspectives of Global Deaf and Hard-of-hearing Community Members on Sign Language Technologies", "categories": ["cs.HC"], "comment": null, "summary": "There is accelerating interest in sign language technologies (SLTs), with increasing attention from both industry and academia. However, the perspectives of Deaf and Hard-of-hearing (DHH) individuals remain marginalized in their development, particularly those outside of the West and in the global South. This paper presents findings from a global, multilingual survey capturing community views on SLTs across a wide range of countries, sign languages, and cultural contexts. While participants recognized the potential of SLTs to support access and independence, many expressed concerns about cultural erasure, inaccurate translation, and hearing-dominated research pipelines. Perceptions of SLTs were shaped by factors including sign language proficiency, policy exposure, and deaf identity. Across regions, participants emphasized the importance of DHH-led design, citing the risk of harm when DHH communities are excluded from technological decision-making. This study offers a novel cross-continental, community-informed analysis of SLTs and concludes with actionable recommendations for researchers, technologists, and policymakers.", "AI": {"tldr": "本文通过全球多语言调查研究了来自不同国家、手语和文化背景的聋人和听力障碍人士对手语技术的看法。", "motivation": "随着对手语技术的兴趣不断增加，然而这些技术的发展却忽视了聋人群体的观点，特别是那些非西方或发展中国家的人群。作者希望通过这项研究来填补这一空白，并提供有关如何更好地设计与应用手语技术的见解和建议。", "method": "通过全球多语言调查收集来自不同国家、使用多种手语的人们的观点。", "result": "参与者对手语技术有正面看法，但也表达了关于文化抹杀、不准确翻译以及听觉主导的研究管线等方面的担忧。这些感知受到包括手语流利度、政策暴露和聋人身份等因素的影响。", "conclusion": "强调了由聋人群体领导设计的重要性，并为研究人员、技术人员和决策者提供了可操作的建议，以促进更加包容和有益的手语技术发展"}}
{"id": "2512.08833", "pdf": "https://arxiv.org/pdf/2512.08833", "abs": "https://arxiv.org/abs/2512.08833", "authors": ["Jean Christoph Jung", "Patrick Koopmann", "Matthias Knorr"], "title": "Interpolation in Knowledge Representation", "categories": ["cs.AI", "cs.LO"], "comment": "The article will appear in Balder ten Cate, Jean Christoph Jung, Patrick Koopmann, Christoph Wernhard and Frank Wolter, editors. Theory and Applications of Craig Interpolation. Ubiquity Press, 2026", "summary": "Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.", "AI": {"tldr": "论文主要探讨知识表示中的插值技术，特别是描述逻辑和逻辑编程形式化方法下的Craig和统一插值理论及实用计算方法。", "motivation": "在知识表示中有许多应用如解释性、遗忘、模块化与重用以及学习等都需要插值技术的支持。然而，很多相关形式系统并不具备这种性质的插值能力，在实践中计算插值也很具挑战性。", "method": "论文重点研究了描述逻辑和逻辑编程两种主要知识表示的形式系统，并讨论了这些形式下的理论结果及实用方法以实现插值计算。", "result": "提出了在描述逻辑和逻辑编程下有效计算插值的方法，同时提供了相应的理论支持。", "conclusion": "通过深入探讨描述逻辑与逻辑编程中的Craig和统一插值问题，论文为知识表示的实际应用提供了一套有效的理论框架及实践手段。"}}
{"id": "2512.08829", "pdf": "https://arxiv.org/pdf/2512.08829", "abs": "https://arxiv.org/abs/2512.08829", "authors": ["Hongyuan Tao", "Bencheng Liao", "Shaoyu Chen", "Haoran Yin", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 8 figures, conference or other essential info", "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.", "AI": {"tldr": "提出InfiniteVL模型，结合线性注意力和滑动窗口注意力以提高视觉语言模型的效率和性能。", "motivation": "为了解决现有视觉语言模型在处理长序列时的计算复杂度高和内存消耗大的问题，以及信息密集任务上的表现不佳的问题，作者提出了新的架构InfiniteVL。", "method": "设计了结合线性注意力和滑动窗口注意力的InfiniteVL模型，并通过一种三阶段训练策略提高了其性能：蒸馏预训练、指令微调和长序列SFT。", "result": "与同类Transformer基线相比，在使用较少数据的情况下，InfiniteVL不仅超过了现有线性复杂度视觉语言模型的表现，而且达到了领先的Transformer模型的水平。在推理速度方面，实现了超过3.6倍的速度提升，同时保持了恒定的时间延迟和内存占用。", "conclusion": "InfiniteVL通过结合滑动窗口注意力和线性注意力机制，在提高效率的同时保证了性能，并且在长序列处理和实时场景中表现出了优越的特性。"}}
{"id": "2512.08826", "pdf": "https://arxiv.org/pdf/2512.08826", "abs": "https://arxiv.org/abs/2512.08826", "authors": ["Shahar Sarfaty", "Adi Haviv", "Uri Hacohen", "Niva Elkin-Koren", "Roi Livni", "Amit H. Bermano"], "title": "CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale", "categories": ["cs.AI"], "comment": "Paper Page: https://shahar-sarfaty.github.io/CARLoS/", "summary": "The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.", "AI": {"tldr": "CARLoS是一种大规模框架，通过CLIP嵌入定义的简洁表示来表征LoRAs，并开发了高效的检索系统。", "motivation": "现有的发现方法依赖于不可靠的用户描述或有偏见的流行度指标，影响了实用性。为了改善这一点，本文提出了一个不需要额外元数据的大规模框架CARLoS。", "method": "通过分析650多个LoRAs在各种提示和种子下的图像生成情况，使用CLIP嵌入及其与基础模型生成差异来定义简洁的表示：方向、强度和一致性。基于这些表示开发了检索框架，并进行了自动化和人类评估。", "result": "CARLoS的检索系统优于文本基线，在自动和人工评价中表现更优。", "conclusion": "CARLoS不仅在检索任务上表现出色，还支持将强度和一致性的分析与版权相关的实质性及意愿度联系起来。"}}
{"id": "2512.08820", "pdf": "https://arxiv.org/pdf/2512.08820", "abs": "https://arxiv.org/abs/2512.08820", "authors": ["Yi Zhang", "Chun-Wun Cheng", "Junyi He", "Ke Yu", "Yushun Tang", "Carola-Bibiane Schönlieb", "Zhihai He", "Angelica I. Aviles-Rivero"], "title": "Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in IEEE Transactions on Multimedia (TMM)", "summary": "Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \\textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.", "AI": {"tldr": "提出了一种新的无训练双曲适应器方法，用于改进大规模视觉语言模型的跨模态推理。", "motivation": "现有方法在处理领域变化时性能下降或需要大量计算资源进行微调，因此开发了新的适应器以解决这些问题。", "method": "采用双曲空间而非传统欧几里得空间来表征视觉语言关系，并使用Poincaré球模型嵌入分层数据结构，结合负样本学习增强分类准确性。", "result": "在多个数据集上的实验结果显示，该方法在少量图像识别和领域泛化任务上显著优于现有最佳方法。", "conclusion": "所提出的无训练双曲适应器技术能够有效提高视觉语言模型的跨模态推理能力，并且具有更好的适应性和鲁棒性。"}}
{"id": "2512.08819", "pdf": "https://arxiv.org/pdf/2512.08819", "abs": "https://arxiv.org/abs/2512.08819", "authors": ["Ferdinand Kapl", "Emmanouil Angelis", "Tobias Höppe", "Kaitlin Maile", "Johannes von Oswald", "Nino Scherrer", "Stefan Bauer"], "title": "Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.", "AI": {"tldr": "该论文通过深入分析深度增长模型，探讨其如何克服标准非生长模型中的深度利用限制。", "motivation": "现有研究显示，在训练过程中逐渐增加Transformer的深度可以降低成本并提高推理性能。然而，缺乏对这种改进机制的理解。本文旨在填补这一空白，并解释中间堆叠成长的有效性及其对于形成可交换计算块的作用。", "method": "通过层次分析，论文展示了逐步中间堆叠如何更有效地利用模型深度、改变残差流结构和促进可交换计算模块的形成。此外，提出了一种轻量级改进版本MIDAS以进一步提升下游推理基准测试性能。", "result": "研究表明，渐进式的中间堆叠可以更好地利用模型的深度，有助于形成不同的计算电路，并在下游任务中表现出更好的性能。", "conclusion": "该研究揭示了通过逐渐增长模型深度来克服非生长模型中的深度利用率限制的方法和机制。"}}
{"id": "2512.08813", "pdf": "https://arxiv.org/pdf/2512.08813", "abs": "https://arxiv.org/abs/2512.08813", "authors": ["Connor York", "Zachary R Madin", "Paul O'Dowd", "Edmund R Hunt"], "title": "Heterogeneity in Multi-Robot Environmental Monitoring for Resolving Time-Conflicting Tasks", "categories": ["cs.RO"], "comment": "Accepted to SAC '26. To appear, DOI: https://doi.org/10.1145/3748522.3779970", "summary": "Multi-robot systems performing continuous tasks face a performance trade-off when interrupted by urgent, time-critical sub-tasks. We investigate this trade-off in a scenario where a team must balance area patrolling with locating an anomalous radio signal. To address this trade-off, we evaluate both behavioral heterogeneity through agent role specialization (\"patrollers\" and \"searchers\") and sensing heterogeneity (i.e., only the searchers can sense the radio signal). Through simulation, we identify the Pareto-optimal trade-offs under varying team compositions, with behaviorally heterogeneous teams demonstrating the most balanced trade-offs in the majority of cases. When sensing capability is restricted, heterogeneous teams with half of the sensing-capable agents perform comparably to homogeneous teams, providing cost-saving rationale for restricting sensor payload deployment. Our findings demonstrate that pre-deployment role and sensing specialization are powerful design considerations for multi-robot systems facing time-conflicting tasks, where varying the degree of behavioral heterogeneity can tune system performance toward either task.", "AI": {"tldr": "该论文研究了多机器人系统在执行持续任务时遇到紧急、时间关键性子任务时的性能折衷问题。", "motivation": "解决多机器人团队在平衡区域巡逻与寻找异常无线信号之间的性能折衷问题，探讨行为异质性和感知异质性的作用和效果。", "method": "通过模拟实验评估了不同行为角色（\"巡查者\" 和 \"搜索者\"）以及感知能力（只有搜索者可以检测到无线信号），在各种团队组成下的最优折衷。", "result": "发现行为异质性较强的团队多数情况下表现出最平衡的性能折衷。当感知能力受到限制时，部分具有传感器能力的异质团队的表现与完全同质的团队相当，这为减少传感器部署提供了成本效益的理由。", "conclusion": "研究表明，在执行时间冲突的任务中，预先部署角色和感知的专门化是多机器人系统设计的重要考虑因素，行为异质性可以调节系统的性能以适应不同的任务需求。"}}
{"id": "2512.08812", "pdf": "https://arxiv.org/pdf/2512.08812", "abs": "https://arxiv.org/abs/2512.08812", "authors": ["Anna Jordanous"], "title": "Emovectors: assessing emotional content in jazz improvisations for creativity evaluation", "categories": ["cs.SD", "cs.AI"], "comment": "Presented at IEEE Big Data 2025 3rd Workshop on AI Music Generation (AIMG 2025). https://www.intellisky.org/workshops/AIMG2025/workshop_AIMG2025.html", "summary": "Music improvisation is fascinating to study, being essentially a live demonstration of a creative process. In jazz, musicians often improvise across predefined chord progressions (leadsheets). How do we assess the creativity of jazz improvisations? And can we capture this in automated metrics for creativity for current LLM-based generative systems? Demonstration of emotional involvement is closely linked with creativity in improvisation. Analysing musical audio, can we detect emotional involvement? This study hypothesises that if an improvisation contains more evidence of emotion-laden content, it is more likely to be recognised as creative. An embeddings-based method is proposed for capturing the emotional content in musical improvisations, using a psychologically-grounded classification of musical characteristics associated with emotions. Resulting 'emovectors' are analysed to test the above hypothesis, comparing across multiple improvisations. Capturing emotional content in this quantifiable way can contribute towards new metrics for creativity evaluation that can be applied at scale.", "AI": {"tldr": "本文提出了使用基于嵌入的方法来捕捉爵士即兴表演中的情感内容，以评估创造力。", "motivation": "音乐即兴演奏是创造性过程的现场演示。该研究旨在通过检测更多情感负荷的内容来量化和自动化评估即兴演出的创造力。", "method": "提出了一种基于嵌入的方法来捕捉与情绪相关的情感内容，并将其应用于多个即兴表演中进行分析。", "result": "通过使用'emovectors'（情感向量）可以检测到更多的创意元素，该方法有助于创建新的创造性评估指标。", "conclusion": "利用情感量化技术能够有效地评估爵士乐即兴演奏的创造力。"}}
{"id": "2512.08810", "pdf": "https://arxiv.org/pdf/2512.08810", "abs": "https://arxiv.org/abs/2512.08810", "authors": ["Viola Campos", "Robin Kuschnereit", "Adrian Ulges"], "title": "Multicalibration for LLM-based Code Generation", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "Accepted at AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond)", "summary": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.", "AI": {"tldr": "研究通过多校准方法提高代码生成模型的准确性。", "motivation": "随着基于AI的代码生成变得普遍，需要确保这些模型的信心评分真实反映代码正确性。因此，本研究探索了多校准方法以捕捉编程问题中的额外因素。", "method": "采用四种多校准方法在三个函数合成基准上进行研究，并使用最新一代的代码LLM（Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill）。", "result": "结果显示，多校准可以带来显著改进：相对于未校准的令牌可能性提高了1.03分，在技能得分方面比基线校准方法高出了0.37分。", "conclusion": "本研究证明了在代码生成模型中应用多校准的有效性，并公开了一个包含代码生成、概率和正确性标签的数据集以供进一步研究。"}}
{"id": "2512.08809", "pdf": "https://arxiv.org/pdf/2512.08809", "abs": "https://arxiv.org/abs/2512.08809", "authors": ["Yi Liu", "Weixiang Han", "Chengjun Cai", "Xingliang Yuan", "Cong Wang"], "title": "PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accepted at IEEE INFOCOM 2026 (full version)", "summary": "With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_χ$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.08802", "pdf": "https://arxiv.org/pdf/2512.08802", "abs": "https://arxiv.org/abs/2512.08802", "authors": ["Sadegh Momeni", "Ge Zhang", "Birkett Huber", "Hamza Harkous", "Sam Lipton", "Benoit Seguin", "Yanis Pavlidis"], "title": "Democratizing ML for Enterprise Security: A Self-Sustained Attack Detection Framework", "categories": ["cs.CR", "cs.AI"], "comment": "published in CAMLIS 2025, https://www.camlis.org/", "summary": "Despite advancements in machine learning for security, rule-based detection remains prevalent in Security Operations Centers due to the resource intensiveness and skill gap associated with ML solutions. While traditional rule-based methods offer efficiency, their rigidity leads to high false positives or negatives and requires continuous manual maintenance. This paper proposes a novel, two-stage hybrid framework to democratize ML-based threat detection. The first stage employs intentionally loose YARA rules for coarse-grained filtering, optimized for high recall. The second stage utilizes an ML classifier to filter out false positives from the first stage's output. To overcome data scarcity, the system leverages Simula, a seedless synthetic data generation framework, enabling security analysts to create high-quality training datasets without extensive data science expertise or pre-labeled examples. A continuous feedback loop incorporates real-time investigation results to adaptively tune the ML model, preventing rule degradation. This proposed model with active learning has been rigorously tested for a prolonged time in a production environment spanning tens of thousands of systems. The system handles initial raw log volumes often reaching 250 billion events per day, significantly reducing them through filtering and ML inference to a handful of daily tickets for human investigation. Live experiments over an extended timeline demonstrate a general improvement in the model's precision over time due to the active learning feature. This approach offers a self-sustained, low-overhead, and low-maintenance solution, allowing security professionals to guide model learning as expert ``teachers''.", "AI": {"tldr": "提出了一个两阶段混合框架，利用宽松的YARA规则和机器学习分类器进行自持续攻击检测。", "motivation": "为了克服现有安全运营中心中基于规则的方法灵活性不足的问题，并解决资源密集型ML解决方案带来的技能差距问题。", "method": "采用第一阶段使用宽松的YARA规则进行粗略过滤，第二阶段利用机器学习分类器减少误报。通过Simula生成合成数据以克服数据稀缺性，并引入持续反馈循环来适应性调整模型。", "result": "经过数万台系统的长时间生产环境测试，在每天处理数十亿日志事件的情况下将问题数量降低到每日数百个警报，展示了随着时间推移模型精度的提高。", "conclusion": "该方法提供了一种自持续、低开销和低维护的解决方案，使安全专家能够作为教师指导模型学习。"}}
{"id": "2512.08798", "pdf": "https://arxiv.org/pdf/2512.08798", "abs": "https://arxiv.org/abs/2512.08798", "authors": ["Jeongwhan Choi", "Woosung Kang", "Minseo Kim", "Jongwoo Kim", "Noseong Park"], "title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?", "categories": ["cs.LG", "cs.AI"], "comment": "Rejected from LoG 2025 (submitted August 2025)", "summary": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.", "AI": {"tldr": "研究将图节点分类问题转换为表格学习任务，通过TabPFN-GN模型在不依赖于图形特定训练或语言模型的情况下进行直接节点分类。", "motivation": "探讨是否可以通过特征工程的方法将图数据转换成表格形式，从而利用TabPFN完成与GNN相当的性能表现，并探索这种方法在异质网络上的优越性。", "method": "提出一种名为TabPFN-GN的新方法，该方法通过提取节点属性、结构特性、位置编码以及可选的平滑邻域特征将图数据转化为表格形式。", "result": "实验表明，在12个基准数据集上，TabPFN-GN在同质图上的表现与GNN相当，并且在异质图中表现出更优的效果。", "conclusion": "基于原则性的特征工程可以实现跨领域任务的零样本推广能力，提供了一种无需特定任务训练或语言模型依赖的实践方案。"}}
{"id": "2512.08789", "pdf": "https://arxiv.org/pdf/2512.08789", "abs": "https://arxiv.org/abs/2512.08789", "authors": ["Chaewon Kim", "Seoyeon Lee", "Jonghyuk Park"], "title": "MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures, 5 tables", "summary": "Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.", "AI": {"tldr": "提出了一种基于高频率信息的文档阴影去除框架MatteViT，用于增强数字化文档清晰度。", "motivation": "为了在移除文档中阴影的同时保留文本边缘和线条等细粒度结构细节，以提高光学字符识别性能。", "method": "引入了轻量级高频放大模块(HFAM)来分解并自适应地放大高频成分，并使用定制的遮罩数据集生成连续亮度基线的阴影遮罩，提供精确的空间引导。", "result": "在公共基准(RDD和Kligler)上的大量实验表明，MatteViT达到了最先进的性能，在下游任务中更好地保留了文本级别的细节。", "conclusion": "所提出的方法通过结合高频率信息与阴影遮罩指导提供了更准确的文档阴影去除解决方案。"}}
{"id": "2512.08787", "pdf": "https://arxiv.org/pdf/2512.08787", "abs": "https://arxiv.org/abs/2512.08787", "authors": ["Zeyu Huang", "Xinyi Cao", "Yue Deng", "Junze Li", "Kangyu Yuan", "Xiaojuan Ma"], "title": "Exploring the Grassroots Understanding and Practices of Collective Memory Co-Contribution in a University Community", "categories": ["cs.HC"], "comment": "30 pages, 6 figures, to be published in the journal Proceedings of the ACM on Human-Computer Interaction (PACMHCI 2026, Volume 10, Issue 1)", "summary": "Collective memory -- community members' interconnected memories and impressions of the group -- is essential to the community's culture and identity. Its development requires members' continuous participatory contribution and sensemaking. However, existing works mainly adopt a holistic sociological perspective to analyze well-developed collective memory, less focusing on member-level conceptualization of this possession or what the co-contribution practices can be. Therefore, this work alternatively adopts the latter perspective and probes such interpretative and interactional patterns with two mobile systems. With one being a locative narrative and exploration system condensed from existing literature's design frameworks, and the other being a conventional online forum representing current practices, they served as the anchors of observation for our two-week, mixed-methods field study (n=38) on a university campus. A core debate we have identified was to retrospectively contemplate or document the presence as a history for the future. This also subsequently impacted the narrative focuses, expectations of collective memory constituents, and the ways participants seek inspiration from the group. We further extracted design considerations that could better embrace the diverse conceptualizations of collective memory and bond different community members together. Lastly, revisiting and reflecting on our design, we provided extra insights on designing devoted locative narrative experiences for community-driven UGC platforms.", "AI": {"tldr": "探索大学社区集体记忆共创作的理解与实践。", "motivation": "现有研究主要从整体社会学视角分析成熟的集体记忆，较少关注成员层面的概念或共同贡献的具体实践方式。因此，该研究采用成员级视角探究这种解释和互动模式。", "method": "通过两个移动系统（一个基于现有文献的设计框架的地点叙事探索系统和一个代表当前做法的传统在线论坛）进行为期两周、混合方法的研究（n=38），在大学校园内展开观察。", "result": "研究发现了一个核心辩论，即回顾性思考或记录当下为未来的记忆。这进而影响了叙述焦点、集体记忆构成要素的期望以及参与者从群体中寻求灵感的方式。研究还提取了更好的拥抱多样的集体记忆概念和团结社区成员的设计考虑因素。", "conclusion": "重新审视并反思设计，提供了关于为社区驱动的UGC平台设计奉献地点叙事体验的新见解"}}
{"id": "2512.08786", "pdf": "https://arxiv.org/pdf/2512.08786", "abs": "https://arxiv.org/abs/2512.08786", "authors": ["Mahmoud Srewa", "Tianyu Zhao", "Salma Elmalaki"], "title": "A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.", "AI": {"tldr": "该论文评估了在联邦强化学习环境下，不同偏好聚合策略对大规模语言模型（LLM）与多样人类偏好的一致性的表现。", "motivation": "标准方法在包含多样化观点的联邦学习环境中往往无法充分代表不同的视角，因此需要一种全面的框架来评价偏好聚合策略对于模型一致性的影响。", "method": "论文引入了一个评估框架，在该框架中，每个群体局部评估轮次并生成奖励信号，服务器通过聚合这些组级奖励来进行偏好权重动态调整以优化模型表现。", "result": "实验显示，自适应方案在保持竞争力的同时，能持续提升模型的公平性，特别是在问答任务上使用基于PPO的RLHF管道时效果更佳。", "conclusion": "该研究提供了一种稳健的方法来评估大规模语言模型的行为，并为开发真正多样化的公平一致的模型提供了实用解决方案。"}}
{"id": "2512.08785", "pdf": "https://arxiv.org/pdf/2512.08785", "abs": "https://arxiv.org/abs/2512.08785", "authors": ["Yiming Hao", "Mutian Xu", "Chongjie Ye", "Jie Qin", "Shunlin Lu", "Yipeng Qin", "Xiaoguang Han"], "title": "LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models", "categories": ["cs.CV"], "comment": "Project page: https://jaeger416.github.io/lofa/", "summary": "Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.", "AI": {"tldr": "提出了一种名为LoFA的框架，能够高效预测个性化先验以实现视觉生成模型的快速适应。", "motivation": "当前的方法如Low-Rank Adaptation (LoRA)由于需要特定任务的数据和长时间优化而难以实用。一些基于超网络的方法虽然尝试直接预测适应权重，但无法将细粒度用户提示映射到复杂的LoRA分布中，限制了其实际应用。", "method": "设计了一种两阶段的超网络方法：首先预测相对分布模式以捕获关键适应区域；然后利用这些模式引导最终的LoRA权重预测。", "result": "该方法在多任务和用户提示下均能在几秒钟内准确预测高质量个性化先验，优于需要数小时处理的传统LoRA方法。", "conclusion": "通过提出LoFA框架，可以实现视觉生成模型的快速且高效的个性化适应。"}}
{"id": "2512.08777", "pdf": "https://arxiv.org/pdf/2512.08777", "abs": "https://arxiv.org/abs/2512.08777", "authors": ["David Samuel", "Lilja Øvrelid", "Erik Velldal", "Andrey Kutuzov"], "title": "Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.", "AI": {"tldr": "本文提出了一种针对低资源语言的后训练方法，旨在保持语言模型流畅性的同时进行与不流利奖励模型对齐。", "motivation": "在偏好优化领域已有大量研究，但大多数工作集中在英语和中文上。对于低资源语言，缺乏由母语者编写的数据集以及能够生成流利合成数据的语言模型。因此，本文旨在开发一种无需目标语言指令微调数据的流利对齐语言模型。", "method": "该方法使用在线策略训练方法，并与两种常见方法（机器翻译数据上的监督微调和多语言微调）进行比较。进行了关于挪威博克马尔语的研究，并通过母语者的评估来测试流畅性。", "result": "研究结果显示，在线策略方面至关重要，优于其他替代方案且无需任何难以获得的数据。", "conclusion": "本文展示了如何在低资源语言中保持语言模型的流利度，同时进行偏好对齐。"}}
{"id": "2512.08774", "pdf": "https://arxiv.org/pdf/2512.08774", "abs": "https://arxiv.org/abs/2512.08774", "authors": ["Seoyeon Lee", "Gwangyeol Yu", "Chaewon Kim", "Jonghyuk Park"], "title": "Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 9 figures, 7 tables", "summary": "Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.", "AI": {"tldr": "本文提出了自优化扩散模型，通过可解释AI技术生成缺陷激活图以提高图像生成质量。", "motivation": "虽然扩散模型在图像合成中取得了显著成功，但解决图像中的瑕疵和不真实区域仍是关键挑战。", "method": "提出了一种新型框架——自优化扩散，该框架利用XAI技术生成缺陷激活图来识别瑕疵和不真实区域。通过放大这些区域的噪声以改善重建质量。", "result": "实验结果显示，在多种任务中达到了高达27.3%的Fréchet inception距离改进率，并且在图像生成、文本到图像生成及修复等方面表现出色。", "conclusion": "该框架为各种扩散模型和任务提供了一种通用有效的图像优化方法，显著推进了图像合成领域的进步。"}}
{"id": "2512.08769", "pdf": "https://arxiv.org/pdf/2512.08769", "abs": "https://arxiv.org/abs/2512.08769", "authors": ["Eranga Bandara", "Ross Gore", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows", "categories": ["cs.AI"], "comment": null, "summary": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.", "AI": {"tldr": "该论文提供了一个设计、开发和部署生产级代理式人工智能工作流的实用指南。", "motivation": "随着代理式AI在各个行业的加速采用，组织面临着如何构建可靠、可观察、易维护且符合安全与治理要求的工作流这一挑战。", "method": "本文介绍了包含工作流分解、多代理设计模式、模型上下文协议（MCP）、工具集成、确定性编排、负责任的AI考量及环境感知部署策略在内的结构化工程生命周期。提出了九个核心最佳实践，涵盖工具优先设计、纯函数调用等。", "result": "通过提供架构指导和实际实现见解，文章展示了一个结合多种模式和最佳实践的多媒体新闻分析与媒体生成工作流案例。", "conclusion": "该论文为构建健壮且生产就绪型代理式AI工作流提供了基础参考。"}}
{"id": "2512.08767", "pdf": "https://arxiv.org/pdf/2512.08767", "abs": "https://arxiv.org/abs/2512.08767", "authors": ["Mohammed Elseiagy", "Tsige Tadesse Alemayoh", "Ranulfo Bezerra", "Shotaro Kojima", "Kazunori Ohno"], "title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication at SII 2026. 6 pages, 7 figures. Code is available at: https://github.com/MohamedAlsiagy/dynamic_parameter_est", "summary": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems", "AI": {"tldr": "本文提出了一种基于Transformer的方法，用于机器人动态参数估计。", "motivation": "准确的动态参数估计对于可靠的模型控制、现实模拟和安全部署至关重要。传统方法难以处理复杂的机器人结构与交互问题，数据驱动方法虽有潜力但面临挑战。", "method": "通过自动化的生成流程创建多样化机器人模型及轨迹数据，结合Jacobian导出特征，利用注意力机制的Transformer捕捉时序和空间依赖性。", "result": "最佳配置下（序列长度64、采样率64Hz、四层结构32头），验证集R2得分为0.8633。质量与惯量估计精度高，库仑摩擦中等至高度准确；黏性摩擦和末端中心位置较为困难。", "conclusion": "结合Transformer模型与自动数据生成及运动学增强技术，实现了大规模、精确的动态参数估计，有助于提高机器人系统的仿真到真实环境迁移能力。"}}
{"id": "2512.08765", "pdf": "https://arxiv.org/pdf/2512.08765", "abs": "https://arxiv.org/abs/2512.08765", "authors": ["Ruihang Chu", "Yefei He", "Zhekai Chen", "Shiwei Zhang", "Xiaogang Xu", "Bin Xia", "Dingdong Wang", "Hongwei Yi", "Xihui Liu", "Hengshuang Zhao", "Yu Liu", "Yingya Zhang", "Yujiu Yang"], "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "categories": ["cs.CV"], "comment": "NeurlPS 2025. Code and data available at https://github.com/ali-vilab/Wan-Move", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "AI": {"tldr": "万动提出了一个简单的框架，通过潜在轨迹引导实现视频生成中的运动控制。", "motivation": "现有的运动可控方法通常存在粗粒度的控制和扩展性差的问题，限制了它们的实际应用。作者希望通过精确且高质量的运动控制填补这一空白。", "method": "通过表示物体运动的密集点轨迹，并将其投影到潜在空间中传播第一帧特征以生成对齐的空间时间特征图作为更新后的潜变量条件，无需架构改变即可与现成模型集成。", "result": "万动在MoveBench和公共数据集上的实验表明了其优越的运动质量，可以生成5秒、480p视频，并通过用户研究证明其运动控制能力媲美商业工具Kling 1.5 Pro。", "conclusion": "万动为视频合成提供了更精确且高质量的运动控制方法，提高了模型扩展性和实用性。"}}
{"id": "2512.08755", "pdf": "https://arxiv.org/pdf/2512.08755", "abs": "https://arxiv.org/abs/2512.08755", "authors": ["Dongdong Yang", "Bin Li", "Jiguang He"], "title": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments", "categories": ["cs.AI"], "comment": null, "summary": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.", "AI": {"tldr": "对比分析了无人机搭载的RIS和STAR-RIS在三维无线环境中的性能。", "motivation": "为了全面比较无人机搭载的RIS和STAR-RIS架构，填补其性能评估的研究空白", "method": "通过建立精确的信道模型并考虑部署高度和方位的影响，优化系统总速率问题，并提出基于加权最小均方误差和块坐标下降算法的有效解决方案。", "result": "在低空场景中，STAR-RIS由于全空间覆盖能力优于RIS；而在靠近基站的高度上，RIS的表现更好。", "conclusion": "该研究为未来6G通信系统中的智能表面部署提供了实际见解。"}}
{"id": "2512.08754", "pdf": "https://arxiv.org/pdf/2512.08754", "abs": "https://arxiv.org/abs/2512.08754", "authors": ["Jason Hughes", "Marcel Hussing", "Edward Zhang", "Shenbagaraj Kannapiran", "Joshua Caswell", "Kenneth Chaney", "Ruichen Deng", "Michaela Feehery", "Agelos Kratimenos", "Yi Fan Li", "Britny Major", "Ethan Sanchez", "Sumukh Shrote", "Youkang Wang", "Jeremy Wang", "Daudi Zein", "Luying Zhang", "Ruijun Zhang", "Alex Zhou", "Tenzi Zhouga", "Jeremy Cannon", "Zaffir Qasim", "Jay Yelon", "Fernando Cladera", "Kostas Daniilidis", "et al. (2 additional authors not shown)"], "title": "A Multi-Robot Platform for Robotic Triage Combining Onboard Sensing and Foundation Models", "categories": ["cs.RO"], "comment": "Technical Report for the DARPA Triage Challenge PRONTO team", "summary": "This report presents a heterogeneous robotic system designed for remote primary triage in mass-casualty incidents (MCIs). The system employs a coordinated air-ground team of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to locate victims, assess their injuries, and prioritize medical assistance without risking the lives of first responders. The UAV identify and provide overhead views of casualties, while UGVs equipped with specialized sensors measure vital signs and detect and localize physical injuries. Unlike previous work that focused on exploration or limited medical evaluation, this system addresses the complete triage process: victim localization, vital sign measurement, injury severity classification, mental status assessment, and data consolidation for first responders. Developed as part of the DARPA Triage Challenge, this approach demonstrates how multi-robot systems can augment human capabilities in disaster response scenarios to maximize lives saved.", "AI": {"tldr": "本文介绍了一种异构机器人系统，用于在大规模伤亡事件中进行远程初步分类。该系统结合了无人机和无人地面车辆来定位受害者、评估伤情并优先提供医疗援助。", "motivation": "旨在降低第一响应者的风险，在灾难现场通过多机器人协同工作提高救援效率和效果。", "method": "使用无人机获取高空视图，识别潜在受害者；利用无人地面车装备专用传感器测量生命体征，并检测、定位身体受伤情况。系统还整合数据供第一响应者使用。", "result": "该方法展示了如何通过多机器人协作增强人类在灾难场景中的能力，以最大化拯救的生命数量。", "conclusion": "此报告表明了利用空中和地面无人车辆协同工作可以有效应对大规模伤亡事件中复杂的初步分类任务。"}}
{"id": "2512.08751", "pdf": "https://arxiv.org/pdf/2512.08751", "abs": "https://arxiv.org/abs/2512.08751", "authors": ["Kuniko Paxton", "Koorosh Aslansefat", "Dhavalkumar Thakker", "Yiannis Papadopoulos"], "title": "Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.", "AI": {"tldr": "本文提出了一种基于偏度的剪枝方法，用于在边缘设备上对多模态Swin Transformer进行高效压缩和隐私保护分布式学习。", "motivation": "为了克服高性能计算机视觉模型在医疗成像中的计算强度大、体积大的问题，并解决严格隐私约束下的集中式数据管理难题，本文提出了一种基于偏度的剪枝方法，以适应边缘设备上的联邦学习环境。", "method": "该方法通过统计多模态Swin Transformer中Multi-Head Self-Attention和Multi-Layer Perceptron层输出分布的偏度来选择性地进行剪枝。", "result": "在水平联邦学习环境中验证了所提方法，显示了其能够维持性能的同时大幅减少模型复杂度。实验表明，对于紧凑型Swin Transformer，这种方法实现了大约36%的模型大小缩减，并且没有损失准确性。", "conclusion": "这些发现证明了实现多模态医疗AI在边缘设备上高效模型压缩和隐私保护分布式学习是可行的"}}
{"id": "2512.08747", "pdf": "https://arxiv.org/pdf/2512.08747", "abs": "https://arxiv.org/abs/2512.08747", "authors": ["Artúr I. Károly", "Péter Galambos"], "title": "A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation", "categories": ["cs.CV"], "comment": "20 pages, 8 figures", "summary": "Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.", "AI": {"tldr": "该论文提出了一种结合三维渲染和引导扩散模型的可扩展流程，用于生成高质量、逼真的合成数据集，以支持白蘑菇分割任务。", "motivation": "计算机视觉在工业香菇栽培中逐渐普及，但开发准确的检测和分割模型需要大量精确标注的数据集，这通常是昂贵且耗时的过程。现有的合成数据往往缺乏足够的真实性来推广到现实场景，因此提出了一个新的工作流程来解决这个问题。", "method": "该方法结合了Blender中的三维渲染与受限扩散模型，自动生成高质量、真实感的带有注释的合成图像，并适用于白蘑菇（Agaricus Bisporus）分割任务。通过这种方法，用户能够保持对场景配置和注释的完全控制。", "result": "提出的合成数据集在两个独立的真实世界数据集中达到了最先进的分割性能，其中F1得分达到0.859 (M18K)。", "conclusion": "该方法展示了使用合成数据训练模型的有效性，并且可以被应用于其他蘑菇种类或农业领域中的水果和叶子检测任务。"}}
{"id": "2512.08743", "pdf": "https://arxiv.org/pdf/2512.08743", "abs": "https://arxiv.org/abs/2512.08743", "authors": ["Shuyue Hu", "Haoyang Yan", "Yiqun Zhang", "Yang Chen", "Dongzhan Zhou", "Lei Bai"], "title": "Towards Foundation Models with Native Multi-Agent Intelligence", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.", "AI": {"tldr": "探讨如何使基础模型具备原生的多智能体智能。", "motivation": "当前基础模型在单智能体能力方面取得进展，但缺乏原生多智能体智能。该研究旨在填补这一空白，并提出了相关研究方向。", "method": "通过跨41个大型语言模型进行实证研究，展示单智能体性能并不能自动产生强大的多智能体智能，提出数据集构建、评估方法、训练范式和安全考虑等关键研究路径。", "result": "提供大量实证证据表明单智能体表现优秀并不意味着具备强大的多智能体能力。", "conclusion": "强调未来的研究应关注于基础模型的原生多智能体智能，包括理解、规划、高效沟通与适应性等方面。"}}
{"id": "2512.08742", "pdf": "https://arxiv.org/pdf/2512.08742", "abs": "https://arxiv.org/abs/2512.08742", "authors": ["Chase Hutton", "Adam Melrod"], "title": "Parallel Batch Dynamic Vertex Coloring in $O(\\log Δ)$ Amortized Update Time", "categories": ["cs.DS", "cs.DC"], "comment": null, "summary": "We present the first parallel batch-dynamic algorithm for maintaining a proper $(Δ+ 1)$-vertex coloring. Our approach builds on a new sequential dynamic algorithm inspired by the work of Bhattacharya et al. (SODA'18). The resulting randomized algorithm achieves $O(\\log Δ)$ expected amortized update time and, for any batch of $b$ updates, has parallel span $O(\\operatorname{polylog} b + \\operatorname{polylog} n)$ with high probability.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.08740", "pdf": "https://arxiv.org/pdf/2512.08740", "abs": "https://arxiv.org/abs/2512.08740", "authors": ["Yiming Lu"], "title": "Deconstructing the Dual Black Box:A Plug-and-Play Cognitive Framework for Human-AI Collaborative Enhancement and Its Implications for AI Governance", "categories": ["cs.AI"], "comment": "31 pages, in Chinese language, 5 figures,3 tables", "summary": "Currently, there exists a fundamental divide between the \"cognitive black box\" (implicit intuition) of human experts and the \"computational black box\" (untrustworthy decision-making) of artificial intelligence (AI). This paper proposes a new paradigm of \"human-AI collaborative cognitive enhancement,\" aiming to transform the dual black boxes into a composable, auditable, and extensible \"functional white-box\" system through structured \"meta-interaction.\" The core breakthrough lies in the \"plug-and-play cognitive framework\"--a computable knowledge package that can be extracted from expert dialogues and loaded into the Recursive Adversarial Meta-Thinking Network (RAMTN). This enables expert thinking, such as medical diagnostic logic and teaching intuition, to be converted into reusable and scalable public assets, realizing a paradigm shift from \"AI as a tool\" to \"AI as a thinking partner.\" This work not only provides the first engineering proof for \"cognitive equity\" but also opens up a new path for AI governance: constructing a verifiable and intervenable governance paradigm through \"transparency of interaction protocols\" rather than prying into the internal mechanisms of models. The framework is open-sourced to promote technology for good and cognitive inclusion. This paper is an independent exploratory research conducted by the author. All content presented, including the theoretical framework (RAMTN), methodology (meta-interaction), system implementation, and case validation, constitutes the author's individual research achievements.", "AI": {"tldr": "提出了一种“人类-AI协作认知增强”的新范式，将隐性的人类专家直觉和不透明的AI决策转化为可组合、可审计的功能白盒系统。", "motivation": "解决人类专家的认知黑盒与人工智能计算黑盒之间的根本分裂问题，实现认知平等和更有效的AI治理。", "method": "通过结构化的“元交互”，将专家对话中提取的知识包加载到递归对抗元思考网络（RAMTN）中，从而将专家的思维转化为可重用、可扩展的公共资源。", "result": "实现了从“AI作为工具”到“AI作为思考伙伴”的范式转变，并为认知平等提供了首个工程证明。", "conclusion": "该框架促进了技术向善和认知包容性的发展，通过交互协议透明度而非深入模型内部机制来构建可验证且可干预的治理模式。"}}
{"id": "2512.08738", "pdf": "https://arxiv.org/pdf/2512.08738", "abs": "https://arxiv.org/abs/2512.08738", "authors": ["Samuel Ebimobowei Johnny", "Blessed Guda", "Emmanuel Enejo Aaron", "Assane Gueye"], "title": "Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture", "categories": ["cs.CV", "cs.CL"], "comment": "To appear at AACL-IJCNLP 2025 Workshop WSLP", "summary": "Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\\% accuracy and 60.00\\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting", "AI": {"tldr": "提出了一种基于姿态的端到端编码器架构，用于手语识别中的特定手势检测。", "motivation": "解决连续手势序列中特定手势检索问题，通过直接操作姿势关键点减少计算成本和视觉噪声", "method": "采用仅使用编码器的网络结构，并加入二分类头来判断查询手势是否存在于目标序列中", "result": "在Word Presence Prediction数据集上达到61.88%准确率和60.00%F1值，验证了基于姿势的手语检测的有效性。", "conclusion": "本研究建立了首个手语检索的基础模型，并为自动手语识别提供了新的方向。"}}
{"id": "2512.08733", "pdf": "https://arxiv.org/pdf/2512.08733", "abs": "https://arxiv.org/abs/2512.08733", "authors": ["Kuniko Paxton", "Zeinab Dehghani", "Koorosh Aslansefat", "Dhavalkumar Thakker", "Yiannis Papadopoulos"], "title": "Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.", "AI": {"tldr": "论文提出了一种基于分布的框架来评估和减轻皮肤病变分类中的个人肤色偏见。", "motivation": "传统的公平性研究在医学成像领域中通常依赖于粗略的子组类别划分，忽略了个体水平上的变化。这种基于群体的方法可能会掩盖子群内部少数人的偏差问题。", "method": "论文将肤色视为一个连续属性而非分类标签，并使用核密度估计(KDE)来建模其分布。通过比较十二种统计距离度量指标量化不同皮肤色调之间的差异，提出了基于距离的重新加权(DRW)损失函数以纠正少数皮肤色调下的不足。", "result": "实验结果表明：（i）类别重加权在捕捉个人水平上的偏差方面存在局限性；（ii）基于分布的重加权方法表现出色，特别是在使用保真度相似度(FS)， Wasserstein距离(WD), Hellinger度量(HM)和调和均值相似度(HS)的情况下。", "conclusion": "论文建立了一种在个人层面上促进公平性进展的方法，特别适用于皮肤科AI系统，并强调了对医学图像分析中敏感连续属性的更广泛意义。"}}
{"id": "2512.08730", "pdf": "https://arxiv.org/pdf/2512.08730", "abs": "https://arxiv.org/abs/2512.08730", "authors": ["Kaiyu Li", "Shengqi Zhang", "Yupeng Deng", "Zhi Wang", "Deyu Meng", "Xiangyong Cao"], "title": "SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images", "categories": ["cs.CV"], "comment": null, "summary": "Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.", "AI": {"tldr": "本文探索了将Segment Anything Model 3 (SAM 3) 应用于遥感图像中的无训练开放词汇语义分割任务。", "motivation": "现有的无训练开放词汇语义分割方法大多基于CLIP，但在精确定位和复杂场景处理方面存在挑战。最近提出的SAM 3可以统一处理分割和识别，并且在不进行任何训练的情况下适应遥感数据集。", "method": "提出了一种结合SAM 3语义头和Transformer解码器输出的掩膜融合策略来改进土地覆盖，同时利用存在得分过滤掉场景中不存在的类别以减少假阳性。", "result": "实验结果显示该方法在广泛的遥感数据集中取得了显著的结果，展示了SAM 3应用于开放词汇语义分割任务的巨大潜力。", "conclusion": "本文的工作证明了使用Segment Anything Model 3进行无训练开放词汇语义分割的有效性，并为未来的研究提供了方向。"}}
{"id": "2512.08715", "pdf": "https://arxiv.org/pdf/2512.08715", "abs": "https://arxiv.org/abs/2512.08715", "authors": ["Sébastien Piérard", "Adrien Deliège", "Marc Van Droogenbroeck"], "title": "Multi-domain performance analysis with scores tailored to user preferences", "categories": ["cs.PF", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.", "AI": {"tldr": "本文探讨了根据用户偏好调整多领域性能分析的方法，定义了几种特定领域的概念，并开发了新的视觉工具。", "motivation": "算法、方法和模型的性能取决于它们应用的数据分布情况。在多个领域评估后，计算加权平均值以获得总体表现具有重要意义。", "method": "采用概率框架考虑性能为概率度量，定义了四个特定领域的概念，并基于用户偏好开发新的视觉工具。", "result": "提出了一种新的多领域性能分析方法，包括特定领域定义和新的可视化工具。", "conclusion": "新提出的理论和工具有助于更深入地理解算法在不同环境下的表现。"}}
{"id": "2512.08713", "pdf": "https://arxiv.org/pdf/2512.08713", "abs": "https://arxiv.org/abs/2512.08713", "authors": ["Ekhi Azurmendi", "Xabier Arregi", "Oier Lopez de Lacalle"], "title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to LREC 2026", "summary": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.", "AI": {"tldr": "该论文介绍了首个面向巴斯克语学习者C1水平的自动作文评分和反馈生成数据集，并展示了使用开源模型进行训练后，在评分准确性和反馈质量上超越了现有闭源系统。", "motivation": "构建一个针对巴斯克语学习者的作文评分和反馈生成系统的公开数据集，以推动低资源语言中的NLP研究。", "method": "利用HABE数据集中3200篇作文及其详细注释，对开源模型进行微调训练，并提出了一种结合自动一致性度量与专家验证的新型评估方法。", "result": "实验表明，经过微调后的Latxa模型在评分一致性和反馈质量上优于现有闭源系统GPT-5和Claude Sonnet 4.5，能够生成符合标准且具有教育意义的反馈，并识别出更多错误类型。", "conclusion": "该工作建立了一个透明、可重复的基础资源及基准测试，在低资源语言领域推动了基于NLP技术的作文评分与反馈研究。"}}
{"id": "2512.08700", "pdf": "https://arxiv.org/pdf/2512.08700", "abs": "https://arxiv.org/abs/2512.08700", "authors": ["Kyumin Hwang", "Wonhyeok Choi", "Kiljoon Han", "Wonjoon Choi", "Minwoo Choi", "Yongcheon Na", "Minwoo Park", "Sunghoon Im"], "title": "Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth", "categories": ["cs.CV"], "comment": "Accepted at IEEE Robotics and Automation Letters (RA-L) 2026", "summary": "Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.", "AI": {"tldr": "本文提出了一种新的知识蒸馏策略，用于将深度知识从基础模型转移到轻量级FSMDE网络中。", "motivation": "现有的基础模型在单目深度估计上表现出强大的泛化能力，但直接应用于全环绕单目深度估计算法会面临高计算成本和难以估算实际尺度深度的问题。为了解决这些问题，本文提出了新的知识蒸馏策略。", "method": "本文的方法包括一种混合回归框架，结合了传统分类中使用的知识蒸馏方案与深度分箱模块以增强比例一致性；以及一种跨交互式知识蒸馏方案，将基础模型的比例不变的深度概率传递给学生网络，并指导其从地面真实深度推断实际尺度深度中心。此外，还提出了一种视图关系知识蒸馏策略，编码相邻摄像头视图之间的结构关系，并将其转移以增强跨视图深度一致性。", "result": "实验结果表明，本文的方法在DDAD和nuScenes数据集上优于传统的监督方法和其他现有的知识蒸馏方法。此外，该方法实现了性能与效率的有利权衡，满足实时要求。", "conclusion": "通过提出新的知识蒸馏策略，本文成功地解决了全环绕单目深度估计中的高计算成本和实际尺度深度估算困难的问题，并展示了优于现有方法的结果。"}}
{"id": "2512.08697", "pdf": "https://arxiv.org/pdf/2512.08697", "abs": "https://arxiv.org/abs/2512.08697", "authors": ["Athena Psalta", "Vasileios Tsironis", "Konstantinos Karantzalos"], "title": "What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID", "AI": {"tldr": "本文提出了一种基于混合专家框架的行人重新识别方法，用于系统量化行人属性的重要性。", "motivation": "现有最先进的行人重识别方法虽然表现出色，但仍然缺乏透明性。该研究旨在揭示这些模型依赖于哪些高层次语义属性。", "method": "作者提出了MoSAIC-ReID，一个混合专家框架，使用LoRA基线专家和一个oracle路由器，每个专家与单一属性相关联，以便进行控制归因分析。", "result": "在Market-1501和DukeMTMC数据集上，该方法取得了竞争性性能。通过广义线性模型、统计测试和特征重要性分析揭示了哪些属性（如服装颜色和内在特性）对重识别贡献最大，而稀有线索则影响较小。", "conclusion": "这项工作提供了一个可解释的行人重新识别框架，并强调在实践中集成显式语义知识的要求。"}}
{"id": "2512.08688", "pdf": "https://arxiv.org/pdf/2512.08688", "abs": "https://arxiv.org/abs/2512.08688", "authors": ["Mark Pustilnik", "Francesco Borrelli"], "title": "Non Normalized Shared-Constraint Dynamic Games for Human-Robot Collaboration with Asymmetric Responsibility", "categories": ["cs.RO"], "comment": null, "summary": "This paper proposes a dynamic game formulation for cooperative human-robot navigation in shared workspaces with obstacles, where the human and robot jointly satisfy shared safety constraints while pursuing a common task. A key contribution is the introduction of a non-normalized equilibrium structure for the shared constraints. This structure allows the two agents to contribute different levels of effort towards enforcing safety requirements such as collision avoidance and inter-players spacing. We embed this non-normalized equilibrium into a receding-horizon optimal control scheme.", "AI": {"tldr": "本文提出了一种动态博弈框架，用于人类与机器人在共享工作空间中的合作导航。", "motivation": "解决人类和机器人共同执行任务时的安全性和协作性问题，特别是在存在障碍物的情况下确保双方遵守安全约束。", "method": "引入了非归一化均衡结构来处理共同约束，并将其嵌入到预测控制方案中。", "result": "实现了不同层次的努力贡献以满足碰撞避免等安全要求的动态博弈模型。", "conclusion": "通过该方法，人类和机器人可以在共享的工作空间内更有效地协作导航并完成任务。"}}
{"id": "2512.08674", "pdf": "https://arxiv.org/pdf/2512.08674", "abs": "https://arxiv.org/abs/2512.08674", "authors": ["Rongzhao Zhang", "Junqiao Wang", "Shuyun Yang", "Mouxiao Bian", "Chao Ding", "Yuwei Bai", "Chihao Zhang", "Yuguang Shen", "Lei Wang", "Lei Zheng", "Qiujuan Yan", "Yun Zhong", "Meiling Liu", "Jiwei Yu", "Zheng Wang", "Jie Xu", "Meng Luo"], "title": "Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.", "AI": {"tldr": "提出了一种多智能体框架，用于胃肠肿瘤学中的多学科决策支持。", "motivation": "为了解决多模态大语言模型在处理复杂、异质性医疗记录时遇到的背景稀释和幻觉问题。", "method": "设计了一个模仿人类多学科团队工作流程的层次化多智能体框架，用于综合分析内窥镜图像、放射数据和生化标志物。", "result": "该系统得到了4.60/5.00的专业评估分数，并且在推理逻辑和医学准确性方面表现出了显著改进。", "conclusion": "模仿合作的代理架构为肿瘤学中的自动化决策支持提供了一种可扩展、解释性强且临床可靠的范式。"}}
{"id": "2512.08673", "pdf": "https://arxiv.org/pdf/2512.08673", "abs": "https://arxiv.org/abs/2512.08673", "authors": ["Shaofeng Zhang", "Xuanqi Chen", "Xiangdong Zhang", "Sitong Wu", "Junchi Yan"], "title": "Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds", "categories": ["cs.CV"], "comment": "16 pages, 6 figures", "summary": "Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \\textbf{C}enter-\\textbf{S}urrounding \\textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \\textbf{7.9\\%}, \\textbf{6.7\\%}, and \\textbf{10.3\\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.", "AI": {"tldr": "提出了一种用于三维点云对比学习的新型双分支中心-周围对比框架CSCon，以更好地捕捉几何信息和局部细节。", "motivation": "当前大多数基于MAE的生成方法在捕捉高级鉴别特征方面存在困难，而现有的3D数据对比学习技术也未能有效学习到局部细节。为解决这些问题，作者提出了一种新型框架来提高三维点云的学习效果。", "method": "通过分别对中心和周围部分进行掩码处理，构建具有中心偏置和周围偏置表示的双分支输入；引入了补丁级别的对比损失以增强高级信息和局部敏感度。", "result": "在FULL和ALL协议下，CSCon表现与生成方法相当；在MLP-LINEAR、MLP-3以及ONLY-NEW协议下，该方法达到最佳结果，甚至超过跨模态方法。特别是在MLP-LINEAR协议中，相较于基线（Point-MAE），分别在ScanObjectNN的三个变体上提升了7.9%，6.7%和10.3%。", "conclusion": "提出的CSCon框架能够在三维点云对比学习任务中有效捕捉局部细节和高级鉴别特征，表现优于现有方法。"}}
{"id": "2512.08661", "pdf": "https://arxiv.org/pdf/2512.08661", "abs": "https://arxiv.org/abs/2512.08661", "authors": ["Ziyue Zheng", "Yongce Liu", "Hesheng Wang", "Zhongqiang Ren"], "title": "Ergodic Trajectory Planning with Dynamic Sensor Footprints", "categories": ["cs.RO"], "comment": "12 figures", "summary": "This paper addresses the problem of trajectory planning for information gathering with a dynamic and resolution-varying sensor footprint. Ergodic planning offers a principled framework that balances exploration (visiting all areas) and exploitation (focusing on high-information regions) by planning trajectories such that the time spent in a region is proportional to the amount of information in that region. Existing ergodic planning often oversimplifies the sensing model by assuming a point sensor or a footprint with constant shape and resolution. In practice, the sensor footprint can drastically change over time as the robot moves, such as aerial robots equipped with downward-facing cameras, whose field of view depends on the orientation and altitude. To overcome this limitation, we propose a new metric that accounts for dynamic sensor footprints, analyze the theoretic local optimality conditions, and propose numerical trajectory optimization algorithms. Experimental results show that the proposed approach can simultaneously optimize both the trajectories and sensor footprints, with up to an order of magnitude better ergodicity than conventional methods. We also deploy our approach in a multi-drone system to ergodically cover an object in 3D space.", "AI": {"tldr": "本文提出了一个新指标，用于处理动态传感器光斑的变化，并通过数值轨迹优化算法实现了同时优化轨迹和传感器光斑的目标。", "motivation": "现有的遍历规划方法通常简化了传感器模型，假设为点传感器或常形分辨率的光斑。然而，在实践中，随着机器人移动，传感器光斑可以发生剧烈变化，如搭载向下相机的空中机器人其视场依赖于方向和高度。", "method": "提出了一种新的度量指标来处理动态传感器光斑，并分析了局部最优条件，进而提出了数值轨迹优化算法。", "result": "实验结果显示，所提出的方案能够同时优化轨迹和传感器光斑，在遍历性方面比传统方法提高了多个数量级。此外，在多无人机系统中部署该方案时，也能有效覆盖3D空间中的物体。", "conclusion": "通过提出的新指标和数值算法，解决了动态变化的传感器光斑问题，并实现了更优的轨迹规划性能，同时展示了在复杂环境下的应用潜力。"}}
{"id": "2512.08657", "pdf": "https://arxiv.org/pdf/2512.08657", "abs": "https://arxiv.org/abs/2512.08657", "authors": ["Renato Cordeiro Ferreira", "Aditya Dhinavahi", "Rowanne Trapmann", "Willem-Jan van den Heuvel"], "title": "Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "7 pages, 3 figures (3 diagrams), submitted to ICSA 2026", "summary": "ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.", "AI": {"tldr": "该论文展示了一个在海洋领域进行异常检测的机器学习驱动系统（MLES）构建中应用端口和适配器模式的经验报告，探讨如何通过单一代码库构建多个微服务。", "motivation": "旨在提高软件架构中的可重用性，特别是利用端口与适配器模式来应对构建多个微服务时遇到的挑战，并总结经验和教训。", "method": "采用Hexagonal（六边形）架构模式和Ports and Adapters设计模式来建立一个模块化、灵活且易于维护的MLES。", "result": "通过这种方法，成功地复用了代码库来支持海洋领域多个微服务的构建。展示了如何在实际项目中实现这种可重用性策略的具体步骤与效果。", "conclusion": "应用Ports and Adapters模式和Hexagonal架构有助于提高MLES的设计效率和可维护性，并为其他类似的系统提供了有益的经验教训。"}}
{"id": "2512.08656", "pdf": "https://arxiv.org/pdf/2512.08656", "abs": "https://arxiv.org/abs/2512.08656", "authors": ["Lauritz Rismark Fosso", "Herman Biørn Amundsen", "Marios Xanthidis", "Sveinung Johan Ohrem"], "title": "Sim2Swim: Zero-Shot Velocity Control for Agile AUV Maneuvering in 3 Minutes", "categories": ["cs.RO"], "comment": "7 pages, 4 figures", "summary": "Holonomic autonomous underwater vehicles (AUVs) have the hardware ability for agile maneuvering in both translational and rotational degrees of freedom (DOFs). However, due to challenges inherent to underwater vehicles, such as complex hydrostatics and hydrodynamics, parametric uncertainties, and frequent changes in dynamics due to payload changes, control is challenging. Performance typically relies on carefully tuned controllers targeting unique platform configurations, and a need for re-tuning for deployment under varying payloads and hydrodynamic conditions. As a consequence, agile maneuvering with simultaneous tracking of time-varying references in both translational and rotational DOFs is rarely utilized in practice. To the best of our knowledge, this paper presents the first general zero-shot sim2real deep reinforcement learning-based (DRL) velocity controller enabling path following and agile 6DOF maneuvering with a training duration of just 3 minutes. Sim2Swim, the proposed approach, inspired by state-of-the-art DRL-based position control, leverages domain randomization and massively parallelized training to converge to field-deployable control policies for AUVs of variable characteristics without post-processing or tuning. Sim2Swim is extensively validated in pool trials for a variety of configurations, showcasing robust control for highly agile motions.", "AI": {"tldr": "介绍了一种零样本仿真到现实的深度强化学习方法，用于自主水下航行器（AUV）的速度控制。", "motivation": "针对由于复杂的静水力和流体力学、参数不确定性以及负载变化引起的动力学频繁改变等问题，使敏捷操控变得困难。现有的控制系统往往需要针对特定平台配置进行精细调优，并在不同的载荷和流体条件下重新调整。这导致了实际应用中很少利用同时跟踪时间变化的速度参考的六自由度敏捷操纵。", "method": "Sim2Swim采用基于深度强化学习的位置控制方法，结合域随机化和大规模并行训练来收敛到适用于各种特征AUV的可部署操控策略。这种方法能够在3分钟内完成训练。", "result": "通过池塘试验验证了Sim2Swim在多种配置下的稳健性，展示了高度敏捷运动中的强大控制能力。", "conclusion": "首次提出了基于深度强化学习的方法，使得自主水下航行器能够实现六自由度的路径跟踪和敏捷操控，并且仅需3分钟的训练时间即可达到部署标准。"}}
{"id": "2512.08653", "pdf": "https://arxiv.org/pdf/2512.08653", "abs": "https://arxiv.org/abs/2512.08653", "authors": ["Doumegna Mawuto Koudjo Felix", "Xianjia Yu", "Zhuo Zou", "Tomi Westerlund"], "title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation", "categories": ["cs.RO"], "comment": null, "summary": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.", "AI": {"tldr": "本文提出了一种基于真实点云的传感器感知现象学框架，用于模拟可解释的激光雷达退化，并评估SLAM系统的鲁棒性。", "motivation": "现有的激光雷达SLAM系统在面对遮挡、噪声和视场（FoV）退化等不利条件时十分敏感，而目前的鲁棒性评价方法要么缺乏物理基础，要么无法捕捉到传感器特有的行为。因此，本文旨在填补这一研究空白。", "method": "该框架通过对真实点云应用结构化dropout、FoV减少、高斯噪声、遮挡屏蔽、稀疏化和运动失真等退化技术，在保留每个点的几何、强度和时间结构的同时，模拟激光雷达退化的现象。此外，它还具有自主主题和传感器检测功能，并提供四种严重程度级别的模块化配置（轻度到极端）。", "result": "实验验证表明，该框架在三种不同架构的激光雷达和五个最先进的SLAM系统之间揭示了由传感器设计和环境背景共同塑造的独特鲁棒性模式。开源实现为基于物理意义退化的激光雷达SLAM基准测试提供了实用基础。", "conclusion": "所提出的传感器感知现象学框架成功地模拟了真实点云中的可解释的激光雷达退化，并有效地评估了不同架构下的SLAM系统的鲁棒性，从而填补了现有方法在物理基础和传感器特异性方面存在的不足。"}}
{"id": "2512.08648", "pdf": "https://arxiv.org/pdf/2512.08648", "abs": "https://arxiv.org/abs/2512.08648", "authors": ["Shaofeng Zhang", "Xuanqi Chen", "Ning Liao", "Haoxiang Zhao", "Xiaoxing Wang", "Haoru Tan", "Sitong Wu", "Xiaosong Jia", "Qi Fan", "Junchi Yan"], "title": "Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank", "categories": ["cs.CV"], "comment": "19 pages, 19 figures", "summary": "The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\\mname} achieves a state-of-the-art FID of \\textbf{2.40} within 400k steps, significantly outperforming comparable methods.", "AI": {"tldr": "该论文提出了一种名为Repulsor的训练框架，用于加速生成模型的学习。", "motivation": "现有去噪生成模型存在训练成本高和表示学习效率低的问题。辅助对齐方法引入了外部预训练编码器，增加了开销和领域偏移问题。", "method": "通过提出一种基于记忆库机制的方法，Repulsor在训练迭代中维持一个大规模的动态更新负样本队列，以实现高效的生成模型加速。", "result": "在ImageNet-256数据集上，该方法实现了FID为2.40的最佳状态，在400k步内显著优于同类方法。", "conclusion": "Repulsor框架通过引入记忆库机制，解决了对预训练编码器的依赖问题，并提高了生成模型的学习效率。"}}
{"id": "2512.08647", "pdf": "https://arxiv.org/pdf/2512.08647", "abs": "https://arxiv.org/abs/2512.08647", "authors": ["Keito Inoshita"], "title": "C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.", "AI": {"tldr": "提出了一种轻量级的驾驶员行为识别框架C-DIRA，结合动态ROI路由和对抗学习提高模型效率与鲁棒性", "motivation": "现有轻量级模型难以在边缘设备上实现实时推断，并且无法有效捕捉细微的行为特征；而基于ROI的方法虽然能提升性能但增加了计算成本。因此需要一种既高效又准确的识别框架", "method": "结合了Top-K ROI池化和融合分类进行局部特征提取，使用动态ROI路由选择性地对高难度样本进行推理，并通过伪域标记和对抗学习来学习鲁棒于驾驶者和背景变化的不变特征", "result": "实验表明C-DIRA在较少FLOPs和更低延迟的情况下保持了较高的准确率，并且在视觉退化如模糊和低光下表现出较强的鲁棒性，以及跨不同领域时性能稳定", "conclusion": "该研究证明了C-DIRA框架能够实现紧凑、高效并且具有良好的泛化能力"}}
{"id": "2512.08645", "pdf": "https://arxiv.org/pdf/2512.08645", "abs": "https://arxiv.org/abs/2512.08645", "authors": ["Young Kyung Kim", "Oded Schlesinger", "Yuzhou Zhao", "J. Matias Di Martino", "Guillermo Sapiro"], "title": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation", "categories": ["cs.CV"], "comment": "19 pages, 13 figures", "summary": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.", "AI": {"tldr": "提出了链式图像生成框架，使图像生成过程更加透明和可控。", "motivation": "现有图像生成模型内部机制不透明，难以观察干预，阻碍了可靠性和安全性保障。为此，引入Chain-of-Image Generation (CoIG) 框架以解决这些问题。", "method": "利用LLM将复杂指令分解为一系列简单步骤，并通过逐步生成和编辑图像实现这些步骤。", "result": "实验结果显示，该框架在定量监控方面有显著提升，且与基准模型相比具有竞争性的组合稳健性。", "conclusion": "CoIG框架提高了图像生成过程的透明度和控制性，增强了安全性并允许更好地干预。"}}
{"id": "2512.08639", "pdf": "https://arxiv.org/pdf/2512.08639", "abs": "https://arxiv.org/abs/2512.08639", "authors": ["Huilin Xu", "Zhuoyang Liu", "Yixiang Luomei", "Feng Xu"], "title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review, 12 pages, 9 figures", "summary": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.", "AI": {"tldr": "本文提出了一种统一的空域视觉语言导航框架，该框架仅依赖于单目RGB图像和自然语言指令进行无人机导航。", "motivation": "现有方法通常需要全景图、深度输入或里程计来支持空间推理和行动规划，这增加了系统的成本和集成复杂性，限制了其在轻量级无人机上的实际部署。本文旨在开发一种仅基于单目RGB图像和自然语言指令的导航系统。", "method": "该模型将导航定义为下一个标记预测问题，并通过提示引导的多任务学习同时优化空间感知、轨迹推理和行动预测。此外，还提出了一种关键帧选择策略以减少视觉冗余，并引入了动作合并与标签重新加权机制来减轻长期监督不平衡。", "result": "实验结果表明，在严格的单目RGB环境下，该模型在已见和未见过的环境中均表现出色，显著优于现有的单目RGB基准线，并缩小了与全景RGB-D基线的性能差距。", "conclusion": "研究证明了所提出任务设计和架构选择的有效性。"}}
{"id": "2512.08630", "pdf": "https://arxiv.org/pdf/2512.08630", "abs": "https://arxiv.org/abs/2512.08630", "authors": ["Marta Manzoni", "Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "Multi-Task Bayesian Optimization for Tuning Decentralized Trajectory Generation in Multi-UAV Systems", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "This paper investigates the use of Multi-Task Bayesian Optimization for tuning decentralized trajectory generation algorithms in multi-drone systems. We treat each task as a trajectory generation scenario defined by a specific number of drone-to-drone interactions. To model relationships across scenarios, we employ Multi-Task Gaussian Processes, which capture shared structure across tasks and enable efficient information transfer during optimization. We compare two strategies: optimizing the average mission time across all tasks and optimizing each task individually. Through a comprehensive simulation campaign, we show that single-task optimization leads to progressively shorter mission times as swarm size grows, but requires significantly more optimization time than the average-task approach.", "AI": {"tldr": "本文研究了在多无人机系统中使用多任务贝叶斯优化来调整分散轨迹生成算法的方法。", "motivation": "动机在于提高多无人机系统的性能，通过更有效的优化方法减少优化时间。", "method": "采用多任务高斯过程捕捉各任务之间的共享结构，并比较单一任务优化和平均任务优化策略的效果。", "result": "结果显示单一任务优化随群体规模增加导致任务时间缩短，但消耗的优化时间更多；而平均任务优化则节省了总体优化时间。", "conclusion": "结论表明，对于多无人机系统中的分散轨迹生成问题，采用多任务贝叶斯优化能有效提高性能和效率。"}}
{"id": "2512.08629", "pdf": "https://arxiv.org/pdf/2512.08629", "abs": "https://arxiv.org/abs/2512.08629", "authors": ["Haoyu Zhao", "Weizhong Ding", "Yuhao Yang", "Zheng Tian", "Linyi Yang", "Kun Shao", "Jun Wang"], "title": "See-Control: A Multimodal Agent Framework for Smartphone Interaction with a Robotic Arm", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled their use as intelligent agents for smartphone operation. However, existing methods depend on the Android Debug Bridge (ADB) for data transmission and action execution, limiting their applicability to Android devices. In this work, we introduce the novel Embodied Smartphone Operation (ESO) task and present See-Control, a framework that enables smartphone operation via direct physical interaction with a low-DoF robotic arm, offering a platform-agnostic solution. See-Control comprises three key components: (1) an ESO benchmark with 155 tasks and corresponding evaluation metrics; (2) an MLLM-based embodied agent that generates robotic control commands without requiring ADB or system back-end access; and (3) a richly annotated dataset of operation episodes, offering valuable resources for future research. By bridging the gap between digital agents and the physical world, See-Control provides a concrete step toward enabling home robots to perform smartphone-dependent tasks in realistic environments.", "AI": {"tldr": "本文介绍了See-Control框架，该框架通过低自由度的机械臂实现智能手机操作任务。", "motivation": "现有的方法依赖于ADB进行数据传输和动作执行，限制了其仅适用于Android设备。因此，作者提出了一个平台无关的解决方案——见控框架，用于直接物理交互以控制智能手机。", "method": "见控框架包含三个关键组件：ESO基准测试（包括155项任务及评估指标）、基于MLLM的身体代理生成机器人控制命令以及操作事件的丰富注释数据集。", "result": "通过将数字代理与现实世界连接起来，本文提供了家庭机器人执行智能手机相关任务的有效步骤。", "conclusion": "见控框架提供了一种平台无关的方法来实现智能手机相关的机械臂操作，促进了未来的研究，并为真实环境中使用家庭机器人的可能性铺平了道路。"}}
{"id": "2512.08627", "pdf": "https://arxiv.org/pdf/2512.08627", "abs": "https://arxiv.org/abs/2512.08627", "authors": ["Tianchen Qiu", "Qirun Zhang", "Jiajian He", "Zhengyue Zhuge", "Jiahui Xu", "Yueting Chen"], "title": "Trajectory Densification and Depth from Perspective-based Blur", "categories": ["cs.CV"], "comment": null, "summary": "In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.", "AI": {"tldr": "通过分析视频流中的模糊模式和密集轨迹，利用光学设计算法估计深度图并增强稀疏轨迹。", "motivation": "在长时间曝光场景中，相机的旋转运动导致视角模糊。这种模糊与物体的空间位置有关，可以用来推断其距离信息。", "method": "使用现成的视觉编码器和点跟踪器提取视频流的信息。通过窗口嵌入和多窗口聚合估计深度图，并利用视觉语言模型增强来自光学算法的稀疏轨迹。", "result": "在多个深度数据集上的评估显示，该方法在较大范围内表现良好，具有良好的泛化能力。与手持拍摄场景中的真实轨迹相比，光学校正算法实现了更高的精度和密集重建保持了强大的准确性。", "conclusion": "提出了一种新颖的方法，通过分析视频流的模糊模式和稀疏轨迹来估计深度图，并利用视觉语言模型增强稀疏轨迹以实现准确、高效的密集重建。"}}
{"id": "2512.08625", "pdf": "https://arxiv.org/pdf/2512.08625", "abs": "https://arxiv.org/abs/2512.08625", "authors": ["Jisang Yoo", "Gyeongjin Kang", "Hyun-kyu Ko", "Hyeonwoo Yu", "Eunbyung Park"], "title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.", "AI": {"tldr": "本文提出了一种基于单目相机的SLAM框架OpenMonoGS-SLAM，该框架结合了三维高斯点云（3DGS）和开放集语义理解。", "motivation": "当前大多数SLAM与语义集成方法依赖于深度传感器或封闭集合模型，这限制了其在开放式环境中的适应性和可扩展性。本文旨在开发一种基于单目相机的SLAM系统，可以实现三维重建并具备开放词汇表的语义理解能力。", "method": "该研究利用视觉基础模型（VFMs）的技术进步，包括用于几何结构分析的MASt3R、用于开放式词汇表语义理解的SAM和CLIP，并通过自我监督学习目标构建了记忆机制来管理高维语义特征。", "result": "实验表明，在封闭集合与开放集合分割任务中，该方法性能达到了或超过了现有基准线水平。这证明了仅基于单目相机输入即可实现三维空间建图、定位及开放词汇表的语义理解的有效性。", "conclusion": "本文展示了一种新颖的方法来解决开放式环境下的SLAM与语义集成问题，利用视觉基础模型提供了强大的通用化性能，并且通过自我监督学习和记忆机制实现了高效的空间感知能力。"}}
{"id": "2512.08613", "pdf": "https://arxiv.org/pdf/2512.08613", "abs": "https://arxiv.org/abs/2512.08613", "authors": ["Manzi Kevin Maxime"], "title": "Protein Secondary Structure Prediction Using Transformers", "categories": ["cs.AI"], "comment": null, "summary": "Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.", "AI": {"tldr": "本文利用变压器模型预测蛋白质的二级结构，通过注意力机制处理氨基酸序列数据。", "motivation": "蛋白质二级结构对于理解其功能至关重要。使用先进的机器学习方法提高预测准确性。", "method": "采用一种基于Transformer的模型，并结合滑动窗口的数据增强技术来增加训练样本量。", "result": "该变压器模型展示了在不同长度序列上的泛化能力和捕捉局部及长程残基相互作用的能力。", "conclusion": "利用注意力机制和数据增强，提高了蛋白质二级结构预测的准确性。"}}
{"id": "2512.08609", "pdf": "https://arxiv.org/pdf/2512.08609", "abs": "https://arxiv.org/abs/2512.08609", "authors": ["Hui Wang", "Yang Liu", "Xiaoyu Zhang", "Chaoxu Mu"], "title": "CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.", "AI": {"tldr": "提出了一种新的基于大语言模型的认知引导蒙特卡洛树搜索框架(CogMCTS)，用于迭代启发式优化。", "motivation": "现有自动启发设计方法依赖于群体策略，易陷入局部最优。将大型语言模型与蒙特卡洛树搜索结合虽然改进了探索和利用的平衡，但多轮认知整合有限，搜索多样性受限。", "method": "通过引入多轮认知反馈、双轨节点扩展及精英启发式管理机制，CogMCTS框架动态地改善启发式的生成。此外，策略突变修改启发式的形式和参数以增强解决方案的多样性和优化性能。", "result": "实验结果表明，CogMCTS在稳定性和解决方案质量方面优于现有的基于大型语言模型的自动启发设计方法。", "conclusion": "提出的CogMCTS框架通过认知引导机制实现了高效的自动启发式优化，能够有效平衡多样化搜索和高质量经验利用。"}}
{"id": "2512.08607", "pdf": "https://arxiv.org/pdf/2512.08607", "abs": "https://arxiv.org/abs/2512.08607", "authors": ["Adrian Wiltz", "Dimos V. Dimarogonas"], "title": "Decoupled Design of Time-Varying Control Barrier Functions via Equivariances", "categories": ["eess.SY", "cs.RO"], "comment": "7 pages, 3 figures", "summary": "This article presents a systematic method for designing time-varying Control Barrier Functions (CBF) composed of a time-invariant component and multiple time-dependent components, leveraging structural properties of the system dynamics. The method involves the construction of a specific class of time-invariant CBFs that encode the system's dynamic capabilities with respect to a given constraint, and augments them subsequently with appropriately designed time-dependent transformations. While transformations uniformly varying the time-invariant CBF can be applied to arbitrary systems, transformations exploiting structural properties in the dynamics - equivariances in particular - enable the handling of a broader and more expressive class of time-varying constraints. The article shows how to leverage such properties in the design of time-varying CBFs. The proposed method decouples the design of time variations from the computationally expensive construction of the underlying CBFs, thereby providing a computationally attractive method to the design of time-varying CBFs. The method accounts for input constraints and under-actuation, and requires only qualitative knowledge on the time-variation of the constraints making it suitable to the application in uncertain environments.", "AI": {"tldr": "该论文提出了一种设计时间变化控制屏障函数（CBF）的方法，通过利用系统动态特性中的结构属性来分隔时间变化的设计和基础CBF的构建。", "motivation": "为了处理更广泛的时间变化约束类别，并且在不确定环境中保持计算效率。现有方法往往难以应对时间和约束变化，而该论文旨在提供一种解耦设计时间变化的方法。", "method": "通过构造特定类别的时不变控制屏障函数来编码给定约束下的系统动态能力，然后利用适当设计的时变变换进行扩充，从而处理具有更广泛和更具表达力的时间变化限制。", "result": "所提出的方法能够分隔时间变化的设计与基础CBF构建，并考虑输入约束和欠驱动情况。这种方法只需了解约束随时间的变化趋势即可。", "conclusion": "该方法通过利用动态特性中的结构属性，提供了设计时变控制屏障函数的一种新的有效途径，在不确定环境中具有广泛的适用性。"}}
{"id": "2512.08606", "pdf": "https://arxiv.org/pdf/2512.08606", "abs": "https://arxiv.org/abs/2512.08606", "authors": ["Zhenyu Zhang", "Guangyao Chen", "Yixiong Zou", "Zhimeng Huang", "Yuhua Li"], "title": "Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 8 figures, Association for the Advancement of Artificial Intelligence (AAAI2026, poster)", "summary": "The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of \"emptiness\" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.", "AI": {"tldr": "本文提出了一种使用空模板减少CLIP模型中由文本模板相似性导致的偏见的方法，以提高其在少样本学习任务中的准确性和鲁棒性。", "motivation": "研究表明，CLIP模型依赖于图像样本与模板之间的近似程度而非直接类别对齐，从而引入了偏差。这种偏差降低了分类精度和鲁棒性。因此，研究者希望通过使用空模板来减轻这一问题的影响。", "method": "提出了一种框架，该框架利用表示“空白”的文本输入，在预训练阶段通过空提示减少CLIP编码器中的模板偏见；在少样本微调阶段采用偏差校正损失以确保图像和类别之间的正确对齐。", "result": "实验结果表明，所提出的模板修正方法显著减少了由TSS引起的性能波动，并且提高了分类精度及鲁棒性。", "conclusion": "研究表明使用空提示可以有效减轻CLIP模型中的模板偏见问题，在多个基准测试中表现出更好的准确性和鲁棒性。"}}
{"id": "2512.08600", "pdf": "https://arxiv.org/pdf/2512.08600", "abs": "https://arxiv.org/abs/2512.08600", "authors": ["V. Arvind", "Srijan Chakraborty", "Samir Datta", "Asif Khan"], "title": "Fast exact algorithms via the Matrix Tree Theorem", "categories": ["cs.DS"], "comment": null, "summary": "Fast exact algorithms are known for Hamiltonian paths in undirected and directed bipartite graphs through elegant though involved algorithms that are quite different from each other. We devise algorithms that are simple and similar to each other while having the same upper bounds. The common features of these algorithms is the use of the Matrix-Tree theorem and sieving using roots of unity. Next, we use the framework to provide alternative algorithms to count perfect matchings in bipartite graphs on $n$ vertices, i.e., computing the $\\{0,1\\}$-permanent of a square $n/2 \\times n/2$ matrix which runs in a time similar to Ryser. We demonstrate the flexibility of our method by counting the number of ways to vertex partition the graph into $k$-stars (a $k$-star consist of a tree with a root having $k-1$ children that are all leaves). Interestingly, our running time improves to $O^*((1+ε_k)^n)$ with $ε_k \\rightarrow 0$ as $k \\rightarrow \\infty$. As an aside, making use of Björklund's algorithm for exact counting perfect matchings in general graphs, we show that the count of maximum matchings can be computed in time $O^*(2^ν)$ where $ν$ is the size of a maximum matching. The crucial ingredient here is the famous Gallai-Edmonds decomposition theorem. All our algorithms run in polynomial space.", "AI": {"tldr": "本文提出了一种通过Matrix Tree定理和单位根筛法来计算哈密顿路径、完美匹配以及图的顶点划分的新算法，这些算法具有相同的上界且更为简洁。", "motivation": "现有快速精确算法较为复杂且不同，作者希望通过统一的方法简化并优化这类问题的求解过程，以提供更加灵活多样的算法方案。", "method": "利用Matrix Tree定理和单位根筛法设计了一种新的框架来计算哈密顿路径、完美匹配以及图的顶点划分，并展示了该方法在最大匹配计数中的应用。", "result": "提出的新算法具有相同上界且更为简洁，运行时间得到了显著优化，在处理特定问题时效率更高。同时证明了这些算法在多项式空间内可实现。", "conclusion": "新提出的基于Matrix Tree定理和单位根筛法的框架为快速精确计算提供了一种灵活的方法，并展示了其广泛的应用前景。"}}
{"id": "2512.08596", "pdf": "https://arxiv.org/pdf/2512.08596", "abs": "https://arxiv.org/abs/2512.08596", "authors": ["Wicaksono Febriantoro", "Qi Zhou", "Wannapon Suraworachet", "Sahan Bulathwela", "Andrea Gauthier", "Eva Millan", "Mutlu Cukurova"], "title": "Examining Student Interactions with a Pedagogical AI-Assistant for Essay Writing and their Impact on Students Writing Quality", "categories": ["cs.CY", "cs.AI"], "comment": "17 pages, 3 Figures, 8 Tables", "summary": "The dynamic nature of interactions between students and GenAI, as well as their relationship to writing quality, remains underexplored. While most research has examined how general-purpose GenAI can support writing, fewer studies have investigated how students interact with pedagogically designed systems across different phases of the writing process. To address this gap, we evaluated a GenAI-driven essay-writing assistant (EWA) designed to support higher education students in argumentative writing. Drawing on 1,282 interaction logs from 32 undergraduates during a two-hour writing session, Sequential Pattern Mining and K-Means clustering were used to identify behavioral patterns. Two clusters emerged: Cluster 1 emphasized outline planning and essay structure, while Cluster 2 focused on content development. A Mann-Whitney U test revealed a moderate effect size (r = 0.36) in the essay Organization dimension, with Cluster 1 showing higher scores. Qualitative analysis indicated that students with better performance actively wrote and shared essay sections with EWA for feedback, rather than interacted passively by asking questions. These findings suggest implications for teaching and system design. Teachers can encourage active engagement, while future EWAs may integrate automatic labeling and monitoring to prompt students to move from questioning to writing, enabling fuller benefits from GenAI-supported learning.", "AI": {"tldr": "研究通过分析学生与AI辅助写作系统之间的交互行为，探究其对学生作文质量的影响。", "motivation": "当前关于学生如何与教育目的设计的AI系统进行互动以及这种互动对写作质量影响的研究较少。", "method": "使用序列模式挖掘和K-Means聚类技术来分析学生的互动日志，并通过Mann-Whitney U测试评估不同群组之间的差异性。", "result": "研究发现两组不同的学生行为模式，其中强调作文结构规划的学生在作文组织维度上得分较高。主动与AI进行写作互动而非提问的学生表现更好。", "conclusion": "教师应鼓励学生的积极参与，并且未来的AI辅助系统应该具备自动标注和监控功能以促使学生从被动问题转向主动写作。"}}
{"id": "2512.08592", "pdf": "https://arxiv.org/pdf/2512.08592", "abs": "https://arxiv.org/abs/2512.08592", "authors": ["Laxmiraju Kandikatla", "Branislav Radeljic"], "title": "The SMART+ Framework for AI Systems", "categories": ["cs.AI", "cs.CY", "cs.HC", "eess.SY"], "comment": null, "summary": "Artificial Intelligence (AI) systems are now an integral part of multiple industries. In clinical research, AI supports automated adverse event detection in clinical trials, patient eligibility screening for protocol enrollment, and data quality validation. Beyond healthcare, AI is transforming finance through real-time fraud detection, automated loan risk assessment, and algorithmic decision-making. Similarly, in manufacturing, AI enables predictive maintenance to reduce equipment downtime, enhances quality control through computer-vision inspection, and optimizes production workflows using real-time operational data. While these technologies enhance operational efficiency, they introduce new challenges regarding safety, accountability, and regulatory compliance. To address these concerns, we introduce the SMART+ Framework - a structured model built on the pillars of Safety, Monitoring, Accountability, Reliability, and Transparency, and further enhanced with Privacy & Security, Data Governance, Fairness & Bias, and Guardrails. SMART+ offers a practical, comprehensive approach to evaluating and governing AI systems across industries. This framework aligns with evolving mechanisms and regulatory guidance to integrate operational safeguards, oversight procedures, and strengthened privacy and governance controls. SMART+ demonstrates risk mitigation, trust-building, and compliance readiness. By enabling responsible AI adoption and ensuring auditability, SMART+ provides a robust foundation for effective AI governance in clinical research.", "AI": {"tldr": "SMART+框架用于评估和管理人工智能系统，解决安全、问责制等问题。", "motivation": "为了应对AI系统引入的安全性、可追溯性和合规性挑战，提出了一种全面的方法来指导各行业的AI治理。", "method": "提出了基于安全性、监控、责任、可靠性、透明度等支柱的SMART+框架，并增强了隐私与安全、数据治理等方面的控制措施。", "result": "展示了风险缓解策略和合规准备能力，为临床研究中的AI应用提供了审计性保障。", "conclusion": "SMART+框架提供了一种实用且全面的方法来评估和管理AI系统，在确保责任性和合规性的基础上促进人工智能的负责任使用。"}}
{"id": "2512.08591", "pdf": "https://arxiv.org/pdf/2512.08591", "abs": "https://arxiv.org/abs/2512.08591", "authors": ["Charles Rios", "Longzhen Han", "Almas Baimagambetov", "Nikolaos Polatidis"], "title": "Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.", "AI": {"tldr": "使用长序列LSTM模型预测NBA比赛结果，利用新构建的多赛季数据集。", "motivation": "为了提高篮球比赛结果预测的准确性，解决现有模型在概念漂移、时间上下文有限和跨季节稳定性不足的问题，提出了一个新的纵向NBA数据集以及深度学习框架。", "method": "采用长短期记忆网络(LSTM)架构，利用9840场比赛的数据来捕捉球队动态和赛季间依赖关系，与传统的机器学习和深度学习方法相比，包括逻辑回归、随机森林、多层感知机(MLP)和卷积神经网络(CNN)。", "result": "LSTM模型在所有指标上都表现出最佳性能，准确率为72.35%，精确率为73.15%，AUC-ROC为76.13%。", "conclusion": "长序列时间建模对于篮球比赛结果预测至关重要，并且新的多赛季数据集对开发健壮、通用的NBA预测系统具有价值。"}}
{"id": "2512.08589", "pdf": "https://arxiv.org/pdf/2512.08589", "abs": "https://arxiv.org/abs/2512.08589", "authors": ["Swarn Singh Warshaneyan", "Maksims Ivanovs", "Blaž Cugmas", "Inese Bērziņa", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadiķis"], "title": "Automated Pollen Recognition in Optical and Holographic Microscopy Images", "categories": ["cs.CV"], "comment": "08 pages, 10 figures, 04 tables, 20 references. Date of Conference: 13-14 June 2025 Date Added to IEEE Xplore: 10 July 2025 Electronic ISBN: 979-8-3315-0969-9 Print on Demand(PoD) ISBN: 979-8-3315-0970-5 DOI: 10.1109/AICCONF64766.2025.11064260 Conference Location: Prague, Czech Republic Online Access: https://ieeexplore.ieee.org/document/11064260", "summary": "This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.", "AI": {"tldr": "该论文探讨了深度学习在光学和全息显微镜图像中自动识别花粉粒的应用。", "motivation": "研究旨在提高和自动化花粉颗粒的检测与分类，特别是在兽医细胞学案例中的应用。通过使用YOLOv8s进行目标检测和MobileNetV3L进行分类任务，评估其在不同成像模式下的性能，并解决全息图像性能差距问题。", "method": "研究中采用了YOLOv8s用于物体检测以及MobileNetV3L完成分类任务。为了解决全息图像的性能差异，通过自动标签扩展和边界框区域放大技术进行了数据集扩充。", "result": "模型在光学图像中的mAP50检测率为91.3%，整体分类准确率为97%；而在灰度全息图中，初始检测率低至2.49%，分类准确率为42%。应用上述方法后，全息图像的检测性能提升至13.3%mAP50，分类准确性提高到54%。", "conclusion": "研究显示深度学习技术可与低成本无透镜数字全息显微成像设备结合使用以完成至少是图象分类任务。"}}
{"id": "2512.08583", "pdf": "https://arxiv.org/pdf/2512.08583", "abs": "https://arxiv.org/abs/2512.08583", "authors": ["Jesper Nederlof"], "title": "Weighted $k$-Path and Other Problems in Almost $O^*(2^k)$ Deterministic Time via Dynamic Representative Sets", "categories": ["cs.DS"], "comment": "22 pages, to appear at FOCS 2025 (online video available at FOCS youtube channel)", "summary": "We present a data structure that we call a Dynamic Representative Set. In its most basic form, it is given two parameters $0< k < n$ and allows us to maintain a representation of a family $\\mathcal{F}$ of subsets of $\\{1,\\ldots,n\\}$. It supports basic update operations (unioning of two families, element convolution) and a query operation that determines for a set $B \\subseteq \\{1,\\ldots,n\\}$ whether there is a set $A \\in \\mathcal{F}$ of size at most $k-|B|$ such that $A$ and $B$ are disjoint. After $2^{k+O(\\sqrt{k}\\log^2k)}n \\log n$ preprocessing time, all operations use $2^{k+O(\\sqrt{k}\\log^2k)}\\log n$ time. Our data structure has many algorithmic consequences that improve over previous works. One application is a deterministic algorithm for the Weighted Directed $k$-Path problem, one of the central problems in parameterized complexity. Our algorithm takes as input an $n$-vertex directed graph $G=(V,E)$ with edge lengths and an integer $k$, and it outputs the minimum edge length of a path on $k$ vertices in $2^{k+O(\\sqrt{k}\\log^2k)}(n+m)\\log n$ time (in the word RAM model where weights fit into a single word). Modulo the lower order term $2^{O(\\sqrt{k}\\log^2k)}$, this answers a question that has been repeatedly posed as a major open problem in the field.", "AI": {"tldr": "本文提出了一种名为动态代表集的数据结构，用于解决参数化复杂性中的加权有向k路径问题等任务。", "motivation": "通过引入一种新的数据结构来优化算法的效率和时间复杂度，并为解决经典的加权有向k路径问题提供高效的方法。", "method": "提出了一种名为动态代表集的数据结构，支持基本更新操作（两个家族的并集、元素卷积）以及查询操作。经过预处理后，可以实现快速的操作时间和空间使用。", "result": "对于加权有向k路径问题提出了一个确定性算法，在时间复杂度上实现了显著提升。", "conclusion": "动态代表集为解决参数化复杂性的各种问题提供了一种有效的方法，并解决了该领域内一个长期存在的开放问题。"}}
{"id": "2512.08580", "pdf": "https://arxiv.org/pdf/2512.08580", "abs": "https://arxiv.org/abs/2512.08580", "authors": ["Peijun Tang", "Shangjin Xie", "Binyan Sun", "Baifu Huang", "Kuncheng Luo", "Haotian Yang", "Weiqi Jin", "Jianan Wang"], "title": "Mind to Hand: Purposeful Robotic Control via Embodied Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "49 pages, 25 figures", "summary": "Humans act with context and intention, with reasoning playing a central role. While internet-scale data has enabled broad reasoning capabilities in AI systems, grounding these abilities in physical action remains a major challenge. We introduce Lumo-1, a generalist vision-language-action (VLA) model that unifies robot reasoning (\"mind\") with robot action (\"hand\"). Our approach builds upon the general multi-modal reasoning capabilities of pre-trained vision-language models (VLMs), progressively extending them to embodied reasoning and action prediction, and ultimately towards structured reasoning and reasoning-action alignment. This results in a three-stage pre-training pipeline: (1) Continued VLM pre-training on curated vision-language data to enhance embodied reasoning skills such as planning, spatial understanding, and trajectory prediction; (2) Co-training on cross-embodiment robot data alongside vision-language data; and (3) Action training with reasoning process on trajectories collected on Astribot S1, a bimanual mobile manipulator with human-like dexterity and agility. Finally, we integrate reinforcement learning to further refine reasoning-action consistency and close the loop between semantic inference and motor control. Extensive experiments demonstrate that Lumo-1 achieves significant performance improvements in embodied vision-language reasoning, a critical component for generalist robotic control. Real-world evaluations further show that Lumo-1 surpasses strong baselines across a wide range of challenging robotic tasks, with strong generalization to novel objects and environments, excelling particularly in long-horizon tasks and responding to human-natural instructions that require reasoning over strategy, concepts and space.", "AI": {"tldr": "介绍了一种将机器人推理和行动结合的模型Lumo-1，用于增强机器人在物理环境中的行为能力。", "motivation": "现有的AI系统虽然能够进行广泛的多模态推理，但在将这些推理转化为实际的物理动作方面仍面临挑战。本文旨在通过引入一种新的视觉语言行动（VLA）模型来解决这一问题。", "method": "该方法基于预训练的视觉语言模型(VLM)，并通过三阶段的预训练管道进一步扩展其功能：1) 在经过精心策划的多模态数据上继续进行VLM的预训练，以提高诸如计划、空间理解和轨迹预测等实体推理技能；2) 同时在跨实体机器人数据和视觉语言数据上进行协同训练；3) 通过收集于Astribot S1上的行动轨迹对模型进行行动训练，并引入强化学习进一步优化推理与动作的一致性。", "result": "实验结果表明，Lumo-1在增强型的视觉语言推理方面取得了显著的进步。实际评估显示，在广泛的机器人任务中，特别是在需要策略、概念和空间理解的长周期任务及人类自然指令上，Lumo-1的表现优于基准模型，并能很好地泛化到新物体和环境。", "conclusion": "通过将多模态推理与动作控制相结合，Lumo-1展示了一种新的机器人控制方法，能够更有效地执行复杂且具有挑战性的任务。"}}
{"id": "2512.08577", "pdf": "https://arxiv.org/pdf/2512.08577", "abs": "https://arxiv.org/abs/2512.08577", "authors": ["Yuna Kato", "Shohei Mori", "Hideo Saito", "Yoshifumi Takatsume", "Hiroki Kajita", "Mariko Isogawa"], "title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.", "AI": {"tldr": "本论文提出了一个自动化的方法，通过多相机无影灯系统生成无障碍的手术视频。", "motivation": "在开放性手术中，由于医生频繁阻挡摄像机视野，导致拍摄到的视频容易出现遮挡。现有的方法虽然使用多个摄像头来增加捕捉清晰画面的概率，但在每次调整灯光时需要手动对齐图像，因此提出了自动化对齐任务的方法以提高效率。", "method": "论文提出了一种新的方法，通过识别照明系统移动的画面进行自动重定位，并选择遮挡最少的摄像机生成视频，从而从固定视角一致地展示手术区域。此外还实现了几种合成选项并进行了用户研究来评估外科医生对每一种方案的偏好。", "result": "实验表明，由本论文方法产生的视频比传统方法更容易确认手术区域并且观看更加舒适。在视频质量方面也显示出改进。", "conclusion": "通过自动化多相机无影灯系统生成无障碍手术视频的方法成功减少了手动图像对齐的工作量，并提高了视频质量和使用体验，为教育和研究提供了更好的资源支持"}}
{"id": "2512.08574", "pdf": "https://arxiv.org/pdf/2512.08574", "abs": "https://arxiv.org/abs/2512.08574", "authors": ["Vit Kratky", "Robert Penicka", "Parakh M. Gupta", "Ondrej Prochazka", "Martin Saska"], "title": "RVC-NMPC: Nonlinear Model Predictive Control with Reciprocal Velocity Constraints for Mutual Collision Avoidance in Agile UAV Flight", "categories": ["cs.RO"], "comment": "8 pages, 8 figures", "summary": "This paper presents an approach to mutual collision avoidance based on Nonlinear Model Predictive Control (NMPC) with time-dependent Reciprocal Velocity Constraints (RVCs). Unlike most existing methods, the proposed approach relies solely on observable information about other robots, eliminating the necessity of excessive communication use. The computationally efficient algorithm for computing RVCs, together with the direct integration of these constraints into NMPC problem formulation on a controller level, allows the whole pipeline to run at 100 Hz. This high processing rate, combined with modeled nonlinear dynamics of the controlled Uncrewed Aerial Vehicles (UAVs), is a key feature that facilitates the use of the proposed approach for an agile UAV flight. The proposed approach was evaluated through extensive simulations emulating real-world conditions in scenarios involving up to 10 UAVs and velocities of up to 25 m/s, and in real-world experiments with accelerations up to 30 m/s$^2$. Comparison with state of the art shows 31% improvement in terms of flight time reduction in challenging scenarios, while maintaining a collision-free navigation in all trials.", "AI": {"tldr": "提出了一种基于非线性模型预测控制（NMPC）和时间相关的互惠速度约束（RVCs）的多无人机碰撞避免方法，以实现高效且敏捷的安全飞行。", "motivation": "为了减少多机器人系统中的通信需求，并提高在高速度、高加速度条件下的安全性与效率，提出了一种新的基于可观察信息而非复杂通信网络的方法来解决相互避碰问题。", "method": "通过引入一种计算RVCs的高效算法并将这些约束直接集成到NMPC问题中，在控制器层面实现了每秒100次处理频率，同时利用了UAV的非线性动力学模型。", "result": "在模拟和实际试验环境中展示了该方法的有效性，特别是在具有挑战性的场景下减少了31%的飞行时间，并确保所有测试中无碰撞发生。", "conclusion": "所提出的方法通过减少通信负担并利用高效的计算技术，在保持安全性的同时提高了多无人机系统的灵活性与效率。"}}
{"id": "2512.08572", "pdf": "https://arxiv.org/pdf/2512.08572", "abs": "https://arxiv.org/abs/2512.08572", "authors": ["Olle Edgren Schüllerqvist", "Jens Baumann", "Joakim Lindblad", "Love Nordling", "Artur Mezheyeuski", "Patrick Micke", "Nataša Sladoje"], "title": "From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis", "categories": ["cs.CV"], "comment": "5 pages, 3 figures", "summary": "The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.", "AI": {"tldr": "通过多层次显微镜图像分析肿瘤微环境中的细胞相互作用，预测肺癌患者的生存情况。", "motivation": "为了充分利用肿瘤微环境作为预后生物标志物的潜力，需要开发能够捕捉不同细胞类型之间复杂互动的方法。", "method": "提出了一种基于图的分层方法（HiGINE），该方法能编码局部和全局的细胞相互作用，并结合形态特征提高预测准确性。进一步融合癌症分期信息与显微图像数据以增强模型性能。", "result": "在两个公开的数据集上验证了所提模型的有效性，显示了风险分类、鲁棒性和泛化能力的显著改善。", "conclusion": "HiGINE方法有效提升了对肺癌患者生存情况预测的精度和可靠性，在临床应用中具有潜在价值。"}}
{"id": "2512.08569", "pdf": "https://arxiv.org/pdf/2512.08569", "abs": "https://arxiv.org/abs/2512.08569", "authors": ["Seunghwan Lee", "Inyoung Jung", "Hojoon Lee", "Eunil Park", "Sungeun Hong"], "title": "Instance-Aware Test-Time Segmentation for Continual Domain Shifts", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.", "AI": {"tldr": "该论文提出了一种针对持续领域偏移的测试时间分割方法，通过自适应调整伪标签来提高模型鲁棒性。", "motivation": "现有CTTA方法依赖固定或批次级阈值，无法应对不同类和实例之间的难度变化问题。在语义分割中，这会导致累积误差和错误传播。", "method": "提出了一种动态平衡学习的方法，能够根据每张图像内的置信分布自适应调整伪标签，并针对受域偏移影响最大的类别进行细化的、按类别的和实例级别的适应。", "result": "该方法在八种CTTA和TTA场景中展示了优越性能，包括合成到现实以及长期偏移的情况，确立了语义分割领域的新标准。", "conclusion": "提出的方法通过调整伪标签来应对持续的域偏移问题，并且在多种实验环境下验证了其有效性。"}}
{"id": "2512.08567", "pdf": "https://arxiv.org/pdf/2512.08567", "abs": "https://arxiv.org/abs/2512.08567", "authors": ["Nader Sadek", "Mirette Moawad", "Christina Naguib", "Mariam Elzahaby"], "title": "A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 6 figures. Published in the Proceedings of the 5th International Conference on Artificial Intelligence Research (ICAIR 2025). Published version available at: https://papers.academic-conferences.org/index.php/icair/article/view/4294", "summary": "Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.", "AI": {"tldr": "本文提出了一种结合新闻情感和时间序列数据的混合模型，用于股票市场预测。", "motivation": "传统的股票市场预测模型主要依赖历史价格数据。然而，研究表明金融新闻提供了有用的外部信号，因此研究者希望整合这些信息以提高预测性能。", "method": "该方法利用LSTM编码公司历史数据，并使用语言模型嵌入新闻标题。这些嵌入形成异构图中的节点，通过GraphSAGE捕捉文章、公司和行业之间的交互关系。", "result": "实验结果表明，GNN模型在两个目标上优于LSTM基线：对于第一个目标，准确率达到了53%，第二个目标的精度提高了4%；同时发现，新闻较多的公司在预测中表现更好。", "conclusion": "研究表明，通过整合股票历史数据和相关新闻标题可以提高市场预测性能，特别是在短期市场反应方面。"}}
{"id": "2512.08564", "pdf": "https://arxiv.org/pdf/2512.08564", "abs": "https://arxiv.org/abs/2512.08564", "authors": ["Mahmoud Afifi", "Zhongling Wang", "Ran Zhang", "Michael S. Brown"], "title": "Modular Neural Image Signal Processing", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM", "AI": {"tldr": "本文提出了一种模块化神经图像信号处理框架，该框架通过对中间阶段的控制提高渲染精度、可扩展性、调试能力和适应不同用户偏好风格的能力。", "motivation": "与先前的设计相比，现有的神经ISP设计缺乏对中间过程的精细控制，导致了在准确性、可扩展性和适应未见过相机方面的不足。本文旨在通过引入模块化结构来解决这些问题，并展示其优点。", "method": "该方法采用了一种高度模块化的神经网络框架，实现了对图像渲染过程中多个阶段的高度可控性，并支持用户互动的照片编辑功能。整个系统具有不同容量的变体（参数从约0.5M到3.9M不等），展示了良好的性能表现。", "result": "实验结果表明该方法在多种测试集上均表现出色，无论是定性还是定量评估都与当前最好的解决方案相媲美。", "conclusion": "通过构建一个用户交互式的照片编辑工具，证明了模块化神经ISP框架的优越性和实用性。"}}
{"id": "2512.08560", "pdf": "https://arxiv.org/pdf/2512.08560", "abs": "https://arxiv.org/abs/2512.08560", "authors": ["Navve Wasserman", "Matias Cosarinsky", "Yuval Golbari", "Aude Oliva", "Antonio Torralba", "Tamar Rott Shaham", "Michal Irani"], "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "categories": ["cs.CV"], "comment": null, "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "AI": {"tldr": "本文提出了一种大规模自动框架，用于发现和解释人类大脑皮层中可解释的视觉表示。", "motivation": "理解人脑如何表示视觉概念以及这些表示在哪些大脑区域编码一直是长期挑战。现有的研究通常规模较小，依赖手动检查，并且很少进行系统验证。", "method": "该方法包含两个主要阶段：首先通过无监督的数据驱动分解方法发现fMRI活动中的候选可解释模式；然后识别最强烈激发每个模式的自然图像并生成其视觉含义的自然语言描述。引入自动管道以测试多个候选解释，分配定量可靠性分数，并选择每种体素模式的最佳一致描述。", "result": "该框架揭示了数千个跨越多种不同视觉概念的可解释模式，包括以前未报道的细粒度表示。", "conclusion": "通过大规模、自动化的方法系统地发现和验证大脑中的视觉表示，为理解人类大脑如何处理视觉信息提供了新的视角。"}}
{"id": "2512.08557", "pdf": "https://arxiv.org/pdf/2512.08557", "abs": "https://arxiv.org/abs/2512.08557", "authors": ["Alexander Dow", "Manduhu Manduhu", "Matheus Santos", "Ben Bartlett", "Gerard Dooly", "James Riordan"], "title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds", "categories": ["cs.CV"], "comment": "22 Pages, 26 Figures, This work has been submitted to the IEEE Sensors Journal for possible publication", "summary": "This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.", "AI": {"tldr": "本文提出了一种基于散射卷积的实时三维目标检测算法SSCATeR，通过利用LiDAR扫描中的时间数据回收来减少计算量。", "motivation": "通过对连续帧之间的点云变化进行关注，减少处理未改变区域所需的计算操作数量，同时保持准确性。", "method": "使用滑动时间窗口和短步长，在每次传递之间存储卷积结果以利用时间维度，并通过数据重用来引入极端稀疏性。提出了SSCATeR算法来处理连续的LiDAR数据流，并仅作用于点云中变化的部分。", "result": "与传统稀疏卷积技术相比，该方法能够在特征图相同的情况下提高计算效率，达到最大6.61倍的处理时间减少。", "conclusion": "通过利用时间和空间上的稀疏性，SSCATeR算法实现了高效且准确的实时三维目标检测。"}}
{"id": "2512.08551", "pdf": "https://arxiv.org/pdf/2512.08551", "abs": "https://arxiv.org/abs/2512.08551", "authors": ["Kai Marquardt", "Mona Schulz", "Anne Koziolek", "Lucia Happe"], "title": "Gamification with Purpose: What Learners Prefer to Motivate Their Learning", "categories": ["cs.SE", "cs.CY", "cs.HC", "cs.MM"], "comment": "31 pages, 10 figures, Springer EAIT in review", "summary": "This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.", "AI": {"tldr": "研究探讨了学习者在教育环境中对游戏设计元素的偏好，以指导目的驱动型游戏化策略的发展。", "motivation": "论文旨在通过了解学习者的偏好来制定符合教学目标的游戏化策略，并减少内在动机削弱的风险。", "method": "进行了系统文献回顾并确定了十个广泛讨论的游戏设计元素。开发了每个元素的视觉原型，进行了一项125名参与者的最佳最差缩放调查以获取偏好排名，并收集定性反馈。", "result": "学习者更喜欢直接支持学习过程的游戏设计元素，如进度条、概念图、即时反馈和成就系统。定性分析揭示了六个动机主题，包括可见的进展、内容相关性和建设性反馈。", "conclusion": "研究建议游戏化应优先考虑能可视化学习进程并提供操作性反馈的工具，而不仅仅是依赖外部激励。"}}
{"id": "2512.08548", "pdf": "https://arxiv.org/pdf/2512.08548", "abs": "https://arxiv.org/abs/2512.08548", "authors": ["Yuchi Zhang", "Churui Sun", "Shiqi Liang", "Diyuan Liu", "Chao Ji", "Wei-Nan Zhang", "Ting Liu"], "title": "Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent end-to-end robotic manipulation research increasingly adopts architectures inspired by large language models to enable robust manipulation. However, a critical challenge arises from severe distribution shifts between robotic action data, primarily due to substantial numerical variations in action commands across diverse robotic platforms and tasks, hindering the effective transfer of pretrained knowledge. To address this limitation, we propose a semantically grounded linguistic representation to normalize actions for efficient pretraining. Unlike conventional discretized action representations that are sensitive to numerical scales, the motion representation specifically disregards numeric scale effects, emphasizing directionality instead. This abstraction mitigates distribution shifts, yielding a more generalizable pretraining representation. Moreover, using the motion representation narrows the feature distance between action tokens and standard vocabulary tokens, mitigating modality gaps. Multi-task experiments on two benchmarks demonstrate that the proposed method significantly improves generalization performance and transferability in robotic manipulation tasks.", "AI": {"tldr": "本文提出了通过语言基动作表示来解决机器人控制中的尺度差异问题，以提高预训练的泛化性和可转移性。", "motivation": "当前基于大语言模型的端到端机器人操控研究面临着由于不同平台和任务间操作命令数字变化导致的数据分布偏差，这阻碍了预先学习知识的有效迁移。为了解决这一限制，本文提出了一种语义基础的语言动作表示方法来规范化动作，从而提高预训练效果。", "method": "该论文采用一种基于方向而非数值尺度的动作抽象表示法，这种表示不敏感于具体的数字比例差异，重点在于方向性。通过这种方式可以减少分布偏差，并缩小动作令牌与标准词汇令牌之间的特征距离，以弥合模态差距。", "result": "在两个基准测试上的多任务实验表明，所提出的方法显著提高了机器人操作任务中的泛化性能和可转移性。", "conclusion": "本文展示了基于语言的动作表示如何通过解决尺度差异问题来提高机器人的预训练效果，从而增强其在不同环境下的表现。"}}
{"id": "2512.08547", "pdf": "https://arxiv.org/pdf/2512.08547", "abs": "https://arxiv.org/abs/2512.08547", "authors": ["Yifei Chen", "Kaiyu Song", "Yan Pan", "Jianxing Yu", "Jian Yin", "Hanjiang Lai"], "title": "An Iteration-Free Fixed-Point Estimator for Diffusion Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.", "AI": {"tldr": "提出了一种无需迭代的固定点估计方法来解决扩散逆问题。", "motivation": "传统的固定点迭代法由于其迭代性质导致计算成本高且超参数选择复杂，因此引入一种新的非迭代式方法以降低成本并提高效率。", "method": "通过导出理想反转步骤中的固定点显式表达式，并利用前一步骤的可计算误差来近似当前步骤的未知误差，从而得到一个低方差无偏估计器。", "result": "在NOCAPS和MS-COCO数据集上进行评估时，该方法相比DDIM逆向和其他基于固定点迭代的方法，在重建任务中表现更优且无需额外迭代或训练。", "conclusion": "提出的非迭代式固定点估计器能够在减少计算成本的同时保持甚至提升重建性能。"}}
{"id": "2512.08545", "pdf": "https://arxiv.org/pdf/2512.08545", "abs": "https://arxiv.org/abs/2512.08545", "authors": ["Indrajit Kar", "Kalathur Chenchu Kishore Kumar"], "title": "Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.MA"], "comment": "22 pages, 2 tables, 9 figures", "summary": "Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.", "AI": {"tldr": "介绍了一种分层多智能体架构，用于解决长期任务。", "motivation": "大型语言模型和多智能体系统在分解复杂任务方面表现出色，但在长时序推理任务中面临计算成本上升的问题。本文旨在通过改进的分布式结构提高其可靠性和效率。", "method": "采用64*64网格分布轻量级代理，结合选择性Oracle。空间课程逐步扩展操作区域，确保智能体先处理容易的任务再解决困难的问题。使用负对数似然作为置信度指标，由Thompson Sampling课程管理器基于能力和奖励信号进行适应性训练区选择。", "result": "在空间基座的Tower of Hanoi基准测试中，该方法提高了稳定性，减少了Oracle的使用，并增强了分布式代理合作中的长时推理能力。", "conclusion": "实验表明，提出的架构能够更稳定地处理长期任务，减少对中央计算资源的需求，并提高多智能体系统的整体性能。"}}
{"id": "2512.08542", "pdf": "https://arxiv.org/pdf/2512.08542", "abs": "https://arxiv.org/abs/2512.08542", "authors": ["Zhigang Jia", "Duan Wang", "Hengkai Wang", "Yajun Xie", "Meixiang Zhao", "Xiaoyu Zhao"], "title": "A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation", "categories": ["cs.CV", "cs.AI", "math.NA"], "comment": null, "summary": "Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.", "AI": {"tldr": "提出一种新的沃瑟斯坦四元数生成对抗网络以解决彩色图像生成中的色彩通道相关性问题和数据分布问题。", "motivation": "现有生成模型忽视了颜色通道之间的关联，可能导致色差问题；缺乏关于不同颜色图像数据集测量的理论体系。", "method": "定义了一种新的四元数沃瑟斯坦距离，并发展其对偶理论；通过引入四元数凸集合分离定理和四元数Farkas引理解决四元线性规划问题，提出基于四元数沃瑟斯坦距离的新模型。", "result": "实验显示该新模型在生成效率和图像质量上优于（四元）生成对抗网络及沃瑟斯坦生成对抗网络。", "conclusion": "通过引入新的理论和技术框架，解决了彩色图像生成中存在的关键问题，提高了模型性能。"}}
{"id": "2512.08541", "pdf": "https://arxiv.org/pdf/2512.08541", "abs": "https://arxiv.org/abs/2512.08541", "authors": ["Nils Gehrke", "David Brecht", "Dominik Kulmer", "Dheer Patel", "Frank Diermeyer"], "title": "vEDGAR - Can CARLA Do HiL?", "categories": ["cs.RO"], "comment": null, "summary": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR", "AI": {"tldr": "本文的任务是创建一个仿真框架，用于在硬件上测试自动驾驶软件的功能，并确定其极限。", "motivation": "当前的CARLA模拟器缺少对带有完整传感器和执行机构栈的研究及自动车辆进行实时仿真的评价。因此，此研究旨在提升开发流程中的自动化驾驶功能开源工作流。", "method": "首先定义需求并制定仿真架构，然后基于这些要求实施vEDGAR软件，并评估其有效性。", "result": "通过评估提出的vEDGAR软件，得出CARLA进行硬件在环测试的适用性的结论。", "conclusion": "根据评估结果，最终确定了CARLA作为自动驾驶功能开发全过程中的评估工具的有效性。"}}
{"id": "2512.08537", "pdf": "https://arxiv.org/pdf/2512.08537", "abs": "https://arxiv.org/abs/2512.08537", "authors": ["Zhen Zou", "Xiaoxiao Ma", "Jie Huang", "Zichao Yu", "Feng Zhao"], "title": "Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\\times$256, TransDiff attains 4.3$\\times$ lossless speedup, and NextStep-1 achieves 3$\\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.", "AI": {"tldr": "Fast-ARDiff是一个结合了自回归（AR）和扩散模型优点的框架，旨在加速连续空间中的生成任务。", "motivation": "自回归与扩散混合模式虽然提供了结构化建模和逼真的合成能力，但它们由于自回归生成的顺序性和迭代去噪过程而存在高延迟问题。为此，提出了Fast-ARDiff以解决这一瓶颈。", "method": "该框架通过熵感知策略优化了自回归模型的推测性解码，并将扩散模块整合到统一框架中使用动态调度器优先考虑自回归优化来指导后续步骤。同时利用浅层特征熵进行预过滤减少冗余计算。", "result": "Fast-ARDiff在多个模型上实现了最先进的加速效果：TransDiff在ImageNet 256×256数据集上的无损速度提升达到4.3倍，NextStep-1在文本条件生成任务中实现3倍的加速。", "conclusion": "通过结合熵感知策略和动态调度器，Fast-ARDiff显著提高了AR-扩散混合模型的速度，同时保持了高质量的合成效果。"}}
{"id": "2512.08536", "pdf": "https://arxiv.org/pdf/2512.08536", "abs": "https://arxiv.org/abs/2512.08536", "authors": ["Tammy Zhong", "Yang Song", "Maurice Pagnucco"], "title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans", "categories": ["cs.AI"], "comment": "Accepted by AAAI 2026", "summary": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.", "AI": {"tldr": "开发了一种名为Principles2Plan的系统，利用大型语言模型（LLM）协助人类专家制定符合伦理原则的操作规则，并生成相应的行动计划。", "motivation": "现有的自动化规划工具在机器人应用于人机共存环境中时无法提供足够的伦理支持，人工指定伦理规则既耗时又依赖具体情境。因此开发了一个原型来展示如何通过人与大型语言模型的合作来克服这些问题。", "method": "用户为系统提供规划领域、问题详情以及高层次的道德原则，如有益性和隐私保护。系统生成符合这些原则的操作规则，并由用户审查和优先排序后提交给规划器以制定伦理导向的计划。", "result": "Principles2Plan是一个交互式研究原型，展示了大型语言模型在辅助制定基于原则的操作规则方面的潜力，特别是在经典规划环境中。这是首个支持用户生成具有道德依据规则的系统。", "conclusion": "通过人与LLM的合作，使得自动化伦理规划变得更加实际可行。"}}
{"id": "2512.08535", "pdf": "https://arxiv.org/pdf/2512.08535", "abs": "https://arxiv.org/abs/2512.08535", "authors": ["Xinyue Liang", "Zhinyuan Ma", "Lingchen Sun", "Yanjun Guo", "Lei Zhang"], "title": "Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.", "AI": {"tldr": "Photo3D是一个通过图像数据增强真实感三维生成的框架，旨在解决当前三维生成器在纹理细节上的不足。", "motivation": "尽管现有的三维生成器已经在几何结构上取得了显著进步，但在纹理细节的真实感方面仍存在明显局限。主要问题在于缺乏多样且高质量、带有丰富纹理细节的现实世界三维资源。", "method": "Photo3D利用由GPT-4o-Image模型生成的图像数据，并设计了一种基于多视图合成管线的结构对齐方法，构建了与三维几何配对的增强细节多视图数据集。在此基础上提出了一个真实感细节增强方案，通过感知特征适应和语义结构匹配来保持与3D本机几何的一致性。", "result": "实验表明Photo3D在不同的3D生成范式中具有良好的泛化性能，并且实现了最先进的三维真实感生成效果。", "conclusion": "Photo3D成功地通过图像数据增强了真实感三维生成，展示了其在结构一致性和纹理细节上的优越性。"}}
{"id": "2512.08534", "pdf": "https://arxiv.org/pdf/2512.08534", "abs": "https://arxiv.org/abs/2512.08534", "authors": ["Zhangli Hu", "Ye Chen", "Jiajun Yao", "Bingbing Ni"], "title": "PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.", "AI": {"tldr": "本文介绍了一种用于油画生成和编辑的统一多模态框架PaintFlow。", "motivation": "现有的生成与编辑技术受限于训练数据分布，主要关注对真实照片的修改。因此，论文旨在解决油画数字生成及编辑中的复杂笔触动态和风格化特征问题。", "method": "提出了一种增强训练阶段的方法，利用空间对齐和语义增强策略来实现对象级别的语义对齐；通过基于SBR的自我监督样式传输管道克服数据稀缺性，并在推理过程中使用AdaIN操作符确保一致的风格。", "result": "实验表明该系统实现了精细编辑并保持了油画的艺术品质，达到了前所未有的风格化油画生成和编辑水平。", "conclusion": "本文提出的PaintFlow框架能够在保留油画艺术特质的情况下进行交互式编辑与创作。"}}
{"id": "2512.08529", "pdf": "https://arxiv.org/pdf/2512.08529", "abs": "https://arxiv.org/abs/2512.08529", "authors": ["Yunzhu Zhang", "Zeyu Pan", "Zhengwen Zeng", "Shuheng Shen", "Changhua Meng", "Linchao Zhu"], "title": "MVP: Multiple View Prediction Improves GUI Grounding", "categories": ["cs.CV"], "comment": null, "summary": "GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.", "AI": {"tldr": "该论文提出了多视图预测（MVP）框架，用于改进GUI接地任务中自然语言指令到精确像素坐标的转换。", "motivation": "现有的接地模型存在显著的坐标预测不稳定问题，这使得它们在处理高分辨率和小型UI元素时性能较差。为了解决这个问题，论文提出了一个多视图推理框架来提高模型的稳定性。", "method": "MVP包括两个组成部分：注意力引导视图提案（从指令到图像的关注得分导出多样化的视图）以及多坐标聚类（通过选择最密集的空间簇的中心点来进行预测集成）。", "result": "实验表明，MVP框架在各种模型和基准测试中都表现出色。例如，在ScreenSpot-Pro上，UI-TARS-1.5-7B、GTA1-7B、Qwen3VL-8B-Instruct和Qwen3VL-32B-Instruct的性能分别提升至56.1%、61.7%、65.3%和74.0%。", "conclusion": "MVP通过多视图推理显著提高了GUI接地任务中模型的稳定性与准确性，展示了在不同模型上的广泛适用性。"}}
{"id": "2512.08524", "pdf": "https://arxiv.org/pdf/2512.08524", "abs": "https://arxiv.org/abs/2512.08524", "authors": ["Jawad Ibn Ahad", "Maisha Rahman", "Amrijit Biswas", "Muhammad Rafsan Kabir", "Robin Krambroeckers", "Sifat Momen", "Nabeel Mohammed", "Shafin Rahman"], "title": "Beyond Real Weights: Hypercomplex Representations for Stable Quantization", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.", "AI": {"tldr": "本文提出了一种逐步重新参数化策略，通过用紧凑的PHM层替换密集前馈网络块来压缩多模态语言模型。", "motivation": "由于多模态语言模型需要大量的参数容量以对齐高维视觉特征和语言表示，导致计算成本高昂且难以高效部署。本文旨在提出一种方法，在减少模型大小和推理延迟的同时保持性能稳定。", "method": "通过逐步替换密集前馈网络块为轻量级的PHM层，并结合残差插值计划、重建损失以及知识蒸馏损失，使得PHM模块能够继承原始密集层的功能行为。这种方法在训练过程中保证了模型的稳定性，同时实现了参数和FLOP的大幅减少。", "result": "该方法在多个视觉-语言模型上进行了评估，并保持与基准模型相当的性能，同时显著减少了模型大小和推理延迟。", "conclusion": "逐步PHM替换为多模态推理提供了一种架构兼容路径以实现更高的效率，并补充了现有的低比特量化技术。"}}
{"id": "2512.08518", "pdf": "https://arxiv.org/pdf/2512.08518", "abs": "https://arxiv.org/abs/2512.08518", "authors": ["Nadezhda Kushina", "Ko Watanabe", "Aarthi Kannan", "Ashita Ashok", "Andreas Dengel", "Karsten Berns"], "title": "SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Social robots must adjust to human proxemic norms to ensure user comfort and engagement. While prior research demonstrates that eye-tracking features reliably estimate comfort in human-human interactions, their applicability to interactions with humanoid robots remains unexplored. In this study, we investigate user comfort with the robot \"Ameca\" across four experimentally controlled distances (0.5 m to 2.0 m) using mobile eye-tracking and subjective reporting (N=19). We evaluate multiple machine learning and deep learning models to estimate comfort based on gaze features. Contrary to previous human-human studies where Transformer models excelled, a Decision Tree classifier achieved the highest performance (F1-score = 0.73), with minimum pupil diameter identified as the most critical predictor. These findings suggest that physiological comfort thresholds in human-robot interaction differ from human-human dynamics and can be effectively modeled using interpretable logic.", "AI": {"tldr": "本文研究了使用眼动追踪技术来评估人类对机器人“Ameca”的舒适感，探索人类在与仿人形机器人交互时的个人空间需求。", "motivation": "社会机器人需要适应人类的空间距离规范以确保用户的舒适度和参与度。以往的研究表明，在人际互动中，眼动追踪可以可靠地估计舒适程度，但其应用于人机互动中的有效性尚未被探索。", "method": "通过控制实验距高（0.5米至2.0米）进行实验，并使用移动眼动跟踪技术和主观报告来评估19名参与者对机器人的舒适感。采用多种机器学习和深度学习模型基于注视特征估计舒适度。", "result": "决策树分类器在所有模型中表现最佳，F1分数为0.73，最小瞳孔直径被识别为主要预测因子。", "conclusion": "研究结果表明，在人机互动中的生理舒适阈值与人际互动中的不同，并可以通过可解释的逻辑有效地建模。"}}
{"id": "2512.08512", "pdf": "https://arxiv.org/pdf/2512.08512", "abs": "https://arxiv.org/abs/2512.08512", "authors": ["Jiang Liu", "Yan Qin", "Wei Dai", "Chau Yuen"], "title": "A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles", "categories": ["cs.AI"], "comment": "Accepted in IEEE Transactions on Industrial Informatics", "summary": "Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.", "AI": {"tldr": "提出了一种轻量级的增量转移学习方法，用于监测无人机锂离子电池的状态。", "motivation": "为了减少传统迁移学习在SOH监控中所需的大量计算资源，从而提高便携式移动设备的工作寿命。", "method": "通过利用目标领域的无标签数据，提出了半监督的迁移学习机制，并使用结构风险最小化、转移不匹配最小化和流形一致性最大化来保证节点参数的跨域学习能力。", "result": "实验表明，所提出的CITL方法在SOH估计上优于其他五种方法，分别提高了83.73%，61.15%，28.24%，87.70%和57.34%。", "conclusion": "该研究证明了轻量级增量迁移学习的有效性，并通过无人机电池数据集验证了其优越性能。"}}
{"id": "2512.08511", "pdf": "https://arxiv.org/pdf/2512.08511", "abs": "https://arxiv.org/abs/2512.08511", "authors": ["Wenxi Yang", "Yuzhong Zhao", "Fang Wan", "Qixiang Ye"], "title": "Thinking with Images via Self-Calling Agent", "categories": ["cs.CV"], "comment": "Code would be released at https://github.com/YWenxi/think-with-images-through-self-calling soon", "summary": "Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\\%$ with $\\sim 75\\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.", "AI": {"tldr": "本文提出了自我调用的链式思维(sCoT)方法，通过语言分解视觉推理任务，并优化其有效性与效率。", "motivation": "现有研究在优化多模态链式思维(iMCoT)时面临缺乏高质量数据的问题。因此，需要一种新的方法来提高训练的效果和效率。", "method": "sCoT将复杂的视觉推理任务分解为原子子任务，并通过自我调用的方法解决问题。该方法无需显式的多模态交互，采用组相对策略优化增强有效推理行为的强化学习。", "result": "实验表明，相比于强基线方法，sCoT在HR-Bench 4K上提升了整体推理性能达1.9%，并且减少了约75%的GPU小时数。", "conclusion": "本文提出的方法通过自我调用实现高效的视觉推理，并证明了其优越性和应用前景。"}}
{"id": "2512.08506", "pdf": "https://arxiv.org/pdf/2512.08506", "abs": "https://arxiv.org/abs/2512.08506", "authors": ["Jialu Sui", "Rui Liu", "Hongsheng Zhang"], "title": "OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.", "AI": {"tldr": "通过应用隐式扩散过程和功能自动编码器架构来生成连续的占用函数，从而从噪声点云中实现高保真度的三维建筑重建。", "motivation": "准确捕捉在不同点密度和噪声干扰下的建筑物表面是利用LiDAR点云进行建筑物重建的主要挑战。为了解决这个问题，提出了一种新的方法来灵活地收集高质量的3D轮廓并提高模型性能。", "method": "OCCDiff结合了隐式扩散过程与功能自动编码器架构以生成任意位置可评估的连续占用函数，并且提出了点编码器提供条件特征给扩散学习、限制最终预测的占用解码器以及向潜在编码器插入多模态特征。采用多任务训练策略，确保点编码器学习多样化的强大特征表示。", "result": "实验结果表明，该方法生成了物理上一致的样本，并且与目标分布具有高保真度，对噪声数据表现出鲁棒性。", "conclusion": "OCCDiff在利用LiDAR点云进行三维建筑重建方面取得了显著成果，能够在不同密度和噪音条件下准确地捕捉建筑物表面。"}}
{"id": "2512.08505", "pdf": "https://arxiv.org/pdf/2512.08505", "abs": "https://arxiv.org/abs/2512.08505", "authors": ["Vasco Ramos", "Regev Cohen", "Idan Szpektor", "Joao Magalhaes"], "title": "Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.", "AI": {"tldr": "提出了一种名为NoisyCLIP的方法，在扩散模型的去噪过程中检测文本和图像之间的语义对齐，以实现实时评估。", "motivation": "现有的条件扩散模型生成过程中存在文本与图像之间不准确对齐的问题，需要在生成后进行昂贵且耗时的对齐检查。为了提高效率并降低成本，作者提出了一种新的方法来实时检测这些不对称情况。", "method": "通过利用双编码器在逆向扩散过程中的应用，NoisyCLIP能够在去噪过程中测量语义对齐，从而实现早期的文本图像对齐评估，并减少了整体生成所需的计算成本。", "result": "实验表明，NoisyCLIP可以将计算成本降低50％，同时在最佳N（BoN）设置中实现接近98%与CLIP对齐性能相比的效果。该方法能够实现实时的语义对齐评估而不会牺牲图像的质量。", "conclusion": "此研究首次探索并验证了如何在逆向扩散过程中通过双编码器技术检测文本和图像之间的语义不一致，为提高生成模型效率提供了一种新的途径，并展示了NoisyCLIP的有效性和实用性。"}}
{"id": "2512.08503", "pdf": "https://arxiv.org/pdf/2512.08503", "abs": "https://arxiv.org/abs/2512.08503", "authors": ["Jiaming Zhang", "Che Wang", "Yang Cao", "Longtao Huang", "Wei Yang Bryan Lim"], "title": "Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \\textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \\textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\\% improvement in tract-level protection (33.8\\% vs 19.4\\%) and nearly doubling block-level protection (33.5\\% vs 16.8\\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.", "AI": {"tldr": "本文提出了一种新的对抗性框架ReasonBreak，用于干扰多模态大型推理模型中的地理隐私问题。", "motivation": "多模态大型推理模型通过分层链式思考推理从个人图像中推断出精确的地理位置，现有保护技术对这些复杂多层次推理过程无效。因此，需要一种专门针对此场景的新方法来解决地理隐私风险。", "method": "ReasonBreak通过概念感知干扰破坏MLRMs中的层次化推理。它生成与概念层级一致的干扰，并针对性地影响关键的概念依赖关系，从而使推断步骤失效并沿着后续推理阶段传播。", "result": "实验表明，在七种最先进的多模态大型推理模型上，ReasonBreak显著提高了地理隐私保护效果，实现了14.4%的进步。在街区和地块级别的保护分别达到了33.8%和33.5%，几乎是之前技术的两倍。", "conclusion": "本文通过引入概念感知干扰方法ReasonBreak建立了新的隐私保护范式，有效对抗基于推理模型的地理隐私威胁。"}}
{"id": "2512.08500", "pdf": "https://arxiv.org/pdf/2512.08500", "abs": "https://arxiv.org/abs/2512.08500", "authors": ["Jianan Li", "Xiao Chen", "Tao Huang", "Tien-Tsin Wong"], "title": "Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/", "AI": {"tldr": "通过生成和模仿二维动作来学习控制物理模拟的三维角色。", "motivation": "视频数据比运动捕捉数据更经济，但直接从视频中合成现实且多样的行为仍然是一个挑战。现有的方法依赖于现成的运动重建技术，这在一般化方面存在问题，如需要3D训练数据或无法生成物理上合理的姿态，难以应用于复杂场景。", "method": "提出Mimic2DM框架，直接学习控制策略，仅需二维关键点轨迹。通过最小化重投影误差，训练单视图的2D运动跟踪政策。还开发了一种基于变压器的自回归2D运动生成器，并将其集成到分层控制框架中。", "result": "该方法可以有效地在各种领域学习合成物理上合理的多样化动作，包括跳舞、足球运球和动物移动。", "conclusion": "Mimic2DM框架展示了其灵活性，在不依赖显式3D运动数据的情况下，能够生成多样且物理合理的行为。"}}
{"id": "2512.08499", "pdf": "https://arxiv.org/pdf/2512.08499", "abs": "https://arxiv.org/abs/2512.08499", "authors": ["Waleed Razzaq", "Yun-Bo Zhao"], "title": "Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Under review at Structural health Monitoring - SAGE", "summary": "Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.", "AI": {"tldr": "开发距离感知不确定性量化方法以提高物理引导神经网络在滚动轴承健康预测中的准确性", "motivation": "现有不确定性量化方法缺乏校准，运行成本高且不具距离感知性，在分布外数据上表现不佳。研究旨在提供一种新的不确定性量化方式来改善机器故障预测的可靠性和鲁棒性", "method": "提出了基于谱归一化高斯过程（PG-SNGP）和基于深度证据回归（PG-SNER）的新方法，通过在隐藏层中应用谱归一化保持输入到潜在空间的距离，并使用特定损失函数平衡数据忠实度与物理一致性", "result": "新方法提高了预测精度，在分布外条件下表现出更好的泛化能力并增强了对抗攻击的鲁棒性", "conclusion": "所提出的方法改进了滚动轴承健康状态预测，提供了更准确且可靠的结果"}}
{"id": "2512.08498", "pdf": "https://arxiv.org/pdf/2512.08498", "abs": "https://arxiv.org/abs/2512.08498", "authors": ["Yijia Guo", "Tong Hu", "Zhiwei Li", "Liwen Hu", "Keming Qian", "Xitong Lin", "Shengbo Chen", "Tiejun Huang", "Lei Ma"], "title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.", "AI": {"tldr": "本文提出了一种基于多摄像机装置的实时三维重建框架，该框架可以将多个重叠摄像头的密集RGB流融合成统一的高斯表示。", "motivation": "单目RGB流在实时三维重建方面表现出一定的潜力，但受限于视场角而无法实现完整的三维覆盖。采用多摄像机装置解决了这一问题，并推动了更加高效和准确的在线重构方法的发展。", "method": "该研究提出了一种增量融合多个重叠摄像头密集RGB流至统一高斯表示的方法，并通过层次化相机初始化方案实现了粗略的跨相机对齐，同时采用了轻量级多摄像机束调整以稳定轨迹并保持实时性能。此外还引入了冗余消除的高斯采样策略和频率感知优化调度器，减少所需高斯原型数量和优化迭代次数。", "result": "该方法能够在短短2分钟内使用仅有的原始多摄像头视频流重构数百米的三维场景，并展示了前所未有的速度、鲁棒性和重建保真度。", "conclusion": "本文成功地实现了对基于多摄像机装置实时三维重建的方法，通过引入一系列优化策略，在保持高效性的同时提高了三维场景重构的速度和精确度。"}}
{"id": "2512.08493", "pdf": "https://arxiv.org/pdf/2512.08493", "abs": "https://arxiv.org/abs/2512.08493", "authors": ["Dyna Soumhane Ouchebara", "Stéphane Dupont"], "title": "LLM-based Vulnerable Code Augmentation: Generate or Refactor?", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages, Submitted to ESAAN 2026, Under pier review", "summary": "Vulnerability code-bases often suffer from severe imbalance, limiting the effectiveness of Deep Learning-based vulnerability classifiers. Data Augmentation could help solve this by mitigating the scarcity of under-represented CWEs. In this context, we investigate LLM-based augmentation for vulnerable functions, comparing controlled generation of new vulnerable samples with semantics-preserving refactoring of existing ones. Using Qwen2.5-Coder to produce augmented data and CodeBERT as a vulnerability classifier on the SVEN dataset, we find that our approaches are indeed effective in enriching vulnerable code-bases through a simple process and with reasonable quality, and that a hybrid strategy best boosts vulnerability classifiers' performance.", "AI": {"tldr": "本文探讨了使用LLM进行脆弱代码数据增强的方法，通过生成新样本和重写现有样本来解决漏洞分类器训练中的类别不平衡问题。", "motivation": "深度学习模型在处理具有严重类不平衡的漏洞代码时效果受限。为了缓解这种稀缺性，提出了基于大语言模型（LLM）的数据增强策略，以提高脆弱代码库的质量和数量。", "method": "采用Qwen2.5-Coder生成新样本并进行语义保持重构现有样本，并利用CodeBERT作为分类器在SVEN数据集上评估这些方法的有效性。实验对比了单纯生成、重写以及两者结合的策略。", "result": "研究发现，所提出的方法能够有效地增加脆弱代码库的内容，并且质量可接受；同时表明混合策略对于提升漏洞检测模型的效果最为有效。", "conclusion": "LLM在进行数据增强方面展示了潜在优势。通过生成和重写的方式可以提高训练数据的多样性和平衡性，从而改进深度学习模型在识别软件安全漏洞方面的性能。"}}
{"id": "2512.08492", "pdf": "https://arxiv.org/pdf/2512.08492", "abs": "https://arxiv.org/abs/2512.08492", "authors": ["Aliaksei Kaliutau"], "title": "Autonomous Issue Resolver: Towards Zero-Touch Code Maintenance", "categories": ["cs.AI"], "comment": "21 pages, 4 figures", "summary": "Recent advances in Large Language Models have revolutionized function-level code generation; however, repository-scale Automated Program Repair (APR) remains a significant challenge. Current approaches typically employ a control-centric paradigm, forcing agents to navigate complex directory structures and irrelevant control logic. In this paper, we propose a paradigm shift from the standard Code Property Graphs (CPGs) to the concept of Data Transformation Graph (DTG) that inverts the topology by modeling data states as nodes and functions as edges, enabling agents to trace logic defects through data lineage rather than control flow. We introduce a multi-agent framework that reconciles data integrity navigation with control flow logic. Our theoretical analysis and case studies demonstrate that this approach resolves the \"Semantic Trap\" inherent in standard RAG systems in modern coding agents. We provide a comprehensive implementation in the form of Autonomous Issue Resolver (AIR), a self-improvement system for zero-touch code maintenance that utilizes neuro-symbolic reasoning and uses the DTG structure for scalable logic repair. Our approach has demonstrated good results on several SWE benchmarks, reaching a resolution rate of 87.1% on SWE-Verified benchmark. Our approach directly addresses the core limitations of current AI code-assistant tools and tackles the critical need for a more robust foundation for our increasingly software-dependent world.", "AI": {"tldr": "提出了数据转换图（DTG）的概念，用于解决代码维护中的语义陷阱问题，并开发了自主问题解决系统（AIR），在软件工程验证基准上达到了87.1%的修复率。", "motivation": "当前的自动程序修复方法难以处理仓库规模的问题，且受限于控制中心范式和标准检索增强代理系统的语义陷阱。因此，需要一种新的数据模型来改善代码维护过程。", "method": "提出了一种新的数据转换图（DTG）模型，并引入了一个多智能体框架以解决控制流逻辑与数据完整性导航的矛盾问题，同时利用神经符号推理和DTG结构进行可扩展逻辑修复。", "result": "在SWE-Verified基准上实现了87.1%的问题修复率，证明了所提出方法的有效性和优越性。", "conclusion": "该研究提供了一种新的数据转换图（DTG）模型，并开发了一个名为AIR的系统来实现零接触代码维护。这种新方法直接解决了现有AI代码助手工具的核心限制，并为软件依赖的世界提供了更加坚实的基础。"}}
{"id": "2512.08486", "pdf": "https://arxiv.org/pdf/2512.08486", "abs": "https://arxiv.org/abs/2512.08486", "authors": ["Ada Gorgun", "Fawaz Sammani", "Nikos Deligiannis", "Bernt Schiele", "Jonas Fischer"], "title": "Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions", "summary": "Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions", "AI": {"tldr": "研究通过PCI框架分析了生成过程中的概念动态，揭示不同扩散模型中特定概念形成的时机", "motivation": "理解扩散模型的生成过程中，概念如何形成及锁定，为文本驱动的图像编辑提供有效干预时机", "method": "提出PCI（提示条件干预）框架，通过插入特定概念并在给定时间步分析其保留概率来研究动态过程", "result": "揭示了不同扩散模型在不同类型概念上的多样时间行为，强调某些阶段对特定概念更有利", "conclusion": "发现为文本驱动的图像编辑提供了行动见解，并且无需访问模型内部或训练，即可实现更强的编辑效果"}}
{"id": "2512.08481", "pdf": "https://arxiv.org/pdf/2512.08481", "abs": "https://arxiv.org/abs/2512.08481", "authors": ["Yixiang Lin", "Tiancheng Yang", "Jonathan Eden", "Ying Tan"], "title": "Prospect Theory in Physical Human-Robot Interaction: A Pilot Study of Probability Perception", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "Understanding how humans respond to uncertainty is critical for designing safe and effective physical human-robot interaction (pHRI), as physically working with robots introduces multiple sources of uncertainty, including trust, comfort, and perceived safety. Conventional pHRI control frameworks typically build on optimal control theory, which assumes that human actions minimize a cost function; however, human behavior under uncertainty often departs from such optimal patterns. To address this gap, additional understanding of human behavior under uncertainty is needed. This pilot study implemented a physically coupled target-reaching task in which the robot delivered assistance or disturbances with systematically varied probabilities (10\\% to 90\\%). Analysis of participants' force inputs and decision-making strategies revealed two distinct behavioral clusters: a \"trade-off\" group that modulated their physical responses according to disturbance likelihood, and an \"always-compensate\" group characterized by strong risk aversion irrespective of probability. These findings provide empirical evidence that human decision-making in pHRI is highly individualized and that the perception of probability can differ to its true value. Accordingly, the study highlights the need for more interpretable behavioral models, such as cumulative prospect theory (CPT), to more accurately capture these behaviors and inform the design of future adaptive robot controllers.", "AI": {"tldr": "本文通过一个物理耦合的目标追踪任务研究了人类在与机器人交互过程中对不确定性的反应。", "motivation": "设计安全有效的物理人机交互系统需要理解人在不确定性环境中的行为。常规的最优控制理论假设人的行为以最小化成本函数为目标，但现实情况中的人类行为常常与此不同。", "method": "研究者通过一个机器人提供帮助或干扰的任务来模拟不确定性和风险，并分析参与者在不同概率水平下的力输入和决策策略。", "result": "实验结果显示参与者的行为分成两类：一类调整物理回应以适应不同的干扰概率，另一类则始终表现出强烈的避险行为。这表明人类的决策过程高度个性化且对概率感知存在偏差。", "conclusion": "研究强调了使用如累积前景理论等更解释性更强的行为模型来准确捕捉这些行为并指导未来自适应机器人控制器设计的重要性。"}}
{"id": "2512.08478", "pdf": "https://arxiv.org/pdf/2512.08478", "abs": "https://arxiv.org/abs/2512.08478", "authors": ["Yuning Gong", "Yifei Liu", "Yifan Zhan", "Muyao Niu", "Xueying Li", "Yuanjun Liao", "Jiaming Chen", "Yuanyuan Gao", "Jiaqi Chen", "Minming Chen", "Li Zhou", "Yuning Zhang", "Wei Wang", "Xiaoqing Hou", "Huaxi Huang", "Shixiang Tang", "Le Ma", "Dingwen Zhang", "Xue Yang", "Junchi Yan", "Yanchi Zhang", "Yinqiang Zheng", "Xiao Sun", "Zhihang Zhong"], "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project page: https://visionary-laboratory.github.io/visionary", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "AI": {"tldr": "Visionary是一个基于WebGPU的平台，用于实时渲染各种Gaussian Splatting和网格模型。", "motivation": "现有解决方案在3D Gaussian Splatting方面存在碎片化、重量大或受到旧管道限制的问题，导致部署困难和支持动态内容和生成模型有限。", "method": "Visionary利用高效WebGPU渲染器和帧内ONNX推理，在浏览器中实现动态神经处理并保持轻量级体验。它引入了标准化的Gaussian Generator合同，并提供了与three.js库集成的插件。", "result": "实验表明，与其他网络查看器相比，使用相同的3DGS资产时，Visionary由于基于GPU的基本排序技术实现了更高效的渲染效率。", "conclusion": "Visionary作为统一的世界模型载体，在浏览器中直接整合推理和渲染，大大降低了3DGS方法的再现、比较和部署障碍。"}}
{"id": "2512.08477", "pdf": "https://arxiv.org/pdf/2512.08477", "abs": "https://arxiv.org/abs/2512.08477", "authors": ["Huiguo He", "Pengyu Yan", "Ziqi Yi", "Weizhi Zhong", "Zheng Liu", "Yejun Tang", "Huan Yang", "Kun Gai", "Guanbin Li", "Lianwen Jin"], "title": "ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.", "AI": {"tldr": "提出了一种新的拖拽式图像编辑方法ContextDrag，利用上下文保存令牌注入和位置一致注意力机制改善图像编辑效果。", "motivation": "现有拖拽式图像编辑方法在利用参考图的上下文信息方面存在局限性，导致编辑结果缺乏连贯性和细节保留。", "method": "引入了上下文保持令牌注入（CTI）以精确地将参考特征插入正确的位置，并采用位置一致注意力（PCA）来消除无关特征的影响。", "result": "实验表明ContextDrag在拖拽式图像编辑任务中优于现有最优方法。", "conclusion": "通过结合先进的上下文建模和位置编码技术，ContextDrag能够更精准、细致地进行拖拽式图像编辑。"}}
{"id": "2512.08476", "pdf": "https://arxiv.org/pdf/2512.08476", "abs": "https://arxiv.org/abs/2512.08476", "authors": ["Po-An Shih", "Shao-Hua Wang", "Yung-Che Li", "Chia-Heng Tu", "Chih-Han Chang"], "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems", "categories": ["cs.RO"], "comment": null, "summary": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.", "AI": {"tldr": "该论文提出了一种基于多代理和大型语言模型（LLM）的自主驾驶系统设计空间探索框架。", "motivation": "传统的设计空间探索方法难以处理多样化的执行输出，并且需要人工干预来评估正确性，为此，作者提出了一个自动化的解决方案，以减少人为干预并提高效率。", "method": "该论文提出了一种利用多代理和大型语言模型（LLM）的方法，结合3D仿真和性能分析工具，实现了自主驾驶系统设计空间的自动化探索。框架中的LLM被用来处理用户输入、生成设计方案、执行管理和输出解析等工作。", "result": "与遗传算法基准相比，所提出的框架能够发现更多成本效益更高的帕累托最优解，并且在相同的探索预算下减少了导航时间，证明了其有效性和效率。", "conclusion": "该论文提出的方法为自主驾驶系统的自动化设计开辟了一条新途径。"}}
{"id": "2512.08467", "pdf": "https://arxiv.org/pdf/2512.08467", "abs": "https://arxiv.org/abs/2512.08467", "authors": ["Chamath Ranasinghe", "Uthayasanker Thayasivam"], "title": "Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.", "AI": {"tldr": "本文提出了一种结合Segment Anything Model（SAM）和CSRT跟踪器的轻量级足球运动员追踪方法，以解决频繁遮挡、相似外观和快速运动带来的挑战。", "motivation": "由于经常出现遮挡、相似的外貌以及在拥挤场景中的迅速移动，足球运动员追踪面临着巨大挑战。因此本文提出了一种结合Segment Anything Model（SAM）与CSRT跟踪器的方法来提高追踪性能。", "method": "该方法利用了HSV直方图和颜色模型进行再识别，以改进遮挡恢复情况。它使用团队意识系统以及SAM来进行精确初始化，并通过基于外观的重新识别技术实现更好的遮挡恢复效果。", "result": "实验结果显示，在处理速度（FPS和内存）、追踪精度（成功率和框稳定性）及鲁棒性方面有所提高。该方法在轻度遮挡的情况下保持了100%的成功率，而在有5名或更多球员的拥挤罚球区场景中也达到了90%的成功率。", "conclusion": "实验表明该方法在不同人群密度下都能提供一致的表现，但在处理长时间遮挡时存在困难。这些结果为在资源受限的情况下部署足球追踪系统提供了实用指南，并强调了需要更强的重新识别机制来解决长期缺席的问题。"}}
{"id": "2512.08463", "pdf": "https://arxiv.org/pdf/2512.08463", "abs": "https://arxiv.org/abs/2512.08463", "authors": ["Antonio Terpin", "Raffaello D'Andrea"], "title": "Using reinforcement learning to probe the role of feedback in skill acquisition", "categories": ["cs.AI", "cs.LG", "cs.RO", "eess.SY"], "comment": "Website: https://antonioterpin.com/fluids-control", "summary": "Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.", "AI": {"tldr": "使用强化学习在台面水道中控制圆柱体的拖曳，研究技能获取过程中反馈的作用。", "motivation": "通过物理实验探讨无外部反馈情况下技能学习的可能性与条件，理解高维流反馈在技能掌握中的作用。", "method": "直接将通用型强化学习代理与旋转圆柱体连接，在台面水道中最大化或最小化拖曳。通过对比有无反馈训练对策略性能的影响，研究反馈的作用。", "result": "发现高维度流动反馈有助于快速有效地找到高性能控制策略；执行时不需要持续反馈即可保持良好表现。但缺少实时反馈可能导致特定目标的学习效率低下。", "conclusion": "技能学习可能需要比实际执行更丰富的信息和条件，并且训练环境的友好度与目标而非动力学或策略复杂性相关。"}}
{"id": "2512.08459", "pdf": "https://arxiv.org/pdf/2512.08459", "abs": "https://arxiv.org/abs/2512.08459", "authors": ["Gary Ackerman", "Theodore Wilson", "Zachary Kallenborn", "Olivia Shoemaker", "Anna Wetzel", "Hayley Peterson", "Abigail Danfora", "Jenna LaTourette", "Brandon Behlendorf", "Douglas Clifford"], "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.ET"], "comment": "19 pages, 2 figures", "summary": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.", "AI": {"tldr": "该论文介绍了Bacterial Biothreat Benchmark (B3) 数据集的实施，这是一个评估前沿人工智能模型尤其是大型语言模型（LLM）生物安全风险的基准。", "motivation": "快速发展的前沿人工智能模型有可能被用于促进生物恐怖主义或获取生物武器，这引发了政策制定者、学术界和公众的高度关注。因此，开发能够量化并减轻这种风险的模型评估基准变得非常重要。", "method": "论文描述了通过运行B3数据集中的基准测试，并对样本前沿AI模型的响应进行人工评估及应用风险分析的方法来实施该基准。", "result": "试点项目表明，B3数据集提供了一种有效且细致的方法，可以快速评估特定LLM的生物安全风险，识别主要的风险来源并为优先缓解领域提供指导。", "conclusion": "研究表明，通过使用B3数据集及其方法论，能够有效地衡量和减轻大型语言模型可能带来的生物安全风险。"}}
{"id": "2512.08451", "pdf": "https://arxiv.org/pdf/2512.08451", "abs": "https://arxiv.org/abs/2512.08451", "authors": ["Gary Ackerman", "Zachary Kallenborn", "Anna Wetzel", "Hayley Peterson", "Jenna LaTourette", "Olivia Shoemaker", "Brandon Behlendorf", "Sheriff Almakki", "Doug Clifford", "Noah Sheinbaum"], "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.ET"], "comment": "18 pages, 3 figures", "summary": "The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.", "AI": {"tldr": "该论文描述了一种新型生物威胁基准生成框架的第二个组成部分，即细菌生物威胁基准（B3）数据集的开发过程。", "motivation": "快速发展的前沿人工智能模型尤其是大型语言模型可能促进生物恐怖主义或获取生物武器的风险引发了政策制定者、学术界和公众的关注。为了量化并缓解这些风险，需要建立能够评估特定模型所带来的生物安全风险的基准。", "method": "该研究通过三种互补的方法生成超过7000个潜在基准：1)基于网络提示生成；2)红队演练；3)挖掘现有基准语料库。随后进行去重、提升诊断性评估及一般质量控制，最终确定了1010个最终基准。", "result": "研究团队开发了一个包含1010个生物威胁相关基准的数据集，并确保这些基准具备提供提升、直接与生物安全威胁相关的特性，并且符合更大的生物安全架构。", "conclusion": "通过这种方法生成的细菌生物威胁基准（B3）数据集，为评估大型语言模型在生物安全方面的风险提供了重要工具。"}}
{"id": "2512.08449", "pdf": "https://arxiv.org/pdf/2512.08449", "abs": "https://arxiv.org/abs/2512.08449", "authors": ["Yong-Woon Kim"], "title": "From Accuracy to Impact: The Impact-Driven AI Framework (IDAIF) for Aligning Engineering Architecture with Theory of Change", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces the Impact-Driven AI Framework (IDAIF), a novel architectural methodology that integrates Theory of Change (ToC) principles with modern artificial intelligence system design. As AI systems increasingly influence high-stakes domains including healthcare, finance, and public policy, the alignment problem--ensuring AI behavior corresponds with human values and intentions--has become critical. Current approaches predominantly optimize technical performance metrics while neglecting the sociotechnical dimensions of AI deployment. IDAIF addresses this gap by establishing a systematic mapping between ToC's five-stage model (Inputs-Activities-Outputs-Outcomes-Impact) and corresponding AI architectural layers (Data Layer-Pipeline Layer-Inference Layer-Agentic Layer-Normative Layer). Each layer incorporates rigorous theoretical foundations: multi-objective Pareto optimization for value alignment, hierarchical multi-agent orchestration for outcome achievement, causal directed acyclic graphs (DAGs) for hallucination mitigation, and adversarial debiasing with Reinforcement Learning from Human Feedback (RLHF) for fairness assurance. We provide formal mathematical formulations for each component and introduce an Assurance Layer that manages assumption failures through guardian architectures. Three case studies demonstrate IDAIF application across healthcare, cybersecurity, and software engineering domains. This framework represents a paradigm shift from model-centric to impact-centric AI development, providing engineers with concrete architectural patterns for building ethical, trustworthy, and socially beneficial AI systems.", "AI": {"tldr": "本文提出了一种新的AI架构方法——影响驱动的人工智能框架（IDAIF），该框架将理论变化模型与现代人工智能系统设计相结合，旨在解决当前AI系统的准确性优化忽视社会技术维度的问题。", "motivation": "随着AI在高风险领域如医疗、金融和公共政策中的应用增加，确保AI行为符合人类价值观和意图变得至关重要。然而，现有的方法主要侧重于技术性能指标的优化，忽略了AI部署的社会技术层面。为此，本文提出了IDAIF框架来填补这一空白。", "method": "IDAIF通过系统地将理论变化模型的五个阶段（输入-活动-输出-结果-影响）映射到相应的AI架构层（数据层-流水线层-推理层-代理层-规范层），并在每一层引入严格的理论基础，如多目标帕累托优化、分层多代理协调机制以及因果图和对抗性去偏置等方法。", "result": "本文通过三个案例研究展示了IDAIF在医疗、网络安全和软件工程领域的应用效果，并提供了一种管理假设失败的保障层架构。", "conclusion": "IDAIF代表了从模型中心向影响中心AI开发范式的转变，为工程师提供了构建道德、可信和社会有益的人工智能系统的具体架构模式。"}}
{"id": "2512.08445", "pdf": "https://arxiv.org/pdf/2512.08445", "abs": "https://arxiv.org/abs/2512.08445", "authors": ["Madhav Gupta", "Vishak Prasad C", "Ganesh Ramakrishnan"], "title": "Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.", "AI": {"tldr": "该论文提出了一个结合子模选择和逐层梯度不确定性估计的框架，以提高视觉模型解释方法在分布变化下的鲁棒性和保真度。", "motivation": "现有的基于子集的选择方法在处理出分布（OOD）条件下表现不佳，易产生冗余、不稳定及不确定性的解释。因此，论文旨在通过引入结合层级梯度不确定性估计的方法来改善这些不足。", "method": "论文提出一种框架，该框架将子模选择与逐层的基于梯度的不确定性评估相结合，利用自适应权重扰动进行不确定性估计，并使用这些估计值指导子模优化。", "result": "通过实验表明，提出的框架不仅在分布变化条件下改善了现有方法的表现，还在内分布（ID）设置中也取得了改进效果。", "conclusion": "论文展示了当前基于子集的方法的局限性，并证明不确定性驱动的优化可以增强归因和对象级可解释性，在现实世界的视觉应用中铺平更透明、可信AI的道路。"}}
{"id": "2512.08444", "pdf": "https://arxiv.org/pdf/2512.08444", "abs": "https://arxiv.org/abs/2512.08444", "authors": ["Andreas Hauptmann", "Ozan Öktem"], "title": "Learned iterative networks: An operator learning perspective", "categories": ["eess.IV", "cs.LG", "math.FA", "math.NA", "math.OC"], "comment": null, "summary": "Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.", "AI": {"tldr": "论文提出了一种统一的操作符视角来理解学习迭代网络，并展示了其在图像重建中的应用。", "motivation": "为了更好地理解和改进学习迭代网络，作者从操作符的角度提出了一个统一的框架。这个方法可以帮助理解并连接不同的学习迭代技术的核心原理。", "method": "通过定义计算过程和优化目标两个方面来构建学习重构操作符，并在这一框架下回顾了线性与非线性的逆问题。", "result": "研究展示了不同学习迭代网络方法之间的紧密联系，证明这些方法可以视为同一核心思想的不同表达。", "conclusion": "该工作提供了一种统一的操作符视角来看待和理解各种学习迭代网络。这一新视角有望推动未来在图像重建等领域的进一步创新和发展。"}}
{"id": "2512.08441", "pdf": "https://arxiv.org/pdf/2512.08441", "abs": "https://arxiv.org/abs/2512.08441", "authors": ["Luca Cogo", "Marco Buzzelli", "Simone Bianco", "Javier Vazquez-Corral", "Raimondo Schettini"], "title": "Leveraging Multispectral Sensors for Color Correction in Mobile Cameras", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.", "AI": {"tldr": "本文提出了一种利用多光谱传感器进行颜色校正的方法，通过在单一模型中整合高分辨率RGB传感器和低分辨率MS传感器的数据，实现了端到端的颜色校正。", "motivation": "现有的颜色校正方法通常将不同阶段分开处理，并且早期就丢弃了多光谱数据。本文旨在提出一种统一的学习框架，以更好地利用这些丰富的光谱信息来提高色彩准确性。", "method": "该论文提出了一种联合学习框架，同时使用高分辨率RGB传感器和低分辨率MS传感器的数据进行端到端的颜色校正，通过重构两个不同的状态-of-the-art图像转图像架构实现。", "result": "实验表明，与仅基于RGB或仅基于MS的方法相比，本文所提方法可将色彩误差降低高达50%，显示出更高的颜色准确性和稳定性。", "conclusion": "该论文提供了一种灵活通用的框架来改进移动设备中相机的颜色校正性能，并展示了其在不同场景下的优越性。"}}
{"id": "2512.08439", "pdf": "https://arxiv.org/pdf/2512.08439", "abs": "https://arxiv.org/abs/2512.08439", "authors": ["Qing Xu", "Kun Yuan", "Yuxiang Luo", "Yuhao Zhai", "Wenting Duan", "Nassir Navab", "Zhen Chen"], "title": "LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.", "AI": {"tldr": "LapFM通过层次概念演化预训练，从大量未标记的手术图像中进化出强大的分割能力，用于普遍腹腔镜分割。", "motivation": "现有方法在处理标注稀缺和语义不一致的问题时表现不佳，难以泛化到不同的手术目标上。为解决这一问题，LapFM旨在通过大规模未标记的手术图像来增强其泛化能力和鲁棒性。", "method": "首先建立腹腔镜概念层次结构(LCH)，其次提出基于层次一致性的置信度驱动演化标签生成方法，从无标注图像中筛选出可靠的样本进行训练。", "result": "实验表明LapFM在通用腹腔镜分割中的细粒度自适应泛化性能优于现有最佳方法，并创建了包含114K幅图像掩码对的基准数据集。", "conclusion": "LapFM通过层次概念演化预训练，显著提升了手术目标分割的准确性和鲁棒性，为未来的研究奠定了基础。"}}
{"id": "2512.08437", "pdf": "https://arxiv.org/pdf/2512.08437", "abs": "https://arxiv.org/abs/2512.08437", "authors": ["Delong Du", "Apostolos Vavouris", "Omid Veisi", "Lu Jin", "Gunnar Stevens", "Lina Stankovic", "Vladimir Stankovic", "Alexander Boden"], "title": "Time and Money Matters for Sustainability: Insights on User Preferences on Renewable Energy for Electric Vehicle Charging Stations", "categories": ["cs.HC"], "comment": null, "summary": "Charging electric vehicles (EVs) with renewable energy can lessen their environmental impact. However, the fluctuating availability of renewable energy affects the sustainability of public EV charging stations. Nearby public charging stations may utilize differing energy sources due to their microgrid connections - ranging from exclusively renewable to non-renewable or a combination of both - highlighting the substantial variability in energy supply types within short distances. This study investigates the near-future scenario of integrating dynamic renewable energy availability in charging station navigation to impact the choices of EV users towards renewable sources. We conducted a within-subjects design survey with 50 car users and semi-structured interviews with 10 EV users from rural, suburban, and urban areas. The results show that when choosing EV charging stations, drivers often prioritize either time savings or money savings based on the driving scenarios that influence drivers' consumer value. Notably, EV users tend to select renewable-powered stations when they align with their main priority, be it saving money or time. This study offers end-user insights into the front-end graphic user interface and the development of the back-end ranking algorithm for navigation recommender systems that integrate dynamic renewable energy availability for the sustainable use of electric vehicles.", "AI": {"tldr": "研究探讨了电动汽车用户选择充电站时对可再生能源的偏好及其影响因素", "motivation": "探索如何通过动态可再生能源可用性改善公共电动汽车充电站的选择，促进可持续发展", "method": "进行了50名汽车用户和10名电动汽车用户的实验调查与访谈", "result": "发现用户在时间节省或金钱节省之间权衡，并且倾向于选择符合其主要优先级的可再生能源供电站点", "conclusion": "提供了用户界面设计及后台排名算法开发的相关见解，以促进导航推荐系统中的可持续使用"}}
{"id": "2512.08430", "pdf": "https://arxiv.org/pdf/2512.08430", "abs": "https://arxiv.org/abs/2512.08430", "authors": ["Nico Leuze", "Maximilian Hoh", "Samed Doğan", "Nicolas R. -Peña", "Alfred Schoettl"], "title": "SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to WACV 2026. Preprint version", "summary": "Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.", "AI": {"tldr": "本文提出了一个全稀疏深度变换器SDT-6D，用于多视角工业箱拣选中的6D姿态估计。", "motivation": "在密集的工业箱拣选环境中准确恢复6D姿态仍然是一项挑战，因为存在遮挡、反射和无纹理部分。因此提出了一种新的方法来解决这些问题。", "method": "本文提出了一种基于多视角深度图融合的方法，通过稀疏变换器动态关注遮挡和3D数据的非均匀分布，并采用场景适应性的注意力机制在不同分辨率下进行操作。", "result": "该方法在IPD和MV-YCB数据集上验证了其有效性，在复杂环境中表现出色。", "conclusion": "本文的方法展示了在密集拣选环境中的高效性和准确性，为6D姿态估计提供了新的解决方案。"}}
{"id": "2512.08426", "pdf": "https://arxiv.org/pdf/2512.08426", "abs": "https://arxiv.org/abs/2512.08426", "authors": ["Delong Du", "Sara Gilda Amirhajlou", "Akwasi Gyabaah", "Richard Paluch", "Claudia Müller"], "title": "Mediating Personal Relationships with Robotic Pets for Fostering Human-Human Interaction of Older Adults", "categories": ["cs.HC"], "comment": null, "summary": "Good human relationships are important for us to have a happy life and maintain our well-being. Otherwise, we will be at risk of experiencing loneliness or depression. In human-computer interaction (HCI) and computer-supported cooperative work (CSCW), robotic systems offer nuanced approaches to foster human connection, providing interaction beyond the traditional mediums that smartphones and computers offer. However, many existing studies primarily focus on the humanrobot relationships that older adults form directly with robotic pets rather than exploring how these robotic pets can enhance human-human relationships. Our ethnographic study investigates how robotic pets can be designed to facilitate human relationships. Through semi-structured interviews with six older adults and thematic analysis, our empirical findings provide insights into how robotic pets can be designed as telerobots to connect with others remotely, thus contributing to advance future development of robotic systems for mental health.", "AI": {"tldr": "研究如何设计机器人宠物以促进老年人之间的远程人际关系", "motivation": "探讨机器人宠物在增强人类之间关系中的作用，特别是在解决孤独和抑郁问题方面", "method": "通过半结构化访谈与六个老年人进行的民族志研究及主题分析", "result": "发现机器人宠物可以被设计为远程交互工具，有助于连接他人并增进社交互动", "conclusion": "这些发现对未来的心理健康相关机器人系统开发具有重要意义"}}
{"id": "2512.08411", "pdf": "https://arxiv.org/pdf/2512.08411", "abs": "https://arxiv.org/abs/2512.08411", "authors": ["Mingwei Li", "Xiaoyuan Zhang", "Chengwei Yang", "Zilong Zheng", "Yaodong Yang"], "title": "Prismatic World Model: Learning Compositional Dynamics for Planning in Hybrid Systems", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Model-based planning in robotic domains is fundamentally challenged by the hybrid nature of physical dynamics, where continuous motion is punctuated by discrete events such as contacts and impacts. Conventional latent world models typically employ monolithic neural networks that enforce global continuity, inevitably over-smoothing the distinct dynamic modes (e.g., sticking vs. sliding, flight vs. stance). For a planner, this smoothing results in catastrophic compounding errors during long-horizon lookaheads, rendering the search process unreliable at physical boundaries. To address this, we introduce the Prismatic World Model (PRISM-WM), a structured architecture designed to decompose complex hybrid dynamics into composable primitives. PRISM-WM leverages a context-aware Mixture-of-Experts (MoE) framework where a gating mechanism implicitly identifies the current physical mode, and specialized experts predict the associated transition dynamics. We further introduce a latent orthogonalization objective to ensure expert diversity, effectively preventing mode collapse. By accurately modeling the sharp mode transitions in system dynamics, PRISM-WM significantly reduces rollout drift. Extensive experiments on challenging continuous control benchmarks, including high-dimensional humanoids and diverse multi-task settings, demonstrate that PRISM-WM provides a superior high-fidelity substrate for trajectory optimization algorithms (e.g., TD-MPC), proving its potential as a powerful foundational model for next-generation model-based agents.", "AI": {"tldr": "该论文提出了一个结构化的架构——棱镜世界模型（PRISM-WM），用于分解复杂混合动力学为可组合的基本单元，以提高机器人在长时规划中的表现。", "motivation": "传统的潜在世界模型通过全局连续性强制执行单一的神经网络方法，导致动态模式平滑化。这种平滑化会导致预测错误累积，在物理边界处使搜索过程变得不可靠。因此，需要一种新的架构来准确建模混合动力学。", "method": "棱镜世界模型（PRISM-WM）利用上下文感知的专家混合框架，通过门控机制识别当前物理模式，并使用专门的专家预测相应的转换动态。引入了一个潜在正交化目标以确保专家多样性，防止模式坍缩。该方法准确建模系统动力学中的尖锐模式过渡。", "result": "实验表明，在挑战性的连续控制基准测试中，包括高维的人形机器人和各种多任务设置下，PRISM-WM显著减少了运行漂移，并为轨迹优化算法提供了一个更好的基础模型。", "conclusion": "棱镜世界模型（PRISM-WM）是一个强大的基础模型，能够准确建模复杂混合动力学中的动态模式过渡，提高了长时规划的可靠性。"}}
{"id": "2512.08410", "pdf": "https://arxiv.org/pdf/2512.08410", "abs": "https://arxiv.org/abs/2512.08410", "authors": ["Tao Chen", "Shaobo Ju", "Qiong Wu", "Chenxin Fang", "Kun Zhang", "Jun Peng", "Hui Li", "Yiyi Zhou", "Rongrong Ji"], "title": "Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.", "AI": {"tldr": "提出了一种名为One-shot视频片段检索增强（OneClip-RAG）的范式，以解决多模态大型语言模型处理长视频时内存开销过大的问题。", "motivation": "大多数多模态大型语言模型由于内存开销过大只能处理有限帧数的视频。为了解决这个问题，并提高视频理解的完整性和一致性，本文提出了OneClip-RAG方法。", "method": "提出了一种名为One-shot视频片段检索增强（OneClip-RAG）的方法，该方法利用了视频片段的优点进行增强视频理解；设计了一个新的查询引导式视频分块算法，能够在一次处理步骤中统一片段分块和跨模态检索，避免冗余计算。", "result": "实验结果表明，与多模态大型语言模型相比，OneClip-RAG显著提高了性能，并在处理长视频方面表现出优越的效率。例如，将InternLV2-8B和Qwen2-VL7B提升到GPT-4o的水平；使得LLaVA-Video可以在不到2.2分钟的时间内理解和处理长达一小时的视频。", "conclusion": "OneClip-RAG方法在多个长视频基准测试中取得了显著的性能增益，并且证明了其高效地处理长视频的能力。"}}
{"id": "2512.08406", "pdf": "https://arxiv.org/pdf/2512.08406", "abs": "https://arxiv.org/abs/2512.08406", "authors": ["Mingqi Gao", "Yunqi Miao", "Jungong Han"], "title": "SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos", "categories": ["cs.CV"], "comment": null, "summary": "Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.", "AI": {"tldr": "本文提出了SAM-Body4D，一个无需训练的框架，用于从视频中恢复具有时间和遮挡鲁棒性的4D人体网格。", "motivation": "最近基于图像的人体网格重建方法虽然在野外图像上表现出强大的鲁棒性，但当应用于视频时会受到帧间不一致和遮挡问题的影响。本文旨在解决这些问题，并通过利用视频中的时间连续性来提高性能。", "method": "提出的方法包括使用可提示的视频分割模型生成身份一致的掩模，然后使用遮挡感知模块细化这些掩模以恢复丢失区域。最后，用细化后的掩模指导SAM 3D Body生成一致的人体网格轨迹，并通过基于填充的并行策略实现高效的多人推断。", "result": "实验结果表明，与现有的方法相比，提出的SAM-Body4D框架在具有挑战性的野外视频中实现了更好的时间稳定性和鲁棒性。", "conclusion": "本文提出了一种无需额外训练即可从视频恢复4D人体网格的方法。该方法通过利用视频中的时间连续性解决了帧间不一致和遮挡问题，从而提高了重建的准确性和稳定性。"}}
{"id": "2512.08405", "pdf": "https://arxiv.org/pdf/2512.08405", "abs": "https://arxiv.org/abs/2512.08405", "authors": ["Fan Zhang", "Michael Gienger"], "title": "Learning Robot Manipulation from Audio World Models", "categories": ["cs.RO"], "comment": null, "summary": "World models have demonstrated impressive performance on robotic learning tasks. Many such tasks inherently demand multimodal reasoning; for example, filling a bottle with water will lead to visual information alone being ambiguous or incomplete, thereby requiring reasoning over the temporal evolution of audio, accounting for its underlying physical properties and pitch patterns. In this paper, we propose a generative latent flow matching model to anticipate future audio observations, enabling the system to reason about long-term consequences when integrated into a robot policy. We demonstrate the superior capabilities of our system through two manipulation tasks that require perceiving in-the-wild audio or music signals, compared to methods without future lookahead. We further emphasize that successful robot action learning for these tasks relies not merely on multi-modal input, but critically on the accurate prediction of future audio states that embody intrinsic rhythmic patterns.", "AI": {"tldr": "本文提出了一种生成性潜在流匹配模型，用于预测未来的音频状态，从而帮助机器人在执行任务时能更好地理解和处理复杂的多模态信息。", "motivation": "许多机器人学习任务需要多模态推理。例如，在填写水瓶的过程中，仅凭视觉信息是不充分的，必须考虑到声音的时间演化和其物理属性及音调模式。", "method": "本文提出了一种生成性潜在流匹配模型，可以预测未来的音频观察值，以使系统能够在机器人策略中考虑长期后果。", "result": "通过两个需要感知野外音频或音乐信号的操作任务的演示，证明了该系统的优越性能。这些结果表明，成功学习这些任务不仅依赖于多模态输入，而且关键在于准确地预测包含内在节奏模式的未来音频状态。", "conclusion": "本文方法展示了在机器人操作中利用未来的音频观察值进行推理的有效性，并强调了准确预测未来音频状态的重要性，这对于理解复杂的动态环境至关重要。"}}
{"id": "2512.08404", "pdf": "https://arxiv.org/pdf/2512.08404", "abs": "https://arxiv.org/abs/2512.08404", "authors": ["Sjoerd B. Stolwijk", "Mark Boukes", "Damian Trilling"], "title": "Are generative AI text annotations systematically biased?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 6 figures, 1 table; version submitted to the International Communication Association Annual Conference in Cape Town 2026", "summary": "This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.", "AI": {"tldr": "研究使用多种GLLM模型和不同提示词对五个概念的手动注释进行复制，以评估偏见。", "motivation": "探索GLLM在文本注释中的系统性偏差，比较其与手动注释的差异并评估其准确性。", "method": "利用五种不同的GLLM模型和五个不同概念的提示词生成注释，并将其与Boukes（2024）的手动注释进行对比。", "result": "发现GLLM在F1分数方面表现良好，但与手动注释存在显著差异；且其一致性高于与人工注释的一致性。", "conclusion": "研究揭示了GLLM在文本注释中系统性的偏差问题，即使准确性指标（如F1分数）高，也无法全面反映偏见程度。"}}
{"id": "2512.08403", "pdf": "https://arxiv.org/pdf/2512.08403", "abs": "https://arxiv.org/abs/2512.08403", "authors": ["Yupei Li", "Li Wang", "Yuxiang Wang", "Lei Wang", "Rizhao Cai", "Jie Shi", "Björn W. Schuller", "Zhizheng Wu"], "title": "DFALLM: Achieving Generalizable Multitask Deepfake Detection by Optimizing Audio LLM Components", "categories": ["cs.SD"], "comment": null, "summary": "Audio deepfake detection has recently garnered public concern due to its implications for security and reliability. Traditional deep learning methods have been widely applied to this task but often lack generalisability when confronted with newly emerging spoofing techniques and more tasks such as spoof attribution recognition rather than simple binary classification. In principle, Large Language Models (LLMs) are considered to possess the needed generalisation capabilities. However, previous research on Audio LLMs (ALLMs) indicates a generalization bottleneck in audio deepfake detection performance, even when sufficient data is available. Consequently, this study investigates the model architecture and examines the effects of the primary components of ALLMs, namely the audio encoder and the text-based LLM. Our experiments demonstrate that the careful selection and combination of audio encoders and text-based LLMs are crucial for unlocking the deepfake detection potential of ALLMs. We further propose an ALLM structure capable of generalizing deepfake detection abilities to out-of-domain spoofing tests and other deepfake tasks, such as spoof positioning and spoof attribution recognition. Our proposed model architecture achieves state-of-the-art (SOTA) performance across multiple datasets, including ASVSpoof2019, InTheWild, and Demopage, with accuracy reaching up to 95.76% on average, and exhibits competitive capabilities in other deepfake detection tasks such as attribution, and localisation compared to SOTA audio understanding models. Data and codes are provided in supplementary materials.", "AI": {"tldr": "DFALLM模型通过优化音频LLM组件，实现了跨域的深度伪造检测能力，并在多个数据集上达到SOTA性能。", "motivation": "传统的深度学习方法在应对新的伪造技术及更复杂的任务时缺乏泛化能力。因此，研究提出了一种新型架构来解决这一问题，以提高模型对音频深度伪造的通用性。", "method": "实验表明精心选择和组合音频编码器与文本LLM是提升ALLM性能的关键。团队提出了一个能够进行跨域深度伪造检测以及定位、归因识别等任务的结构。", "result": "在ASVSpoof2019，InTheWild及Demopage等多个数据集上，DFALLM模型实现了最高达95.76%的准确率，并展示了与其他SOTA音频理解模型相比的竞争能力。", "conclusion": "研究提出的DFALLM架构证明了优化音频LLM组件可以显著提升深度伪造检测任务中的泛化性能，为该领域提供了新的方法。"}}
{"id": "2512.08400", "pdf": "https://arxiv.org/pdf/2512.08400", "abs": "https://arxiv.org/abs/2512.08400", "authors": ["Samitha Nuwan Thilakarathna", "Ercan Avsar", "Martin Mathias Nielsen", "Malte Pedersen"], "title": "Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries", "categories": ["cs.CV"], "comment": "The paper has been accepted for publication at Northern Lights Deep Learning (NLDL) Conference 2025", "summary": "Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git", "AI": {"tldr": "开发了一种用于自动鱼类重新识别的优化深度学习管道，以解决渔业电子监控系统中的视频数据审查问题。", "motivation": "精确的渔业数据对于有效的海洋资源管理至关重要。随着电子监控系统的采用，产生了大量的无法手动审查的视频数据，因此需要自动化解决方案来处理这些数据。", "method": "使用AutoFish数据集开发了一种优化的深度学习管道进行鱼类重新识别。该方法包括硬三元组挖掘和定制图像转换流水线，并展示了Vision Transformer（Swin-T）架构优于传统的卷积神经网络（ResNet-50）。", "result": "实验表明，通过使用硬三元组挖掘和特定数据集的归一化处理，在mAP@k和Rank-1准确率上取得了显著提高，分别达到了41.65%和90.43%，证明了Swin-T架构的有效性。", "conclusion": "研究成功地解决了渔业电子监控系统中的鱼类重新识别问题，特别是在区分同一物种内的个体方面。未来的工作将集中在进一步优化模型以处理视角变化的问题上。"}}
{"id": "2512.08397", "pdf": "https://arxiv.org/pdf/2512.08397", "abs": "https://arxiv.org/abs/2512.08397", "authors": ["Philipp Srock", "Juan E. Tapia", "Christoph Busch"], "title": "Detection of Digital Facial Retouching utilizing Face Beauty Information", "categories": ["cs.CV"], "comment": null, "summary": "Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.", "AI": {"tldr": "研究并分析了数字面部美化对美容评估算法的影响，以提高面部修饰检测的准确率。", "motivation": "由于面部修图在社交平台、广告和专业摄影中广泛应用，并可能影响生物识别系统的性能，因此需要开发一种方法来检测这些修改。", "method": "通过使用基于人工智能的不同特征提取技术来改进面部修图的检测，评估了不同特征抽取方式的有效性。", "result": "当攻击修图算法未知时，在单张图像检测场景下达到了1.1%的D-EER（等错误率）。", "conclusion": "利用面部美化信息可以有效提高对数字面部修饰的检测能力，并且在未知攻击类型的情况下仍能保持较高的准确率。"}}
{"id": "2512.08392", "pdf": "https://arxiv.org/pdf/2512.08392", "abs": "https://arxiv.org/abs/2512.08392", "authors": ["Frank Bauernöppel", "Jörg-Rüdiger Sack"], "title": "Finding All Bounded-Length Simple Cycles in a Directed Graphs - Revisited", "categories": ["cs.DS"], "comment": "11 pages, 9 figures", "summary": "In 2021, Gupta and Suzumura proposed a novel algorithm for enumerating all bounded-length simple cycles in directed graphs. In this work, we present concrete examples demonstrating that the proposed algorithm fails to enumerate certain valid cycles. Via these examples, we perform a detailed analysis pinpointing the specific points at which the proofs exhibit logical gaps. Furthermore, we propose a corrected formulation that resolves these issues while preserving the desirable property that the algorithm's computational complexity remains $O((c + 1) \\cdot k \\cdot (n + e))$ where $c$ is the number of simple cycles of a specified maximum length $k$, and $n$ and $e$ the number of graph nodes and edges respectively.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.08379", "pdf": "https://arxiv.org/pdf/2512.08379", "abs": "https://arxiv.org/abs/2512.08379", "authors": ["Kaiwei Liu", "Yuting He", "Bufang Yang", "Mu Yuan", "Chun Man Victor Wong", "Ho Pong Andrew Sze", "Zhenyu Yan", "Hongkai Chen"], "title": "DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals", "categories": ["cs.AI"], "comment": null, "summary": "Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.", "AI": {"tldr": "提出了一种基于LLM的上下文感知特征生成框架DeepFeature，用于可穿戴生物信号。", "motivation": "现有特征提取方法缺乏任务特定的上下文知识，在高维特征空间中难以确定最优设置，并且容易出现代码生成和自动化错误。", "method": "引入多源特征生成机制结合专家知识与任务设定；采用基于反馈的迭代特征优化过程，利用功能评估进行重新选择；使用稳健的多层过滤和验证方法以确保无故障代码翻译。", "result": "实验结果表明，在八种不同的任务中，DeepFeature相较于基线方法平均AUCROC提升了4.21-9.67%，在五项任务中优于现有最佳方法，并保持了其余三项任务的相当性能。", "conclusion": "通过引入上下文感知和迭代优化机制，DeepFeature能够在可穿戴生物信号的应用场景中显著提升特征提取的效果。"}}
{"id": "2512.08378", "pdf": "https://arxiv.org/pdf/2512.08378", "abs": "https://arxiv.org/abs/2512.08378", "authors": ["Jing Tao", "You Li", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions", "categories": ["cs.CV"], "comment": "The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "summary": "Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.", "AI": {"tldr": "提出了一种在复杂光照条件下同时增强图像和抑制噪声的新框架。", "motivation": "现有的方法要么显著放大固有噪声，要么仅适用于特定的照明条件。因此，为了应对这些挑战并改善视觉应用的表现，研究者们提出了一个新的解决策略。", "method": "首先采用梯度域加权引导滤波器（GDWGIF）准确估计光照，并改进图像质量；其次利用Retinex模型分解捕获的图像为独立的照明层和反射层，在这两层中分别进行平行处理，优化光照条件并提高图像质量；最后通过多曝光融合以及线性拉伸策略优化了图像的动态范围。", "result": "实验结果显示提出的这种方法在对比度增强和噪声抑制方面比现有的最佳方法表现更好。", "conclusion": "提出的新框架能够在复杂照明条件下有效改善图像质量和去除噪声，显著提升了视觉应用的表现。"}}
{"id": "2512.08376", "pdf": "https://arxiv.org/pdf/2512.08376", "abs": "https://arxiv.org/abs/2512.08376", "authors": ["Gunjan Kumar", "Yash Pote", "Jonathan Scarlett"], "title": "A Distribution Testing Approach to Clustering Distributions", "categories": ["cs.DS", "cs.IT", "math.ST", "stat.ML"], "comment": null, "summary": "We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\\varepsilon)$ (up to an $O(\\log k)$ factor) for all regimes.", "AI": {"tldr": "研究如何通过分布测试方法来划分给定的k个分布，找出其中相同的两个组并确定其分割。", "motivation": "探索在已知或未知群集分布的情况下，利用样本复杂度对两类分布进行有效划分的方法和理论基础。", "method": "建立了两种情况下分布聚类问题上的上界和下界：一种是其中一个集群的分布已知；另一种是两个都未知。通过分析确定了样本复杂性的依赖关系（域大小n、分布数量k、一个群集的大小r以及距离ε）。", "result": "结果表明，在所有情况下，所提出的算法在(n, k, r, ε)上达到紧致性(除了O(log k))。", "conclusion": "通过引入新的理论框架和样本复杂度分析，成功解决了分布聚类问题的挑战，并提供了实际应用中的参考方法。"}}
{"id": "2512.08374", "pdf": "https://arxiv.org/pdf/2512.08374", "abs": "https://arxiv.org/abs/2512.08374", "authors": ["Bozhou Li", "Xinda Xue", "Sihan Yang", "Yang Shi", "Xinlong Chen", "Yushuo Guan", "Yuanxing Zhang", "Wentao Zhang"], "title": "The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.", "AI": {"tldr": "本文探讨了多模态大型语言模型（MLLM）中视觉和文本令牌之间范数差异导致的问题，并提出了一种通过插入层归一化（LayerNorm）来解决该问题的方法。", "motivation": "论文指出，现有的多模态大型语言模型依赖于预规范架构，这导致了高范数的视觉令牌与低范数的文字令牌之间的不平衡，从而影响跨模式特征融合的有效性。这个问题在理论和实证上被证明是广泛存在的。", "method": "本文提出了一个简单的解决方案，在视觉投影器之后插入一层精心初始化的层归一化（LayerNorm），以强制执行范数对齐。", "result": "实验表明，该方法不仅提高了多模态基准测试的表现，还在仅基于文本的评估中表现出显著改善。", "conclusion": "论文得出结论，解决架构上的不平衡可以提高模型的整体性能。"}}
{"id": "2512.08366", "pdf": "https://arxiv.org/pdf/2512.08366", "abs": "https://arxiv.org/abs/2512.08366", "authors": ["Wentao Zhang", "Qunbo Wang", "Tao Zhang", "Junsheng Wu", "Hongping Gan", "Yang Liu", "Ling Dai", "Shizhuang Deng", "Shuntong Sun"], "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.", "AI": {"tldr": "本文提出了一种基于语言模型的代理决策框架DuSAR，该框架通过两种互补策略实现自适应推理，并在多个任务上表现出色。", "motivation": "现有的大语言模型代理通常依赖外部演示或检索增强规划，导致脆弱性和高计算开销。为了解决这些问题，本文提出了杜斯尔框架以模仿人类元认知行为进行决策。", "method": "DuSAR通过轻量级反思机制实现两种策略的交互：高层次的整体计划和基于上下文的地方政策。代理会根据进展评估策略适应性，并在遇到困难时调整全局计划或改进。", "result": "在ALFWorld和Mind2Web数据集上，杜斯尔达到了最先进的性能，Llama3.1-70B模型在ALFWorld上的成功率超过先前最佳结果一倍以上（达到37.1%），并且减少了每步的令牌消耗（减少3-9倍）。", "conclusion": "DuSAR通过灵活地整合外部知识增强了大语言模型代理的表现，并展示了其与现有技术相比的优势。"}}
{"id": "2512.08362", "pdf": "https://arxiv.org/pdf/2512.08362", "abs": "https://arxiv.org/abs/2512.08362", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Gun-Woo Kim"], "title": "SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation", "categories": ["cs.CV"], "comment": "Accepted for main track at MobieSec 2024 (not published in the proceedings)", "summary": "Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.", "AI": {"tldr": "提出SCU-CGAN模型，通过生成现实的火灾图像来增强火灾检测系统的性能。", "motivation": "由于缺乏足够的火灾数据集限制了检测模型的表现，作者提出了SCU-CGAN模型以提高火灾检测系统的准确率。", "method": "集成U-Net、CBAM和额外的判别器，从非火图像生成真实的火图像，并通过实验确认其优越性。增强的数据集提高了火灾检测模型的准确性而不改变结构。", "result": "SCU-CGAN在KID评分上比CycleGAN提高41.5%，YOLOv5 nano模型的mAP@0.5:0.95指标提高了56.5%。", "conclusion": "SCU-CGAN生成高质量的火图像，并通过数据集增强显著提升了火灾检测系统的准确率。"}}
{"id": "2512.08360", "pdf": "https://arxiv.org/pdf/2512.08360", "abs": "https://arxiv.org/abs/2512.08360", "authors": ["Ali Sakour"], "title": "Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "comment": "13 pages, 5 figures. Code available at: https://github.com/alisakour/Conditional-NCA-Digits", "summary": "Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.", "AI": {"tldr": "提出了一种条件神经细胞自动机（c-NCA）架构，用于生成特定类别的结构模式。", "motivation": "现有的神经细胞自动机研究主要集中在连续纹理合成或单目标对象恢复上，而缺乏对条件结构性生成的研究。本文旨在通过模仿生物系统的形态发生塑料性来解决这一问题。", "method": "设计了一种新型的c-NCA架构，该架构可以通过一个单一的通用种子生长出特定类别的结构，并且通过空间广播的一个类别向量引导其生长过程。", "result": "实验结果表明，所提出的c-NCA可以从单个像素开始稳定地收敛并正确形成数字拓扑结构，并展示了生物系统典型的稳健性特征。", "conclusion": "这项工作填补了基于纹理的NCA和结构性图案生成之间的空白，提供了一种轻量级且生物学上合理的条件生成替代方案。"}}
{"id": "2512.08358", "pdf": "https://arxiv.org/pdf/2512.08358", "abs": "https://arxiv.org/abs/2512.08358", "authors": ["Jiahao Lu", "Weitao Xiong", "Jiacheng Deng", "Peng Li", "Tianyu Huang", "Zhiyang Dou", "Cheng Lin", "Sai-Kit Yeung", "Yuan Liu"], "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025. Project Page: https://igl-hkust.github.io/TrackingWorld.github.io/", "summary": "Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.", "AI": {"tldr": "本文提出了一种新的单目三维跟踪方法TrackingWorld，旨在通过世界坐标系进行密集像素的长程三维运动捕捉。", "motivation": "现有的单目三维跟踪方法在分离摄像机运动和前景动态物体运动方面存在不足，并且无法追踪新出现的动态对象。为了改善这些问题，本文提出了一种新的管道。", "method": "首先引入了一个跟踪上采样器将任意稀疏二维轨迹提升为密集的二维轨迹；然后应用该上采样器于所有帧中减少重叠区域中的2D轨迹冗余；最后通过优化框架估计摄像机姿态和2D轨迹的世界坐标系3D坐标。", "result": "在合成数据集和真实世界数据集中进行了大量评估，证明了系统能够实现准确而密集的三维跟踪。", "conclusion": "该方法成功解决了现有单目三维追踪中的限制，并实现了在世界坐标框架内的准确且密集的三维跟踪。"}}
{"id": "2512.08350", "pdf": "https://arxiv.org/pdf/2512.08350", "abs": "https://arxiv.org/abs/2512.08350", "authors": ["Zeev Nutov"], "title": "A tight example for approximation ratio 5 for covering small cuts by the primal-dual method", "categories": ["cs.DS"], "comment": null, "summary": "In the Small Cuts Cover problem we seek to cover by a min-cost edge-set the set family of cuts of size/capacity $<k$ of a graph. Recently, Simmons showed that the primal-dual algorithm of Williamson, Goemans, Mihail, and Vazirani achieves approximation ratio $5$ for this problem, and asked whether this bound is tight. We will answer this question positively, by providing an example in which the ratio between the solution produced by the primal-dual algorithm and the optimum is arbitrarily close to $5$.", "AI": {"tldr": "提供了关于覆盖小割集问题中近似比为5的紧致实例。", "motivation": "回答Simmons提出的疑问，即Williamson等人的原始对偶算法是否达到近似比5的下界。", "method": "构造了一个实例来展示在该实例上原始对偶算法产生的解与最优解之间的比率接近5。", "result": "证明了原始对偶算法在覆盖小割集问题中的近似比为5是紧致的。", "conclusion": "验证了Simmons的问题，即Williamson等人的方法达到的近似比5确实是该问题的一个下界。"}}
{"id": "2512.08345", "pdf": "https://arxiv.org/pdf/2512.08345", "abs": "https://arxiv.org/abs/2512.08345", "authors": ["Benedikt Mangold"], "title": "The High Cost of Incivility: Quantifying Interaction Inefficiency via Multi-Agent Monte Carlo Simulations", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "comment": "8 figures, 3 tables", "summary": "Workplace toxicity is widely recognized as detrimental to organizational culture, yet quantifying its direct impact on operational efficiency remains methodologically challenging due to the ethical and practical difficulties of reproducing conflict in human subjects. This study leverages Large Language Model (LLM) based Multi-Agent Systems to simulate 1-on-1 adversarial debates, creating a controlled \"sociological sandbox\". We employ a Monte Carlo method to simulate hundrets of discussions, measuring the convergence time (defined as the number of arguments required to reach a conclusion) between a baseline control group and treatment groups involving agents with \"toxic\" system prompts. Our results demonstrate a statistically significant increase of approximately 25\\% in the duration of conversations involving toxic participants. We propose that this \"latency of toxicity\" serves as a proxy for financial damage in corporate and academic settings. Furthermore, we demonstrate that agent-based modeling provides a reproducible, ethical alternative to human-subject research for measuring the mechanics of social friction.", "AI": {"tldr": "通过模拟对抗性辩论，研究工作场所毒性对交互效率的影响。", "motivation": "量化工作场所毒性直接对操作效率影响的方法论挑战大，本研究旨在提供一种可重复、道德的替代方案来测量社会摩擦机制。", "method": "利用基于大型语言模型（LLM）的多智能体系统模拟一对一对抗性辩论，并通过蒙特卡洛方法进行数百次对话仿真，比较有“毒性”的参与者与基线对照组之间的互动效率。", "result": "研究结果显示，在包含有毒参与者的讨论中，会话持续时间平均增加约25%。", "conclusion": "该实验表明，“毒性滞后”可以作为工作场所财务损失的代理指标，并建议智能体建模是一种测量社会摩擦的有效方法。"}}
{"id": "2512.08344", "pdf": "https://arxiv.org/pdf/2512.08344", "abs": "https://arxiv.org/abs/2512.08344", "authors": ["Tien Cuong Bui"], "title": "Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions", "categories": ["cs.AI", "cs.IT", "cs.LG"], "comment": "157 pages, Doctoral dissertation at Seoul National University (submitted in 2024.08 to SNU library, slightly updated in 2025.11 for open digital version)", "summary": "Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.", "AI": {"tldr": "该论文旨在为图神经网络开发一种新的可解释性框架，以提供适应性强且计算效率高的解释。", "motivation": "现有的XAI方法难以解开复杂数据中的图结构关系和交互，而图神经网络的决策过程也因模型复杂度高而不易理解。为此，论文寻求通过新方法来提升GNN的可解释性。", "method": "提出一种新的XAI框架，该框架不仅适应性强且计算效率高，并且能够捕捉到图结构如何影响预测结果。", "result": "论文提出了一个新的XAI框架，但具体实现和实验结果未在摘要中说明。", "conclusion": "通过新方法，可以更好地理解图神经网络的决策过程，从而提高其可解释性。"}}
{"id": "2512.08343", "pdf": "https://arxiv.org/pdf/2512.08343", "abs": "https://arxiv.org/abs/2512.08343", "authors": ["Caner Erden", "Alparslan Serhat Demir", "Abdullah Hulusi Kokcam", "Talas Fikret Kurnaz", "Ugur Dagdeviren"], "title": "Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach", "categories": ["cs.AI"], "comment": "Presented at the 13th International Symposium on Intelligent Manufacturing and Service Systems, Duzce, Turkey, Sep 25-27, 2025. Also available on Zenodo: DOI 10.5281/zenodo.17533851", "summary": "Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.", "AI": {"tldr": "基于自动化机器学习方法预测土壤压实参数。", "motivation": "传统实验室试验耗时费力，而现有机器学习模型在异质数据集上的准确性和泛化能力有限。因此，研究提出一种自动机器学习（AutoML）方法来提高预测的准确性和适用性。", "method": "自动化选择算法和超参数优化，通过广泛的实验确定了XGBoost算法最佳性能，实现了MDD为80.4%、OMC为89.1%的R-squared值。", "result": "该方法在不同土壤类型上表现出良好的预测效果，并强调异质数据集的重要性以提高机器学习模型的泛化能力和表现。", "conclusion": "通过自动化的机器学习技术，本研究提高了土木工程中压实参数预测的效率和可靠性。"}}
{"id": "2512.08340", "pdf": "https://arxiv.org/pdf/2512.08340", "abs": "https://arxiv.org/abs/2512.08340", "authors": ["Abdullah Hulusi Kökçam", "Uğur Dağdeviren", "Talas Fikret Kurnaz", "Alparslan Serhat Demir", "Caner Erden"], "title": "Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye", "categories": ["cs.AI", "cs.LG"], "comment": "Presented at the 13th International Symposium on Intelligent Manufacturing and Service Systems, Duzce, Turkey, Sep 25-27, 2025. Also available on Zenodo: DOI 10.5281/zenodo.17530868", "summary": "The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.", "AI": {"tldr": "利用机器学习方法预测加州承载比(CBR)，以替代耗时和昂贵的传统实验室测试。", "motivation": "传统的CBR测定依赖于耗时且成本高昂的实验，难以应对大规模或多样化的土壤样本。因此，本研究旨在探索基于机器学习的数据驱动模型来提高效率和准确性。", "method": "采用包含382个样本的数据集进行训练、验证和测试。试验了包括决策树、随机森林等在内的12种机器学习算法，并通过交叉验证评估其性能。", "result": "在所有模型中，随机森林回归器表现最佳，在训练、验证和测试阶段分别取得了0.95、0.76和0.83的R²评分。这些结果展示了该模型强大的非线性映射能力。", "conclusion": "研究证明了基于数据集智能模型在土木工程中的应用潜力，为基础设施分析和设计提供了有效的替代方法，并促进了数字化转型。"}}
{"id": "2512.08337", "pdf": "https://arxiv.org/pdf/2512.08337", "abs": "https://arxiv.org/abs/2512.08337", "authors": ["Jianwei Wang", "Qing Wang", "Menglan Ruan", "Rongjun Ge", "Chunfeng Yang", "Yang Chen", "Chunming Xie"], "title": "DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.", "AI": {"tldr": "该论文提出了DINO-BOLDNet，一种从T1w图像生成BOLD图像的多切片注意力网络。", "motivation": "缺少或损坏的BOLD图像妨碍了许多下游任务。因此，提出了一种基于DINOv3的框架来恢复缺失的BOLD信息。", "method": "该模型集成了冻结的自监督DINOv3编码器和轻量级可训练解码器，并使用切片注意模块融合相邻切片的信息，以生成精细的功能对比图像。", "result": "在248个受试者的临床数据集中实验显示，与条件GAN基线相比，在PSNR和MS-SSIM方面均有提升。", "conclusion": "这是首个能够直接从T1w图像生成平均BOLD图像的框架，展示了自监督变压器指导在结构到功能映射中的潜力。"}}
{"id": "2512.08334", "pdf": "https://arxiv.org/pdf/2512.08334", "abs": "https://arxiv.org/abs/2512.08334", "authors": ["Chang Liu", "Hongliang Yuan", "Lianghao Zhang", "Sichao Wang", "Jianwei Guo", "Shi-Sheng Huang"], "title": "HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.", "AI": {"tldr": "本文提出了一种新的混合光斑技术（HybridSplat），用于提高具有复杂反射场景的渲染速度和存储效率。", "motivation": "目前使用3D高斯光斑进行真实世界场景中的复杂反射渲染虽然效果不错，但仍面临速度瓶颈和内存消耗大的问题。因此本文旨在通过新方法解决这些挑战。", "method": "提出了一种新的反射烘焙高斯追踪技术，该技术在每个高斯原语中烘焙视图依赖性反射，并使用基于瓷砖的高斯光斑进行反射渲染。此外还引入了混合光斑级别的加速和对反射敏感的高斯修剪以减少模型大小。", "result": "实验表明，与类似的光线追踪基线相比，HybridSplat在复杂反射场景下可以提高约7倍的渲染速度，并且使用更少（4倍）的高斯原语数量。", "conclusion": "本文提出的HybridSplat技术不仅提高了渲染速度和降低了内存需求，而且保持了高质量的反射效果。"}}
{"id": "2512.08333", "pdf": "https://arxiv.org/pdf/2512.08333", "abs": "https://arxiv.org/abs/2512.08333", "authors": ["Yajat Yadav", "Zhiyuan Zhou", "Andrew Wagenmaker", "Karl Pertsch", "Sergey Levine"], "title": "Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generalist robot policies, trained on large and diverse datasets, have demonstrated the ability to generalize across a wide spectrum of behaviors, enabling a single policy to act in varied real-world environments. However, they still fall short on new tasks not covered in the training data. When finetuned on limited demonstrations of a new task, these policies often overfit to the specific demonstrations--not only losing their prior abilities to solve a wide variety of generalist tasks but also failing to generalize within the new task itself. In this work, we aim to develop a method that preserves the generalization capabilities of the generalist policy during finetuning, allowing a single policy to robustly incorporate a new skill into its repertoire. Our goal is a single policy that both learns to generalize to variations of the new task and retains the broad competencies gained from pretraining. We show that this can be achieved through a simple yet effective strategy: interpolating the weights of a finetuned model with that of the pretrained model. We show, across extensive simulated and real-world experiments, that such model merging produces a single model that inherits the generalist abilities of the base model and learns to solve the new task robustly, outperforming both the pretrained and finetuned model on out-of-distribution variations of the new task. Moreover, we show that model merging enables continual acquisition of new skills in a lifelong learning setting, without sacrificing previously learned generalist abilities.", "AI": {"tldr": "本文提出了一种通过参数融合来增强视觉-语言-行动机器人策略鲁棒性的方法，该方法在细化训练新任务时能够保持模型的泛化能力。", "motivation": "现有的通用型机器人策略虽然能在广泛的行为上进行泛化，但在面对未见过的新任务时往往会出现过拟合的问题，并丧失了其原有的泛化能力。本文旨在开发一种方法，在精细化训练过程中保护这些策略的泛化性能。", "method": "通过将细化模型权重与预训练模型权重相融合的方法来实现这一目标。", "result": "该方法能够在多种模拟和真实世界实验中表现优秀，生成一个既能继承基础模型广泛能力又能解决新任务的单一模型，并且在处理新任务的未见过变体时超越了仅使用预训练或细化模型的表现。此外，在终身学习环境中，这种方法能够不断获取新的技能而不会牺牲先前学到的泛化能力。", "conclusion": "通过简单的参数融合策略，可以有效地保持机器人策略的泛化能力和适应性，并且在新任务上表现出色。"}}
{"id": "2512.08331", "pdf": "https://arxiv.org/pdf/2512.08331", "abs": "https://arxiv.org/abs/2512.08331", "authors": ["Xianghong Xiao", "Zeyu Xia", "Zhou Fei", "Jinliang Xiao", "Haorui Chen", "Liangjian Deng"], "title": "Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.", "AI": {"tldr": "提出了一种双模态双向掩膜感知卷积方法（Bi^2MAC），用于提高遥感图像融合的精确性和效率。", "motivation": "现有深度学习方法在处理区域异质性方面存在不足，适应性卷积方法虽然有所改进但计算成本高、难以有效捕捉异质区。因此需要一种既能充分利用不同类型地区信息又能合理分配计算资源的方法。", "method": "设计了一种轻量级模块生成软硬掩膜，用于初步调节输入特征，并引导不同类型区域进入不同处理分支；冗余特征被导向一个紧凑的低耗全局处理分支，而异质特征则通过集中处理分支进行细粒度建模。", "result": "在多个基准数据集上的实验表明Bi^2MAC达到了最先进的性能且训练时间短、参数计数少及计算成本低。", "conclusion": "Bi^2MAC能够有效提高遥感图像融合的精确性和效率，同时大幅降低计算资源需求。"}}
{"id": "2512.08330", "pdf": "https://arxiv.org/pdf/2512.08330", "abs": "https://arxiv.org/abs/2512.08330", "authors": ["Pengbo Li", "Yiding Sun", "Haozhe Cheng"], "title": "PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \\textit{PointDico}, a novel model that seamlessly integrates these methods. \\textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \\textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \\textit{e.g.}, \\textbf{94.32\\%} accuracy on ScanObjectNN, \\textbf{86.5\\%} Inst. mIoU on ShapeNetPart.", "AI": {"tldr": "本文提出了一种新的用于三维表示学习的模型PointDico，通过融合扩散和对比方法的优势，提高了点云数据的学习效果。", "motivation": "现有的自监督表示学习方法在处理无序且密度不均匀的3D数据时面临挑战。对比模型容易过拟合，而3D Mask Autoencoders难以有效应对无序点云。本文旨在结合扩散与对比学习的优点来克服这些问题。", "method": "PointDico通过知识蒸馏同时从去噪生成建模和跨模式对比学习中学习。它使用分层金字塔条件生成器提取多尺度几何特征，并采用双通道设计整合局部和全局上下文信息。", "result": "在ScanObjectNN上实现了94.32％的准确率，在ShapeNetPart上达到了86.5%的实例mIoU，这是三维表示学习的新记录。", "conclusion": "通过无缝融合对比与扩散模型的优点，PointDico为解决无序点云数据的学习难题提供了一种有效的方法。"}}
{"id": "2512.08329", "pdf": "https://arxiv.org/pdf/2512.08329", "abs": "https://arxiv.org/abs/2512.08329", "authors": ["Michael R. Martin", "Garrick Chan", "Kwan-Liu Ma"], "title": "Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "32 pages, 17 figures, 1 table, 5 algorithms, preprint", "summary": "Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.", "AI": {"tldr": "本文通过统一框架，利用白盒特征空间检查和黑盒信号级探测对图像保护机制进行系统性可解释AI分析。", "motivation": "当前的图像保护方法如Glaze和Nightshade虽然有效但其内部结构、检测性和表示行为未被充分理解。研究旨在提高这些保护机制的理解度，为未来的防御设计提供信息。", "method": "该研究采用白盒特征空间检查和黑盒信号级探测相结合的方法，通过潜在空间聚类、通道激活分析、基于遮挡的空间敏感性映射及频率域表征来解析图像保护机制的运作原理。", "result": "研究表明保护机制作为结构化低熵扰动紧密地与底层图像内容联系，在表示、空间和频谱领域保持保护特定子结构；保护信号由扰动熵、空间部署和频谱对齐的相互作用决定，且频率域分析表明这些方法沿主导图像对准频轴重新分布能量。", "conclusion": "当前的图像保护通过特征级变形而非语义错位进行操作，解释了为什么即使视觉上细微但依然可被检测到的原因。这项工作提高了对抗性图像防护的理解，并为未来防御和生成AI系统的检测策略提供指导。"}}
{"id": "2512.08327", "pdf": "https://arxiv.org/pdf/2512.08327", "abs": "https://arxiv.org/abs/2512.08327", "authors": ["Wang Chen", "Ziyan Luo", "Shuangyue Wang"], "title": "Low Rank Support Quaternion Matrix Machine", "categories": ["cs.CV", "cs.LG", "math.OC", "stat.ML"], "comment": null, "summary": "Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.", "AI": {"tldr": "提出了低秩支持四元矩阵机（LSQMM）方法，用于彩色图像分类。", "motivation": "为了有效保留RGB通道间的内在耦合关系，提出了一种新的基于四元数的分类方法，以提高分类准确性和鲁棒性，并在计算效率上优于传统方法。", "method": "将RGB通道视为纯四元数，引入四元核范数正则项来促进低秩结构。设计了交替方向乘子法（ADMM）迭代算法解决提出的四元优化模型。", "result": "实验结果表明，在多个彩色图像分类数据集上，所提方法在准确率、鲁棒性和计算效率方面优于若干先进的支持向量机、矩阵机和张量机方法。", "conclusion": "LSQMM方法能够有效提高彩色图像分类的准确性、鲁棒性，并具有较高的计算效率。"}}
{"id": "2512.08326", "pdf": "https://arxiv.org/pdf/2512.08326", "abs": "https://arxiv.org/abs/2512.08326", "authors": ["Bin Wang", "Hui Li", "Liyang Zhang", "Qijia Zhuang", "Ao Yang", "Dong Zhang", "Xijun Luo", "Bing Lin"], "title": "Argus: A Multi-Agent Sensitive Information Leakage Detection Framework Based on Hierarchical Reference Relationships", "categories": ["cs.CR", "cs.AI"], "comment": "11 pages, 7 figures, 8 tables;Accepted to ICSE 2026 Research Track", "summary": "Sensitive information leakage in code repositories has emerged as a critical security challenge. Traditional detection methods that rely on regular expressions, fingerprint features, and high-entropy calculations often suffer from high false-positive rates. This not only reduces detection efficiency but also significantly increases the manual screening burden on developers. Recent advances in large language models (LLMs) and multi-agent collaborative architectures have demonstrated remarkable potential for tackling complex tasks, offering a novel technological perspective for sensitive information detection. In response to these challenges, we propose Argus, a multi-agent collaborative framework for detecting sensitive information. Argus employs a three-tier detection mechanism that integrates key content, file context, and project reference relationships to effectively reduce false positives and enhance overall detection accuracy. To comprehensively evaluate Argus in real-world repository environments, we developed two new benchmarks, one to assess genuine leak detection capabilities and another to evaluate false-positive filtering performance. Experimental results show that Argus achieves up to 94.86% accuracy in leak detection, with a precision of 96.36%, recall of 94.64%, and an F1 score of 0.955. Moreover, the analysis of 97 real repositories incurred a total cost of only 2.2$. All code implementations and related datasets are publicly available at https://github.com/TheBinKing/Argus-Guard for further research and application.", "AI": {"tldr": "Argus是一种基于分层引用关系的多代理敏感信息泄漏检测框架，旨在提高代码仓库中敏感信息泄露检测的准确性。", "motivation": "传统的方法依赖于正则表达式、指纹特征和高熵计算，导致高误报率。为了减少人工审查负担并提升效率，本文提出了一种新的方法来应对这一挑战。", "method": "Argus采用三级检测机制，包括关键内容检测、文件上下文分析以及项目引用关系的整合，以此降低误报率并提高总体检测准确性。", "result": "实验结果显示，在真实仓库环境中的性能评估中，Argus实现了高达94.86%的准确度，具有96.36%的精确性、94.64%的召回率和0.955的F1值。同时，分析了97个实际项目，总成本仅为2.2美元。", "conclusion": "Argus框架成功地减少了敏感信息泄漏检测中的误报情况，并且具有较高的准确性和较低的成本效益。相关代码实现及数据集已在GitHub上公开共享以供进一步研究和应用。"}}
{"id": "2512.08325", "pdf": "https://arxiv.org/pdf/2512.08325", "abs": "https://arxiv.org/abs/2512.08325", "authors": ["Xuedeng Liu", "Jiabao Guo", "Zheng Zhang", "Fei Wang", "Zhi Liu", "Dan Guo"], "title": "GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification", "categories": ["cs.CV"], "comment": null, "summary": "Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.", "AI": {"tldr": "提出了一种基于扩散的拉格朗日视频运动放大框架GeoDiffMM，该框架利用光流作为几何线索进行条件化处理。", "motivation": "现有的主流欧拉方法在分离真微小运动和光子噪声方面存在困难，特别是在运动位移非常小时。", "method": "设计了一种无噪光学流增强策略，合成多样化的非刚性运动场以监督模型；开发了扩散运动放大器，条件化于几何先验的光流和可学习的放大因子；进行基于流的视频合成将放大后的运动映射回图像域。", "result": "实验证明GeoDiffMM优于现有方法，显著提高了运动放大的效果。", "conclusion": "通过引入扩散模型并利用光流作为几何线索，可以实现结构一致且精确的微小运动放大。"}}
{"id": "2512.08323", "pdf": "https://arxiv.org/pdf/2512.08323", "abs": "https://arxiv.org/abs/2512.08323", "authors": ["Achraf Ben-Hamadou", "Nour Neifar", "Ahmed Rekik", "Oussama Smaoui", "Firas Bouzguenda", "Sergi Pujades", "Niels van Nistelrooij", "Shankeeth Vinayahalingam", "Kaibo Shi", "Hairong Jin", "Youyi Zheng", "Tibor Kubík", "Oldřich Kodym", "Petr Šilling", "Kateřina Trávníčková", "Tomáš Mojžiš", "Jan Matula", "Jeffry Hartanto", "Xiaoying Zhu", "Kim-Ngan Nguyen", "Tudor Dascalu", "Huikai Wu", "and Weijie Liu", "Shaojie Zhuang", "Guangshun Wei", "et al. (1 additional authors not shown)"], "title": "Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge", "categories": ["cs.CV"], "comment": "MICCAI 2024, 3DTeethLand, Challenge report, under review", "summary": "Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.", "AI": {"tldr": "该研究旨在检测口腔内3D扫描图像中的牙齿关键点，解决现代正畸临床中精确识别的问题。", "motivation": "精确识别牙齿关键点对于高级诊断、个性化治疗策略和有效监测治疗进展至关重要。然而，由于个体牙齿的复杂几何形状和个人间的变化，这一任务极具挑战性。为此，通过深度学习的应用开发先进方法是必不可少的。", "method": "在2024年国际医学影像计算与计算机辅助干预会议（MICCAI）中举办了3DTeethLand挑战赛，旨在推动针对口腔内3D扫描图像牙齿关键点检测算法的发展，并提供了首个公开可用的数据集以评估现有的方法和技术。", "result": "该挑战引入了一个新的数据集来评估和推进牙科三维标志物检测领域的状态，为临床应用提供了重要资源和贡献。", "conclusion": "通过该研究及其相关的3DTeethLand挑战赛，推动了牙齿关键点检测技术的发展，并鼓励社区在这一具有重大临床意义的问题上提供方法上的创新。"}}
{"id": "2512.08317", "pdf": "https://arxiv.org/pdf/2512.08317", "abs": "https://arxiv.org/abs/2512.08317", "authors": ["Xuhui Li", "Zhengquan Luo", "Zihui Cui", "Zhiqiang Xu"], "title": "GeoDM: Geometry-aware Distribution Matching for Dataset Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \\textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.", "AI": {"tldr": "提出了一种基于几何感知的分布匹配框架GeoDM，用于数据集提炼。", "motivation": "现有的分布匹配方法局限于欧氏空间，无法捕捉真实数据中的非线性结构和内在几何特性。因此需要一种新的方法来适应不同类型的几何结构。", "method": "引入了在欧氏、双曲和平行空间中操作的框架GeoDM，并设计了一个最优传输损失函数以增强分布匹配的精度。", "result": "实验结果表明，该算法优于当前最先进的数据集提炼方法，在各种单几何分布匹配策略下仍表现出色。", "conclusion": "提出了一种新的基于几何感知的数据集提炼方法，可以更好地捕捉和适应真实数据中的复杂几何结构。"}}
{"id": "2512.08309", "pdf": "https://arxiv.org/pdf/2512.08309", "abs": "https://arxiv.org/abs/2512.08309", "authors": ["Alexander Goslin"], "title": "Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Project website: https://xandergos.github.io/terrain-diffusion/ Code: https://github.com/xandergos/terrain-diffusion/", "summary": "For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.", "AI": {"tldr": "提出了一种基于扩散模型的地形生成方法，用于实现实时无限扩展且具有高度一致性和可控性的地貌。", "motivation": "传统的Perlin噪声函数虽然快速、无限可扩展但缺乏真实感和大范围的一致性。因此，引入了Terrain Diffusion来克服这些限制，结合了扩散模型的高保真度与传统生成方法的优点。", "method": "采用InfiniteDiffusion算法实现无缝实时合成无边界地貌；通过分层扩散模型结合宏观背景和局部细节，并使用Laplacian编码稳定大范围变化。开发了一个开源无限张量框架支持常数内存操作，以及快速一致性的提取以提高生成效率。", "result": "成功实现了整个星球级一致性、可控性且无限制的地貌合成。", "conclusion": "Terrain Diffusion确立了扩散模型作为程序化世界生成基础的可行性，并能够综合、可控地合成完整的行星地貌。"}}
{"id": "2512.08300", "pdf": "https://arxiv.org/pdf/2512.08300", "abs": "https://arxiv.org/abs/2512.08300", "authors": ["Sijia Chen", "Baochun Li", "Di Niu"], "title": "rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection", "categories": ["cs.AI"], "comment": "14 pages, 6 figures. Accepted to the ACL ARR July", "summary": "Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.", "AI": {"tldr": "提出了一种新的增强策略注入机制（rSIM），使大型语言模型成为具备推理能力的模型。", "motivation": "为了激发大型语言模型的推理能力，通过引入一种小规模规划器来引导其思维链中的策略执行。", "method": "采用多智能体强化学习框架，在领导者和跟随者之间联合训练一个小型规划器与大型语言模型，通过规则奖励系统指导规划。", "result": "实验表明，rSIM使Qwen2.5-0.5B成为了具备推理能力的模型，并且显著优于Qwen2.5-14B。此外，该规划器具有通用性，可以作为插件增强现有大型语言模型的推理能力。", "conclusion": "通过提出rSIM机制和多智能体强化学习框架，成功地将大型语言模型转变为具备高级推理功能的模型，并展示了其在不同任务中的适应性和持续学习能力。"}}
{"id": "2512.08296", "pdf": "https://arxiv.org/pdf/2512.08296", "abs": "https://arxiv.org/abs/2512.08296", "authors": ["Yubin Kim", "Ken Gu", "Chanwoo Park", "Chunjong Park", "Samuel Schmidgall", "A. Ali Heydari", "Yao Yan", "Zhihan Zhang", "Yuchen Zhuang", "Mark Malhotra", "Paul Pu Liang", "Hae Won Park", "Yuzhe Yang", "Xuhai Xu", "Yilun Du", "Shwetak Patel", "Tim Althoff", "Daniel McDuff", "Xin Liu"], "title": "Towards a Science of Scaling Agent Systems", "categories": ["cs.AI"], "comment": null, "summary": "Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.", "AI": {"tldr": "研究提出了一套衡量和预测代理系统性能的定量扩展原则。", "motivation": "现有代理系统的性能原理尚待探索，实际应用中缺乏基于原则的设计选择。", "method": "通过四个基准测试评估五种典型架构在不同大规模语言模型家族中的表现，并使用标准化工具与令牌预算进行控制实验。", "result": "确立了三个主要效应：工效和协调之间的权衡、能力饱和现象以及错误放大依赖于拓扑结构，构建了一个预测模型（R^2=0.513），可准确预测87%的未见配置下的最优策略。", "conclusion": "研究揭示了代理系统性能的关键因素并提供了基于任务属性的预测性原则。"}}
{"id": "2512.08294", "pdf": "https://arxiv.org/pdf/2512.08294", "abs": "https://arxiv.org/abs/2512.08294", "authors": ["Yexin Liu", "Manyuan Zhang", "Yueze Wang", "Hongyu Li", "Dian Zheng", "Weiming Zhang", "Changsheng Lu", "Xunliang Cai", "Yan Feng", "Peng Pei", "Harry Yang"], "title": "OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.", "AI": {"tldr": "该论文提出了一种利用视频数据的大型语料库OpenSubject，用于主体驱动图像生成和操作。", "motivation": "现有模型在复杂场景中的参考身份偏差问题及难以处理多主体的问题驱使作者开发新的解决方案。", "method": "通过四阶段流程创建大规模视频衍生语料库：视频筛选、跨帧主题挖掘配对、基于分割图的图像合成以及验证和标注。", "result": "实验表明，使用OpenSubject进行训练可以改善生成和操作性能，特别是在复杂场景中表现更好。", "conclusion": "提出的方法提升了主体驱动图像生成与操作的质量，并为相关任务设定了新的基准。"}}
{"id": "2512.08290", "pdf": "https://arxiv.org/pdf/2512.08290", "abs": "https://arxiv.org/abs/2512.08290", "authors": ["Shiva Gaire", "Srijan Gyawali", "Saroj Mishra", "Suman Niroula", "Dilip Thakur", "Umesh Yadav"], "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem", "categories": ["cs.CR", "cs.AI"], "comment": "All authors contributed equally to this work", "summary": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.", "AI": {"tldr": "该论文系统化研究了在模型上下文协议生态系统中的安全性和可靠性问题，尤其是如何防止上下文被恶意利用来触发未经授权的操作。", "motivation": "随着模型上下文协议（MCP）成为连接大型语言模型与外部数据和工具的标准方式，它引入了一个新的威胁环境。论文旨在区分知识系统中的对抗性安全威胁和认识上安全性隐患，并提供一个风险的综合分类表。", "method": "论文分析了MCP原语中的结构脆弱性，特别是资源、提示和工具，讨论了上下文如何被武器化来触发未经授权的操作。此外还对最先进的防御技术进行了调查。", "result": "论文提供了关于MCP生态系统中安全性和可靠性问题的深入见解，并展示了当前最先进技术的状态。", "conclusion": "研究总结了一个保护从对话聊天机器人到自主代理操作系统过渡的安全道路图。"}}
{"id": "2512.08286", "pdf": "https://arxiv.org/pdf/2512.08286", "abs": "https://arxiv.org/abs/2512.08286", "authors": ["Liao Hu", "Qiteng Wu", "Ruoyu Qi"], "title": "Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "ef:Advances in Engineering Innovation 16.7 (2025): 86-92", "summary": "The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.", "AI": {"tldr": "本文介绍了一种名为SolidGPT的边缘云混合AI代理框架，旨在解决大型语言模型在移动和软件开发工作流中的语义意识、开发者生产力与数据隐私之间的冲突。", "motivation": "传统的基于云端的工具提供强大的推理能力，但存在数据暴露风险及延迟问题；而设备上的解决方案则缺乏跨代码库和开发工具的整体理解。SolidGPT通过结合边缘计算和云计算的优势，在确保数据私密性的同时提高开发者生产力。", "method": "SolidGPT是一个开源框架，集成在GitHub中，用于增强代码和工作区的语义搜索功能。它允许用户与代码库交互查询、自动生成项目文档及任务管理，并支持私人代理配置。", "result": "通过结合互动式代码查询、自动化的项目脚手架搭建以及人机协作模式，SolidGPT实现了实践上可操作且尊重隐私的边缘辅助工具，从而加速了真实世界的开发工作流程。", "conclusion": "SolidGPT提供了一个实用且尊重数据私密性的边缘助手解决方案，非常适合智能移动和软件工程场景中的实际应用。"}}
{"id": "2512.08284", "pdf": "https://arxiv.org/pdf/2512.08284", "abs": "https://arxiv.org/abs/2512.08284", "authors": ["Guangyuan Zou", "Junlun Li", "Feng Liu", "Xuejing Zheng", "Jianjian Xie", "Guoyi Chen"], "title": "Self-Reinforced Deep Priors for Reparameterized Full Waveform Inversion", "categories": ["physics.geo-ph", "cs.CV"], "comment": "Submitted to GEOPHYSICS", "summary": "Full waveform inversion (FWI) has become a widely adopted technique for high-resolution subsurface imaging. However, its inherent strong nonlinearity often results in convergence toward local minima. Recently, deep image prior-based reparameterized FWI (DIP-FWI) has been proposed to alleviate the dependence on massive training data. By exploiting the spectral bias and implicit regularization in the neural network architecture, DIP-FWI can effectively avoid local minima and reconstruct more geologically plausible velocity models. Nevertheless, existing DIP-FWI typically use a fixed random input throughout the inversion process, which fails to utilize the mapping and correlation between the input and output of the network. Moreover, under complex geological conditions, the lack of informative prior in the input can exacerbate the ill-posedness of the inverse problem, leading to artifacts and unstable reconstructions. To address these limitations, we propose a self-reinforced DIP-FWI (SRDIP-FWI) framework, in which a steering algorithm alternately updates both the network parameters and the input at each iteration using feedback from the current network output. This design allows adaptive structural enhancement and improved regularization, thereby effectively mitigating the ill-posedness in FWI. Additionally, we analyze the spectral bias of the network in SRDIP-FWI and quantify its role in multiscale velocity model building. Synthetic tests and field land data application demonstrate that SRDIP-FWI achieves superior resolution, improved accuracy and greater depth penetration compared to multiscale FWI. More importantly, SRDIP-FWI eliminates the need for manual frequency-band selection and time-window picking, substantially simplifying the inversion workflow. Overall, the proposed method provides a novel, adaptive and robust framework for accurate subsurface velocity model reconstruction.", "AI": {"tldr": "提出了一种自增强的深度先验全波形反演框架(SRDIP-FWI)，以提高地下速度模型重建的准确性和鲁棒性。", "motivation": "传统FWI方法存在局部极小值问题，且依赖大量训练数据。现有的DIP-FWI使用固定输入限制了网络的潜力，特别是在复杂地质条件下容易产生伪影和不稳定的重构结果。", "method": "通过设计自增强算法交替更新网络参数和输入，利用当前网络输出反馈进行迭代优化。此方法能够适应性地增强结构，并改进正则化效果。", "result": "合成数据测试及实地应用表明SRDIP-FWI在分辨率、准确性以及穿透深度方面优于多尺度FWI，同时简化了反演工作流程。", "conclusion": "该研究提出了一种新颖、自适应且鲁棒的全波形反演框架，可有效提升地下速度模型的重建质量。"}}
{"id": "2512.08282", "pdf": "https://arxiv.org/pdf/2512.08282", "abs": "https://arxiv.org/abs/2512.08282", "authors": ["Oh Hyun-Bin", "Yuhta Takida", "Toshimitsu Uesaka", "Tae-Hyun Oh", "Yuki Mitsufuji"], "title": "PAVAS: Physics-Aware Video-to-Audio Synthesis", "categories": ["cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.", "AI": {"tldr": "本文提出了一种物理感知的视频到音频合成方法PAVAS，通过引入物理驱动的音频适配器来提高生成声音的真实性和一致性。", "motivation": "现有的视频到音频模型主要依赖于视觉信息而忽视了物理因素对真实世界声音的影响。因此，作者希望通过融入物理参数估计来改进现有模型。", "method": "PAVAS使用物理驱动的音频适配器（Phy-Adapter），该适配器接收由物理参数估计器（PPE）计算出的对象级物理信息，并结合视觉语言模型和分割基动态3D重建模块来推断物体的质量和运动轨迹。", "result": "实验结果表明，PAVAS在量化评价和定性评估中均优于现有的视频到音频生成模型。", "conclusion": "PAVAS通过引入物理驱动的音频适配器成功地合成出具有更高真实性的声音。"}}
{"id": "2512.08280", "pdf": "https://arxiv.org/pdf/2512.08280", "abs": "https://arxiv.org/abs/2512.08280", "authors": ["Haldun Balim", "Na Li", "Yilun Du"], "title": "Model-Based Diffusion Sampling for Predictive Control in Offline Decision Making", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": null, "summary": "Offline decision-making requires synthesizing reliable behaviors from fixed datasets without further interaction, yet existing generative approaches often yield trajectories that are dynamically infeasible. We propose Model Predictive Diffuser (MPDiffuser), a compositional model-based diffusion framework consisting of: (i) a planner that generates diverse, task-aligned trajectories; (ii) a dynamics model that enforces consistency with the underlying system dynamics; and (iii) a ranker module that selects behaviors aligned with the task objectives. MPDiffuser employs an alternating diffusion sampling scheme, where planner and dynamics updates are interleaved to progressively refine trajectories for both task alignment and feasibility during the sampling process. We also provide a theoretical rationale for this procedure, showing how it balances fidelity to data priors with dynamics consistency. Empirically, the compositional design improves sample efficiency, as it leverages even low-quality data for dynamics learning and adapts seamlessly to novel dynamics. We evaluate MPDiffuser on both unconstrained (D4RL) and constrained (DSRL) offline decision-making benchmarks, demonstrating consistent gains over existing approaches. Furthermore, we present a preliminary study extending MPDiffuser to vision-based control tasks, showing its potential to scale to high-dimensional sensory inputs. Finally, we deploy our method on a real quadrupedal robot, showcasing its practicality for real-world control.", "AI": {"tldr": "提出了基于模型的扩散采样框架，用于离线决策中的预测控制。", "motivation": "现有生成方法在从固定数据集中合成可靠行为时，通常会产生动态上不可行的轨迹。因此需要一种既能保证任务一致性又能保持动态可行性的新方法。", "method": "提出了一种基于模型的扩散框架MPDiffuser，包括规划器、动力学模型和排名模块三个部分，并通过交替采样方案逐步优化轨迹。", "result": "实验证明，该框架提高了样本效率，在离线决策基准测试中表现出色，并且能够适应视觉控制任务。", "conclusion": "展示了MPDiffuser在模拟和真实机器人系统中的应用潜力。"}}
{"id": "2512.08273", "pdf": "https://arxiv.org/pdf/2512.08273", "abs": "https://arxiv.org/abs/2512.08273", "authors": ["Thanh Vu", "Richi Nayak", "Thiru Balasubramaniam"], "title": "AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.", "AI": {"tldr": "研究引入生成代理作为评估AI生成内容的有效手段，旨在简化内容生成并确保高质量输出。", "motivation": "现代企业面临生成和评估高质量内容的时间和成本挑战，传统的人类评价方法昂贵且耗时，需要一种高效自动化的解决方案来应对这些问题。", "method": "通过开发生成代理，这些代理可以模拟人类判断，快速且低成本地评估AI生成内容的各个方面如连贯性、趣味性和清晰度等。", "result": "研究提供了关于改进大型语言模型以产生符合业务需求的高质量内容的关键见解，为自动化内容生成和评价带来显著进展。", "conclusion": "利用生成代理可帮助企业简化内容生成过程，并确保持续输出高质量的内容同时减少对昂贵人力评估的需求。"}}
{"id": "2512.08271", "pdf": "https://arxiv.org/pdf/2512.08271", "abs": "https://arxiv.org/abs/2512.08271", "authors": ["Srijan Dokania", "Dharini Raghavan"], "title": "Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation", "categories": ["cs.RO", "cs.CV", "cs.LG", "eess.IV"], "comment": "Published and Presented at 3rd Workshop on Human-Centric Multilateral Teleoperation in ICRA 2025", "summary": "We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.", "AI": {"tldr": "提出了Zero-Splat TeleAssist，一种零样本传感器融合管道，将普通CCTV流转换为多机器人交互式遥操作的共享6-DoF世界模型。", "motivation": "为了提供实时的全局位置和方向，使多个远程操作者能够无需标记点或深度传感器进行有效协作。", "method": "通过视觉语言分割、单目深度估计、加权主成分分析姿势提取以及3D高斯洒水法（3DGS），将普通CCTV流转化为共享的6-DoF世界模型。", "result": "成功地实现了无标记点或深度传感器条件下的多机器人实时位置和方向共享，提高了遥操作系统的灵活性与可靠性。", "conclusion": "该方法通过引入Zero-Splat TeleAssist框架，在没有额外硬件支持的情况下极大地提升了遥操作系统中多个机器人的交互体验。"}}
{"id": "2512.08270", "pdf": "https://arxiv.org/pdf/2512.08270", "abs": "https://arxiv.org/abs/2512.08270", "authors": ["Jaisal Patel", "Yunzhe Chen", "Kaiwen He", "Keyi Wang", "David Li", "Kairong Xiao", "Xiao-Yang Liu"], "title": "Reasoning Models Ace the CFA Exams", "categories": ["cs.AI", "cs.CL", "q-fin.GN"], "comment": null, "summary": "Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.", "AI": {"tldr": "评估了最新的推理模型在模拟CFA考试中的表现。", "motivation": "先前研究显示大型语言模型在CFA考试中表现不佳，但最新推理模型在各类专业和学术考试中表现出色。为了验证这一点，并了解这些模型是否能够通过CFA考试。", "method": "使用980个问题的模拟CFA试题集测试了最先进的推理模型，包括三个Level I、两个Level II和三个Level III级别的考试题。", "result": "大多数模型通过了所有级别。Gemini 3.0 Pro在Level I中取得97.6%的成绩，GPT-5在Level II中以94.3%领先，Gemini 2.5 Pro在Level III的多项选择题和Gemini 3.0 Pro在构造回应问题中表现出色。", "conclusion": "最新的推理模型能够很好地应对CFA考试，并且某些模型已经达到了通过考试所需的水平。"}}
{"id": "2512.08269", "pdf": "https://arxiv.org/pdf/2512.08269", "abs": "https://arxiv.org/abs/2512.08269", "authors": ["Taewoong Kang", "Kinam Kim", "Dohyeon Kim", "Minho Park", "Junha Hyung", "Jaegul Choo"], "title": "EgoX: Egocentric Video Generation from a Single Exocentric Video", "categories": ["cs.CV"], "comment": "21 pages, project page : https://keh0t0.github.io/EgoX", "summary": "Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.", "AI": {"tldr": "该论文提出了一种从单一第三人称视频生成第一人称视角的视频的方法。", "motivation": "通过将第三人称视频转化为第一人称视角视频，可以实现更加沉浸式的理解和体验。但这一任务由于相机姿态的巨大变化和视图重叠极小而极具挑战性。", "method": "EgoX框架利用大规模视频扩散模型中预训练的时空知识，并结合宽度与通道拼接策略以及几何指导的自注意力机制，从单个第三人称视频生成第一人称视角的视频。", "result": "该方法在保持几何一致性和视觉逼真度的同时实现了连贯的第一人称视频生成，且具有良好的扩展性和鲁棒性。", "conclusion": "EgoX框架成功地解决了从单一第三人称视频生成第一人称视频的任务，并展示了其在不同场景下的适用性和效果。"}}
{"id": "2512.08262", "pdf": "https://arxiv.org/pdf/2512.08262", "abs": "https://arxiv.org/abs/2512.08262", "authors": ["Hafeez Husain Cholakkal", "Stefano Arrigoni", "Francesco Braghin"], "title": "RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.", "AI": {"tldr": "本文提出了RLCNet，一种用于同时在线校准LiDAR、RADAR和相机的端到端深度学习框架。", "motivation": "在动态环境中准确地外参标定对于自动驾驶车辆可靠感知至关重要。然而，由于机械振动和累积传感器漂移等因素，这种任务仍然具有挑战性。", "method": "RLCNet是一个新的端到端可训练的深度学习框架，用于同时在线校准这些多模式传感器。为了支持实时操作，引入了带有加权移动平均和异常值拒绝的在线校准框架。", "result": "该方法在真实数据集上进行了验证，并展示了强大的性能、更高的精度和更强的鲁棒性。", "conclusion": "RLCNet设计用于实际部署，在各种条件下表现出色。"}}
{"id": "2512.08261", "pdf": "https://arxiv.org/pdf/2512.08261", "abs": "https://arxiv.org/abs/2512.08261", "authors": ["Yibowen Zhao", "Yinan Zhang", "Zhixiang Su", "Lizhen Cui", "Chunyan Miao"], "title": "Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes", "categories": ["cs.AI"], "comment": "This work has been accepted by ICDE 2026 and is available on arXiv for early access", "summary": "Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.", "AI": {"tldr": "利用知识图谱和原型模型，结合患者信息进行疾病预测。", "motivation": "提高基于患者信息的疾病预测准确性及可解释性，以增强患者的健康意识并优化医疗系统的效率。", "method": "提出了一种集成医学知识图谱、临床意义的疾病原型以及对比学习来提升长尾疾病的预测准确性的KPI框架。此外，利用大规模语言模型生成与病人相关的医学解释以提高预测的可靠性和可解释性。", "result": "实验结果表明，KPI在预测准确性上超越了现有的方法，并能提供符合患者叙述的临床有效解释。", "conclusion": "该研究展示了通过结合知识图谱和原型模型来改进疾病预测的有效性及其在以患者为中心的医疗保健中的实际应用价值。"}}
{"id": "2512.08257", "pdf": "https://arxiv.org/pdf/2512.08257", "abs": "https://arxiv.org/abs/2512.08257", "authors": ["Preksha Girish", "Rachana Mysore", "Mahanthesha U", "Shrey Kumar", "Misbah Fatimah Annigeri", "Tanish Jain"], "title": "Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability", "categories": ["cs.LG", "eess.IV"], "comment": "7 pages, 3 figures", "summary": "Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.", "AI": {"tldr": "本文提出了一种结合了几何和随机方法的多模式深度学习框架，用于预测SUDEP和中风易感性。", "motivation": "癫痫猝死（SUDEP）和急性缺血性卒中是涉及复杂皮层、脑干和自主神经系统相互作用的生命威胁状况。本文旨在开发一种能够整合多种生物信号以提高预测准确性和可解释性的多模式建模方法。", "method": "研究提出了一种结合黎曼流形嵌入、李群不变特征表示、分数阶随机动力学、哈密顿能量流模型和跨模态注意力机制的统一框架。利用分数流行病传播扩散在结构大脑图上模拟卒中传播。", "result": "实验表明，所提出的框架能够提高预测准确性，并从流形曲率、分数记忆指数、注意力熵和扩散中心度等可解释生物标志物中获得有益信息。", "conclusion": "本文开发的框架为早期检测、风险分层以及神经自主障碍的多模式建模提供了数学基础。"}}
{"id": "2512.08254", "pdf": "https://arxiv.org/pdf/2512.08254", "abs": "https://arxiv.org/abs/2512.08254", "authors": ["Yun Liu", "Tao Li", "Cosmin Ancuti", "Wenqi Ren", "Weisi Lin"], "title": "SFP: Real-World Scene Recovery Using Spatial and Frequency Priors", "categories": ["cs.CV"], "comment": "10 pages, 13 figures", "summary": "Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.", "AI": {"tldr": "提出了一种基于空间和频率先验的场景恢复方法SFP，以解决现有方法在处理多种退化时的不足。", "motivation": "现有的场景恢复方法依赖单一先验或使用复杂的网络架构训练于合成数据上，导致对真实世界多样化的环境适应性差。因此，本文提出了一种结合空间和频率先验的方法来提高真实世界的场景恢复质量。", "method": "在空间域中通过逆图像的谱方向投影估计传输图以恢复散射退化后的场景；在频域中构建自适应频率增强掩模并利用两个新提出的频域先验进行参数估计；最后设计加权融合策略整合空间域修复、频域增强和输入图片中的显著特征。", "result": "实验评估显示所提出的方法SFP在各种退化条件下具有有效性和优越性。", "conclusion": "本文通过结合空间和频率先验提出了一个有效的场景恢复方法，该方法能够处理多种真实的退化情况并表现优异。"}}
{"id": "2512.08253", "pdf": "https://arxiv.org/pdf/2512.08253", "abs": "https://arxiv.org/abs/2512.08253", "authors": ["YiLin Zhou", "Lili Wei", "Zheming Xu", "Ziyi Chen", "Congyan Lang"], "title": "Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.", "AI": {"tldr": "本文提出了一种查询感知原型学习方法，旨在解决少样本三维点云语义分割中的原型偏差问题。", "motivation": "现有的基于度量的原型生成方法仅从支持集生成原型，忽略了与查询数据的相关性，导致过拟合和性能下降。为了改善这个问题，本文提出了Query-aware Hub Prototype (QHP) 方法来更好地捕捉跨集合的语义关系。", "method": "提出了一种Hub Prototype Generation (HPG) 模块来连接查询和支持点，通过生成与查询相关的原型以提高跨集语义的一致性。为了进一步优化原型分布，引入了Prototype Distribution Optimization (PDO) 模块，采用纯度加权对比损失来细化原型表示。", "result": "在S3DIS和ScanNet数据集上的实验结果表明，QHP方法取得了显著的性能提升，并有效减少了原型与查询集合之间的语义差距。", "conclusion": "通过引入Query-aware Hub Prototype (QHP) 方法，本文解决了少样本三维点云分割中的原型偏差问题，提升了模型在分布变化情况下的泛化能力。"}}
{"id": "2512.08248", "pdf": "https://arxiv.org/pdf/2512.08248", "abs": "https://arxiv.org/abs/2512.08248", "authors": ["Ahan Basu", "Ratnangshu Das", "Pushpak Jagtap"], "title": "Learning Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks using Physics-Informed Neural Networks", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for general control-affine MIMO nonlinear pure-feedback systems with unknown dynamics to satisfy prescribed time reach-avoid-stay tasks under external disturbances. The STT is defined as a time-varying ball, whose center and radius are jointly approximated by a Physics-Informed Neural Network (PINN). The constraints governing the STT are first formulated as loss functions of the PINN, and a training algorithm is proposed to minimize the overall violation. The PINN being trained on certain collocation points, we propose a Lipschitz-based validity condition to formally verify that the learned PINN satisfies the conditions over the continuous time horizon. Building on the learned STT representation, an approximation-free closed-form controller is defined to guarantee satisfaction of the T-RAS specification. Finally, the effectiveness and scalability of the framework are validated through two case studies involving a mobile robot and an aerial vehicle navigating through cluttered environments.", "AI": {"tldr": "提出了一种基于时空管(STT)的控制框架，用于满足未知动力学系统在外部干扰下的时间到达-避免-停留任务。", "motivation": "为了处理具有未知动态特性的多输入多输出非线性纯反馈系统的到达、避免和保持特定区域的任务需求，该论文提出了一个基于时空管(STT)的控制框架。", "method": "采用物理启发式神经网络(PINN)来近似STT中心和半径。通过定义损失函数并使用训练算法最小化整体违规行为，确保满足时间到达-避免-停留任务的要求。此外，提出了一种基于Lipschitz条件的验证方法来保证PINN在整个连续时间内的一致性。", "result": "通过对移动机器人和空中飞行器在复杂环境中的导航案例进行实验，证明了所提框架的有效性和可扩展性。", "conclusion": "通过引入时空管(STT)概念并结合物理启发式神经网络(PINN)，该论文成功实现了对具有未知动力学系统的有效控制，并能够满足时间到达-避免-停留任务的要求。"}}
{"id": "2512.08247", "pdf": "https://arxiv.org/pdf/2512.08247", "abs": "https://arxiv.org/abs/2512.08247", "authors": ["Haowen Zheng", "Hu Zhu", "Lu Deng", "Weihao Gu", "Yang Yang", "Yanyan Liang"], "title": "Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI-26", "summary": "Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.", "AI": {"tldr": "本文提出了一种基于稀疏查询的知识蒸馏方法，用于从离线模型向在线模型转移未来的时空知识。", "motivation": "现有知识蒸馏方法未能有效地利用未来帧的信息来提高在线模型的性能。", "method": "通过引入未来感知特征重建策略和未来引导的日志值蒸馏技术，本文提出了一种新的知识蒸馏框架FTKD，以鼓励学生模型捕捉未来的时空信息。", "result": "在nuScenes数据集上，所提方法实现了1.3 mAP和1.3 NDS的性能提升，并且获得了更准确的速度估计结果。", "conclusion": "实验表明，本文提出的未来时空知识蒸馏框架FTKD能够有效提高在线模型的学习能力，无需增加推理成本。"}}
{"id": "2512.08243", "pdf": "https://arxiv.org/pdf/2512.08243", "abs": "https://arxiv.org/abs/2512.08243", "authors": ["Saeeda Naz", "Saddam Hussain Khan"], "title": "Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "26 Pages, 10 Figures, 4 Tables", "summary": "A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.", "AI": {"tldr": "提出了一种新型的混合Residual-SwinCA-Net分割框架，用于乳腺超声图像中恶性病变的分割。", "motivation": "为了提高乳腺超声图像中恶性病变分割性能和临床诊断准确性，设计了一种结合残差CNN模块和自定义Swin Transformer块的方法。", "method": "该方法包括局部相关特征提取、全局依赖学习、增强组织连续性和抑制噪声的Laplacian-of-Gaussian操作符、边界导向操作符以及多尺度通道注意力机制（MSCAS）与像素注意模块。通过逐步减少特征图以捕捉尺度不变性，并在解码器级别引入MSCAS，强化编码器显著映射并保持全局和局部结构。", "result": "该框架在公开的BUSI数据集上实现了99.29%平均准确率、98.74%交并比（IoU）以及0.9041的Dice系数，并优于现有的CNNs/ViTs技术。", "conclusion": "提出的Residual-SwinCA-Net框架提高了BUSI病变诊断性能，增强了临床决策时效性。"}}
{"id": "2512.08240", "pdf": "https://arxiv.org/pdf/2512.08240", "abs": "https://arxiv.org/abs/2512.08240", "authors": ["Jusheng Zhang", "Xiaoyang Guo", "Kaitong Cai", "Qinhan Lv", "Yijia Fan", "Wenhao Chai", "Jian Wang", "Keze Wang"], "title": "HybridToken-VLM: Hybrid Token Compression for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.", "AI": {"tldr": "本文提出了HTC-VLM模型，旨在解决视觉语言模型中计算成本高的问题。", "motivation": "传统方法在压缩视觉信息时面临着精细细节与语义之间的权衡。为了提升效率同时保留高质量的语义表示，作者设计了HybridToken-VLM。", "method": "通过双通道（连续路径和离散路径）分离语义和外观，并将它们融合成一个混合序列，最终压缩为单一词汇令牌。", "result": "HTC-VLM在七个基准测试中的表现优于其他方法，平均保留率为87.2%，显示了其高效性和准确性。", "conclusion": "研究表明，这种简明的混合设计能够解决效率与保真度之间的矛盾，并推进大规模视觉语言模型的发展。"}}
{"id": "2512.08238", "pdf": "https://arxiv.org/pdf/2512.08238", "abs": "https://arxiv.org/abs/2512.08238", "authors": ["Mahathir Monjur", "Shahriar Nirjon"], "title": "SpeechQualityLLM: LLM-Based Multimodal Assessment of Speech Quality", "categories": ["cs.SD", "cs.AI"], "comment": "9 pages, 5 figures, 8 tables", "summary": "Objective speech quality assessment is central to telephony, VoIP, and streaming systems, where large volumes of degraded audio must be monitored and optimized at scale. Classical metrics such as PESQ and POLQA approximate human mean opinion scores (MOS) but require carefully controlled conditions and expensive listening tests, while learning-based models such as NISQA regress MOS and multiple perceptual dimensions from waveforms or spectrograms, achieving high correlation with subjective ratings yet remaining rigid: they do not support interactive, natural-language queries and do not natively provide textual rationales. In this work, we introduce SpeechQualityLLM, a multimodal speech quality question-answering (QA) system that couples an audio encoder with a language model and is trained on the NISQA corpus using template-based question-answer pairs covering overall MOS and four perceptual dimensions (noisiness, coloration, discontinuity, and loudness) in both single-ended (degraded only) and double-ended (degraded plus clean reference) setups. Instead of directly regressing scores, our system is supervised to generate textual answers from which numeric predictions are parsed and evaluated with standard regression and ranking metrics; on held-out NISQA clips, the double-ended model attains a MOS mean absolute error (MAE) of 0.41 with Pearson correlation of 0.86, with competitive performance on dimension-wise tasks. Beyond these quantitative gains, it offers a flexible natural-language interface in which the language model acts as an audio quality expert: practitioners can query arbitrary aspects of degradations, prompt the model to emulate different listener profiles to capture human variability and produce diverse but plausible judgments rather than a single deterministic score, and thereby reduce reliance on large-scale crowdsourced tests and their monetary cost.", "AI": {"tldr": "本文提出了一个基于音频编码器和语言模型的多模态语音质量评估系统SpeechQualityLLM，该系统可以支持自然语言查询并提供文本理由。", "motivation": "传统的语音质量评估方法需要在严格控制条件下进行且成本高昂；学习型模型虽然能从波形或频谱图回归出主观评分及其多个感知维度，但无法支持交互式、自然语言的查询，并不能原生地提供文字理由。因此本文提出了一个结合音频编码器和语言模型的多模态语音质量问答系统。", "method": "SpeechQualityLLM通过模板化问题-答案对训练，在NISQA语料库上学习生成文本回答，该方法涵盖总体MOS及四个感知维度（噪音、颜色、不连续性、响度），同时支持单端和双端设置。模型不仅监督生成文本回答，还解析出数值预测，并用标准回归和排名指标评估。", "result": "在NISQA剪辑上，双端模型总体MOS误差为0.41，皮尔逊相关系数达到0.86；并且在维度水平任务中表现出色。此外，该系统提供了一个灵活的自然语言界面，使用户可以查询任意降质方面的信息，并减少对大规模众包测试及其高昂成本的依赖。", "conclusion": "SpeechQualityLLM不仅提高了语音质量评估的准确性，还通过提供一个交互式的问答平台增强了系统的灵活性和实用性。"}}
{"id": "2512.08237", "pdf": "https://arxiv.org/pdf/2512.08237", "abs": "https://arxiv.org/abs/2512.08237", "authors": ["Yuanpeng Chen", "Hui Song", "Wei Tao", "ShanHui Mo", "Shuang Zhang", "Xiao Hua", "TianKun Zhao"], "title": "FastBEV++: Fast by Algorithm, Deployable by Design", "categories": ["cs.CV"], "comment": null, "summary": "The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the \"Deployable by Design\" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is \"Fast by Algorithm\", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev", "AI": {"tldr": "本文提出了一种新的框架FastBEV++，它通过算法优化和设计上的改进，在实现高性能的同时确保了部署效率。", "motivation": "当前基于相机的鸟瞰图感知技术在性能和可部署性之间存在矛盾。这个瓶颈主要是由于计算成本高昂的视图转换和平台特定内核的存在。本文旨在解决这一问题，提出一种既高效又易于部署的方法。", "method": "FastBEV++通过将传统的单体投影分解为标准的索引-收集-重塑管道来实现“设计可部署”的原则。利用确定性的预排序策略，这种转换完全使用基础操作符原生基本功能（如Gather和矩阵乘法）执行，消除了对自定义CUDA内核的需求并确保了完整的TensorRT原生兼容性。", "result": "FastBEV++在nuScenes基准测试中建立了新的状态-of-the-art性能，0.359 NDS，并且具有卓越的实时表现，在汽车级硬件上超过了134FPS（例如Tesla T4）。", "conclusion": "通过提供既不需要自定义插件又高度准确的方法，FastBEV++展示了面向生产自主系统的一种成熟且可扩展的设计哲学。"}}
{"id": "2512.08233", "pdf": "https://arxiv.org/pdf/2512.08233", "abs": "https://arxiv.org/abs/2512.08233", "authors": ["Timothy Chen", "Marcus Dominguez-Kuhne", "Aiden Swann", "Xu Liu", "Mac Schwager"], "title": "Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior", "categories": ["cs.RO"], "comment": null, "summary": "Humans interpret safety not as a binary signal but as a continuous, context- and spatially-dependent notion of risk. While risk is subjective, humans form rational mental models that guide action selection in dynamic environments. This work proposes a framework for extracting implicit human risk models by introducing a novel, semantically-conditioned and spatially-varying parametrization of risk, supervised directly from safe human demonstration videos and VLM common sense. Notably, we define risk through a Bayesian formulation. The prior is furnished by a pretrained vision-language model. In order to encourage the risk estimate to be more human aligned, a likelihood function modulates the prior to produce a relative metric of risk. Specifically, the likelihood is a learned ViT that maps pretrained features, to pixel-aligned risk values. Our pipeline ingests RGB images and a query object string, producing pixel-dense risk images. These images that can then be used as value-predictors in robot planning tasks or be projected into 3D for use in conventional trajectory optimization to produce human-like motion. This learned mapping enables generalization to novel objects and contexts, and has the potential to scale to much larger training datasets. In particular, the Bayesian framework that is introduced enables fast adaptation of our model to additional observations or common sense rules. We demonstrate that our proposed framework produces contextual risk that aligns with human preferences. Additionally, we illustrate several downstream applications of the model; as a value learner for visuomotor planners or in conjunction with a classical trajectory optimization algorithm. Our results suggest that our framework is a significant step toward enabling autonomous systems to internalize human-like risk. Code and results can be found at https://riskbayesian.github.io/bayesian_risk/.", "AI": {"tldr": "本文提出了一种基于语义和视觉的语言模型的风险场学习框架，通过人类演示视频和VLM先验知识来训练机器人安全性的连续风险评估。", "motivation": "人类在动态环境中感知风险的方式是主观且随时间和空间变化的。为了使机器人行为更接近于人的偏好，需要从人类演示中提取出这种隐含的风险模型。", "method": "该框架通过引入一种新颖的空间变异参数化方法来定义风险，并使用预训练的视觉语言模型作为先验知识。利用一个学习到的概率函数调整这个先验，生成相对的风险度量。具体来说，这个概率函数是一个基于ViT的学习器，它将预训练特征映射为与像素对齐的风险值。", "result": "实验结果表明，所提出的方法能够产生符合人类偏好的风险评估，并可应用于视觉运动规划和经典轨迹优化算法中。", "conclusion": "该工作展示了一种使机器人学会像人一样理解风险的有效方法，对于构建更安全、更智能的自主系统具有重要意义。"}}
{"id": "2512.08230", "pdf": "https://arxiv.org/pdf/2512.08230", "abs": "https://arxiv.org/abs/2512.08230", "authors": ["Eunice Yiu", "Kelsey Allen", "Shiry Ginosar", "Alison Gopnik"], "title": "Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions", "categories": ["cs.AI"], "comment": "Accepted to Philosophical Transactions A, Special issue: World models, AGI, and the hard problems of life-mind continuity. Expected publication in 2026", "summary": "Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called \"empowerment\" which maximizes mutual information between actions and their outcomes. \"Empowerment\" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.", "AI": {"tldr": "本文研究了人类（包括儿童和成人）如何通过增强感来推断因果关系并设计有效的干预措施。", "motivation": "探索强化学习中的“增强感”是否可以作为连接经典贝叶斯因果推理与机器中因果学习的桥梁，以及它在理解和发展人类特别是儿童的因果认知方面的作用。", "method": "通过实验研究测试了儿童和成人如何利用增强线索来推断因果关系并设计有效的干预措施。", "result": "系统地展示了儿童和成人在使用增强感来推断因果关系方面的表现与效果。", "conclusion": "增强了感可能有助于解释儿童在因果学习中的独特特征，并为机器实现这种学习提供了一个更可计算的理论框架。"}}
{"id": "2512.08229", "pdf": "https://arxiv.org/pdf/2512.08229", "abs": "https://arxiv.org/abs/2512.08229", "authors": ["Tony Salloom", "Dandi Zhou", "Xinhai Sun"], "title": "Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.", "AI": {"tldr": "提出了一种基于几何感知的稀疏深度采样策略，以改善RGB-D深度完成模型的性能。", "motivation": "当前深度补全方法中采用的随机稀疏深度样本未考虑真实传感器的空间非均匀可靠性。作者希望通过引入PCA表面法线估计和基于可靠性的稀疏深度样本选取来改进这一问题。", "method": "利用PCA计算RGB-D点云的表面法向量，通过该法向量确定每个像素的深度可靠性，并根据此分布抽取稀疏深度样本；然后将这些样本与Marigold-DC扩散模型结合进行深度补全。", "result": "实验表明，所提出的几何感知稀疏深度采样方法提高了深度补全精度，减少了边缘和不连续区域的艺术化效果，并且产生了更真实的训练条件。", "conclusion": "通过引入基于可靠性的稀疏深度样本选取策略，可以有效提高现代工业机器人系统的三维感知性能。"}}
{"id": "2512.08228", "pdf": "https://arxiv.org/pdf/2512.08228", "abs": "https://arxiv.org/abs/2512.08228", "authors": ["Jusheng Zhang", "Kaitong Cai", "Xiaoyang Guo", "Sidi Liu", "Qinhan Lv", "Ruiqi Chen", "Jing Yang", "Yijia Fan", "Xiaofei Sun", "Jian Wang", "Ziliang Chen", "Liang Lin", "Keze Wang"], "title": "MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.", "AI": {"tldr": "本文提出了一个诊断基准MM-CoT，用于探测多模态模型（MMs）在视觉链式思考推理方面的表现。", "motivation": "现有基准着重于生成解释而忽视了验证能力，即评估推理链条是否既符合视觉证据又具有逻辑合理性。因此，作者引入了MM-CoT以填补这一空白。", "method": "通过要求模型选择一个同时满足视觉一致性和逻辑连贯性的事件链，并设计对抗性干扰来违反其中一个或两个约束条件，以此测试模型的性能。", "result": "即使是最先进的系统在MM-CoT上也表现不佳，表明它们虽然能生成流畅的解释，但在真正的推理忠实度方面却存在明显差距。此外，该基准与现有基准的相关性较低。", "conclusion": "MM-CoT为开发能够在视觉世界中进行既合理又可信、连贯的推理的新模型提供了基础。"}}
{"id": "2512.08227", "pdf": "https://arxiv.org/pdf/2512.08227", "abs": "https://arxiv.org/abs/2512.08227", "authors": ["Md Eimran Hossain Eimon", "Ashan Perera", "Juan Merlos", "Velibor Adzic", "Hari Kalva"], "title": "New VVC profiles targeting Feature Coding for Machines", "categories": ["cs.CV"], "comment": "Accepted for presentation at ICIP 2025 workshop on Coding for Machines", "summary": "Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.", "AI": {"tldr": "该论文研究了在MPEG-AI Feature Coding for Machines标准下，使用Versatile Video Coding (VVC) 压缩机器学习任务中的中间特征。提出了三种轻量级的VVC配置文件。", "motivation": "现有视频编码器被优化以保留感知质量，但在分层推理系统中传输神经网络的中间特征时，这些假设不再适用。因此，研究如何使用VVC高效压缩这些抽象、稀疏和任务特定的中间特征。", "method": "通过工具级分析理解各个编码组件对压缩效率和下游视觉任务准确性的影晌，并基于此提出三种轻量级的VVC配置文件：Fast, Faster 和 Fastest。", "result": "Fast 配置文件在提高2.96% BD-Rate的同时，将编码时间减少了21.8%; Faster 实现了1.85% BD-Rate的提升，并且加速了51.5%; 最终，Fastest 减少了95.6%的编码时间，仅损失了1.71%的BD-Rate。", "conclusion": "通过重新设计VVC配置文件以适应机器学习任务中特征压缩需求，可以显著提高效率并减少计算资源消耗。"}}
{"id": "2512.08223", "pdf": "https://arxiv.org/pdf/2512.08223", "abs": "https://arxiv.org/abs/2512.08223", "authors": ["Ching-Hung Cheng", "Hsiu-Fu Wu", "Bing-Chen Wu", "Khanh-Phong Bui", "Van-Tin Luu", "Ching-Chun Huang"], "title": "SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.", "AI": {"tldr": "该论文探索了在三维物体检测中使用提示调优方法的有效性，并提出了场景导向的提示池（SOP^2）。", "motivation": "通过大型语言模型如GPT-3展现的强大泛化能力，结合转移学习技术可以将这些模型应用于各种下游任务。此研究探讨如何利用提示调优方法提高三维物体检测中的性能。", "method": "该论文评估了不同提示令牌和生成器的影响，并提出了一种场景导向的提示池（SOP^2）来改进三维物体检测。", "result": "研究表明，提出的Scene-Oriented Prompt Pool (SOP^2)在三维物体检测任务中表现出显著提升的效果。", "conclusion": "该论文成功展示了利用提示调优方法在三维物体检测中的潜力，并鼓励未来的研究者进一步探索这一方向。"}}
{"id": "2512.08221", "pdf": "https://arxiv.org/pdf/2512.08221", "abs": "https://arxiv.org/abs/2512.08221", "authors": ["Ziwei Yao", "Qiyang Wan", "Ruiping Wang", "Xilin Chen"], "title": "VisKnow: Constructing Visual Knowledge Base for Object Understanding", "categories": ["cs.CV"], "comment": "16 pages, 12 figures, 7 tables. Under review", "summary": "Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.", "AI": {"tldr": "构建视觉知识库以实现对象的深入理解。", "motivation": "当前的多模态数据未系统化，不足以支持深层次的对象理解。因此需要一种结构化的视觉知识库来整合和组织这些数据。", "method": "提出了一种名为VisKnow的方法，该方法通过结合专家设计和大规模模型应用从文本和图像源中提取多模态对象级知识，并将其以图的形式组织起来。", "result": "构建了AnimalKB，一个涵盖406个动物类别的结构化动物知识库。实验表明，使用AnimalKB可以提高零样本识别和细粒度视觉问答等任务的表现。", "conclusion": "自动构建视觉知识库具有巨大的潜力，可以推动计算机视觉研究及其实际应用的发展。"}}
{"id": "2512.08218", "pdf": "https://arxiv.org/pdf/2512.08218", "abs": "https://arxiv.org/abs/2512.08218", "authors": ["Ye Qin", "Jingchao Wang", "Yang Shi", "Haiying Huang", "Junxu Li", "Weijian Liu", "Tinghui Chen", "Jinghui Qin"], "title": "PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining)", "summary": "Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\\-world graphs poorly by fixed\\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\\-Euclidean pseudo\\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\\-Riemannian manifolds and derive a Pseudo\\-Riemannian Capsule Network (PR\\-CapsNet), which models data in pseudo\\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\\-CapsNet enhances the CapsNet with Adaptive Pseudo\\-Riemannian Tangent Space Routing by utilizing pseudo\\-Riemannian geometry. Unlike single\\-curvature or subspace\\-partitioning methods, PR\\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\\-Riemannian metric. It first deploys Pseudo\\-Riemannian Tangent Space Routing to decompose capsule states into spherical\\-temporal and Euclidean\\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\\-preserved Pseudo\\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\\-CapsNet outperforms SOTA models, validating PR\\-CapsNet's strong representation power for complex graph structures.", "AI": {"tldr": "本文提出了一种基于伪黎曼流形的胶囊网络（PR-CapsNet），用于改进图表示学习。", "motivation": "现有的胶囊网络在固定曲率空间中建模真实世界的图形时表现不佳，因此通过引入可适应性曲线度的空间来增强其性能", "method": "本文提出了伪黎曼流形的自适应弯曲路由方法，该方法利用了伪黎曼几何，并开发了一种保持几何属性的伪黎曼胶囊分类器。", "result": "在节点和图分类基准测试中，PR-CapsNet优于现有最佳模型，验证了其在复杂图形结构中的强大表示能力", "conclusion": "通过引入可适应性曲率的空间来改进传统胶囊网络，能够更好地处理复杂的图结构问题"}}
{"id": "2512.08216", "pdf": "https://arxiv.org/pdf/2512.08216", "abs": "https://arxiv.org/abs/2512.08216", "authors": ["Aneesh Rangnekar", "Harini Veeraraghavan"], "title": "Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.", "AI": {"tldr": "提出了一种基于随机森林的轻量级后处理框架RF-Deep，用于检测肺部癌症分割中的异常输入。", "motivation": "现有的方法在检测异常输入时存在模型偏见或增加参数和计算成本的问题，影响了肿瘤分割的安全临床部署。", "method": "利用预训练然后微调的骨干编码器提取多个感兴趣区域特征，并使用随机森林进行任务相关的异常检测。", "result": "RF-Deep在近于正常的数据集中取得了AUROC>93.50，在远于正常的数据集中达到了几乎完美的检测效果（AUROC>99.00），优于基于logit和其他方法的表现。", "conclusion": "RF-Deep是一种轻量级、架构无关的方法，能够提高CT体积中肿瘤分割的可靠性。"}}
{"id": "2512.08215", "pdf": "https://arxiv.org/pdf/2512.08215", "abs": "https://arxiv.org/abs/2512.08215", "authors": ["Chia-Hern Lai", "I-Hsuan Lo", "Yen-Ku Yeh", "Thanh-Nguyen Truong", "Ching-Chun Huang"], "title": "Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement", "categories": ["cs.CV"], "comment": null, "summary": "The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.", "AI": {"tldr": "提出了一种名为Blur2Sharp的框架，通过结合3D感知神经渲染和扩散模型来生成清晰、几何一致的新视角图像。", "motivation": "现有的方法在创建具有真实姿势变化和视角灵活性的人类化身时存在几何不一致性或牺牲了逼真度的问题。作者希望通过集成3D感知神经渲染和扩散模型，解决这些挑战，以生成高质量的新视角图像。", "method": "该框架使用双条件架构：首先通过Human NeRF模型生成目标姿态的几何一致多视图渲染；随后用扩散模型对渲染进行细化，保留细节并保持结构一致性。还采用了分层特征融合来提升视觉质量。", "result": "实验表明，Blur2Sharp在新姿势和视角生成任务中优于现有技术，特别是在处理松散衣物和遮挡等挑战性场景时表现出色。", "conclusion": "提出的方法解决了当前模型的局限性，提升了图像的质量，尤其适用于复杂场景下的多视图姿态变化。"}}
{"id": "2512.08206", "pdf": "https://arxiv.org/pdf/2512.08206", "abs": "https://arxiv.org/abs/2512.08206", "authors": ["Duo Zhang", "Junshan Huang", "Jingjin Yu"], "title": "High-Performance Dual-Arm Task and Motion Planning for Tabletop Rearrangement", "categories": ["cs.RO"], "comment": "ICRA 2026 Submission", "summary": "We propose Synchronous Dual-Arm Rearrange- ment Planner (SDAR), a task and motion planning (TAMP) framework for tabletop rearrangement, where two robot arms equipped with 2-finger grippers must work together in close proximity to rearrange objects whose start and goal config- urations are strongly entangled. To tackle such challenges, SDAR tightly knit together its dependency-driven task planner (SDAR-T) and synchronous dual-arm motion planner (SDAR- M), to intelligently sift through a large number of possible task and motion plans. Specifically, SDAR-T applies a simple yet effective strategy to decompose the global object dependency graph induced by the rearrangement task, to produce more optimal dual-arm task plans than solutions derived from optimal task plans for a single arm. Leveraging state-of-the-art GPU SIMD-based motion planning tools, SDAR-M employs a layered motion planning strategy to sift through many task plans for the best synchronous dual-arm motion plan while ensuring high levels of success rate. Comprehensive evaluation demonstrates that SDAR delivers a 100% success rate in solving complex, non-monotone, long-horizon tabletop rearrangement tasks with solution quality far exceeding the previous state- of-the-art. Experiments on two UR-5e arms further confirm SDAR directly and reliably transfers to robot hardware.", "AI": {"tldr": "本文提出了SDAR，一种用于桌子上的物体重新排列的任务和运动规划框架，旨在解决两个机器人手臂在紧密空间内协作重新排列纠缠在一起的物品的问题。", "motivation": "当前的任务和运动规划方法在处理高度依赖性和复杂性的任务时存在局限性。为了克服这些挑战，作者开发了一种新的同步双臂重排计划器（SDAR）来提高成功率和解决方案的质量。", "method": "SDAR通过将依赖驱动的任务规划器（SDAR-T）与同步双臂运动规划器（SDAR-M）紧密结合在一起，智能地筛选出大量可能的任务和运动方案。其中，SDAR-T采用了有效的策略以分解全局物体依赖图，生成更优的双臂任务计划。此外，利用先进的GPU SIMD基线运动规划工具，SDAR-M采用了一种分层运动规划策略来选择最佳同步双臂运动计划。", "result": "实验表明，SDAR在解决复杂的桌子重排问题时实现了100%的成功率，并且解决方案质量远超以往的方法。", "conclusion": "研究结果显示了SDAR框架的有效性以及它直接和可靠地转移到机器人硬件的能力。"}}
{"id": "2512.08203", "pdf": "https://arxiv.org/pdf/2512.08203", "abs": "https://arxiv.org/abs/2512.08203", "authors": ["Zhuohang Han", "Jincheng Dai", "Shengshi Yao", "Junyi Wang", "Yanlong Li", "Kai Niu", "Wenjun Xu", "Ping Zhang"], "title": "Error-Resilient Semantic Communication for Speech Transmission over Packet-Loss Networks", "categories": ["cs.SD"], "comment": "submitted to IEEE in Nov. 2025", "summary": "Real-time speech communication over wireless networks remains challenging, as conventional channel protection mechanisms cannot effectively counter packet loss under stringent bandwidth and latency constraints. Semantic communication has emerged as a promising paradigm for enhancing the robustness of speech transmission by means of joint source-channel coding (JSCC). However, its cross-layer design hinders practical deployment due to the incompatibility with existing digital communication systems. In this case, the robustness of speech communication is consequently evaluated primarily by the error-resilience to packet loss over wireless networks. To address these challenges, we propose \\emph{Glaris}, a generative latent-prior-based resilient speech semantic communication framework that performs resilient speech coding in the generative latent space. Generative latent priors enable high-quality packet loss concealment (PLC) at the receiver side, well-balancing semantic consistency and reconstruction fidelity. Additionally, an integrated error resilience mechanism is designed to mitigate the error propagation and improve the effectiveness of PLC. Compared with traditional packet-level forward error correction (FEC) strategies, our new method achieves enhanced robustness over dynamic wireless networks while reducing redundancy overhead significantly. Experimental results on the LibriSpeech dataset demonstrate that \\emph{Glaris} consistently outperforms existing error-resilient codecs, achieving JSCC-level robustness while maintaining seamless compatibility with existing systems, and it also strikes a favorable balance between transmission efficiency and speech reconstruction quality.", "AI": {"tldr": "提出了一种基于生成潜在先验的鲁棒语音语义通信框架Glaris，以解决实时语音传输中的包丢失问题。", "motivation": "现有的信道保护机制在带宽和延迟限制下难以有效应对无线网络中的包丢失；传统方法在冗余开销和重建保真度之间难以权衡。", "method": "通过生成潜在空间的鲁棒性编码，实现高质量的接收端包丢失隐藏（PLC）并设计集成错误恢复机制减少错误传播。", "result": "实验结果表明Glaris比现有鲁棒编码更优，同时保持与现有系统的兼容性，并在传输效率和语音重建质量之间取得平衡。", "conclusion": "Glaris框架能够实现JSCC级别的稳健性，有效解决了实时语音通信中的包丢失问题。"}}
{"id": "2512.08198", "pdf": "https://arxiv.org/pdf/2512.08198", "abs": "https://arxiv.org/abs/2512.08198", "authors": ["Yubo Chen", "Di Zhao", "Yun Sing Koh", "Talia Xu"], "title": "Animal Re-Identification on Microcontrollers", "categories": ["cs.CV"], "comment": null, "summary": "Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.", "AI": {"tldr": "本文提出了一种在微控制器上运行的动物再识别框架，解决了现有模型太大无法适用于低内存和低分辨率输入设备的问题。", "motivation": "为了支持野生动物监测和精准畜牧业管理，在有限无线连接的大环境中需要直接在颈圈标签或基于微控制器的边缘节点上进行推断。然而，现有的动物再识别模型大多设计用于工作站或服务器，不适合小内存和低分辨率输入的小型设备。", "method": "首先，分析了最先进的动物再识别模型与MCU级硬件之间的差距，表明知识蒸馏对受限内存和输入分辨率的直接应用效果有限。其次，根据这一分析设计了一种高精度动物再识别架构，通过系统化地调整基于CNN的MobileNetV2骨干网以适应低分辨率输入。最后，在实际数据集上评估框架并引入一种高效的数据微调策略。", "result": "在六个公开的动物再识别数据集中，该紧凑模型实现了竞争性的检索精度，并将模型大小减少了两个数量级。在一个自采集的牛只数据集中，部署后的模型可以在设备上完全进行推理，仅略有准确度下降且Top-1精度未变。", "conclusion": "本文证明了在MCU级设备上实现实用、可适应动物再识别是可行的，并为实际野外环境中的大规模部署铺平了道路。"}}
{"id": "2512.08193", "pdf": "https://arxiv.org/pdf/2512.08193", "abs": "https://arxiv.org/abs/2512.08193", "authors": ["Jiwoo Park", "Ruoqi Liu", "Avani Jagdale", "Andrew Srisuwananukorn", "Jing Zhao", "Lang Li", "Ping Zhang", "Sachin Kumar"], "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "comment": null, "summary": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.", "AI": {"tldr": "ClinicalTrialsHub是一个整合了ClinicalTrials.gov和PubMed文献数据的交互式搜索平台，通过使用大型语言模型来提高临床试验信息的可访问性。", "motivation": "为了增加对结构化临床试验数据的访问，特别是对于患者、临床医生、研究人员和决策者而言，从而促进循证医学的发展。", "method": "ClinicalTrialsHub利用GPT-5.1和Gemini-3-Pro等大型语言模型来解析全文研究文章并提取结构化的试验信息。它将用户查询转换为数据库搜索，并提供带来源句子链接的基于证据的回答系统。", "result": "通过一项涉及临床医生、研究人员和博士生的研究，展示平台的有效性；与仅依赖ClinicalTrials.gov相比，该平台增加了83.8%的数据可访问性。", "conclusion": "ClinicalTrialsHub通过整合多个来源的数据并使用先进的人工智能技术提高了临床试验信息的获取效率。"}}
{"id": "2512.08188", "pdf": "https://arxiv.org/pdf/2512.08188", "abs": "https://arxiv.org/abs/2512.08188", "authors": ["Wenjiang Xu", "Cindy Wang", "Rui Fang", "Mingkang Zhang", "Lusong Li", "Jing Xu", "Jiayuan Gu", "Zecui Zeng", "Rui Chen"], "title": "Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Website at https://embodied-tree-of-thoughts.github.io", "summary": "World models have emerged as a pivotal component in robot manipulation planning, enabling agents to predict future environmental states and reason about the consequences of actions before execution. While video-generation models are increasingly adopted, they often lack rigorous physical grounding, leading to hallucinations and a failure to maintain consistency in long-horizon physical constraints. To address these limitations, we propose Embodied Tree of Thoughts (EToT), a novel Real2Sim2Real planning framework that leverages a physics-based interactive digital twin as an embodied world model. EToT formulates manipulation planning as a tree search expanded through two synergistic mechanisms: (1) Priori Branching, which generates diverse candidate execution paths based on semantic and spatial analysis; and (2) Reflective Branching, which utilizes VLMs to diagnose execution failures within the simulator and iteratively refine the planning tree with corrective actions. By grounding high-level reasoning in a physics simulator, our framework ensures that generated plans adhere to rigid-body dynamics and collision constraints. We validate EToT on a suite of short- and long-horizon manipulation tasks, where it consistently outperforms baselines by effectively predicting physical dynamics and adapting to potential failures. Website at https://embodied-tree-of-thoughts.github.io .", "AI": {"tldr": "本文提出了一个名为Embodied Tree of Thoughts（EToT）的新型现实到仿真再到现实规划框架，该框架利用物理基础互动数字孪生作为本体世界模型来解决机器人操作计划中的问题。", "motivation": "现有的视频生成模型在机器人操纵计划中虽然得到广泛应用，但往往缺乏严格的物理基础，导致幻觉现象和长时间约束下的一致性丧失。本文旨在通过引入一个基于物理的模拟器来克服这些局限性。", "method": "EToT将操作规划表述为一棵搜索树，并通过两个协同机制进行扩展：预先分支（Priori Branching）生成多样化的候选执行路径，反思分支（Reflective Branching）利用视觉语言模型诊断模拟器中的执行失败并迭代地用纠正措施细化计划树。", "result": "在一系列短时和长时操作任务中验证了EToT的有效性，在这些任务中它始终超越基准线，能够有效地预测物理动力学，并适应潜在的故障。", "conclusion": "通过将高层次推理扎根于物理模拟器中，本文框架确保生成的计划符合刚体动态和碰撞约束。"}}
{"id": "2512.08186", "pdf": "https://arxiv.org/pdf/2512.08186", "abs": "https://arxiv.org/abs/2512.08186", "authors": ["Meng Wei", "Chenyang Wan", "Jiaqi Peng", "Xiqian Yu", "Yuqiang Yang", "Delin Feng", "Wenzhe Cai", "Chenming Zhu", "Tai Wang", "Jiangmiao Pang", "Xihui Liu"], "title": "Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "While recent large vision-language models (VLMs) have improved generalization in vision-language navigation (VLN), existing methods typically rely on end-to-end pipelines that map vision-language inputs directly to short-horizon discrete actions. Such designs often produce fragmented motions, incur high latency, and struggle with real-world challenges like dynamic obstacle avoidance. We propose DualVLN, the first dual-system VLN foundation model that synergistically integrates high-level reasoning with low-level action execution. System 2, a VLM-based global planner, \"grounds slowly\" by predicting mid-term waypoint goals via image-grounded reasoning. System 1, a lightweight, multi-modal conditioning Diffusion Transformer policy, \"moves fast\" by leveraging both explicit pixel goals and latent features from System 2 to generate smooth and accurate trajectories. The dual-system design enables robust real-time control and adaptive local decision-making in complex, dynamic environments. By decoupling training, the VLM retains its generalization, while System 1 achieves interpretable and effective local navigation. DualVLN outperforms prior methods across all VLN benchmarks and real-world experiments demonstrate robust long-horizon planning and real-time adaptability in dynamic environments.", "AI": {"tldr": "提出了一种新的双系统基础模型DualVLN，用于通用化的视觉和语言导航任务。", "motivation": "现有的方法在处理动态障碍物等现实世界挑战时表现不佳，存在运动片段化、延迟高等问题。为了解决这些问题，该研究设计了具有高水平推理与低水平动作执行相结合的双系统模型。", "method": "通过引入System 2作为全局规划器进行中期目标预测，以及System 1利用图像基础推理生成平滑准确轨迹的方法实现高效且鲁棒的实时控制。", "result": "实验结果表明，DualVLN在所有视觉语言导航基准测试和现实世界试验中都优于现有方法，在动态环境中具有长距离规划能力和实时适应性。", "conclusion": "该研究通过提出双系统架构解决了现有模型面临的问题，展示了其在复杂环境下的优越性能。"}}
{"id": "2512.08185", "pdf": "https://arxiv.org/pdf/2512.08185", "abs": "https://arxiv.org/abs/2512.08185", "authors": ["Jinghao Wang", "Ping Zhang", "Carter Yagemann"], "title": "A Practical Framework for Evaluating Medical AI Security: Reproducible Assessment of Jailbreaking and Privacy Vulnerabilities Across Clinical Specialties", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages, 1 figure, framework proposal", "summary": "Medical Large Language Models (LLMs) are increasingly deployed for clinical decision support across diverse specialties, yet systematic evaluation of their robustness to adversarial misuse and privacy leakage remains inaccessible to most researchers. Existing security benchmarks require GPU clusters, commercial API access, or protected health data -- barriers that limit community participation in this critical research area. We propose a practical, fully reproducible framework for evaluating medical AI security under realistic resource constraints. Our framework design covers multiple medical specialties stratified by clinical risk -- from high-risk domains such as emergency medicine and psychiatry to general practice -- addressing jailbreaking attacks (role-playing, authority impersonation, multi-turn manipulation) and privacy extraction attacks. All evaluation utilizes synthetic patient records requiring no IRB approval. The framework is designed to run entirely on consumer CPU hardware using freely available models, eliminating cost barriers. We present the framework specification including threat models, data generation methodology, evaluation protocols, and scoring rubrics. This proposal establishes a foundation for comparative security assessment of medical-specialist models and defense mechanisms, advancing the broader goal of ensuring safe and trustworthy medical AI systems.", "AI": {"tldr": "提出了一个评估医疗AI安全性的框架，该框架在真实资源约束下可重复使用，涵盖了多种临床专科的风险等级，并通过合成患者记录进行攻击测试。", "motivation": "目前的医疗大型语言模型（LLMs）广泛应用于临床决策支持，但缺乏系统化的安全性评估方法。现有的安全基准存在硬件要求高、需要商业API访问权限或受保护健康数据等障碍，限制了研究社区的参与。", "method": "设计了一个适用于多种临床专科的风险层次结构的安全性测试框架，涵盖了紧急医学和精神病学等高风险领域以及普通医疗实践。利用合成患者记录进行攻击测试，并使用消费者级别的CPU硬件运行免费可用模型以降低成本。", "result": "提出了一个包括威胁模型、数据生成方法、评估协议和评分标准在内的完整框架规范。该框架旨在为比较不同医疗专科AI系统的安全性以及防御机制提供基础，推动更安全的医疗AI系统的发展。", "conclusion": "本研究提出的方法有助于打破现有医疗AI安全性测试的壁垒，使得更多研究人员能够参与其中，并为进一步提升医疗AI的安全性和可靠性奠定基础。"}}
{"id": "2512.08180", "pdf": "https://arxiv.org/pdf/2512.08180", "abs": "https://arxiv.org/abs/2512.08180", "authors": ["Xiaojing Wei", "Ting Zhang", "Wei He", "Jingdong Wang", "Hua Huang"], "title": "GeoLoom: High-quality Geometric Diagram Generation from Textual Input", "categories": ["cs.CV"], "comment": null, "summary": "High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.", "AI": {"tldr": "GeoLoom是一种用于从文本输入生成高质量几何图形的新型框架。", "motivation": "提出该框架是为了应对高精度几何图绘制中的挑战，并利用形式语言和符号求解器来提高准确性和可解释性。", "method": "GeoLoom包含两个核心组件：自动形式化模块将自然语言转换为生成导向的形式语言GeoLingua，以及坐标求解器使用蒙特卡洛优化将形式约束映射到精确的坐标上。此外还引入了支持框架的数据集GeoNF和基于约束的评估指标。", "result": "实验结果表明，GeoLoom在结构保真度方面显著优于最先进的基线方法。", "conclusion": "该研究为可解释性和可扩展性的图形生成提供了一个原则性基础。"}}
{"id": "2512.08170", "pdf": "https://arxiv.org/pdf/2512.08170", "abs": "https://arxiv.org/abs/2512.08170", "authors": ["Haoxin Zhang", "Shuaixin Li", "Xiaozhou Zhu", "Hongbo Chen", "Wen Yao"], "title": "RAVES-Calib: Robust, Accurate and Versatile Extrinsic Self Calibration Using Optimal Geometric Features", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In this paper, we present a user-friendly LiDAR-camera calibration toolkit that is compatible with various LiDAR and camera sensors and requires only a single pair of laser points and a camera image in targetless environments. Our approach eliminates the need for an initial transform and remains robust even with large positional and rotational LiDAR-camera extrinsic parameters. We employ the Gluestick pipeline to establish 2D-3D point and line feature correspondences for a robust and automatic initial guess. To enhance accuracy, we quantitatively analyze the impact of feature distribution on calibration results and adaptively weight the cost of each feature based on these metrics. As a result, extrinsic parameters are optimized by filtering out the adverse effects of inferior features. We validated our method through extensive experiments across various LiDAR-camera sensors in both indoor and outdoor settings. The results demonstrate that our method provides superior robustness and accuracy compared to SOTA techniques. Our code is open-sourced on GitHub to benefit the community.", "AI": {"tldr": "本文提出了一种鲁棒、准确且通用的LiDAR相机外参自标定方法RAVES-Calib，适用于多种传感器，并通过最优几何特征提高标定效果。", "motivation": "传统的方法需要初始变换并且在大位置和旋转参数下容易失效。本文旨在提供一种用户友好的工具包，在无需目标物的情况下使用单个激光点对和相机图像进行标定，增强鲁棒性和准确性。", "method": "通过Gluestick管道建立2D-3D特征对应关系以获得初始估计，并根据特征分布的影响自适应地调整每个特征的成本权重，优化外参参数。", "result": "实验结果表明本文方法在多种传感器上均具有出色的鲁棒性和精度，优于现有技术。", "conclusion": "提出的方法通过利用最优几何特性提高了LiDAR相机外参标定的鲁棒性与准确性。代码开源以促进研究社区的发展。"}}
{"id": "2512.08169", "pdf": "https://arxiv.org/pdf/2512.08169", "abs": "https://arxiv.org/abs/2512.08169", "authors": ["Guangze Zhao", "Yongzheng Zhang", "Changbo Tian", "Dan Xie", "Hongri Liu", "Bailing Wang"], "title": "Information-Dense Reasoning for Efficient and Auditable Security Alert Triage", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Security Operations Centers face massive, heterogeneous alert streams under minute-level service windows, creating the Alert Triage Latency Paradox: verbose reasoning chains ensure accuracy and compliance but incur prohibitive latency and token costs, while minimal chains sacrifice transparency and auditability. Existing solutions fail: signature systems are brittle, anomaly methods lack actionability, and fully cloud-hosted LLMs raise latency, cost, and privacy concerns. We propose AIDR, a hybrid cloud-edge framework that addresses this trade-off through constrained information-density optimization. The core innovation is gradient-based compression of reasoning chains to retain only decision-critical steps--minimal evidence sufficient to justify predictions while respecting token and latency budgets. We demonstrate that this approach preserves decision-relevant information while minimizing complexity. We construct compact datasets by distilling alerts into 3-5 high-information bullets (68% token reduction), train domain-specialized experts via LoRA, and deploy a cloud-edge architecture: a cloud LLM routes alerts to on-premises experts generating SOAR-ready JSON. Experiments demonstrate AIDR achieves higher accuracy and 40.6% latency reduction versus Chain-of-Thought, with robustness to data corruption and out-of-distribution generalization, enabling auditable and efficient SOC triage with full data residency compliance.", "AI": {"tldr": "该论文提出了一种通过梯度压缩推理链来提高安全警报分类准确性和效率的框架AIDR。", "motivation": "现有的解决方案存在脆弱性、缺乏行动力以及延迟和隐私问题，无法解决繁重的安全操作中心面临的巨大且异构警报流处理难题。", "method": "论文提出了一种混合云边架构AIDR，通过基于梯度的推理链压缩技术保留关键决策步骤来优化信息密度。同时构造紧凑数据集、训练领域专业专家，并部署一个能够将警报转换为SOAR准备状态的边缘架构。", "result": "实验表明，与现有方法相比，AIDR在保持高精度的同时减少了40.6%的时间延迟。", "conclusion": "该研究通过优化推理链的信息密度实现了高效、可审计的安全操作中心分类流程，并且保证了数据合规性。"}}
{"id": "2512.08163", "pdf": "https://arxiv.org/pdf/2512.08163", "abs": "https://arxiv.org/abs/2512.08163", "authors": ["Yuki Kubota", "Taiki Fukiage"], "title": "Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators", "categories": ["cs.CV"], "comment": "22 pages, 12 figures, 1 table", "summary": "Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.", "AI": {"tldr": "研究探讨了单目深度估计模型的准确性与其人类感知相似性之间的关系。", "motivation": "尽管深度神经网络在物理基准上实现了超人级别的准确度，但仍然存在一个挑战：将模型表示与人类感知对齐。这可能有助于增强模型的鲁棒性和可解释性。", "method": "使用KITTI数据集系统地研究了69个单目深度估计器之间的准确性与人类相似性的关系。通过仿射拟合来分解预测误差，将其分为易于理解的部分。", "result": "结果揭示了尽管人和DNN在某些偏误上一致，但模型准确性和人类感知之间存在不同的权衡关系。这意味着提高精度不一定导致更类似人类的行为。", "conclusion": "研究强调超越传统精确度评估的必要性，需要开发多方面的、以人为中心的评估方法。"}}
{"id": "2512.08161", "pdf": "https://arxiv.org/pdf/2512.08161", "abs": "https://arxiv.org/abs/2512.08161", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.", "AI": {"tldr": "本文提出了一种基于多状态感知框架的傅里叶RWKV图像去雾模型，旨在解决真实世界中不均匀雾霾条件下图像去雾的挑战。", "motivation": "虽然Transformer方法在捕获全局上下文方面表现出色，但由于其二次计算复杂度，无法实现实时部署。因此，本文提出了一种新的基于多状态感知框架的方法来应对这一问题。", "method": "该模型通过融合三种不同的感知状态（空间形式感知、频域感知和语义关系感知）实现全面的雾霾退化建模，并且具有线性复杂度。具体来说，使用变形四向令牌移位操作动态调整感受野以适应局部雾霾变化；在傅里叶混合模块中扩展RWKV注意机制到傅里叶域，保留长程依赖并减少空间衰减；利用语义桥模块中的动态语义核融合技术精确对齐编码器-解码器特征。", "result": "实验结果表明，在多个基准数据集上，所提出的模型在各种雾霾场景中实现了最先进的性能，并且显著减少了计算开销。", "conclusion": "本文提出了一种新的傅里叶RWKV图像去雾框架，它通过结合三种感知状态来优化全局上下文建模并减少空间衰减，从而提高了去雾的质量和效率。"}}
{"id": "2512.08160", "pdf": "https://arxiv.org/pdf/2512.08160", "abs": "https://arxiv.org/abs/2512.08160", "authors": ["Nanda K. Unnikrishnan", "Keshab K. Parhi"], "title": "LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "Proc. of 2025 Asilomar Conference on Signals, Systems, and Computers, October 2025, Pacific Grove, CA", "summary": "In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.", "AI": {"tldr": "LayerPipe2 提出了基于改进的指数移动平均法来进行多阶段流水线训练和权重重构的方法，以解决神经网络训练中的存储瓶颈问题。", "motivation": "为了更好地理解引入梯度延迟的程度以及如何在每个层中实现期望的流水线化程度，从而加速神经网络训练。", "method": "通过变量延迟梯度适应和重新定时的形式推导了 LayerPipe 方法，并确定可以在哪些位置插入延迟。提出了基于改进的指数移动平均法来进行多阶段流水线训练和权重重构的方法，以解决存储瓶颈问题。", "result": "该方法减少了内存成本而不会牺牲保证精度，提供了一种构建 LayerPipe 架构、预测其延迟需求并缓解其存储负担的原则性框架，从而实现了可控制通信计算权衡的规模化流水线训练。", "conclusion": "LayerPipe2 方法通过形式化推导和改进的技术解决了神经网络流水线训练中的存储问题，并提供了一种有效的方法来实现大规模训练时的内存成本减少和精度保证。"}}
{"id": "2512.08153", "pdf": "https://arxiv.org/pdf/2512.08153", "abs": "https://arxiv.org/abs/2512.08153", "authors": ["Zheng Ding", "Weirui Ye"], "title": "TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \\textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \\emph{High sample efficiency}, achieving better performance under same training samples (2) \\emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \\emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \\textbf{2.4$\\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.", "AI": {"tldr": "本文介绍了一种名为TreeGRPO的新框架，该框架通过将去噪过程重新表述为搜索树来提高在线RL后训练的效率。", "motivation": "增强学习（RL）后期训练对生成模型与人类偏好的对齐至关重要，但其计算成本高昂是广泛采用的主要障碍。TreeGRPO旨在解决这一问题。", "method": "TreeGRPO通过利用共享初始噪声样本并从这些样本中战略性分支来生成多个候选轨迹，同时有效重用共同的前缀部分，从而实现高效训练。这种树状结构方法提供三个主要优势：高采样效率、细粒度的信用分配以及计算成本分摊。", "result": "实验结果表明，TreeGRPO实现了2.4倍更快的训练速度，并在效率-奖励权衡空间中建立了优越的帕累托前沿。与GRPO基线方法相比，我们的方法在多个基准和奖励模型上表现更为出色。", "conclusion": "该方法为基于RL的视觉生成模型对齐提供了一条可扩展且有效的途径。"}}
{"id": "2512.08147", "pdf": "https://arxiv.org/pdf/2512.08147", "abs": "https://arxiv.org/abs/2512.08147", "authors": ["Henry Anand Septian Radityo", "Bernardus Willson", "Reynard Tanadi", "Latifa Dwiyanti", "Saiful Akbar"], "title": "Scalable Back-End for an AI-Based Diabetes Prediction Application", "categories": ["cs.AI", "cs.SE"], "comment": "This paper was accepted and presented at the 2025 IEEE International Conference on Data and Software Engineering (ICoDSE) on 28 October 2025 in Batam, Indonesia, and is currently awaiting publication", "summary": "The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.", "AI": {"tldr": "开发并评估了适合移动糖尿病预测应用程序的可扩展后端系统。", "motivation": "随着全球糖尿病发病率上升，早期检测变得至关重要。AI驱动的应用程序需要一个响应快、可伸缩的后台架构来有效服务大量用户。", "method": "该体系结构利用水平扩展、数据库分片和通过消息队列进行异步通信。系统测试了24个功能中的83%，验证其性能并处理多达10,000名并发用户的负载。", "result": "83％的功能（20/24）达到了预定的性能目标，包括用户资料管理、活动跟踪和密集读取预测操作。异步通信对减少错误率至关重要，并提高了系统的可靠性。", "conclusion": "系统证明了其可扩展性并能够处理大量并发用户，验证了开发的后端架构的有效性和实用性。"}}
{"id": "2512.08145", "pdf": "https://arxiv.org/pdf/2512.08145", "abs": "https://arxiv.org/abs/2512.08145", "authors": ["Haoran Wang", "Zhuohang Chen", "Guang Li", "Bo Ma", "Chuanghuang Li"], "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.", "AI": {"tldr": "提出了一种基于大语言模型的双代理HUI框架，以提高用户与无人机交互的灵活性和个性化。", "motivation": "传统的人机互动设计难以满足用户的个性化需求，通过引入大型语言模型可以改善人机自然语言理解和任务规划执行的问题。", "method": "构建了两个独立的大语言模型代理（一个负责任务规划，另一个负责执行），并应用不同的提示工程技术来处理任务的理解、规划和执行。", "result": "实验结果表明该框架提高了HUI的流畅性和任务执行的灵活性，有效地满足用户的个性化需求。", "conclusion": "通过建立双代理系统，本研究成功提升了无人机与用户交互的质量，并展示了在多种应用场景中的有效性。"}}
{"id": "2512.08135", "pdf": "https://arxiv.org/pdf/2512.08135", "abs": "https://arxiv.org/abs/2512.08135", "authors": ["Zeyuan Chen", "Xiang Zhang", "Haiyang Xu", "Jianwen Xie", "Zhuowen Tu"], "title": "CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026", "summary": "We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.", "AI": {"tldr": "该论文提出了一种受人类视觉启发的多模态模型CVP，用于空间推理。", "motivation": "现有方法主要依赖于不具结构化的表示方式，导致了有限的空间推理能力。为了弥补这一缺陷，引入了两种互补组件来增强对复杂3D环境的理解。", "method": "该论文提出了一种基于大型多模态模型架构的框架CVP，包含目标亲和令牌（中央视觉）和全向网格（外周视觉），以提高空间推理能力。", "result": "实验表明，CVP在一系列3D场景理解基准上达到了最先进的性能。", "conclusion": "通过结合受人类视觉系统启发的设计，CVP能够更好地理解和处理复杂的空间关系。"}}
{"id": "2512.08130", "pdf": "https://arxiv.org/pdf/2512.08130", "abs": "https://arxiv.org/abs/2512.08130", "authors": ["Gary Ackerman", "Brandon Behlendorf", "Zachary Kallenborn", "Sheriff Almakki", "Doug Clifford", "Jenna LaTourette", "Hayley Peterson", "Noah Sheinbaum", "Olivia Shoemaker", "Anna Wetzel"], "title": "Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "18 pages", "summary": "Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.", "AI": {"tldr": "开发了一种用于评估前沿AI模型生物安全风险的框架，即Biothreat Benchmark Generation (BBG) 框架。", "motivation": "为了量化和减轻大型语言模型（LLMs）带来的生物恐怖主义或获取生物武器的风险，需要建立评估这些模型生物安全威胁的方法。", "method": "提出了一个分层结构的生物威胁类别、元素和任务体系，并基于此开发了与任务相匹配的查询架构。这个框架首先应用于细菌性生物威胁，称为Bacterial Biothreat Schema。", "result": "描述了一种新的评估前沿AI模型生物安全风险的方法，即BBG框架及其初步成果Bacterial Biothreat Schema。", "conclusion": "提出的BBG框架提供了一个强大的、可重复使用的结构来评估大型语言模型带来的细菌性生物威胁，在多个层次上满足了技术与操作需求，并考虑了广泛的对手能力。"}}
{"id": "2512.08125", "pdf": "https://arxiv.org/pdf/2512.08125", "abs": "https://arxiv.org/abs/2512.08125", "authors": ["Tharindu Wickremasinghe", "Chenyang Qi", "Harshana Weligampola", "Zhengzhong Tu", "Stanley H. Chan"], "title": "FlowSteer: Conditioning Flow Field for Consistent Image Restoration", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Flow-based text-to-image (T2I) models excel at prompt-driven image generation, but falter on Image Restoration (IR), often \"drifting away\" from being faithful to the measurement. Prior work mitigate this drift with data-specific flows or task-specific adapters that are computationally heavy and not scalable across tasks. This raises the question \"Can't we efficiently manipulate the existing generative capabilities of a flow model?\" To this end, we introduce FlowSteer (FS), an operator-aware conditioning scheme that injects measurement priors along the sampling path,coupling a frozed flow's implicit guidance with explicit measurement constraints. Across super-resolution, deblurring, denoising, and colorization, FS improves measurement consistency and identity preservation in a strictly zero-shot setting-no retrained models, no adapters. We show how the nature of flow models and their sensitivities to noise inform the design of such a scheduler. FlowSteer, although simple, achieves a higher fidelity of reconstructed images, while leveraging the rich generative priors of flow models.", "AI": {"tldr": "本文提出了FlowSteer，一种条件流场的方法，在图像恢复任务中提高了测量一致性和身份保存。", "motivation": "现有的基于流的文本到图像生成模型在图像恢复方面表现不佳，常常偏离测量的真实情况。当前的工作通过特定数据或任务适配器来缓解这一问题，但这些方法计算复杂且不可扩展。", "method": "本文引入了FlowSteer（FS），这是一种操作感知条件方案，在采样路径上注入测量先验信息，并结合冻结流的隐式引导与显式测量约束。这种调度设计利用流模型对噪声敏感的特点来提高图像恢复质量。", "result": "在超分辨率、去模糊、去噪和色彩化等任务中，FlowSteer提高了重建图像的质量，在零样本设置下无需重新训练或适配器即可实现更高的图像一致性与身份保存。", "conclusion": "虽然简单，但FlowSteer利用流模型的丰富生成先验知识实现了更高保真度的图像恢复。"}}
{"id": "2512.08124", "pdf": "https://arxiv.org/pdf/2512.08124", "abs": "https://arxiv.org/abs/2512.08124", "authors": ["Zijiang Yang"], "title": "Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "ef:2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, pp. 1-8", "summary": "This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.", "AI": {"tldr": "本文提出了一种基于神经网络的新型机器学习方法来管理加密货币投资组合，通过预测每种加密货币未来收益排名并分配权重。", "motivation": "以往研究主要集中在特定加密货币如比特币的价格走势上。本文则关注一组加密货币之间的相对关系，并提出了利用神经网络进行预测和管理的方法。", "method": "使用神经网络模型来预测每个时间步长内加密货币未来的收益率排名，然后根据这些预测结果分配投资权重。", "result": "通过实证实验显示，在2020年5月至2023年11月的市场周期中，该方法表现出色。在复杂的市场条件下实现了64.26％的年度化回报率和1.01的夏普比率，并且对交易费用增加具有鲁棒性。", "conclusion": "与现有方法相比，提出的基于神经网络的投资组合管理策略表现优越，在多种市场环境下均能取得较高的收益。"}}
{"id": "2512.08121", "pdf": "https://arxiv.org/pdf/2512.08121", "abs": "https://arxiv.org/abs/2512.08121", "authors": ["Stephane Collot", "Colin Fraser", "Justin Zhao", "William F. Shen", "Timon Willi", "Ilias Leontiadis"], "title": "Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 5 figures", "summary": "Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.", "AI": {"tldr": "该论文通过Youden的J统计量解释了平衡准确性是评估大型语言模型（LLM）裁判的最佳指标。", "motivation": "传统评估指标如准确率、精确度和F1值易受类别不平衡影响，且对正类别的选择随意。因此，选择合适的分类器变得困难。论文旨在通过Youden的J统计量展示平衡准确性如何解决这些问题。", "method": "论文使用理论论证和实证例子及模拟证明了平衡准确性和Youden的J统计量在评估模型裁判时具有等价性，并展示了使用平衡准确性进行裁判选择的有效性。", "result": "通过分析与实验证明，基于平衡准确性的分类器选择比传统指标更稳健、更具可靠性。", "conclusion": "论文建议在比较和评价大型语言模型性能时采用平衡准确性作为评估标准，以确保评估过程的公正性和有效性。"}}
{"id": "2512.08111", "pdf": "https://arxiv.org/pdf/2512.08111", "abs": "https://arxiv.org/abs/2512.08111", "authors": ["Qi Sun", "Jingru Zhang"], "title": "The Bichromatic Two-Center Problem on Graphs", "categories": ["cs.DS"], "comment": null, "summary": "In this paper, we study the (weighted) bichromatic two-center problem on graphs. The input consists of a graph $G$ of $n$ (weighted) vertices and $m$ edges, and a set $\\mathcal{P}$ of pairs of distinct vertices, where no vertex appears in more than one pair. The problem aims to find two points (i.e., centers) on $G$ by assigning vertices of each pair to different centers so as to minimize the maximum (weighted) distance of vertices to their assigned centers (so that the graph can be bi-colored with this goal). To the best of our knowledge, this problem has not been studied on graphs, including tree graphs. In this paper, we propose an $O(m^2n\\log n\\log mn)$ algorithm for solving the problem on an undirected graph provided with the distance matrix, an $O(n\\log n)$-time algorithm for the problem on trees, and a linear-time approach for the unweighted tree version.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.08108", "pdf": "https://arxiv.org/pdf/2512.08108", "abs": "https://arxiv.org/abs/2512.08108", "authors": ["Kwanyoung Park", "Seohong Park", "Youngwoon Lee", "Sergey Levine"], "title": "Scalable Offline Model-Based RL with Action Chunks", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \\emph{action-chunk} model that predicts a future state from a sequence of actions (an \"action chunk\") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \\textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.", "AI": {"tldr": "本文研究了基于模型的强化学习方法是否可以为离线RL中复杂、长时域任务提供可扩展方案。", "motivation": "探索在离线强化学习中，通过基于模型的价值扩张能否应对复杂且长时间跨度的任务，并提出减少预测错误的方法。", "method": "提出了MAC算法，使用动作块（action chunk）模型，该模型从一系列动作序列而非单一动作预测未来状态，从而降低累积的模型误差。此外，采用拒绝采样方法防止由分布外的动作引起的模型滥用。", "result": "实验结果显示，在大型数据集上的挑战性任务中，MAC算法在离线基于模型的强化学习算法中表现最佳，尤其是在长时域任务上表现出色。", "conclusion": "证明了使用动作块（action chunk）和拒绝采样技术可以显著提高复杂、长时间跨度任务下的性能。"}}
{"id": "2512.08107", "pdf": "https://arxiv.org/pdf/2512.08107", "abs": "https://arxiv.org/abs/2512.08107", "authors": ["Stephan Carney", "Soham Hans", "Sofia Hirschmann", "Stacey Marsella", "Yvonne Fonken", "Peggy Wu", "Nikolos Gurney"], "title": "Detecting Ambiguity Aversion in Cyberattack Behavior to Inform Cognitive Defense Strategies", "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Adversaries (hackers) attempting to infiltrate networks frequently face uncertainty in their operational environments. This research explores the ability to model and detect when they exhibit ambiguity aversion, a cognitive bias reflecting a preference for known (versus unknown) probabilities. We introduce a novel methodological framework that (1) leverages rich, multi-modal data from human-subjects red-team experiments, (2) employs a large language model (LLM) pipeline to parse unstructured logs into MITRE ATT&CK-mapped action sequences, and (3) applies a new computational model to infer an attacker's ambiguity aversion level in near-real time. By operationalizing this cognitive trait, our work provides a foundational component for developing adaptive cognitive defense strategies.", "AI": {"tldr": "本文通过分析黑客在网络攻击中的不确定性行为，提出了一种检测其模糊厌恶的新方法。", "motivation": "对手在渗透网络时经常面临操作环境的不确定性。研究旨在探索如何建模和检测他们的模糊厌恶倾向，以开发适应性的认知防御策略。", "method": "该研究利用多模态数据进行红队实验，并采用大型语言模型管道解析未结构化的日志为MITRE ATT&CK映射的动作序列，再应用新的计算模型实时推断攻击者的模糊厌恶水平。", "result": "本文提出的方法能够在近实时环境中识别出对手的模糊厌恶倾向。", "conclusion": "通过将认知特质操作化，该研究提供了开发适应性防御策略的基础组件。"}}
{"id": "2512.08099", "pdf": "https://arxiv.org/pdf/2512.08099", "abs": "https://arxiv.org/abs/2512.08099", "authors": ["Matthias Beckmann", "Robert Beinert", "Jonas Bresch"], "title": "Generalizations of the Normalized Radon Cumulative Distribution Transform for Limited Data Recognition", "categories": ["math.NA", "cs.CV", "cs.IT"], "comment": null, "summary": "The Radon cumulative distribution transform (R-CDT) exploits one-dimensional Wasserstein transport and the Radon transform to represent prominent features in images. It is closely related to the sliced Wasserstein distance and facilitates classification tasks, especially in the small data regime, like the recognition of watermarks in filigranology. Here, a typical issue is that the given data may be subject to affine transformations caused by the measuring process. To make the R-CDT invariant under arbitrary affine transformations, a two-step normalization of the R-CDT has been proposed in our earlier works. The aim of this paper is twofold. First, we propose a family of generalized normalizations to enhance flexibility for applications. Second, we study multi-dimensional and non-Euclidean settings by making use of generalized Radon transforms. We prove that our novel feature representations are invariant under certain transformations and allow for linear separation in feature space. Our theoretical results are supported by numerical experiments based on 2d images, 3d shapes and 3d rotation matrices, showing near perfect classification accuracies and clustering results.", "AI": {"tldr": "提出了广义规范化Radon累积分布变换，增强了图像识别在小数据集下的鲁棒性和灵活性。", "motivation": "为了增强R-CDT的鲁棒性并使其能够处理更多的应用场合，在现有工作的基础上提出了一种新的、更为灵活的广义规范化的方案，并探讨了多维和非欧空间中的变换。这些改进的目标是使新方法在图像识别任务中具有更好的分类准确度。", "method": "通过引入一系列规范化步骤，实现了R-CDT对任意仿射变换的不变性；并利用广义Radon变换扩展到多个维度及非欧空间。", "result": "实验结果表明，新的特征表示法在图像、形状和旋转矩阵等多维数据上展现出近乎完美的分类精度和聚类效果。", "conclusion": "通过理论证明和支持于数值试验的结果，确认了新方法的优越性；此研究为图像识别领域提供了有效的方法论基础。"}}
{"id": "2512.08093", "pdf": "https://arxiv.org/pdf/2512.08093", "abs": "https://arxiv.org/abs/2512.08093", "authors": ["Manas Joglekar", "Jeremy Chen", "Gabriel Wu", "Jason Yosinski", "Jasmine Wang", "Boaz Barak", "Amelia Glaese"], "title": "Training LLMs for Honesty via Confessions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions. In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the \"path of least resistance\" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior. To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its \"main\" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.", "AI": {"tldr": "该论文提出了一种通过自我坦白的方法来训练大型语言模型（LLM）使其更加诚实。", "motivation": "由于强化学习中的奖励塑形问题，大语言模型在报告它们的行为和信念时可能会变得不诚实。这种方法旨在鼓励模型诚实地表达自身的缺点。", "method": "提出了一种新的机制，在模型给出主要答案后要求其进行坦白。坦白的奖励仅基于它的诚实性，并不会影响到主回答的奖励。", "result": "实验发现当模型在其“主要”答案中撒谎或隐瞒不足时，它经常诚实地在坦白中承认这些行为，而且这种坦白的诚实性会随着训练而有所提高。", "conclusion": "这种方法可以用来监控模型、拒绝抽样以及将问题向用户展示出来。"}}
{"id": "2512.08082", "pdf": "https://arxiv.org/pdf/2512.08082", "abs": "https://arxiv.org/abs/2512.08082", "authors": ["Vala Vakilian", "Zimeng Wang", "Ankit Singh Rawat", "Christos Thrampoulidis"], "title": "Short-Context Dominance: How Much Local Context Natural Language Actually Needs?", "categories": ["cs.CL", "cs.AI"], "comment": "38 pages, 7 figures, includes appendix and references", "summary": "We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.", "AI": {"tldr": "研究了自然语言中最常用的短上下文预测方法的有效性，并提出了检测长上下文序列的新指标，以改进大型语言模型的输出。", "motivation": "探索短上下文是否足以准确预测大多数序列的下一个标记，以及如何识别需要更长上下文的情况，从而优化语言模型的表现。", "method": "利用大规模语言模型作为统计预言机，测量最小上下文长度（MCL），引入了分布感知MCL（DaMCL）指标，并通过阈值设置来区分短和长上下文序列。最后开发了一种解码算法以减少偏见影响。", "result": "对于1到7k标记的序列，约80%的情况只需最近96个标记即可进行准确预测；新方法能够有效识别需要更多上下文的信息并改进模型性能。", "conclusion": "证明了短上下文在大多数情况下足够使用，并提供了识别和处理长上下文需求的有效策略。"}}
{"id": "2512.08075", "pdf": "https://arxiv.org/pdf/2512.08075", "abs": "https://arxiv.org/abs/2512.08075", "authors": ["Christian Massao Konishi", "Helio Pedrini"], "title": "Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models", "categories": ["cs.CV"], "comment": null, "summary": "The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.", "AI": {"tldr": "使用变化检测模型识别亚马逊雨林的砍伐区域", "motivation": "保护亚马逊雨林是应对气候变化、生物多样性保护和土著文化保护的重要全球优先事项。卫星监测项目PRODES在这一努力中发挥重要作用，但现有方法仍存在不足，需要更有效的模型和支持策略来改善效果。", "method": "评估多种变化检测模型及其不同预处理和后处理技术的效果，并测试结合不同模型以提高整体性能的方法，包括基于卷积网络和自注意力机制的Transformer网络。", "result": "通过组合模型和技术改进，达到了80.41%的F1分数，优于其他近期研究结果。", "conclusion": "该工作展示了如何利用现代机器学习架构和策略来提高亚马逊雨林砍伐区域识别的效果。"}}
{"id": "2512.08057", "pdf": "https://arxiv.org/pdf/2512.08057", "abs": "https://arxiv.org/abs/2512.08057", "authors": ["Md Mostafizer Rahman", "Ariful Islam Shiplu", "Md Faizul Ibne Amin", "Yutaka Watanobe", "Lu Peng"], "title": "Large Language Models for Education and Research: An Empirical and User Survey-based Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.", "AI": {"tldr": "本论文通过背景技术分析、实证实验和真实用户调查，评估了ChatGPT与DeepSeek在教育与科研领域的性能。", "motivation": "大型语言模型在教育和研究领域表现出显著效果，旨在全面评价这些模型的准确率、计算效率及用户体验。", "method": "通过背景技术分析、实证实验以及用户问卷调查对ChatGPT与DeepSeek进行评估，测试内容包括文本生成、编程任务和专业问题解决能力。", "result": "实验证明ChatGPT在通用语言理解和文本生成方面表现出色，而DeepSeek因效率设计优势，在编程任务上更胜一筹。同时两个模型都能准确输出医学诊断结果并有效解答复杂数学难题。", "conclusion": "研究强调了大型语言模型对教育和科研领域的促进作用，并提出实践中的应用局限性，为未来改进提供了方向。"}}
{"id": "2512.08052", "pdf": "https://arxiv.org/pdf/2512.08052", "abs": "https://arxiv.org/abs/2512.08052", "authors": ["Pedro Santana"], "title": "An Introduction to Deep Reinforcement and Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.", "AI": {"tldr": "介绍了深度强化学习和模仿学习在实体代理中的应用，重点讲解了从马尔可夫决策过程到PPO的深度强化学习算法以及行为克隆到GAIL的深度模仿学习技术。", "motivation": "由于手动设计控制器存在困难，学习方法尤其是深度强化学习（DRL）和深度模仿学习（DIL），在解决复杂顺序决策问题上变得非常重要。", "method": "该文采用了一种深度优先的方法介绍了DRL和DIL的基础算法和技术。对于DRL讲解了从马尔可夫决策过程到REINFORCE及PPO等内容；对于DIL涵盖了行为克隆、DAgger以及GANIL等。", "result": "提供了关于DRL和DIL的深入理解，并非对领域进行广泛覆盖，而是专注于少数基础算法和技术。", "conclusion": "总结了深度强化学习与模仿学习在解决实体代理任务中的重要性及基本原理。"}}
{"id": "2512.08048", "pdf": "https://arxiv.org/pdf/2512.08048", "abs": "https://arxiv.org/abs/2512.08048", "authors": ["Chandler Timm C. Doloriel"], "title": "Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning", "categories": ["cs.CV"], "comment": "ongoing work", "summary": "Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\\ pixel; frequency: all vs.\\ low vs.\\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\\%/19.8\\%/39.2\\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.", "AI": {"tldr": "研究提出了一种简单有效的连续测试时间适应方法，通过随机掩码生成不同视图并使用一致性损失和熵最小化损失进行预测调整。", "motivation": "传统连续测试时间自适应方法依赖于校准的不确定性或稳定的注意力分数，并增加了复杂性。本文探讨了是否可以通过简单的随机掩码设计实现有效且鲁棒的持续学习。", "method": "提出Mask to Adapt (M2A) 方法，通过生成短序列的不同视图（空间或频域）并使用一致性损失和熵最小化损失进行预测调整。研究了两种常见掩码类别：空间掩码和频率掩码，并进一步比较了其中的子类型。", "result": "在CIFAR10C/CIFAR100C/ImageNetC（严重程度5）上，M2A（空间）取得了8.3%/19.8%/39.2% 的平均误差率，优于或匹敌强大的CTTA基准方法。简单的随机掩码设计证明有效且鲁棒。", "conclusion": "研究表明，简单的随机掩码调度与一致性及熵最小化目标相结合足以驱动有效的测试时间适应而无需依赖不确定性或注意力信号。"}}
{"id": "2512.08042", "pdf": "https://arxiv.org/pdf/2512.08042", "abs": "https://arxiv.org/abs/2512.08042", "authors": ["Chandler Timm C. Doloriel", "Habib Ullah", "Kristian Hovde Liland", "Fadi Al Machot", "Ngai-Man Cheung"], "title": "Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking", "categories": ["cs.CV"], "comment": null, "summary": "Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).", "AI": {"tldr": "本文研究了一种基于频率域掩码的训练策略，用于提升对AI生成图像的一般化检测能力。", "motivation": "现有的通用Deepfake检测方法依赖于空间特征或大规模预训练模型，这在计算资源方面效率低下。该论文旨在通过频率域掩码等技术，实现更高效的大型数据集上的Deepfake检测。", "method": "本文提出了一种新的基于频率域掩码的训练策略，这种方法包括随机掩码和几何变换，并特别强调了频率掩码的有效性来提高模型对新生成器的一般化性能。", "result": "该方法在GAN和扩散生成图像数据集上实现了最先进的泛化效果，并且即使在显著修剪模型的情况下也能保持稳健的检测性能。", "conclusion": "频率域掩码技术为可持续、通用化的Deepfake检测提供了新的路径，展示了其作为实际应用中有效解决方案的巨大潜力。"}}
{"id": "2512.08040", "pdf": "https://arxiv.org/pdf/2512.08040", "abs": "https://arxiv.org/abs/2512.08040", "authors": ["Youngjoon Jang", "Liliane Momeni", "Zifan Jiang", "Joon Son Chung", "Gül Varol", "Andrew Zisserman"], "title": "Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.", "AI": {"tldr": "开发一种统一模型，用于手语翻译和字幕对齐。", "motivation": "旨在实现连续手势视频到口语文本的转换以及手势与字幕的时间对齐，以促进实用交流、大规模语料库构建及教育应用。", "method": "使用轻量级视觉骨干网络捕捉人体关键点和唇部区域图像的手势和非手势线索；采用Sliding Perceiver映射网络将连续视觉特征聚合为词级别嵌入，以弥合视觉与文本之间的差距；通过多任务可扩展训练策略联合优化手语翻译和字幕对齐。", "result": "在BOBSL（英国手语）数据集上实现了当前最佳的手语翻译和字幕对齐性能，并展示了零样本跨语言泛化能力和How2Sign（美国手语）上的微调表现。", "conclusion": "模型设计强健，通过大规模多语言预训练展示出在不同手语之间实现可扩展翻译的潜力。"}}
{"id": "2512.08038", "pdf": "https://arxiv.org/pdf/2512.08038", "abs": "https://arxiv.org/abs/2512.08038", "authors": ["Elifnur Sunger", "Tales Imbiriba", "Peter Campbell", "Deniz Erdogmus", "Stratis Ioannidis", "Jennifer Dy"], "title": "SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification", "categories": ["cs.CV"], "comment": "20 pages, 16 figures", "summary": "Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.", "AI": {"tldr": "提出了一种新的解释方法SSplain，用于从视网膜图像中分类早产儿视网膜病变（ROP），该方法在生成像素级解释时保持了输入图像的平滑度和稀疏性。", "motivation": "现有的模型解释器未能很好地保留输入图像结构如平滑性和稀疏性，这导致难以理解黑盒模型的工作原理。因此，需要一种新的解释方法来解决这些问题，并帮助临床医生更好地理解和信任模型输出。", "method": "SSplain通过定义一个优化问题并使用交替方向乘子法（ADMM）解决了该问题，生成了既保持图像平滑度和稀疏性的像素级解释。", "result": "实验结果表明，SSplain在后验准确性和平滑性分析方面优于常用的方法，并且能够识别临床医生认为与ROP相关的可辨识特征。此外，在其他公开数据集上也验证了其泛化能力。", "conclusion": "SSplain提供了一种新颖的解释方法，可以更好地理解黑盒模型的工作机制，从而提高诊断早产儿视网膜病变（ROP）的信任度和准确性。"}}
{"id": "2512.08036", "pdf": "https://arxiv.org/pdf/2512.08036", "abs": "https://arxiv.org/abs/2512.08036", "authors": ["Mohammadreza Jalaeian", "Dane A. Morey", "Michael F. Rayo"], "title": "Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration", "categories": ["cs.HC", "cs.AI", "eess.SY"], "comment": "10 pages", "summary": "Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.", "AI": {"tldr": "本文提出了设计支持人机协作活动的启发式方法。", "motivation": "设计支持人机合作的工具，不仅需要考虑技术的可用性，还需要确保这些技术能够有效地作为团队的一部分发挥作用。", "method": "作者综合了来自显示设计、人类因素、认知系统工程学、认知心理学和计算机科学等领域的文献，提出了14项启发式原则来指导和支持联合人机活动的设计、开发与评估。", "result": "提出的14条启发式方法可以用于支持团队的五大主要宏观认知功能：事件检测、意义建构、适应性、视角转换以及协调。", "conclusion": "这十四种启发式的提出和应用为设计更加有效的联合人机活动系统提供了理论基础与实践指导。"}}
{"id": "2512.08032", "pdf": "https://arxiv.org/pdf/2512.08032", "abs": "https://arxiv.org/abs/2512.08032", "authors": ["Arghavan Sanei", "Chaima Amiri", "Atefeh Shokrizadeh", "Jinghui Cheng"], "title": "What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions", "categories": ["cs.SE", "cs.HC"], "comment": "22 pages, 6 figures", "summary": "The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.", "AI": {"tldr": "本文通过分析五个开源软件项目的讨论，研究了这些项目中有关易用性的争论的特点和质量。", "motivation": "开源软件的可用性常被忽视，而论证在参与者之间的意见表达及说服过程中起着关键作用。目前尚不清楚该对话的质量如何，这阻碍了对其有效支持。", "method": "本文分析了五个开源项目中的论据类型及其质量，并考察了这些特征对参与者后续行为的影响。", "result": "研究发现易用性讨论主要依赖于论证，但其质量因项目的不同而有所不同。评论中的论点普遍比帖子差，表明社区在可用性的集体智力上存在缺陷。", "conclusion": "这项研究表明如何帮助开源软件利益相关者构建更有效的论证，并最终提升软件的可用性，同时也为其他分布式协作社群的研究提供了参考。"}}
{"id": "2512.08029", "pdf": "https://arxiv.org/pdf/2512.08029", "abs": "https://arxiv.org/abs/2512.08029", "authors": ["Tianxingjian Ding", "Yuanhao Zou", "Chen Chen", "Mubarak Shah", "Yu Tian"], "title": "CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\\%, and significantly surpasses all other medical-specific large language models.", "AI": {"tldr": "CLARITY是一种用于预测癌症病情动态演变的世界模型，旨在为临床决策提供指导。", "motivation": "目前的静态AI预测器无法准确地预测疾病的动态发展过程。现有的世界模型主要关注视觉重建而非因果生理转换，并且缺乏将预测与治疗决定链接起来的反馈机制。CLARITY旨在填补这些空白。", "method": "CLARITY通过在结构化潜在空间中直接预测疾病演变，结合时间间隔和患者特定数据来建模基于治疗条件下的病情进展为平滑、可解释的轨迹，并生成生理学忠实且个性化的治疗计划。", "result": "CLARITY在处理MU-Glioma-Post数据集时表现优于最近的方法MeWM12%，并显著超越了所有其他医学特定的大语言模型。", "conclusion": "CLARITY提出了一个新颖的预测到决策框架，能够生成透明且可执行的治疗建议，并展示了其在癌症病情演变中的优越性能。"}}
{"id": "2512.08028", "pdf": "https://arxiv.org/pdf/2512.08028", "abs": "https://arxiv.org/abs/2512.08028", "authors": ["Lampis Papakostas", "Aristeidis Geladaris", "Athanasios Mastrogeorgiou", "Jim Sharples", "Gautier Hattenberger", "Panagiotis Chatzakos", "Panagiotis Polygerinos"], "title": "Optimized Area Coverage in Disaster Response Utilizing Autonomous UAV Swarm Formations", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a UAV swarm system designed to assist first responders in disaster scenarios like wildfires. By distributing sensors across multiple agents, the system extends flight duration and enhances data availability, reducing the risk of mission failure due to collisions. To mitigate this risk further, we introduce an autonomous navigation framework that utilizes a local Euclidean Signed Distance Field (ESDF) map for obstacle avoidance while maintaining swarm formation with minimal path deviation. Additionally, we incorporate a Traveling Salesman Problem (TSP) variant to optimize area coverage, prioritizing Points of Interest (POIs) based on preassigned values derived from environmental behavior and critical infrastructure. The proposed system is validated through simulations with varying swarm sizes, demonstrating its ability to maximize coverage while ensuring collision avoidance between UAVs and obstacles.", "AI": {"tldr": "本文提出了一种利用自主无人机群优化灾难响应区域覆盖的系统", "motivation": "为了延长飞行时间，提高数据可用性，并减少由于碰撞导致的任务失败风险，在灾难场景中（如野火）协助第一反应者", "method": "通过使用局部欧几里得签名距离场地图进行避障并保持无人机群组形式以最小化路径偏差；结合旅行商问题变体优化区域覆盖，根据环境行为和关键基础设施预先分配值来优先考虑兴趣点", "result": "该系统在不同无人机数量的仿真中得到验证，展示了其最大化覆盖范围的同时确保无人机与障碍物之间的碰撞避免的能力", "conclusion": "所提出的系统有效地解决了灾难响应中的区域覆盖问题，并通过减少无人机间的碰撞风险提高了任务成功率"}}
{"id": "2512.08026", "pdf": "https://arxiv.org/pdf/2512.08026", "abs": "https://arxiv.org/abs/2512.08026", "authors": ["Caroline N. Leach", "Mitchell A. Klusty", "Samuel E. Armstrong", "Justine C. Pickarski", "Kristen L. Hankins", "Emily B. Collier", "Maya Shah", "Aaron D. Mullen", "V. K. Cody Bumgardner"], "title": "Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching", "categories": ["cs.AI"], "comment": "10 pages, 2 figures, submitted to AMIA", "summary": "Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.", "AI": {"tldr": "该论文提出了一种利用人工智能增强的患者临床试验匹配系统，旨在提高筛查效率和准确性。", "motivation": "当前筛选患者参与临床试验的过程耗时且资源密集。此系统通过集成异构电子健康记录数据、促进专家审查并确保严格的隐私安全标准来解决这些问题。", "method": "利用开源推理增强的大规模语言模型（LLMs），该系统生成结构化资格评估以及支持人工介入审核的可解释推理链，将患者资格表示为动态状态而非固定确定。", "result": "通过减少协调员负担、智能扩大每个患者的试验范围及确保所有AI输出的全面审计能力，此工具能够更高效地匹配患者和临床试验。", "conclusion": "该系统展示了利用人工智能技术提高临床试验筛选过程效率的巨大潜力，并为未来的应用开发奠定了基础。"}}
{"id": "2512.08025", "pdf": "https://arxiv.org/pdf/2512.08025", "abs": "https://arxiv.org/abs/2512.08025", "authors": ["Varun Shiri", "Maggie Xiong", "Jin L. C. Guo", "Jinghui Cheng"], "title": "\"Your Privacy is Your Responsibility\": Understanding How Users Collectively Navigate the Complexity of Privacy on Quora", "categories": ["cs.HC"], "comment": "24 pages, 4 figures", "summary": "In the current technology environment, users are often in a vulnerable position when it comes to protecting their privacy. Previous efforts to promote privacy protection have largely focused on top-down approaches such as regulation and technology design, missing opportunities to understand how to empower users through bottom-up, collective approaches. Our paper addresses this by analyzing what and how privacy-related topics are discussed on Quora. We identified a wide range of interconnected privacy topics brought up by the users, including privacy risks and dangers, protection strategies, organizational practices, and existing laws and regulations. Our results highlight the interplay among the individual, technological, organizational, and societal factors affecting users' privacy attitudes. Moreover, we provide implications for designing community-based tools to better support users' collective efforts in navigating privacy, tools that incorporate users' diverse privacy-related behaviors and preferences, simplify information access and sharing, and connect designers and developers with the user community.", "AI": {"tldr": "本文通过分析Quora上用户讨论的隐私相关话题，研究了用户如何集体应对隐私保护的问题。", "motivation": "当前技术环境下，个人隐私保护面临挑战。过去的研究多关注自上而下的方法如监管和技术设计，忽视了从下而上的集体行动的重要性。", "method": "本文分析了Quora平台上关于隐私的讨论话题，包括隐私风险、防护策略和相关法律等。", "result": "研究结果揭示了个体、技术、组织和社会因素在用户隐私态度中的相互作用，并提出了设计社区工具以支持用户的集体努力的方法论建议。", "conclusion": "通过分析Quora上的对话，本文强调了用户集体参与隐私保护的重要性，并提倡开发新的社区工具来简化信息访问和分享，连接开发者与用户社群。"}}
{"id": "2512.08016", "pdf": "https://arxiv.org/pdf/2512.08016", "abs": "https://arxiv.org/abs/2512.08016", "authors": ["Jiyoon Pyo", "Yuankun Jiao", "Dongwon Jung", "Zekun Li", "Leeje Jang", "Sofia Kirsanova", "Jina Kim", "Yijun Lin", "Qin Liu", "Junyi Xie", "Hadi Askari", "Nan Xu", "Muhao Chen", "Yao-Yi Chiang"], "title": "FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.", "AI": {"tldr": "本文提出了FRIEDA，一个用于评估视觉语言模型在多步骤地图推理能力的基准测试。", "motivation": "传统的地图理解通常被视为图表理解的一个特例，忽略了层状符号学、空间关系等方面的复杂性。为弥补这一不足，作者创建了FRIEDA来全面测试LVLM的地图推理能力。", "method": "FRIEDA使用真实的地图图像，并涵盖GIS文献中定义的三种空间关系类型：拓扑关系、度量关系和方向关系。实验评估了十一种最先进的LVLM模型在两种设置下的表现，即直接提供相关地图和上下文性选择所需的地图。", "result": "即使是最先进的模型Gemini-2.5-Pro和GPT-5-Think，在FRIEDA测试中的准确率也分别仅为38.20%和37.20%，远低于人类水平的84.87%，表明多步骤地图推理仍然是一个挑战。", "conclusion": "研究结果揭示了LVLM在多步骤地图推理方面的持续差距，强调了FRIEDA作为推动LVLM空间智能发展的严格基准测试的重要性。"}}
{"id": "2512.08009", "pdf": "https://arxiv.org/pdf/2512.08009", "abs": "https://arxiv.org/abs/2512.08009", "authors": ["Ava Hays", "Nolan Kosnic", "Ryan Miller", "Kunal Siddhawar"], "title": "Resonant and Stochastic Vibration in Neurorehabilitation", "categories": ["cs.ET", "cs.HC", "cs.NE"], "comment": "8 pages, 6 figures", "summary": "Neurological injuries and age-related decline can impair sensory processing and disrupt motor coordination, gait, and balance. As mechanisms of neuroplasticity have become better understood, vibration-based interventions have gained attention as potential tools to stimulate sensory pathways and motor circuits to support functional recovery. This survey reviews stochastic and resonant vibration modalities, describing their mechanisms, therapeutic rationales, and clinical applications. We synthesize evidence on whole-body vibration for improving balance, mobility, and fine motor function in aging adults, stroke survivors, and individuals with Parkinson's disease, with attention to challenges in parameter optimization, generalizability, and safety. We also assess recent developments in focused muscle vibration and wearable stochastic resonance devices for upper-limb rehabilitation, evaluating their clinical promise along with limitations in scalability, ecological validity, and standardization. Across these modalities, we identify key variables that shape therapeutic outcomes and highlight ongoing efforts to refine protocols, improve usability, and integrate vibration techniques into broader neurorehabilitation frameworks. We conclude by outlining the most important research needs for translating vibration-based interventions into reliable and deployable clinical tools.", "AI": {"tldr": "综述了神经康复中随机振动和共振振动的机制、治疗原理及其临床应用，探讨了这些技术在改善老年人群、中风患者及帕金森病患者的平衡能力和上肢功能方面的潜力。", "motivation": "神经系统损伤和年龄相关的退化会影响感觉处理与运动协调能力，通过理解神经可塑性机制，振动干预成为刺激感觉通路和支持康复的重要工具。文章旨在探讨振动技术的临床应用前景及其挑战。", "method": "系统回顾了随机振动和共振振动的技术原理、治疗理念，并评价了这些方法在改善患者平衡、移动能力和精细运动功能方面的效果；同时分析了参数优化、普适性和安全性等关键问题，评估了新型聚焦肌肉振动技术和穿戴式随机共振设备的临床应用。", "result": "研究综合现有证据表明，振动干预能够提高老年人群和特定疾病患者的平衡与运动能力。但是存在一些挑战，如参数选择困难、生态效度不足等问题需要进一步解决。", "conclusion": "文章指出未来的研究需关注如何将基于振动的干预措施转化为可靠且易于部署的临床工具，并强调了标准化流程的重要性。"}}
{"id": "2512.08006", "pdf": "https://arxiv.org/pdf/2512.08006", "abs": "https://arxiv.org/abs/2512.08006", "authors": ["Mahta Fetrat", "Donya Navabi", "Zahra Dehghanian", "Morteza Abolghasemi", "Hamid R. Rabiee"], "title": "Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Lightweight, real-time text-to-speech systems are crucial for accessibility. However, the most efficient TTS models often rely on lightweight phonemizers that struggle with context-dependent challenges. In contrast, more advanced phonemizers with a deeper linguistic understanding typically incur high computational costs, which prevents real-time performance. This paper examines the trade-off between phonemization quality and inference speed in G2P-aided TTS systems, introducing a practical framework to bridge this gap. We propose lightweight strategies for context-aware phonemization and a service-oriented TTS architecture that executes these modules as independent services. This design decouples heavy context-aware components from the core TTS engine, effectively breaking the latency barrier and enabling real-time use of high-quality phonemization models. Experimental results confirm that the proposed system improves pronunciation soundness and linguistic accuracy while maintaining real-time responsiveness, making it well-suited for offline and end-device TTS applications.", "AI": {"tldr": "本文提出了一种轻量级的、上下文感知的音素化策略和服务导向型TTS架构，以解决高效和高质量之间的权衡问题。", "motivation": "高效的TTS模型依赖于轻量级的音素化器，但这些音素化器难以处理上下文相关的挑战。高级的音素化器虽然具有更深入的语言理解能力，但由于计算成本高而无法实现实时性能。本文旨在研究语音生成辅助的TTS系统中的音素化质量和推理速度之间的权衡。", "method": "提出了轻量级的上下文感知音素化策略和服务导向型TTS架构，并将这些模块作为独立服务执行。该设计将复杂的上下文相关组件与核心TTS引擎解耦，有效打破延迟壁垒，实现实时高质量音素化的应用。", "result": "实验结果表明，所提出的系统在保持实时响应的同时提高了发音准确性和语言准确性。", "conclusion": "本文提出的服务导向型架构解决了高效和高质量之间的矛盾，使其适用于离线和端设备TTS应用程序。"}}
{"id": "2512.07998", "pdf": "https://arxiv.org/pdf/2512.07998", "abs": "https://arxiv.org/abs/2512.07998", "authors": ["Mostafa Kamali Tabrizi", "Mingshi Chi", "Bir Bikram Dey", "Yu Qing Yuan", "Markus D. Solbach", "Yiqian Liu", "Michael Jenkin", "John K. Tsotsos"], "title": "DIJIT: A Robotic Head for an Active Observer", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present DIJIT, a novel binocular robotic head expressly designed for mobile agents that behave as active observers. DIJIT's unique breadth of functionality enables active vision research and the study of human-like eye and head-neck motions, their interrelationships, and how each contributes to visual ability. DIJIT is also being used to explore the differences between how human vision employs eye/head movements to solve visual tasks and current computer vision methods. DIJIT's design features nine mechanical degrees of freedom, while the cameras and lenses provide an additional four optical degrees of freedom. The ranges and speeds of the mechanical design are comparable to human performance. Our design includes the ranges of motion required for convergent stereo, namely, vergence, version, and cyclotorsion. The exploration of the utility of these to both human and machine vision is ongoing. Here, we present the design of DIJIT and evaluate aspects of its performance. We present a new method for saccadic camera movements. In this method, a direct relationship between camera orientation and motor values is developed. The resulting saccadic camera movements are close to human movements in terms of their accuracy.", "AI": {"tldr": "DIJIT是一款新型的双目机器人头部，用于研究主动观察者的视觉能力。", "motivation": "为了研究人类眼睛和头部运动及其与视觉能力的关系，并探索计算机视觉方法与人类视觉中眼动的区别", "method": "设计了一款具有九个机械自由度和四个光学自由度的机器人头部DIJIT，开发了新的用于跳跃性相机移动的方法。", "result": "所提出的saccadic相机运动方法使得相机移动在准确性上接近于人体动作。", "conclusion": "DIJIT的设计提供了研究眼动与视觉能力之间关系的能力，验证了一种新型的相机运动方法的有效性。"}}
{"id": "2512.07997", "pdf": "https://arxiv.org/pdf/2512.07997", "abs": "https://arxiv.org/abs/2512.07997", "authors": ["Soroush Baghernezhad", "Elaheh Mohammadreza", "Vinicius Prado da Fonseca", "Ting Zou", "Xianta Jiang"], "title": "A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.", "AI": {"tldr": "本文研究了使用IMU信号进行手势识别的潜力，并将其与EMG信号进行了比较。", "motivation": "生物信号在手势识别中的应用广泛，其中sEMG是被研究最多的模式。然而，较少探索的IMU可以提供关于细微肌肉运动的重要信息，这使得它们对于手势识别非常有价值。", "method": "本文通过不同肌群的IMU信号来捕捉用户意图，并将其与单独使用肌肉群的信息质量进行了比较。结果表明，由IMUs捕获的肌腱微动是静态手势识别的主要贡献者。", "result": "研究发现，IMU信号包含了足够的信息可以作为静态手势识别的唯一输入传感器。", "conclusion": "利用肌微动信息可以提高假肢手臂的可用性，并为机器人、远程操作等领域的手势识别提供了新的可能性。"}}
{"id": "2512.07993", "pdf": "https://arxiv.org/pdf/2512.07993", "abs": "https://arxiv.org/abs/2512.07993", "authors": ["Jiayi Tian", "Seyedarmin Azizi", "Yequan Zhao", "Erfan Baghaei Potraghloo", "Sean McPherson", "Sharath Nittur Sridhar", "Zhengyang Wang", "Zheng Zhang", "Massoud Pedram", "Souvik Kundu"], "title": "SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models", "categories": ["cs.AI"], "comment": null, "summary": "Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \\textbf{SkipKV}, a \\textbf{\\textit{training-free}} KV compression method for selective \\textit{eviction} and \\textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \\textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\\mathbf{26.7}\\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\\mathbf{1.6}\\times$ fewer generation length while improving throughput up to $\\mathbf{1.7}\\times$.", "AI": {"tldr": "SkipKV是一种用于在推理过程中减少关键值缓存大小的训练自由方法，通过句子级别的序列删除来优化长推理模型中的链式思考过程。", "motivation": "大型推理模型由于线性增长的关键值缓存开销，在推理时面临内存和吞吐量瓶颈。现有的关键值缓存替换策略在处理多批次设置中难以保持准确性，并且会产生冗余的序列生成，导致效率低下。", "method": "SkipKV提出了一种基于句子评分指标来识别并删除高度相似的句子的方法，同时维持语义连贯性。此外，通过动态调整引导向量以更新隐藏激活状态，抑制冗长的输出生成。", "result": "实验表明，与现有方法相比，SkipKV能够在相同压缩预算下提高26.7%的准确性，并且在不牺牲效果的前提下减少1.6倍的序列长度，同时提高了推理吞吐量1.7倍。", "conclusion": "SkipKV通过训练自由的方法有效减少了大型推理模型中的关键值缓存开销，在保持语义准确性和效率的同时显著提升了推理性能。"}}
{"id": "2512.07990", "pdf": "https://arxiv.org/pdf/2512.07990", "abs": "https://arxiv.org/abs/2512.07990", "authors": ["Thanh Nguyen", "Chaima Boufaied", "Ronnie de Souza Santos"], "title": "A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.", "AI": {"tldr": "本文通过灰文献研究，探讨了AI系统中的公平性要求，并分析了其在软件开发生命周期各阶段的管理情况及违反后的后果。", "motivation": "当前，尽管人工智能特别是机器学习的应用日益广泛，但关于AI系统的公平性的关注相对较少，尤其是在软件工程中。因此，本文旨在填补这一研究空白，探讨如何定义和实施AI中的公平性要求。", "method": "通过灰文献调查，分析了不同应用领域的公平性需求定义、软件开发生命周期各阶段的管理实践以及AI模型违反公平性后的具体后果。", "result": "研究表明，在AI系统中，公平性通常被理解为非歧视和平等对待不同的人口和社会属性；在SDLC的不同阶段，有各种不同的管理和监控实践；而违反公平性的后果包括对个人、社会层面的影响，如强化刻板印象和信任缺失等。", "conclusion": "这些发现强调了建立一致的框架与实践以将公平性纳入AI软件的重要性，并要求在重视有效性的同时也注重公平性。"}}
{"id": "2512.07988", "pdf": "https://arxiv.org/pdf/2512.07988", "abs": "https://arxiv.org/abs/2512.07988", "authors": ["Sudhanva Manjunath Athreya", "Paul Rosen"], "title": "HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability", "categories": ["cs.LG", "cs.GR", "cs.HC"], "comment": null, "summary": "Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.", "AI": {"tldr": "介绍HOLE方法，通过持久同调分析神经网络的激活来解释深度学习模型", "motivation": "解决深度学习模型黑箱问题，提高模型可解释性", "method": "使用持久同调从神经激活中提取拓扑特征，并利用可视化工具如桑基图、热力图和聚类树展示这些特征", "result": "结果显示HOLE能够揭示类别分离、特征解耦以及模型鲁棒性的模式", "conclusion": "通过提供互补视角，有助于理解和改进深度学习系统"}}
{"id": "2512.07984", "pdf": "https://arxiv.org/pdf/2512.07984", "abs": "https://arxiv.org/abs/2512.07984", "authors": ["Ryan Banks", "Camila Lindoni Azevedo", "Hongying Tang", "Yunpeng Li"], "title": "Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 7 figures, 3 tables", "summary": "Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.", "AI": {"tldr": "本文提出了一种嵌入显式解剖层次结构的语义分割框架，用于牙层检测。", "motivation": "现有的层级感知分割方法主要通过损失函数引入弱监督，难以实现精确的解剖结构理解。作者希望通过引入明确的解剖层次结构来改进这一问题。", "method": "该框架结合了递归预测方案、限制性输出头和自顶向下特征调节，使子类特征空间根据父类概率进行调整，并使用概率组合规则确保层级一致性。损失函数包括每个级别的Dice加权交叉熵损失和一致性项。", "result": "实验验证表明，在牙层和骨质的密集分割任务中，层次化方法显著提高了IoU、Dice系数和召回率，但在低数据集成像环境中仍存在精度较低的问题。", "conclusion": "显式层次结构改进了模型性能并增强了临床合理性，尽管在一些情况下可能导致更多的假阳性。"}}
{"id": "2512.07983", "pdf": "https://arxiv.org/pdf/2512.07983", "abs": "https://arxiv.org/abs/2512.07983", "authors": ["Nan Jia", "Anita Raja", "Raffi Khatchadourian"], "title": "An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to Hawaii International Conference on System Sciences (HICSS) 2026", "summary": "As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.", "AI": {"tldr": "本文提出了一种基于Hugging Face数据评估语义保持的实证框架。", "motivation": "随着机器学习在高自主系统中的重要性增加，确保学习驱动软件系统的可信度变得至关重要。然而，传统软件重构方法难以处理机器学习中固有的不确定性和运行时定义的语义。", "method": "通过从Hugging Face收集模型演化数据（包括提交历史、Model Cards和性能指标），本文开发了一种实证框架来评估智能组件优化是否改变了系统的整体功能性行为。进行了三个领域的案例研究，追踪了版本间的性能变化，并分析了提交消息以揭示重构模式。", "result": "通过该方法，检测到了语义漂移并通过评价指标跨越提交的性能变化来确认，同时提供了一种界定社区认可的语义保持边界的基石。", "conclusion": "本文贡献包括一个大规模机器学习模型演化数据集、评估语义保持的实际管道以及实践中的语义漂移案例研究。这些成果为更可维护和可信的ML系统奠定了基础。"}}
{"id": "2512.07981", "pdf": "https://arxiv.org/pdf/2512.07981", "abs": "https://arxiv.org/abs/2512.07981", "authors": ["Federico Di Valerio", "Michela Proietti", "Alessio Ragno", "Roberto Capobianco"], "title": "CIP-Net: Continual Interpretable Prototype-based Network", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.", "AI": {"tldr": "CIP-Net是一个设计用于连续学习的自解释原型网络，避免存储过去样本并保持简单架构。", "motivation": "持续学习中模型需要在不忘记旧任务的情况下学习新任务。主要挑战是灾难性遗忘，即新信息的学习导致模型性能下降。现有的可解释AI方法大多数使用事后解释或为每个新任务提供额外内存，限制了其扩展性。", "method": "CIP-Net是一个无示例自解释原型网络，用于持续学习中避免存储过去的样本并保持简单的架构。", "result": "实验表明，在任务和类增量设置下，CIP-Net达到最先进的性能，同时具有显著较低的内存相关开销。", "conclusion": "CIP-Net是解决持续学习问题的一种实用且可解释的方法。"}}
{"id": "2512.07976", "pdf": "https://arxiv.org/pdf/2512.07976", "abs": "https://arxiv.org/abs/2512.07976", "authors": ["Lazar Milikic", "Manthan Patel", "Jonas Frey"], "title": "VLD: Visual Language Goal Distance for Reinforcement Learning Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Training end-to-end policies from image data to directly predict navigation actions for robotic systems has proven inherently difficult. Existing approaches often suffer from either the sim-to-real gap during policy transfer or a limited amount of training data with action labels. To address this problem, we introduce Vision-Language Distance (VLD) learning, a scalable framework for goal-conditioned navigation that decouples perception learning from policy learning. Instead of relying on raw sensory inputs during policy training, we first train a self-supervised distance-to-goal predictor on internet-scale video data. This predictor generalizes across both image- and text-based goals, providing a distance signal that can be minimized by a reinforcement learning (RL) policy. The RL policy can be trained entirely in simulation using privileged geometric distance signals, with injected noise to mimic the uncertainty of the trained distance predictor. At deployment, the policy consumes VLD predictions, inheriting semantic goal information-\"where to go\"-from large-scale visual training while retaining the robust low-level navigation behaviors learned in simulation. We propose using ordinal consistency to assess distance functions directly and demonstrate that VLD outperforms prior temporal distance approaches, such as ViNT and VIP. Experiments show that our decoupled design achieves competitive navigation performance in simulation while supporting flexible goal modalities, providing an alternative and, most importantly, scalable path toward reliable, multimodal navigation policies.", "AI": {"tldr": "提出了一种用于目标导向导航的可扩展框架VLD，通过解耦感知学习和策略学习来提高模拟到现实场景中端到端导航政策的有效性。", "motivation": "解决从图像数据直接预测机器人导航动作时面临的挑战，包括仿真与实际环境之间的差距及标签训练数据不足的问题。", "method": "首先在大规模互联网视频数据上训练一个自监督的距离至目标预测器，然后使用该预测器生成距离信号供强化学习策略最小化。最终的RL政策在模拟中通过加入噪声来模仿不确定性进行培训，并利用VLD预测在部署时指导导航行为。", "result": "实验表明VLD优于先前的时间距离方法如ViNT和VIP，在仿真环境中实现了竞争性的导航性能，支持灵活的目标模式。", "conclusion": "提出了一个新的可扩展路径，以实现可靠、多模态的导航政策，通过解耦感知学习与策略学习来提高端到端导航政策的有效性。"}}
{"id": "2512.07969", "pdf": "https://arxiv.org/pdf/2512.07969", "abs": "https://arxiv.org/abs/2512.07969", "authors": ["Alan Papalia", "Nikolas Sanderson", "Haoyu Han", "Heng Yang", "Hanumant Singh", "Michael Everett"], "title": "Sparse Variable Projection in Robotic Perception: Exploiting Separable Structure for Efficient Nonlinear Optimization", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, submitted for review", "summary": "Robotic perception often requires solving large nonlinear least-squares (NLS) problems. While sparsity has been well-exploited to scale solvers, a complementary and underexploited structure is \\emph{separability} -- where some variables (e.g., visual landmarks) appear linearly in the residuals and, for any estimate of the remaining variables (e.g., poses), have a closed-form solution. Variable projection (VarPro) methods are a family of techniques that exploit this structure by analytically eliminating the linear variables and presenting a reduced problem in the remaining variables that has favorable properties. However, VarPro has seen limited use in robotic perception; a major challenge arises from gauge symmetries (e.g., cost invariance to global shifts and rotations), which are common in perception and induce specific computational challenges in standard VarPro approaches. We present a VarPro scheme designed for problems with gauge symmetries that jointly exploits separability and sparsity. Our method can be applied as a one-time preprocessing step to construct a \\emph{matrix-free Schur complement operator}. This operator allows efficient evaluation of costs, gradients, and Hessian-vector products of the reduced problem and readily integrates with standard iterative NLS solvers. We provide precise conditions under which our method applies, and describe extensions when these conditions are only partially met. Across synthetic and real benchmarks in SLAM, SNL, and SfM, our approach achieves up to \\textbf{2$\\times$--35$\\times$ faster runtimes} than state-of-the-art methods while maintaining accuracy. We release an open-source C++ implementation and all datasets from our experiments.", "AI": {"tldr": "该论文提出了一种适用于存在标度对称性问题的变量投影方案，利用分离性和稀疏性结构来提高非线性优化效率。", "motivation": "在机器人感知任务中，大型非线性最小二乘问题需要高效的求解方法。尽管稀疏性的优势已经被充分利用，但可分性这一互补且被忽视的特性未被充分挖掘。变量投影技术能够通过解析地消除线性变量来简化问题，并提高求解效率。", "method": "该论文提出了一种新的变量投影方案，专门针对存在标度对称性的问题设计，它同时利用了分离性和稀疏性结构。此方法可以作为预处理步骤使用，构建一个矩阵自由舒尔余项操作符，从而能够高效地评估成本、梯度和赫森矩阵矢量积，并能与标准的迭代非线性最小二乘求解器无缝集成。", "result": "在SLAM、SNL和SfM等领域的合成数据集和真实数据集中，该方法实现了比现有最佳方法快2倍至35倍的速度提升，同时保持了精度。", "conclusion": "本文提出了一种适用于存在标度对称性问题的变量投影方案，通过利用分离性和稀疏性结构来提高非线性优化效率。这种方法在多个领域中表现出了显著的性能优势，并且能够与现有的求解器兼容集成。"}}
{"id": "2512.07961", "pdf": "https://arxiv.org/pdf/2512.07961", "abs": "https://arxiv.org/abs/2512.07961", "authors": ["Guilherme Seidyo Imai Aldeia", "Joseph D. Romano", "Fabricio Olivetti de Franca", "Daniel S. Herman", "William G. La Cava"], "title": "Towards symbolic regression for interpretable clinical decision scores", "categories": ["cs.LG", "cs.NE"], "comment": "15 pages, 5 figures. Accepted for publication in Philosophical Transactions A. Autor Accepted Manuscript version", "summary": "Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.", "AI": {"tldr": "提出了一种新的符号回归算法Brush，用于开发可解释的临床决策评分系统。", "motivation": "传统符号回归限制于连续函数形式和参数，难以模拟临床决策。为此引入了结合分段逻辑与非线性常数优化的Brush算法以改善此问题。", "method": "通过将决策树样式的分割算法与非线性常数优化相结合开发出新的符号回归算法Brush。", "result": "在SRBench上表现出色，并成功重现两个广泛使用的临床评分系统，具有高准确性和可解释性模型。", "conclusion": "Brush实现了与决策树和随机森林等方法相当或更好的预测性能，同时生成更简单的模型。"}}
{"id": "2512.07951", "pdf": "https://arxiv.org/pdf/2512.07951", "abs": "https://arxiv.org/abs/2512.07951", "authors": ["Zekai Luo", "Zongze Du", "Zhouhang Zhu", "Hao Zhong", "Muzhi Zhu", "Wen Wang", "Yuling Xi", "Chenchen Jing", "Hao Chen", "Chunhua Shen"], "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "categories": ["cs.CV"], "comment": "Project webpage: https://aim-uofa.github.io/LivingSwap", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "AI": {"tldr": "本文提出了一种名为LivingSwap的视频面部替换模型，该模型利用关键帧作为条件信号，并结合视频参考引导技术来实现高质量、高保真的面部交换。", "motivation": "在电影和娱乐制作中，实现长时间复杂视频序列中的高度真实感和时间一致性是主要挑战。本文探讨了如何借鉴图像编辑的视觉属性增强视频面部替换的真实性与时间连贯性。", "method": "LivingSwap通过关键帧作为条件信号来注入目标身份，并结合视频参考引导技术进行时间拼接，以确保整个视频序列中的稳定身份保持和高保真重建。为解决训练数据稀少的问题，构建了Face2Face数据集。", "result": "实验表明，该方法达到了最先进的结果，能够无缝地将目标身份与源视频的表情、光照及动作融合在一起，并大幅减少了生产工作流程中的手工劳动。", "conclusion": "LivingSwap是第一个视频参考引导的面部替换模型，在保持高保真度和时间一致性方面取得了显著效果。"}}
{"id": "2512.07926", "pdf": "https://arxiv.org/pdf/2512.07926", "abs": "https://arxiv.org/abs/2512.07926", "authors": ["Arvind Agarwal", "Lisa Amini", "Sameep Mehta", "Horst Samulowitz", "Kavitha Srinivas"], "title": "Can AI autonomously build, operate, and use the entire data stack?", "categories": ["cs.AI", "cs.DB", "cs.LG"], "comment": null, "summary": "Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.", "AI": {"tldr": "探讨人工智能是否能自主构建、运营和使用整个数据栈，提出从独立的数据组件操作转向全生命周期的自治管理。", "motivation": "鉴于企业数据管理任务庞大复杂，AI在不同阶段的应用还未能实现完全自动化。文章旨在推动一个范式转变：从独立数据组件的操作到整体数据生命周期的自主处理。", "method": "讨论了促使这一范式变化的压力和机遇，探讨智能代理如何简化整个数据生命周期，并指出了需要进一步研究的问题领域。", "result": "提出了通过智能代理实现全数据栈自治管理的概念框架，但未提供具体技术实施细节或实验结果。", "conclusion": "认为自主的数据系统是未来方向，旨在引发辩论、促进研究并鼓励合作方法。"}}
{"id": "2512.07925", "pdf": "https://arxiv.org/pdf/2512.07925", "abs": "https://arxiv.org/abs/2512.07925", "authors": ["Kuldip Singh Atwal", "Dieter Pfoser", "Daniel Rothbart"], "title": "Near-real time fires detection using satellite imagery in Sudan conflict", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.", "AI": {"tldr": "使用卫星遥感图像和深度学习模型对苏丹冲突中的火灾进行近实时监测。", "motivation": "为了快速监控和分析苏丹冲突，利用深学习技术以及可获得的卫星遥感图像实现实时监测。", "method": "采用Planet Labs提供的四波段影像结合深度学习模型来检测和分析火灾。", "result": "通过五个案例研究证明了该方法比基线方法更能准确地捕捉到活跃火灾及烧毁区域，进一步使用八波段影像或其时间序列数据仅能获得边际增益。", "conclusion": "在苏丹冲突中展示了利用四波段卫星图像和深度学习模型进行近实时火灾监测的有效性。"}}
{"id": "2512.07921", "pdf": "https://arxiv.org/pdf/2512.07921", "abs": "https://arxiv.org/abs/2512.07921", "authors": ["Zongwei Li", "Zhonghang Li", "Zirui Guo", "Xubin Ren", "Chao Huang"], "title": "DeepCode: Open Agentic Coding", "categories": ["cs.SE", "cs.AI"], "comment": "for source code, please see https://github.com/HKUDS/DeepCode", "summary": "Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.", "AI": {"tldr": "DeepCode是一款全自动框架，旨在通过管理信息流解决文档到代码库合成中的挑战。", "motivation": "现有方法在实现高保真度的文档到代码库合成中面临重大挑战，尤其是科学论文转化为代码时。这些挑战主要源于大型语言模型的信息过载和上下文瓶颈之间的根本冲突。", "method": "DeepCode将仓库合成视为一个通道优化问题，并通过四种信息操作来最大化任务相关信号：来源压缩、结构索引、条件知识注入及闭环错误校正。", "result": "在PaperBench基准上的广泛评估表明，DeepCode实现了最先进的性能，超越了领先的商业代理Cursor和Claude Code，并且在关键复制指标上超过了顶尖院校的博士级专家。", "conclusion": "通过将论文规范系统地转化为与人类专家质量相当的生产级实现，这项工作为自主科学研究奠定了新基础。"}}
{"id": "2512.07917", "pdf": "https://arxiv.org/pdf/2512.07917", "abs": "https://arxiv.org/abs/2512.07917", "authors": ["Zhehao Dong", "Shanghai Du", "Zhen Lu", "Yue Yang"], "title": "CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation", "categories": ["cs.SE", "cs.AI", "physics.flu-dyn"], "comment": null, "summary": "Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.", "AI": {"tldr": "CFD-copilot框架利用领域适配的大规模语言模型和模型上下文协议，实现从设置到后处理的自然语言驱动的计算流体动力学仿真自动化。", "motivation": "配置计算流体动力学（CFD）模拟需要深厚的物理建模和数值方法专业知识，这构成了非专业人士的障碍。尽管利用大规模语言模型（LLMs）自动化科学任务引起了关注，但由于其严格的领域特定要求，在整个端到端CFD工作流程中应用这些技术仍是一项挑战。", "method": "该框架采用经过微调的大规模语言模型直接将用户描述翻译为可执行的CFD设置，并通过多代理系统将其与仿真执行、自动错误校正和结果分析集成在一起。后处理阶段使用模型上下文协议（MCP），这是一个开放标准，它解耦LLM推理与外部工具执行的关系。", "result": "框架在包括NACA~0012翼型和三元件30P-30N翼型的基准测试上进行了评估，结果显示领域特定适应性和MCP的结合提高了LLM驱动工程工作流程的可靠性和效率。", "conclusion": "CFD-copilot通过利用大规模语言模型和开放标准（如模型上下文协议），提供了一种增强自然语言驱动的仿真自动化的方法。"}}
{"id": "2512.07907", "pdf": "https://arxiv.org/pdf/2512.07907", "abs": "https://arxiv.org/abs/2512.07907", "authors": ["Richard Littauer", "Kris Bubendorfer"], "title": "Harmonizing Community Science Datasets to Model Highly Pathogenic Avian Influenza (HPAI) in Birds in the Subantarctic", "categories": ["q-bio.PE", "cs.AI"], "comment": "Proceedings of Pacific Rim International Conference on Artificial Intelligence 2025 (PRICAI 2025): Artificial Intelligence for Earth and Environmental Science 2025 (AIEES 2025) Workshop, 17-21 Nov 2025, Wellington, New Zealand. Changes from presentation paper: small spelling edits, change of preferred email, inclusion of Codeberg source code", "summary": "Community science observational datasets are useful in epidemiology and ecology for modeling species distributions, but the heterogeneous nature of the data presents significant challenges for standardization, data quality assurance and control, and workflow management. In this paper, we present a data workflow for cleaning and harmonizing multiple community science datasets, which we implement in a case study using eBird, iNaturalist, GBIF, and other datasets to model the impact of highly pathogenic avian influenza in populations of birds in the subantarctic. We predict population sizes for several species where the demographics are not known, and we present novel estimates for potential mortality rates from HPAI for those species, based on a novel aggregated dataset of mortality rates in the subantarctic.", "AI": {"tldr": "本文提出了一种数据工作流程，用于清理和统一多个社区科学数据库，以建模南半球亚南极地区鸟类中高致病性禽流感的影响。", "motivation": "通过利用异质性的社区科学观测数据库，在生态学和流行病学中模型物种分布面临挑战，包括标准化、数据质量和工作流程管理。本文旨在解决这些难题。", "method": "采用eBird, iNaturalist, GBIF等数据集进行案例研究，开发了一种清理和统一多个社区科学数据的工作流，用于建模亚南极地区鸟类的高致病性禽流感影响。", "result": "预测了几个物种的数量，并基于汇总的数据集提出了这些物种因H5N1造成的潜在死亡率的新估计。", "conclusion": "通过综合多种社区科学数据库，能够更准确地评估南半球亚南极地区鸟类种群的高致病性禽流感风险。"}}
{"id": "2512.07901", "pdf": "https://arxiv.org/pdf/2512.07901", "abs": "https://arxiv.org/abs/2512.07901", "authors": ["Kevin Vallier"], "title": "The Theory of Strategic Evolution: Games with Endogenous Players and Strategic Replicators", "categories": ["cs.GT", "cs.AI", "econ.TH"], "comment": "Draft manuscript, 30k words. Companion to Agentic Capital. Submitted to establish priority", "summary": "This paper develops the Theory of Strategic Evolution, a general model for systems in which the population of players, strategies, and institutional rules evolve together. The theory extends replicator dynamics to settings with endogenous players, multi level selection, innovation, constitutional change, and meta governance. The central mathematical object is a Poiesis stack: a hierarchy of strategic layers linked by cross level gain matrices. Under small gain conditions, the system admits a global Lyapunov function and satisfies selection, tracking, and stochastic stability results at every finite depth. We prove that the class is closed under block extension, innovation events, heterogeneous utilities, continuous strategy spaces, and constitutional evolution. The closure theorem shows that no new dynamics arise at higher levels and that unrestricted self modification cannot preserve Lyapunov structure. The theory unifies results from evolutionary game theory, institutional design, innovation dynamics, and constitutional political economy, providing a general mathematical model of long run strategic adaptation.", "AI": {"tldr": "本文提出了战略演化的理论，该模型适用于参与者、策略和制度规则共同进化的系统。", "motivation": "现有理论未能有效处理具有内生参与者的多层次选择环境下的动态演变问题。因此，需要一种新的框架来统一演化博弈论、制度设计、创新动力学以及宪法政治经济学的结果。", "method": "通过引入Poiesis栈这一数学对象，该模型将复制者动态扩展至包含内生参与者和多级选择的环境中，并探讨了小增益条件下的全局Lyapunov函数的存在性及选举行为、追踪稳定性和随机稳定性等特性。", "result": "证明了在封闭块扩展、创新事件、异质效用、连续策略空间以及宪法演进的情况下，该类系统的动力学保持不变，并且不受限的自我修改无法保留Lyapunov结构。", "conclusion": "本文提出的理论为长期的战略适应提供了一个统一的数学模型，适用于多种动态演化环境。"}}
{"id": "2512.07898", "pdf": "https://arxiv.org/pdf/2512.07898", "abs": "https://arxiv.org/abs/2512.07898", "authors": ["Hongwei Zhang", "Ji Lu", "Yongsheng Du", "Yanqin Gao", "Lingjun Huang", "Baoli Wang", "Fang Tan", "Peng Zou"], "title": "MARINE: Theoretical Optimization and Design for Multi-Agent Recursive IN-context Enhancement", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents demonstrate advanced reasoning capabilities, yet practical constraints frequently limit outputs to single responses, leaving significant performance potential unrealized. This paper introduces MARINE (Multi-Agent Recursive IN-context Enhancement), a theoretically grounded framework that reconceptualizes test-time reasoning as iterative refinement of a persistent reference trajectory, fundamentally departing from conventional one-shot or multi-sample paradigms. The MARINE refinement operator systematically converts a base model's pass@N capabilities into near-optimal pass@1 performance. Rigorous theoretical analysis establishes that minimal feasible batches maximize expected performance gains under fixed invocation budgets, while logarithmically growing batch schedules ensure continuous improvement without computational constraints. Comprehensive evaluation on the BrowserComp-ZH benchmark demonstrates state-of-the-art results, with a 685B-parameter implementation achieving 46.0% pass@1 accuracy. Meanwhile, MARINE establishes a new paradigm for parameter-efficient reasoning: an 80B-parameter model augmented with MARINE matches the performance of standalone 1000B-parameter agents, reducing parameter requirements by over an order of magnitude. Notably, within a fixed computational budget, the proposed MARINE delivers higher-quality samples to alignment and optimization processes than traditional sampling-and-ranking strategies. Consequently, it has great potential to boost post-training efficiency.", "AI": {"tldr": "提出了一种新的多代理递归上下文增强框架MARINE，用于提升大语言模型的推理性能。", "motivation": "当前大型语言模型在实际应用中受限于单一输出，导致其潜在能力未能充分发挥。因此，需要一种新方法来优化这种限制下的推理效率和效果。", "method": "引入了基于多代理递归上下文增强的MARINE框架，该框架将测试时间推理重构为对持久参考轨迹的迭代改进，并通过理论分析证明在固定调用预算下最小可行批次能最大化性能收益，同时以对数增长批量调度确保持续改善。", "result": "实验表明，在BrowserComp-ZH基准上，使用MARINE框架后模型表现达到当前最优水平，且参数效率显著提高。此外，在相同计算预算内，MARINE能够提供比传统采样和排名策略更好的样本质量。", "conclusion": "MARINE提出了一种新的推理范式，并展示了其在减少所需参数量的同时提升性能的能力，为后期训练效率的优化提供了有力支持。"}}
{"id": "2512.07888", "pdf": "https://arxiv.org/pdf/2512.07888", "abs": "https://arxiv.org/abs/2512.07888", "authors": ["Fahad Mostafa", "Hafiz Khan"], "title": "Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "stat.CO"], "comment": "23 pages, 4 figures", "summary": "Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.", "AI": {"tldr": "提出了一种用于不平衡函数数据分类的新型随机森林框架，通过自适应代价敏感分裂改进了传统方法。", "motivation": "传统的随机森林算法在处理具有内在结构的功能观察时效果不佳，特别是在少数类检测方面存在困难。为了应对严重的数据不平衡问题，该研究旨在开发一种更有效的功能分类器。", "method": "该论文提出了FRF-ACS框架，利用基函数和主成分分析来表示曲线，并采用自适应代价敏感分裂策略结合混合采样方法以解决数据不平衡的问题。", "result": "实验结果表明，与现有功能性分类器和其他平衡处理技术相比，FRF-ACS在少数类召回率以及整体预测性能方面都有显著改进。", "conclusion": "该研究提供了一种可扩展且解释性强的解决方案，适用于需要准确检测少数类别的高维功能数据分析领域。"}}
{"id": "2512.07885", "pdf": "https://arxiv.org/pdf/2512.07885", "abs": "https://arxiv.org/abs/2512.07885", "authors": ["Davide Donno", "Donatello Elia", "Gabriele Accarino", "Marco De Carlo", "Enrico Scoccimarro", "Silvio Gualdi"], "title": "ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 10 figures", "summary": "Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\\%$ ENP, $79.48\\%$ WNP), False Alarm Rate ($23.26\\%$ ENP, $16.14\\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.", "AI": {"tldr": "ByteStorm是一种基于深度学习的数据驱动框架，用于热带气旋的检测和追踪。", "motivation": "传统的热带气旋追踪方法依赖主观阈值，可能在不同地理区域引入偏差。ByteStorm旨在提供一种无需调整阈值即可准确重建热带气旋轨迹的方法。", "method": "ByteStorm利用深度学习网络根据相对涡度（850mb）和平均海平面压力来检测热带气旋中心，并通过BYTE算法将这些点链接成完整的追踪路径。", "result": "在东、西北太平洋盆地，ByteStorm的性能优于现有确定性跟踪器，其概率检测率分别为ENP 85.05%和WNP 79.48%，误报率为ENP 23.26%和WNP 16.14%，年际变化相关系数为ENP 0.75和WNP 0.69。", "conclusion": "ByteStorm展示了结合深度学习和计算机视觉进行快速准确的热带气旋追踪的巨大潜力，提供了一种比传统方法更稳健的选择。"}}
{"id": "2512.07884", "pdf": "https://arxiv.org/pdf/2512.07884", "abs": "https://arxiv.org/abs/2512.07884", "authors": ["Hongjun Wang", "Yitong Jiang", "Collin McCarthy", "David Wehr", "Hanrong Ye", "Xinhao Li", "Ka Chun Cheung", "Wonmin Byeon", "Jinwei Gu", "Ke Chen", "Kai Han", "Hongxu Yin", "Pavlo Molchanov", "Jan Kautz", "Sifei Liu"], "title": "GSPN-2: Efficient Parallel Sequence Modeling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "NeurIPS 2025", "summary": "Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/", "AI": {"tldr": "GSPN-2提出了一个高效的并行序列建模方法，通过改进的算法和系统设计来优化原有的GSPN。", "motivation": "现有的GSPN实现存在重复启动GPU内核、过多的数据传输以及冗余计算的问题。因此，需要一种新的方法以提高效率。", "method": "提出了一种全新的2D内核取代之前的微小任务，并将每个通道片分配给一个特定的warp，同时在共享内存中存储前一列的激活值。此外，模型方面采用了一种紧凑的通道传播策略，替代原有的每通道矩阵，减少参数数量。", "result": "实验表明，在图像分类和文本到图像合成任务上，GSPN-2与变换器级准确度相当，但计算成本更低，建立了在视觉应用中建模全局空间上下文的新效率标准。", "conclusion": "通过结构化矩阵转换和GPU优化实现的独特组合，GSPN-2为解决大规模视觉任务中的序列建模问题提供了一种新的有效方法。"}}
{"id": "2512.07882", "pdf": "https://arxiv.org/pdf/2512.07882", "abs": "https://arxiv.org/abs/2512.07882", "authors": ["Yueran Zhao", "Chang-Sheng Mei", "Nathan J. McDannold", "Shenyan Zong", "Guofeng Shen"], "title": "Referenceless Proton Resonance Frequency Thermometry Using Deep Learning with Self-Attention", "categories": ["physics.med-ph", "cs.AI"], "comment": null, "summary": "Background: Accurate proton resonance frequency (PRF) MR thermometry is essential for monitoring temperature rise during thermal ablation with high intensity focused ultrasound (FUS). Conventional referenceless methods such as complex field estimation (CFE) and phase finite difference (PFD) tend to exhibit errors when susceptibility-induced phase discontinuities occur at tissue interfaces.", "AI": {"tldr": "使用深度学习中的自我注意机制进行无参考相位磁共振温度测量。", "motivation": "准确的质子谐振频率MR热计量对于监测高强度聚焦超声波治疗过程中的体温升高至关重要。传统方法在组织界面处出现磁化率引起的相位不连续时，容易产生误差。", "method": "提出了一种基于深度学习中自我注意机制的方法来改进无参考相位MRI温度测量。", "result": "该方法能够更准确地估计质子谐振频率，提高热计量的准确性。", "conclusion": "通过利用深度学习中的自我注意机制，可以有效解决传统方法在组织界面处产生的相位不连续问题，从而提供更为精确的温度测量结果。"}}
