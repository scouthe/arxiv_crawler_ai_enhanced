{"id": "2512.05967", "pdf": "https://arxiv.org/pdf/2512.05967", "abs": "https://arxiv.org/abs/2512.05967", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiovì"], "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.", "AI": {"tldr": "该论文提出了一种增强的检索增强生成架构，通过整合实体链接来提升教育平台中问答系统的准确性，特别是在意大利语教育领域。", "motivation": "当前基于语义相似度的RAG系统在专业领域中存在术语歧义问题，影响检索相关性和事实准确性，特别是在教育平台中需要确保事实准确性的场景。", "method": "提出增强的RAG架构，包含基于Wikidata的实体链接模块，并实现三种重排序策略：混合评分加权模型、互惠排名融合和交叉编码器重排序器。", "result": "实验在两个基准测试上进行：自定义学术数据集和标准SQuAD-it数据集。在特定领域环境中，基于互惠排名融合的混合方案显著优于基线和交叉编码器方法；而在通用领域数据集上，交叉编码器取得最佳结果。", "conclusion": "研究证实了领域不匹配效应，强调了领域适应和混合排序策略对提升检索增强生成的事实精度和可靠性的重要性，展示了实体感知RAG系统在教育环境中的潜力。"}}
{"id": "2512.05965", "pdf": "https://arxiv.org/pdf/2512.05965", "abs": "https://arxiv.org/abs/2512.05965", "authors": ["Hongyu Li", "Manyuan Zhang", "Dian Zheng", "Ziyu Guo", "Yimeng Jia", "Kaituo Feng", "Hao Yu", "Yexin Liu", "Yan Feng", "Peng Pei", "Xunliang Cai", "Linjiang Huang", "Hongsheng Li", "Si Liu"], "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor", "categories": ["cs.CV"], "comment": "Project page: https://appletea233.github.io/think-while-edit", "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.", "AI": {"tldr": "提出EditThinker框架，通过迭代推理提升图像编辑模型的指令遵循能力", "motivation": "现有基于指令的图像编辑方法虽然美学质量高，但单次成功率有限，缺乏深思熟虑的推理过程", "method": "提出Think-while-Edit循环框架，训练单一MLLM作为推理引擎，联合生成批判分数、推理过程和优化指令，并使用强化学习对齐推理与编辑", "result": "在四个基准测试上显著提升了任何图像编辑模型的指令遵循能力，效果提升幅度大", "conclusion": "EditThinker框架通过模拟人类认知循环的迭代推理，有效解决了图像编辑中的指令遵循挑战"}}
{"id": "2512.05964", "pdf": "https://arxiv.org/pdf/2512.05964", "abs": "https://arxiv.org/abs/2512.05964", "authors": ["Kevin Black", "Allen Z. Ren", "Michael Equi", "Sergey Levine"], "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.", "AI": {"tldr": "提出一种训练时动作条件化的实时分块方法，替代推理时修复方法，以降低计算开销并保持性能", "motivation": "现有的实时分块方法在推理时通过修复技术条件化先前承诺的动作，这会引入计算开销并增加推理延迟", "method": "在训练时模拟推理延迟，直接对动作前缀进行条件化，无需修改模型架构或机器人运行时，仅需少量代码改动", "result": "在模拟实验中，训练时RTC在较高推理延迟下优于推理时RTC；在真实世界实验中，训练时RTC在保持任务性能和速度的同时计算成本更低", "conclusion": "训练时动作条件化是推理时修复的实用替代方案，适用于实时机器人控制，具有计算效率优势"}}
{"id": "2512.05962", "pdf": "https://arxiv.org/pdf/2512.05962", "abs": "https://arxiv.org/abs/2512.05962", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "AI": {"tldr": "该论文提出了一种通过过滤驱动推理的方法来解决LLMs在强化学习训练中多样性损失的问题，通过α-散度族近似目标分布，在Lean定理证明基准上实现了覆盖率和精度的帕累托前沿最优性能。", "motivation": "现有基于强化学习调优LLMs的方法往往导致显著的多样性损失，这是因为RL隐式优化了\"模式寻求\"或\"零强制\"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。", "method": "从显式目标分布出发，通过过滤掉错误答案同时保留正确答案的相对概率，使用α-散度族近似该目标分布，通过插值模式寻求和质量覆盖散度来直接控制精度-多样性权衡。", "result": "在Lean定理证明基准上，该方法在覆盖率-精度帕累托前沿上实现了最先进的性能，在覆盖率轴上优于所有先前方法。", "conclusion": "通过过滤驱动推理并使用α-散度族近似目标分布，可以有效解决LLMs在强化学习训练中的多样性损失问题，实现更好的精度-多样性平衡。"}}
{"id": "2512.05960", "pdf": "https://arxiv.org/pdf/2512.05960", "abs": "https://arxiv.org/abs/2512.05960", "authors": ["Munsif Ali", "Najmul Hassan", "Lucia Ventura", "Davide Di Bari", "Simonepietro Canese"], "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.", "AI": {"tldr": "提出AQUA-Net水下图像增强模型，通过自适应频率融合和光照感知机制解决水下图像颜色失真、低对比度和雾状外观问题，同时降低计算复杂度。", "motivation": "水下图像因波长依赖性光吸收和散射而存在严重颜色失真、低对比度和雾状外观问题，现有深度学习模型计算复杂度高，限制了实时水下应用的实际部署。", "method": "采用残差编码器-解码器架构，集成频率和光照两个辅助分支。频率融合编码器从傅里叶域提取频率线索丰富空间表示；光照感知解码器基于Retinex理论，通过学习的光照图进行自适应曝光校正，分离反射和光照效应。", "result": "在多个基准数据集上的实验表明，AQUA-Net在定性和定量评估中与SOTA方法性能相当，同时使用更少的参数。消融研究证实频率和光照分支提供互补贡献，改善可见性和颜色表示。", "conclusion": "AQUA-Net展示了强大的泛化能力和鲁棒性，为真实世界水下成像应用提供了有效解决方案，特别是在保持高性能的同时降低了计算复杂度。"}}
{"id": "2512.05959", "pdf": "https://arxiv.org/pdf/2512.05959", "abs": "https://arxiv.org/abs/2512.05959", "authors": ["David Anugraha", "Patrick Amadeus Irawan", "Anshul Singh", "En-Shiun Annie Lee", "Genta Indra Winata"], "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.", "AI": {"tldr": "提出M4-RAG基准测试，用于评估多语言多模态检索增强生成系统，涵盖42种语言和56种方言，包含超过80,000个文化多样性图像-问题对", "motivation": "现有的视觉语言模型受限于静态训练数据，而多语言多模态RAG研究不足，需要能够访问最新、文化相关、多语言信息的评估基准", "method": "构建大规模基准测试，包含42种语言和56种方言的80,000多个文化多样性图像-问题对，建立受控检索环境，包含数百万精心策划的多语言文档，模拟真实检索条件", "result": "系统评估显示RAG对小规模VLMs有持续益处，但对大规模模型效果不佳甚至降低性能，揭示了模型规模与当前检索效果之间的关键不匹配", "conclusion": "M4-RAG为推进下一代RAG系统提供了基础，这些系统能够跨语言、模态和文化语境进行无缝推理"}}
{"id": "2512.05958", "pdf": "https://arxiv.org/pdf/2512.05958", "abs": "https://arxiv.org/abs/2512.05958", "authors": ["Sara Patel", "Mingxun Zhou", "Giulia Fanti"], "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.", "AI": {"tldr": "提出MaxShapley算法，用于生成式搜索引擎中的公平内容归因，以线性计算复杂度实现近似Shapley值的归因质量", "motivation": "基于大语言模型的生成式搜索引擎正在取代传统搜索，改变了信息提供者的补偿方式，需要公平机制来归因和补偿内容提供者对其生成答案的贡献", "method": "MaxShapley算法，作为Shapley值的特例，利用可分解的最大和效用函数，在文档数量上实现线性计算复杂度（而非Shapley值的指数复杂度）", "result": "在三个多跳QA数据集（HotPotQA、MuSiQUE、MS MARCO）上评估，MaxShapley达到与精确Shapley计算相当的归因质量，同时显著减少资源消耗（如比先前SOTA方法减少8倍资源消耗）", "conclusion": "MaxShapley为生成式搜索生态系统提供了一种高效、公平的内容归因机制，有助于可持续的搜索生态系统发展"}}
{"id": "2512.05955", "pdf": "https://arxiv.org/pdf/2512.05955", "abs": "https://arxiv.org/abs/2512.05955", "authors": ["Haowen Liu", "Shaoxiong Yao", "Haonan Chen", "Jiawei Gao", "Jiayuan Mao", "Jia-Bin Huang", "Yilun Du"], "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io", "AI": {"tldr": "SIMPACT是一个在测试时通过模拟循环世界建模为视觉语言模型(VLMs)提供物理推理能力的框架，无需额外训练即可实现精细的机器人操作任务规划。", "motivation": "视觉语言模型(VLMs)在常识和语义推理方面表现出色，但缺乏对物理动态的深入理解。这是因为VLMs是在静态的互联网规模视觉语言数据上训练的，这些数据不包含因果交互或动作条件变化。因此，在需要物理理解、推理和相应动作规划的精细机器人操作任务中利用VLMs具有挑战性。", "method": "SIMPACT是一个测试时的模拟驱动动作规划框架。从单个RGB-D观察开始，高效构建物理模拟，使VLM能够提出明智的动作、观察模拟推演，并迭代优化其推理。通过将语言推理与物理预测相结合，模拟增强的VLM能够以物理基础的方式理解接触动态和动作结果。", "result": "该方法在五个需要精细物理推理的具有挑战性的真实世界刚体和可变形物体操作任务上展示了最先进的性能，优于现有的通用机器人操作模型。结果表明，在测试时通过高效模拟将物理理解嵌入VLM推理中，为实现通用化具身智能提供了有前景的路径。", "conclusion": "通过模拟循环世界建模为VLMs提供物理推理能力，无需额外训练即可显著提升其在精细机器人操作任务中的表现，为实现通用化具身智能提供了有效途径。"}}
{"id": "2512.05954", "pdf": "https://arxiv.org/pdf/2512.05954", "abs": "https://arxiv.org/abs/2512.05954", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code", "categories": ["cs.AI"], "comment": null, "summary": "We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems", "AI": {"tldr": "SymPyBench是一个大规模合成基准测试，包含15,045个大学物理问题，支持无限参数配置，提供结构化推理步骤和可执行Python代码，用于评估科学推理能力。", "motivation": "当前缺乏能够评估科学推理能力的大规模、动态、可执行的基准测试。需要能够测试模型在参数化问题上的表现，并量化其一致性和不确定性的评估方法。", "method": "创建包含15,045个参数化物理问题的基准测试，支持三种问题类型：MC-Symbolic、MC-Numerical和自由形式。每个问题都附带结构化推理步骤和可执行Python代码。引入三种新颖评估指标：一致性分数、失败率和混淆率。", "result": "实验使用最先进的指令调优语言模型进行测试，揭示了模型在科学推理方面的优势和局限性。基准测试能够有效量化模型在不同问题变体上的表现差异和不确定性。", "conclusion": "SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础，通过其动态、代码驱动的特性提供了更全面的科学推理能力评估框架。"}}
{"id": "2512.05953", "pdf": "https://arxiv.org/pdf/2512.05953", "abs": "https://arxiv.org/abs/2512.05953", "authors": ["Yunhao Cao", "Zubin Bhaumik", "Jessie Jia", "Xingyi He", "Kuan Fang"], "title": "Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning", "categories": ["cs.RO"], "comment": null, "summary": "We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.", "AI": {"tldr": "提出COIL框架，通过3D关键点对应关系表示任务，实现灵活的视觉运动控制", "motivation": "现有方法通常假设固定数量的关键点或均匀时间间隔，难以适应不同用户意图和任务需求的空间和时间粒度变化", "method": "使用3D关键点对应关系定义任务，设计具有时空注意力机制的条件策略，通过自监督管道在仿真中训练，自动生成对应标签", "result": "在真实世界操作任务中，无论是稀疏还是密集的任务规范，COIL都优于先前方法，能够跨任务、对象和运动模式泛化", "conclusion": "COIL提供了一种灵活的任务表示方法，通过3D对应关系实现鲁棒的视觉运动控制，适应不同的空间和时间粒度需求"}}
{"id": "2512.05951", "pdf": "https://arxiv.org/pdf/2512.05951", "abs": "https://arxiv.org/abs/2512.05951", "authors": ["Teofil Bodea", "Masanori Misono", "Julian Pritzi", "Patrick Sabanic", "Thore Sommer", "Harshavardhan Unnibhavi", "David Schall", "Nuno Santos", "Dimitrios Stavrakakis", "Pramod Bhatotia"], "title": "Trusted AI Agents in the Cloud", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.", "AI": {"tldr": "Omega系统是一个可信AI代理平台，通过端到端隔离、跨主体可验证信任建立和监督外部交互，确保云环境中AI代理的安全性和合规性。", "motivation": "当前基于大语言模型的AI代理作为云服务部署时，在复杂的多方生态系统中运行，存在数据泄露、篡改或意外行为的风险。现有的机密虚拟机仅提供二进制级别的保护，无法保证跨主体信任、加速器级隔离或监督代理行为。", "method": "Omega基于机密虚拟机和机密GPU构建可信代理平台，在单个CVM内使用嵌套隔离托管多个代理。通过差分认证建立跨主体信任，提供策略规范和强制执行框架来管理数据访问、工具使用和代理间通信。", "result": "在AMD SEV-SNP和NVIDIA H100上实现，Omega完全保护了CVM-GPU间的代理状态，实现了高性能，同时支持云规模的高密度、策略合规的多代理部署。", "conclusion": "Omega系统解决了云环境中AI代理的安全挑战，通过端到端隔离和可验证信任机制，为可信AI代理提供了可行的解决方案，支持大规模、合规的云部署。"}}
{"id": "2512.05950", "pdf": "https://arxiv.org/pdf/2512.05950", "abs": "https://arxiv.org/abs/2512.05950", "authors": ["Zalish Mahmud", "Anantaa Kotal", "Aritran Piplai"], "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025", "AI": {"tldr": "提出Impugan，一种基于条件生成对抗网络（cGAN）的鲁棒数据填补方法，用于处理不完整数据和整合异构数据集", "motivation": "现实应用中数据不完整问题普遍存在，传感器故障、记录不一致、多源数据差异等导致缺失值问题，传统填补方法基于线性和独立性假设，难以处理复杂异构数据，导致偏差或过度平滑估计", "method": "使用条件生成对抗网络（cGAN），在完整样本上训练学习缺失变量对观测变量的依赖关系，推理时生成器从可用特征重建缺失条目，判别器通过区分真实与填补数据来强制实现真实性", "result": "在基准数据集和多源整合任务实验中，Impugan相比领先基线方法实现了高达82%的地球移动距离（EMD）降低和70%的互信息偏差（MI）降低", "conclusion": "对抗训练的生成模型为填补和合并不完整、异构数据提供了可扩展且原则性的方法，能够捕捉传统方法无法表示的非线性和多模态关系"}}
{"id": "2512.05946", "pdf": "https://arxiv.org/pdf/2512.05946", "abs": "https://arxiv.org/abs/2512.05946", "authors": ["Truong Thanh Hung Nguyen", "Truong Thinh Nguyen", "Hung Cao"], "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem", "categories": ["cs.AI", "cs.ET", "cs.SE"], "comment": "Quantum Software Engineering Practices at The 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026)", "summary": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.", "AI": {"tldr": "提出了一种变分量子彩虹深度Q网络（VQR-DQN），将环形拓扑变分量子电路与彩虹DQN结合，用于优化资源分配问题", "motivation": "资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表示能力，需要利用量子叠加和纠缠来增强表示能力", "method": "将人力资源分配问题建模为马尔可夫决策过程，结合环形拓扑变分量子电路与彩虹DQN，利用量子叠加和纠缠增强策略表示", "result": "在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少了26.8%的归一化完工时间，相比Double DQN和经典彩虹DQN提升了4.9-13.4%", "conclusion": "量子增强的深度强化学习在大规模资源分配问题中具有潜力，电路表达能力、纠缠和策略质量之间存在理论联系"}}
{"id": "2512.05943", "pdf": "https://arxiv.org/pdf/2512.05943", "abs": "https://arxiv.org/abs/2512.05943", "authors": ["Shima Imani", "Seungwhan Moon", "Lambert Mathias", "Lu Zhang", "Babak Damavandi"], "title": "TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.", "AI": {"tldr": "提出TRACE框架，通过透明推理和一致性评估来分析和增强视觉语言模型的逐步推理能力", "motivation": "当前大型视觉语言模型在可靠数学和科学推理方面仍存在挑战，标准最终答案评估往往掩盖推理错误，导致无声故障持续存在", "method": "引入TRACE框架，利用辅助推理集（ARS）将复杂问题分解为子问题对，通过基于一致性的指标评估中间步骤，暴露标准评估忽略的故障", "result": "实验表明ARS间的一致性与最终答案正确性相关，能精确定位推理失败步骤，提供模型改进的可操作信号；TRACE定义置信区域区分可靠与不可靠推理路径", "conclusion": "TRACE框架通过诊断推理轨迹而非仅最终结果，为视觉语言模型的推理能力提供了有效的分析、过滤、调试和优化工具"}}
{"id": "2512.05941", "pdf": "https://arxiv.org/pdf/2512.05941", "abs": "https://arxiv.org/abs/2512.05941", "authors": ["Zhiyuan Jiang", "Shenghao Xie", "Wenyi Li", "Wenqiang Zu", "Peihang Li", "Jiahao Qiu", "Siqi Pei", "Lei Ma", "Tiejun Huang", "Mengdi Wang", "Shilong Liu"], "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick", "summary": "Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.", "AI": {"tldr": "本文提出了一种名为ZoomClick的训练免费方法，通过利用缩放作为GUI定位的强先验，解锁缩放的全部潜力用于动态空间聚焦和自适应上下文切换。", "motivation": "现有的GUI定位方法依赖大规模边界框监督，但仍面临跨平台泛化、复杂布局分析和细粒度元素定位等挑战。缩放作为一种强大但未被充分探索的GUI定位先验，具有解决这些问题的潜力。", "method": "提出ZoomClick方法，通过表征缩放的四个关键属性（预缩放、深度、收缩尺寸、最小裁剪尺寸），解锁缩放的全部能力用于动态空间聚焦和自适应上下文切换。该方法无需训练，可直接应用于现有模型。", "result": "实验表明，该方法显著提升了通用视觉语言模型和专用GUI定位模型的性能，在多个主流基准测试中达到最先进水平。例如，UI-Venus-72B在ScreenSpot-Pro上达到73.1%的成功率。同时提出了GUIZoom-Bench基准用于评估模型对缩放的适应性。", "conclusion": "缩放是GUI定位的强大先验，ZoomClick方法通过解锁缩放的潜力显著提升了现有模型的性能。提出的GUIZoom-Bench基准将激励未来研究改进缩放技术，用于GUI定位任务的进一步训练和测试时扩展。"}}
{"id": "2512.05937", "pdf": "https://arxiv.org/pdf/2512.05937", "abs": "https://arxiv.org/abs/2512.05937", "authors": ["Anne Sielemann", "Valentin Barner", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn", "Juergen Beyerer"], "title": "Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "8 pages, 2 figures, 7 tables", "summary": "Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...]. Download: synset.de/datasets/synset-signset-ger/background-effect", "AI": {"tldr": "研究背景对深度学习分类任务和特征重要性的影响，通过合成数据集量化分析背景相关性对交通标志识别的影响", "motivation": "当前可解释AI方法（如SHAP和GradCAM）虽然能分析输入特征对分类结果的重要性，但难以定量测试这些解释的可靠性。真实数据中的相关性难以避免，而合成数据又缺乏足够的真实性和随机性量化", "method": "系统生成六个合成数据集用于交通标志识别任务，这些数据集仅在相机变化程度和背景相关性上有所不同。通过控制变量方法，量化背景相关性、相机变化水平和交通标志形状对分类性能和背景特征重要性的独立影响", "result": "量化了背景特征在何时以及多大程度上获得重要性以支持分类任务，基于训练域的变化。提供了背景相关性对分类性能影响的具体测量结果", "conclusion": "通过受控的合成数据集，能够更准确地评估背景对深度学习分类模型的影响，为可解释AI方法提供了更可靠的定量分析基础，有助于区分健康分类器与过度拟合虚假相关性的模型"}}
{"id": "2512.05936", "pdf": "https://arxiv.org/pdf/2512.05936", "abs": "https://arxiv.org/abs/2512.05936", "authors": ["Anne Sielemann", "Lena Loercher", "Max-Lion Schumacher", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn"], "title": "Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 8 figures, 3 tables", "summary": "In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.", "AI": {"tldr": "提出一个用于德国交通标志识别的合成数据集生成管道和数据集，结合数据驱动和解析建模的优势", "motivation": "解决交通标志识别任务中真实数据获取困难、新标志数据稀缺的问题，同时为可解释AI和鲁棒性测试提供可控参数环境", "method": "结合GAN纹理生成（数据驱动的污损和磨损效果）与解析场景调制（物理正确的光照和详细参数化），生成包含211种德国交通标志的合成数据集", "result": "创建了包含105500张图像的Synset Signset Germany数据集，包含2020年新发布的罕见交通标志，提供掩码、分割图像和详细元数据，在真实数据集GTSRB上评估了真实度", "conclusion": "提出的合成数据生成方法结合了数据驱动和解析建模的优势，为交通标志识别提供了高质量、参数可控的合成数据集，支持可解释AI和鲁棒性测试应用"}}
{"id": "2512.05932", "pdf": "https://arxiv.org/pdf/2512.05932", "abs": "https://arxiv.org/abs/2512.05932", "authors": ["L. Dudzik", "M. Roschani", "A. Sielemann", "K. Trampert", "J. Ziehn", "J. Beyerer", "C. Neumann"], "title": "Physically-Based Simulation of Automotive LiDAR", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter. Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties. Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern. The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.", "AI": {"tldr": "提出了一种用于模拟汽车激光雷达的物理基础分析模型，包含光晕效应、回波脉冲宽度和环境光等特性，并通过光学实验室测量系统性地确定模型参数。", "motivation": "现有激光雷达模拟方法往往缺乏物理基础，无法准确模拟实际系统中的关键特性如光晕效应、回波脉冲宽度和环境光影响，需要一种更精确的物理基础模拟方法。", "method": "采用基于物理的渲染方法，在近红外域建立分析模型，包含单次反射和逆向反射，通过光栅化渲染图像进行着色或光线追踪，使用0.01°分辨率的高精度测角仪进行实验室测量确定系统参数。", "result": "成功为Valeo Scala Gen. 2和Blickfeld Cube 1两种不同特性的汽车激光雷达系统提取了相关模型参数，验证了方法的有效性。", "conclusion": "提出的物理基础模拟方法能够准确模拟汽车激光雷达的关键特性，通过系统性的实验室测量可以确定模型参数，适用于不同特性的激光雷达系统。"}}
{"id": "2512.05930", "pdf": "https://arxiv.org/pdf/2512.05930", "abs": "https://arxiv.org/abs/2512.05930", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.", "AI": {"tldr": "PRiSM是一个用于评估视觉语言模型科学推理能力的多模态基准测试，通过Python代码执行进行验证，包含超过24,750个大学水平的物理和数学问题。", "motivation": "当前评估视觉语言模型在科学领域（如数学和物理）能力的基准测试存在局限性：缺乏中间推理步骤、对变化的鲁棒性不足、缺少验证科学正确性的机制，且大多是静态数据集，无法全面评估模型的概念理解、符号推理和形式法则遵循能力。", "method": "开发了PRiSM基准测试，包含动态文本和视觉输入、生成图像，以及丰富的结构化输出（可执行的Python代码用于生成和验证真实答案、详细的逐步推理）。使用基于代理的PrismAgent管道生成结构化问题实例，支持Python驱动的自动真实答案生成。", "result": "提出了五个针对性评估任务：泛化能力、符号程序合成、扰动鲁棒性、推理修正和歧义消解。通过对现有视觉语言模型的全面评估，揭示了它们在科学推理方面的失败模式、不确定性行为和局限性。", "conclusion": "PRiSM基准测试能够对多模态视觉语言模型进行细粒度实验审计，深入洞察其科学推理能力，为评估模型在科学领域的表现提供了更全面、动态和可验证的框架。"}}
{"id": "2512.05928", "pdf": "https://arxiv.org/pdf/2512.05928", "abs": "https://arxiv.org/abs/2512.05928", "authors": ["Pedro Vidal", "Bernardo Biesseck", "Luiz E. L. Coelho", "Roger Granada", "David Menotti"], "title": "A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition", "categories": ["cs.CV"], "comment": "18 pages, 17 figures", "summary": "Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.", "AI": {"tldr": "比较不同合成人脸数据生成技术在面部识别任务中的效果", "motivation": "面部识别面临隐私法规、数据集退化、人口统计偏差、可解释性、鲁棒性等挑战，合成人脸数据被认为是解决这些问题的有前景方案", "method": "使用扩散模型、GANs和3D模型等不同技术生成合成人脸数据集，在八个领先数据集上评估准确性、rank-1、rank-5以及FPR=0.01%时的TPR", "result": "合成数据能够捕捉真实变化，但性能仍与真实数据存在差距；扩散模型、GANs和3D模型显示出显著进展，但仍面临挑战", "conclusion": "合成人脸数据在面部识别中具有潜力，能够缓解隐私问题、控制面部属性、减轻人口统计偏差，但需要进一步研究来缩小与真实数据的性能差距"}}
{"id": "2512.05927", "pdf": "https://arxiv.org/pdf/2512.05927", "abs": "https://arxiv.org/abs/2512.05927", "authors": ["Zhiting Mei", "Tenny Yin", "Micah Baker", "Ola Shorinwa", "Anirudha Majumdar"], "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", "AI": {"tldr": "提出C3方法，用于训练连续尺度校准的可控视频模型，实现亚补丁级别的密集置信度估计，精确定位生成视频帧中的不确定性区域", "motivation": "当前可控视频模型经常产生与现实物理不符的幻觉，但在机器人策略评估和规划等任务中缺乏评估和表达置信度的能力，阻碍了幻觉缓解", "method": "开发了三个核心创新：1) 通过严格适当评分规则训练视频模型的正确性和校准性；2) 在潜在空间估计不确定性，避免像素空间方法的不稳定性和高昂成本；3) 将密集潜在空间不确定性映射到可解释的像素级RGB空间不确定性", "result": "在大型机器人学习数据集（Bridge和DROID）和真实世界评估中，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测", "conclusion": "C3方法使视频模型能够知道何时不知道，通过亚补丁级别的密集置信度估计精确定位生成视频帧中的不确定性，为可控视频生成提供了校准的不确定性量化解决方案"}}
{"id": "2512.05926", "pdf": "https://arxiv.org/pdf/2512.05926", "abs": "https://arxiv.org/abs/2512.05926", "authors": ["Wenyan Luo", "Dustin G. Mixon"], "title": "BalLOT: Balanced $k$-means clustering with optimal transport", "categories": ["stat.ML", "cs.DS", "cs.IT", "cs.LG", "math.OC"], "comment": "20 pages, 9 figures", "summary": "We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.", "AI": {"tldr": "提出BalLOT方法，使用最优传输解决平衡k-means聚类问题", "motivation": "解决平衡k-means聚类这一基础问题，传统方法在平衡约束下效果有限", "method": "基于最优传输的交替最小化方法，将平衡k-means转化为最优传输问题求解", "result": "实验证明方法快速有效，理论分析显示对通用数据产生整数耦合，在随机球模型下能精确或部分恢复聚类结构", "conclusion": "BalLOT为平衡k-means聚类提供了理论保证的快速有效解决方案"}}
{"id": "2512.05925", "pdf": "https://arxiv.org/pdf/2512.05925", "abs": "https://arxiv.org/abs/2512.05925", "authors": ["Federico Bianchi", "Yongchan Kwon", "Zachary Izzo", "Linjun Zhang", "James Zou"], "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.", "AI": {"tldr": "开发基于GPT-5的论文正确性检查器，系统量化已发表AI论文中的客观错误，发现错误数量随时间增加", "motivation": "同行评审出版物是研究的基础，但其中的错误会在文献中传播，造成后续研究的混淆和可重复性问题。研究加速和同行评审压力使得错误更难被发现和避免", "method": "使用基于GPT-5的论文正确性检查器，系统识别顶级AI会议和期刊论文中的客观错误（公式、推导、计算、图表等有明确事实依据的错误），排除主观评价。人类专家验证AI识别的错误", "result": "1) 已发表论文包含不可忽视的客观错误数量；2) 每篇论文平均错误数随时间增加（NeurIPS从2021年3.8个增至2025年5.9个，增长55.3%）；3) AI检查器精度83.2%（316个潜在错误中263个被确认）；4) AI检查器能为75.8%的识别错误提出正确修正", "conclusion": "前沿大语言模型在检测和修正已发表论文中的客观错误方面具有潜力，有助于建立更坚实的知识基础。虽然大多数问题相对较小，但修正它们可以减少文献混淆并增强可重复性"}}
{"id": "2512.05922", "pdf": "https://arxiv.org/pdf/2512.05922", "abs": "https://arxiv.org/abs/2512.05922", "authors": ["Khang Le", "Anh Mai Vu", "Thi Kim Trang Vo", "Ha Thach", "Ngoc Bui Lam Quang", "Thanh-Huy Nguyen", "Minh H. N. Le", "Zhu Han", "Chandra Mohan", "Hien Van Nguyen"], "title": "LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation", "categories": ["cs.CV"], "comment": "Note: Khang Le and Anh Mai Vu contributed equally", "summary": "Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.", "AI": {"tldr": "提出了一种用于弱监督组织病理学分割的可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖，实现单阶段端到端训练", "motivation": "弱监督组织病理学分割面临类间同质性、类内异质性和CAM区域收缩问题，现有两阶段聚类原型方法成本高、对超参数敏感且原型发现与分割学习解耦", "method": "提出无聚类单阶段可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖，端到端训练可学习原型", "result": "在BCSS-WSSS数据集上达到最先进性能，mIoU和mDice优于先前方法，分割图显示更清晰的边界和更少的错误标签", "conclusion": "可学习原型框架相比聚类原型方法能覆盖更多样化和互补的类内区域，提供一致有效的弱监督组织病理学分割解决方案"}}
{"id": "2512.05920", "pdf": "https://arxiv.org/pdf/2512.05920", "abs": "https://arxiv.org/abs/2512.05920", "authors": ["Jiawen Yang", "Yihui Cao", "Xuanyu Tian", "Yuyao Zhang", "Hongjiang Wei"], "title": "NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.", "AI": {"tldr": "提出NICE（神经隐式颅面模型），用于正颌手术的面部软组织变形预测，通过隐式神经表示准确重建解剖结构并预测手术结果。", "motivation": "正颌手术用于矫正牙颌面骨骼畸形，但术后面部外观预测具有挑战性，因为骨骼移动与面部软组织之间存在复杂的非线性相互作用。现有方法（生物力学模型、参数化模型和深度学习方法）要么计算效率低，要么无法充分捕捉这些复杂相互作用。", "method": "NICE包含形状模块和手术模块：形状模块使用区域特定的隐式符号距离函数（SDF）解码器重建面部表面、上颌骨和下颌骨；手术模块使用区域特定的变形解码器，这些解码器由共享的手术潜在代码驱动，以建模面部表面对骨骼移动的复杂非线性生物力学响应，并融入解剖先验知识。变形解码器输出点位移场，实现精确的手术结果建模。", "result": "大量实验表明，NICE优于当前最先进的方法，显著提高了关键面部区域（如嘴唇和下巴）的预测准确性，同时稳健地保持了解剖完整性。", "conclusion": "这项工作为增强正颌手术的手术规划和患者咨询提供了临床可行的工具，通过隐式神经表示有效解决了面部软组织变形预测的挑战。"}}
{"id": "2512.05918", "pdf": "https://arxiv.org/pdf/2512.05918", "abs": "https://arxiv.org/abs/2512.05918", "authors": ["Xiaobo Wu", "Youmin Zhang"], "title": "A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following", "categories": ["eess.SP", "cs.RO", "stat.ML"], "comment": null, "summary": "Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol.", "AI": {"tldr": "提出一种基于残差方差匹配的递归最小二乘滤波器（RVM-RLS），用于无人机地形跟随任务中的实时航点估计，以提高在非线性时变系统中的估计精度。", "motivation": "现有实时滤波算法在非线性时变系统中难以在测量噪声下保持准确的航点估计，这可能导致飞行不稳定和火灾检测遗漏，特别是在无人机执行地形跟随的火灾巡逻任务中。", "method": "提出RVM-RLS滤波器，采用残差方差匹配估计（RVME）准则，自适应地估计非线性时变无人机地形跟随系统的实时航点。", "result": "在模拟地形环境中的无人机在线地形跟随系统验证表明，RVM-RLS滤波器相比基准算法在多个评估指标上提高了约88%的航点估计精度。", "conclusion": "该方法在实时滤波方面取得了方法学进展，RVM-RLS滤波器在无人机在线火灾巡逻中具有实际应用潜力。"}}
{"id": "2512.05908", "pdf": "https://arxiv.org/pdf/2512.05908", "abs": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": "Accepted at LLM4Code Workshop, ICSE 2026", "summary": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "AI": {"tldr": "提出一种基于自然语言摘要的多仓库微服务架构缺陷定位方法，将代码库转换为层次化NL摘要，通过NL-to-NL搜索替代跨模态检索，实现从仓库到目录到文件的缺陷定位路径。", "motivation": "微服务架构中的多仓库缺陷定位面临三大挑战：自然语言缺陷报告与代码之间的语义鸿沟、LLM上下文限制、以及需要先识别正确仓库。现有方法如跨模态检索和RAG系统效果有限。", "method": "将代码库转换为层次化自然语言摘要（文件、目录、仓库级别），采用两阶段搜索策略：首先将缺陷报告路由到相关仓库，然后在仓库内进行自上而下的定位（仓库→目录→文件）。", "result": "在包含46个仓库和110万行代码的工业系统DNext上评估，Pass@10达到0.82，MRR达到0.50，显著优于检索基线和GitHub Copilot、Cursor等RAG系统。", "conclusion": "工程化的自然语言表示比原始源代码在可扩展缺陷定位中更有效，提供可解释的仓库→目录→文件搜索路径，这对构建企业AI工具的信任至关重要。"}}
{"id": "2512.05906", "pdf": "https://arxiv.org/pdf/2512.05906", "abs": "https://arxiv.org/abs/2512.05906", "authors": ["Lennart P. L. Landsmeer", "Amirreza Movahedin", "Said Hamdioui", "Christos Strydis"], "title": "EventQueues: Autodifferentiable spike event queues for brain simulation on AI accelerators", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neural networks (SNNs), central to computational neuroscience and neuromorphic machine learning (ML), require efficient simulation and gradient-based training. While AI accelerators offer promising speedups, gradient-based SNNs typically implement sparse spike events using dense, memory-heavy data-structures. Existing exact gradient methods lack generality, and current simulators often omit or inefficiently handle delayed spikes. We address this by deriving gradient computation through spike event queues, including delays, and implementing memory-efficient, gradient-enabled event queue structures. These are benchmarked across CPU, GPU, TPU, and LPU platforms. We find that queue design strongly shapes performance. CPUs, as expected, perform well with traditional tree-based or FIFO implementations, while GPUs excel with ring buffers for smaller simulations, yet under higher memory pressure prefer more sparse data-structures. TPUs seem to favor an implementation based on sorting intrinsics. Selective spike dropping provides a simple performance-accuracy trade-off, which could be enhanced by future autograd frameworks adapting diverging primal/tangent data-structures.", "AI": {"tldr": "该论文提出了EventQueues，一种可自动微分的脉冲事件队列，用于在AI加速器上进行大脑模拟和脉冲神经网络训练。", "motivation": "脉冲神经网络在计算神经科学和神经形态机器学习中很重要，但现有方法在AI加速器上存在内存效率低、梯度计算不通用、延迟脉冲处理效率差等问题。", "method": "通过脉冲事件队列推导梯度计算（包括延迟），实现内存高效且支持梯度的队列数据结构，并在CPU、GPU、TPU、LPU等平台上进行基准测试。", "result": "队列设计对性能影响显著：CPU适合传统树结构或FIFO实现，GPU在小规模模拟中环形缓冲区表现好但在高内存压力下稀疏数据结构更优，TPU适合基于排序原语的实现。选择性脉冲丢弃提供了性能-精度权衡。", "conclusion": "EventQueues为AI加速器上的脉冲神经网络模拟提供了高效、可微分的解决方案，不同硬件平台需要不同的队列设计，未来自动微分框架可适应不同的原始/切线数据结构。"}}
{"id": "2512.05905", "pdf": "https://arxiv.org/pdf/2512.05905", "abs": "https://arxiv.org/abs/2512.05905", "authors": ["Wenhao Yan", "Sheng Ye", "Zhuoyi Yang", "Jiayan Teng", "ZhenHui Dong", "Kairui Wen", "Xiaotao Gu", "Yong-Jin Liu", "Jie Tang"], "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations", "categories": ["cs.CV"], "comment": null, "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.", "AI": {"tldr": "SCAIL是一个通过上下文学习3D一致姿态表示来实现工作室级角色动画的框架，旨在解决现有方法在复杂运动和跨身份动画中保持结构保真度和时间一致性的挑战。", "motivation": "尽管现有方法可以将驱动视频的运动转移到参考图像，但在涉及复杂运动和跨身份动画的野外场景中，往往无法保持结构保真度和时间一致性，难以达到工作室级的生产标准。", "method": "提出了两个关键创新：1）新颖的3D姿态表示，提供更鲁棒和灵活的运动信号；2）在扩散变换器架构中引入全上下文姿态注入机制，实现对完整运动序列的有效时空推理。同时开发了确保多样性和质量的策划数据管道。", "result": "实验表明SCAIL实现了最先进的性能，将角色动画推向工作室级的可靠性和真实感。建立了全面的基准进行系统评估。", "conclusion": "SCAIL框架通过创新的3D姿态表示和全上下文姿态注入机制，成功解决了复杂场景中角色动画的结构保真度和时间一致性问题，显著提升了动画质量，使其更接近工作室级标准。"}}
{"id": "2512.05880", "pdf": "https://arxiv.org/pdf/2512.05880", "abs": "https://arxiv.org/abs/2512.05880", "authors": ["Simon Guiroy", "Mats Richter", "Sarath Chandar", "Christopher Pal"], "title": "Neural Coherence : Find higher performance to out-of-distribution tasks from few samples", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.", "AI": {"tldr": "该论文提出了一种名为\"神经一致性\"的新方法，用于在数据稀缺、无标签且分布外的情况下，从预训练模型的大量检查点中选择最佳模型作为下游任务的起点。", "motivation": "当目标任务数据稀缺、无标签且分布外时，传统的基于分布内验证数据的方法变得不可靠或不适用。需要一种能够在仅使用少量无标签目标样本的情况下，可靠选择预训练模型检查点的方法。", "method": "提出\"神经一致性\"概念，通过表征模型在源域和目标域上的激活统计特性，定义具有高数据效率的模型选择方法。该方法仅需少量无标签目标样本即可工作。", "result": "在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，并在元学习设置中评估。相比现有基线方法，该方法显著提高了跨不同目标域的泛化性能，并展示了在训练数据选择中的有效性。", "conclusion": "神经一致性是一个强大的原理，能够在数据稀缺、无标签且分布外的情况下，有效选择预训练模型检查点，显著提升下游任务的泛化性能，并具有在训练数据选择等更多应用场景的潜力。"}}
{"id": "2512.05866", "pdf": "https://arxiv.org/pdf/2512.05866", "abs": "https://arxiv.org/abs/2512.05866", "authors": ["Md. Mahbub Hasan Akash", "Aria Tasnim Mridula", "Sheekar Banerjee", "Ishtiak Al Mamoon"], "title": "Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator", "categories": ["cs.CV"], "comment": "This paper has been accepted for presentation at the IEEE 28th International Conference on Computer and Information Technology (ICCIT), December 2025", "summary": "Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.", "AI": {"tldr": "提出了一种基于Swin Transformer和PatchGAN判别器的水下图像重建方法，用于解决水下图像的颜色失真、低对比度和雾霾效应问题。", "motivation": "水下成像对海洋探索、环境监测和基础设施检查至关重要，但水会导致严重的图像退化，包括波长依赖性吸收和散射造成的颜色失真、低对比度和雾霾效应。传统方法和基于卷积神经网络的方法由于感受野有限和无法建模全局依赖关系，往往无法充分解决这些挑战。", "method": "提出了一种新颖的深度学习框架，将Swin Transformer架构集成到生成对抗网络(GAN)中用于水下图像重建。生成器采用带有Swin Transformer块的U-Net结构，以捕获局部特征和长程依赖关系，这对整个图像的颜色校正至关重要。使用PatchGAN判别器进行对抗训练，确保高频细节的保留。", "result": "在EUVP数据集上训练和评估模型，定量结果显示最先进的性能：PSNR为24.76 dB，SSIM为0.89，显著优于现有方法。视觉结果显示有效的颜色平衡恢复、对比度改善和雾霾减少。消融研究证实了Swin Transformer设计相对于卷积替代方案的优越性。", "conclusion": "所提出的方法为各种海洋应用提供了稳健的水下图像重建解决方案，通过结合Swin Transformer的全局建模能力和GAN的对抗训练机制，有效解决了水下图像退化的关键问题。"}}
{"id": "2512.05865", "pdf": "https://arxiv.org/pdf/2512.05865", "abs": "https://arxiv.org/abs/2512.05865", "authors": ["Florent Draye", "Anson Lei", "Ingmar Posner", "Bernhard Schölkopf"], "title": "Sparse Attention Post-Training for Mechanistic Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\\approx 0.3 \\%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.", "AI": {"tldr": "提出一种简单的后训练方法，使Transformer注意力机制变得稀疏而不损失性能，通过稀疏正则化将注意力连接减少到约0.3%，同时保持原始预训练损失", "motivation": "现有稀疏注意力方法主要关注计算效率，而本文利用稀疏性作为结构先验，旨在通过稀疏化暴露更有组织和可解释的连接模式，从而简化全局电路结构", "method": "在约束损失目标下应用灵活的稀疏正则化，通过后训练过程减少注意力连接，该方法可应用于高达10亿参数的模型", "result": "在保持原始预训练损失的同时，将注意力连接减少到约0.3%的边，任务特定电路涉及更少的组件（注意力头和MLP），连接边减少高达100倍", "conclusion": "Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可以作为构建更有结构和可解释模型的指导原则"}}
{"id": "2512.05863", "pdf": "https://arxiv.org/pdf/2512.05863", "abs": "https://arxiv.org/abs/2512.05863", "authors": ["Tasnimul Hassan", "Md Faisal Karim", "Haziq Jeelani", "Elham Behnam", "Robert Green", "Fayeq Jeelani Syed"], "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.", "AI": {"tldr": "本文提出了一种基于检索增强生成（RAG）的医疗问答系统，通过结合领域特定知识检索与开源大语言模型来回答医疗问题，旨在提高事实准确性并减少幻觉。", "motivation": "直接将大语言模型应用于临床领域面临事实准确性不足和幻觉问题等挑战，需要开发能够保持事实正确性并提供可靠医疗信息的问答系统。", "method": "使用检索增强生成（RAG）框架，结合医疗文献检索与开源LLMs（LLaMA~2和Falcon），采用低秩适应（LoRA）进行高效领域微调，通过检索相关医学文献来增强模型回答的事实基础。", "result": "在PubMedQA和MedMCQA基准数据集上的评估显示，检索增强显著提高了答案准确性。微调的LLaMA~2模型在PubMedQA上达到71.8%的准确率，相比55.4%的零样本基线有显著提升，同时通过提供来源引用保持透明度，减少了约60%的无支持内容。", "conclusion": "RAG增强的开源大语言模型在可靠生物医学问答方面具有巨大潜力，为实用的临床信息学应用指明了方向，通过将答案基于检索证据显著提高了系统可靠性。"}}
{"id": "2512.05859", "pdf": "https://arxiv.org/pdf/2512.05859", "abs": "https://arxiv.org/abs/2512.05859", "authors": ["Abhijith Punnappurath", "Luxi Zhao", "Ke Zhao", "Hue Nguyen", "Radek Grzeszczuk", "Michael S. Brown"], "title": "Edit-aware RAW Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.", "AI": {"tldr": "提出一种可插拔的编辑感知损失函数，用于改进从sRGB图像重建RAW数据的鲁棒性，使其对不同的渲染风格和编辑操作更具适应性。", "motivation": "用户通常在拍摄后编辑相机图像以达到理想的照片处理风格。虽然RAW域编辑提供更高的准确性和灵活性，但大多数编辑都是在相机的显示参考输出（如8位sRGB JPEG）上进行的，因为RAW图像很少被存储。现有的RAW重建方法可以从sRGB图像恢复RAW数据，但这些方法通常针对像素级RAW重建保真度进行优化，在多样化的渲染风格和编辑操作下性能会下降。", "method": "引入一个可插拔的编辑感知损失函数，可以集成到任何现有的RAW重建框架中。该损失函数包含一个模块化的可微分图像信号处理器（ISP），模拟具有可调参数的真实照片处理流程。在训练过程中，每个ISP模块的参数从精心设计的分布中随机采样，这些分布模拟了实际相机处理中的变化。损失在sRGB空间中计算，通过这个可微分ISP渲染的地面真实RAW和重建RAW之间的差异。", "result": "在各种编辑条件下，结合该损失函数可将sRGB重建质量提高1.5-2 dB PSNR。当应用于元数据辅助的RAW重建方法时，该方法能够针对目标编辑进行微调，获得进一步的性能提升。", "conclusion": "由于照片编辑是消费级成像中RAW重建的主要动机，这种简单而有效的损失函数提供了一种通用机制，可以增强现有方法的编辑保真度和渲染灵活性。"}}
{"id": "2512.05853", "pdf": "https://arxiv.org/pdf/2512.05853", "abs": "https://arxiv.org/abs/2512.05853", "authors": ["Shiji Zhao", "Shukun Xiong", "Yao Huang", "Yan Jin", "Zhenyu Wu", "Jiyang Guan", "Ranjie Duan", "Jialing Tao", "Hui Xue", "Xingxing Wei"], "title": "VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.", "AI": {"tldr": "提出VRSA（视觉推理序列攻击）方法，通过将有害文本分解为多个顺序相关的子图像，诱导多模态大语言模型逐步外化并聚合完整的有害意图，实现对MLLMs的越狱攻击。", "motivation": "多模态大语言模型（MLLMs）因其强大的跨模态理解和生成能力被广泛应用，但更多模态也带来了更多被用于越狱攻击的漏洞。由于MLLMs具有强大的推理能力，以往的越狱攻击主要探索文本模态的推理安全风险，而视觉模态中的类似威胁在很大程度上被忽视了。为了全面评估视觉推理任务中的潜在安全风险，需要研究针对视觉模态的越狱攻击方法。", "method": "提出VRSA方法，包含三个关键技术：1）自适应场景优化（Adaptive Scene Refinement）：优化与原始有害查询最相关的场景，增强图像序列中场景的合理性；2）语义连贯补全（Semantic Coherent Completion）：结合场景中的上下文信息迭代重写每个子文本，确保生成图像的语义连续性；3）文本-图像一致性对齐（Text-Image Consistency Alignment）：保持语义一致性。通过将原始有害文本分解为多个顺序相关的子图像，诱导MLLMs逐步外化并聚合完整的有害意图。", "result": "一系列实验表明，VRSA在开源和闭源MLLMs（如GPT-4o和Claude-4.5-Sonnet）上都能实现比最先进的越狱攻击方法更高的攻击成功率。", "conclusion": "VRSA方法有效揭示了多模态大语言模型在视觉推理任务中的安全风险，通过视觉序列攻击能够成功诱导模型输出有害内容，表明视觉模态中的安全威胁需要得到更多关注和防护。"}}
{"id": "2512.05844", "pdf": "https://arxiv.org/pdf/2512.05844", "abs": "https://arxiv.org/abs/2512.05844", "authors": ["Daniel Rose", "Roxane Axel Jacob", "Johannes Kirchmair", "Thierry Langer"], "title": "NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.", "AI": {"tldr": "提出NEAT模型，一种用于3D分子生成的自回归集合变换器，通过邻域引导实现高效、顺序无关的分子图生成", "motivation": "现有自回归模型在3D分子生成中存在局限性：文本有自然顺序，但分子图中的下一个原子预测应该对原子排列顺序保持不变。先前工作使用规范顺序或焦点原子来回避这个问题，作者认为这是不必要的", "method": "提出NEAT模型，将分子图视为原子集合，使用自回归流模型学习图边界上可接受原子的顺序无关分布，实现邻域引导、高效的自回归集合变换", "result": "NEAT在3D分子生成中达到接近最先进的性能，具有高计算效率和原子级排列不变性，为可扩展分子设计建立了实用基础", "conclusion": "NEAT通过处理分子图为原子集合并学习顺序无关分布，解决了自回归模型中顺序假设与分子图排列不变性之间的不匹配问题，为高效、可扩展的3D分子生成提供了实用解决方案"}}
{"id": "2512.05837", "pdf": "https://arxiv.org/pdf/2512.05837", "abs": "https://arxiv.org/abs/2512.05837", "authors": ["Kaichen Ouyang"], "title": "Wasserstein Evolution : Evolutionary Optimization as Phase Transition", "categories": ["cs.NE"], "comment": "19 pages, 5 figures", "summary": "This paper establishes a novel connection between evolutionary computation and statistical physics by formalizing evolutionary optimization as a phase transition process. We introduce Wasserstein Evolution (WE), a principled optimization framework that implements the Wasserstein gradient flow of a free energy functional, mathematically bridging evolutionary dynamics with thermodynamics. WE directly translates the physical competition between potential gradient forces (exploitation) and entropic forces (exploration) into algorithmic dynamics, providing an adaptive, theoretically grounded mechanism for balancing exploration and exploitation. Experiments on challenging benchmark functions demonstrate that WE achieves competitive convergence performance while maintaining dramatically higher population diversity than classical methods (GA, DE, CMA-ES).This superior entropy preservation enables effective navigation of multi-modal landscapes without premature convergence, validating the physical interpretation of optimization as a disorder-to-order transition. Our work provides not only an effective optimization algorithm but also a new paradigm for understanding evolutionary computation through statistical physics.", "AI": {"tldr": "该论文建立了一个进化计算与统计物理学之间的新连接，将进化优化形式化为相变过程，提出了Wasserstein Evolution（WE）优化框架。", "motivation": "传统进化算法缺乏理论基础，探索与开发平衡机制不够系统。作者希望从统计物理学角度为进化计算提供新的理论基础，将优化过程理解为从无序到有序的相变。", "method": "提出了Wasserstein Evolution（WE）框架，实现自由能泛函的Wasserstein梯度流，将势梯度力（开发）与熵力（探索）的物理竞争转化为算法动力学。", "result": "在挑战性基准函数上的实验表明，WE实现了有竞争力的收敛性能，同时保持了比经典方法（GA、DE、CMA-ES）显著更高的种群多样性，有效避免了早熟收敛。", "conclusion": "该工作不仅提供了一个有效的优化算法，还通过统计物理学为理解进化计算提供了新范式，验证了优化作为无序到有序相变的物理解释。"}}
{"id": "2512.05836", "pdf": "https://arxiv.org/pdf/2512.05836", "abs": "https://arxiv.org/abs/2512.05836", "authors": ["Clarissa W. Ong", "Hiba Arnaout", "Kate Sheehan", "Estella Fox", "Eugen Owtscharow", "Iryna Gurevych"], "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.", "AI": {"tldr": "开发一个端到端管道，利用大型语言模型从治疗会话转录本中自动生成个性化心理网络，以支持临床案例概念化和治疗规划。", "motivation": "心理治疗个性化需要基于个性化网络选择治疗模块，但传统方法需要密集的纵向数据，难以规模化。本研究旨在利用LLM解决这一可扩展性问题，从治疗转录本中自动生成临床相关的网络。", "method": "1) 标注77个治疗转录本中的3364个心理过程及其维度；2) 应用上下文学习联合识别心理过程及其维度；3) 引入两步法将过程分组为临床有意义的聚类；4) 生成解释增强的聚类间关系。", "result": "方法在少量训练示例下达到高性能；专家评估显示，多步方法生成的网络在临床效用和可解释性上优于直接提示法（90%专家偏好）；网络在临床相关性、新颖性和有用性方面获得72-75%的评分。", "conclusion": "研究证明了使用LLM从治疗转录本创建临床相关网络的可行性，具有自下而上的案例概念化和潜在主题识别优势。未来研究应检验这些网络相对于其他个性化方法（包括统计估计网络）是否能改善治疗结果。"}}
{"id": "2512.05830", "pdf": "https://arxiv.org/pdf/2512.05830", "abs": "https://arxiv.org/abs/2512.05830", "authors": ["Muhammet Cagri Yeke", "Samil Sirin", "Kivilcim Yuksel", "Abdurrahman Gumus"], "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 11 figures, 5 tables", "summary": "This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.", "AI": {"tldr": "该论文提出了一种基于图像转换和深度学习的相位OTDR事件检测方法，将一维光纤传感数据转换为灰度图像，再组合成多通道RGB表示，用于六种光纤事件的分类。", "motivation": "传统相位OTDR数据分析方法在处理复杂光纤传感数据时存在局限性，需要更鲁棒和高效的事件检测方法。通过将一维数据转换为图像形式，可以利用先进的深度学习模型进行更准确的事件分类。", "method": "提出了一种新颖的数据转换方法：1）使用Gramian Angular Difference Field、Gramian Angular Summation Field和Recurrence Plot三种技术将一维相位OTDR数据转换为灰度图像；2）将这些灰度图像组合成多通道RGB表示；3）应用迁移学习模型（EfficientNetB0和DenseNet121）进行分类；4）采用5折交叉验证评估模型性能。", "result": "该方法在公开的相位OTDR数据集上取得了优异的分类性能：EfficientNetB0模型达到98.84%准确率，DenseNet121模型达到98.24%准确率。5折交叉验证测试准确率分别为99.07%和98.68%。同时减少了数据集大小并提高了分析效率。", "conclusion": "图像化分析方法在光纤传感数据解释中具有变革潜力，显著提高了光纤监测系统的准确性和可靠性。该方法为复杂光纤事件检测提供了高效解决方案，相关代码和图像数据集已在GitHub开源以支持进一步研究。"}}
{"id": "2512.05825", "pdf": "https://arxiv.org/pdf/2512.05825", "abs": "https://arxiv.org/abs/2512.05825", "authors": ["Shuhei Watanabe"], "title": "Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\\lfloor \\frac{M + 1}{2} \\rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.", "AI": {"tldr": "提出一种近似盒分解算法，用于加速基于超体积的多目标贝叶斯优化中的采集函数计算", "motivation": "超体积改进计算是超体积贝叶斯优化的主要计算瓶颈，现有精确盒分解算法在最坏情况下具有超多项式内存复杂度，而文献中缺乏对近似算法的严格算法描述", "method": "提供Couckuyt等人(2012)提出的近似盒分解算法的全面数学和算法细节，填补文献中的描述空白", "result": "提供了近似盒分解算法的完整数学框架和算法描述，使该算法能够被更广泛地理解和应用", "conclusion": "该论文填补了文献中近似盒分解算法描述的空缺，为加速超体积贝叶斯优化提供了重要的算法基础"}}
{"id": "2512.05824", "pdf": "https://arxiv.org/pdf/2512.05824", "abs": "https://arxiv.org/abs/2512.05824", "authors": ["Hafsa Akebli", "Adam Shephard", "Vincenzo Della Mea", "Nasir Rajpoot"], "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma", "categories": ["cs.AI", "cs.CV"], "comment": "4 pages, 2 figures", "summary": "Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.", "AI": {"tldr": "开发一个多模态肿瘤学智能体（MOA），用于低级别胶质瘤中IDH1突变的预测，整合组织病理学工具和临床基因组数据推理", "motivation": "低级别胶质瘤中IDH1突变定义了具有特定预后和治疗意义的临床亚组，需要准确预测方法来指导临床决策", "method": "提出多模态肿瘤学智能体（MOA），整合基于TITAN基础模型的病理学工具进行IDH1突变预测，结合PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理", "result": "在TCGA-LGG队列的488名患者中评估：MOA无病理学工具时F1-score为0.826（优于临床基线0.798）；融合病理学特征后达到最高性能F1-score为0.912，超过病理学基线（0.894）和融合病理学-临床基线（0.897）", "conclusion": "MOA通过外部生物医学资源丰富了互补的突变相关信息，能够准确预测IDH1突变，展示了多模态整合在肿瘤学预测中的优势"}}
{"id": "2512.05815", "pdf": "https://arxiv.org/pdf/2512.05815", "abs": "https://arxiv.org/abs/2512.05815", "authors": ["Marios-Nektarios Stamatopoulos", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints", "categories": ["cs.RO"], "comment": null, "summary": "This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV.", "AI": {"tldr": "提出一个用于多无人机空中3D打印的安全感知调度框架，通过优化任务分配和调度实现无冲突协作，同时考虑任务依赖、安全约束和资源限制", "motivation": "解决多无人机同时进行空中3D打印时的协调和任务规划问题，需要处理任务依赖关系、无人机间安全约束、材料使用和电池限制等复杂因素，实现高效无冲突的协作", "method": "提出一个优化问题框架，将施工任务分解为子任务，考虑无人机团队、有限体积和电池约束，生成包含任务分配和调度的最优任务计划；采用重要性优先级加速计算，通过效用最大化动态确定最优无人机数量", "result": "通过Gazebo仿真验证了框架的有效性，无人机在任务控制模块协调下基于生成的最优调度计划执行打印任务，同时保持在材料和电池约束范围内", "conclusion": "该框架能够实现多无人机空中3D打印的安全协调协作，平衡了最小化任务完成时间与部署过多无人机之间的权衡，为大规模空中建造提供了可行的解决方案"}}
{"id": "2512.05814", "pdf": "https://arxiv.org/pdf/2512.05814", "abs": "https://arxiv.org/abs/2512.05814", "authors": ["Fubao Zhu", "Zhanyuan Jia", "Zhiguo Wang", "Huan Huang", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chen Zhao", "Weihua Zhou"], "title": "UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection", "categories": ["cs.CV"], "comment": "The code is already available on GitHub: https://github.com/chenzhao2023/UG_FADDA_AlzhemiersClassification", "summary": "Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.", "AI": {"tldr": "提出UG-FedDA框架，将不确定性量化与联邦域适应相结合，用于多中心阿尔茨海默病检测，解决跨站点异质性和隐私保护问题。", "motivation": "现有阿尔茨海默病分类框架在多中心研究中面临挑战：忽视站点间异质性、缺乏不确定性量化机制，限制了模型的鲁棒性和临床适用性。", "method": "提出不确定性引导的联邦域适应框架，使用自注意力transformer提取多模板ROI特征，集成不确定性量化来指导特征对齐，通过降低不确定样本的权重来缓解源-目标分布偏移。", "result": "在ADNI、AIBL和OASIS三个公开数据集上，UG-FedDA在三个分类任务（AD vs. NC、MCI vs. AD、NC vs. MCI）中均取得一致的跨域改进。例如AD vs. NC任务中，在三个数据集上的准确率分别为90.54%、89.04%和77.78%。", "conclusion": "UG-FedDA框架不仅能够高效适应多个站点，还能保持严格的隐私保护，为多中心阿尔茨海默病检测提供了有效的解决方案。"}}
{"id": "2512.05812", "pdf": "https://arxiv.org/pdf/2512.05812", "abs": "https://arxiv.org/abs/2512.05812", "authors": ["Fabian Konstantinidis", "Moritz Sackmann", "Ulrich Hofmann", "Christoph Stiller"], "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.", "AI": {"tldr": "该论文提出了一种高效且鲁棒的多智能体驾驶模拟行为模型，通过实例中心化场景表示和查询中心化对称上下文编码器来优化交通参与者的行为控制。", "motivation": "可扩展的多智能体驾驶模拟需要既真实又计算高效的行为模型。现有方法在计算效率和交互建模方面存在不足，需要一种能够同时提高效率并保持行为真实性的解决方案。", "method": "采用实例中心化场景表示，每个交通参与者和地图元素在各自的局部坐标系中建模；使用查询中心化对称上下文编码器处理局部框架间的相对位置编码；通过对抗性逆强化学习训练行为模型，并提出自适应奖励变换来平衡鲁棒性和真实性。", "result": "实验表明，该方法能够随token数量高效扩展，显著减少训练和推理时间；在位置准确性和鲁棒性方面优于多个智能体中心化基线方法。", "conclusion": "提出的实例中心化表示和查询中心化编码器相结合的方法，为多智能体驾驶模拟提供了高效且鲁棒的行为建模解决方案，在计算效率和性能之间取得了良好平衡。"}}
{"id": "2512.05809", "pdf": "https://arxiv.org/pdf/2512.05809", "abs": "https://arxiv.org/abs/2512.05809", "authors": ["Saurav Jha", "M. Jehanzeb Mirza", "Wei Lin", "Shiqi Yang", "Sarath Chandar"], "title": "Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling", "categories": ["cs.CV", "cs.AI"], "comment": "Extended abstract at World Modeling Workshop 2026", "summary": "Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.", "AI": {"tldr": "本文系统研究了世界模型在空间推理任务中测试时缩放的有效性，提出了基于空间断言的验证框架ViSA，揭示了当前方法的局限性。", "motivation": "视觉语言模型在需要多视角理解和具身视角转换的空间推理任务中仍然存在局限。MindJourney等近期方法试图通过测试时缩放来弥补这一差距，但缺乏对这些验证器行为的系统分析。", "method": "提出了Verification through Spatial Assertions (ViSA)框架，将测试时奖励建立在可验证的、帧锚定的微观断言上，通过更平衡的探索行为纠正轨迹选择偏差。", "result": "在SAT-Real基准上，ViSA框架持续改善了空间推理能力并纠正了轨迹选择偏差；但在具有挑战性的MMSI-Bench上，包括ViSA在内的所有验证器都无法实现一致的缩放，表明当前世界模型形成了信息瓶颈。", "conclusion": "研究揭示了基于世界模型的测试时验证的\"坏、好、丑\"三个方面：验证器缺乏有意义的校准、随机评分同样有效、存在系统性动作偏差；ViSA框架提供了改进但仍有局限，当前世界模型的想象视图无法丰富细粒度推理。"}}
{"id": "2512.05808", "pdf": "https://arxiv.org/pdf/2512.05808", "abs": "https://arxiv.org/abs/2512.05808", "authors": ["Sushmita Bhattacharya", "Ninad Jadhav", "Hammad Izhar", "Karen Li", "Kevin George", "Robert Wood", "Stephanie Gil"], "title": "Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots", "categories": ["cs.RO"], "comment": "ef:International Symposium of Experimental Robotics 2025", "summary": "We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.", "AI": {"tldr": "开发了一个使用自主无人机实时与抹香鲸在海面会合的系统，结合模型强化学习和现场传感器数据进行导航决策", "motivation": "解决在海洋环境中实时追踪鲸鱼并与它们会合的技术挑战，包括多鲸鱼环境下的实时声学追踪、分布式通信决策以及机载信号处理等", "method": "采用基于模型的强化学习方法，结合现场传感器数据和经验性鲸鱼潜水模型来指导导航决策，系统包括声学追踪、分布式通信和机载信号处理模块", "result": "在多米尼加海域成功进行了与抹香鲸的实际会合实验，同时进行了陆地硬件实验和基于海洋生物学家表面观测数据的鲸鱼轨迹模拟", "conclusion": "该系统能够有效实现无人机与鲸鱼的实时会合，为解决海洋生物追踪和观测提供了创新的自主机器人解决方案"}}
{"id": "2512.05806", "pdf": "https://arxiv.org/pdf/2512.05806", "abs": "https://arxiv.org/abs/2512.05806", "authors": ["Martino Gulisano", "Marco Gabiccini"], "title": "Global stability of vehicle-with-driver dynamics via Sum-of-Squares programming", "categories": ["cs.RO", "eess.SY"], "comment": "20 pages, 7 figures, 2 tables", "summary": "This work estimates safe invariant subsets of the Region of Attraction (ROA) for a seven-state vehicle-with-driver system, capturing both asymptotic stability and the influence of state-safety bounds along the system trajectory. Safe sets are computed by optimizing Lyapunov functions through an original iterative Sum-of-Squares (SOS) procedure. The method is first demonstrated on a two-state benchmark, where it accurately recovers a prescribed safe region as the 1-level set of a polynomial Lyapunov function. We then describe the distinguishing characteristics of the studied vehicle-with-driver system: the control dynamics mimic human driver behavior through a delayed preview-tracking model that, with suitable parameter choices, can also emulate digital controllers. To enable SOS optimization, a polynomial approximation of the nonlinear vehicle model is derived, together with its operating-envelope constraints. The framework is then applied to understeering and oversteering scenarios, and the estimated safe sets are compared with reference boundaries obtained from exhaustive simulations. The results show that SOS techniques can efficiently deliver Lyapunov-defined safe regions, supporting their potential use for real-time safety assessment, for example as a supervisory layer for active vehicle control.", "AI": {"tldr": "通过Sum-of-Squares编程估计车辆-驾驶员系统的安全不变子集，用于实时安全评估和主动车辆控制", "motivation": "需要估计车辆-驾驶员系统的安全不变区域，以支持实时安全评估和作为主动车辆控制的监督层", "method": "使用原始迭代Sum-of-Squares程序优化Lyapunov函数，首先在二状态基准上验证，然后应用于七状态车辆-驾驶员系统，包括多项式近似非线性车辆模型和操作约束", "result": "在转向不足和转向过度场景中，估计的安全集与详尽模拟得到的参考边界进行比较，结果显示SOS技术能有效提供Lyapunov定义的安全区域", "conclusion": "SOS技术能够高效地提供Lyapunov定义的安全区域，支持其在实时安全评估中的潜在应用，例如作为主动车辆控制的监督层"}}
{"id": "2512.05803", "pdf": "https://arxiv.org/pdf/2512.05803", "abs": "https://arxiv.org/abs/2512.05803", "authors": ["Blanca Inigo", "Benjamin D. Killeen", "Rebecca Choi", "Michelle Song", "Ali Uneri", "Majid Khan", "Christopher Bailey", "Axel Krieger", "Mathias Unberath"], "title": "3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic systems are transforming image-guided interventions by enhancing accuracy and minimizing radiation exposure. A significant challenge in robotic assistance lies in surgical path planning, which often relies on the registration of intraoperative 2D images with preoperative 3D CT scans. This requirement can be burdensome and costly, particularly in procedures like vertebroplasty, where preoperative CT scans are not routinely performed. To address this issue, we introduce a differentiable rendering-based framework for 3D transpedicular path planning utilizing bi-planar 2D X-rays. Our method integrates differentiable rendering with a vertebral atlas generated through a Statistical Shape Model (SSM) and employs a learned similarity loss to refine the SSM shape and pose dynamically, independent of fixed imaging geometries. We evaluated our framework in two stages: first, through vertebral reconstruction from orthogonal X-rays for benchmarking, and second, via clinician-in-the-loop path planning using arbitrary-view X-rays. Our results indicate that our method outperformed a normalized cross-correlation baseline in reconstruction metrics (DICE: 0.75 vs. 0.65) and achieved comparable performance to the state-of-the-art model ReVerteR (DICE: 0.77), while maintaining generalization to arbitrary views. Success rates for bipedicular planning reached 82% with synthetic data and 75% with cadaver data, exceeding the 66% and 31% rates of a 2D-to-3D baseline, respectively. In conclusion, our framework facilitates versatile, CT-free 3D path planning for robot-assisted vertebroplasty, effectively accommodating real-world imaging diversity without the need for preoperative CT scans.", "AI": {"tldr": "提出一种基于可微分渲染的3D路径规划框架，用于机器人辅助椎体成形术，仅使用双平面X射线而不需要术前CT扫描", "motivation": "机器人辅助手术中，手术路径规划通常需要将术中2D图像与术前3D CT扫描配准，这在椎体成形术等不常规进行术前CT扫描的手术中既繁琐又昂贵", "method": "结合可微分渲染与统计形状模型生成的椎体图谱，使用学习相似性损失动态优化SSM形状和姿态，不依赖固定成像几何", "result": "在椎体重建方面优于归一化互相关基线（DICE: 0.75 vs 0.65），与最先进模型ReVerteR相当（DICE: 0.77）；双椎弓根路径规划成功率在合成数据上达82%，尸体数据上达75%，显著优于2D-to-3D基线", "conclusion": "该框架实现了无需CT的机器人辅助椎体成形术3D路径规划，有效适应真实世界成像多样性，具有临床应用潜力"}}
{"id": "2512.05802", "pdf": "https://arxiv.org/pdf/2512.05802", "abs": "https://arxiv.org/abs/2512.05802", "authors": ["Jiahua Dong", "Xudong Wang", "Wenqi Liang", "Zongyan Han", "Meng Cao", "Duzhen Zhang", "Hanbin Zhao", "Zhi Han", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Bring Your Dreams to Life: Continual Text-to-Video Customization", "categories": ["cs.CV"], "comment": "Accepted to AAAI2026", "summary": "Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.", "AI": {"tldr": "提出一种持续文本到视频定制模型（CCVD），能够连续学习新概念来生成视频，解决遗忘和概念忽视问题", "motivation": "现有定制化文本到视频生成方法假设个性化概念是静态的，不会随时间扩展，并且在持续学习新概念时存在遗忘和概念忽视问题", "method": "提出持续定制视频扩散模型（CCVD），包含概念特定属性保留模块和任务感知概念聚合策略来应对灾难性遗忘，以及可控条件合成来增强区域特征和对齐视频上下文", "result": "广泛的实验比较表明，CCVD在持续学习新概念方面优于现有的定制化文本到视频生成模型", "conclusion": "CCVD模型能够有效解决持续学习中的遗忘和概念忽视问题，实现高质量的持续文本到视频定制生成"}}
{"id": "2512.05794", "pdf": "https://arxiv.org/pdf/2512.05794", "abs": "https://arxiv.org/abs/2512.05794", "authors": ["Rebonto Haque", "Oliver M. Turnbull", "Anisha Parsan", "Nithin Parsan", "John J. Yang", "Charlotte M. Deane"], "title": "Mechanistic Interpretability of Antibody Language Models Using SAEs", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.", "AI": {"tldr": "研究使用稀疏自编码器（SAEs）对自回归抗体语言模型p-IgGen进行机制可解释性分析，比较TopK和Ordered两种SAE方法在特征识别和生成控制方面的表现", "motivation": "探索如何更好地理解和控制抗体语言模型的内部表示，为领域特定的蛋白质语言模型提供机制可解释性方法，解决特征识别与生成控制之间的平衡问题", "method": "使用TopK SAEs和Ordered SAEs两种稀疏自编码器技术分析自回归抗体语言模型p-IgGen，通过特征激活模式分析、概念相关性评估和生成控制实验来比较两种方法的性能", "result": "TopK SAEs能够揭示具有生物学意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAEs通过层次结构能够可靠识别可控制特征，但激活模式更复杂且可解释性较差", "conclusion": "对于将潜在特征映射到概念的任务，TopK SAEs已经足够；但当需要精确的生成控制时，Ordered SAEs更为合适。这些发现推进了领域特定蛋白质语言模型的机制可解释性研究"}}
{"id": "2512.05783", "pdf": "https://arxiv.org/pdf/2512.05783", "abs": "https://arxiv.org/abs/2512.05783", "authors": ["Maryam Yousefi", "Soodeh Bakhshandeh"], "title": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.", "AI": {"tldr": "提出了一种使用曲率正则化的变分自编码器，用于从稀疏深度数据中重建完整的3D场景", "motivation": "当深度传感器只能提供5%的必要测量数据时，重建完整的3D场景变得困难。自动驾驶车辆和机器人无法容忍稀疏重建引入的几何误差。", "method": "通过离散拉普拉斯算子实现曲率正则化，相比标准变分自编码器，该方法仅增加15%的训练开销且推理成本为零", "result": "相比标准变分自编码器，重建精度提高了18.1%，证明了单一精心设计的正则化项可以超越复杂的多约束组合方法", "conclusion": "挑战了几何深度学习中的隐含假设，证明单一精心设计的正则化项不仅匹配而且超越了复杂多约束公式的有效性"}}
{"id": "2512.05774", "pdf": "https://arxiv.org/pdf/2512.05774", "abs": "https://arxiv.org/abs/2512.05774", "authors": ["Ziyang Wang", "Honglu Zhou", "Shijie Wang", "Junnan Li", "Caiming Xiong", "Silvio Savarese", "Mohit Bansal", "Michael S. Ryoo", "Juan Carlos Niebles"], "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://activevideoperception.github.io/", "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.", "AI": {"tldr": "提出了一种主动视频感知框架（AVP），通过迭代的规划-观察-反思过程，让智能体主动决定观察视频的什么内容、何时观察以及在哪里观察，以高效获取与查询相关的证据。", "motivation": "长视频理解面临挑战，因为回答真实世界查询通常依赖于稀疏、时间分散的线索，这些线索被埋藏在数小时冗余且无关的内容中。现有的代理框架依赖查询无关的标注器来感知视频信息，这浪费了计算资源在无关内容上，并模糊了细粒度的时间和空间信息。", "method": "AVP采用迭代的计划-观察-反思过程：规划器提出有针对性的视频交互，观察器执行这些交互以提取带时间戳的证据，反射器评估证据是否足以回答查询，要么停止给出答案，要么触发进一步观察。", "result": "在五个长视频理解基准测试中，AVP实现了最高性能，显著优于现有方法。AVP在平均准确率上比最佳代理方法高出5.7%，同时仅需要18.4%的推理时间和12.4%的输入标记。", "conclusion": "主动视频感知框架通过让智能体主动决定观察内容，能够高效地从像素中获取紧凑、与查询相关的证据，显著提高了长视频理解的效率和准确性。"}}
{"id": "2512.05765", "pdf": "https://arxiv.org/pdf/2512.05765", "abs": "https://arxiv.org/abs/2512.05765", "authors": ["Edward Y. Chang"], "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics", "categories": ["cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.", "AI": {"tldr": "该论文提出AGI缺失的关键层是协调层，而非模式匹配能力本身。作者认为大型语言模型提供了必要的System-1模式库，但需要System-2协调层来选择和约束这些模式以实现推理。", "motivation": "针对批评者认为LLMs只是\"模式匹配器\"、无法实现真正推理和规划的观点，作者认为这种批评找错了瓶颈。真正的瓶颈不是模式匹配能力本身，而是缺乏一个协调层来选择和约束这些模式。", "method": "提出UCCT理论（语义锚定理论），将推理建模为由有效支持度、表征失配和自适应锚定预算控制的相变过程。基于此理论设计了MACI架构，包含诱饵（行为调制辩论）、过滤（苏格拉底式判断）和持久性（事务性记忆）三个组件。", "result": "通过UCCT理论框架，将无基础生成解释为未锚定的最大似然先验检索，而\"推理\"则是在锚点将后验概率向目标导向约束转移时出现。将常见批评重新解释为可测试的协调失败。", "conclusion": "通往AGI的道路应该通过LLMs而非绕过它们。LLMs提供了必要的System-1模式库，而缺失的System-2协调层可以通过UCCT理论和MACI架构来实现，从而解决当前LLMs的推理限制。"}}
{"id": "2512.05762", "pdf": "https://arxiv.org/pdf/2512.05762", "abs": "https://arxiv.org/abs/2512.05762", "authors": ["Ruochen Chen", "Thuy Tran", "Shaifali Parashar"], "title": "FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted for WACV", "summary": "We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.", "AI": {"tldr": "FNOPT是一个自监督布料仿真框架，使用傅里叶神经算子和元优化方法，通过将时间积分表述为优化问题来训练分辨率无关的神经优化器。", "motivation": "现有神经仿真器通常依赖大量地面真实数据或牺牲细节，在不同分辨率和运动模式上泛化能力差。需要一种无需重新训练就能在不同分辨率下保持稳定和准确的方法。", "method": "将时间积分表述为优化问题，使用傅里叶神经算子（FNO）参数化神经优化器，仅需在粗网格上使用基于物理的损失进行自监督训练。", "result": "FNOPT能够泛化到更精细的分辨率，捕捉细尺度皱纹并保持仿真稳定性，在分布外设置中优于先前基于学习的方法，在准确性和鲁棒性方面表现优异。", "conclusion": "基于FNO的元优化是先前布料神经仿真器的有吸引力的替代方案，减少了对策划数据的需求，提高了跨分辨率可靠性。"}}
{"id": "2512.05760", "pdf": "https://arxiv.org/pdf/2512.05760", "abs": "https://arxiv.org/abs/2512.05760", "authors": ["Zeyuan Ma", "Wenqi Huang", "Guo-Huan Song", "Hongshu Guo", "Sijie Ma", "Zhiguang Cao", "Yue-Jiao Gong"], "title": "Evolutionary System 2 Reasoning: An Empirical Proof", "categories": ["cs.AI"], "comment": null, "summary": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.", "AI": {"tldr": "提出进化推理优化（ERO）框架，通过进化算法在LLM群体中搜索具有强大推理能力的个体，以解决LLMs在系统2推理能力方面的不足。", "motivation": "尽管大语言模型在特定任务上表现出色，但在一般智能和系统2推理（慢思考）能力方面仍有不足。研究旨在探索机器智能（如LLMs）是否能像人类一样进化获得推理能力，而不仅仅是特定技能。", "method": "提出进化推理优化（ERO）框架：1）初始化多个LLMs作为种群；2）采用进化策略进化种群，最大化最佳个体的量化推理分数；3）通过适者生存原则在LLM群体中搜索具有强大推理能力的个体。", "result": "实验发现：1）最新LLMs（如GPT-5）仍表现出有限的系统2推理能力；2）通过简单的ERO进化循环，相对较弱的模型（Qwen-7B）可以被增强，涌现出强大的推理能力。", "conclusion": "进化方法可以有效提升LLMs的推理能力，证明了机器智能可以通过进化获得类似人类的推理能力。ERO框架为LLMs推理能力的提升提供了新途径。"}}
{"id": "2512.05759", "pdf": "https://arxiv.org/pdf/2512.05759", "abs": "https://arxiv.org/abs/2512.05759", "authors": ["Johannes Meyer", "Jasper Hoffmann", "Felix Schulz", "Dominik Merkle", "Daniel Buescher", "Alexander Reiterer", "Joschka Boedecker", "Wolfram Burgard"], "title": "Label-Efficient Point Cloud Segmentation with Active Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.", "AI": {"tldr": "提出一种基于主动学习的标签高效点云分割方法，通过2D网格划分点云并使用网络集成估计不确定性来选择标注区域", "motivation": "3D点云语义分割的标注成本很高，主动学习可以自动选择需要标注的数据，减少达到满意性能所需的标注总量", "method": "使用2D网格将点云划分为柱状区域，采用网络集成方法估计网络输出的不确定性，基于不确定性选择最有价值的区域进行标注", "result": "在S3DIS、Toronto-3D和Freiburg城市点云数据集上评估，性能达到或优于复杂的state-of-the-art方法，并发现标注面积比标注点数更能衡量主动学习算法效果", "conclusion": "提出了一种简单易实现的点云主动学习分割策略，在多个数据集上表现优异，且标注面积是比标注点数更有意义的评估指标"}}
{"id": "2512.05754", "pdf": "https://arxiv.org/pdf/2512.05754", "abs": "https://arxiv.org/abs/2512.05754", "authors": ["Xinjian Wu", "Hongmei Wang", "Yuan Zhou", "Qinglin Lu"], "title": "USV: Unified Sparsification for Accelerating Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.", "AI": {"tldr": "提出USV框架，通过统一稀疏化策略加速视频扩散模型，同时优化模型内部计算和采样过程", "motivation": "现有视频扩散模型存在两个主要冗余：全局时空注意力的二次复杂度和长迭代去噪轨迹的计算开销。现有加速方法通常只针对单一维度，效果有限", "method": "USV框架学习动态的、数据和时间步相关的稀疏化策略，协同剪枝冗余注意力连接、自适应合并语义相似token、减少去噪步数，将这些策略统一在一个优化目标中", "result": "在大规模视频生成基准测试中，USV实现了去噪过程83.3%的加速和端到端22.7%的加速，同时保持高视觉保真度", "conclusion": "统一的动态稀疏化是实现高效高质量视频生成的实用路径，多维度协同设计使先前独立的加速策略能够相互增强"}}
{"id": "2512.05753", "pdf": "https://arxiv.org/pdf/2512.05753", "abs": "https://arxiv.org/abs/2512.05753", "authors": ["Wencheng Cai", "Xuchao Gao", "Congying Han", "Mingqiang Li", "Tiande Guo"], "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.", "AI": {"tldr": "提出一种基于强化学习的快速抗干扰认知雷达部署算法（FARDA），用于解决现代战争中雷达快速部署对抗干扰的问题，相比传统进化算法大幅提升部署速度。", "motivation": "现代战争中快速部署认知雷达对抗干扰是关键挑战，现有方法主要基于进化算法，存在耗时严重、易陷入局部最优的问题，需要更高效的部署方案。", "method": "将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并设计新的奖励格式，形成FARDA框架。", "result": "实验结果表明，该方法在覆盖性能上与进化算法相当，但部署速度提升约7000倍。消融实验验证了FARDA各组成部分的必要性。", "conclusion": "提出的FARDA框架通过强化学习有效解决了雷达快速部署问题，在保持覆盖性能的同时大幅提升部署效率，为现代战争中的抗干扰雷达部署提供了高效解决方案。"}}
{"id": "2512.05751", "pdf": "https://arxiv.org/pdf/2512.05751", "abs": "https://arxiv.org/abs/2512.05751", "authors": ["Remo Burn", "Victor F. Ksoll", "Hubert Klahr", "Thomas Henning"], "title": "Exoplanet formation inference using conditional invertible neural networks", "categories": ["astro-ph.EP", "cs.NE", "physics.data-an"], "comment": "10 pages, accepted poster for the Machine Learning and the Physical Sciences Workshop at the 39th conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem.", "AI": {"tldr": "使用条件可逆神经网络（cINN）从观测到的系外行星特征推断其形成历史，包括出生位置等关键参数", "motivation": "传统的系外行星形成解释通常是定性的，由于行星形成模型中关键参数的不确定性，需要一种定量方法来追溯行星的出生位置和形成历史", "method": "使用条件可逆神经网络（cINN）在合成数据上进行训练，数据来自全球行星形成模型，追踪从尘埃到成熟巨行星的完整形成过程，包括单行星系统和多行星系统", "result": "在多行星数据上训练（每个行星作为独立点）效果良好，而单行星数据覆盖范围有限且对新行星属性的外推能力较差，多行星系统需要更多训练数据", "conclusion": "cINN为系外行星形成历史推断提供了有前景的定量方法，但需要更多多行星系统数据来应对更高维度问题"}}
{"id": "2512.05746", "pdf": "https://arxiv.org/pdf/2512.05746", "abs": "https://arxiv.org/abs/2512.05746", "authors": ["Shizhuo Mao", "Hongtao Zou", "Qihu Xie", "Song Chen", "Yi Kang"], "title": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.", "AI": {"tldr": "提出HQ-DM框架，通过单哈达玛变换实现扩散模型的量化感知训练，解决低比特量化下的激活异常值问题", "motivation": "扩散模型在图像生成领域应用广泛，但计算和内存成本高，难以部署。现有量化方法在低比特量化时无法有效处理激活矩阵中的异常值，导致性能显著下降", "method": "提出HQ-DM量化感知训练框架，对激活矩阵应用单哈达玛变换，有效减少激活异常值，同时保持量化下的模型性能。相比传统的双哈达玛变换，该方法能无缝支持INT卷积操作并防止权重异常值放大", "result": "在ImageNet 256x256数据集上使用LDM-4模型进行条件生成，W4A4和W4A3量化方案相比现有最优方法分别将Inception Score提高了12.8%和467.73%", "conclusion": "HQ-DM框架通过单哈达玛变换有效解决了扩散模型低比特量化中的激活异常值问题，显著提升了量化模型的性能，为扩散模型的轻量化部署提供了有效解决方案"}}
{"id": "2512.05740", "pdf": "https://arxiv.org/pdf/2512.05740", "abs": "https://arxiv.org/abs/2512.05740", "authors": ["Lennart Maack", "Julia-Kristin Graß", "Lisa-Marie Toscha", "Nathaniel Melling", "Alexander Schlaefer"], "title": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.", "AI": {"tldr": "提出一个隐私保护的框架，将大型通用LLM的知识蒸馏到高效、本地部署的视觉语言模型中，用于结直肠癌手术中的解剖结构解释", "motivation": "当前视觉语言模型在特定手术领域理解（如完全结肠系膜切除术中的解剖标志识别和解释）方面存在不足，且需要本地可部署模型以避免患者数据泄露到外部大型模型", "method": "通过仅使用文本上下文和二进制分割掩码（不包含敏感图像）提示教师LLM生成专家监督数据集，然后使用该数据集对本地可部署VLM进行监督微调和直接偏好优化", "result": "评估证实，使用生成的数据集微调VLM显著提高了其在手术领域知识方面的表现，相比基础VLM有大幅提升", "conclusion": "这项工作验证了一种数据高效且符合隐私保护要求的方法，用于训练手术领域优化的本地可部署VLM，以增强手术场景理解能力"}}
{"id": "2512.05734", "pdf": "https://arxiv.org/pdf/2512.05734", "abs": "https://arxiv.org/abs/2512.05734", "authors": ["Jinfeng Zhong", "Emmanuel Bacry", "Agathe Guilloux", "Jean-François Muzy"], "title": "KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.", "AI": {"tldr": "KANFormer是一个基于深度学习的模型，用于预测限价订单的成交时间，通过结合市场层面和代理层面的信息，利用生存分析方法来提高填充概率预测的准确性。", "motivation": "现有模型主要依赖限价订单簿的快照序列，忽略了与LOB动态相关的代理行为以及订单在队列中的位置信息，这些信息对于捕捉执行可能性模式至关重要。需要更有效地整合这些信息来提高预测准确性。", "method": "KANFormer结合了扩张因果卷积网络和Transformer编码器，并通过Kolmogorov-Arnold Networks（KANs）增强非线性逼近能力。模型整合了与LOB动态相关的代理行为以及订单在队列中的位置信息，使用生存分析方法进行时间到填充的预测。", "result": "在CAC 40指数期货数据上的评估显示，KANFormer在校准指标（右删失对数似然、集成Brier分数）和区分指标（C指数、时间依赖性AUC）方面均优于现有方法。使用SHAP进行的特征重要性分析进一步验证了模型的可解释性。", "conclusion": "结合丰富的市场信号和表达能力强的神经架构能够实现准确且可解释的填充概率预测。KANFormer通过整合代理层面信息和订单位置信息，显著提高了限价订单成交时间预测的性能。"}}
{"id": "2512.05732", "pdf": "https://arxiv.org/pdf/2512.05732", "abs": "https://arxiv.org/abs/2512.05732", "authors": ["Ippokratis Pantelidis", "Korbinian Randl", "Aron Henriksson"], "title": "Efficient Text Classification with Conformal In-Context Learning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.", "AI": {"tldr": "本文提出并系统评估了CICLe框架，这是一种结合轻量级基础分类器和保形预测的资源高效文本分类方法，旨在减少LLM提示的候选类别集，提高分类效率和性能。", "motivation": "大型语言模型在文本分类中虽然表现出强大的上下文学习能力，但其效果严重依赖提示设计且计算成本高昂。现有方法缺乏对CICLe框架在不同领域适用性和效率优势的系统性探索。", "method": "CICLe框架整合轻量级基础分类器与保形预测技术，通过自适应减少候选类别集来指导LLM提示，减少所需的样本数量和提示长度。", "result": "在多样化的NLP分类基准测试中，CICLe持续优于基础分类器，在样本充足时超越少样本提示基线，在低数据情况下表现相当。效率方面，减少样本数和提示长度分别达34.45%和25.16%，并支持使用更小模型保持竞争力，特别适用于类别不平衡任务。", "conclusion": "CICLe是一种实用且可扩展的高效文本分类方法，结合了传统分类器的鲁棒性和LLMs的适应性，在数据和计算效率方面取得显著提升。"}}
{"id": "2512.05714", "pdf": "https://arxiv.org/pdf/2512.05714", "abs": "https://arxiv.org/abs/2512.05714", "authors": ["Max Martin Gnewuch", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "Big Tech-Funded AI Papers Have Higher Citation Impact, Greater Insularity, and Larger Recency Bias", "categories": ["cs.DL", "cs.AI", "cs.CL"], "comment": "Published at IEEE (ACDSA)", "summary": "Over the past four decades, artificial intelligence (AI) research has flourished at the nexus of academia and industry. However, Big Tech companies have increasingly acquired the edge in computational resources, big data, and talent. So far, it has been largely unclear how many papers the industry funds, how their citation impact compares to non-funded papers, and what drives industry interest. This study fills that gap by quantifying the number of industry-funded papers at 10 top AI conferences (e.g., ICLR, CVPR, AAAI, ACL) and their citation influence. We analyze about 49.8K papers, about 1.8M citations from AI papers to other papers, and about 2.3M citations from other papers to AI papers from 1998-2022 in Scopus. Through seven research questions, we examine the volume and evolution of industry funding in AI research, the citation impact of funded papers, the diversity and temporal range of their citations, and the subfields in which industry predominantly acts. Our findings reveal that industry presence has grown markedly since 2015, from less than 2 percent to more than 11 percent in 2020. Between 2018 and 2022, 12 percent of industry-funded papers achieved high citation rates as measured by the h5-index, compared to 4 percent of non-industry-funded papers and 2 percent of non-funded papers. Top AI conferences engage more with industry-funded research than non-funded research, as measured by our newly proposed metric, the Citation Preference Ratio (CPR). We show that industry-funded research is increasingly insular, citing predominantly other industry-funded papers while referencing fewer non-funded papers. These findings reveal new trends in AI research funding, including a shift towards more industry-funded papers and their growing citation impact, greater insularity of industry-funded work than non-funded work, and a preference of industry-funded research to cite recent work.", "AI": {"tldr": "分析大型科技公司资助的AI论文在引用影响力、封闭性和时效性偏见方面的特征", "motivation": "过去几十年AI研究在学术界和工业界蓬勃发展，但大型科技公司在计算资源、大数据和人才方面占据优势。目前尚不清楚工业界资助了多少论文，其引用影响力如何，以及什么驱动了工业界的兴趣。", "method": "分析了10个顶级AI会议（如ICLR、CVPR、AAAI、ACL）约49.8K篇论文、约180万次AI论文引用和约230万次其他论文引用AI论文的数据（1998-2022年）。通过七个研究问题，使用新提出的指标Citation Preference Ratio (CPR)等方法进行分析。", "result": "工业界资助的论文具有更高的引用影响力（2018-2022年间12%达到高引用率，而非工业界资助为4%，非资助为2%），表现出更强的封闭性（主要引用其他工业界资助论文），且更倾向于引用近期工作。", "conclusion": "AI研究资助呈现新趋势：工业界资助论文比例增加且引用影响力增长，工业界资助工作比非资助工作更具封闭性，工业界资助研究更偏好引用近期工作。"}}
{"id": "2512.05711", "pdf": "https://arxiv.org/pdf/2512.05711", "abs": "https://arxiv.org/abs/2512.05711", "authors": ["Ali Krayani", "Seyedeh Fatemeh Sadati", "Lucio Marcenaro", "Carlo Regazzoni"], "title": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning", "categories": ["cs.RO", "cs.AI", "eess.SP", "eess.SY"], "comment": "This paper has been accepted for the 2026 IEEE Consumer Communications & Networking Conference (IEEE CCNC 2026)", "summary": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.", "AI": {"tldr": "提出了一种基于贝叶斯主动推理的分层轨迹规划框架，用于无人机在对抗性干扰条件下的智能抗干扰和自适应轨迹规划", "motivation": "无人机在对抗性干扰环境下需要智能的抗干扰和自适应轨迹规划能力，传统方法缺乏对干扰环境的有效建模和在线适应能力", "method": "采用贝叶斯主动推理方法，结合专家演示和概率生成模型，编码高层符号规划、低层运动策略和无线信号反馈，实现在线干扰预测和干扰源定位", "result": "仿真结果表明，该方法实现了接近专家水平的性能，显著降低了通信干扰和任务成本，相比无模型强化学习方法具有更好的鲁棒性和泛化能力", "conclusion": "贝叶斯主动推理框架为无人机在对抗性干扰环境下的智能轨迹规划提供了有效解决方案，能够在未知干扰源位置的情况下实现自适应抗干扰"}}
{"id": "2512.05710", "pdf": "https://arxiv.org/pdf/2512.05710", "abs": "https://arxiv.org/abs/2512.05710", "authors": ["Jianan Sun", "Dongzhihan Wang", "Mingyu Fan"], "title": "Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.", "AI": {"tldr": "提出了一种基于流形感知的点云补全框架，通过显式整合非线性几何信息来提升几何一致性和语义连贯性", "motivation": "现有方法主要依赖欧几里得距离，忽略了点云内在的非线性几何结构，导致几何一致性不足和语义模糊问题", "method": "引入两个关键模块：测地线距离近似器(GDA)估计点间测地线距离以捕捉潜在流形拓扑；流形感知特征提取器(MAFE)利用基于测地线的k-NN分组和测地线关系注意力机制指导层次特征提取", "result": "在基准数据集上的大量实验表明，该方法在重建质量方面持续优于现有最先进方法", "conclusion": "通过整合测地线感知的关系注意力机制，该方法能够提升重建点云的语义连贯性和结构保真度"}}
{"id": "2512.05700", "pdf": "https://arxiv.org/pdf/2512.05700", "abs": "https://arxiv.org/abs/2512.05700", "authors": ["Ben Malin", "Tatiana Kalganova", "Nikolaos Boulgouris"], "title": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, conference paper", "summary": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.", "AI": {"tldr": "提出一种基于度量融合的方法来改进大型语言模型（LLM）忠实度评估的准确性，通过结合多个基础忠实度指标并利用树模型确定各指标重要性，从而创建与人类判断相关性更强的综合评估指标。", "motivation": "当前LLM忠实度评估存在准确性不足的问题，需要更可靠的评估方法来增强对LLM输出的信任度，以便在更多样化的场景中安全部署这些模型。", "method": "将多个基础忠实度指标融合为综合指标，使用基于树结构的模型确定各指标重要性权重，该权重由人类对LLM响应忠实度的判断驱动，并在问答和对话领域的数据集上进行验证。", "result": "融合后的指标在所有测试领域中与人类判断的相关性更强，提高了LLM忠实度评估的准确性，为模型部署提供了更高的可信度。", "conclusion": "通过度量融合方法可以显著改进LLM忠实度评估，增强对模型的信任，促进其在更广泛场景中的应用，同时提供了可复现的数据集和评估框架。"}}
{"id": "2512.05698", "pdf": "https://arxiv.org/pdf/2512.05698", "abs": "https://arxiv.org/abs/2512.05698", "authors": ["Xusheng Guo", "Wanfa Zhang", "Shijia Zhao", "Qiming Xia", "Xiaolong Xie", "Mingming Wang", "Hai Wu", "Chenglu Wen"], "title": "OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning", "categories": ["cs.CV"], "comment": "The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.", "AI": {"tldr": "OWL提出了一种无监督3D目标检测方法，通过占用引导预热和大模型先验推理来解决伪标签质量差的问题，显著提升了无监督3D检测性能。", "motivation": "现有无监督3D目标检测方法依赖伪标签进行自训练，但初始伪标签通常不准确，会误导优化过程。同时，如何有效过滤和精炼伪标签是一个关键挑战。", "method": "1) 占用引导预热策略：初始化骨干网络权重，使其具备空间感知能力，减少错误伪标签对网络收敛的干扰。2) 实例提示推理模块：利用大模型先验知识评估伪标签质量，实现精确过滤和精炼。3) 权重自适应自训练策略：动态重新加权伪标签，通过自训练提升性能。", "result": "在Waymo Open Dataset和KITTI数据集上的大量实验表明，OWL比现有最先进的无监督方法性能提升超过15.0% mAP，证明了方法的有效性。", "conclusion": "OWL通过占用引导预热、大模型先验推理和权重自适应自训练，有效解决了无监督3D目标检测中伪标签质量差的问题，显著提升了检测性能，为减少自动驾驶标注成本提供了有前景的解决方案。"}}
{"id": "2512.05693", "pdf": "https://arxiv.org/pdf/2512.05693", "abs": "https://arxiv.org/abs/2512.05693", "authors": ["Zhiying Du", "Bei Liu", "Yaobo Liang", "Yichao Shen", "Haidong Cao", "Xiangyu Zheng", "Zhiyuan Feng", "Zuxuan Wu", "Jiaolong Yang", "Yu-Gang Jiang"], "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.", "AI": {"tldr": "提出HiMoE-VLA框架，通过分层混合专家架构处理机器人演示数据的异质性，实现通用视觉-语言-动作策略", "motivation": "机器人演示数据存在显著的异质性（如不同机器人本体、动作空间、传感器配置、控制频率等），现有方法缺乏处理这种异质性的显式设计，导致集成困难、泛化受限、迁移性能下降", "method": "引入分层混合专家（HiMoE）架构作为动作模块，自适应处理多源异质性，逐步将其抽象为共享知识表示，有效整合多样化的机器人数据", "result": "在仿真基准和真实机器人平台上进行广泛实验，相比现有VLA基线方法，HiMoE-VLA表现出持续的性能提升，实现了更高的准确性和跨不同机器人及动作空间的鲁棒泛化", "conclusion": "HiMoE-VLA框架通过分层混合专家架构有效处理机器人数据的异质性，为具身智能的基础模型发展提供了新思路，代码和模型已公开"}}
{"id": "2512.05683", "pdf": "https://arxiv.org/pdf/2512.05683", "abs": "https://arxiv.org/abs/2512.05683", "authors": ["Yong En Kok", "Bowen Deng", "Alexander Bentley", "Andrew J. Parkes", "Michael G. Somekh", "Amanda J. Wright", "Michael P. Pound"], "title": "Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction", "categories": ["cs.CV", "physics.optics"], "comment": null, "summary": "Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at https://github.com/janetkok/ZRNet.", "AI": {"tldr": "提出ZRNet物理信息图神经网络框架，联合执行Zernike系数预测和光学图像恢复，解决显微镜成像中的光学像差问题", "motivation": "现有方法通常只处理有限样本类型和模态的轻微像差，将问题视为黑盒映射而未利用波前畸变的光学物理原理，无法有效处理复杂大振幅像差", "method": "提出Zernike图模块显式建模Zernike多项式间的物理关系，引入频率感知对齐损失在傅里叶域对齐Zernike系数预测和图像特征，确保物理一致性", "result": "在CytoImageNet数据集上的广泛实验表明，该方法在图像恢复和Zernike系数预测方面均达到最先进性能，适用于多种显微镜模态和生物样本", "conclusion": "ZRNet通过物理信息学习和频率感知对齐，有效解决了复杂大振幅光学像差校正问题，为显微镜图像质量提升提供了新方法"}}
{"id": "2512.05682", "pdf": "https://arxiv.org/pdf/2512.05682", "abs": "https://arxiv.org/abs/2512.05682", "authors": ["Yiming Shu", "Jiahui Xu", "Linghuan Kong", "Fangni Zhang", "Guodong Yin", "Chen Sun"], "title": "Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.", "AI": {"tldr": "提出一个场景感知的不确定性量化框架，为轨迹预测提供统计保证的预测区间和可靠性评估", "motivation": "现有深度学习轨迹预测器缺乏适应异构现实场景的不确定性感知框架，而可靠的不确定性量化对安全关键自动驾驶系统至关重要", "method": "使用Frenet坐标系将预测轨迹投影到地图参考路径，采用CopulaCPTS作为保形校准方法生成场景特定的时间预测区间，通过轨迹可靠性判别器分析平均误差和校准置信区间建立可靠性模型，利用联合风险模型识别关键点并分割轨迹", "result": "在nuPlan真实世界数据集上评估，证明框架在不同驾驶场景下有效进行场景感知的不确定性量化和可靠性评估", "conclusion": "提出的框架能够为轨迹预测提供统计保证的不确定性量化和可靠性评估，为下游规划模块提供可操作的可靠性结果"}}
{"id": "2512.05681", "pdf": "https://arxiv.org/pdf/2512.05681", "abs": "https://arxiv.org/abs/2512.05681", "authors": ["Tereza Novotna", "Jakub Harasta"], "title": "Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods", "categories": ["cs.CL", "cs.AI"], "comment": "The manuscript has been accepted for presentation as a short paper at the 38th International Conference on Legal Knowledge and Information Systems (JURIX 2025) in Torino, Italy", "summary": "Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.", "AI": {"tldr": "比较两种嵌入模型在捷克宪法法院判决检索中的性能，评估在噪声制度标签下的语义相似决策检索效果", "motivation": "案例法检索是一项耗时的任务，通常通过查询数据库完成。需要评估不同嵌入模型在噪声标签环境下的性能，以支持法律检索系统的开发", "method": "比较两种模型：1) 通用大型嵌入器(OpenAI)，2) 领域特定BERT(在约30,000个判决上训练，使用滑动窗口和注意力池化)。提出噪声感知评估框架，包括IDF加权关键词重叠作为分级相关性、通过两个阈值(0.20平衡，0.28严格)进行二值化、配对自助法显著性检验，以及nDCG诊断和定性分析", "result": "尽管绝对nDCG值较低(在噪声标签下预期如此)，但通用OpenAI嵌入器在两个阈值设置下(@10/@20/@100)都明显优于领域预训练的BERT；差异具有统计显著性。诊断表明低绝对值归因于标签漂移和强理想标准，而非缺乏实用性", "conclusion": "提出的评估框架足够稳健，可用于在噪声黄金数据集下进行评估，这在处理来自遗留司法数据库的异构标签数据时很典型。通用嵌入器在语义相似决策检索中表现优于领域特定模型"}}
{"id": "2512.05674", "pdf": "https://arxiv.org/pdf/2512.05674", "abs": "https://arxiv.org/abs/2512.05674", "authors": ["Gargi Panda", "Soumitra Kundu", "Saumik Bhattacharya", "Aurobinda Routray"], "title": "Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.", "AI": {"tldr": "提出了一种基于算法展开的3D卷积稀疏编码网络（3D-CSCNet）用于高光谱解混任务，结合投影单纯形体积最大化算法进行端元初始化", "motivation": "现有算法展开网络未能充分利用高光谱图像的三维数据特性，且缺乏有效的端元初始化方法，需要同时学习光谱和空间关系", "method": "基于3D卷积稀疏编码模型构建自动编码器框架，通过深度算法展开设计3D-CSC块估计丰度矩阵，使用投影单纯形体积最大化算法初始化端元", "result": "在三个真实数据集和一个模拟数据集上，3D-CSCNet在不同信噪比水平下均优于现有最先进方法", "conclusion": "3D-CSCNet通过结合3D卷积稀疏编码和投影单纯形体积最大化，有效解决了高光谱解混问题，在性能和鲁棒性方面表现出色"}}
{"id": "2512.05672", "pdf": "https://arxiv.org/pdf/2512.05672", "abs": "https://arxiv.org/abs/2512.05672", "authors": ["Yeobin Hong", "Suhyeon Lee", "Hyungjin Chung", "Jong Chul Ye"], "title": "InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.", "AI": {"tldr": "提出InverseCrafter方法，将4D视频生成任务重新表述为潜在空间中的修复问题，通过编码像素空间退化算子到连续多通道潜在掩码，避免重复的VAE操作和反向传播计算开销。", "motivation": "当前可控4D视频生成方法通常依赖于微调预训练的视频扩散模型，这种方法计算成本高，需要大规模数据集和架构修改，且经常遭受灾难性遗忘问题，丢失模型原有的生成先验。", "method": "提出一种高效的修复逆求解器，将4D生成任务重新表述为潜在空间中的修复问题。核心方法是将像素空间退化算子编码为连续的多通道潜在掩码，从而绕过重复的VAE操作和反向传播的计算瓶颈。", "result": "InverseCrafter在相机控制任务中实现了可比的新视角生成和更优的测量一致性，且计算开销接近零。同时，在通用视频修复和编辑任务中也表现出色。", "conclusion": "InverseCrafter提供了一种高效、计算成本低的4D视频生成方法，避免了传统微调方法的计算开销和灾难性遗忘问题，在多个任务中表现出优越性能。"}}
{"id": "2512.05669", "pdf": "https://arxiv.org/pdf/2512.05669", "abs": "https://arxiv.org/abs/2512.05669", "authors": ["Talha Enes Koksal", "Abdurrahman Gumus"], "title": "Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.", "AI": {"tldr": "提出一种基于深度学习和几何特征的实时序列面部表情识别方法，使用MediaPipe FaceMesh进行面部关键点检测，提取几何特征，并通过ConvLSTM1D网络进行时序分类。", "motivation": "面部表情识别对于增强人机交互和开发情感感知系统至关重要，实时检测和解释面部表情在用户体验个性化和智能监控等应用中变得越来越重要。", "method": "使用MediaPipe FaceMesh进行快速准确的面部关键点检测，提取欧几里得距离和角度等几何特征，通过分析连续帧之间的特征差异来捕捉表情的时序动态（起始、顶点、偏移阶段），采用ConvLSTM1D网络和多层感知机块进行分类。", "result": "在CK+、Oulu-CASIA（VIS和NIR）、MMI等多个公开数据集上分别达到93%、79%、77%、68%的准确率，在消费级硬件上实现约165帧/秒的实时处理速度。", "conclusion": "该方法为面部表情分析领域提供了一个快速、准确且适应性强的解决方案，展示了在情感感知技术和个性化用户体验方面的潜力，为更复杂的人机交互系统铺平了道路。"}}
{"id": "2512.05667", "pdf": "https://arxiv.org/pdf/2512.05667", "abs": "https://arxiv.org/abs/2512.05667", "authors": ["Jilles Steeve Dibangoye", "Thibaut Le Marre", "Ocan Sankur", "François Schwarzentruber"], "title": "On Dynamic Programming Theory for Leader-Follower Stochastic Games", "categories": ["cs.GT", "cs.AI"], "comment": "31 pages, 5 figures", "summary": "Leader-follower general-sum stochastic games (LF-GSSGs) model sequential decision-making under asymmetric commitment, where a leader commits to a policy and a follower best responds, yielding a strong Stackelberg equilibrium (SSE) with leader-favourable tie-breaking. This paper introduces a dynamic programming (DP) framework that applies Bellman recursion over credible sets-state abstractions formally representing all rational follower best responses under partial leader commitments-to compute SSEs. We first prove that any LF-GSSG admits a lossless reduction to a Markov decision process (MDP) over credible sets. We further establish that synthesising an optimal memoryless deterministic leader policy is NP-hard, motivating the development of ε-optimal DP algorithms with provable guarantees on leader exploitability. Experiments on standard mixed-motive benchmarks-including security games, resource allocation, and adversarial planning-demonstrate empirical gains in leader value and runtime scalability over state-of-the-art methods.", "AI": {"tldr": "该论文提出了一个用于领导者-追随者随机博弈的动态规划框架，通过可信集的状态抽象来计算强Stackelberg均衡", "motivation": "领导者-追随者一般和随机博弈（LF-GSSGs）建模了非对称承诺下的顺序决策问题，但现有方法在计算效率和最优性保证方面存在不足，需要开发具有理论保证的动态规划算法", "method": "引入基于可信集的动态规划框架，将LF-GSSG无损约简为马尔可夫决策过程，并开发ε-最优DP算法，在可信集上应用Bellman递归", "result": "证明了合成最优无记忆确定性领导者策略是NP难的，但开发的ε-最优DP算法具有可证明的领导者可剥削性保证，在安全博弈、资源分配和对抗规划等基准测试中表现出优于现有方法的领导者价值和运行时可扩展性", "conclusion": "该论文提出的动态规划框架为领导者-追随者随机博弈提供了理论基础和实用算法，通过可信集的状态抽象实现了强Stackelberg均衡的有效计算，解决了非对称承诺下的顺序决策问题"}}
{"id": "2512.05666", "pdf": "https://arxiv.org/pdf/2512.05666", "abs": "https://arxiv.org/abs/2512.05666", "authors": ["Irene Weber"], "title": "Feasibility of AI-Assisted Programming for End-User Development", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "12 pages, 3 figures", "summary": "End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and \"copilots\", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.", "AI": {"tldr": "探讨AI辅助编程在终端用户开发中的可行性，研究非程序员能否通过AI助手成功开发基础Web应用", "motivation": "低代码/无代码平台虽广泛用于终端用户开发，但生成式AI和大型语言模型助手为终端用户提供了直接从自然语言提示生成和优化代码的新可能，有望提供更大灵活性、更广适用性、更快开发速度、更好可重用性并减少供应商锁定", "method": "通过案例研究，让非程序员通过与AI助手交互来开发基础Web应用，分析任务完成情况和参与者反馈", "result": "大多数研究参与者在合理时间内成功完成任务，并支持AI辅助终端用户编码作为终端用户开发的可行方法", "conclusion": "AI辅助终端用户编码是可行的终端用户开发范式，可能在未来补充甚至取代低代码/无代码模型，对实践、未来研究和学术教学具有潜在影响"}}
{"id": "2512.05665", "pdf": "https://arxiv.org/pdf/2512.05665", "abs": "https://arxiv.org/abs/2512.05665", "authors": ["Shuai Dong", "Siyuan Wang", "Xingyu Liu", "Zhongyu Wei"], "title": "Interleaved Latent Visual Reasoning with Selective Perceptual Modeling", "categories": ["cs.CL", "cs.CV"], "comment": "11 pages, 6 figures. Code available at https://github.com/XD111ds/ILVR", "summary": "Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.", "AI": {"tldr": "提出ILVR框架，通过交错潜在视觉表示实现动态状态演化和精确感知建模的统一", "motivation": "现有交错推理范式因重复编码像素密集图像而计算成本高昂，而潜在视觉推理方法要么因过度压缩特征牺牲精确感知建模，要么因静态非交错结构无法建模动态问题", "method": "ILVR框架交错文本生成与潜在视觉表示，使用动量教师模型从辅助图像中选择性蒸馏相关特征到稀疏监督目标，自适应选择机制引导模型自主生成上下文感知的视觉信号", "result": "在多模态推理基准测试中，ILVR显著优于现有方法，有效弥合细粒度感知与顺序多模态推理之间的差距", "conclusion": "ILVR框架成功统一了动态状态演化与精确感知建模，通过交错潜在视觉表示和选择性感知建模解决了现有方法的局限性"}}
{"id": "2512.05663", "pdf": "https://arxiv.org/pdf/2512.05663", "abs": "https://arxiv.org/abs/2512.05663", "authors": ["Johannes Meier", "Jonathan Michel", "Oussema Dhaouadi", "Yung-Hsu Yang", "Christoph Reich", "Zuria Bauer", "Stefan Roth", "Marc Pollefeys", "Jacques Kaiser", "Daniel Cremers"], "title": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection", "categories": ["cs.CV"], "comment": null, "summary": "Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.", "AI": {"tldr": "提出LeAD-M3D方法，通过非对称蒸馏技术实现实时单目3D目标检测，在保持高精度的同时大幅提升推理速度", "motivation": "解决单目3D检测中存在的深度模糊、视角变化和3D推理计算成本高的问题，现有方法要么依赖LiDAR或几何先验，要么牺牲效率换取精度", "method": "采用三个关键技术：1) 非对称增强去噪蒸馏(A2D2)，从干净图像的教师模型向混合噪声的学生模型传递几何知识；2) 3D感知一致性匹配(CM3D)，将3D MGIoU集成到匹配分数中；3) 置信度门控3D推理(CGI3D)，限制昂贵3D回归到高置信度区域", "result": "在KITTI和Waymo数据集上达到最先进精度，在Rope3D数据集上获得最佳car AP，推理速度比之前的高精度方法快3.6倍", "conclusion": "证明了在单目3D检测中，高保真度和实时效率可以同时实现，无需LiDAR、立体视觉或几何假设，设定了新的帕累托前沿"}}
{"id": "2512.05658", "pdf": "https://arxiv.org/pdf/2512.05658", "abs": "https://arxiv.org/abs/2512.05658", "authors": ["Pietro Ferrazzi", "Aitor Soroa", "Rodrigo Agerri"], "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.", "AI": {"tldr": "提出一种基于检索增强生成的方法，为多语言医学问答生成基于事实医学知识的推理轨迹，支持英语、意大利语和西班牙语，旨在提高LLMs在医学QA中的可靠性和透明度。", "motivation": "现有医学问答方法主要关注英语，且依赖通用LLMs的知识蒸馏，这引发了对其医学知识可靠性的担忧。需要开发基于事实医学知识的多语言推理方法，以支持更安全、透明的临床决策支持工具。", "method": "采用检索增强生成方法，从维基百科医学信息中检索相关事实，生成多语言推理轨迹。为MedQA和MedMCQA数据集扩展意大利语和西班牙语版本，生成50万条推理轨迹。通过上下文学习（few-shot）和监督微调两种方式利用这些轨迹。", "result": "在医学QA基准测试中，无论是域内还是域外设置，该方法都提升了性能。使用推理轨迹的模型在8B参数LLMs中达到了最先进的结果。发布了完整资源套件：推理轨迹、翻译的QA数据集、Medical-Wikipedia和微调模型。", "conclusion": "提出的基于事实医学知识的多语言推理轨迹生成方法能够提高LLMs在医学问答中的性能和可靠性，为开发更安全、透明的多语言临床决策支持工具提供了资源支持。"}}
{"id": "2512.05651", "pdf": "https://arxiv.org/pdf/2512.05651", "abs": "https://arxiv.org/abs/2512.05651", "authors": ["Nan Zhong", "Mian Zou", "Yiran Xu", "Zhenxing Qian", "Xinpeng Zhang", "Baoyuan Wu", "Kede Ma"], "title": "Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.", "AI": {"tldr": "提出一种基于相机元数据的自监督AI生成图像检测方法，利用EXIF标签学习数字摄影固有特征，实现跨模型的AI图像检测", "motivation": "现有AI生成图像检测器通常依赖于特定生成模型的内部假设，限制了跨模型适用性。需要一种不依赖生成模型内部信息、能有效检测各种AI生成图像的方法", "method": "通过自监督方式训练特征提取器：在相机拍摄的真实照片上，分类分类EXIF标签（如相机型号、场景类型）并进行序数和连续EXIF标签的成对排序（如焦距、光圈值）。利用这些EXIF诱导特征，首先通过高斯混合模型建模摄影图像分布进行单类检测，然后将学习到的提取器作为强正则化器用于二元分类器，处理空间置乱补丁的高频残差", "result": "在各种生成模型上的广泛实验表明，该方法显著推进了现有技术水平，对野外样本具有强大的泛化能力，并对常见的良性图像扰动具有鲁棒性", "conclusion": "基于相机元数据的自监督方法为AI生成图像检测提供了有效的解决方案，不依赖特定生成模型内部信息，具有更好的跨模型适用性和实际应用价值"}}
{"id": "2512.05638", "pdf": "https://arxiv.org/pdf/2512.05638", "abs": "https://arxiv.org/abs/2512.05638", "authors": ["Suman Sanyal"], "title": "Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \\emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \\emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \\emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.", "AI": {"tldr": "该论文提出了\"模块化喷射\"框架，用于诊断监督学习管道中的\"海市蜃楼\"与可识别性问题，通过分析模块级表示来评估模型内部分解的唯一性。", "motivation": "传统监督学习主要通过预测风险评估模型，但这种方法无法确定模型内部分解是否由数据和评估设计唯一确定。需要一种方法来诊断多个不同模块分解是否产生不可区分的观测结果。", "method": "引入模块化喷射概念，通过估计经验喷射（局部线性响应映射）来描述每个模块对输入微小结构化扰动的反应。提出MoJet算法进行经验喷射估计和海市蜃楼诊断。", "result": "在双模块线性回归管道中证明了喷射可识别性定理：在温和的秩假设和模块级喷射可访问条件下，内部分解是唯一确定的。而仅基于风险的评估则允许大量实现相同输入-输出映射的海市蜃楼分解。", "conclusion": "模块化喷射框架能够诊断监督学习管道中的可识别性问题，区分海市蜃楼与可识别机制，为理解模型内部表示提供了比单纯预测风险更丰富的分析工具。"}}
{"id": "2512.05635", "pdf": "https://arxiv.org/pdf/2512.05635", "abs": "https://arxiv.org/abs/2512.05635", "authors": ["Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Experts-Guided Unbalanced Optimal Transport for ISP Learning from Unpaired and/or Paired Data", "categories": ["cs.CV"], "comment": null, "summary": "Learned Image Signal Processing (ISP) pipelines offer powerful end-to-end performance but are critically dependent on large-scale paired raw-to-sRGB datasets. This reliance on costly-to-acquire paired data remains a significant bottleneck. To address this challenge, we introduce a novel, unsupervised training framework based on Optimal Transport capable of training arbitrary ISP architectures in both unpaired and paired modes. We are the first to successfully apply Unbalanced Optimal Transport (UOT) for this complex, cross-domain translation task. Our UOT-based framework provides robustness to outliers in the target sRGB data, allowing it to discount atypical samples that would be prohibitively costly to map. A key component of our framework is a novel ``committee of expert discriminators,'' a hybrid adversarial regularizer. This committee guides the optimal transport mapping by providing specialized, targeted gradients to correct specific ISP failure modes, including color fidelity, structural artifacts, and frequency-domain realism. To demonstrate the superiority of our approach, we retrained existing state-of-the-art ISP architectures using our paired and unpaired setups. Our experiments show that while our framework, when trained in paired mode, exceeds the performance of the original paired methods across all metrics, our unpaired mode concurrently achieves quantitative and qualitative performance that rivals, and in some cases surpasses, the original paired-trained counterparts. The code and pre-trained models are available at: https://github.com/gosha20777/EGUOT-ISP.git.", "AI": {"tldr": "提出一种基于非平衡最优传输的专家引导框架，用于在无配对和/或有配对数据的情况下学习图像信号处理管道", "motivation": "传统学习型ISP管道严重依赖大规模配对原始图像到sRGB数据集，这些数据获取成本高昂，成为主要瓶颈。需要一种能够在无配对或少量配对数据下训练ISP的方法。", "method": "提出基于非平衡最优传输的无监督训练框架，首次将UOT应用于跨域图像转换任务。框架包含专家委员会判别器作为混合对抗正则化器，专门针对ISP的特定失败模式（色彩保真度、结构伪影、频域真实性）提供针对性梯度指导。", "result": "在有配对模式下，框架在所有指标上超过原始配对方法的性能；在无配对模式下，定量和定性性能与原始配对训练方法相当甚至超越。框架对目标sRGB数据中的异常值具有鲁棒性。", "conclusion": "提出的专家引导非平衡最优传输框架能够有效解决ISP学习中的配对数据依赖问题，在无配对和有配对数据下都能实现优异性能，为ISP训练提供了更灵活和成本效益的解决方案。"}}
{"id": "2512.05619", "pdf": "https://arxiv.org/pdf/2512.05619", "abs": "https://arxiv.org/abs/2512.05619", "authors": ["Menghua Jiang", "Haokai Gao", "Shuhao Chen", "Yin Chen"], "title": "Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting", "categories": ["cs.AI"], "comment": "Accepted by ECAI 2025", "summary": "Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist", "AI": {"tldr": "提出了一种新的子句加权方案DeepDist，针对部分最大可满足性问题(PMS)和加权部分最大可满足性问题(WPMS)设计不同的权重更新策略，并结合新的初始化方法和消元方法，显著提升了随机局部搜索算法的性能。", "motivation": "现有的随机局部搜索算法在处理PMS和WPMS问题时，通常采用统一的子句权重更新策略，未能充分考虑这两种问题类型之间的结构差异，导致性能受限。需要设计能够区分PMS和WPMS特性的加权方案。", "method": "1) 提出新颖的子句加权方案，首次根据PMS和WPMS实例的不同条件分别更新子句权重；2) 引入新的初始化方法，更好地适应两种实例类型的特性；3) 提出消元方法，优先满足单元子句和硬子句，与加权方案形成互补；4) 基于这些方法开发了名为DeepDist的SLS求解器。", "result": "在近期MaxSAT评估的基准测试中，DeepDist超越了最先进的SLS求解器。与TT-Open-WBO-Inc结合的混合求解器甚至超越了MaxSAT评估2024的冠军SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS，证明了方法的有效性。", "conclusion": "通过区分PMS和WPMS的不同特性并设计相应的加权策略，DeepDist显著提升了随机局部搜索算法在最大可满足性问题上的性能，为(W)PMS求解提供了新的有效方法。"}}
{"id": "2512.05613", "pdf": "https://arxiv.org/pdf/2512.05613", "abs": "https://arxiv.org/abs/2512.05613", "authors": ["Pasquale De Marinis", "Pieter M. Blok", "Uzay Kaymak", "Rogier Brussee", "Gennaro Vessio", "Giovanna Castellano"], "title": "DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model", "categories": ["cs.CV"], "comment": null, "summary": "Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.", "AI": {"tldr": "DistillFSS是一个跨域少样本语义分割框架，通过教师-学生蒸馏过程将支持集知识直接嵌入到模型参数中，实现无需测试时支持图像的快速轻量推理。", "motivation": "跨域少样本语义分割面临三大挑战：源域和目标域存在显著分布偏移、标签空间不重叠、支持图像稀缺。传统的基于episode的方法在测试时不可靠且计算量大，需要一种更高效可靠的解决方案。", "method": "采用教师-学生蒸馏框架，将少样本推理能力内化到学生网络中的专用层。通过教师网络驱动专门化，消除测试时对支持图像的需求，结合微调可扩展到大型支持集并显著降低计算开销。", "result": "在涵盖医学影像、工业检测和遥感的新CD-FSS基准测试中，DistillFSS在多项指标上达到或超越最先进基线方法，特别是在多类和多样本场景中表现优异，同时提供显著的效率提升。", "conclusion": "DistillFSS通过将少样本知识蒸馏到轻量分割模型中，解决了跨域少样本分割的关键挑战，实现了高效、快速且可扩展的推理，为实际应用提供了实用解决方案。"}}
{"id": "2512.05610", "pdf": "https://arxiv.org/pdf/2512.05610", "abs": "https://arxiv.org/abs/2512.05610", "authors": ["Juho Korkeala", "Jesse Muhojoki", "Josef Taher", "Klaara Salolahti", "Matti Hyyppä", "Antero Kukko", "Juha Hyyppä"], "title": "NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections", "categories": ["cs.CV"], "comment": "19 pages, 8 figures", "summary": "Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.", "AI": {"tldr": "提出NormalView方法，一种传感器无关的基于投影的深度学习技术，用于从点云数据中分类树种，通过将局部几何信息嵌入二维投影并使用图像分类网络YOLOv11进行处理。", "motivation": "激光扫描是评估森林环境分解的重要工具，移动激光扫描（MLS）在树木级库存方面显示出极高精度。需要开发传感器无关的方法，仅依赖几何信息进行树种分类，以适用于不同扫描设备。", "method": "NormalView方法将局部几何信息（法向量估计）嵌入二维投影，使用图像分类网络YOLOv11处理投影图像。同时研究了多光谱辐射强度信息对分类性能的影响。", "result": "在MLS数据上达到95.5%的整体准确率（宏平均94.8%），在ALS数据上达到91.8%的整体准确率（宏平均79.1%）。发现多扫描仪强度信息对树种分类有益，多光谱ALS数据中使用所有三个通道强度信息的模型表现最佳。", "conclusion": "研究表明，当增强几何信息并与最先进的图像分类骨干网络结合时，基于投影的方法可以获得卓越结果。这些方法是传感器无关的，仅依赖几何信息。作者公开了研究中使用的MLS数据集。"}}
{"id": "2512.05599", "pdf": "https://arxiv.org/pdf/2512.05599", "abs": "https://arxiv.org/abs/2512.05599", "authors": ["Panagiotis Giannikos", "Lampis Papakostas", "Evangelos Katralis", "Panagiotis Mavridis", "George Chryssinas", "Myrto Inglezou", "Nikolaos Panagopoulos", "Antonis Porichis", "Athanasios Mastrogeorgiou", "Panagiotis Chatzakos"], "title": "An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.", "AI": {"tldr": "开发了一个用于废弃电子电气设备(WEEE)中电池自动分拣的集成系统，结合X射线成像、AI目标检测与分割以及Delta机器人操作", "motivation": "电池回收日益重要，但现有系统无法实现跨多种WEEE类型的准确识别和分拣；高能量密度电池的不当处理存在安全隐患", "method": "集成专用X射线双能量成像子系统与先进预处理算法，使用YOLO和U-Net模型进行电池检测与分割，通过智能跟踪和位置估计算法引导Delta机器人进行选择性提取", "result": "系统在NVIDIA Isaac Sim开发的逼真仿真环境和真实设置中进行了验证", "conclusion": "提出了一种能够准确识别和分拣WEEE中电池的自主解决方案，解决了现有系统的局限性"}}
{"id": "2512.05597", "pdf": "https://arxiv.org/pdf/2512.05597", "abs": "https://arxiv.org/abs/2512.05597", "authors": ["Ruihong Yin", "Xuepeng Shi", "Oleksandr Bailo", "Marco Manfredi", "Theo Gevers"], "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.", "AI": {"tldr": "提出Fast SceneScript，一种用于3D场景布局估计的高效结构化语言模型，通过多令牌预测减少自回归迭代次数，显著加速推理", "motivation": "现有的基于语言模型的感知通用方法虽然取得了SOTA结果，但依赖自回归的下一个令牌预测，推理速度慢。需要一种既能保持准确性又能显著加速推理的方法", "method": "采用多令牌预测减少自回归迭代次数；引入自推测解码和置信度引导解码来过滤不可靠令牌；设计参数高效机制减少MTP的参数开销", "result": "在ASE和Structured3D基准测试中，Fast SceneScript每次解码器推理步骤可生成多达9个令牌而不影响准确性，仅增加约7.5%的额外参数", "conclusion": "Fast SceneScript通过多令牌预测和置信度引导解码，在保持3D场景布局估计准确性的同时显著提高了推理效率，为结构化语言模型提供了高效的解决方案"}}
{"id": "2512.05594", "pdf": "https://arxiv.org/pdf/2512.05594", "abs": "https://arxiv.org/abs/2512.05594", "authors": ["Roos M. Bakker", "Daan L. Di Scala", "Maaike H. T. de Boer", "Stephan A. Raaijmakers"], "title": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to Semantic Web Journal, under review", "summary": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.", "AI": {"tldr": "本文提出了OntoAxiom基准测试，系统评估大型语言模型在识别本体公理（如子类、不相交、子属性、定义域、值域等）方面的性能，比较不同提示策略和模型规模的效果。", "motivation": "本体开发需要大量建模和领域专业知识，自动化本体学习过程具有重要意义。随着大型语言模型的发展，需要系统评估LLMs在识别本体公理方面的能力，以支持本体工程师的工作。", "method": "构建包含9个中等规模本体（共17,118个三元组，2,771个公理）的OntoAxiom基准测试，评估12个LLMs在三种few-shot设置和两种提示策略（直接查询所有公理 vs 逐个公理查询）下的性能，重点关注子类、不相交、子属性、定义域和值域五类公理。", "result": "逐个公理（AbA）提示策略比直接方法获得更高的F1分数；不同公理类型识别难度差异显著（如FOAF本体的子类公理得分为0.642，音乐本体仅0.218）；大型模型优于小型模型，但小型模型在资源受限环境下仍有应用价值；整体性能尚不足以完全自动化公理识别。", "conclusion": "LLMs虽然不能完全自动化公理识别，但能为本体工程师提供有价值的候选公理，支持本体的开发和优化。AbA提示策略更有效，性能受公理类型和领域影响，模型规模是重要因素但非唯一决定因素。"}}
{"id": "2512.05593", "pdf": "https://arxiv.org/pdf/2512.05593", "abs": "https://arxiv.org/abs/2512.05593", "authors": ["Rong Wang", "Wei Mao", "Changsheng Lu", "Hongdong Li"], "title": "Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer", "categories": ["cs.CV"], "comment": "Accepted to 3DV 2026", "summary": "We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.", "AI": {"tldr": "提出一种无需蒙皮的高保真布料动画生成方法，通过独立估计顶点位置和法线来分离低频形状和高频皱纹细节，并利用图像传输技术提升动画质量。", "motivation": "现有方法主要依赖线性混合蒙皮来获取低频姿态服装形状，但由于缺乏明确的蒙皮监督，在姿态变化时会产生错位形状，从而破坏高频信号并无法恢复高保真皱纹。", "method": "提出蒙皮自由方法：1) 独立估计顶点位置（低频形状）和顶点法线（高频皱纹细节）；2) 将顶点属性编码为渲染纹理图像，通过2D图像传输实现3D服装变形；3) 利用预训练图像模型恢复皱纹细节；4) 提出多模态融合从传输图像中恢复变形3D服装。", "result": "在各种服装类型上显著提高了动画质量，相比最先进方法恢复了更精细的皱纹细节，且对多样拓扑结构的服装具有更好的可扩展性。", "conclusion": "通过分离频率模态、图像传输和多模态融合的方法，能够有效生成高保真3D服装变形动画，解决了传统蒙皮方法导致的形状错位和皱纹细节损失问题。"}}
{"id": "2512.05592", "pdf": "https://arxiv.org/pdf/2512.05592", "abs": "https://arxiv.org/abs/2512.05592", "authors": ["Katsuhiko Yamamoto", "Koichi Miyazaki", "Shogo Seki"], "title": "The T12 System for AudioMOS Challenge 2025: Audio Aesthetics Score Prediction System Using KAN- and VERSA-based Models", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by IEEE ASRU 2025", "summary": "We propose an audio aesthetics score (AES) prediction system by CyberAgent (AESCA) for AudioMOS Challenge 2025 (AMC25) Track 2. The AESCA comprises a Kolmogorov--Arnold Network (KAN)-based audiobox aesthetics and a predictor from the metric scores using the VERSA toolkit. In the KAN-based predictor, we replaced each multi-layer perceptron layer in the baseline model with a group-rational KAN and trained the model with labeled and pseudo-labeled audio samples. The VERSA-based predictor was designed as a regression model using extreme gradient boosting, incorporating outputs from existing metrics. Both the KAN- and VERSA-based models predicted the AES, including the four evaluation axes. The final AES values were calculated using an ensemble model that combined four KAN-based models and a VERSA-based model. Our proposed T12 system yielded the best correlations among the submitted systems, in three axes at the utterance level, two axes at the system level, and the overall average.", "AI": {"tldr": "提出T12音频美学评分预测系统，用于AudioMOS Challenge 2025 Track 2，结合KAN和VERSA两种模型进行音频美学评分预测", "motivation": "解决音频美学评分预测问题，为AudioMOS Challenge 2025竞赛提供有效的音频美学评估系统", "method": "使用KAN-based模型（将基线MLP层替换为group-rational KAN）和VERSA-based模型（基于XGBoost回归），结合标注和伪标注音频样本训练，最后通过集成模型融合四个KAN模型和一个VERSA模型", "result": "在提交的系统中获得最佳相关性：在utterance级别三个评估轴上，系统级别两个评估轴上，以及整体平均表现最佳", "conclusion": "提出的T12系统在音频美学评分预测任务中表现出色，证明了KAN和VERSA模型组合的有效性"}}
{"id": "2512.05579", "pdf": "https://arxiv.org/pdf/2512.05579", "abs": "https://arxiv.org/abs/2512.05579", "authors": ["Panagiota Moraiti", "Panagiotis Giannikos", "Athanasios Mastrogeorgiou", "Panagiotis Mavridis", "Linghao Zhou", "Panagiotis Chatzakos"], "title": "A Comprehensive Framework for Automated Quality Control in the Automotive Industry", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper presents a cutting-edge robotic inspection solution designed to automate quality control in automotive manufacturing. The system integrates a pair of collaborative robots, each equipped with a high-resolution camera-based vision system to accurately detect and localize surface and thread defects in aluminum high-pressure die casting (HPDC) automotive components. In addition, specialized lenses and optimized lighting configurations are employed to ensure consistent and high-quality image acquisition. The YOLO11n deep learning model is utilized, incorporating additional enhancements such as image slicing, ensemble learning, and bounding-box merging to significantly improve performance and minimize false detections. Furthermore, image processing techniques are applied to estimate the extent of the detected defects. Experimental results demonstrate real-time performance with high accuracy across a wide variety of defects, while minimizing false detections. The proposed solution is promising and highly scalable, providing the flexibility to adapt to various production environments and meet the evolving demands of the automotive industry.", "AI": {"tldr": "本文提出了一个用于汽车行业自动化质量控制的综合框架，通过机器人视觉系统检测铝高压压铸汽车零部件的表面和螺纹缺陷。", "motivation": "汽车制造业需要高效、准确的质量控制方法来检测铝高压压铸零部件的表面和螺纹缺陷，传统人工检测方法效率低、一致性差，需要自动化解决方案来提高检测精度和生产效率。", "method": "系统集成一对协作机器人，配备高分辨率相机视觉系统，使用YOLO11n深度学习模型，结合图像切片、集成学习和边界框合并等增强技术，并应用图像处理技术估计缺陷程度。", "result": "实验结果表明系统具有实时性能，对各种缺陷检测准确率高，同时最小化误检率，系统具有高度可扩展性，能够适应不同生产环境。", "conclusion": "提出的解决方案前景广阔且高度可扩展，为汽车行业提供了灵活适应各种生产环境并满足不断变化需求的自动化质量控制框架。"}}
{"id": "2512.05578", "pdf": "https://arxiv.org/pdf/2512.05578", "abs": "https://arxiv.org/abs/2512.05578", "authors": ["Zheng Sun", "Zhipeng Dong", "Shixiong Wang", "Zhongyi Chu", "Fei Chen"], "title": "A Hyperspectral Imaging Guided Robotic Grasping System", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025", "summary": "Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM.", "AI": {"tldr": "提出一种基于高光谱成像引导的机器人抓取系统，结合PRISM成像机制和SpectralGrasp框架，实现材料识别和抓取策略生成", "motivation": "高光谱成像技术能够精确识别和分析材料，但将其与机器人抓取系统集成存在部署复杂性和成本过高的问题，需要解决这些挑战", "method": "系统包含PRISM（多面反射成像扫描机制）和SpectralGrasp框架。PRISM实现高精度、无失真的高光谱成像，简化系统集成和成本；SpectralGrasp利用高光谱图像的空间和光谱信息生成机器人抓取策略", "result": "在纺织品识别方面显著优于人类性能，在分类成功率方面优于基于RGB的方法，一系列对比实验验证了系统的有效性", "conclusion": "该研究展示了将高光谱成像与机器人抓取系统集成的潜在优势，在复杂动态环境中增强了识别和抓取能力，为相关应用提供了有效解决方案"}}
{"id": "2512.05576", "pdf": "https://arxiv.org/pdf/2512.05576", "abs": "https://arxiv.org/abs/2512.05576", "authors": ["Ting-Ting Xie", "Yixin Zhang"], "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning", "categories": ["cs.AI"], "comment": "2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent", "summary": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.", "AI": {"tldr": "提出CureAgent框架，一种无需训练的执行器-分析师架构，用于解决临床推理中的上下文利用失败问题", "motivation": "现有基于小语言模型的临床代理（如TxAgent）存在\"上下文利用失败\"问题，模型能够成功检索生物医学证据，但无法基于这些信息进行诊断推理", "method": "采用模块化架构，将工具执行的句法精度与临床推理的语义鲁棒性解耦，通过专门的TxAgents（执行器）与长上下文基础模型（分析师）协同工作，并引入分层集成策略", "result": "在CURE-Bench上实现了最先进的性能，无需昂贵的端到端微调，发现了上下文-性能悖论和行动空间维度诅咒等重要扩展见解", "conclusion": "该方法展示了无需训练架构工程的潜力，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础"}}
{"id": "2512.05571", "pdf": "https://arxiv.org/pdf/2512.05571", "abs": "https://arxiv.org/abs/2512.05571", "authors": ["Xingyu Zhang", "Anna Reithmeir", "Fryderyk Kögl", "Rickmer Braren", "Julia A. Schnabel", "Daniel M. Lang"], "title": "MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.", "AI": {"tldr": "提出MedDIFT框架，利用预训练医学扩散模型的多尺度特征作为体素描述符，实现无需训练的3D医学图像对应关系匹配", "motivation": "传统医学图像配准方法依赖局部强度相似性度量，无法捕捉全局语义结构，在低对比度或解剖变异区域容易产生误匹配", "method": "利用预训练潜在医学扩散模型的中间表示，提取多尺度特征融合为体素描述符，通过余弦相似度进行匹配，可选局部搜索先验", "result": "在公开肺部CT数据集上，MedDIFT达到与最先进学习模型UniGradICON相当的精度，优于传统B样条配准方法，且无需任务特定训练", "conclusion": "扩散模型中间层特征能有效编码几何和语义信息，多尺度特征融合和适度噪声可提升性能，为医学图像对应关系提供无需训练的高效解决方案"}}
{"id": "2512.05564", "pdf": "https://arxiv.org/pdf/2512.05564", "abs": "https://arxiv.org/abs/2512.05564", "authors": ["Zijun Wang", "Panwen Hu", "Jing Wang", "Terry Jingchen Zhang", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Zutao Jiang", "Hanhui Li", "Xiaodan Liang"], "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.", "AI": {"tldr": "提出ProPhy框架，通过渐进式物理对齐实现动态世界模拟，解决现有视频生成模型在物理一致性方面的不足", "motivation": "现有视频生成模型在处理大规模或复杂动态时难以产生物理一致的结果，主要因为现有方法对物理提示的响应是各向同性的，忽视了生成内容与局部物理线索之间的细粒度对齐", "method": "提出渐进式物理对齐框架，采用两阶段混合物理专家机制：语义专家从文本描述推断语义级物理原理，细化专家捕获令牌级物理动态；引入物理对齐策略，将视觉语言模型的物理推理能力转移到细化专家中", "result": "在物理感知视频生成基准测试上的广泛实验表明，ProPhy比现有最先进方法产生更真实、动态和物理一致的结果", "conclusion": "ProPhy框架通过渐进式物理对齐和混合物理专家机制，显著提升了视频生成模型的物理一致性，为动态世界模拟提供了更有效的解决方案"}}
{"id": "2512.05557", "pdf": "https://arxiv.org/pdf/2512.05557", "abs": "https://arxiv.org/abs/2512.05557", "authors": ["Xingxi Yin", "Yicheng Li", "Gong Yan", "Chenglin Li", "Jian Zhao", "Cong Huang", "Yue Deng", "Yin Zhang"], "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.", "AI": {"tldr": "提出了一个名为\"2K-Characters-10K-Stories\"的视觉叙事数据集，包含2000个独特风格化角色和10000个插图故事，首次实现了大规模独特身份与解耦控制信号的配对，用于序列身份一致性", "motivation": "现有数据集在精确瞬态属性控制下的序列身份一致性方面存在不足，缺乏足够的保真度，无法将稳定身份与瞬态属性解耦，限制了姿势、表情和场景构成的结构化控制，从而制约了可靠的序列合成", "method": "采用人机协同管道(Human-in-the-Loop pipeline)，利用专家验证的角色模板和LLM引导的叙事规划生成高度对齐的结构化数据；提出解耦控制方案分离持久身份与瞬态属性；通过质量门控循环集成MMLM评估、自动提示调优和局部图像编辑来强制像素级一致性", "result": "在该数据集上微调的模型在生成视觉叙事方面达到了与闭源模型相当的性能，证明了数据集的有效性", "conclusion": "提出的2K-Characters-10K-Stories数据集解决了可控视觉叙事中序列身份一致性的长期挑战，通过解耦控制和质量门控机制实现了精确的属性控制和像素级一致性，为结构化视觉叙事生成提供了高质量基准"}}
{"id": "2512.05556", "pdf": "https://arxiv.org/pdf/2512.05556", "abs": "https://arxiv.org/abs/2512.05556", "authors": ["Sanjeev Shrestha", "Rahul Dubey", "Hui Liu"], "title": "Improving Local Fidelity Through Sampling and Modeling Nonlinearity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.", "AI": {"tldr": "提出一种改进LIME解释方法的新技术，通过采样和建模非线性关系来提高局部保真度", "motivation": "随着黑盒机器学习模型日益复杂且在关键领域应用增多，需要提供可靠的预测解释。LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确", "method": "使用多元自适应回归样条(MARS)建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布采样，而不是像LIME那样重新加权样本", "result": "在三个UCI数据集上对不同分类器和不同核宽度进行评估，结果显示该方法相比基线产生更忠实的解释，平均降低37%的均方根误差，显著提高局部保真度", "conclusion": "提出的方法通过结合MARS建模非线性关系和N-ball采样技术，能够生成高保真度的解释，有效解决了LIME方法在捕捉非线性关系方面的局限性"}}
{"id": "2512.05546", "pdf": "https://arxiv.org/pdf/2512.05546", "abs": "https://arxiv.org/abs/2512.05546", "authors": ["Weijue Bu", "Guan Yuan", "Guixian Zhang"], "title": "Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.", "AI": {"tldr": "提出Conscious Gaze (CG-VLM)框架，通过游戏论解释性方法在推理时自适应调整注意力机制，缓解视觉语言模型中的幻觉问题", "motivation": "现有解码策略仅在输出层干预，无法纠正内部推理漂移；基于启发式头部抑制或全局导向向量的内部控制方法缺乏理论依据，需要更精确、有理论基础的幻觉缓解方法", "method": "基于Harsanyi交互构建认知需求传感器估计视觉-文本协同，识别需要视觉接地的时刻；通过聚焦共识诱导模块在注意力漂移前选择性重定向中层注意力到视觉标记", "result": "在POPE和CHAIR基准测试中，在InstructBLIP、LLaVA、Qwen-VL和mPLUG等模型上取得最先进结果，同时保持通用能力", "conclusion": "标记级感知能够实现精确、上下文感知的干预，而不会损害基础知识，为视觉语言模型的幻觉缓解提供了有理论依据的训练免费解决方案"}}
{"id": "2512.05542", "pdf": "https://arxiv.org/pdf/2512.05542", "abs": "https://arxiv.org/abs/2512.05542", "authors": ["Jonathan Geuter", "Gregor Kornhardt"], "title": "RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 3 figures. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundations of Reasoning in Language Models", "summary": "Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\\{m_i\\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.", "AI": {"tldr": "提出RoBoN方法，这是一种在推理时利用多个LLM进行路由选择的最佳n策略，通过在线路由机制在多个模型间分配生成任务，无需额外训练即可提升性能。", "motivation": "传统的最佳n方法通常依赖单一模型生成响应，但研究表明不同LLM在不同任务上具有互补优势。如何在不增加训练成本的情况下，利用多个模型的多样性来提升推理性能成为研究动机。", "method": "RoBoN采用顺序路由机制，基于奖励模型和响应一致性信号，在多个LLM之间逐个路由生成任务。该方法保持计算量不变，无需额外训练，可与任何插件式奖励模型配合使用。", "result": "在多个推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）上，RoBoN在较大的n值时始终优于传统的单一模型最佳n方法，绝对准确率提升高达3.4%，同时也优于均匀多模型组合基线。", "conclusion": "研究表明，在推理时利用模型间的多样性可以显著提升最佳n性能，超过任何单一组成模型的表现。RoBoN提供了一种简单、无需训练的路径，可在测试时使用多个LLM进行扩展。"}}
{"id": "2512.05539", "pdf": "https://arxiv.org/pdf/2512.05539", "abs": "https://arxiv.org/abs/2512.05539", "authors": ["Swantje Mahncke", "Malte Ott"], "title": "Ideal Observer for Segmentation of Dead Leaves Images", "categories": ["cs.CV", "math.ST", "stat.ME"], "comment": "41 pages, 16 figures", "summary": "The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider \"dead leaves\" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects (\"leaves\") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.", "AI": {"tldr": "该论文提出了一种基于贝叶斯理想观察者的死叶图像分割方法，用于计算像素分割的后验概率，为人类视觉和计算机视觉算法提供性能上限基准。", "motivation": "人类视觉环境由空间分布的不同表面组成，物体之间的遮挡关系是视觉场景的重要特征。死叶模型能够模拟这种遮挡关系，但需要一种理论方法来评估分割决策的性能上限，以便与人类视觉和计算机视觉算法进行比较。", "method": "基于先前工作，推导出贝叶斯理想观察者模型，用于计算给定像素集的分割后验概率。该方法扩展了先前工作，提供了后验概率计算的逐步解释，并描述了实际应用该计算的决定因素。", "result": "建立了死叶图像模型及其关联的理想观察者框架，可用于研究有限像素集的分割决策，为分割性能提供了原则性的上限基准。", "conclusion": "死叶图像模型和贝叶斯理想观察者为视觉分割研究提供了理论基础和性能评估框架，可用于比较人类视觉和计算机视觉算法的分割性能，确定其与理论最优性能的差距。"}}
{"id": "2512.05536", "pdf": "https://arxiv.org/pdf/2512.05536", "abs": "https://arxiv.org/abs/2512.05536", "authors": ["Johannes Ellemose", "Niklas Elmqvist"], "title": "Eye of the Beholder: Towards Measuring Visualization Complexity", "categories": ["cs.HC"], "comment": null, "summary": "Constructing expressive and legible visualizations is a key activity for visualization designers. While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/", "AI": {"tldr": "研究如何测量可视化图表的感知复杂度，通过众包收集人类评分，并评估三种估计方法的效果", "motivation": "尽管存在许多可视化设计指南，但关于具体图形特征如何影响感知视觉复杂度的研究仍然有限，需要系统性地测量和理解可视化复杂度", "method": "1) 通过众包研究收集人类对多样化可视化的复杂度评分；2) 评估三种估计方法：图像分析指标、基于手动编码特征的多线性回归、使用大语言模型（GPT-4o mini）的自动特征提取", "result": "图像复杂度指标与人类感知的可视化复杂度无相关性；手动特征编码能产生合理的预测模型但需要大量努力；零样本LLM在评分复杂度和提取相关特征方面表现出强大能力", "conclusion": "可视化复杂度确实\"因人而异\"，但可以通过零样本LLM提示有效近似，为评估可视化复杂度提供了可扩展的方法"}}
{"id": "2512.05534", "pdf": "https://arxiv.org/pdf/2512.05534", "abs": "https://arxiv.org/abs/2512.05534", "authors": ["Yiming Tang", "Harshvardhan Saini", "Yizhen Liao", "Dianbo Liu"], "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.", "AI": {"tldr": "本文为稀疏字典学习（SDL）在机制可解释性领域建立了首个统一的理论框架，分析了其优化景观，并解释了经验观察到的现象。", "motivation": "随着AI模型能力增强，理解其内部表示和处理机制变得至关重要。现有稀疏字典学习方法（如稀疏自编码器、转码器、交叉编码器）在经验上成功但缺乏理论理解，现有理论仅限于带权重约束的稀疏自编码器，需要为更广泛的SDL方法提供理论基础。", "method": "开发了首个统一的理论框架，将SDL视为统一的优化问题，展示了不同方法如何实例化该框架，并对优化景观进行了严格分析。设计了受控实验验证理论结果。", "result": "首次为经验观察到的现象提供了理论解释，包括特征吸收、死神经元和神经元重采样技术。建立了SDL方法的理论基础，填补了现有理论空白。", "conclusion": "该工作为稀疏字典学习在机制可解释性领域提供了首个统一的理论基础，不仅解释了现有经验现象，还为未来SDL方法的设计和分析提供了理论指导。"}}
{"id": "2512.05530", "pdf": "https://arxiv.org/pdf/2512.05530", "abs": "https://arxiv.org/abs/2512.05530", "authors": ["Chuang Yu", "Jinmiao Zhao", "Mingxuan Zhao", "Yunpeng Liu", "Xiujun Shu", "Yuanhao Feng", "Bo Wang", "Xiangyu Yue"], "title": "MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models", "categories": ["cs.AI"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of \"Understand -> Rethink -> Correct\", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND", "AI": {"tldr": "提出MIND推理框架，通过\"理解->反思->纠正\"的类人认知能力，实现从被动模仿推理到主动判别推理的范式演进，提升多模态大模型在复杂场景下的推理能力。", "motivation": "当前多模态大语言模型在推理任务中存在多理由语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题，需要更强大的认知推理能力。", "method": "提出RAD范式自动扩展数据集生成多样理由；设计P2CL两阶段校正学习策略，第一阶段增强多理由正向学习，第二阶段实现主动逻辑判别和纠正；引入MCA优化策略缓解多理由语义空间表示纠缠。", "result": "在涵盖科学、常识和数学场景的多个公开数据集上实现了最先进的性能表现，为推进MLLMs向更高层次认知智能提供了新视角。", "conclusion": "MIND框架通过类人认知推理范式，显著提升了多模态大模型在复杂场景下的推理能力，为认知智能发展提供了有效路径。"}}
{"id": "2512.05529", "pdf": "https://arxiv.org/pdf/2512.05529", "abs": "https://arxiv.org/abs/2512.05529", "authors": ["Kunyi Yang", "Qingyu Wang", "Cheng Yuan", "Yutong Ban"], "title": "See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors", "categories": ["cs.CV", "cs.AI"], "comment": "The first two authors contributed equally", "summary": "Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.", "AI": {"tldr": "提出DepSeg框架，利用单目深度作为几何先验，结合预训练视觉基础模型实现免训练的手术场景分割", "motivation": "腹腔镜场景的像素级分割对计算机辅助手术至关重要，但密集标注成本高昂，难以规模化应用", "method": "使用预训练单目深度估计网络获取相对深度图，提出深度引导点提示，SAM2将其转换为类别无关掩码，通过预训练视觉特征池化和模板匹配进行分类", "result": "在CholecSeg8k数据集上，DepSeg相比直接SAM2自动分割基线显著提升（35.9% vs. 14.7% mIoU），即使仅使用10-20%的对象模板仍保持竞争力", "conclusion": "深度引导提示和基于模板的分类提供了一种标注高效的分割方法，展示了免训练框架在手术场景分割中的潜力"}}
{"id": "2512.05528", "pdf": "https://arxiv.org/pdf/2512.05528", "abs": "https://arxiv.org/abs/2512.05528", "authors": ["Taketo Akama", "Zhuohao Zhang", "Tsukasa Nagashima", "Takagi Yutaka", "Shun Minamikawa", "Natalia Polouliakh"], "title": "Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "comment": null, "summary": "Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.", "AI": {"tldr": "解码在生态有效的音乐聆听中对音乐元素的选择性听觉注意力", "motivation": "音乐作为人类文化和情感表达的重要形式，目前缺乏客观工具来量化听众在自然聆听体验中的注意力和感知焦点。视觉艺术已有眼动追踪研究专家与新手的差异，但听觉艺术的类似方法尚未充分发展。", "method": "使用自然主义、工作室制作的歌曲和仅四个电极的轻量级消费级EEG设备，分析真实世界般的音乐聆听过程中的神经反应，测试在最小化参与者负担和保持音乐体验真实性的条件下解码是否可行。", "result": "研究发现音乐注意力不仅可以在新歌曲中解码，还可以跨新受试者解码，与现有方法相比在测试条件下表现出性能改进。消费级设备可以可靠地捕获信号，音乐中的神经解码在现实世界环境中是可行的。", "conclusion": "这项研究为教育、个性化音乐技术和治疗干预等应用铺平了道路，证明了在生态有效的音乐聆听环境中解码选择性听觉注意力的可行性。"}}
{"id": "2512.05524", "pdf": "https://arxiv.org/pdf/2512.05524", "abs": "https://arxiv.org/abs/2512.05524", "authors": ["Chinthani Sugandhika", "Chen Li", "Deepu Rajan", "Basura Fernando"], "title": "VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.", "AI": {"tldr": "提出VOST-SGG框架，将视觉语言模型(VLM)的常识推理能力集成到时空场景图生成(ST-SGG)中，解决现有单阶段模型语义信息不足和模态单一的问题。", "motivation": "现有DETR风格的单阶段ST-SGG模型存在两个关键限制：1) 基于注意力的可学习查询是语义无信息且实例无关初始化的；2) 仅依赖单模态视觉特征进行谓词分类。需要集成VLM的常识推理能力来提升ST-SGG性能。", "method": "提出双源查询初始化策略，分离\"关注什么\"和\"在哪里关注\"，实现语义基础的what-where推理；构建多模态特征库，融合来自VLM的视觉、文本和空间线索，改进谓词分类。", "result": "在Action Genome数据集上的广泛实验表明，该方法实现了最先进的性能，验证了集成VLM辅助语义先验和多模态特征对ST-SGG的有效性。", "conclusion": "VOST-SGG框架成功将VLM的常识推理能力集成到ST-SGG中，通过双源查询初始化和多模态特征融合解决了现有模型的局限性，为下游推理任务提供了更好的可解释表示。"}}
{"id": "2512.05519", "pdf": "https://arxiv.org/pdf/2512.05519", "abs": "https://arxiv.org/abs/2512.05519", "authors": ["Bohui Shen", "Shrikar Bhatta", "Alex Ireebanije", "Zexuan Liu", "Abhinav Choudhry", "Ece Gumusel", "Kyrie Zhixuan Zhou"], "title": "User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As AI-generated video platforms rapidly advance, ethical challenges such as copyright infringement emerge. This study examines how users make sense of AI-generated videos on OpenAI's Sora by conducting a qualitative content analysis of user comments. Through a thematic analysis, we identified four dynamics that characterize how users negotiate authenticity, authorship, and platform governance on Sora. First, users acted as critical evaluators of realism, assessing micro-details such as lighting, shadows, fluid motion, and physics to judge whether AI-generated scenes could plausibly exist. Second, users increasingly shifted from passive viewers to active creators, expressing curiosity about prompts, techniques, and creative processes. Text prompts were perceived as intellectual property, generating concerns about plagiarism and remixing norms. Third, users reported blurred boundaries between real and synthetic media, worried about misinformation, and even questioned the authenticity of other commenters, suspecting bot-generated engagement. Fourth, users contested platform governance: some perceived moderation as inconsistent or opaque, while others shared tactics for evading prompt censorship through misspellings, alternative phrasing, emojis, or other languages. Despite this, many users also enforced ethical norms by discouraging the misuse of real people's images or disrespectful content. Together, these patterns highlighted how AI-mediated platforms complicate notions of reality, creativity, and rule-making in emerging digital ecosystems. Based on the findings, we discuss governance challenges in Sora and how user negotiations inform future platform governance.", "AI": {"tldr": "研究通过分析Sora平台用户评论，探讨用户如何在AI生成视频平台上协商真实性、所有权和治理问题", "motivation": "随着AI生成视频平台的快速发展，出现了版权侵权等伦理挑战，需要理解用户如何理解和协商AI生成内容", "method": "采用质性内容分析方法，对OpenAI Sora平台的用户评论进行主题分析", "result": "识别出四个关键动态：1)用户作为真实性的批判性评估者；2)从被动观看者转向主动创作者；3)真实与合成媒体边界模糊；4)用户对平台治理的争议", "conclusion": "AI中介平台使现实、创造力和规则制定的概念复杂化，用户协商为未来平台治理提供了重要启示"}}
{"id": "2512.05518", "pdf": "https://arxiv.org/pdf/2512.05518", "abs": "https://arxiv.org/abs/2512.05518", "authors": ["Jason Vega", "Gagandeep Singh"], "title": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.", "AI": {"tldr": "提出一种名为PRESTO的新方法，通过匹配目标分布的token排名而非概率来实现真正的深度安全对齐，有效防御Rank-Assisted Prefilling攻击", "motivation": "现有基于数据增强的监督微调防御方法虽然声称实现\"深度\"安全对齐，但存在根本缺陷：当目标分布熵较低时，模型可以通过将大量概率质量转移到少数拒绝token而忽略有害token的高排名来\"博弈\"SFT目标，导致Rank-Assisted Prefilling攻击仍然有效", "method": "提出PRESTO方法：基于匹配token排名的视角，通过正则化对有害预填充token的关注来实现深度安全对齐。具体来说，在数据增强防御基础上添加注意力停止机制，防止模型过度关注有害预填充内容", "result": "在三个流行的开源LLM上，添加PRESTO后，在RAP攻击下的平均StrongREJECT分数提高了高达4.7倍，同时对模型实用性影响较小", "conclusion": "通过匹配token排名而非概率可以实现真正的深度安全对齐，PRESTO方法有效解决了现有防御的漏洞，为LLM安全对齐提供了新的视角和实用解决方案"}}
{"id": "2512.05515", "pdf": "https://arxiv.org/pdf/2512.05515", "abs": "https://arxiv.org/abs/2512.05515", "authors": ["Yuhua Wen", "Qifei Li", "Yingying Zhou", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "title": "DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2025", "summary": "Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.", "AI": {"tldr": "提出DashFusion框架，通过双流对齐和分层瓶颈融合解决多模态情感分析中的对齐与融合问题", "motivation": "多模态情感分析面临对齐和融合两大挑战：对齐需要同步跨模态的时序和语义信息，融合需要整合对齐后的特征。现有方法往往孤立处理这两个问题，导致性能和效率受限", "method": "1. 双流对齐模块：通过时序对齐（跨模态注意力建立帧级对应）和语义对齐（对比学习确保特征空间一致性）同步多模态特征；2. 监督对比学习利用标签信息细化模态特征；3. 分层瓶颈融合通过压缩瓶颈令牌逐步整合多模态信息", "result": "在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上实现了最先进的性能，消融研究证实了对齐和融合技术的有效性", "conclusion": "DashFusion通过统一的框架同时解决对齐和融合问题，在性能和计算效率之间取得了良好平衡，为多模态情感分析提供了有效的解决方案"}}
{"id": "2512.05513", "pdf": "https://arxiv.org/pdf/2512.05513", "abs": "https://arxiv.org/abs/2512.05513", "authors": ["Chinthani Sugandhika", "Chen Li", "Deepu Rajan", "Basura Fernando"], "title": "Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to \"show what they know\" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.", "AI": {"tldr": "提出Know-Show基准测试，用于评估视频语言模型在时空基础推理方面的能力，即模型在推理动作及其语义的同时，将其推断基于视觉和时间证据的能力。", "motivation": "当前大型视频语言模型在多模态理解方面取得了显著进展，但其推理在空间和时间维度上的基础仍然薄弱。需要一个新的基准来统一评估推理和定位能力，以揭示现有模型的局限性。", "method": "构建Know-Show基准，包含来自Charades、Action Genome和Ego4D的2.5K个人工编写问题，涵盖空间（人物、物体、人物-物体、手-物体）和时间维度五个互补场景。同时提出GRAM方法，通过基于注意力的视频令牌选择和显式时间戳编码来增强视频语言模型的细粒度基础能力。", "result": "实验显示现有视频语言模型（Qwen、VideoLLaVA、GPT-4o、Gemini等）在\"展示所知\"和\"知道所展示\"方面存在显著差距，特别是在细粒度的手-物体交互方面。GRAM方法能有效提升模型的时空基础推理能力。", "conclusion": "Know-Show为评估视频语言理解中的基础推理建立了统一标准，揭示了当前模型的局限性，并为开发可解释和可靠的多模态推理系统提供了见解。GRAM作为一种无需训练的插件方法，能有效增强模型的细粒度基础能力。"}}
{"id": "2512.05511", "pdf": "https://arxiv.org/pdf/2512.05511", "abs": "https://arxiv.org/abs/2512.05511", "authors": ["Chuang Yu", "Jinmiao Zhao", "Yunpeng Liu", "Yaokun Li", "Xiujun Shu", "Yuanhao Feng", "Bo Wang", "Yimian Dai", "Xiangyu Yue"], "title": "Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm", "categories": ["cs.CV"], "comment": null, "summary": "While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework", "AI": {"tldr": "提出一种基于视觉基础模型的红外小目标检测新范式，通过语义对齐调制融合和协作优化隐式自蒸馏策略，在保持高效推理的同时显著提升检测精度。", "motivation": "虽然大规模视觉基础模型在多个视觉领域展现出强大的泛化能力，但其在单帧红外小目标检测任务中的潜力尚未被充分探索。现有方法存在精度不足、评估体系碎片化等问题。", "method": "提出基础驱动高效范式，包含语义对齐调制融合模块实现全局语义先验与任务特征的动态对齐和深度融合，以及协作优化隐式自蒸馏策略通过参数共享和同步反向传播实现隐式语义迁移。", "result": "在多个公开数据集上实现了最先进的性能，提出的整体红外小目标评估指标为公平模型比较提供了稳定全面的基础。", "conclusion": "该工作首次系统地将视觉基础模型引入红外小目标检测任务，提出的高效范式能够无缝适配现有编码器-解码器方法，在不增加推理开销的情况下显著提升检测精度。"}}
{"id": "2512.05508", "pdf": "https://arxiv.org/pdf/2512.05508", "abs": "https://arxiv.org/abs/2512.05508", "authors": ["Yash Choudhary", "Preeti Rao", "Pushpak Bhattacharyya"], "title": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.", "AI": {"tldr": "该论文提出了一种利用歌词特征预测音乐流行度的新方法，通过LLM提取歌词的高维嵌入表示，并结合音频、社交元数据进行多模态预测。", "motivation": "现有音乐流行度预测研究主要关注音频特征、社交元数据或模型架构，而歌词的作用被低估。作者认为歌词包含丰富的语义、句法和序列信息，对预测音乐流行度有重要价值。", "method": "开发了自动化流水线，使用LLM提取歌词的高维嵌入表示（LyricsAENet），然后构建多模态架构HitMusicLyricNet，整合音频、歌词和社交元数据特征，预测0-100范围内的流行度分数。", "result": "在包含超过10万首曲目的SpotGenTrack数据集上，该方法在MAE和MSE指标上分别比现有基线方法提升了9%和20%。消融实验证实性能提升主要来自LLM驱动的歌词特征流水线。", "conclusion": "歌词在音乐流行度预测中具有重要作用，通过LLM提取的密集歌词表示能够显著提升预测性能，为音乐产业提供了新的技术方案。"}}
{"id": "2512.05506", "pdf": "https://arxiv.org/pdf/2512.05506", "abs": "https://arxiv.org/abs/2512.05506", "authors": ["Junho Myung", "Hyunseung Lim", "Hana Oh", "Hyoungwook Jin", "Nayeon Kang", "So-Yeon Ahn", "Hwajung Hong", "Alice Oh", "Juho Kim"], "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms", "categories": ["cs.HC"], "comment": "Under Review", "summary": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools.", "AI": {"tldr": "研究LLM在K-12 EFL课堂中作为实时写作支架的成效与局限，重点关注课堂约束条件对学生互动和教学动态的影响", "motivation": "LLM作为英语写作支架工具具有潜力，但在真实K-12课堂环境中的有效性尚未充分探索，需要考虑课堂约束条件如学生能力差异和时间限制等因素", "method": "在韩国中学八年级英语课堂进行为期六周的部署研究，涉及157名学生，观察LLM作为实时学习支持工具的实际应用效果", "result": "支架方法提高了学生语法正确性，但降低了低水平学生的动机并增加了系统依赖；观察到课堂动态挑战，外向学生主导教师注意力，系统辅助使教师难以识别困难学生", "conclusion": "基于研究发现，讨论了将LLM整合到实时写作课堂作为包容性教育工具的设计指南，强调需要更平衡的支架策略"}}
{"id": "2512.05495", "pdf": "https://arxiv.org/pdf/2512.05495", "abs": "https://arxiv.org/abs/2512.05495", "authors": ["Ratnangshu Das", "Ahan Basu", "Christos Verginis", "Pushpak Jagtap"], "title": "Spatiotemporal Tubes for Differential Drive Robots with Model Uncertainty", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for differential-drive mobile robots with dynamic uncertainties and external disturbances, guaranteeing the satisfaction of Temporal Reach-Avoid-Stay (T-RAS) specifications. The approach employs circular STT, characterized by smoothly time-varying center and radius, to define dynamic safe corridors that guide the robot from the start region to the goal while avoiding obstacles. In particular, we first develop a sampling-based synthesis algorithm to construct a feasible STT that satisfies the prescribed timing and safety constraints with formal guarantees. To ensure that the robot remains confined within this tube, we then design analytically a closed-form, approximation-free control law. The resulting controller is computationally efficient, robust to disturbances and {model uncertainties}, and requires no model approximations or online optimization. The proposed framework is validated through simulation studies on a differential-drive robot and benchmarked against state-of-the-art methods, demonstrating superior robustness, accuracy, and computational efficiency.", "AI": {"tldr": "本文提出了一种基于时空管道的控制框架，用于具有动态不确定性和外部扰动的差速驱动机器人，保证满足时间可达-避障-停留规范。", "motivation": "差速驱动机器人在动态不确定性和外部扰动下难以同时满足时间约束和安全约束，现有方法通常需要模型近似或在线优化，计算效率低且缺乏鲁棒性保证。", "method": "采用圆形时空管道定义动态安全走廊，首先开发基于采样的合成算法构建满足时间和安全约束的可行管道，然后设计闭式无近似的控制律确保机器人保持在管道内。", "result": "提出的控制器计算效率高，对扰动和模型不确定性具有鲁棒性，无需模型近似或在线优化，仿真验证表明在鲁棒性、精度和计算效率方面优于现有方法。", "conclusion": "该框架为具有不确定性的差速驱动机器人提供了一种保证时间可达-避障-停留规范的有效解决方案，具有形式化保证和实际应用价值。"}}
{"id": "2512.05494", "pdf": "https://arxiv.org/pdf/2512.05494", "abs": "https://arxiv.org/abs/2512.05494", "authors": ["Fan Zhang", "Zhiwei Gu", "Hua Wang"], "title": "Decoding with Structured Awareness: Integrating Directional, Frequency-Spatial, and Structural Attention for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "To address the limitations of Transformer decoders in capturing edge details, recognizing local textures and modeling spatial continuity, this paper proposes a novel decoder framework specifically designed for medical image segmentation, comprising three core modules. First, the Adaptive Cross-Fusion Attention (ACFA) module integrates channel feature enhancement with spatial attention mechanisms and introduces learnable guidance in three directions (planar, horizontal, and vertical) to enhance responsiveness to key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module fuses features from Spatial, Fourier and Wavelet domains, achieving joint frequency-spatial representation that strengthens global dependency and structural modeling while preserving local information such as edges and textures, making it particularly effective in complex and blurred boundary scenarios. Finally, the Structural-aware Multi-scale Masking Module (SMMM) optimizes the skip connections between encoder and decoder by leveraging multi-scale context and structural saliency filtering, effectively reducing feature redundancy and improving semantic interaction quality. Working synergistically, these modules not only address the shortcomings of traditional decoders but also significantly enhance performance in high-precision tasks such as tumor segmentation and organ boundary extraction, improving both segmentation accuracy and model generalization. Experimental results demonstrate that this framework provides an efficient and practical solution for medical image segmentation.", "AI": {"tldr": "提出一种用于医学图像分割的新型解码器框架，通过三个核心模块解决Transformer解码器在边缘细节、局部纹理和空间连续性建模方面的局限性。", "motivation": "传统Transformer解码器在医学图像分割中存在三个主要问题：难以捕捉边缘细节、识别局部纹理以及建模空间连续性，这影响了高精度分割任务（如肿瘤分割和器官边界提取）的性能。", "method": "提出包含三个核心模块的解码器框架：1) 自适应交叉融合注意力模块（ACFA），集成通道特征增强与空间注意力机制，引入三个方向（平面、水平和垂直）的可学习引导；2) 三重特征融合注意力模块（TFFA），融合空间、傅里叶和小波域特征，实现联合频率-空间表示；3) 结构感知多尺度掩码模块（SMMM），利用多尺度上下文和结构显著性过滤优化编码器-解码器跳跃连接。", "result": "实验结果表明，该框架显著提升了医学图像分割的准确性，特别是在复杂和模糊边界场景中表现优异，提高了分割精度和模型泛化能力，为肿瘤分割和器官边界提取等高精度任务提供了高效实用的解决方案。", "conclusion": "提出的解码器框架通过三个协同工作的模块有效解决了传统解码器的局限性，在医学图像分割任务中实现了性能的显著提升，特别是在边缘细节保持、局部纹理识别和空间连续性建模方面表现出色。"}}
{"id": "2512.05492", "pdf": "https://arxiv.org/pdf/2512.05492", "abs": "https://arxiv.org/abs/2512.05492", "authors": ["Qi Zhu", "Jingyi Zhang", "Naishan Zheng", "Wei Yu", "Jinghao Zhang", "Deyi Ji", "Feng Zhao"], "title": "WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field", "categories": ["cs.CV"], "comment": null, "summary": "Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.", "AI": {"tldr": "该论文提出了一种名为WaterWave的方法，通过基于小波的时间一致性场将水下图像增强技术扩展到视频流，解决单帧增强方法缺乏时间一致性的问题。", "motivation": "由于水下成像复杂，获取成对的水下视频数据非常困难。现有视频增强方法通常直接逐帧应用单图像增强模型，但缺乏时间一致性，导致视频不自然。论文从局部时间频率角度重新思考自然视频中的时间流形，观察到动态场景中存在时间一致性先验。", "method": "提出了一种在基于小波的时间一致性场中进行增强视频信号的隐式表示方法。在时间一致性先验约束下，逐步过滤和衰减不一致分量，同时保留运动细节和场景。设计了水下流校正模块，考虑水下场景的传输特性来校正估计的光流，以更准确地表示时间频带。", "result": "实验表明，WaterWave显著提高了使用单图像水下增强方法生成的视频质量。在UOSTrack和MAT等下游水下跟踪任务中表现出巨大潜力，精确度分别比原始视频提高了19.7%和9.7%。", "conclusion": "WaterWave通过基于小波的时间一致性场，成功地将水下图像增强扩展到视频流，解决了时间一致性问题，为水下视频增强和下游跟踪任务提供了有效的解决方案。"}}
{"id": "2512.05482", "pdf": "https://arxiv.org/pdf/2512.05482", "abs": "https://arxiv.org/abs/2512.05482", "authors": ["Mai Tsujimoto"], "title": "Concept-based Explainable Data Mining with VLM for 3D Detection", "categories": ["cs.CV"], "comment": "28 pages including appendix. Code: https://github.com/mm1129/concept_based_rare_detector_2025", "summary": "Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.", "AI": {"tldr": "提出一种利用2D视觉语言模型挖掘稀有对象以增强3D目标检测性能的跨模态框架", "motivation": "自动驾驶系统中稀有对象检测具有挑战性，仅依赖点云数据效果有限；视觉语言模型在图像理解方面表现出色，但其通过智能数据挖掘增强3D目标检测的潜力尚未充分探索", "method": "提出跨模态框架，结合目标检测、语义特征提取、降维和多方面异常检测技术，使用隔离森林和t-SNE异常检测方法配合概念过滤，系统识别驾驶场景中的稀有关键对象", "result": "在nuScenes数据集上的实验表明，该概念引导的数据挖掘策略仅使用少量训练数据就能提升3D目标检测性能，特别是在拖车和自行车等挑战性类别上相比随机数据有明显改进", "conclusion": "该方法能有效提取和标注目标稀有对象概念，显著减少标注负担并专注于最有价值的训练样本，对安全关键自动驾驶系统中数据集的高效管理具有重要意义"}}
{"id": "2512.05481", "pdf": "https://arxiv.org/pdf/2512.05481", "abs": "https://arxiv.org/abs/2512.05481", "authors": ["Jialin Li", "Yiwei Ren", "Kai Pan", "Dong Wei", "Pujin Cheng", "Xian Wu", "Xiaoying Tang"], "title": "UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.", "AI": {"tldr": "提出UniFS模型，这是一个统一的多对比度MRI重建框架，能够处理多种k空间欠采样模式而无需重新训练", "motivation": "现有MCMR方法通常需要为每种k空间欠采样模式单独训练模型，且往往只关注空间信息而忽略频率特征，限制了实际应用", "method": "UniFS包含三个核心模块：跨模态频率融合模块、基于自适应掩码的提示学习模块和双分支互补细化模块，通过频率-空间融合提取域不变特征", "result": "在BraTS和HCP数据集上，针对多种k空间欠采样模式和加速因子（包括未见过的模式）进行评估，UniFS在多个场景下达到最先进性能", "conclusion": "UniFS通过统一的频率-空间融合框架，显著提高了多对比度MRI重建的泛化能力，能够处理多种欠采样模式而无需重新训练"}}
{"id": "2512.05478", "pdf": "https://arxiv.org/pdf/2512.05478", "abs": "https://arxiv.org/abs/2512.05478", "authors": ["Jingyuan Yang", "Zihuan Bai", "Hui Huang"], "title": "EmoStyle: Emotion-Driven Image Stylization", "categories": ["cs.CV"], "comment": null, "summary": "Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.", "AI": {"tldr": "提出EmoStyle框架，用于情感驱动的图像风格化任务，通过构建数据集和开发情感-内容推理器来解决现有方法忽视情感影响的问题", "motivation": "现有图像风格化方法主要关注视觉外观的转换，但忽视了艺术风格所承载的情感影响。艺术长期以来都是表达情感的重要媒介，因此需要开发能够唤起特定情感的风格化方法", "method": "1) 构建EmoStyleSet数据集，从ArtEmis中提取内容-情感-风格化图像三元组；2) 提出情感-内容推理器，自适应地整合情感线索与内容信息；3) 开发风格量化器，将连续风格特征转换为情感相关的码本条目", "result": "通过定性和定量评估（包括用户研究）表明，EmoStyle在保持内容一致性的同时增强了情感表达能力。学习到的情感感知风格字典可适应其他生成任务，展示了更广泛的应用潜力", "conclusion": "该工作为情感驱动的图像风格化奠定了基础，扩展了AI生成艺术的创作潜力，建立了情感影响图像风格化的新研究方向"}}
{"id": "2512.05475", "pdf": "https://arxiv.org/pdf/2512.05475", "abs": "https://arxiv.org/abs/2512.05475", "authors": ["Saumya Biswas", "Jiten Oswal"], "title": "PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "22 pages, 9 figures, 4 tables", "summary": "In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.", "AI": {"tldr": "比较不同对称性等变量子机器学习模型在分子几何结构学习中的性能，发现置换对称嵌入是最具泛化性的量子机器学习模型", "motivation": "研究几何量子机器学习模型在不同分子几何结构中的性能差异，探索对称性等变对模型泛化能力的影响，为分子几何学习提供模型选择标准", "method": "使用两种分子数据集（线性LiH分子和三角锥形NH3分子），比较无对称等变、旋转等变、置换等变以及图嵌入置换等变的量子机器学习模型，以经典等变模型作为性能基准", "result": "图嵌入特征被证明是提高几何数据集可训练性的有效途径，置换对称嵌入被发现是几何学习中最具泛化性的量子机器学习模型", "conclusion": "分子几何结构与模型性能差异揭示了模型泛化性的选择标准，置换对称嵌入的量子机器学习模型在分子几何学习中表现出最佳泛化性能"}}
{"id": "2512.05472", "pdf": "https://arxiv.org/pdf/2512.05472", "abs": "https://arxiv.org/abs/2512.05472", "authors": ["Yiting Dong", "Zhaofei Yu", "Jianhao Ding", "Zijie Xu", "Tiejun Huang"], "title": "Unleashing Temporal Capacity of Spiking Neural Networks through Spatiotemporal Separation", "categories": ["cs.NE"], "comment": null, "summary": "Spiking Neural Networks (SNNs) are considered naturally suited for temporal processing, with membrane potential propagation widely regarded as the core temporal modeling mechanism. However, existing research lack analysis of its actual contributions in complex temporal tasks. We design Non-Stateful (NS) models progressively removing membrane propagation to quantify its stage-wise role. Experiments reveal a counterintuitive phenomenon: moderate removal in shallow or deep layers improves performance, while excessive removal causes collapse. We attribute this to spatio-temporal resource competition where neurons encode both semantics and dynamics within limited range, with temporal state consuming capacity for spatial learning. Based on this, we propose Spatial-Temporal Separable Network (STSep), decoupling residual blocks into independent spatial and temporal branches. The spatial branch focuses on semantic extraction while the temporal branch captures motion through explicit temporal differences. Experiments on Something-Something V2, UCF101, and HMDB51 show STSep achieves superior performance, with retrieval task and attention analysis confirming focus on motion rather than static appearance. This work provides new perspectives on SNNs' temporal mechanisms and an effective solution for spatiotemporal modeling in video understanding.", "AI": {"tldr": "提出时空分离网络（STSep），通过解耦脉冲神经网络的时空处理能力，解决时空资源竞争问题，提升视频理解任务的性能", "motivation": "现有研究缺乏对脉冲神经网络在复杂时序任务中膜电位传播机制实际贡献的分析，需要量化其阶段性作用并解决时空资源竞争问题", "method": "设计无状态（NS）模型逐步移除膜电位传播以量化其作用，基于时空资源竞争理论提出时空分离网络（STSep），将残差块解耦为独立的空间和时间分支", "result": "在Something-Something V2、UCF101和HMDB51数据集上取得优越性能，检索任务和注意力分析确认模型更关注运动而非静态外观", "conclusion": "为脉冲神经网络的时序机制提供了新视角，并为视频理解中的时空建模提供了有效解决方案，揭示了时空分离策略的优越性"}}
{"id": "2512.05469", "pdf": "https://arxiv.org/pdf/2512.05469", "abs": "https://arxiv.org/abs/2512.05469", "authors": ["Zubair Ahmed Mohammad"], "title": "How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 9 figures, 3 tables. Code and reproducible experiments are available at: https://github.com/zubair0831/ensemble-generalization-gap", "summary": "Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.", "AI": {"tldr": "该研究从偏差-方差视角探讨集成学习如何在表格数据上平衡准确率和过拟合问题，通过四个分类任务验证集成方法在保持高准确率的同时控制泛化差距的能力。", "motivation": "集成模型通常比单一学习器获得更高准确率，但其保持较小泛化差距的能力尚未被充分理解。研究旨在探究集成方法如何在表格分类任务中平衡准确率和过拟合，为实际应用提供模型选择指导。", "method": "使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单一决策树和九种集成方法。计算数据集复杂度指标（线性度得分、Fisher比率、噪声估计）来解释集成方法何时能有效控制方差。", "result": "集成方法通过平均或受控提升减少方差，能在保持高准确率的同时控制泛化差距。在线性且干净的数据上，线性模型已能很好泛化，集成方法增益有限；在具有非线性结构的数据集上，树基集成将测试准确率提高5-7个百分点，同时保持差距低于3%；在噪声大或高度不平衡的数据集上，集成方法仍具竞争力但需要正则化以避免过拟合。", "conclusion": "研究清晰展示了集成方法何时以及如何保持高准确率同时控制过拟合，为实际表格数据应用中的模型选择提供了实用指导。数据集复杂度指标能有效预测集成方法控制方差的效果。"}}
{"id": "2512.05468", "pdf": "https://arxiv.org/pdf/2512.05468", "abs": "https://arxiv.org/abs/2512.05468", "authors": ["Takara Taniguchi", "Yudai Ueda", "Atsuya Muramatsu", "Kohki Hashimoto", "Ryo Yagi", "Hideya Ochiai", "Chaodit Aswakul"], "title": "University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.", "AI": {"tldr": "开发泰国朱拉隆功大学建筑识别数据集（CUBR），用于支持面向任务的物联网传感器系统中的无线自组织联邦学习（WAFL）应用", "motivation": "随着边缘设备性能提升，边缘训练成为未来方向。WAFL-ViT作为面向任务的传感器系统，需要针对特定任务构建专用数据集。现有UTBR数据集无法满足泰国大学建筑识别需求", "method": "开发专门针对泰国朱拉隆功大学的建筑识别数据集（CUBR），作为WAFL-ViT系统的案例研究。数据集包含泰国大学建筑的图像数据", "result": "成功构建了CUBR数据集，并在WAFL场景下进行测试。结果显示，在WAFL场景下的训练比自训练场景获得更高的准确率", "conclusion": "CUBR数据集为泰国大学建筑识别任务提供了专门的数据支持，验证了WAFL在边缘设备协作学习中的优势，为面向任务的物联网传感器系统提供了实用的数据集资源"}}
{"id": "2512.05464", "pdf": "https://arxiv.org/pdf/2512.05464", "abs": "https://arxiv.org/abs/2512.05464", "authors": ["Panatchakorn Anantaprayoon", "Nataliia Babina", "Jad Tarifi", "Nima Asgharbeygi"], "title": "Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures, to appear in AAAI 2026 AIGOV Workshop", "summary": "Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.", "AI": {"tldr": "提出了一种面向集体智能的动态对齐框架，通过自我改进机制实现可扩展的开放式LLM对齐", "motivation": "传统基于人类反馈的对齐方法资源密集且难以扩展，而现有AI反馈的自改进方法仍受限于传统对齐价值观。随着AI向AGI/ASI发展，需要更全面、可扩展的对齐方法", "method": "提出动态对齐框架，包含两个核心组件：1) 基于LLM的自动训练数据集生成；2) 自我奖励机制，策略模型评估自身输出候选并为GRPO学习分配奖励", "result": "实验结果表明，该方法成功将模型对齐到集体智能价值观，同时保持了通用NLP能力", "conclusion": "动态对齐框架为开放式LLM对齐提供了可扩展的自我改进方法，超越了传统对齐规范，促进了综合智能能力的发展"}}
{"id": "2512.05461", "pdf": "https://arxiv.org/pdf/2512.05461", "abs": "https://arxiv.org/abs/2512.05461", "authors": ["Bolun Zhang", "Linzhuo Li", "Yunqi Chen", "Qinlin Zhao", "Zihan Zhu", "Xiaoyuan Yi", "Xing Xie"], "title": "Knowing Your Uncertainty -- On the application of LLM in social sciences", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "49 pages, 10 figures", "summary": "Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research.", "AI": {"tldr": "提出一个评估大语言模型在社会科学研究中不确定性的统一框架，帮助研究人员更严谨地将LLMs应用于社会科学任务", "motivation": "大语言模型正快速融入计算社会科学研究，但其黑盒训练和推理中的随机性设计给科学研究带来独特挑战，需要明确评估不确定性以确保研究严谨性", "method": "引入一个二维统一框架评估LLM不确定性：任务类型（分类、短文本生成、长文本生成）和验证类型（参考数据或评估标准的可用性），并将现有不确定性量化方法映射到该分类中", "result": "建立了一个系统化的不确定性评估框架，为研究人员提供了实用的方法建议，将计算机科学和社会科学文献中的不确定性量化方法整合到统一的分类体系中", "conclusion": "该框架既提供了方法学保障，又为将LLMs整合到严谨的社会科学研究中提供了实用指南，强调了在社会科学任务中应用LLMs时必须明确评估不确定性的重要性"}}
{"id": "2512.05460", "pdf": "https://arxiv.org/pdf/2512.05460", "abs": "https://arxiv.org/abs/2512.05460", "authors": ["Dehong Zheng", "Zhongzhi Zhang"], "title": "ProbeWalk: Fast Estimation of Biharmonic Distance on Graphs via Probe-Driven Random Walks", "categories": ["cs.SI", "cs.DS"], "comment": null, "summary": "The biharmonic distance is a fundamental metric on graphs that measures the dissimilarity between two nodes, capturing both local and global structures. It has found applications across various fields, including network centrality, graph clustering, and machine learning. These applications typically require efficient evaluation of pairwise biharmonic distances. However, existing algorithms remain computationally expensive. The state-of-the-art method attains an absolute-error guarantee epsilon_abs with time complexity O(L^5 / epsilon_abs^2), where L denotes the truncation length. In this work, we improve the complexity to O(L^3 / epsilon^2) under a relative-error guarantee epsilon via probe-driven random walks. We provide a relative-error guarantee rather than an absolute-error guarantee because biharmonic distances vary by orders of magnitude across node pairs. Since L is often very large in real-world networks (for example, L >= 10^3), reducing the L-dependence from the fifth to the third power yields substantial gains. Extensive experiments on real-world networks show that our method delivers 10x-1000x per-query speedups at matched relative error over strong baselines and scales to graphs with tens of millions of nodes.", "AI": {"tldr": "提出了一种名为ProbeWalk的新方法，通过探针驱动的随机游走快速估计图上的双调和距离，显著提高了计算效率", "motivation": "双调和距离是图上的基本度量，在多个领域有重要应用，但现有算法计算成本高昂，特别是对于大规模现实网络，需要更高效的算法", "method": "采用探针驱动的随机游走方法，通过创新的采样策略来估计双调和距离，将时间复杂度从O(L^5/ε_abs^2)降低到O(L^3/ε^2)", "result": "在真实网络上的实验表明，该方法在匹配相对误差下实现了10-1000倍的每查询加速，能够扩展到数千万节点的图", "conclusion": "ProbeWalk方法通过探针驱动的随机游走显著提高了双调和距离的计算效率，解决了大规模图分析中的计算瓶颈问题"}}
{"id": "2512.05453", "pdf": "https://arxiv.org/pdf/2512.05453", "abs": "https://arxiv.org/abs/2512.05453", "authors": ["Luc Moreau", "Alfred Rossi", "Sophie Stalla-Bourdillon"], "title": "Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments", "categories": ["cs.DB", "cs.AI", "cs.CY", "cs.LO"], "comment": "17 pages, 8 figures. Code and examples available at https://github.com/alfredr/parajudica", "summary": "Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.", "AI": {"tldr": "开发一个基于RDF/SPARQL的规则系统，用于在多框架环境下评估上下文相关的数据合规状态", "motivation": "解决在多个同时适用的合规框架下实施基于策略的数据访问控制（PBAC）的挑战", "method": "创建开放、模块化、可扩展的RDF/SPARQL规则系统，并开发相应的元模型", "result": "通过应用于现有法律框架和行业标准，展示了该资源和元模型的实用性，为比较框架分析提供了见解", "conclusion": "Parajudica系统可用于合规策略执行、合规监控、数据发现和风险评估等多种应用场景"}}
{"id": "2512.05450", "pdf": "https://arxiv.org/pdf/2512.05450", "abs": "https://arxiv.org/abs/2512.05450", "authors": ["Pawel Weichbroth"], "title": "Classification and taxonomy of mobile application usability issues", "categories": ["cs.HC"], "comment": "55 pages, 5 figures, 9 tables, 129 references", "summary": "Despite years of research on testing the usability of mobile applications, our understanding of the issues their users experience still remains fragmented and underexplored. While most earlier studies has provided interesting insights, they have varying limitations in methodology, input diversity, and depth of analysis.On the contrary, this study employs a triangulation strategy, using two research methods (systematic literature review and interview) and two data sources (scholarly literature and expert knowledge) to explore the traits underlying usability issues. Our study contributes to the field of human-computer interaction (HCI) by presenting a catalog of 16 usability issue categories, enriched with corresponding keywords and extended into a taxonomy, as well as a novel three-tier app-user-resource (AUR) classification system. At the first app level, usability issues arise from user interface design, as well as from efficiency, errors, and operability. At the second user level, they influence cognitive load, effectiveness, ease of use, learnability, memorability, and understandability. At the third resource level, usability issues stem from network quality and hardware, such as battery life, CPU speed, physical device button size and availability, RAM capacity, and screen size. The root cause of the usability issues is the user interface design. Detailed findings and takeaways for both researchers and practitioners are also discussed. Further research could focus on developing a measurement model for the identified variables to confirm the direction and strength of their relationships with perceived usability. Software vendors can also benefit by updating existing quality assurance programs, reviews and audits tools, as well as testing checklists.", "AI": {"tldr": "该研究通过系统文献综述和专家访谈的三角测量策略，提出了一个包含16个可用性问题类别的分类目录，并构建了应用-用户-资源（AUR）三层分类系统，以系统化理解移动应用可用性问题。", "motivation": "尽管移动应用可用性测试研究已有多年，但对其可用性问题的理解仍然零散且探索不足。先前研究在方法论、输入多样性和分析深度方面存在局限，需要更系统化的方法来探索可用性问题的本质特征。", "method": "采用三角测量策略，结合两种研究方法（系统文献综述和访谈）和两种数据源（学术文献和专家知识），通过综合分析探索可用性问题的底层特征。", "result": "提出了包含16个可用性问题类别的分类目录，并扩展为分类学体系；创建了新颖的应用-用户-资源（AUR）三层分类系统：应用层面涉及界面设计、效率、错误和可操作性；用户层面影响认知负荷、有效性、易用性、可学习性、可记忆性和可理解性；资源层面源于网络质量和硬件因素。", "conclusion": "该研究为HCI领域提供了系统化的可用性问题分类框架，根因在于用户界面设计。研究成果对研究人员和从业者均有实用价值，未来可开发测量模型验证变量关系，软件供应商可更新质量保证程序和测试工具。"}}
{"id": "2512.05449", "pdf": "https://arxiv.org/pdf/2512.05449", "abs": "https://arxiv.org/abs/2512.05449", "authors": ["Robert Yang"], "title": "The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems", "categories": ["cs.AI"], "comment": "4 pages + appendix. AAAI 2026 FAST Workshop (Oral)", "summary": "Large language models display a peculiar form of inconsistency: they \"know\" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of \"self-control\" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as \"scheming\" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.", "AI": {"tldr": "该论文提出将\"意志薄弱\"(akrasia)作为分析智能体AI系统不一致性和目标漂移的基础概念，并引入了Akrasia Benchmark来量化评估模型在不同诱惑条件下的\"自我控制\"能力。", "motivation": "大语言模型表现出一种特殊的不一致性：它们\"知道\"正确答案但未能据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为意志薄弱。作者认为需要将这种不一致性概念化，以分析智能体AI系统中的目标漂移和不稳定性。", "method": "提出了Akrasia Benchmark的初步版本，包含四种结构化提示条件：基线(B)、同义词(S)、时间(T)和诱惑(X)，用于测量模型局部响应与其先前承诺相矛盾的情况。该基准支持跨模型家族、解码策略和诱惑类型的\"自我控制\"量化比较。", "result": "基准能够定量比较不同模型在面临诱惑时的自我控制能力。此外，论文还概述了微观层面的意志薄弱如何在多智能体系统中累积成宏观层面的不稳定性，这可能被解释为\"阴谋\"或故意不对齐。", "conclusion": "通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典的能动性理论联系起来，为哲学、心理学和新兴的智能体AI科学之间建立了经验桥梁，为分析智能体系统的不稳定性和目标漂移提供了新的概念框架。"}}
{"id": "2512.05446", "pdf": "https://arxiv.org/pdf/2512.05446", "abs": "https://arxiv.org/abs/2512.05446", "authors": ["Cheng-Yuan Ho", "He-Bi Yang", "Jui-Chiu Chiang", "Yu-Lun Liu", "Wen-Hsiao Peng"], "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.", "AI": {"tldr": "提出TED-4DGS方法，用于动态3D高斯溅射（4DGS）的率失真优化压缩，通过时间激活和嵌入变形机制实现高效动态场景表示。", "motivation": "现有动态3DGS方法存在局限性：时空4DGS使用过度指定、短寿命的高斯基元，而规范3DGS变形缺乏明确的时间控制。需要设计更紧凑高效的变形方案和率失真优化压缩策略。", "method": "基于稀疏锚点的3DGS表示，每个规范锚点分配可学习的时间激活参数控制出现/消失，轻量级锚点时间嵌入查询共享变形库生成锚点特定变形。使用INR超先验建模锚点属性分布，通道自回归模型捕获锚点内相关性。", "result": "在多个真实世界数据集上实现了最先进的率失真性能，是首批追求动态3DGS表示率失真优化压缩框架的尝试之一。", "conclusion": "TED-4DGS统一了两种方法的优势，通过时间激活和嵌入变形实现了高效的4DGS压缩，为动态3DGS表示提供了紧凑且率失真优化的解决方案。"}}
{"id": "2512.05442", "pdf": "https://arxiv.org/pdf/2512.05442", "abs": "https://arxiv.org/abs/2512.05442", "authors": ["Hua Wang", "Jinghao Lu", "Fan Zhang"], "title": "IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AAAI 2026", "summary": "Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.", "AI": {"tldr": "提出IdealTSF框架，利用非理想负样本来增强时间序列预测模型的性能", "motivation": "时间序列数据中常见的缺失值和异常值问题阻碍了深度学习预测模型的进一步发展，现有研究主要关注特征提取或将非理想数据作为正样本进行知识迁移，但未能有效利用这些负样本提升事件预测能力", "method": "提出IdealTSF框架，包含三个渐进步骤：预训练（从负样本数据中提取知识）、训练（将序列数据转换为理想正样本）、优化（应用带有对抗扰动的负优化机制），结合理想正样本和非理想负样本进行时间序列预测", "result": "大量实验表明，负样本数据在基本注意力架构中释放了显著的时间序列预测潜力，IdealTSF特别适用于具有噪声样本或低质量数据的应用场景", "conclusion": "非理想负样本能够有效增强时间序列预测模型的性能，IdealTSF框架通过整合正负样本的渐进式训练方法，为处理噪声数据和低质量数据提供了有效的解决方案"}}
{"id": "2512.05439", "pdf": "https://arxiv.org/pdf/2512.05439", "abs": "https://arxiv.org/abs/2512.05439", "authors": ["Tarun Suresh", "Nalin Wadhwa", "Debangshu Banerjee", "Gagandeep Singh"], "title": "BEAVER: An Efficient Deterministic LLM Verifier", "categories": ["cs.AI", "cs.FL"], "comment": null, "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "AI": {"tldr": "BEAVER是一个用于计算大语言模型约束满足确定性概率边界的实用框架，提供可证明的可靠保证。", "motivation": "随着大语言模型从研究原型转向生产系统，需要可靠的方法来验证模型输出是否满足特定约束。基于采样的估计方法只能提供直觉，无法提供可靠的保证。", "method": "使用新颖的token trie和frontier数据结构，系统性地探索生成空间，在任何迭代中都能保持可证明的可靠边界。形式化了验证问题并证明了方法的可靠性。", "result": "在正确性验证、隐私验证和安全代码生成任务上评估BEAVER，相比基线方法，在相同计算预算下获得6-8倍更紧的概率边界，识别出3-4倍更多的高风险实例。", "conclusion": "BEAVER能够提供精确的特征描述和风险评估，这是宽松边界或经验评估无法提供的，为LLM约束验证提供了首个实用的确定性验证框架。"}}
{"id": "2512.05438", "pdf": "https://arxiv.org/pdf/2512.05438", "abs": "https://arxiv.org/abs/2512.05438", "authors": ["Benoit Marteau", "Shaun Q. Y. Tan", "Jieru Li", "Andrew Hornback", "Yishan Zhong", "Shaunna Wang", "Christian Lowson", "Jason Woloff", "Joshua M. Pahys", "Steven W. Hwang", "Coleman Hilton", "May D. Wang"], "title": "EXR: An Interactive Immersive EHR Visualization in Extended Reality", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.MM"], "comment": "11 pages, 6 figures. Preprint version. This paper has been accepted to IEEE ICIR 2025. This is the author-prepared version and not the final published version. The final version will appear in IEEE Xplo", "summary": "This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.", "AI": {"tldr": "设计并实现了一个扩展现实(XR)平台，用于电子健康记录(EHR)的沉浸式交互可视化，将结构化与非结构化患者数据整合到共享3D环境中，支持直观探索和实时协作。", "motivation": "传统2D界面在展示复杂医疗数据时存在局限性，需要更直观、沉浸式的可视化工具来支持临床决策和协作，特别是在处理结构化与非结构化医疗数据时。", "method": "开发模块化XR平台，集成基于FHIR的EHR数据、容积医学影像和AI生成的分割结果；使用合成EHR数据集和CT衍生脊柱模型，通过AI分割流水线处理，在共享3D环境中可视化。", "result": "成功实现了将EHR数据、医学影像和AI分割结果整合到交互式3D环境中的平台，支持多用户实时协作和直观数据探索，展示了在临床决策支持中的潜力。", "conclusion": "这种集成的XR解决方案可以为下一代临床决策支持工具奠定基础，使先进的数据基础设施能够在交互式和空间丰富的环境中直接访问，有望改善医疗数据可视化和协作。"}}
{"id": "2512.05433", "pdf": "https://arxiv.org/pdf/2512.05433", "abs": "https://arxiv.org/abs/2512.05433", "authors": ["Kim Marriott", "Matthew Butler", "Leona Holloway", "Bill Jolley", "Bongshin Lee", "Bruce Maguire", "Danielle Albers Szafir"], "title": "From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation", "categories": ["cs.HC"], "comment": "To be published by IEEE as part of the 2025 Visualization Conference (VIS)", "summary": "Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.", "AI": {"tldr": "该论文提出了一个框架，用于研究触觉信息图表相对于文本描述对盲人和低视力人群的益处，为触觉优先的信息图表设计建立理论基础。", "motivation": "虽然触觉图形被广泛用于向盲人和低视力人群展示地图和统计图表，但与视觉信息图表相比，我们缺乏对良好设计的触觉信息图表相对于文本描述的优势的清晰理解。随着商品化可刷新触觉显示器的出现，触觉图形的使用预计会增长，因此需要建立理论基础。", "method": "引入了一个包含编码、感知和认知三个组件的框架，通过该框架来检验视觉信息图表的已知益处，并探索这些益处对触觉信息图表的适用性。", "result": "建立了一个初步的理论基础，用于触觉优先的信息图表设计，并识别了未来的研究方向。该框架有助于理解触觉信息图表相对于文本描述的优势。", "conclusion": "该研究为触觉信息图表的设计提供了理论框架，填补了当前对触觉图形优势理解不足的空白，并为未来的触觉优先设计研究奠定了基础。"}}
{"id": "2512.05432", "pdf": "https://arxiv.org/pdf/2512.05432", "abs": "https://arxiv.org/abs/2512.05432", "authors": ["Jeffrey N. A. Aryee", "Patrick Davies", "Godfred A. Torsah", "Mercy M. Apaw", "Cyril D. Boateng", "Sam M. Mwando", "Chris Kwisanga", "Eric Jobunga", "Leonard K. Amekudzi"], "title": "Building Capacity for Artificial Intelligence in Africa: A Cross-Country Survey of Challenges and Governance Pathways", "categories": ["cs.CY", "cs.AI"], "comment": "16 pages, 4 figures, 1 table", "summary": "Artificial intelligence (AI) is transforming education and the workforce, but access to AI learning opportunities in Africa remains uneven. With rapid demographic shifts and growing labour market pressures, AI has become a strategic development priority, making the demand for relevant skills more urgent. This study investigates how universities and industries engage in shaping AI education and workforce preparation, drawing on survey responses from five African countries (Ghana, Namibia, Rwanda, Kenya and Zambia). The findings show broad recognition of AI importance but limited evidence of consistent engagement, practical training, or equitable access to resources. Most respondents who rated the AI component of their curriculum as very relevant reported being well prepared for jobs, but financial barriers, poor infrastructure, and weak communication limit participation, especially among students and underrepresented groups. Respondents highlighted internships, industry partnerships, and targeted support mechanisms as critical enablers, alongside the need for inclusive governance frameworks. The results showed both the growing awareness of AI's potential and the structural gaps that hinder its translation into workforce capacity. Strengthening university-industry collaboration and addressing barriers of access, funding, and policy are central to ensuring that AI contributes to equitable and sustainable development across the continent.", "AI": {"tldr": "调查非洲五国AI教育与人才培养的现状、挑战及治理路径，分析大学与产业在AI能力建设中的作用", "motivation": "AI正在改变教育和劳动力市场，但非洲的AI学习机会分布不均。随着人口结构快速变化和劳动力市场压力增加，AI已成为战略发展重点，相关技能需求日益迫切，需要了解非洲AI能力建设的现状和挑战", "method": "对加纳、纳米比亚、卢旺达、肯尼亚和赞比亚五个非洲国家进行跨国家调查，收集大学和产业界对AI教育和劳动力准备的反馈数据", "result": "研究发现：广泛认识到AI重要性但缺乏持续参与、实践培训或公平获取资源；课程相关性高的受访者就业准备更好；财务障碍、基础设施差和沟通不畅限制参与，特别是学生和弱势群体；实习、产业合作和针对性支持是关键推动因素；需要包容性治理框架", "conclusion": "加强大学与产业合作，解决获取、资金和政策障碍，对于确保AI促进非洲大陆公平和可持续发展至关重要。研究显示AI潜力认知增长与阻碍转化为劳动力能力的结构差距并存"}}
{"id": "2512.05430", "pdf": "https://arxiv.org/pdf/2512.05430", "abs": "https://arxiv.org/abs/2512.05430", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "title": "ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Submitted to LREC 2026. This work is an evolution of our earlier preprint arXiv:2507.23334", "summary": "Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.", "AI": {"tldr": "该论文提出了ArtistMus基准和MusWikiDB向量数据库，用于评估检索增强生成（RAG）在音乐问答（MQA）任务中的表现，旨在解决大语言模型在音乐领域知识不足的问题。", "motivation": "现有大语言模型在音乐相关推理方面效果有限，因为预训练数据中音乐知识稀疏。虽然音乐信息检索和计算音乐学已探索结构化多模态理解，但缺乏基于艺术家元数据和历史背景的事实性和上下文音乐问答资源。", "method": "构建了MusWikiDB向量数据库（包含144K音乐相关维基百科页面的320万段落）和ArtistMus基准（包含500位多样化艺术家的1000个问题）。使用检索增强生成（RAG）方法，通过RAG风格微调提升事实召回和上下文推理能力。", "result": "RAG显著提高事实准确性：开源模型提升高达56.8个百分点（如Qwen3 8B从35.0提升至91.8），接近专有模型性能。RAG风格微调进一步提升事实召回和上下文推理，在领域内和领域外基准上均有改善。MusWikiDB相比通用维基百科语料库准确率提高约6个百分点，检索速度快40%。", "conclusion": "MusWikiDB和ArtistMus为音乐信息检索和领域特定问答研究提供了重要资源，为音乐等文化丰富领域的检索增强推理建立了基础，并公开发布这些资源以推动相关研究进展。"}}
{"id": "2512.05422", "pdf": "https://arxiv.org/pdf/2512.05422", "abs": "https://arxiv.org/abs/2512.05422", "authors": ["Jiangtong Tan", "Lin Liu", "Jie Huanng", "Xiaopeng Zhang", "Qi Tian", "Feng Zhao"], "title": "ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \\textbf{ParaUni}. It extracts features from variants VLM's layers in a \\textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \\textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.", "AI": {"tldr": "ParaUni提出了一种增强统一多模态模型生成能力的方法，通过并行提取视觉语言模型各层次特征并进行强化学习驱动的层次化信息交互", "motivation": "现有统一多模态模型在视觉生成任务中难以充分平衡充分交互与灵活实现，因为视觉语言模型和扩散模型之间存在巨大的表示差异。视觉语言模型各层次包含从低层细节到高层语义的丰富层次化信息，但现有方法未能充分利用这些信息", "method": "提出ParaUni框架：1）并行提取视觉语言模型各层次特征，通过层集成模块(LIM)高效整合细粒度细节和语义抽象；2）设计层动态调整机制(LDAM)，利用强化学习根据不同奖励对层次特征进行动态调整，使层次特性与多种奖励改进对齐", "result": "实验表明ParaUni能够利用互补的多层特征显著提高生成质量，并在强化学习阶段展现出对多种奖励改进的强大潜力", "conclusion": "ParaUni通过并行信息交互和强化学习驱动的层次化调整，有效解决了统一多模态模型中信息交互不足的问题，显著提升了视觉生成性能"}}
{"id": "2512.05418", "pdf": "https://arxiv.org/pdf/2512.05418", "abs": "https://arxiv.org/abs/2512.05418", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems", "categories": ["cs.CV"], "comment": null, "summary": "UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.", "AI": {"tldr": "评估深度学习模型在不同分辨率下对树木分支分割的性能，为自主林业系统中的嵌入式系统提供准确性与效率权衡的基准", "motivation": "无人机自主林业操作需要快速精确的树木分支分割，以在变化的像素分辨率和操作条件下实现安全导航和自动修剪，但现有方法在准确性和效率之间的权衡缺乏系统评估", "method": "使用Urban Street Tree数据集，在三种分辨率（256x256, 512x512, 1024x1024）下评估不同深度学习模型，采用标准指标（IoU, Dice）和专门指标（Thin Structure IoU, Connectivity Preservation Rate），共测试22种配置", "result": "在256x256分辨率下，U-Net with MiT-B4表现最佳；512x512分辨率下，MiT-B4在IoU、Dice、TS-IoU和Boundary-F1上领先；1024x1024分辨率下，U-Net+MiT-B3在IoU/Dice和精度上表现最好，而U-Net++在边界质量上最优；PSPNet是最有效的选项，但IoU比最佳模型低11.8-25.7个百分点", "conclusion": "研究建立了多分辨率基准，为嵌入式林业系统提供了准确性与计算效率之间的权衡参考，不同分辨率和模型组合适用于不同的应用场景需求"}}
{"id": "2512.05415", "pdf": "https://arxiv.org/pdf/2512.05415", "abs": "https://arxiv.org/abs/2512.05415", "authors": ["Masato Shibukawa", "Fumi Yoshida", "Toshifumi Yanagisawa", "Takashi Ito", "Hirohisa Kurosaki", "Makoto Yoshikawa", "Kohki Kamiya", "Ji-an Jiang", "Wesley Fraser", "JJ Kavelaars", "Susan Benecchi", "Anne Verbiscer", "Akira Hatakeyama", "Hosei O", "Naoya Ozaki"], "title": "Moving object detection from multi-depth images with an attention-enhanced CNN", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "14 pages, 22 figures, submitted to PASJ", "summary": "One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.", "AI": {"tldr": "提出一种结合注意力机制的多输入卷积神经网络，用于从多深度图像中检测太阳系中的运动天体，显著降低人工验证工作量", "motivation": "太阳系宽视场巡天数据中运动天体检测面临的主要挑战是区分真实天体信号与噪声等干扰源。传统方法依赖人工验证，成本高昂且效率低下，需要减少对人工干预的依赖", "method": "提出多输入卷积神经网络，集成卷积块注意力模块。采用多输入架构同时处理多个堆叠图像，注意力模块使模型能够聚焦于空间和通道维度上的关键特征", "result": "在约2000张观测图像的数据集上评估模型性能，达到近99%的准确率和AUC>0.99。通过调整检测阈值，新模型相比人工验证减少了超过99%的工作量", "conclusion": "提出的注意力增强多输入CNN模型在运动天体检测中表现出优异的分类性能，显著降低了人工验证的工作量，为太阳系天体巡天提供了高效的自动化解决方案"}}
{"id": "2512.05412", "pdf": "https://arxiv.org/pdf/2512.05412", "abs": "https://arxiv.org/abs/2512.05412", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications", "categories": ["cs.CV"], "comment": null, "summary": "Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.", "AI": {"tldr": "提出一个集成YOLO目标检测与SGBM立体视觉的计算机视觉框架，用于辐射松自主修剪应用中的树枝检测与深度估计", "motivation": "辐射松人工修剪存在安全风险，工作高度极端且地形复杂，需要开发成本效益高的自主修剪系统来提升工人安全和作业效率", "method": "集成YOLO目标检测与SGBM立体匹配算法，仅使用立体相机输入，无需昂贵的LiDAR传感器，实现精确的树枝检测和深度估计", "result": "YOLO在树枝分割上达到82.0% mAPmask50-95，优于Mask R-CNN；集成系统能在2米操作范围内准确定位树枝，每帧处理时间低于1秒", "conclusion": "该研究证明了成本效益高的自主修剪系统的可行性，能够显著提升商业林业中的工人安全和工作效率"}}
{"id": "2512.05411", "pdf": "https://arxiv.org/pdf/2512.05411", "abs": "https://arxiv.org/abs/2512.05411", "authors": ["Pranav Pushkar Mishra", "Kranti Prakash Yeole", "Ramyashree Keshavamurthy", "Mokshit Bharat Surana", "Fatemeh Sarayloo"], "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems", "categories": ["cs.IR", "cs.AI"], "comment": "7 pages, 3 figures, 3 tables", "summary": "In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.", "AI": {"tldr": "提出一个系统化的企业知识检索框架，利用LLM生成的元数据增强RAG系统性能", "motivation": "在企业环境中，从大型复杂知识库中高效检索相关信息对运营生产力和决策制定至关重要，现有RAG系统在文档检索准确性方面存在改进空间", "method": "采用结构化管道动态生成文档片段的元数据，比较三种分块策略（语义、递归、朴素）与高级嵌入技术结合的效果，使用交叉编码器重排进行基准真值生成", "result": "元数据增强方法始终优于纯内容基线，递归分块结合TF-IDF加权嵌入达到82.5%精确率（vs 73.3%语义纯内容），朴素分块结合前缀融合达到最高Hit Rate@10为0.925", "conclusion": "元数据增强提高了向量聚类质量并减少检索延迟，是跨知识领域RAG系统的关键优化，为企业部署高性能、可扩展文档检索解决方案提供实用见解"}}
{"id": "2512.05410", "pdf": "https://arxiv.org/pdf/2512.05410", "abs": "https://arxiv.org/abs/2512.05410", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images", "categories": ["cs.CV"], "comment": null, "summary": "Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.", "AI": {"tldr": "提出一种基于遗传算法的参数优化框架，用于优化辐射松树枝图像视差图生成的SGBM和WLS算法参数，提高无人机林业应用中的距离测量精度", "motivation": "传统立体匹配算法（如SGBM+WLS）在无人机应用中具有速度优势，但需要繁琐的手动参数调优，限制了其在林业应用中的精度和实用性", "method": "采用遗传算法（GA）框架系统搜索SGBM和WLS算法的最优参数配置，通过多图像质量指标进行综合评估，为资源受限的无人机系统提供实用解决方案", "result": "实验结果显示，GA优化方法相比基线配置：均方误差降低42.86%，峰值信噪比提高8.47%，结构相似性提高28.52%，并在不同成像条件下表现出优异的泛化性能", "conclusion": "遗传算法参数优化框架有效解决了传统立体匹配算法的手动调参问题，显著提高了辐射松树枝图像视差图的质量，为无人机林业应用提供了高效精确的解决方案"}}
{"id": "2512.05402", "pdf": "https://arxiv.org/pdf/2512.05402", "abs": "https://arxiv.org/abs/2512.05402", "authors": ["Sithumi Wickramasinghe", "Bikramjit Das", "Dorien Herremans"], "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NE"], "comment": null, "summary": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.", "AI": {"tldr": "该论文提出了一个基于深度学习的比特币挖矿硬件投资回报预测框架，用于指导ASIC矿机的最佳购买时机决策。", "motivation": "比特币挖矿硬件采购面临市场波动大、技术快速淘汰和协议驱动的收入周期等挑战，目前缺乏科学的决策框架来指导何时购买新的ASIC矿机，导致资本密集型挖矿行业存在较高的财务风险。", "method": "提出MineROI-Net，一个开源的基于Transformer的架构，将硬件采购决策建模为时间序列分类任务，预测购买ASIC矿机在一年内是否会产生盈利、边际或亏损的投资回报。", "result": "在2015-2024年间发布的20种ASIC矿机数据上评估，MineROI-Net达到83.7%的准确率和83.1%的宏F1分数，在检测亏损期和盈利期分别达到93.6%和98.5%的精确率，优于LSTM和TSLANet基线模型。", "conclusion": "MineROI-Net提供了一个实用的数据驱动工具，能够有效指导挖矿硬件采购时机，降低资本密集型挖矿操作的财务风险，模型已在GitHub上开源。"}}
{"id": "2512.05398", "pdf": "https://arxiv.org/pdf/2512.05398", "abs": "https://arxiv.org/abs/2512.05398", "authors": ["Zhuoyuan Wu", "Xurui Yang", "Jiahui Huang", "Yue Wang", "Jun Gao"], "title": "The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/wuzy2115/DYNAPO", "summary": "Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects. Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background. However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding. In this work, we introduce the Dynamic Prior (\\ourmodel) to robustly identify dynamic objects without task-specific training, leveraging the powerful reasoning capabilities of Vision-Language Models (VLMs) and the fine-grained spatial segmentation capacity of SAM2. \\ourmodel can be seamlessly integrated into state-of-the-art pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation. Extensive experiments on both synthetic and real-world videos demonstrate that \\ourmodel not only achieves state-of-the-art performance on motion segmentation, but also significantly improves accuracy and robustness for structural 3D understanding.", "AI": {"tldr": "提出Dynamic Prior方法，利用视觉语言模型和SAM2的强大能力，无需特定任务训练即可鲁棒识别动态物体，从而提升动态视频中的3D结构理解", "motivation": "传统SfM方法在动态物体存在时难以准确估计相机位姿和3D几何结构，现有学习方法受限于大规模运动分割数据集，导致分割不准确和3D理解性能下降", "method": "结合视觉语言模型的推理能力和SAM2的细粒度空间分割能力，构建Dynamic Prior来识别动态物体，可无缝集成到最先进的相机位姿优化、深度重建和4D轨迹估计流程中", "result": "在合成和真实世界视频上的大量实验表明，该方法不仅在运动分割上达到最先进性能，还显著提高了3D结构理解的准确性和鲁棒性", "conclusion": "Dynamic Prior通过利用预训练视觉语言模型和分割模型的能力，无需特定训练即可有效处理动态物体，为动态视频的3D结构理解提供了强大解决方案"}}
{"id": "2512.05397", "pdf": "https://arxiv.org/pdf/2512.05397", "abs": "https://arxiv.org/abs/2512.05397", "authors": ["Rachel Poonsiriwong", "Chayapatr Archiwaranguprok", "Constanze Albrecht", "Peggy Yin", "Nattavudh Powthavee", "Hal Hershfield", "Monchai Lertsutthiwong", "Kavin Winson", "Pat Pataranutaporn"], "title": "Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Major life transitions demand high-stakes decisions, yet people often struggle to imagine how their future selves will live with the consequences. To support this limited capacity for mental time travel, we introduce AI-enabled digital twins that have ``lived through'' simulated life scenarios. Rather than predicting optimal outcomes, these simulations extend prospective cognition by making alternative futures vivid enough to support deliberation without assuming which path is best. We evaluate this idea in a randomized controlled study (N=192) using multimodal synthesis - facial age progression, voice cloning, and large language model dialogue - to create personalized avatars representing participants 30 years forward. Young adults 18 to 28 years old described pending binary decisions and were assigned to guided imagination or one of four avatar conditions: single-option, balanced dual-option, or expanded three-option with a system-generated novel alternative. Results showed asymmetric effects: single-sided avatars increased shifts toward the presented option, while balanced presentation produced movement toward both. Introducing a system-generated third option increased adoption of this new alternative compared to control, suggesting that AI-generated future selves can expand choice by surfacing paths that might otherwise go unnoticed. Participants rated evaluative reasoning and eudaimonic meaning-making as more important than emotional or visual vividness. Perceived persuasiveness and baseline agency predicted decision change. These findings advance understanding of AI-mediated episodic prospection and raise questions about autonomy in AI-augmented decisions.", "AI": {"tldr": "该论文研究利用AI数字孪生技术创建未来自我模拟，以帮助人们在重大人生决策中更好地想象不同选择的结果，从而影响决策过程并扩展选择范围。", "motivation": "人们在面临重大人生转折时，往往难以想象未来自我如何承受不同选择的后果。现有的心理时间旅行能力有限，需要技术手段来扩展前瞻性认知，帮助人们更生动地想象不同未来场景。", "method": "采用随机对照研究（N=192），使用多模态合成技术（面部年龄进展、语音克隆、大语言模型对话）创建代表参与者30年后状态的个性化数字孪生。参与者被分配到引导想象组或四种数字孪生条件之一：单选项、平衡双选项、扩展三选项（包含系统生成的新替代方案）。", "result": "单侧数字孪生增加了向呈现选项的转变，平衡呈现则产生了向两个选项的移动。引入系统生成的第三选项增加了对新替代方案的采纳。参与者认为评估推理和幸福意义构建比情感或视觉生动性更重要。感知说服力和基线能动性预测了决策变化。", "conclusion": "AI生成的未来自我模拟能够扩展人类选择范围，揭示可能被忽视的人生路径。这些发现推进了对AI介导的情景前瞻的理解，并提出了AI增强决策中自主性的重要问题。"}}
{"id": "2512.05394", "pdf": "https://arxiv.org/pdf/2512.05394", "abs": "https://arxiv.org/abs/2512.05394", "authors": ["Shizhan Liu", "Xinran Deng", "Zhuoyi Yang", "Jiayan Teng", "Xiaotao Gu", "Jie Tang"], "title": "Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\\times$ speedup in text-to-video generation convergence and a 10\\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.", "AI": {"tldr": "该论文提出了一种具有光谱结构的视频VAE（SSVAE），通过分析VAE潜在空间的光谱特性并引入正则化方法，显著提升了视频扩散模型的训练效率和生成质量。", "motivation": "现有的视频VAE通常只关注重建保真度，而忽略了潜在空间的结构特性，这会影响扩散模型的训练难度和效果。论文发现VAE潜在空间的光谱特性对扩散训练至关重要。", "method": "提出两种轻量级、与骨干网络无关的正则化方法：局部相关正则化（Local Correlation Regularization）和潜在掩码重建（Latent Masked Reconstruction），用于引导VAE潜在空间形成有利于扩散训练的光谱结构。", "result": "SSVAE在文本到视频生成任务中实现了3倍的收敛速度提升和10%的视频奖励增益，性能优于现有的开源VAE模型。", "conclusion": "通过关注VAE潜在空间的光谱结构特性并设计相应的正则化方法，可以显著提升视频扩散模型的训练效率和生成质量，为视频生成领域提供了新的优化方向。"}}
{"id": "2512.05391", "pdf": "https://arxiv.org/pdf/2512.05391", "abs": "https://arxiv.org/abs/2512.05391", "authors": ["Qingqiao Hu", "Weimin Lyu", "Meilong Xu", "Kehan Qi", "Xiaoling Hu", "Saumya Gupta", "Jiawei Zhou", "Chao Chen"], "title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "20 pages", "summary": "Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.", "AI": {"tldr": "LoC-Path提出了一种用于病理学多模态大语言模型的高效压缩框架，通过减少WSI图像中的冗余特征来降低计算成本", "motivation": "现有病理学多模态大语言模型在处理全切片图像时，需要处理数千个图像块特征，计算成本过高。研究发现这些特征存在大量全局和局部冗余，只有少量图像块真正与诊断任务相关", "method": "设计稀疏令牌合并器(STM)和MAE预训练重采样器来去除局部冗余并压缩全局冗余；提出交叉注意力路由适配器(CARA)和令牌重要性评分器(TIS)来高效集成压缩后的视觉表示与语言模型", "result": "实验表明，该方法在性能上与现有最先进的全切片MLLM相当，同时显著降低了计算和内存需求", "conclusion": "LoC-Path通过减少冗余特征处理，为病理学多模态大语言模型提供了一种高效的计算框架，能够在保持性能的同时大幅降低资源消耗"}}
{"id": "2512.05389", "pdf": "https://arxiv.org/pdf/2512.05389", "abs": "https://arxiv.org/abs/2512.05389", "authors": ["Yuxuan Chen", "Ian Leong Ting Lo", "Bao Guo", "Netitorn Kawmali", "Chun Kit Chan", "Ruoyu Wang", "Jia Pan", "Lei Yang"], "title": "CLIO: A Tour Guide Robot with Co-speech Actions for Visual Attention Guidance and Enhanced User Engagement", "categories": ["cs.HC"], "comment": "10 pages, 7 figures, human-robot interaction", "summary": "While audio guides can offer rich information about an exhibit, it is challenging for visitors to focus on specific exhibit details based only on the verbal description. We present \\textit{CLIO}, a tour guide robot with co-speech actions to direct visitors' visual attention and thus enhance the overall user engagement in a guided tour. \\textit{CLIO} is equipped with designed actions to engage visitors. It builds eye contact with the visitor through tracking a visitor's face and blinking its eyes, or orient their attention by its head movement and laser pointer. We further use a Large Language Model (LLM) to coordinate the designed actions with a given narrative script for exhibition. We conducted a user study to evaluate the \\textit{CLIO} system in a mock-up exhibition of historical photographs. We collected feedback from questionnaires and quantitative data from a mobile eye tracker. Experimental results validated that the engaging actions are well designed and demonstrated its efficacy in guiding visual attention of the visitors. It was evidenced that \\textit{CLIO} achieved an enhanced engagement compared to the baseline system with only audio guidance.", "AI": {"tldr": "CLIO是一个具有伴随语音动作的导览机器人，通过视觉注意力引导增强用户在展览中的参与度", "motivation": "传统的音频导览难以引导参观者关注展品的具体细节，需要一种能够有效引导视觉注意力的导览系统来增强用户参与度", "method": "开发了CLIO导览机器人系统，配备面部跟踪、眨眼、头部运动和激光指针等动作来引导注意力，并使用大型语言模型协调动作与解说脚本", "result": "用户研究表明，CLIO系统在历史照片展览中有效引导了参观者的视觉注意力，相比仅音频导览的基线系统显著增强了用户参与度", "conclusion": "CLIO系统通过伴随语音的视觉引导动作成功增强了导览体验，证明了机器人辅助视觉注意力引导在博物馆导览中的有效性"}}
{"id": "2512.05386", "pdf": "https://arxiv.org/pdf/2512.05386", "abs": "https://arxiv.org/abs/2512.05386", "authors": ["Jakub Kopko", "David Graber", "Saltuk Mustafa Eyrilmez", "Stanislav Mazurenko", "David Bednar", "Jiri Sedlar", "Josef Sivic"], "title": "Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to NeurIPS 2025 AI4Science Workshop", "summary": "As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.", "AI": {"tldr": "评估可学习蛋白-配体评分函数在未见蛋白靶点上的泛化能力，揭示标准基准测试的局限性，并探索提升泛化性能的方法", "motivation": "随着机器学习在分子设计中日益重要，需要确保可学习蛋白-配体评分函数在新型蛋白靶点上的可靠性。现有评分函数在标准基准上表现良好，但其在训练数据之外的泛化能力仍面临重大挑战", "method": "使用模拟有限已知结构和实验亲和力测量的数据集划分来评估最先进评分函数的泛化能力；研究大规模自监督预训练是否能弥合泛化差距；探索利用有限测试靶点数据提升评分函数性能的简单方法", "result": "分析显示常用基准测试未能反映泛化到新型靶点的真实挑战；初步证据表明大规模自监督预训练具有弥合泛化差距的潜力；简单方法在利用有限测试靶点数据改善评分函数性能方面显示出一定效果", "conclusion": "需要更严格的评估协议，并为设计具有扩展到新型蛋白靶点预测能力的评分函数提供实用指导，强调泛化能力评估的重要性"}}
{"id": "2512.05385", "pdf": "https://arxiv.org/pdf/2512.05385", "abs": "https://arxiv.org/abs/2512.05385", "authors": ["Yingjie Xia", "Tao Liu", "Jinglei Shi", "Qingsong Xie", "Heng Guo", "Jian Yang", "Xi Wang"], "title": "ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration", "categories": ["cs.CV"], "comment": null, "summary": "Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.", "AI": {"tldr": "提出ShaRP框架，通过浅层剪枝加速视频大语言模型推理，解决预填充阶段视觉token过多导致的计算负载问题", "motivation": "视频大语言模型在预填充阶段需要处理大量视觉token，计算负载高；现有注意力剪枝方法在浅层解码器剪枝时性能下降严重，尤其是在高压缩率下", "method": "提出改进的注意力剪枝框架ShaRP，集成段感知因果掩码、位置去偏和token去重，增强token选择能力，实现浅层有效剪枝", "result": "在多个视频理解基准测试中取得有竞争力的性能，无需重新训练即可在高压缩率下保持稳定性能", "conclusion": "ShaRP为加速VLLM推理建立了新范式，通过改进的注意力剪枝机制实现浅层有效剪枝，平衡计算效率与模型性能"}}
{"id": "2512.05383", "pdf": "https://arxiv.org/pdf/2512.05383", "abs": "https://arxiv.org/abs/2512.05383", "authors": ["Mara Downing", "Matthew Peng", "Jacob Granley", "Michael Beyeler", "Tevfik Bultan"], "title": "Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation", "categories": ["cs.SE", "cs.AI"], "comment": "20 pages, 4 figures, 2 tables", "summary": "Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.", "AI": {"tldr": "提出一种基于覆盖引导模糊测试的自动化安全测试框架，用于检测机器学习驱动的神经刺激系统中的不安全刺激模式", "motivation": "机器学习模型越来越多地用于神经假体设备（如视觉假体）中生成电刺激模式，虽然这些模型提供了精确和个性化的控制，但当模型输出直接传递到神经组织时，也引入了新的安全风险", "method": "将软件测试中的覆盖引导模糊测试技术应用于神经刺激领域，通过扰动模型输入并跟踪产生的刺激是否违反电荷密度、瞬时电流或电极共激活等生物物理限制，将编码器视为黑盒，并使用覆盖度量来指导探索", "result": "应用于视网膜和皮层深度刺激编码器时，该方法系统地揭示了超出既定安全限制的多样化刺激机制，两种违规输出覆盖度量能够识别最多数量和最多样化的不安全输出，实现了跨架构和训练策略的可解释比较", "conclusion": "违规导向的模糊测试将安全评估重新定义为经验性、可重复的过程，通过将安全从训练启发式转变为已部署模型的可测量属性，为下一代神经接口的证据基准测试、监管准备和伦理保证奠定了基础"}}
{"id": "2512.05379", "pdf": "https://arxiv.org/pdf/2512.05379", "abs": "https://arxiv.org/abs/2512.05379", "authors": ["Taslim Mahbub", "Shi Feng"], "title": "Mitigating Self-Preference by Authorship Obfuscation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.", "AI": {"tldr": "研究通过作者身份混淆来减轻语言模型评估中的自我偏好偏见", "motivation": "语言模型评估器存在自我偏好偏见，即倾向于选择自己生成的答案而非其他模型或人类的答案。这种偏见难以消除，因为前沿语言模型能够识别自己的输出，即使评估候选没有标注来源。", "method": "采用黑盒扰动方法对评估候选进行作者身份混淆，包括简单的同义词替换等文本扰动技术，以减少语言模型识别自己输出的能力。", "result": "简单的同义词替换等扰动可以预测性地减少自我偏好偏见。但当扰动扩展到更彻底地消除候选文本间的风格差异时，自我偏好偏见会重新出现，表明自我识别和自我偏好发生在多个语义层面。", "conclusion": "作者身份混淆是减轻自我偏好的有前景方法，但完全消除偏见仍然具有挑战性，因为自我识别和自我偏好发生在多个语义层面。"}}
{"id": "2512.05377", "pdf": "https://arxiv.org/pdf/2512.05377", "abs": "https://arxiv.org/abs/2512.05377", "authors": ["Honglu Sun", "Hao Jing", "Zhixiang Dai", "Sa Xiao", "Wei Xue", "Jian Sun", "Qifeng Lu"], "title": "China Regional 3km Downscaling Based on Residual Corrective Diffusion Model", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.", "AI": {"tldr": "基于残差校正扩散模型的中国区域3公里降尺度预报方法", "motivation": "数值天气预报中高效生成高分辨率预报是一个基本挑战，传统方法包括动力降尺度和统计降尺度。深度学习为统计降尺度提供了强大工具，特别是扩散模型和生成对抗网络等超分辨率模型。", "method": "采用基于扩散模型的降尺度框架CorrDiff，在原始工作基础上将研究区域扩大近20倍，不仅考虑地表变量，还包含六个气压层的高层变量作为目标降尺度变量，并添加全局残差连接以提高精度。将训练好的模型应用于CMA-GFS（25公里全球网格预报）和SFF（基于球形傅里叶神经算子的数据驱动深度学习天气模型），生成中国区域3公里预报。", "result": "实验结果表明，该方法降尺度后的预报在目标变量的MAE方面普遍优于CMA-MESO（高分辨率区域模型）的直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比相应的确定性回归模型产生更真实的预测。", "conclusion": "基于残差校正扩散模型的降尺度方法能够有效生成中国区域3公里高分辨率天气预报，在多个变量上优于传统高分辨率区域模型，生成模型能够产生更真实的精细尺度细节。"}}
{"id": "2512.05374", "pdf": "https://arxiv.org/pdf/2512.05374", "abs": "https://arxiv.org/abs/2512.05374", "authors": ["Charlie Summers", "Haneen Mohammed", "Eugene Wu"], "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control", "categories": ["cs.CR", "cs.AI", "cs.DB"], "comment": "7 pages, 7 figures, CIDR 2026", "summary": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.", "AI": {"tldr": "该论文提出为LLM代理系统引入数据流控制机制，以解决代理执行复杂任务时产生的政策违规、流程腐败和安全漏洞等风险。", "motivation": "当前LLM代理在执行复杂状态任务时存在重大风险，包括政策违规、流程腐败和安全漏洞，这些问题源于缺乏对代理行为产生的不良数据流的可见性和管理机制。目前代理工作流只能以临时方式执行这些政策。", "method": "借鉴数据库管理系统将数据验证和访问控制从应用程序转移到DBMS的经验，提出系统应原生支持数据流控制并强制执行DFC政策。论文描述了为DBMS开发便携式DFC实例的早期工作。", "result": "提出了一个更广泛的研究议程，旨在为代理生态系统开发数据流控制机制，将DFC从数据库系统扩展到代理工作流中。", "conclusion": "需要为LLM代理系统开发原生数据流控制机制，以解决当前代理工作流中的安全和管理问题，类似于数据库管理系统的发展历程。"}}
{"id": "2512.05373", "pdf": "https://arxiv.org/pdf/2512.05373", "abs": "https://arxiv.org/abs/2512.05373", "authors": ["Lijinghua Zhang", "Hengrui Cai"], "title": "Text Rationalization for Robust Causal Effect Estimation", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "comment": null, "summary": "Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.", "AI": {"tldr": "提出一种名为CATR的文本合理化框架，用于在因果效应估计中通过选择稀疏的必要词元子集来解决文本数据带来的挑战", "motivation": "文本数据在因果推断中的应用日益增多，但高维文本特征会导致正性假设违反、倾向得分极端化、权重不稳定和估计方差膨胀等问题", "method": "提出Confounding-Aware Token Rationalization (CATR)框架，使用残差独立性诊断选择稀疏的必要词元子集，保留足够的混杂信息以保证无混杂性", "result": "在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法能够产生更准确、稳定和可解释的因果效应估计", "conclusion": "CATR通过丢弃无关文本同时保留关键信号，缓解了观测层面的正性假设违反问题，稳定了下游因果效应估计器，为文本数据的因果推断提供了有效解决方案"}}
{"id": "2512.05371", "pdf": "https://arxiv.org/pdf/2512.05371", "abs": "https://arxiv.org/abs/2512.05371", "authors": ["Changwen Xing", "SamZaak Wong", "Xinlai Wan", "Yanfeng Lu", "Mengli Zhang", "Zebin Ma", "Lei Qi", "Zhengxiong Li", "Nan Guan", "Zhe Jiang", "Xi Wang", "Jun Yang"], "title": "ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted by the AAAl26 Conference Main Track", "summary": "While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).", "AI": {"tldr": "提出ChipMind框架，通过知识图谱增强检索推理来解决大语言模型在长上下文电路设计规范处理中的限制", "motivation": "大语言模型在集成电路开发自动化方面潜力巨大，但受限于有限的上下文窗口；现有上下文扩展方法难以对复杂电路规范进行有效的语义建模和多跳推理", "method": "1. 将电路规范转换为领域特定知识图谱ChipKG；2. 采用ChipKG增强推理机制，结合信息论自适应检索动态追踪逻辑依赖，以及意图感知语义过滤去除无关噪声", "result": "在工业级规范推理基准测试中显著优于现有方法，平均提升34.59%（最高达72.73%）", "conclusion": "ChipMind填补了LLM辅助硬件设计学术研究与实际工业部署之间的关键空白"}}
{"id": "2512.05365", "pdf": "https://arxiv.org/pdf/2512.05365", "abs": "https://arxiv.org/abs/2512.05365", "authors": ["Zag ElSayed", "Craig Erickson", "Ernest Pedapati"], "title": "MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare", "categories": ["cs.AI", "q-bio.QM"], "comment": "6 pages, 4 figures", "summary": "Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.", "AI": {"tldr": "MCP-AI是一个结合模型上下文协议（MCP）与临床应用的创新医疗AI框架，支持自主推理、长期状态管理和可验证工作流，代表从传统临床决策支持系统和提示型大语言模型的重大转变。", "motivation": "传统医疗AI系统难以将上下文推理、长期状态管理和人类可验证工作流整合为统一框架。随着医疗系统日益复杂，对自主、上下文感知的临床推理框架的需求变得迫切，需要支持可解释、可组合和安全导向的AI解决方案。", "method": "基于模型上下文协议（MCP）构建新型架构，MCP文件捕获临床目标、患者上下文、推理状态和任务逻辑，形成可重用和可审计的内存对象。系统支持自适应、纵向和跨护理环境的协作推理，与HL7/FHIR接口连接，并符合HIPAA和FDA SaMD等监管标准。", "result": "通过两个用例验证：1）脆性X综合征伴抑郁症的诊断建模；2）2型糖尿病和高血压的远程协调。协议支持医生在环验证，简化临床流程，并保证AI责任在医疗提供者之间的安全转移。", "conclusion": "MCP-AI为即将到来的临床环境提供了一个可扩展、可解释、可组合和安全导向的AI基础，代表了医疗AI系统架构的重要进步，能够更好地满足复杂医疗环境的需求。"}}
{"id": "2512.05362", "pdf": "https://arxiv.org/pdf/2512.05362", "abs": "https://arxiv.org/abs/2512.05362", "authors": ["Sanchit Kaul", "Joseph Luna", "Shray Arora"], "title": "PoolNet: Deep Learning for 2D to 3D Video Process Validation", "categories": ["cs.CV", "cs.LG"], "comment": "All code related to this paper can be found at https://github.com/sanchitkaul/PoolNet.git", "summary": "Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.", "AI": {"tldr": "PoolNet是一个用于2D到3D视频处理验证的深度学习框架，能够区分适合SfM处理的场景和不适合的场景，显著减少获取结构运动数据的时间。", "motivation": "从序列和非序列图像数据中提取结构运动信息耗时且计算昂贵，大多数公开数据由于相机姿态变化不足、场景元素遮挡和噪声数据而不适合处理。", "method": "引入PoolNet深度学习框架，用于帧级和场景级的野外数据验证，通过深度学习模型评估数据是否适合SfM处理。", "result": "模型成功区分了适合SfM处理的场景和不适合的场景，同时显著减少了获取结构运动数据所需的时间。", "conclusion": "PoolNet提供了一个有效的深度学习解决方案，能够快速验证2D到3D视频处理的数据质量，提高SfM处理的效率和成功率。"}}
{"id": "2512.05359", "pdf": "https://arxiv.org/pdf/2512.05359", "abs": "https://arxiv.org/abs/2512.05359", "authors": ["Zekai Shao", "Yufan Hu", "Jingyuan Liu", "Bin Fan", "Hongmin Liu"], "title": "Group Orthogonal Low-Rank Adaptation for RGB-T Tracking", "categories": ["cs.CV"], "comment": "13 pages, 8 figures. Accepted by AAAI 2026. Extended version", "summary": "Parameter-efficient fine-tuning has emerged as a promising paradigm in RGB-T tracking, enabling downstream task adaptation by freezing pretrained parameters and fine-tuning only a small set of parameters. This set forms a rank space made up of multiple individual ranks, whose expressiveness directly shapes the model's adaptability. However, quantitative analysis reveals low-rank adaptation exhibits significant redundancy in the rank space, with many ranks contributing almost no practical information. This hinders the model's ability to learn more diverse knowledge to address the various challenges in RGB-T tracking. To address this issue, we propose the Group Orthogonal Low-Rank Adaptation (GOLA) framework for RGB-T tracking, which effectively leverages the rank space through structured parameter learning. Specifically, we adopt a rank decomposition partitioning strategy utilizing singular value decomposition to quantify rank importance, freeze crucial ranks to preserve the pretrained priors, and cluster the redundant ranks into groups to prepare for subsequent orthogonal constraints. We further design an inter-group orthogonal constraint strategy. This constraint enforces orthogonality between rank groups, compelling them to learn complementary features that target diverse challenges, thereby alleviating information redundancy. Experimental results demonstrate that GOLA effectively reduces parameter redundancy and enhances feature representation capabilities, significantly outperforming state-of-the-art methods across four benchmark datasets and validating its effectiveness in RGB-T tracking tasks.", "AI": {"tldr": "提出了一种用于RGB-T跟踪的组正交低秩适应框架（GOLA），通过结构化参数学习有效利用秩空间，减少参数冗余并增强特征表示能力", "motivation": "在RGB-T跟踪中，参数高效微调通过冻结预训练参数并仅微调少量参数来实现下游任务适应。然而，定量分析显示低秩适应在秩空间存在显著冗余，许多秩几乎不提供实际信息，这限制了模型学习多样化知识以应对RGB-T跟踪中各种挑战的能力", "method": "采用秩分解分区策略，利用奇异值分解量化秩重要性，冻结关键秩以保留预训练先验，将冗余秩聚类为组；设计组间正交约束策略，强制秩组之间正交，使它们学习针对不同挑战的互补特征，从而缓解信息冗余", "result": "实验结果表明，GOLA有效减少了参数冗余并增强了特征表示能力，在四个基准数据集上显著优于最先进的方法，验证了其在RGB-T跟踪任务中的有效性", "conclusion": "提出的GOLA框架通过结构化参数学习和正交约束，有效解决了低秩适应中的秩空间冗余问题，提升了RGB-T跟踪的性能和适应性"}}
{"id": "2512.05356", "pdf": "https://arxiv.org/pdf/2512.05356", "abs": "https://arxiv.org/abs/2512.05356", "authors": ["Jason Weston", "Jakob Foerster"], "title": "AI & Human Co-Improvement for Safer Co-Superintelligence", "categories": ["cs.AI"], "comment": null, "summary": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.", "AI": {"tldr": "该论文提出\"人类与AI协同改进\"作为比单纯AI自我改进更安全、更可实现的目标，旨在通过人类研究者与AI系统的协作实现\"协同超级智能\"。", "motivation": "当前AI领域的自我改进目标存在危险性且实现需要较长时间，需要寻找更安全、更可实现的目标来推动AI发展，同时确保人类安全。", "method": "通过人类研究者与AI系统在AI研究全流程（从构思到实验）中的协作，专门提升AI系统与人类研究者共同进行AI研究的能力，将人类研究改进纳入循环。", "result": "协同改进方法既能加速AI研究进程，又能通过人类与AI的共生关系，赋予两者更安全的超级智能能力。", "conclusion": "聚焦于人类研究改进的协同改进方法比单纯的AI自我改进更快速、更安全，是实现协同超级智能的更好途径。"}}
{"id": "2512.05354", "pdf": "https://arxiv.org/pdf/2512.05354", "abs": "https://arxiv.org/abs/2512.05354", "authors": ["Yang Zheng", "Hao Tan", "Kai Zhang", "Peng Wang", "Leonidas Guibas", "Gordon Wetzstein", "Wang Yifan"], "title": "SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training", "categories": ["cs.CV", "cs.GR"], "comment": "project page https://y-zheng18.github.io/SplatPainter/", "summary": "The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \\ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.", "AI": {"tldr": "提出SplatPainter方法，通过测试时训练实现从2D编辑到3D高斯表示的交互式创作", "motivation": "3D高斯溅射技术虽然革新了3D资产创建，但缺乏交互式精炼和编辑能力。现有方法速度慢、破坏原始资产特性或缺乏精细控制", "method": "采用状态感知前馈模型，直接预测紧凑特征丰富的高斯表示属性更新，利用测试时训练创建状态感知的迭代工作流程", "result": "单一架构可执行多种任务：高保真局部细节精炼、局部涂绘和一致全局重新着色，均以交互速度运行", "conclusion": "为流畅直观的3D内容创作铺平道路，实现了从2D视图编辑到3D高斯资产的连续编辑能力"}}
{"id": "2512.05350", "pdf": "https://arxiv.org/pdf/2512.05350", "abs": "https://arxiv.org/abs/2512.05350", "authors": ["Munazza Zaib", "Wei Wang", "Dulaji Hidellaarachchi", "Isma Farah Siddiqui"], "title": "Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": null, "summary": "Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.", "AI": {"tldr": "本文提出了一种混合方法学框架，专门用于研究软件工程领域中神经多样性女性面临的独特挑战，旨在填补该群体在SE研究中缺乏系统性关注的空白。", "motivation": "神经多样性女性在软件工程领域面临性别偏见和神经差异的双重挑战，但现有研究尚未系统性地关注这一群体。误诊、伪装和男性中心的工作文化加剧了她们的压力、职业倦怠和离职率，需要专门的研究方法来识别和解决这些问题。", "method": "提出混合方法学框架，整合InclusiveMag包容性框架和GenderMag走查流程，专门针对神经多样性女性在SE中的情境。研究分为三个阶段：文献综述确定挑战范围、开发人物角色和分析流程、在协作工作坊中应用该方法。", "result": "通过目标文献综述，将神经多样性女性在SE中面临的挑战综合为认知、社交、组织、结构和职业发展五个维度，揭示了误诊/延迟诊断和伪装如何加剧排斥问题，为后续开发包容性分析方法奠定了基础。", "conclusion": "本研究首次系统性地关注软件工程领域中神经多样性女性的独特挑战，提出的混合方法学框架为开发包容性分析工具和支持可操作性变革提供了理论基础，有助于改善该群体的工作环境和职业发展。"}}
{"id": "2512.05343", "pdf": "https://arxiv.org/pdf/2512.05343", "abs": "https://arxiv.org/abs/2512.05343", "authors": ["Elisabetta Fedele", "Francis Engelmann", "Ian Huang", "Or Litany", "Marc Pollefeys", "Leonidas Guibas"], "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://spacecontrol3d.github.io/", "summary": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/", "AI": {"tldr": "SpaceControl是一种无需训练、在测试时实现3D生成空间控制的方法，允许用户通过几何输入（从简单图元到详细网格）精确控制生成对象的几何形状", "motivation": "现有3D生成方法主要依赖文本或图像提示，但文本描述存在模糊性，图像编辑繁琐，难以提供对对象几何形状的直观精确控制", "method": "提出训练无关的测试时方法，接受多种几何输入，与预训练生成模型无缝集成，通过可控参数在几何保真度和输出真实感之间权衡", "result": "在几何忠实度和视觉质量方面优于基于训练和优化的基线方法，用户研究证实其优越性，并开发了交互式界面支持在线编辑", "conclusion": "SpaceControl为3D生成提供了有效的空间控制解决方案，无需额外训练，支持多种几何输入，在创意工作流程中具有实用价值"}}
{"id": "2512.05342", "pdf": "https://arxiv.org/pdf/2512.05342", "abs": "https://arxiv.org/abs/2512.05342", "authors": ["Saitao Zhang", "Yubiao Luo", "Shiqing Wang", "Pushen Zuo", "Yongxiang Li", "Lunshuai Pan", "Zheng Miao", "Zhong Sun"], "title": "First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing", "categories": ["cs.ET", "cs.AR", "cs.NE"], "comment": null, "summary": "Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration.", "AI": {"tldr": "首次演示利用内存模拟矩阵计算进行深度神经网络的二阶训练", "motivation": "二阶优化方法利用曲率信息，比一阶方法（如SGD和Adam）提供更快、更稳定的收敛，但实际应用受到二阶信息矩阵求逆计算成本过高的限制，特别是在大规模神经网络训练中", "method": "使用电阻随机存取存储器（RRAM）的内存模拟矩阵计算（AMC）技术，实现单步矩阵求逆，构建二阶优化器", "result": "在手写字母分类任务中，相比带动量的SGD和Adam分别减少26%和61%的训练轮次；在更大任务中，相比最先进的数字处理器，吞吐量提升5.88倍，能效提升6.9倍", "conclusion": "证明了AMC电路用于二阶神经网络训练的可行性和有效性，为能效AI加速开辟了新路径"}}
{"id": "2512.05338", "pdf": "https://arxiv.org/pdf/2512.05338", "abs": "https://arxiv.org/abs/2512.05338", "authors": ["Hiroki Hasegawa", "Yukihiko Okada"], "title": "Interaction Tensor Shap", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages", "summary": "Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.", "AI": {"tldr": "提出Interaction Tensor SHAP (IT SHAP)方法，通过张量网络收缩将高阶Shapley交互表示为多项式时间可计算的形式，解决了现有方法无法高效计算高阶交互的问题。", "motivation": "随着机器学习模型变得越来越深和高维，理解单个特征和组合特征如何影响预测变得困难。现有的Shapley值方法无法高效评估高阶交互：Shapley Taylor交互指数需要指数级子集枚举，而当前基于张量的方法仅限于一阶效应。", "method": "引入Interaction Tensor SHAP (IT SHAP)，将Shapley Taylor交互指数重新表述为值张量和权重张量的收缩，并假设权重张量具有多项式TT秩的有限状态张量链表示。在张量链结构化的模型和分布张量下，将指数复杂度降低到NC2并行时间。", "result": "IT SHAP将STII的指数复杂度Θ(4^n)降低到多项式时间，同时保持了STII的公理精确性。该方法为高维模型中的主效应和高阶交互提供了统一、公理化且计算可行的公式。", "conclusion": "IT SHAP为可扩展的交互感知可解释AI建立了基础，使得之前因组合结构而无法进行交互分析的大型黑盒模型能够进行交互分析，具有重要的实际应用意义。"}}
{"id": "2512.05335", "pdf": "https://arxiv.org/pdf/2512.05335", "abs": "https://arxiv.org/abs/2512.05335", "authors": ["Yuxiang Liu", "Shengfan Cao"], "title": "State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning", "categories": ["cs.RO"], "comment": null, "summary": "We study visual domain transfer for end-to-end imitation learning in a realistic and challenging setting where target-domain data are strictly off-policy, expert-free, and scarce. We first provide a theoretical analysis showing that the target-domain imitation loss can be upper bounded by the source-domain loss plus a state-conditional latent KL divergence between source and target observation models. Guided by this result, we propose State- Conditional Adversarial Learning, an off-policy adversarial framework that aligns latent distributions conditioned on system state using a discriminator-based estimator of the conditional KL term. Experiments on visually diverse autonomous driving environments built on the BARC-CARLA simulator demonstrate that SCAL achieves robust transfer and strong sample efficiency.", "AI": {"tldr": "提出一种用于端到端模仿学习的视觉域迁移方法，在目标域数据离策略、无专家且稀缺的挑战性场景下实现有效迁移", "motivation": "解决端到端模仿学习中视觉域迁移的实际挑战：目标域数据严格离策略、无专家指导且数据稀缺，传统方法难以有效迁移", "method": "提出状态条件对抗学习（SCAL）框架，通过理论分析证明目标域模仿损失可由源域损失加状态条件潜在KL散度上界，使用基于判别器的条件KL项估计器对齐系统状态条件下的潜在分布", "result": "在基于BARC-CARLA模拟器构建的视觉多样化自动驾驶环境中，SCAL实现了鲁棒的域迁移和强大的样本效率", "conclusion": "SCAL为在现实挑战性场景下的视觉域迁移提供了一种有效的离策略对抗学习框架，通过状态条件分布对齐显著提升了模仿学习的迁移性能"}}
{"id": "2512.05334", "pdf": "https://arxiv.org/pdf/2512.05334", "abs": "https://arxiv.org/abs/2512.05334", "authors": ["Samaneh Mohtadi", "Kevin Roitero", "Stefano Mizzaro", "Gianluca Demartini"], "title": "The Effect of Document Summarization on LLM-Based Relevance Judgments", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.", "AI": {"tldr": "研究文档摘要对基于大语言模型的相关性判断的影响，探索在信息检索评估中使用摘要替代完整文档的可行性", "motivation": "信息检索系统评估需要相关性判断，但人工标注成本高、耗时长。虽然大语言模型已被用作自动评估器，但现有研究大多直接使用完整文档内容，未考虑文档摘要对评估可靠性的影响", "method": "使用最先进的大语言模型在多个TREC数据集上进行比较实验，对比基于完整文档的判断与基于不同长度LLM生成摘要的判断，分析它们与人工标注的一致性、对检索效果评估的影响以及对系统排序稳定性的影响", "result": "基于摘要的判断在系统排序稳定性方面与完整文档判断相当，但会引入系统性的标签分布偏移和偏差，这些偏差因模型和数据集而异", "conclusion": "文档摘要既为大规模信息检索评估提供了更高效的机会，也是一个重要的方法学选择，对自动判断的可靠性有重要影响，需要在效率和准确性之间权衡"}}
{"id": "2512.05325", "pdf": "https://arxiv.org/pdf/2512.05325", "abs": "https://arxiv.org/abs/2512.05325", "authors": ["Ömer Faruk Akgül", "Yusuf Hakan Kalaycı", "Rajgopal Kannan", "Willie Neiswanger", "Viktor Prasanna"], "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.", "AI": {"tldr": "LYNX是一种在线早期退出机制，通过将模型的隐藏状态意识转化为置信度控制的停止决策，在推理过程中动态决定何时停止生成，以减少计算浪费并提高准确性。", "motivation": "大型推理模型在复杂任务上表现出色，但经常\"过度思考\"：在已经有足够信息正确回答后继续推理。这会浪费推理计算资源，甚至可能损害准确性。现有早期停止方法要么需要额外采样和启发式方法操作解码，要么依赖辅助验证模型，或者仅作为事后分析流程而没有正式保证。", "method": "LYNX将退出决策附加到自然出现的推理线索（如\"hmm\"、\"wait\"）上，在这些线索标记处使用强制退出的监督训练轻量级探针，并通过分割共形预测包装得到的分数，以获得对过早退出的分布无关控制。该方法在通用数学语料上训练和校准一次探针，然后跨基准、解码温度甚至非数学任务重复使用。", "result": "在1.5B到32B参数的三个模型系列上，每个基础模型的单一数学训练探针产生了强大的准确性-效率权衡。在GSM8K上，LYNX匹配或提高基线准确性同时减少40-65%的标记；在MATH-500上提高准确性达12个百分点，减少约35-60%的标记；在AIME 2024上恢复基线准确性并节省超过50%的标记；在非数学基准CommonsenseQA上，零样本转移实现了适度的准确性提升和高达70%的标记减少。", "conclusion": "LYNX提供了一种有效的在线早期退出机制，相比现有方法具有竞争性或更优的帕累托前沿，同时保持完全在线、推理时无需代理模型，并提供明确的用户可调置信度保证。该方法展示了跨任务和模型的可迁移性，显著提高了推理效率。"}}
{"id": "2512.05323", "pdf": "https://arxiv.org/pdf/2512.05323", "abs": "https://arxiv.org/abs/2512.05323", "authors": ["Adam Lizerbram", "Shane Stevenson", "Iman Khadir", "Matthew Tu", "Samuel S. P. Shen"], "title": "Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition", "categories": ["cs.LG", "cs.AI", "stat.ML", "stat.OT"], "comment": "26 pages, 12 figures", "summary": "Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.", "AI": {"tldr": "测试FourCastNetv2模型在初始条件受噪声干扰下的鲁棒性，特别是对飓风Florence的预测能力", "motivation": "评估AI天气预报模型对输入噪声和不确定性的鲁棒性，这对于极端天气事件（如飓风）的预测可靠性至关重要", "method": "进行两个实验：1）在飓风Florence的ERA5初始条件中加入不同水平的高斯噪声，观察轨迹和强度预测；2）使用完全随机的初始条件，观察模型对无意义输入的反应", "result": "FCNv2在低到中等噪声水平下能准确保持飓风特征；即使在高噪声下也能维持基本轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预测", "conclusion": "FCNv2对初始条件噪声表现出良好的鲁棒性，但存在系统性低估风暴强度的倾向；该方法简单且可移植到其他数据驱动的AI天气预报模型"}}
{"id": "2512.05318", "pdf": "https://arxiv.org/pdf/2512.05318", "abs": "https://arxiv.org/abs/2512.05318", "authors": ["Vignesh Kothapalli", "Ata Fatahibaarzi", "Hamed Firooz", "Maziar Sanjabi"], "title": "To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 45 figures, 3 tables", "summary": "Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.", "AI": {"tldr": "研究在元训练中过度使用思维链示例的隐藏成本，并提出CoT-Recipe方法来优化元训练序列中思维链和非思维链示例的混合比例。", "motivation": "思维链提示结合少样本上下文学习在大型语言模型中展现了强大的推理能力，但在预训练知识不足的新任务上效果不佳。研究发现，在元训练中过度包含思维链示例会降低性能，特别是在思维链监督有限的情况下。", "method": "使用CoT-ICL Lab框架进行受控研究，提出CoT-Recipe方法，这是一种正式的方法来调节元训练序列中思维链和非思维链示例的混合比例。", "result": "通过CoT-Recipe的精心调节，即使在没有思维链示例的上下文情况下，也能将transformer在新任务上的准确率提高高达300%。在预训练LLM（Qwen2.5系列）上应用这些技术，在符号推理任务中观察到准确率提升高达130%。", "conclusion": "思维链示例虽然有助于推理，但在元训练中过度使用会带来性能下降的隐藏成本。通过CoT-Recipe方法精心调节思维链和非思维链示例的比例，可以显著提升模型在新任务上的推理性能。"}}
{"id": "2512.05314", "pdf": "https://arxiv.org/pdf/2512.05314", "abs": "https://arxiv.org/abs/2512.05314", "authors": ["Ke Mao", "Timotej Kapus", "Cons T Åhs", "Matteo Marescotti", "Daniel Ip", "Ákos Hajdu", "Sopot Cela", "Aparup Banerjee"], "title": "WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 4 figures, 48th International Conference on Software Engineering: Software Engineering in Practice", "summary": "The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes. WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.", "AI": {"tldr": "WhatsCode是一个针对WhatsApp的大规模AI开发系统，通过AI辅助工具提升开发者效率，在合规相关的大规模工业环境中部署，支持超过20亿用户，处理数百万行代码。", "motivation": "学术文献中缺乏关于在合规相关的大规模工业环境中部署AI辅助开发工具的研究，尽管行业采用率在增长。WhatsApp需要处理大规模代码库和严格的隐私合规要求。", "method": "WhatsCode从针对性的隐私自动化演变为自主代理工作流，集成端到端功能开发和DevOps流程。系统采用两种稳定的人机协作模式：一键部署高置信度变更（60%）和指挥-修订复杂决策（40%）。", "result": "在25个月（2023-2025年）中，WhatsCode将自动化隐私验证覆盖率从15%提高到53%（提升3.5倍），识别隐私需求，生成超过3000个被接受的代码变更（接受率9%-100%），提交692个自动化重构/修复变更，711个框架采用，141个功能开发辅助，在错误分类中保持86%的精确度。", "conclusion": "组织因素（如所有权模型、采用动态和风险管理）与技术能力同样重要。有效的人机协作（而非完全自动化）驱动可持续的业务影响，为大规模AI工具在合规环境中的部署提供基于证据的指导。"}}
{"id": "2512.05311", "pdf": "https://arxiv.org/pdf/2512.05311", "abs": "https://arxiv.org/abs/2512.05311", "authors": ["Sadat Shahriar", "Navid Ayoobi", "Arjun Mukherjee"], "title": "The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?", "categories": ["cs.LG", "cs.AI"], "comment": "Published in RANLP 2025", "summary": "With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.", "AI": {"tldr": "评估最先进的机器学习模型在区分人类与LLM生成的科学想法方面的能力，特别是在经过连续改写阶段后，检测性能如何变化", "motivation": "随着LLM作为研究代理的日益普及，区分LLM与人类生成的想法对于理解LLM研究能力的认知差异变得至关重要。虽然检测LLM生成的文本已被广泛研究，但区分人类与LLM生成的科学想法仍是一个未探索的领域。", "method": "系统评估最先进的机器学习模型区分人类与LLM生成想法的能力，特别是在经过连续改写阶段后。研究还探讨了将研究问题作为上下文信息对检测性能的影响。", "result": "检测性能在经过五次连续改写后平均下降25.4%。将研究问题作为上下文信息可将检测性能提高最多2.97%。检测算法在想法被改写成简化的非专家风格时表现最差，这是导致可区分LLM特征消失的主要原因。", "conclusion": "LLM生成的科学想法在经过迭代改写后，其可检测特征会显著减弱，这给区分人类与LLM生成的想法带来了挑战。特别是当想法被简化改写时，检测变得尤为困难。"}}
{"id": "2512.05310", "pdf": "https://arxiv.org/pdf/2512.05310", "abs": "https://arxiv.org/abs/2512.05310", "authors": ["Brandon Biggs", "David Sloan", "Brett Oppegaard", "Nicholas A. Giudice", "James M. Coughlan", "Bruce N. Walker"], "title": "Systematically Evaluating Equivalent Purpose for Digital Maps", "categories": ["cs.HC"], "comment": "In press at Journal on Technology and Persons with Disabilities, volume 14", "summary": "Digital geographic maps remain largely inaccessible to blind and low-vision individuals (BLVIs), despite global legislation adopting the Web Content Accessibility Guidelines (WCAG). A critical gap exists in defining \"equivalent purpose\" for maps under WCAG Success Criterion 1.1.1, which requires that non-text content provide a text alternative that serves the \"equivalent purpose\". This paper proposes a systematic framework for evaluating map accessibility, called the Map Equivalent-Purpose Framework (MEP Framework), defining purpose through three items (Generalized, Spatial Information, and Spatial Relationships), and establishing 15 measurable criteria for equivalent information communication. Eight text map representations were evaluated against visual map baselines using the proposed MEP Framework. Results show that legacy methods such as tables and turn-by-turn directions fail to meet the MEP Framework criteria, while Audiom Maps, Multi User Domain (MUD) Maps, and Audio Descriptions meet the criteria. The evaluation highlights the necessity of holistic, systematic approaches to ensure non-visual maps convey all generalized spatial information and relationships present in visual maps. The MEP Framework provides a replicable methodology for comprehensively assessing digital map accessibility, clarifying WCAG's \"equivalent purpose\", and guiding compliant and usable map creation. Compliant maps will support BLVIs' participation in map-dependent professions and civic engagement.", "AI": {"tldr": "提出一个系统性的数字地图无障碍评估框架（MEP框架），用于评估数字地图是否能为视障人士提供与视觉地图\"等效目的\"的文本替代方案", "motivation": "数字地理地图对视障人士仍然难以访问，尽管全球立法采用了WCAG标准。WCAG成功标准1.1.1要求非文本内容提供具有\"等效目的\"的文本替代方案，但对于地图的\"等效目的\"缺乏明确定义，这是实现地图无障碍的关键障碍", "method": "提出了地图等效目的框架（MEP框架），通过三个项目（通用信息、空间信息、空间关系）定义地图目的，并建立了15个可衡量的等效信息沟通标准。使用该框架评估了8种文本地图表示方法相对于视觉地图基线的表现", "result": "传统方法如表格和逐向导航无法满足MEP框架标准，而音频地图、多用户领域地图和音频描述能够满足标准。评估强调了需要整体性、系统性的方法来确保非视觉地图传达视觉地图中的所有通用空间信息和关系", "conclusion": "MEP框架提供了一个可复制的全面评估数字地图无障碍性的方法，澄清了WCAG的\"等效目的\"概念，并指导创建符合标准且可用的地图。符合标准的地图将支持视障人士参与依赖地图的职业和公民参与"}}
{"id": "2512.05303", "pdf": "https://arxiv.org/pdf/2512.05303", "abs": "https://arxiv.org/abs/2512.05303", "authors": ["Christian Westerdahl", "Jonas Poulsen", "Daniel Holmelund", "Peter Nicholas Hansen", "Fletcher Thompson", "Roberto Galeazzi"], "title": "Seabed-to-Sky Mapping of Maritime Environments with a Dual Orthogonal SONAR and LiDAR Sensor Suite", "categories": ["cs.RO"], "comment": null, "summary": "Critical maritime infrastructure increasingly demands situational awareness both above and below the surface, yet existing ''seabed-to-sky'' mapping pipelines either rely on GNSS (vulnerable to shadowing/spoofing) or expensive bathymetric sonars. We present a unified, GNSS-independent mapping system that fuses LiDAR-IMU with a dual, orthogonally mounted Forward Looking Sonars (FLS) to generate consistent seabed-to-sky maps from an Autonomous Surface Vehicle. On the acoustic side, we extend orthogonal wide-aperture fusion to handle arbitrary inter-sonar translations (enabling heterogeneous, non-co-located models) and extract a leading edge from each FLS to form line-scans. On the mapping side, we modify LIO-SAM to ingest both stereo-derived 3D sonar points and leading-edge line-scans at and between keyframes via motion-interpolated poses, allowing sparse acoustic updates to contribute continuously to a single factor-graph map. We validate the system on real-world data from Belvederekanalen (Copenhagen), demonstrating real-time operation with approx. 2.65 Hz map updates and approx. 2.85 Hz odometry while producing a unified 3D model that spans air-water domains.", "AI": {"tldr": "提出一种GNSS独立的海床到天空统一测绘系统，融合LiDAR-IMU和正交安装的双前视声纳，从自主水面艇生成一致的海床到天空地图", "motivation": "关键海事基础设施需要水面和水下的态势感知，现有海床到天空测绘管道要么依赖GNSS（易受遮蔽/欺骗攻击），要么使用昂贵的测深声纳", "method": "1) 扩展正交宽孔径融合处理任意声纳间平移，从每个FLS提取前沿形成线扫描；2) 修改LIO-SAM以通过运动插值位姿在关键帧处和之间摄入立体声纳3D点和前沿线扫描，使稀疏声学更新能持续贡献到单一因子图地图", "result": "在哥本哈根Belvederekanalen的真实数据验证，系统实时运行，约2.65Hz地图更新和约2.85Hz里程计，生成跨越空气-水领域的统一3D模型", "conclusion": "提出了一种GNSS独立的统一测绘系统，融合LiDAR-IMU和正交声纳，实现了实时海床到天空地图生成，解决了现有系统对GNSS依赖或昂贵设备的问题"}}
{"id": "2512.05300", "pdf": "https://arxiv.org/pdf/2512.05300", "abs": "https://arxiv.org/abs/2512.05300", "authors": ["Yonggang Jiang", "Yaowei Long", "Thatchaphol Saranurak", "Benyu Wang"], "title": "Crude Approximation of Directed Minimum Cut and Arborescence Packing in Almost Linear Time", "categories": ["cs.DS"], "comment": null, "summary": "We give almost-linear-time algorithms for approximating rooted minimum cut and maximum arborescence packing in directed graphs, two problems that are dual to each other [Edm73]. More specifically, for an $n$-vertex, $m$-edge directed graph $G$ whose $s$-rooted minimum cut value is $k$, our first algorithm computes an $s$-rooted cut of size at most $O(k\\log^{5} n)$ in $m^{1+o(1)}$ time, and our second algorithm packs $k$ $s$-rooted arborescences with $n^{o(1)}$ congestion in $m^{1+o(1)}$ time, certifying that the $s$-rooted minimum cut is at least $k / n^{o(1)}$. Our first algorithm also works for weighted graphs. Prior to our work, the fastest algorithms for computing the $s$-rooted minimum cut were exact but had super-linear running time: either $\\tilde{O}(mk)$ [Gab91] or $\\tilde{O}(m^{1+o(1)}\\min\\{\\sqrt{n},n/m^{1/3}\\})$ [CLN+22]. The fastest known algorithms for packing $s$-rooted arborescences had no congestion, but required $\\tilde{O}(m \\cdot \\mathrm{poly}(k))$ time [BHKP08].", "AI": {"tldr": "本文提出了在有向图中近似计算根最小割和最大树状图填充的几乎线性时间算法", "motivation": "现有的根最小割精确算法需要超线性时间，而树状图填充算法要么需要多项式时间要么没有拥塞保证，需要更高效的近似算法", "method": "设计了两种几乎线性时间算法：1) 计算O(k log⁵ n)大小的根割；2) 以n^{o(1)}拥塞填充k个树状图", "result": "在m^{1+o(1)}时间内实现了近似根最小割和树状图填充，相比现有算法在时间效率上有显著提升", "conclusion": "首次实现了有向图中根最小割和树状图填充问题的几乎线性时间近似算法，解决了计算效率瓶颈"}}
{"id": "2512.05299", "pdf": "https://arxiv.org/pdf/2512.05299", "abs": "https://arxiv.org/abs/2512.05299", "authors": ["Ahmad Yehia", "Jiseop Byeon", "Tianyi Wang", "Huihai Wang", "Yiming Xu", "Junfeng Jiao", "Christian Claudel"], "title": "ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety", "categories": ["eess.SY", "cs.AR", "cs.CV", "cs.ET", "cs.RO", "eess.IV"], "comment": "8 pages, 3 figures, 1 table", "summary": "Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.", "AI": {"tldr": "开发AR碰撞避免系统ARCAS，通过增强现实技术为弱势道路使用者提供实时空间警报，提升交通安全", "motivation": "弱势道路使用者在混合交通中面临高碰撞风险，现有安全系统主要关注驾驶员或车辆辅助，缺乏直接针对弱势道路使用者的支持系统", "method": "融合路边360度3D LiDAR与SLAM头显跟踪技术，采用自动3D校准程序，在用户透视视图中准确叠加世界锁定的3D边界框和方向箭头，支持多头显协调共享世界锚定", "result": "在真实世界行人与电动滑板车、车辆交互的180次试验中，ARCAS几乎使行人的碰撞时间翻倍，并将对应方的反应余量提高达4倍", "conclusion": "验证了LiDAR驱动的AR引导的可行性和有效性，突显了可穿戴AR作为下一代城市移动安全工具的潜力"}}
{"id": "2512.05297", "pdf": "https://arxiv.org/pdf/2512.05297", "abs": "https://arxiv.org/abs/2512.05297", "authors": ["Xianglong Hou", "Xinquan Huang", "Paris Perdikaris"], "title": "CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators", "categories": ["cs.LG", "cs.AI", "math.NA"], "comment": null, "summary": "Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.", "AI": {"tldr": "提出Continuous Flow Operator (CFO)框架，通过流匹配学习连续时间PDE动力学，避免自回归预测的误差累积问题", "motivation": "传统神经算子方法使用自回归预测方案，在长时间推演中会累积误差，且需要均匀时间离散化。需要一种能够学习连续时间PDE动力学而不增加计算负担的方法", "method": "利用流匹配技术直接学习PDE的右侧项，无需通过ODE求解器反向传播。通过时间样条拟合轨迹数据，使用有限差分估计时间导数构造概率路径，其速度场近似真实PDE动力学。然后通过流匹配训练神经算子预测这些解析速度场", "result": "在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO表现出优越的长时间稳定性。仅使用25%不规则子采样时间点训练的CFO优于使用完整数据训练的自回归基线，相对误差减少高达87%。推理时仅需50%的函数评估即可超越基线", "conclusion": "CFO框架能够学习连续时间PDE动力学，具有时间分辨率不变性、优越的长时间稳定性、显著的数据效率，并支持反向时间推理和任意时间查询"}}
{"id": "2512.05292", "pdf": "https://arxiv.org/pdf/2512.05292", "abs": "https://arxiv.org/abs/2512.05292", "authors": ["Fan Zhang", "Jinfeng Chen", "Joseph J. B. Mvogo Ahanda", "Hanz Richter", "Ge Lv", "Bin Hu", "Qin Lin"], "title": "Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "In commercial robotic systems, it is common to encounter a closed inner-loop torque controller that is not user-modifiable. However, the outer-loop controller, which sends kinematic commands such as position or velocity for the inner-loop controller to track, is typically exposed to users. In this work, we focus on the development of an easily integrated add-on at the outer-loop layer by combining disturbance rejection control and robust control barrier function for high-performance tracking and safe control of the whole dynamic system of an industrial manipulator. This is particularly beneficial when 1) the inner-loop controller is imperfect, unmodifiable, and uncertain; and 2) the dynamic model exhibits significant uncertainty. Stability analysis, formal safety guarantee proof, and hardware experiments with a PUMA robotic manipulator are presented. Our solution demonstrates superior performance in terms of simplicity of implementation, robustness, tracking precision, and safety compared to the state of the art. Video: https://youtu.be/zw1tanvrV8Q", "AI": {"tldr": "针对工业机器人系统开发了一种易于集成的外环附加控制器，结合扰动抑制控制和鲁棒控制屏障函数，实现高性能跟踪和系统安全控制", "motivation": "商业机器人系统通常具有封闭的内环扭矩控制器，用户无法修改，而外环控制器对用户开放。当内环控制器不完善、不可修改且不确定，以及动态模型存在显著不确定性时，需要一种易于集成的外环解决方案", "method": "结合扰动抑制控制和鲁棒控制屏障函数，在外环层开发易于集成的附加控制器，通过稳定性分析和形式化安全保证证明确保系统性能", "result": "在PUMA机器人机械臂上进行硬件实验，相比现有技术，在实现简单性、鲁棒性、跟踪精度和安全性方面表现出优越性能", "conclusion": "提出的方法为具有封闭架构的工业机器人系统提供了一种有效的外环安全运动控制解决方案，特别适用于内环控制器不完善且动态模型不确定的场景"}}
{"id": "2512.05288", "pdf": "https://arxiv.org/pdf/2512.05288", "abs": "https://arxiv.org/abs/2512.05288", "authors": ["Feijiang Han"], "title": "Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.", "AI": {"tldr": "该论文提出了首个系统性的WebShell恶意软件家族分类自动化研究，通过动态函数调用追踪和行为分析，结合LLM数据增强，构建了全面的表示学习方法基准测试框架。", "motivation": "当前WebShell检测研究主要集中在区分恶意与良性样本，但缺乏对恶意软件家族的深入分析和自动化分类。手动专家分析效率低下，无法应对快速演变的威胁。需要从被动检测转向主动防御，通过自动化家族分类来理解攻击者战术并实现精准快速响应。", "method": "1) 提取动态函数调用轨迹以捕获抗加密和混淆的内在行为特征；2) 使用大语言模型合成新变种来增强数据集规模和多样性；3) 将轨迹抽象为序列、图和树结构；4) 基准测试全面的表示学习方法，包括经典序列嵌入、Transformer模型和各种结构感知算法。", "result": "在四个真实世界家族标注数据集上进行了监督和无监督设置的广泛实验，建立了稳健的基线，并为数据抽象、表示模型和学习范式的有效组合提供了实用见解。", "conclusion": "该研究填补了WebShell家族分类自动化的空白，为从被动检测转向深入分析和主动防御提供了系统框架，通过动态行为分析和综合表示学习方法的基准测试，为实际应用提供了有效指导。"}}
{"id": "2512.05277", "pdf": "https://arxiv.org/pdf/2512.05277", "abs": "https://arxiv.org/abs/2512.05277", "authors": ["Kevin Cannons", "Saeed Ranjbar Alvar", "Mohammad Asiful Hossain", "Ahmad Rezaei", "Mohsen Gholami", "Alireza Heidarikhazaei", "Zhou Weimin", "Yong Zhang", "Mohammad Akbari"], "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \\href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \\href{https://github.com/vbdi/tad_bench}{Github}, respectively.", "AI": {"tldr": "该论文提出了自动驾驶领域的首个时间理解基准测试TAD，用于评估视觉语言模型在自动驾驶场景中的时序推理能力，并提出了两种无需训练的方法来提升模型性能。", "motivation": "当前自动驾驶中的时间理解仍然是一个重大挑战，现有的时间推理基准主要关注体育、烹饪、电影等其他视频内容，缺乏专门针对自动驾驶场景的基准测试。自动驾驶的自我中心视角和动态场景变化需要专门的时间理解能力评估。", "method": "提出了TAD基准测试，包含近6000个问答对，涵盖7个人工设计的任务。提出了两种无需训练的解决方案：Scene-CoT（利用思维链）和TCogMap（整合自我中心时间认知地图），这些方法可以与现有视觉语言模型集成。", "result": "当前最先进的模型在TAD基准上表现不佳，主要由于细粒度运动理解不足。提出的Scene-CoT和TCogMap方法将TAD的平均准确率提升了高达17.72%。", "conclusion": "通过引入TAD基准、评估多个最先进模型并提出有效改进方法，这项工作旨在推动自动驾驶时间理解的未来研究，为解决自动驾驶中的时序推理挑战提供了新的工具和方向。"}}
{"id": "2512.05272", "pdf": "https://arxiv.org/pdf/2512.05272", "abs": "https://arxiv.org/abs/2512.05272", "authors": ["Ahmet Berke Gokmen", "Ajad Chhatkuli", "Luc Van Gool", "Danda Pani Paudel"], "title": "Inferring Compositional 4D Scenes without Ever Seeing One", "categories": ["cs.CV"], "comment": "Project page: https://github.com/insait-institute/COM4D", "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.", "AI": {"tldr": "提出COM4D方法，仅使用静态多物体或动态单物体监督，就能一致地联合预测4D/3D物体的结构和时空配置，无需4D组合训练数据", "motivation": "现实世界场景通常由多个静态和动态物体组成，但现有方法往往一次只关注一个物体，依赖类别特定的参数化形状模型，导致场景配置不一致且受限于建模类别", "method": "通过精心设计的空间和时间注意力机制在2D视频输入上进行训练，将学习解耦为物体组合学习和单物体动态学习，通过注意力混合机制在推理时结合独立学习的注意力", "result": "COM4D能够从单目视频直接重建完整且持久的4D场景，在4D物体重建和组合3D重建等现有分离问题上达到最先进结果", "conclusion": "COM4D方法避免了4D组合训练数据的依赖，通过解耦学习和注意力混合实现了对多物体交互4D场景的一致重建，为4D场景理解提供了新的数据驱动解决方案"}}
{"id": "2512.05270", "pdf": "https://arxiv.org/pdf/2512.05270", "abs": "https://arxiv.org/abs/2512.05270", "authors": ["Tianyi Wang", "Jiseop Byeon", "Ahmad Yehia", "Huihai Wang", "Yiming Xu", "Tianyi Zeng", "Ziran Wang", "Junfeng Jiao", "Christian Claudel"], "title": "XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "eess.SY"], "comment": "10 pages, 5 figures", "summary": "As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots' inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework's effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.", "AI": {"tldr": "XR-DT是一个扩展现实增强的数字孪生框架，用于实现人机双向理解，通过融合物理和虚拟空间来提升移动机器人在共享工作空间中的安全性、效率和可解释性。", "motivation": "随着移动机器人在共享工作空间中与人类协同工作，确保安全、高效和可解释的人机交互成为迫切挑战。现有研究主要关注人类行为预测，但人类如何感知、解释和信任机器人的推理却关注不足，这阻碍了在安全关键和社会嵌入环境中的部署。", "method": "提出分层XR-DT架构，集成虚拟现实、增强现实和混合现实层，融合实时传感器数据、Unity游戏引擎中的模拟环境和通过可穿戴AR设备捕获的人类反馈。设计了具有统一扩散策略的自主移动机器人系统用于上下文感知任务适应，并提出链式思维提示机制，让多模态大语言模型能够推理人类指令和环境上下文，同时利用基于AutoGen的多智能体协调层增强动态任务中的鲁棒性和协作。", "result": "初步实验结果表明，该系统能够准确预测人类和机器人的轨迹，验证了XR-DT框架在人机交互任务中的有效性。", "conclusion": "通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，该系统实现了可解释、可信赖和自适应的人机交互，为安全关键和社会嵌入环境中的机器人部署提供了有前景的解决方案。"}}
{"id": "2512.05268", "pdf": "https://arxiv.org/pdf/2512.05268", "abs": "https://arxiv.org/abs/2512.05268", "authors": ["Niki Nezakati", "Arnab Ghosh", "Amit Roy-Chowdhury", "Vishwanath Saragadam"], "title": "CARD: Correlation Aware Restoration with Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.", "AI": {"tldr": "提出CARD方法，一种无需训练、基于扩散模型的图像恢复方法，专门处理现实世界中常见的空间相关噪声问题。", "motivation": "现有扩散模型在图像恢复中通常假设噪声是独立同分布的高斯噪声，但现实传感器（如滚动快门传感器）产生的噪声具有空间相关性，这限制了现有方法的实际效果。", "method": "CARD首先对含噪观测进行白化处理，将相关噪声转换为独立同分布形式，然后用噪声白化更新替换扩散恢复步骤，继承了DDRM的闭式采样效率，同时能处理相关噪声。", "result": "在合成相关噪声的标准基准测试和新提出的CIN-D真实相关噪声数据集上，CARD在去噪、去模糊和超分辨率任务中均优于现有方法。", "conclusion": "CARD通过显式处理相关噪声，显著提升了扩散模型在真实世界图像恢复任务中的性能，同时贡献了CIN-D数据集填补了真实相关噪声评估的空白。"}}
{"id": "2512.05267", "pdf": "https://arxiv.org/pdf/2512.05267", "abs": "https://arxiv.org/abs/2512.05267", "authors": ["Osvaldo Simeone", "Yaniv Romano"], "title": "Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective", "categories": ["cs.IT", "cs.AI", "cs.LG"], "comment": null, "summary": "In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.", "AI": {"tldr": "本文从信息论视角系统综述了数据有限场景下人工智能的不确定性感知与数据效率提升方法，包括不确定性量化、数据增强和理论分析框架。", "motivation": "在机器人、通信、医疗等特定应用场景中，AI系统常面临训练数据有限的挑战，导致认知不确定性（可减少的不确定性）增加，从而限制预测性能。需要系统研究如何在数据稀缺条件下提升AI系统的数据效率和不确定性感知能力。", "method": "采用信息论视角，综述了四种主要方法：1）广义贝叶斯学习框架，通过广义后验在参数空间量化认知不确定性；2）信息论泛化边界，理论分析训练数据量与预测不确定性的关系；3）有限样本统计保证的不确定性量化方法，如保形预测和保形风险控制；4）结合有限标注数据与丰富模型预测或合成数据的数据效率提升方法。", "result": "论文系统梳理了数据有限场景下AI不确定性量化与数据效率提升的理论框架和方法体系，从信息论角度建立了数据稀缺与预测不确定性之间的理论联系，为实际应用提供了多种具有统计保证的解决方案。", "conclusion": "通过信息论视角，本文为数据有限场景下的AI系统提供了系统的理论框架和实用方法，强调了信息度量在量化数据稀缺影响中的核心作用，为未来研究指明了方向。"}}
{"id": "2512.05259", "pdf": "https://arxiv.org/pdf/2512.05259", "abs": "https://arxiv.org/abs/2512.05259", "authors": ["Georgios Chatzichristodoulou", "Niki Efthymiou", "Panagiotis Filntisis", "Georgios Pavlakos", "Petros Maragos"], "title": "Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization", "categories": ["cs.CV"], "comment": null, "summary": "While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.", "AI": {"tldr": "提出AionHMR框架，解决现有3D人体姿态估计方法在儿童和婴儿上泛化能力不足的问题，实现年龄包容性的人体网格重建，并用于动作保留的数据匿名化", "motivation": "现有3D人体姿态和形状估计方法在成人上表现良好，但在儿童和婴儿上泛化能力差，存在领域差距。同时需要隐私保护的数据匿名化方法，既能保留动作信息又能保护隐私", "method": "1. 基于优化的方法扩展现有最佳模型，整合SMPL-A人体模型，同时准确建模成人、儿童和婴儿；2. 为公开的儿童和婴儿图像数据库生成伪地面真值标注；3. 基于这些训练数据开发专门的基于transformer的深度学习模型，实现实时3D年龄包容性人体重建", "result": "方法显著提高了儿童和婴儿的形状和姿态估计精度，同时不影响成人的准确性。重建的网格作为原始图像的隐私保护替代品，保留了关键的动作、姿态和几何信息，支持匿名化数据集发布。还创建了3D-BabyRobot数据集，包含儿童与机器人互动的动作保留3D重建", "conclusion": "该工作填补了3D人体建模中的关键领域差距，为包容性、隐私感知和年龄多样化的3D人体建模奠定了基础，实现了既能保护隐私又能保留重要动作信息的年龄包容性人体重建"}}
{"id": "2512.05257", "pdf": "https://arxiv.org/pdf/2512.05257", "abs": "https://arxiv.org/abs/2512.05257", "authors": ["Bychkov Oleksii", "Bychkova Sophia", "Lytvynchuk Khrystyna"], "title": "Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence", "categories": ["cs.AI"], "comment": "9 pages", "summary": "This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.", "AI": {"tldr": "该论文提出可能性理论作为解决Dempster-Shafer理论悖论的基础，为可靠人工智能提供数学严谨的不确定性处理框架", "motivation": "解决Zadeh悖论和Dempster-Shafer理论中的逻辑不一致问题，为人工智能系统提供可靠的不确定性推理基础", "method": "采用Bychkov文章中发展的公理化可能性理论方法，使用可能性和必要性测度的对偶装置，从零开始构建逻辑一致的不确定性处理框架", "result": "通过经典医疗诊断困境的案例研究，证明可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能逻辑", "conclusion": "可能性理论不仅是Dempster-Shafer理论的替代方案，而且为解决其悖论提供了根本性解决方案，为可靠人工智能奠定了坚实基础"}}
{"id": "2512.05247", "pdf": "https://arxiv.org/pdf/2512.05247", "abs": "https://arxiv.org/abs/2512.05247", "authors": ["Spencer Gibson", "Yun William Yu"], "title": "Incorporating indel channels into average-case analysis of seed-chain-extend", "categories": ["cs.DS", "q-bio.QM"], "comment": "25 pages (10 page main text + 2 page biblio + 13 page appendix); conference submission", "summary": "Given a sequence $s_1$ of $n$ letters drawn i.i.d. from an alphabet of size $σ$ and a mutated substring $s_2$ of length $m < n$, we often want to recover the mutation history that generated $s_2$ from $s_1$. Modern sequence aligners are widely used for this task, and many employ the seed-chain-extend heuristic with $k$-mer seeds. Previously, Shaw and Yu showed that optimal linear-gap cost chaining can produce a chain with $1 - O\\left(\\frac{1}{\\sqrt{m}}\\right)$ recoverability, the proportion of the mutation history that is recovered, in $O\\left(mn^{2.43θ} \\log n\\right)$ expected time, where $θ< 0.206$ is the mutation rate under a substitution-only channel and $s_1$ is assumed to be uniformly random. However, a gap remains between theory and practice, since real genomic data includes insertions and deletions (indels), and yet seed-chain-extend remains effective. In this paper, we generalize those prior results by introducing mathematical machinery to deal with the two new obstacles introduced by indel channels: the dependence of neighboring anchors and the presence of anchors that are only partially correct. We are thus able to prove that the expected recoverability of an optimal chain is $\\ge 1 - O\\Bigl(\\frac{1}{\\sqrt{m}}\\Bigr)$ and the expected runtime is $O(mn^{3.15 \\cdot θ_T}\\log n)$, when the total mutation rate given by the sum of the substitution, insertion, and deletion mutation rates ($θ_T = θ_i + θ_d + θ_s$) is less than $0.159$.", "AI": {"tldr": "将indel通道纳入seed-chain-extend平均情况分析的理论研究", "motivation": "现有理论分析仅考虑替换突变，而实际基因组数据包含插入和删除(indels)，理论与实际存在差距，需要将indel通道纳入分析", "method": "引入数学工具处理indel通道带来的两个新障碍：相邻锚点的依赖性和部分正确锚点的存在，分析最优链的期望可恢复性和时间复杂度", "result": "证明当总突变率θ_T < 0.159时，最优链的期望可恢复性≥1 - O(1/√m)，期望运行时间为O(mn^{3.15·θ_T}log n)", "conclusion": "成功将indel通道纳入seed-chain-extend的平均情况分析，填补了理论与实际之间的差距，为包含indels的真实基因组数据分析提供了理论支持"}}
{"id": "2512.05246", "pdf": "https://arxiv.org/pdf/2512.05246", "abs": "https://arxiv.org/abs/2512.05246", "authors": ["Ankit Gupta", "Onur Dizdar", "Yun Chen", "Fehmi Emre Kadan", "Ata Sattarzadeh", "Stephen Wang"], "title": "NeuromorphicRx: From Neural to Spiking Receiver", "categories": ["cs.NE", "cs.IT"], "comment": null, "summary": "In this work, we propose a novel energy-efficient spiking neural network (SNN)-based receiver for 5G-NR OFDM system, called neuromorphic receiver (NeuromorphicRx), replacing the channel estimation, equalization and symbol demapping blocks. We leverage domain knowledge to design the input with spiking encoding and propose a deep convolutional SNN with spike-element-wise residual connections. We integrate an SNN with artificial neural network (ANN) hybrid architecture to obtain soft outputs and employ surrogate gradient descent for training. We focus on generalization across diverse scenarios and robustness through quantized aware training. We focus on interpretability of NeuromorphicRx for 5G-NR signals and perform detailed ablation study for 5G-NR signals. Our extensive numerical simulations show that NeuromorphicRx is capable of achieving significant block error rate performance gain compared to 5G-NR receivers and similar performance compared to its ANN-based counterparts with 7.6x less energy consumption.", "AI": {"tldr": "提出一种基于脉冲神经网络的能量高效5G-NR OFDM接收器NeuromorphicRx，替代传统信道估计、均衡和符号解映射模块", "motivation": "传统5G-NR接收器能耗较高，需要开发更节能的接收方案；脉冲神经网络在能耗效率方面具有优势，但需要解决其在通信系统中的应用挑战", "method": "1) 利用领域知识设计脉冲编码输入；2) 构建具有脉冲元素级残差连接的深度卷积SNN；3) 采用SNN-ANN混合架构获取软输出；4) 使用代理梯度下降进行训练；5) 通过量化感知训练增强鲁棒性", "result": "NeuromorphicRx相比传统5G-NR接收器在块错误率性能上有显著提升，与基于ANN的对应方案性能相当，但能耗降低7.6倍；具有良好的泛化能力和鲁棒性", "conclusion": "NeuromorphicRx成功展示了SNN在5G通信系统中的实际应用潜力，为开发能量高效的下一代无线接收器提供了新途径，同时保持了高性能和可解释性"}}
{"id": "2512.05242", "pdf": "https://arxiv.org/pdf/2512.05242", "abs": "https://arxiv.org/abs/2512.05242", "authors": ["Uwe M. Borghoff", "Mark Minas", "Jannis Schopp"], "title": "Learning to Code with Context: A Study-Based Approach", "categories": ["cs.SE", "cs.AI"], "comment": "36 pages, 7 figures, 5 tables", "summary": "The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.", "AI": {"tldr": "研究基于项目的编程课程中生成式AI工具的使用情况，并开发了一个基于RAG的上下文感知LLM助手系统", "motivation": "生成式AI工具正在改变软件开发方式，软件工程教育需要适应这一变化，确保学生不仅学习传统开发方法，还能有意义且负责任地使用新技术", "method": "在大学编程项目中开展用户研究，让学生协作开发电脑游戏，观察他们如何使用生成式AI工具；同时开发并测试一个基于RAG的本地部署LLM助手系统，该系统能够利用项目文档和源代码提供上下文支持", "result": "研究发现学生在软件开发过程的不同阶段如何使用生成式AI工具，识别了这些工具最有效的任务类型，分析了学生遇到的挑战；同时通过RAG系统分析了模型行为、参数敏感性和常见失败模式", "conclusion": "研究加深了对教育软件项目中上下文感知AI支持的理解，为未来将AI辅助工具整合到软件工程课程中提供了指导"}}
{"id": "2512.05240", "pdf": "https://arxiv.org/pdf/2512.05240", "abs": "https://arxiv.org/abs/2512.05240", "authors": ["Dmitrii Torbunov", "Onur Okuducu", "Yi Huang", "Odera Dim", "Rebecca Coles", "Yonggang Cui", "Yihui Ren"], "title": "IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.", "AI": {"tldr": "提出IE2Video任务：从单个初始RGB帧和后续事件相机数据重建RGB视频序列，通过混合捕获范式（稀疏RGB关键帧+连续事件流）降低功耗同时保持标准视频输出", "motivation": "传统RGB相机在连续视频监控中功耗高，事件相机功耗低但输出异步事件流而非标准视频，需要一种既能降低捕获功耗又能生成标准视频的方法", "method": "研究两种架构策略：1）基于自回归模型的HyperE2VID用于RGB生成；2）通过学习的编码器和低秩适配将事件表示注入预训练的文本到视频扩散模型（LTX）", "result": "基于扩散的方法比自回归基线在感知质量上提升33%（LPIPS 0.283 vs 0.422），在三个事件相机数据集（BS-ERGB, HS-ERGB far/close）上验证，支持32-128帧序列长度，展示跨数据集泛化能力", "conclusion": "提出的混合捕获范式能有效降低功耗，基于预训练扩散模型的方法在事件到视频重建任务中表现优异，为低功耗视频监控系统提供了可行方案"}}
{"id": "2512.05239", "pdf": "https://arxiv.org/pdf/2512.05239", "abs": "https://arxiv.org/abs/2512.05239", "authors": ["Ruofan Gao", "Amjed Tahir", "Peng Liang", "Teo Susnjak", "Foutse Khomh"], "title": "A Survey of Bugs in AI-Generated Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.", "AI": {"tldr": "对AI生成代码中的bug进行系统性调查和分类分析", "motivation": "AI代码生成模型被广泛使用以提高生产力，但生成的代码存在质量问题。现有研究分散且缺乏系统性总结，需要全面了解AI生成代码中的bug类型、分布、修复策略及其与特定模型的关联。", "method": "对现有AI生成代码文献进行系统性分析，建立对生成代码中bug和缺陷的整体理解，提供bug类型和模式的分类，并讨论不同模型的bug特征。", "result": "建立了AI生成代码中bug的系统性分类框架，揭示了不同模型生成的代码中bug的类型分布和模式特征，总结了现有的bug修复和缓解策略。", "conclusion": "AI生成代码中存在多种类型的bug，需要系统性的质量评估和改进策略。该调查为未来模型改进和质量评估提供了参考框架，有助于提高AI代码生成的可信度和维护性。"}}
{"id": "2512.05234", "pdf": "https://arxiv.org/pdf/2512.05234", "abs": "https://arxiv.org/abs/2512.05234", "authors": ["Felix Mulitze", "Herbert Woisetschläger", "Hans Arno Jacobsen"], "title": "MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the peer-reviewed AI4NextG Workshop at NeurIPS 2025", "summary": "The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.", "AI": {"tldr": "提出MAR-FL系统，一种通信高效的P2P联邦学习系统，通过迭代分组聚合显著降低通信开销，同时保持对网络动态变化的鲁棒性", "motivation": "下一代无线系统与分布式机器学习融合需要高效且鲁棒的联邦学习方法，现有P2P FL方法通信复杂度过高，限制了实际可扩展性", "method": "采用迭代分组聚合机制，将节点分组进行局部聚合，然后通过多轮迭代实现全局模型收敛，避免全连接通信", "result": "将通信复杂度从O(N^2)降低到O(N log N)，在节点数量增加时仍能保持有效性，同时具备对不可靠客户端和网络动态的鲁棒性", "conclusion": "MAR-FL系统通过创新的分组聚合策略，在保持P2P FL去中心化优势的同时，显著降低了通信开销，提高了系统的可扩展性和实用性"}}
{"id": "2512.05230", "pdf": "https://arxiv.org/pdf/2512.05230", "abs": "https://arxiv.org/abs/2512.05230", "authors": ["Jonathan Yang", "Chelsea Finn", "Dorsa Sadigh"], "title": "Invariance Co-training for Robot Visual Generalization", "categories": ["cs.RO", "cs.AI"], "comment": "14 pages, 10 figures", "summary": "Reasoning from diverse observations is a fundamental capability for generalist robot policies to operate in a wide range of environments. Despite recent advancements, many large-scale robotic policies still remain sensitive to key sources of observational variation such as changes in camera perspective, lighting, and the presence of distractor objects. We posit that the limited generalizability of these models arises from the substantial diversity required to robustly cover these quasistatic axes, coupled with the current scarcity of large-scale robotic datasets that exhibit rich variation across them. In this work, we propose to systematically examine what robots need to generalize across these challenging axes by introducing two key auxiliary tasks, state similarity and invariance to observational perturbations, applied to both demonstration data and static visual data. We then show that via these auxiliary tasks, leveraging both more-expensive robotic demonstration data and less-expensive, visually rich synthetic images generated from non-physics-based simulation (for example, Unreal Engine) can lead to substantial increases in generalization to unseen camera viewpoints, lighting configurations, and distractor conditions. Our results demonstrate that co-training on this diverse data improves performance by 18 percent over existing generative augmentation methods. For more information and videos, please visit https://invariance-cotraining.github.io", "AI": {"tldr": "该论文提出了一种通过辅助任务（状态相似性和观测扰动不变性）来提升机器人视觉泛化能力的方法，利用机器人演示数据和视觉丰富的合成图像进行协同训练，以应对相机视角、光照和干扰物等观测变化。", "motivation": "当前大规模机器人策略在面对相机视角变化、光照变化和干扰物存在等观测变化时泛化能力有限，主要原因是覆盖这些变化轴所需的多样性数据稀缺，而现有机器人数据集在这些方面的变化不够丰富。", "method": "提出了两个关键辅助任务：状态相似性和观测扰动不变性，应用于演示数据和静态视觉数据。通过协同训练利用昂贵的机器人演示数据和廉价的、视觉丰富的合成图像（来自非物理仿真如Unreal Engine），提升模型对观测变化的鲁棒性。", "result": "该方法在未见过的相机视角、光照配置和干扰物条件下的泛化性能显著提升，比现有生成增强方法提高了18%的性能。", "conclusion": "通过引入状态相似性和观测扰动不变性辅助任务，并结合机器人演示数据和合成图像进行协同训练，可以有效提升机器人策略对观测变化的泛化能力，为解决机器人视觉泛化问题提供了新思路。"}}
{"id": "2512.05229", "pdf": "https://arxiv.org/pdf/2512.05229", "abs": "https://arxiv.org/abs/2512.05229", "authors": ["Yanis Lahrach", "Christian Hughes", "Ian Abraham"], "title": "Search at Scale: Improving Numerical Conditioning of Ergodic Coverage Optimization for Multi-Scale Domains", "categories": ["cs.RO"], "comment": null, "summary": "Recent methods in ergodic coverage planning have shown promise as tools that can adapt to a wide range of geometric coverage problems with general constraints, but are highly sensitive to the numerical scaling of the problem space. The underlying challenge is that the optimization formulation becomes brittle and numerically unstable with changing scales, especially under potentially nonlinear constraints that impose dynamic restrictions, due to the kernel-based formulation. This paper proposes to address this problem via the development of a scale-agnostic and adaptive ergodic coverage optimization method based on the maximum mean discrepancy metric (MMD). Our approach allows the optimizer to solve for the scale of differential constraints while annealing the hyperparameters to best suit the problem domain and ensure physical consistency. We also derive a variation of the ergodic metric in the log space, providing additional numerical conditioning without loss of performance. We compare our approach with existing coverage planning methods and demonstrate the utility of our approach on a wide range of coverage problems.", "AI": {"tldr": "提出一种尺度无关的自适应遍历覆盖优化方法，基于最大均值差异度量，改善多尺度领域中数值条件化问题", "motivation": "现有遍历覆盖规划方法对问题空间的数值缩放高度敏感，优化公式在尺度变化时变得脆弱且数值不稳定，尤其是在非线性约束下", "method": "基于最大均值差异度量开发尺度无关的自适应遍历覆盖优化方法，允许优化器求解微分约束的尺度，同时退火超参数以适应问题域；在log空间推导遍历度量的变体以提供额外数值条件化", "result": "与现有覆盖规划方法相比，该方法在广泛的覆盖问题上展示了其效用，改善了数值稳定性", "conclusion": "提出的方法解决了遍历覆盖优化中的尺度敏感性问题，通过自适应优化和数值条件化改进，为多尺度领域提供了更稳健的解决方案"}}
{"id": "2512.05225", "pdf": "https://arxiv.org/pdf/2512.05225", "abs": "https://arxiv.org/abs/2512.05225", "authors": ["Patrizio Angelini", "Michael A. Bekos", "Giuseppe Di Battista", "Fabrizio Frati", "Luca Grilli", "Giacomo Ortali"], "title": "On Planar Straight-Line Dominance Drawings", "categories": ["cs.CG", "cs.DS"], "comment": "A preliminary version appears at WADS '25", "summary": "We study the following question, which has been considered since the 90's: Does every $st$-planar graph admit a planar straight-line dominance drawing? We show concrete evidence for the difficulty of this question, by proving that, unlike upward planar straight-line drawings, planar straight-line dominance drawings with prescribed $y$-coordinates do not always exist and planar straight-line dominance drawings cannot always be constructed via a contract-draw-expand inductive approach. We also show several classes of $st$-planar graphs that always admit a planar straight-line dominance drawing. These include $st$-planar $3$-trees in which every stacking operation introduces two edges incoming into the new vertex, $st$-planar graphs in which every vertex is adjacent to the sink, $st$-planar graphs in which no face has the left boundary that is a single edge, and $st$-planar graphs that have a leveling with span at most two.", "AI": {"tldr": "研究平面直线支配图的存在性问题，特别是针对st-平面图是否总是存在平面直线支配图这一长期开放问题", "motivation": "自90年代以来，研究者一直探讨st-平面图是否总是存在平面直线支配图。这个问题在平面图绘制领域具有重要意义，因为支配图能够直观展示有向图中的支配关系", "method": "通过证明某些构造方法的局限性来展示问题的难度：1) 证明具有指定y坐标的平面直线支配图不一定存在；2) 证明收缩-绘制-扩展的归纳方法不总是可行。同时识别了几类总是存在这种绘制的st-平面图", "result": "证明了问题的困难性，并确定了四类总是存在平面直线支配图的st-平面图：1) 每个堆叠操作引入两条指向新顶点边的st-平面3-树；2) 每个顶点都与汇点相邻的st-平面图；3) 没有面以单边作为左边界的st-平面图；4) 具有跨度不超过2的水平化的st-平面图", "conclusion": "st-平面图是否总是存在平面直线支配图的问题仍然开放且具有挑战性。虽然证明了某些构造方法的局限性，但也识别了多个总是存在这种绘制的图类，为理解这一问题的边界提供了重要见解"}}
{"id": "2512.05212", "pdf": "https://arxiv.org/pdf/2512.05212", "abs": "https://arxiv.org/abs/2512.05212", "authors": ["Georgios Mappouras", "Charalambos Rossides"], "title": "On the Computability of Artificial General Intelligence", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.", "AI": {"tldr": "该论文探讨了人工通用智能（AGI）的可计算性，通过形式化证明表明任何算法（包括AI模型）都无法展示其初始算法中不存在的全新功能能力，从而为AGI设定了计算上限。", "motivation": "近年来AI快速发展，人们开始关注人类何时能开发出达到人类智能水平的AGI。论文旨在通过计算理论的角度，探索任何机器可计算过程（算法）的上限，特别是针对AGI能否真正实现创造性突破的问题。", "method": "采用先前研究中关于AGI的定义（即在某个领域中以创造性方式创新，解锁该领域中新的、先前未知的功能能力）。基于此定义，通过形式化证明方法，论证任何算法都无法展示其初始算法中不存在的全新功能能力。", "result": "形式化证明表明：1）没有算法能够展示其初始算法中不存在的全新功能能力；2）因此，没有算法（包括AI模型）能在任何研究领域（科学、工程、艺术、体育等）中实现真正的创造性；3）AI模型只能展示现有功能能力及其组合与排列。", "conclusion": "该证明对AI发展的未来具有重要意义，表明AI无法实现真正的创造性突破，只能基于现有能力进行组合。这一结论也对人类智能的起源提出了新的思考，暗示人类智能可能具有超越算法计算能力的特性。"}}
{"id": "2512.05211", "pdf": "https://arxiv.org/pdf/2512.05211", "abs": "https://arxiv.org/abs/2512.05211", "authors": ["Ioannis Mandralis", "Severin Schumacher", "Morteza Gharib"], "title": "Wake Vectoring for Efficient Morphing Flight", "categories": ["cs.RO"], "comment": null, "summary": "Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.", "AI": {"tldr": "该论文提出了一种被动尾流转向机制，用于在变形飞行机器人进行空中重构时恢复垂直推力，从而提高变形飞行效率。", "motivation": "变形飞行机器人在空中重构时面临关键的气动挑战：倾斜推进器以实现形状变化会减少垂直推力，从而削弱稳定性和控制能力。需要一种解决方案来在变形过程中恢复失去的推力。", "method": "设计了一种被动尾流转向机制，通过内部偏转器拦截并重定向转子尾流向下，被动引导原本会被浪费的气流动量。该方案无需电子设备，集成到新型机器人系统ATMO中。", "result": "在原本不会产生有用推力的配置中，实现了高达40%的垂直推力恢复，显著扩展了变形过程中的悬停和机动能力。", "conclusion": "研究为变形飞行机器人设计指明了新方向，即通过受火箭和飞机推力转向启发的被动气动结构，实现高效、敏捷的飞行，而无需增加机械复杂性。"}}
{"id": "2512.05209", "pdf": "https://arxiv.org/pdf/2512.05209", "abs": "https://arxiv.org/abs/2512.05209", "authors": ["Vsevolod Plohotnuk", "Artyom Panshin", "Nikola Banić", "Simone Bianco", "Michael Freeman", "Egor Ershov"], "title": "DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).", "AI": {"tldr": "提出了DEAR数据集，这是第一个基于人类主观偏好系统评估图像渲染美学质量的数据集，用于超越传统图像质量评估的渲染美学评估任务", "motivation": "传统图像质量评估主要关注技术退化（如噪声、模糊、压缩伪影），但渲染美学评估在摄影编辑、内容创作和AI生成图像等领域日益重要，由于缺乏反映主观风格偏好的数据集，这一领域尚未得到充分探索", "method": "基于MIT-Adobe FiveK数据集构建，通过大规模众包收集成对人类偏好评分，每个图像对由25个不同的人类评估者评估，共有13,648名评估者参与，总共收集了大量成对偏好数据", "result": "创建了DEAR数据集，包含捕捉细微、上下文敏感美学偏好的人类标注，支持风格偏好预测、美学基准测试和个性化美学建模等多种应用，并在HuggingFace上发布了包含100个图像及其标注的子集", "conclusion": "DEAR是第一个系统解决基于人类主观偏好的图像渲染美学评估的数据集，为超越传统失真评估的美学质量评估研究提供了重要资源"}}
{"id": "2512.05201", "pdf": "https://arxiv.org/pdf/2512.05201", "abs": "https://arxiv.org/abs/2512.05201", "authors": ["Ali Al Housseini", "Jaime Llorca", "Luca Turchet", "Tiziano Leidi", "Cristina Rottondi", "Omran Ayoub"], "title": "MuMeNet: A Network Simulator for Musical Metaverse Communications", "categories": ["cs.NI", "cs.SD"], "comment": "To be published in 2025 IEEE 6th International Symposium on the Internet of Sounds (IS2)", "summary": "The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using \"live audience interaction in a virtual concert\" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.", "AI": {"tldr": "开发MuMeNet网络模拟器，专门用于模拟和分析音乐元宇宙（Musical Metaverse）在5G/6G网络中的通信需求和服务提供问题", "motivation": "音乐元宇宙（如虚拟音乐会、协作作曲等）正在快速发展，但底层网络和服务基础设施的要求限制了其增长。现有模拟工具无法捕捉音乐元宇宙会话的交互性、异构性和多播导向特性，需要专门的分析工具", "method": "1. 形式化建模音乐元宇宙的服务和网络图模型，以\"虚拟音乐会中的实时观众互动\"为参考场景；2. 开发MuMeNet离散事件网络模拟器，专门针对音乐元宇宙的需求和流量动态；3. 在参考场景上运行基于线性编程的编排策略进行性能分析", "result": "成功开发了MuMeNet模拟器，能够有效模拟音乐元宇宙的通信需求。通过在实际音乐元宇宙工作负载下运行线性编程编排策略，展示了模拟器的有效性，并提供了性能分析", "conclusion": "MuMeNet是针对音乐元宇宙通信需求的第一个专门网络模拟器，为5G/6G网络中音乐元宇宙会话的服务提供问题提供了建模和分析工具，有助于推动音乐元宇宙的发展"}}
{"id": "2512.05198", "pdf": "https://arxiv.org/pdf/2512.05198", "abs": "https://arxiv.org/abs/2512.05198", "authors": ["Rowan Bradbury", "Dazhi Zhong"], "title": "Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.", "AI": {"tldr": "提出像素等效潜在合成（PELC）原则，通过DecFormer实现扩散模型中真正的像素空间等效潜在合成，解决传统线性插值方法在掩码边界产生的伪影和颜色偏移问题。", "motivation": "当前扩散模型中的潜在修复主要依赖在降采样掩码下对VAE潜在进行线性插值，这种方法无法实现像素空间等效合成，导致掩码边界出现明显伪影、全局颜色偏移和退化。", "method": "提出像素等效潜在合成（PELC）原则，并设计DecFormer——一个770万参数的Transformer模型，预测每通道的混合权重和离流形残差校正，实现掩码一致的潜在融合，确保解码后结果与像素空间alpha合成完全匹配。", "result": "在FLUX.1系列模型上，DecFormer恢复了全局颜色一致性、软掩码支持、锐利边界和高保真掩码处理，将边缘误差指标降低了高达53%。作为修复先验，轻量级LoRA结合DecFormer能达到与完全微调的FLUX.1-Fill模型相当的保真度。", "conclusion": "PELC为像素等效潜在编辑提供了通用解决方案，不仅显著提升了扩散模型修复质量，还能扩展到复杂颜色校正等更广泛的潜在编辑任务。"}}
{"id": "2512.05179", "pdf": "https://arxiv.org/pdf/2512.05179", "abs": "https://arxiv.org/abs/2512.05179", "authors": ["Aurélie Montfrond"], "title": "Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages, 2 figures", "summary": "Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.", "AI": {"tldr": "该论文研究了通过微调BERT模型来构建面向大学教育领域的特定领域问答系统，针对利默里克大学电子与计算机工程系的课程信息提供学生咨询服务。", "motivation": "现有的科学问答研究主要集中在聊天机器人系统，缺乏对基础模型在特定领域推理能力的微调探索。目前存在针对生物医学和科学文献的领域特定BERT变体（如BioBERT和SciBERT），但还没有专门针对大学课程材料的基础模型。该研究旨在填补这一空白，展示将基础模型适应教育领域的可行性。", "method": "1. 构建包含1,203个问答对的自定义数据集，采用SQuAD格式，数据来源于大学模块手册，并通过人工和合成方式补充生成；2. 使用PyTorch对BERT模型进行微调；3. 使用精确匹配（Exact Match）和F1分数评估模型性能。", "result": "结果显示，即使是适度的微调也能显著改善假设框架和知识提取能力。实验证明了将基础模型适应教育领域的可行性，微调后的BERT模型在学术问答对上的表现有效，为构建首个面向大学的领域特定问答模型奠定了基础。", "conclusion": "该研究表明，通过微调BERT模型处理学术问答对可以获得有效结果，突出了向首个大学领域特定问答模型扩展的潜力，为实现自主教育知识系统提供了可能。这项工作为教育自然语言处理资源在大学规模的应用开辟了新途径。"}}
{"id": "2512.05176", "pdf": "https://arxiv.org/pdf/2512.05176", "abs": "https://arxiv.org/abs/2512.05176", "authors": ["Brittany Johnson", "Erin Reddick", "Angela D. R. Smith"], "title": "Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as \"general purpose\" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of \"culturally-informed\" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.", "AI": {"tldr": "开发一个文化智能和价值观推理质量基准（CIVIQ），用于评估LLM与社区社会价值观和常识的对齐程度，以解决现有基准主要关注国家层面而忽视美国内部多元文化的问题。", "motivation": "当前LLM主要对齐西方白人主流叙事，与边缘化文化和群体的经验和视角存在错位。虽然已有\"文化知情\"LLM（如ChatBlackGPT）的开发，但缺乏有效的开发和评估工具。现有的国家对齐基准（如KorNAT）在美国多元文化背景下效果有限。", "method": "采用复制研究方法，将韩国国家LLM对齐基准KorNAT的开发过程进行转化，应用于美国多元文化背景，开发CIVIQ基准，重点关注社区层面的社会价值观和常识对齐。", "result": "提出了CIVIQ基准框架，为实践中AI技术的文化对齐研究和开发提供了关键基础，支持更广泛的文化代表性评估。", "conclusion": "CIVIQ基准填补了美国多元文化背景下LLM文化对齐评估的空白，为开发更具包容性和代表性的AI技术提供了重要工具，促进了边缘化文化和群体在AI系统中的更好体现。"}}
{"id": "2512.05172", "pdf": "https://arxiv.org/pdf/2512.05172", "abs": "https://arxiv.org/abs/2512.05172", "authors": ["Wentao Wang", "Chunyang Liu", "Kehua Sheng", "Bo Zhang", "Yan Wang"], "title": "Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.", "AI": {"tldr": "提出Semore框架，一种基于视觉语言模型（VLM）的视觉强化学习方法，通过双路径骨干网络从RGB流中同时提取语义和运动表示", "motivation": "现有基于LLM的RL方法主要关注控制策略指导，面临骨干网络表示能力有限的问题，需要同时提取语义和运动信息来增强视觉RL效果", "method": "使用VLM从观察中提取关键信息，利用预训练的CLIP实现文本-图像对齐，通过双路径骨干网络分别提取语义和运动表示，采用分离监督方法指导表示提取并允许自发交互", "result": "在VLM特征级指导下，该方法相比最先进方法展现出高效和自适应能力，实验验证了其优越性能", "conclusion": "Semore框架通过VLM指导的增强语义运动表示，有效解决了视觉RL中表示能力有限的问题，为视觉强化学习提供了新的解决方案"}}
{"id": "2512.05171", "pdf": "https://arxiv.org/pdf/2512.05171", "abs": "https://arxiv.org/abs/2512.05171", "authors": ["Aleksandr Abramov"], "title": "Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry", "categories": ["eess.IV", "cs.RO"], "comment": null, "summary": "Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.", "AI": {"tldr": "提出一种用于多相机系统的两阶段标定方法，利用场景几何特征进行标定，无需精确平面图、物理访问放置标定图案或同步视频流。", "motivation": "传统多相机系统标定方法在现实条件下存在局限性：缺乏精确平面图、无法物理访问放置标定图案、或无法获取同步视频流。这些限制使得在真实世界环境中部署精确的多相机跟踪系统变得困难。", "method": "采用两阶段标定方法：第一阶段基于操作员标注的自然几何基元（平行线、垂直线、等长线段）进行单个相机的部分标定，估计关键参数并投影有效视场到水平面；第二阶段通过交互操作投影的多边形，调整位置、尺度和旋转以对齐平面图或虚拟标定元素，确定剩余外参。", "result": "该方法仅需每个相机的静态图像，无需物理访问或同步视频。实现为实用的Web服务，比较分析和演示视频证实了方法的适用性、准确性和灵活性。", "conclusion": "该方法克服了传统方法的限制，使得在先前认为不可行的场景中部署精确的多相机跟踪系统成为可能。"}}
{"id": "2512.05169", "pdf": "https://arxiv.org/pdf/2512.05169", "abs": "https://arxiv.org/abs/2512.05169", "authors": ["Abdelmalik Moujahid", "Fadi Dornaika"], "title": "Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Artif Intell Rev 58, 234 (2025)", "summary": "Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.", "AI": {"tldr": "对多视图聚类（MVC）技术进行全面综述，系统分类现有方法，分析优缺点，并探讨未来研究方向", "motivation": "单视图学习方法存在局限性，无法充分利用多源、多领域数据的丰富语义信息；多视图数据虽然复杂但实用价值更高，需要系统梳理MVC技术以推动领域发展", "method": "系统回顾140多篇文献，将MVC方法分为七大类：协同训练、协同正则化、子空间、深度学习、核方法、锚点法和图方法；分析早期融合、晚期融合和联合学习等集成策略", "result": "建立了MVC方法的系统分类框架，识别了各类方法的优缺点和实际挑战（如可扩展性、不完整数据处理），提供了在医疗、多媒体和社交网络分析等领域的应用案例", "conclusion": "多视图聚类是克服单视图方法局限性的有效途径，未来需要在跨学科应用、新兴趋势和实际挑战方面进一步研究，以推动该领域的持续发展"}}
{"id": "2512.05167", "pdf": "https://arxiv.org/pdf/2512.05167", "abs": "https://arxiv.org/abs/2512.05167", "authors": ["Fang Li"], "title": "Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education", "categories": ["cs.AI"], "comment": "Accepted by the 39th annual Consortium for Computing Sciences in Colleges (CCSC:SE)", "summary": "This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.", "AI": {"tldr": "设计一个两部分的AI课程，系统性地连接传统机器学习与现代大语言模型，为学生提供从基础到前沿的全面AI教育", "motivation": "传统AI教育往往将机器学习与新兴的大语言模型技术割裂开来，学生难以理解AI技术的演进脉络，也无法全面掌握从传统到现代的完整技能体系。需要一种创新的教学方法来弥合这一鸿沟。", "method": "设计了一个两部分的课程结构：第一部分教授基础机器学习概念，第二部分专注于当代大语言模型应用。课程采用顺序互补的设计，包含详细的课程架构、实施策略、评估方法，并在两个七周的夏季学期中实施。", "result": "该集成教学方法增强了学生对AI领域全景的理解，更好地为他们应对快速发展的AI行业需求做好准备。课程实施结果显示学生能够全面掌握从传统到现代的AI技术。", "conclusion": "这种两部分的课程设计成功地连接了传统机器学习与现代大语言模型，为AI教育提供了一种有效的创新教学方法，能够培养具备全面技能和行业适应能力的学生。"}}
{"id": "2512.05162", "pdf": "https://arxiv.org/pdf/2512.05162", "abs": "https://arxiv.org/abs/2512.05162", "authors": ["C. M. Wyss"], "title": "How to Tame Your LLM: Semantic Collapse in Continuous Systems", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.DS", "math.PR"], "comment": "35 pages, 1 figure. Exolytica AI Technical Report XTR-2025-01", "summary": "We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,μ) \\to L^2(M,μ)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.", "AI": {"tldr": "该论文提出了一个关于大语言模型语义动态的通用理论，将LLM形式化为连续状态机，并证明了语义特征定理：在温和的规律性假设下，转移算子的主导特征函数会诱导出有限个具有不变意义的谱盆地，从而解释了离散符号语义如何从连续计算中涌现。", "motivation": "理解大语言模型如何从连续的激活流形中涌现出离散的符号语义，解释语义崩溃现象，并为LLM的语义动态提供一个严格的数学框架。", "method": "将大语言模型形式化为连续状态机，使用转移算子编码语义质量的传播，在紧致性、遍历性和有界雅可比矩阵等假设下分析算子的谱特性，证明语义特征定理，并将理论扩展到随机和绝热（时间非齐次）设置。", "result": "证明了转移算子的主导特征函数会诱导出有限个谱盆地，每个盆地都可在实数上的o-极小结构中定义，表明谱的可聚合性与逻辑可驯性是一致的，连续激活流形会崩溃为有限、逻辑可解释的本体论。", "conclusion": "该理论为理解LLM中离散语义如何从连续计算中涌现提供了严格的数学基础，解释了语义崩溃现象，并表明在温和条件下，连续系统会自然地组织成有限个逻辑可解释的语义类别。"}}
{"id": "2512.05156", "pdf": "https://arxiv.org/pdf/2512.05156", "abs": "https://arxiv.org/abs/2512.05156", "authors": ["Igor Halperin"], "title": "Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.LG", "q-fin.CP"], "comment": "23 pages, 6 figures", "summary": "Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\\bf Q}$ and ${\\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.", "AI": {"tldr": "该论文提出了两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型在给定任务中的忠实度，并管理幻觉问题。", "motivation": "评估大型语言模型对给定任务的忠实度是一个复杂挑战，当前缺乏有效的无监督度量方法来量化模型的语义忠实度和控制幻觉。", "method": "将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文C到答案A的转换。将问题-上下文-答案三元组建模为共享主题上的概率分布，使用转换矩阵Q和A分别编码查询目标和实际结果，通过KL散度优化计算语义忠实度度量，并提出基于热力学的语义熵产生度量。", "result": "提出的语义忠实度度量将最小KL散度映射到[0,1]区间，高分表示更高忠实度；语义熵产生度量显示高忠实度通常对应低熵产生。两种度量可单独或联合用于LLM评估和幻觉控制，并在SEC 10-K文件摘要任务上进行了验证。", "conclusion": "该框架为LLM忠实度评估提供了理论严谨的无监督度量方法，基于信息论和热力学的SF和SEP度量能够有效评估模型表现并管理幻觉问题，具有实际应用价值。"}}
{"id": "2512.05152", "pdf": "https://arxiv.org/pdf/2512.05152", "abs": "https://arxiv.org/abs/2512.05152", "authors": ["Kun Wang", "Donglin Di", "Tonghua Su", "Lei Fan"], "title": "EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models", "categories": ["cs.CV"], "comment": "6pages, 5figures, published to 2025 IEEE International Conference on Multimedia and Expo (ICME), Nantes, France, 2025", "summary": "Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.", "AI": {"tldr": "提出EFDiT方法，通过层级嵌入器和超分辨率技术解决细粒度图像生成中的语义信息纠缠和细节不足问题", "motivation": "现有基于扩散模型的类别条件生成方法主要关注常见类别，在细粒度图像生成中存在语义信息纠缠和生成图像细节不足的问题", "method": "1. 引入层级嵌入器整合超类和子类的语义信息；2. 在感知信息生成阶段引入超分辨率概念，通过增强和退化模型提升细节特征；3. 提出高效的ProAttention机制", "result": "在公开基准测试中，该方法在性能上优于其他最先进的微调方法", "conclusion": "EFDiT方法通过层级语义整合和细节增强技术，有效解决了细粒度图像生成中的关键问题，提升了生成质量"}}
{"id": "2512.05150", "pdf": "https://arxiv.org/pdf/2512.05150", "abs": "https://arxiv.org/abs/2512.05150", "authors": ["Zhenglin Cheng", "Peng Sun", "Jianguo Li", "Tao Lin"], "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows", "categories": ["cs.CV"], "comment": "arxiv v0", "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.", "AI": {"tldr": "提出TwinFlow框架，用于训练单步生成模型，绕过对固定预训练教师模型的依赖，避免标准对抗网络训练，实现大规模高效模型构建", "motivation": "现有大型多模态生成模型通常基于多步框架（如扩散和流匹配），推理效率低（需要40-100次函数评估）。现有的少步加速方法存在局限性：蒸馏方法需要迭代蒸馏过程或在极少步数下性能显著下降；结合对抗训练的蒸馏方法则存在训练不稳定、复杂度高和GPU内存开销大等问题", "method": "提出TwinFlow框架，通过自对抗流实现单步生成，无需固定预训练教师模型，避免标准对抗网络训练。框架包含自对抗训练机制，使模型能够自我改进生成质量", "result": "在文本到图像任务上，1-NFE时获得0.83的GenEval分数，优于SANA-Sprint和RCGM等基线方法。在Qwen-Image-20B上进行全参数训练，仅用1-NFE就能匹配原始100-NFE模型的性能，计算成本降低100倍，质量下降很小", "conclusion": "TwinFlow是一个简单有效的框架，能够训练单步生成模型，解决了现有方法在推理效率、训练稳定性和计算开销方面的限制，特别适合构建大规模高效模型"}}
{"id": "2512.05145", "pdf": "https://arxiv.org/pdf/2512.05145", "abs": "https://arxiv.org/abs/2512.05145", "authors": ["Inna Wanyin Lin", "Yushi Hu", "Shuyue Stella Li", "Scott Geng", "Pang Wei Koh", "Luke Zettlemoyer", "Tim Althoff", "Marjan Ghazvininejad"], "title": "Self-Improving VLM Judges Without Human Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.", "AI": {"tldr": "提出一个无需人工标注的自训练VLM评判器框架，通过自合成数据迭代提升视觉语言模型的评判能力", "motivation": "当前训练VLM评判器主要依赖大规模人工偏好标注，这种方法成本高昂且随着模型快速改进容易过时，需要一种无需人工标注的自训练方法", "method": "采用三阶段迭代框架：1)生成不同质量级别的多模态指令-响应对；2)为每对生成推理轨迹和评判，移除不符合预期质量级别的数据；3)在正确的评判答案及其推理轨迹上进行训练", "result": "在Multimodal RewardBench和VL-RewardBench上评估，将Llama-3.2-11B多模态评判器的整体准确率从0.38提升到0.51，经常超越更大的模型如Llama-3.2-90B、GPT-4o和Claude 3.5 Sonnet，在通用性、幻觉和推理维度表现尤为突出", "conclusion": "无需人工标注的自训练方法展示了强大的潜力，表明未来可以开发出能够与快速改进的VLM能力同步进化的自评判器"}}
{"id": "2512.05140", "pdf": "https://arxiv.org/pdf/2512.05140", "abs": "https://arxiv.org/abs/2512.05140", "authors": ["Georges Le Bellier", "Nicolas Audebert"], "title": "FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Mar 2026, Tucson (AZ), United States", "summary": "The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.", "AI": {"tldr": "FlowEO是一个基于生成模型的遥感图像无监督域自适应框架，利用流匹配技术学习从源域到目标域的语义保持映射，用于地球观测图像的分类和语义分割任务。", "motivation": "地球观测数据存在固有的异质性（不同传感器、地理区域、采集时间、大气条件），导致训练域和部署域之间的分布偏移严重限制了预训练遥感模型的泛化能力，因此需要无监督域自适应技术来解决实际应用中的挑战。", "method": "FlowEO采用流匹配技术学习从源域到目标域图像分布的语义保持映射，通过生成模型在图像空间进行域自适应，能够处理SAR到光学图像转换、自然灾害引起的时空和语义偏移等复杂域适应场景。", "result": "实验结果表明，FlowEO在四个数据集上的分类和语义分割任务中优于现有的图像翻译域自适应方法，同时在感知图像质量方面达到相当或更好的水平。", "conclusion": "FlowEO展示了基于流匹配的无监督域自适应在遥感领域的潜力，能够有效处理地球观测数据中的分布偏移问题，为大规模环境监测和分析提供了实用解决方案。"}}
{"id": "2512.05139", "pdf": "https://arxiv.org/pdf/2512.05139", "abs": "https://arxiv.org/abs/2512.05139", "authors": ["Yang Xiang", "Jingwen Zhong", "Yige Yan", "Petros Koutrakis", "Eric Garshick", "Meredith Franklin"], "title": "Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.", "AI": {"tldr": "提出一种基于迁移学习和生成模型的卫星图像时空降尺度框架，通过结合轻量级U-Net迁移编码器和扩散生成模型，从粗分辨率输入重建细分辨率卫星图像。", "motivation": "解决从粗分辨率卫星图像重建细分辨率图像的挑战，特别是在有限训练数据情况下实现长期环境监测和暴露评估的需求。传统方法在保持物理一致性和时空自相关性方面存在不足。", "method": "采用迁移学习框架：1）先在粗分辨率数据上预训练轻量级U-Net学习时空表征；2）冻结编码器并将其作为物理有意义的潜在特征迁移到更大的降尺度模型中；3）结合扩散生成模型进行细分辨率重建。使用MERRA-2（50km）作为粗分辨率源域，GEOS-5 Nature Run（7km）作为细分辨率目标域。", "result": "模型在不同季节和区域分割中表现出色（R² = 0.65到0.94），优于确定性U-Net、变分自编码器和先前迁移学习基线。通过半变异函数、ACF/PACF和基于滞后的RMSE/R²评估显示，预测的降尺度图像保持了物理一致的空间变异性和时间自相关性，能够实现超出G5NR记录的稳定自回归重建。", "conclusion": "迁移增强的扩散模型为有限训练周期下长时序粗分辨率图像的降尺度提供了稳健且物理一致的解决方案，对改善环境暴露评估和长期环境监测具有重要意义。"}}
{"id": "2512.05137", "pdf": "https://arxiv.org/pdf/2512.05137", "abs": "https://arxiv.org/abs/2512.05137", "authors": ["Yunfei Zhang", "Yizhuo He", "Yuanxun Shao", "Zhengtao Yao", "Haoyan Xu", "Junhao Dong", "Zhen Yao", "Zhikang Dong"], "title": "ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.", "AI": {"tldr": "提出ChromouVQA基准测试，用于评估视觉语言模型在色觉伪装图像下的表现，通过石原式色觉测试板扩展构建多任务视觉问答基准", "motivation": "当前视觉语言模型在目标嵌入杂乱背景需要图形-背景分离的场景中表现不佳，缺乏针对色觉伪装场景的系统性评估基准", "method": "基于石原式色觉测试板扩展构建大规模多任务基准，使用多种填充几何形状、变化色度分离、密度、大小、遮挡和旋转，记录完整元数据，并提出模型无关的对比学习框架对齐轮廓与伪装渲染", "result": "人类与VLM评估显示存在显著差距，特别是在微妙色度对比或破坏性几何填充条件下；提出的对比学习方法改善了全局形状恢复", "conclusion": "ChromouVQA提供了一个紧凑、可控的基准用于可重复评估和扩展，有助于推动视觉语言模型在色觉伪装场景下的研究"}}
{"id": "2512.05136", "pdf": "https://arxiv.org/pdf/2512.05136", "abs": "https://arxiv.org/abs/2512.05136", "authors": ["Yujie Xiao", "Gongzhen Tang", "Deyun Zhang", "Jun Li", "Guangkun Nie", "Haoyu Wang", "Shun Huang", "Tong Liu", "Qinghao Zhao", "Kangyin Chen", "Shenda Hong"], "title": "Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.", "AI": {"tldr": "开发一个可解释的AI-ECG模型，通过微调心电图基础模型来预测冠状动脉CT血管造影中四支主要冠状动脉的严重或完全狭窄结果", "motivation": "冠状动脉疾病是全球主要健康负担，准确识别责任血管和评估狭窄程度对个体化治疗至关重要。虽然冠状动脉CT血管造影是CAD诊断的一线非侵入性方法，但其依赖高端设备、有辐射暴露、需要严格患者配合，限制了大规模使用。随着人工智能和心电图技术的普及，AI-ECG为CAD筛查提供了有前景的替代方案。", "method": "开发了一个可解释的AI-ECG模型，通过微调心电图基础模型来预测冠状动脉CT血管造影中四支主要冠状动脉（右冠状动脉、左主干、左前降支、左回旋支）的严重或完全狭窄。模型在内部和外部验证集上进行评估，并在临床正常心电图亚组中测试稳健性。", "result": "内部验证集上，模型对RCA、LM、LAD、LCX的AUC分别为0.794、0.818、0.744、0.755；外部验证集上AUC达到0.749、0.971、0.667、0.727。在临床正常心电图亚组中性能保持稳定。亚组分析确认了模型在不同人口统计学和采集时间层面的稳定性。可解释性分析揭示了高风险和低风险组之间明显的波形差异。", "conclusion": "该研究成功开发了一个可解释的AI-ECG模型，能够准确预测冠状动脉CT血管造影中的冠状动脉狭窄结果。模型在内部和外部验证中表现出色，在正常心电图亚组中保持稳健，为冠状动脉疾病的筛查提供了有前景的非侵入性替代方案，并揭示了与冠状动脉狭窄相关的关键心电图特征。"}}
{"id": "2512.05134", "pdf": "https://arxiv.org/pdf/2512.05134", "abs": "https://arxiv.org/abs/2512.05134", "authors": ["Zihao Wu"], "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models", "categories": ["cs.CV", "cs.DC", "cs.LG"], "comment": "8 pages main, 8 pages appendix, 16 figures, 5 tables. Code: https://github.com/zihaowu25/InvarDiff", "summary": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.", "AI": {"tldr": "提出InvarDiff方法，利用扩散模型中确定采样过程中的特征不变性，通过跨时间步和跨层的缓存机制加速扩散模型推理", "motivation": "扩散模型虽然能生成高质量图像，但迭代采样过程导致推理速度缓慢，存在大量冗余计算", "method": "基于少量确定性运行计算每个时间步、每层、每个模块的二进制缓存计划矩阵，使用重采样校正避免漂移，通过分位数变化度量决定哪些模块可以重用而非重新计算", "result": "在DiT和FLUX模型上实现2-3倍端到端加速，标准质量指标影响最小，视觉质量几乎无退化", "conclusion": "InvarDiff通过利用扩散模型中的特征不变性，实现了训练免费的高效加速，在保持生成质量的同时显著提升推理速度"}}
{"id": "2512.05132", "pdf": "https://arxiv.org/pdf/2512.05132", "abs": "https://arxiv.org/abs/2512.05132", "authors": ["Wenshuo Wang", "Fan Zhang"], "title": "Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.", "AI": {"tldr": "提出频率表示学习（FRL）方法，解决零样本超分辨率时空预测中的尺度锚定问题，使模型在低分辨率训练后能在高分辨率推理时降低误差而非维持相同误差水平。", "motivation": "现有研究将跨分辨率保持相似误差视为成功的多分辨率泛化，但作为数值求解器替代的深度学习模型应在分辨率增加时降低误差。根本限制在于低分辨率数据的奈奎斯特频率限制了其能表示的物理定律频率上限，导致模型难以处理高分辨率推理中未见过的频率分量，造成误差被锚定在低分辨率水平。", "method": "提出架构无关的频率表示学习（FRL），通过分辨率对齐的频率表示和频谱一致性训练来缓解尺度锚定问题：在具有更高奈奎斯特频率的网格上，FRL增强变体在高频带的频率响应更稳定。", "result": "FRL方法允许误差随分辨率增加而降低，在任务和分辨率范围内显著优于基线方法，同时仅产生适度的计算开销。", "conclusion": "识别并定义了尺度锚定这一新问题，提出的频率表示学习通过改善频率表示能力，使模型能够从低分辨率训练中学习到准确的高分辨率推理，突破了现有方法的局限性。"}}
{"id": "2512.05131", "pdf": "https://arxiv.org/pdf/2512.05131", "abs": "https://arxiv.org/abs/2512.05131", "authors": ["Tianling Xu", "Shengzhe Gan", "Leslie Gu", "Yuelei Li", "Fangneng Zhan", "Hanspeter Pfister"], "title": "AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Under review", "summary": "Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .", "AI": {"tldr": "AREA3D是一个主动3D重建代理，利用前馈3D重建模型和视觉语言引导，通过解耦视角不确定性建模和集成语义指导，实现高效自主的3D场景重建。", "motivation": "现有主动3D重建方法依赖手工设计的几何启发式规则，导致冗余观测且重建质量提升有限，需要更智能的视角选择和不确定性估计方法。", "method": "提出解耦视角不确定性建模与前馈重建器的方法，避免昂贵的在线优化；集成视觉语言模型提供高层语义指导，超越纯几何线索；实现精确的不确定性估计和多样化视角选择。", "result": "在场景级和对象级基准测试中，AREA3D实现了最先进的重建精度，特别是在稀疏视角情况下表现优异。", "conclusion": "AREA3D通过结合前馈3D重建和视觉语言指导，实现了高效主动3D重建，解决了传统方法依赖手工规则和冗余观测的问题，为自主场景重建提供了新思路。"}}
{"id": "2512.05128", "pdf": "https://arxiv.org/pdf/2512.05128", "abs": "https://arxiv.org/abs/2512.05128", "authors": ["Lucas Heublein", "Thorsten Nowak", "Tobias Feigl", "Jaspar Pahl", "Felix Ott"], "title": "GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System", "categories": ["eess.SP", "cs.AI"], "comment": "9 pages, 26 figures, 2 tables", "summary": "Jamming devices disrupt signals from the global navigation satellite system (GNSS) and pose a significant threat by compromising the reliability of accurate positioning. Consequently, the detection and localization of these interference signals are essential to achieve situational awareness, mitigating their impact, and implementing effective countermeasures. In this paper, we utilize a two-times-two patch antenna system (i.e., the software defined radio device Ettus USRP X440) to predict the angle, elevation, and distance to the jamming source based on in-phase and quadrature (IQ) samples. We propose to use an inertial measurement unit (IMU) attached to the antenna system to predict the relative movement of the antenna in dynamic scenarios. We present a synthetic aperture system that enables coherent spatial imaging using platform motion to synthesize larger virtual apertures, offering superior angular resolution without mechanically rotating antennas. While classical angle-of-arrival (AoA) methods exhibit reduced accuracy in multipath environments due to signal reflections and scattering, leading to localization errors, we utilize a methodology that fuses IQ and Fast Fourier Transform (FFT)-computed spectrograms with 22 AoA features and the predicted relative movement to enhance GNSS jammer direction finding.", "AI": {"tldr": "提出一种在动态场景中使用惯性辅助多天线系统进行GNSS干扰源方向探测的方法", "motivation": "GNSS干扰设备威胁定位系统可靠性，需要有效检测和定位干扰源以实现态势感知和反制", "method": "使用双天线系统采集IQ样本，结合IMU预测天线相对运动，采用合成孔径技术提高角度分辨率，融合IQ数据、FFT频谱图和22个AoA特征进行方向探测", "result": "该方法能够在动态场景中提高GNSS干扰源方向探测的准确性，特别是在多径环境下优于传统AoA方法", "conclusion": "提出的惯性辅助多天线系统结合合成孔径技术和特征融合方法，有效提升了动态场景下GNSS干扰源方向探测的性能"}}
{"id": "2512.05126", "pdf": "https://arxiv.org/pdf/2512.05126", "abs": "https://arxiv.org/abs/2512.05126", "authors": ["Kaidi Wang", "Yi He", "Wenhao Guan", "Weijie Wu", "Hongwu Ding", "Xiong Zhang", "Di Wu", "Meng Meng", "Jian Luan", "Lin Li", "Qingyang Hong"], "title": "SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Video dubbing aims to generate high-fidelity speech that is precisely temporally aligned with the visual content. Existing methods still suffer from limitations in speech naturalness and audio-visual synchronization, and are limited to monolingual settings. To address these challenges, we propose SyncVoice, a vision-augmented video dubbing framework built upon a pretrained text-to-speech (TTS) model. By fine-tuning the TTS model on audio-visual data, we achieve strong audiovisual consistency. We propose a Dual Speaker Encoder to effectively mitigate inter-language interference in cross-lingual speech synthesis and explore the application of video dubbing in video translation scenarios. Experimental results show that SyncVoice achieves high-fidelity speech generation with strong synchronization performance, demonstrating its potential in video dubbing tasks.", "AI": {"tldr": "SyncVoice是一个基于预训练TTS模型的视频配音框架，通过视觉增强实现高质量的音频-视觉同步，并支持跨语言视频翻译场景", "motivation": "现有视频配音方法在语音自然度和音视频同步性方面存在局限，且仅限于单语言设置，需要解决这些问题以实现高质量的多语言视频配音", "method": "基于预训练TTS模型构建视觉增强框架，通过音频-视觉数据微调实现音视频一致性，采用双说话人编码器缓解跨语言语音合成中的语言间干扰", "result": "实验结果表明SyncVoice能够实现高保真度的语音生成和强大的同步性能，在视频配音任务中展现出潜力", "conclusion": "SyncVoice通过视觉增强的预训练TTS模型有效解决了视频配音中的自然度和同步性问题，并扩展到跨语言视频翻译应用"}}
{"id": "2512.05122", "pdf": "https://arxiv.org/pdf/2512.05122", "abs": "https://arxiv.org/abs/2512.05122", "authors": ["Unnikrishnan Radhakrishnan"], "title": "Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN", "categories": ["cs.AI"], "comment": "Presented at 2025 International Workshop on Low-Cost Digital Solutions for Industrial Automation (LODISA)", "summary": "Small and medium-sized enterprises (SMEs) still depend heavily on tacit, experience-based know-how that rarely makes its way into formal documentation. This paper introduces a large-language-model (LLM)-driven conversational assistant that captures such knowledge on the shop floor and converts it incrementally and interactively into standards-compliant Business Process Model and Notation (BPMN) 2.0 diagrams. Powered by Gemini 2.5 Pro and delivered through a lightweight Gradio front-end with client-side bpmn-js visualisation, the assistant conducts an interview-style dialogue: it elicits process details, supports clarifying dialogue and on-demand analysis, and renders live diagrams that users can refine in real time. A proof-of-concept evaluation in an equipment-maintenance scenario shows that the chatbot produced an accurate \"AS-IS\" model, flagged issues via on-diagram annotations, and generated an improved \"TO-BE\" variant, all within about 12-minutes, while keeping API costs within an SME-friendly budget. The study analyses latency sources, model-selection trade-offs, and the challenges of enforcing strict XML schemas, then outlines a roadmap toward agentic and multimodal deployments. The results demonstrate that conversational LLMs can potentially be used to lower the skill and cost barriers to rigorous process documentation, helping SMEs preserve institutional knowledge, enhance operational transparency, and accelerate continuous-improvement efforts.", "AI": {"tldr": "开发基于大语言模型的对话助手，帮助中小企业将隐性知识转化为标准BPMN流程文档", "motivation": "中小企业依赖隐性知识但缺乏正式文档化，传统流程建模工具需要专业技能且成本高昂", "method": "使用Gemini 2.5 Pro驱动的对话助手，通过Gradio前端和bpmn-js可视化，进行交互式访谈式对话，实时生成和优化BPMN 2.0图表", "result": "在设备维护场景中，聊天机器人在约12分钟内生成准确的\"现状\"模型，标注问题，并生成改进的\"未来\"方案，API成本控制在中小企业友好范围内", "conclusion": "对话式大语言模型能够降低流程文档化的技能和成本门槛，帮助中小企业保存机构知识、增强运营透明度并加速持续改进"}}
{"id": "2512.05121", "pdf": "https://arxiv.org/pdf/2512.05121", "abs": "https://arxiv.org/abs/2512.05121", "authors": ["Tianshun Han", "Benjia Zhou", "Ajian Liu", "Yanyan Liang", "Du Zhang", "Zhen Lei", "Jun Wan"], "title": "PESTalk: Speech-Driven 3D Facial Animation with Personalized Emotional Styles", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "PESTalk is a novel method for generating 3D facial animations with personalized emotional styles directly from speech. It overcomes key limitations of existing approaches by introducing a Dual-Stream Emotion Extractor (DSEE) that captures both time and frequency-domain audio features for fine-grained emotion analysis, and an Emotional Style Modeling Module (ESMM) that models individual expression patterns based on voiceprint characteristics. To address data scarcity, the method leverages a newly constructed 3D-EmoStyle dataset. Evaluations demonstrate that PESTalk outperforms state-of-the-art methods in producing realistic and personalized facial animations.", "AI": {"tldr": "PESTalk是一种从语音直接生成具有个性化情感风格的3D面部动画的新方法，通过双流情感提取器和情感风格建模模块实现个性化情感表达。", "motivation": "现有方法在生成具有个性化情感风格的3D面部动画方面存在局限，缺乏对个体情感表达差异的建模能力，且面临3D情感动画数据稀缺的问题。", "method": "提出双流情感提取器（DSEE）同时捕捉时域和频域音频特征进行细粒度情感分析，设计情感风格建模模块（ESMM）基于声纹特征建模个体表情模式，并构建了3D-EmoStyle数据集解决数据稀缺问题。", "result": "评估表明PESTalk在生成逼真且个性化的面部动画方面优于现有最先进方法，能够产生更自然、更具个体特色的情感表达。", "conclusion": "PESTalk通过创新的双流情感提取和个性化情感风格建模，成功实现了从语音到个性化情感3D面部动画的高质量生成，为语音驱动的面部动画提供了新的解决方案。"}}
{"id": "2512.05119", "pdf": "https://arxiv.org/pdf/2512.05119", "abs": "https://arxiv.org/abs/2512.05119", "authors": ["Rongyang Zhang", "Yuqing Huang", "Chengqiang Lu", "Qimeng Wang", "Yan Gao", "Yi Wu", "Yao Hu", "Yin Xu", "Wei Wang", "Hao Wang", "Enhong Chen"], "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "26 pages, 6 figures, NeurIPS 2025 D&B Track poster", "summary": "In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.", "AI": {"tldr": "提出了RAG-IGBench基准，专门用于评估基于检索增强生成（RAG）的开放域问答中的交错图像-文本生成任务", "motivation": "现实场景中，提供视觉增强的响应能显著提升理解和记忆，但现有评估方法主要依赖单模态指标，无法充分评估图像-文本交错生成的质量和一致性", "method": "构建了RAG-IGBench基准，整合多模态大语言模型与检索机制，从社交平台获取最新公开内容，并设计了创新的评估指标来测量文本质量、图像质量及其一致性", "result": "通过在最先进的多模态大语言模型上进行广泛实验，提供了深入分析；验证了评估指标与人工评估的高度相关性；在训练集上微调的模型在多个基准上表现出改进性能", "conclusion": "RAG-IGBench是一个全面且实用的基准，能够有效评估交错图像-文本生成任务，其数据集质量和评估指标都具有实际应用价值"}}
{"id": "2512.05117", "pdf": "https://arxiv.org/pdf/2512.05117", "abs": "https://arxiv.org/abs/2512.05117", "authors": ["Prakhar Kaushik", "Shravan Chaudhari", "Ankit Vaidya", "Rama Chellappa", "Alan Yuille"], "title": "The Universal Weight Subspace Hypothesis", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "37 pages", "summary": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.", "AI": {"tldr": "该论文提出了通用权重子空间假说，发现深度神经网络在不同任务训练后会收敛到相似的参数子空间", "motivation": "研究深度神经网络在不同任务训练后是否存在内在的、共享的参数结构，探索模型参数空间的本质组织方式", "method": "对1100多个模型进行模态谱分析，包括500个Mistral-7B LoRA模型、500个视觉Transformer模型和50个LLaMA-8B模型，应用谱分解技术识别共享子空间", "result": "发现神经网络无论初始化、任务或领域如何，都会系统性地收敛到共享的谱子空间，仅需几个主方向就能捕获大部分方差", "conclusion": "深度神经网络存在内在的通用子空间结构，这对模型复用、多任务学习、模型融合以及高效训练算法开发具有重要意义"}}
{"id": "2512.05116", "pdf": "https://arxiv.org/pdf/2512.05116", "abs": "https://arxiv.org/abs/2512.05116", "authors": ["Zhen Liu", "Tim Z. Xiao", "Carles Domingo-Enrich", "Weiyang Liu", "Dinghuai Zhang"], "title": "Value Gradient Guidance for Flow Matching Alignment", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at NeurIPS 2025; 26 pages, 20 figures", "summary": "While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.", "AI": {"tldr": "提出VGG-Flow方法，通过最优控制理论实现流匹配模型的高效对齐，在保持概率先验的同时实现偏好对齐", "motivation": "现有流匹配模型对齐方法无法同时实现高效适应和概率先验保持，需要一种既能快速适应又保持概率合理性的方法", "method": "基于最优控制理论，提出梯度匹配方法VGG-Flow，通过将微调速度场与预训练速度场的最优差异与价值函数的梯度场匹配，结合奖励模型的一阶信息和价值函数的启发式初始化", "result": "在Stable Diffusion 3文本到图像流匹配模型上验证，在有限计算预算下能够有效对齐流匹配模型，同时保持先验特性", "conclusion": "VGG-Flow方法成功解决了流匹配模型对齐中效率与先验保持的平衡问题，为生成模型偏好对齐提供了有效解决方案"}}
