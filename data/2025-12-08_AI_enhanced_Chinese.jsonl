{"id": "2512.05967", "pdf": "https://arxiv.org/pdf/2512.05967", "abs": "https://arxiv.org/abs/2512.05967", "authors": ["Francesco Granata", "Francesco Poggi", "Misael Mongiovì"], "title": "Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.", "AI": {"tldr": "该研究提出了一种增强的检索增强生成架构，通过集成实体链接技术来提高意大利教育问答系统的准确性，特别是在专业领域中的事实准确性。", "motivation": "传统的基于语义相似度的RAG系统在专业领域中经常无法保证事实准确性，因为术语歧义会影响检索相关性。特别是在教育平台这样的专业领域，需要更精确的事实基础来支持可靠的AI辅导工具。", "method": "提出了一种增强的RAG架构，集成了基于Wikidata的实体链接模块，并实现了三种重排序策略来结合语义和实体信息：混合分数加权模型、互惠排名融合和交叉编码器重排序器。", "result": "实验在两个基准测试上进行：自定义学术数据集和标准SQuAD-it数据集。结果显示，在特定领域环境中，基于互惠排名融合的混合方案显著优于基线和交叉编码器方法，而交叉编码器在通用领域数据集上取得最佳结果。", "conclusion": "研究证实了领域不匹配效应的存在，并强调了领域适应和混合排名策略对于提高检索增强生成中事实精确性和可靠性的重要性。同时证明了实体感知RAG系统在教育环境中的潜力，有助于开发自适应且可靠的基于AI的辅导工具。"}}
{"id": "2512.05965", "pdf": "https://arxiv.org/pdf/2512.05965", "abs": "https://arxiv.org/abs/2512.05965", "authors": ["Hongyu Li", "Manyuan Zhang", "Dian Zheng", "Ziyu Guo", "Yimeng Jia", "Kaituo Feng", "Hao Yu", "Yexin Liu", "Yan Feng", "Peng Pei", "Xunliang Cai", "Linjiang Huang", "Hongsheng Li", "Si Liu"], "title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor", "categories": ["cs.CV"], "comment": "Project page: https://appletea233.github.io/think-while-edit", "summary": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community.", "AI": {"tldr": "提出EditThinker框架，通过迭代推理循环提升图像编辑模型的指令跟随能力", "motivation": "现有基于指令的图像编辑方法虽然能生成高质量图像，但单次编辑成功率有限，缺乏深思熟虑的过程，导致指令跟随能力不足", "method": "提出\"思考-编辑\"循环框架，训练单一MLLM（EditThinker）作为推理引擎，联合生成批判评分、推理过程和优化指令，并使用强化学习对齐思考与编辑过程", "result": "在四个基准测试上的实验表明，该方法显著提升了任意图像编辑模型的指令跟随能力，效果提升幅度很大", "conclusion": "EditThinker框架通过模拟人类认知循环，实现了迭代推理的图像编辑，有效解决了现有方法指令跟随能力不足的问题"}}
{"id": "2512.05964", "pdf": "https://arxiv.org/pdf/2512.05964", "abs": "https://arxiv.org/abs/2512.05964", "authors": ["Kevin Black", "Allen Z. Ren", "Michael Equi", "Sergey Levine"], "title": "Training-Time Action Conditioning for Efficient Real-Time Chunking", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Real-time chunking (RTC) enables vision-language-action models (VLAs) to generate smooth, reactive robot trajectories by asynchronously predicting action chunks and conditioning on previously committed actions via inference-time inpainting. However, this inpainting method introduces computational overhead that increases inference latency. In this work, we propose a simple alternative: simulating inference delay at training time and conditioning on action prefixes directly, eliminating any inference-time overhead. Our method requires no modifications to the model architecture or robot runtime, and can be implemented with only a few additional lines of code. In simulated experiments, we find that training-time RTC outperforms inference-time RTC at higher inference delays. In real-world experiments on box building and espresso making tasks with the $π_{0.6}$ VLA, we demonstrate that training-time RTC maintains both task performance and speed parity with inference-time RTC while being computationally cheaper. Our results suggest that training-time action conditioning is a practical drop-in replacement for inference-time inpainting in real-time robot control.", "AI": {"tldr": "提出一种训练时动作条件化的实时分块方法，替代推理时修复技术，以降低计算开销并保持实时机器人控制的性能", "motivation": "现有的实时分块方法通过推理时修复技术对已执行动作进行条件化，但这会引入计算开销并增加推理延迟。需要一种更高效的方法来减少推理时的计算负担", "method": "在训练时模拟推理延迟，直接对动作前缀进行条件化，无需修改模型架构或机器人运行时系统，仅需少量代码即可实现", "result": "在模拟实验中，训练时RTC在较高推理延迟下优于推理时RTC；在真实世界实验中（盒子搭建和意式浓缩制作任务），训练时RTC在保持任务性能和速度的同时计算成本更低", "conclusion": "训练时动作条件化是推理时修复技术的实用替代方案，能够在实时机器人控制中保持性能的同时降低计算开销"}}
{"id": "2512.05962", "pdf": "https://arxiv.org/pdf/2512.05962", "abs": "https://arxiv.org/abs/2512.05962", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "AI": {"tldr": "该论文提出了一种通过过滤驱动推理的方法来解决LLMs在强化学习训练中多样性损失的问题，使用α-散度家族来平衡精度和多样性", "motivation": "强化学习训练LLMs进行推理任务时会导致显著的多样性损失，这是因为RL隐式优化了\"模式寻求\"或\"零强制\"的反向KL散度，使模型集中在目标分布的高概率区域而忽略其他区域", "method": "从显式的目标分布出发，通过过滤掉错误答案同时保留正确答案的相对概率，使用α-散度家族来近似这个目标分布，从而在模式寻求和质量覆盖散度之间插值，直接控制精度-多样性权衡", "result": "在Lean定理证明基准测试中，该方法在覆盖精度帕累托前沿上实现了最先进的性能，在覆盖轴上优于所有先前方法", "conclusion": "通过显式目标分布和α-散度家族的方法，能够有效解决LLMs在推理任务中的多样性损失问题，在保持精度的同时提高覆盖范围"}}
{"id": "2512.05960", "pdf": "https://arxiv.org/pdf/2512.05960", "abs": "https://arxiv.org/abs/2512.05960", "authors": ["Munsif Ali", "Najmul Hassan", "Lucia Ventura", "Davide Di Bari", "Simonepietro Canese"], "title": "AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater images often suffer from severe color distortion, low contrast, and a hazy appearance due to wavelength-dependent light absorption and scattering. Simultaneously, existing deep learning models exhibit high computational complexity, which limits their practical deployment for real-time underwater applications. To address these challenges, this paper presents a novel underwater image enhancement model, called Adaptive Frequency Fusion and Illumination Aware Network (AQUA-Net). It integrates a residual encoder decoder with dual auxiliary branches, which operate in the frequency and illumination domains. The frequency fusion encoder enriches spatial representations with frequency cues from the Fourier domain and preserves fine textures and structural details. Inspired by Retinex, the illumination-aware decoder performs adaptive exposure correction through a learned illumination map that separates reflectance from lighting effects. This joint spatial, frequency, and illumination design enables the model to restore color balance, visual contrast, and perceptual realism under diverse underwater conditions. Additionally, we present a high-resolution, real-world underwater video-derived dataset from the Mediterranean Sea, which captures challenging deep-sea conditions with realistic visual degradations to enable robust evaluation and development of deep learning models. Extensive experiments on multiple benchmark datasets show that AQUA-Net performs on par with SOTA in both qualitative and quantitative evaluations while using less number of parameters. Ablation studies further confirm that the frequency and illumination branches provide complementary contributions that improve visibility and color representation. Overall, the proposed model shows strong generalization capability and robustness, and it provides an effective solution for real-world underwater imaging applications.", "AI": {"tldr": "提出AQUA-Net水下图像增强模型，通过自适应频率融合和光照感知机制解决水下图像颜色失真、低对比度和模糊问题，同时降低计算复杂度", "motivation": "水下图像存在严重的颜色失真、低对比度和模糊外观，现有深度学习模型计算复杂度高，限制了实时应用部署", "method": "采用残差编码器-解码器架构，集成频率域和光照域双辅助分支；频率融合编码器从傅里叶域提取频率线索，光照感知解码器基于Retinex理论进行自适应曝光校正", "result": "在多个基准数据集上达到SOTA性能，同时使用更少参数；频率和光照分支提供互补贡献，改善可见性和颜色表示；模型展示出强大的泛化能力和鲁棒性", "conclusion": "AQUA-Net通过联合空间、频率和光照设计，能够恢复水下图像的颜色平衡、视觉对比度和感知真实感，为实际水下成像应用提供有效解决方案"}}
{"id": "2512.05959", "pdf": "https://arxiv.org/pdf/2512.05959", "abs": "https://arxiv.org/abs/2512.05959", "authors": ["David Anugraha", "Patrick Amadeus Irawan", "Anshul Singh", "En-Shiun Annie Lee", "Genta Indra Winata"], "title": "M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.", "AI": {"tldr": "M4-RAG是一个大规模多语言多文化多模态检索增强生成基准，用于评估跨语言和模态的检索增强视觉问答系统", "motivation": "视觉语言模型在视觉问答中表现良好，但受限于静态训练数据。检索增强生成可以访问最新、文化相关和多语言信息，但多语言多模态RAG研究不足，需要建立基准来评估其性能", "method": "构建包含42种语言、56种区域方言和语域的大规模基准，包含超过80,000个文化多样性图像-问题对。建立受控检索环境，包含数百万个精心策划的多语言文档，平衡真实性与可重复性", "result": "系统评估显示，RAG对小规模视觉语言模型有持续益处，但对更大模型无法扩展，甚至经常降低其性能，揭示了模型规模与当前检索效果之间的关键不匹配", "conclusion": "M4-RAG为推进下一代RAG系统奠定了基础，这些系统能够跨语言、模态和文化背景进行无缝推理，揭示了当前RAG方法在大规模模型上的局限性"}}
{"id": "2512.05958", "pdf": "https://arxiv.org/pdf/2512.05958", "abs": "https://arxiv.org/abs/2512.05958", "authors": ["Sara Patel", "Mingxun Zhou", "Giulia Fanti"], "title": "MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.", "AI": {"tldr": "提出MaxShapley算法，用于生成式搜索引擎中公平的内容贡献度归因和补偿机制", "motivation": "随着基于大语言模型的生成式搜索引擎取代传统搜索，需要建立公平的机制来归因和补偿内容提供者对其生成答案的贡献", "method": "MaxShapley算法，利用可分解的最大和效用函数计算归因值，计算复杂度从指数级降低到线性", "result": "在三个多跳QA数据集上评估，MaxShapley达到与精确Shapley值相当的归因质量，同时显著降低计算资源消耗（最高减少8倍）", "conclusion": "MaxShapley为生成式搜索生态系统提供了一种高效、公平的内容贡献归因解决方案"}}
{"id": "2512.05955", "pdf": "https://arxiv.org/pdf/2512.05955", "abs": "https://arxiv.org/abs/2512.05955", "authors": ["Haowen Liu", "Shaoxiong Yao", "Haonan Chen", "Jiawei Gao", "Jiayuan Mao", "Jia-Bin Huang", "Yilun Du"], "title": "SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit remarkable common-sense and semantic reasoning capabilities. However, they lack a grounded understanding of physical dynamics. This limitation arises from training VLMs on static internet-scale visual-language data that contain no causal interactions or action-conditioned changes. Consequently, it remains challenging to leverage VLMs for fine-grained robotic manipulation tasks that require physical understanding, reasoning, and corresponding action planning. To overcome this, we present SIMPACT, a test-time, SIMulation-enabled ACTion Planning framework that equips VLMs with physical reasoning through simulation-in-the-loop world modeling, without requiring any additional training. From a single RGB-D observation, SIMPACT efficiently constructs physics simulations, enabling the VLM to propose informed actions, observe simulated rollouts, and iteratively refine its reasoning. By integrating language reasoning with physics prediction, our simulation-enabled VLM can understand contact dynamics and action outcomes in a physically grounded way. Our method demonstrates state-of-the-art performance on five challenging, real-world rigid-body and deformable manipulation tasks that require fine-grained physical reasoning, outperforming existing general-purpose robotic manipulation models. Our results demonstrate that embedding physics understanding via efficient simulation into VLM reasoning at test time offers a promising path towards generalizable embodied intelligence. Project webpage can be found at https://simpact-bot.github.io", "AI": {"tldr": "SIMPACT是一个通过模拟增强视觉语言模型物理推理能力的框架，无需额外训练即可实现精细的机器人操作任务规划", "motivation": "视觉语言模型虽然具备常识和语义推理能力，但缺乏对物理动力学的理解，难以应用于需要物理理解和动作规划的精细机器人操作任务", "method": "提出测试时模拟启用的动作规划框架，通过模拟循环世界建模为VLM提供物理推理能力，从单次RGB-D观测构建物理模拟，让VLM提出动作、观察模拟结果并迭代优化推理", "result": "在五个具有挑战性的真实世界刚体和可变形物体操作任务中达到最先进性能，优于现有的通用机器人操作模型", "conclusion": "通过测试时将物理理解嵌入VLM推理，为通用具身智能提供了一条有前景的路径"}}
{"id": "2512.05954", "pdf": "https://arxiv.org/pdf/2512.05954", "abs": "https://arxiv.org/abs/2512.05954", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code", "categories": ["cs.AI"], "comment": null, "summary": "We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems", "AI": {"tldr": "SymPyBench是一个大规模合成基准测试，包含15,045个大学物理问题，具有完全参数化特性，支持无限输入配置，并附带结构化推理步骤和可执行的Python代码生成真实解。", "motivation": "当前科学推理基准测试缺乏动态性、可扩展性和可解释性。需要一种能够评估模型在不同问题变体上一致性和可靠性的基准，以开发更稳健和可解释的科学推理系统。", "method": "创建包含三种问题类型（MC-Symbolic、MC-Numerical、free-form）的合成基准，每个问题完全参数化，附带结构化推理步骤和可执行Python代码。引入三种新颖评估指标：一致性分数、失败率和混淆率，以量化模型在不同问题变体上的表现。", "result": "实验使用最先进的指令调优语言模型进行测试，揭示了模型在科学推理方面的优势和局限性。基准测试支持动态评估，能够量化模型在不同问题配置下的表现变异性。", "conclusion": "SymPyBench为开发更稳健和可解释的科学推理系统奠定了基础，通过其动态、代码驱动的特性，能够更全面地评估模型的科学推理能力，特别是跨问题变体的一致性和可靠性。"}}
{"id": "2512.05953", "pdf": "https://arxiv.org/pdf/2512.05953", "abs": "https://arxiv.org/abs/2512.05953", "authors": ["Yunhao Cao", "Zubin Bhaumik", "Jessie Jia", "Xingyi He", "Kuan Fang"], "title": "Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning", "categories": ["cs.RO"], "comment": null, "summary": "We introduce Correspondence-Oriented Imitation Learning (COIL), a conditional policy learning framework for visuomotor control with a flexible task representation in 3D. At the core of our approach, each task is defined by the intended motion of keypoints selected on objects in the scene. Instead of assuming a fixed number of keypoints or uniformly spaced time intervals, COIL supports task specifications with variable spatial and temporal granularity, adapting to different user intents and task requirements. To robustly ground this correspondence-oriented task representation into actions, we design a conditional policy with a spatio-temporal attention mechanism that effectively fuses information across multiple input modalities. The policy is trained via a scalable self-supervised pipeline using demonstrations collected in simulation, with correspondence labels automatically generated in hindsight. COIL generalizes across tasks, objects, and motion patterns, achieving superior performance compared to prior methods on real-world manipulation tasks under both sparse and dense specifications.", "AI": {"tldr": "提出COIL框架，通过3D关键点对应关系表示任务，实现灵活的视觉运动控制", "motivation": "现有方法通常假设固定数量的关键点或均匀时间间隔，难以适应不同用户意图和任务需求的空间和时间粒度变化", "method": "使用3D关键点对应关系定义任务，设计具有时空注意力机制的条件策略，通过自监督管道训练，利用模拟演示自动生成对应标签", "result": "COIL在真实世界操作任务中，在稀疏和密集规范下均优于先前方法，能够跨任务、对象和运动模式泛化", "conclusion": "COIL提供了一种灵活的任务表示方法，通过对应关系导向的学习框架实现了鲁棒的视觉运动控制"}}
{"id": "2512.05951", "pdf": "https://arxiv.org/pdf/2512.05951", "abs": "https://arxiv.org/abs/2512.05951", "authors": ["Teofil Bodea", "Masanori Misono", "Julian Pritzi", "Patrick Sabanic", "Thore Sommer", "Harshavardhan Unnibhavi", "David Schall", "Nuno Santos", "Dimitrios Stavrakakis", "Pramod Bhatotia"], "title": "Trusted AI Agents in the Cloud", "categories": ["cs.CR", "cs.AI", "cs.MA"], "comment": null, "summary": "AI agents powered by large language models are increasingly deployed as cloud services that autonomously access sensitive data, invoke external tools, and interact with other agents. However, these agents run within a complex multi-party ecosystem, where untrusted components can lead to data leakage, tampering, or unintended behavior. Existing Confidential Virtual Machines (CVMs) provide only per binary protection and offer no guarantees for cross-principal trust, accelerator-level isolation, or supervised agent behavior. We present Omega, a system that enables trusted AI agents by enforcing end-to-end isolation, establishing verifiable trust across all contributing principals, and supervising every external interaction with accountable provenance. Omega builds on Confidential VMs and Confidential GPUs to create a Trusted Agent Platform that hosts many agents within a single CVM using nested isolation. It also provides efficient multi-agent orchestration with cross-principal trust establishment via differential attestation, and a policy specification and enforcement framework that governs data access, tool usage, and inter-agent communication for data protection and regulatory compliance. Implemented on AMD SEV-SNP and NVIDIA H100, Omega fully secures agent state across CVM-GPU, and achieves high performance while enabling high-density, policy-compliant multi-agent deployments at cloud scale.", "AI": {"tldr": "开发Omega系统，为云端AI代理提供端到端的安全隔离和可信执行环境，解决多主体环境中的安全风险", "motivation": "当前基于大语言模型的AI代理在云端部署时面临复杂多主体环境的安全挑战，现有机密虚拟机仅提供二进制级别保护，无法保证跨主体信任、加速器级隔离或代理行为监督，存在数据泄露、篡改和意外行为风险", "method": "基于AMD SEV-SNP和NVIDIA H100的机密虚拟机和机密GPU构建可信代理平台，采用嵌套隔离技术在同一CVM内托管多个代理，通过差异认证建立跨主体信任，并提供策略规范与执行框架来管理数据访问、工具使用和代理间通信", "result": "Omega实现了CVM-GPU间代理状态的完全安全保护，在保持高性能的同时支持高密度、符合策略的多代理部署，能够满足云规模的安全需求", "conclusion": "Omega系统通过端到端隔离、可验证的跨主体信任建立以及有监督的代理交互，为云端AI代理提供了全面的安全保护框架，解决了现有机密计算技术的局限性"}}
{"id": "2512.05950", "pdf": "https://arxiv.org/pdf/2512.05950", "abs": "https://arxiv.org/abs/2512.05950", "authors": ["Zalish Mahmud", "Anantaa Kotal", "Aritran Piplai"], "title": "Impugan: Learning Conditional Generative Models for Robust Data Imputation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\\% lower Earth Mover's Distance (EMD) and 70\\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025", "AI": {"tldr": "提出Impugan，一种基于条件生成对抗网络（cGAN）的数据插补方法，用于处理不完整和异构数据", "motivation": "现实世界数据普遍存在缺失值，传统插补方法基于线性性和独立性假设，难以处理复杂异构数据，导致估计偏差或过度平滑", "method": "使用条件生成对抗网络（cGAN），生成器根据观测特征重建缺失值，判别器区分真实数据与插补数据，通过对抗训练学习非线性多模态关系", "result": "在基准数据集和多源集成任务中，相比领先基线方法，Impugan实现了高达82%的地球移动距离（EMD）降低和70%的互信息偏差（MI）降低", "conclusion": "对抗训练的生成模型为不完整、异构数据的插补和融合提供了可扩展且原理性的方法"}}
{"id": "2512.05946", "pdf": "https://arxiv.org/pdf/2512.05946", "abs": "https://arxiv.org/abs/2512.05946", "authors": ["Truong Thanh Hung Nguyen", "Truong Thinh Nguyen", "Hung Cao"], "title": "Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem", "categories": ["cs.AI", "cs.ET", "cs.SE"], "comment": "Quantum Software Engineering Practices at The 41st ACM/SIGAPP Symposium On Applied Computing (SAC 2026)", "summary": "Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.", "AI": {"tldr": "提出了一种结合变分量子电路和Rainbow DQN的量子增强深度强化学习方法，用于优化资源分配问题", "motivation": "资源分配问题是NP难问题，传统深度强化学习方法受限于经典函数逼近器的表达能力，量子计算可以提供更强的表示能力", "method": "将环状拓扑变分量子电路与Rainbow DQN结合，构建VQR-DQN模型，将人力资源分配问题建模为马尔可夫决策过程", "result": "在四个人力资源分配基准测试中，VQR-DQN相比随机基线减少了26.8%的归一化完工时间，比Double DQN和经典Rainbow DQN提升了4.9-13.4%", "conclusion": "量子增强的深度强化学习在大规模资源分配问题中具有潜力，电路表达能力和纠缠与策略质量之间存在理论联系"}}
{"id": "2512.05943", "pdf": "https://arxiv.org/pdf/2512.05943", "abs": "https://arxiv.org/abs/2512.05943", "authors": ["Shima Imani", "Seungwhan Moon", "Lambert Mathias", "Lu Zhang", "Babak Damavandi"], "title": "TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.", "AI": {"tldr": "提出TRACE框架，用于分析和增强视觉语言模型中的逐步推理过程，通过透明推理和一致性评估来诊断推理轨迹而非仅关注最终结果。", "motivation": "当前大型视觉语言模型在可靠数学和科学推理方面仍面临挑战，标准最终答案评估往往掩盖推理错误，导致无声故障持续存在。", "method": "引入TRACE框架，利用辅助推理集（ARS）将复杂问题分解为子问题对，通过基于一致性的指标评估中间步骤，并暴露标准评估忽略的故障。", "result": "实验表明ARS间的一致性与最终答案正确性相关，有助于定位推理失败的具体步骤，为模型改进提供可操作的信号。TRACE定义置信区域区分可靠与不可靠推理路径。", "conclusion": "TRACE框架通过透明推理轨迹分析，有效诊断视觉语言模型的推理错误，支持过滤、调试和模型精炼，提升数学和科学推理的可靠性。"}}
{"id": "2512.05941", "pdf": "https://arxiv.org/pdf/2512.05941", "abs": "https://arxiv.org/abs/2512.05941", "authors": ["Zhiyuan Jiang", "Shenghao Xie", "Wenyi Li", "Wenqiang Zu", "Peihang Li", "Jiahao Qiu", "Siqi Pei", "Lei Ma", "Tiejun Huang", "Mengdi Wang", "Shilong Liu"], "title": "Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/Princeton-AI2-Lab/ZoomClick", "summary": "Grounding is a fundamental capability for building graphical user interface (GUI) agents. Although existing approaches rely on large-scale bounding box supervision, they still face various challenges, such as cross-platform generalization, complex layout analysis, and fine-grained element localization. In this paper, we investigate zoom as a strong yet underexplored prior for GUI grounding, and propose a training-free method, ZoomClick. By characterizing four key properties of zoom (i.e., pre-zoom, depth, shrink size, minimal crop size), we unlock its full capabilities for dynamic spatial focusing and adaptive context switching. Experiments demonstrate that our method significantly boosts the performance of both general vision-language and specialized GUI grounding models, achieving state-of-the-art results on several mainstream benchmarks; for example, UI-Venus-72B attains a 73.1% success rate on ScreenSpot-Pro. Furthermore, we present GUIZoom-Bench, a benchmark for evaluating model adaptability to zoom, aiming to inspire future research on improving zoom for further training and test-time scaling in GUI grounding tasks.", "AI": {"tldr": "提出ZoomClick方法，利用缩放作为GUI定位的先验，无需训练即可提升GUI定位性能", "motivation": "现有GUI定位方法依赖大规模边界框监督，但仍面临跨平台泛化、复杂布局分析和细粒度元素定位等挑战，缩放作为一种强大但未被充分探索的先验具有潜力", "method": "提出ZoomClick方法，通过表征缩放的四个关键属性（预缩放、深度、收缩尺寸、最小裁剪尺寸）来实现动态空间聚焦和自适应上下文切换，无需训练", "result": "方法显著提升了通用视觉语言模型和专用GUI定位模型的性能，在多个主流基准测试中达到最先进水平，例如UI-Venus-72B在ScreenSpot-Pro上达到73.1%的成功率", "conclusion": "缩放是GUI定位的强大先验，ZoomClick方法有效解锁了其潜力，同时提出的GUIZoom-Bench基准将推动未来在GUI定位任务中改进缩放的研究"}}
{"id": "2512.05937", "pdf": "https://arxiv.org/pdf/2512.05937", "abs": "https://arxiv.org/abs/2512.05937", "authors": ["Anne Sielemann", "Valentin Barner", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn", "Juergen Beyerer"], "title": "Measuring the Effect of Background on Classification and Feature Importance in Deep Learning for AV Perception", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "8 pages, 2 figures, 7 tables", "summary": "Common approaches to explainable AI (XAI) for deep learning focus on analyzing the importance of input features on the classification task in a given model: saliency methods like SHAP and GradCAM are used to measure the impact of spatial regions of the input image on the classification result. Combined with ground truth information about the location of the object in the input image (e.g., a binary mask), it is determined whether object pixels had a high impact on the classification result, or whether the classification focused on background pixels. The former is considered to be a sign of a healthy classifier, whereas the latter is assumed to suggest overfitting on spurious correlations. A major challenge, however, is that these intuitive interpretations are difficult to test quantitatively, and hence the output of such explanations lacks an explanation itself. One particular reason is that correlations in real-world data are difficult to avoid, and whether they are spurious or legitimate is debatable. Synthetic data in turn can facilitate to actively enable or disable correlations where desired but often lack a sufficient quantification of realism and stochastic properties. [...] Therefore, we systematically generate six synthetic datasets for the task of traffic sign recognition, which differ only in their degree of camera variation and background correlation [...] to quantify the isolated influence of background correlation, different levels of camera variation, and considered traffic sign shapes on the classification performance, as well as background feature importance. [...] Results include a quantification of when and how much background features gain importance to support the classification task based on changes in the training domain [...]. Download: synset.de/datasets/synset-signset-ger/background-effect", "AI": {"tldr": "该论文通过系统生成六个合成数据集，量化背景相关性对深度学习分类性能和特征重要性的影响，特别是在交通标志识别任务中。", "motivation": "当前可解释AI方法（如SHAP和GradCAM）用于分析输入特征对分类任务的重要性时，存在解释结果难以定量验证的问题。特别是难以区分背景像素的重要性是健康分类器的表现还是对虚假相关性的过拟合，需要系统量化背景相关性的影响。", "method": "系统生成六个合成数据集用于交通标志识别任务，这些数据集仅在相机变化程度和背景相关性方面有所不同。通过控制这些变量，量化背景相关性、相机变化水平和交通标志形状对分类性能和背景特征重要性的独立影响。", "result": "研究量化了背景特征在分类任务中获得重要性的时机和程度，基于训练域的变化。结果显示背景相关性如何影响分类器的特征关注模式，为解释背景特征重要性提供了定量依据。", "conclusion": "通过合成数据集的系统实验，该研究为量化背景相关性对深度学习分类器的影响提供了方法论框架，有助于更可靠地解释特征重要性分析结果，特别是在区分健康分类与过拟合方面。"}}
{"id": "2512.05936", "pdf": "https://arxiv.org/pdf/2512.05936", "abs": "https://arxiv.org/abs/2512.05936", "authors": ["Anne Sielemann", "Lena Loercher", "Max-Lion Schumacher", "Stefan Wolf", "Masoud Roschani", "Jens Ziehn"], "title": "Synset Signset Germany: a Synthetic Dataset for German Traffic Sign Recognition", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 8 figures, 3 tables", "summary": "In this paper, we present a synthesis pipeline and dataset for training / testing data in the task of traffic sign recognition that combines the advantages of data-driven and analytical modeling: GAN-based texture generation enables data-driven dirt and wear artifacts, rendering unique and realistic traffic sign surfaces, while the analytical scene modulation achieves physically correct lighting and allows detailed parameterization. In particular, the latter opens up applications in the context of explainable AI (XAI) and robustness tests due to the possibility of evaluating the sensitivity to parameter changes, which we demonstrate with experiments. Our resulting synthetic traffic sign recognition dataset Synset Signset Germany contains a total of 105500 images of 211 different German traffic sign classes, including newly published (2020) and thus comparatively rare traffic signs. In addition to a mask and a segmentation image, we also provide extensive metadata including the stochastically selected environment and imaging effect parameters for each image. We evaluate the degree of realism of Synset Signset Germany on the real-world German Traffic Sign Recognition Benchmark (GTSRB) and in comparison to CATERED, a state-of-the-art synthetic traffic sign recognition dataset.", "AI": {"tldr": "提出一个用于德国交通标志识别的合成数据集生成管道和数据集，结合数据驱动和解析建模的优势", "motivation": "现有交通标志识别数据集缺乏真实世界的污损、磨损效果，且难以获得新发布的交通标志数据，需要一种既能生成逼真图像又能提供详细参数控制的合成数据方法", "method": "结合GAN生成纹理实现数据驱动的污损和磨损效果，同时使用解析场景调制实现物理正确的光照和详细参数化，生成包含211个德国交通标志类别的105500张图像", "result": "创建了Synset Signset Germany数据集，包含掩码、分割图像和丰富的元数据，在真实世界GTSRB基准测试中验证了真实度，并与最先进的合成数据集CATERED进行了比较", "conclusion": "提出的合成管道和数据集结合了数据驱动和解析建模的优势，能够生成逼真的交通标志图像，支持可解释AI和鲁棒性测试，特别适用于新发布的稀有交通标志"}}
{"id": "2512.05932", "pdf": "https://arxiv.org/pdf/2512.05932", "abs": "https://arxiv.org/abs/2512.05932", "authors": ["L. Dudzik", "M. Roschani", "A. Sielemann", "K. Trampert", "J. Ziehn", "J. Beyerer", "C. Neumann"], "title": "Physically-Based Simulation of Automotive LiDAR", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present an analytic model for simulating automotive time-of-flight (ToF) LiDAR that includes blooming, echo pulse width, and ambient light, along with steps to determine model parameters systematically through optical laboratory measurements. The model uses physically based rendering (PBR) in the near-infrared domain. It assumes single-bounce reflections and retroreflections over rasterized rendered images from shading or ray tracing, including light emitted from the sensor as well as stray light from other, non-correlated sources such as sunlight. Beams from the sensor and sensitivity of the receiving diodes are modeled with flexible beam steering patterns and with non-vanishing diameter. Different (all non-real time) computational approaches can be chosen based on system properties, computing capabilities, and desired output properties. Model parameters include system-specific properties, namely the physical spread of the LiDAR beam, combined with the sensitivity of the receiving diode; the intensity of the emitted light; the conversion between the intensity of reflected light and the echo pulse width; and scenario parameters such as environment lighting, positioning, and surface properties of the target(s) in the relevant infrared domain. System-specific properties of the model are determined from laboratory measurements of the photometric luminance on different target surfaces aligned with a goniometer at 0.01° resolution, which marks the best available resolution for measuring the beam pattern. The approach is calibrated for and tested on two automotive LiDAR systems, the Valeo Scala Gen. 2 and the Blickfeld Cube 1. Both systems differ notably in their properties and available interfaces, but the relevant model parameters could be extracted successfully.", "AI": {"tldr": "提出了一种用于模拟汽车激光雷达的物理基础分析模型，包含光晕效应、回波脉冲宽度和环境光等关键因素，并通过光学实验室测量系统性地确定模型参数。", "motivation": "当前汽车激光雷达模拟缺乏物理准确性，特别是在处理光晕效应、回波脉冲宽度和环境光等关键物理现象方面。需要一种能够准确模拟真实激光雷达系统行为的物理基础模型，以支持自动驾驶系统的开发和测试。", "method": "开发了一个基于物理的渲染（PBR）分析模型，在近红外域工作。模型假设单次反射和逆向反射，使用光栅化渲染图像或光线追踪。包含传感器发射光和来自其他非相关光源（如阳光）的杂散光。光束发射和接收二极管灵敏度使用灵活的波束转向模式和非零直径建模。通过光学实验室测量（使用测角仪在0.01°分辨率下测量不同目标表面的光度亮度）系统性地确定模型参数。", "result": "成功为两种不同的汽车激光雷达系统（Valeo Scala Gen. 2和Blickfeld Cube 1）提取了相关模型参数。两种系统在特性和可用接口方面差异显著，但模型参数均能成功提取。模型能够准确模拟包括光晕效应、回波脉冲宽度和环境光在内的关键物理现象。", "conclusion": "提出的物理基础激光雷达模拟模型能够准确捕捉真实激光雷达系统的关键物理特性。通过系统性的实验室测量方法可以确定模型参数，为不同激光雷达系统提供一致的模拟框架。该模型支持基于系统特性、计算能力和所需输出特性的不同计算方法选择。"}}
{"id": "2512.05930", "pdf": "https://arxiv.org/pdf/2512.05930", "abs": "https://arxiv.org/abs/2512.05930", "authors": ["Shima Imani", "Seungwhan Moon", "Adel Ahmadyan", "Lu Zhang", "Kirmani Ahmed", "Babak Damavandi"], "title": "PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Evaluating vision-language models (VLMs) in scientific domains like mathematics and physics poses unique challenges that go far beyond predicting final answers. These domains demand conceptual understanding, symbolic reasoning, and adherence to formal laws, requirements that most existing benchmarks fail to address. In particular, current datasets tend to be static, lacking intermediate reasoning steps, robustness to variations, or mechanisms for verifying scientific correctness. To address these limitations, we introduce PRiSM, a synthetic, fully dynamic, and multimodal benchmark for evaluating scientific reasoning via grounded Python code. PRiSM includes over 24,750 university-level physics and math problems, and it leverages our scalable agent-based pipeline, PrismAgent, to generate well-structured problem instances. Each problem contains dynamic textual and visual input, a generated figure, alongside rich structured outputs: executable Python code for ground truth generation and verification, and detailed step-by-step reasoning. The dynamic nature and Python-powered automated ground truth generation of our benchmark allow for fine-grained experimental auditing of multimodal VLMs, revealing failure modes, uncertainty behaviors, and limitations in scientific reasoning. To this end, we propose five targeted evaluation tasks covering generalization, symbolic program synthesis, perturbation robustness, reasoning correction, and ambiguity resolution. Through comprehensive evaluation of existing VLMs, we highlight their limitations and showcase how PRiSM enables deeper insights into their scientific reasoning capabilities.", "AI": {"tldr": "PRiSM是一个用于评估视觉语言模型在科学推理能力方面的多模态基准测试，通过Python代码执行进行接地评估，包含超过24,750个大学水平的物理和数学问题。", "motivation": "当前评估视觉语言模型在科学领域（如数学和物理）的基准测试存在局限性：缺乏中间推理步骤、对变化的鲁棒性不足、无法验证科学正确性，需要更全面的评估方法来测试概念理解、符号推理和形式定律遵循能力。", "method": "使用基于代理的PrismAgent流水线生成结构化问题实例，包含动态文本和视觉输入、生成的图表，以及丰富的结构化输出：用于生成和验证真实值的可执行Python代码，以及详细的逐步推理步骤。", "result": "PRiSM基准测试能够对多模态视觉语言模型进行细粒度实验审计，揭示失败模式、不确定性行为和科学推理局限性，并提出了五个针对性评估任务：泛化、符号程序合成、扰动鲁棒性、推理修正和歧义解析。", "conclusion": "PRiSM通过Python接地的动态评估方法，为深入理解视觉语言模型的科学推理能力提供了更全面的评估框架，揭示了现有模型的局限性并展示了更深入的洞察能力。"}}
{"id": "2512.05928", "pdf": "https://arxiv.org/pdf/2512.05928", "abs": "https://arxiv.org/abs/2512.05928", "authors": ["Pedro Vidal", "Bernardo Biesseck", "Luiz E. L. Coelho", "Roger Granada", "David Menotti"], "title": "A Comparative Study on Synthetic Facial Data Generation Techniques for Face Recognition", "categories": ["cs.CV"], "comment": "18 pages, 17 figures", "summary": "Facial recognition has become a widely used method for authentication and identification, with applications for secure access and locating missing persons. Its success is largely attributed to deep learning, which leverages large datasets and effective loss functions to learn discriminative features. Despite these advances, facial recognition still faces challenges in explainability, demographic bias, privacy, and robustness to aging, pose variations, lighting changes, occlusions, and facial expressions. Privacy regulations have also led to the degradation of several datasets, raising legal, ethical, and privacy concerns. Synthetic facial data generation has been proposed as a promising solution. It mitigates privacy issues, enables experimentation with controlled facial attributes, alleviates demographic bias, and provides supplementary data to improve models trained on real data. This study compares the effectiveness of synthetic facial datasets generated using different techniques in facial recognition tasks. We evaluate accuracy, rank-1, rank-5, and the true positive rate at a false positive rate of 0.01% on eight leading datasets, offering a comparative analysis not extensively explored in the literature. Results demonstrate the ability of synthetic data to capture realistic variations while emphasizing the need for further research to close the performance gap with real data. Techniques such as diffusion models, GANs, and 3D models show substantial progress; however, challenges remain.", "AI": {"tldr": "比较不同合成人脸数据生成技术在面部识别任务中的有效性", "motivation": "面部识别面临隐私法规导致数据集退化、人口统计偏差、可解释性、鲁棒性等挑战，合成人脸数据被提出作为有前景的解决方案", "method": "比较使用不同技术生成的合成人脸数据集在面部识别任务中的表现，评估指标包括准确率、rank-1、rank-5和TPR@FPR=0.01%，在八个领先数据集上进行评估", "result": "合成数据能够捕捉真实变化，但性能与真实数据仍有差距；扩散模型、GANs和3D模型等技术显示出显著进展", "conclusion": "合成数据在缓解隐私问题、控制面部属性、减轻人口统计偏差方面具有潜力，但仍需进一步研究以缩小与真实数据的性能差距"}}
{"id": "2512.05927", "pdf": "https://arxiv.org/pdf/2512.05927", "abs": "https://arxiv.org/abs/2512.05927", "authors": ["Zhiting Mei", "Tenny Yin", "Micah Baker", "Ola Shorinwa", "Anirudha Majumdar"], "title": "World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent advances in generative video models have led to significant breakthroughs in high-fidelity video synthesis, specifically in controllable video generation where the generated video is conditioned on text and action inputs, e.g., in instruction-guided video editing and world modeling in robotics. Despite these exceptional capabilities, controllable video models often hallucinate - generating future video frames that are misaligned with physical reality - which raises serious concerns in many tasks such as robot policy evaluation and planning. However, state-of-the-art video models lack the ability to assess and express their confidence, impeding hallucination mitigation. To rigorously address this challenge, we propose C3, an uncertainty quantification (UQ) method for training continuous-scale calibrated controllable video models for dense confidence estimation at the subpatch level, precisely localizing the uncertainty in each generated video frame. Our UQ method introduces three core innovations to empower video models to estimate their uncertainty. First, our method develops a novel framework that trains video models for correctness and calibration via strictly proper scoring rules. Second, we estimate the video model's uncertainty in latent space, avoiding training instability and prohibitive training costs associated with pixel-space approaches. Third, we map the dense latent-space uncertainty to interpretable pixel-level uncertainty in the RGB space for intuitive visualization, providing high-resolution uncertainty heatmaps that identify untrustworthy regions. Through extensive experiments on large-scale robot learning datasets (Bridge and DROID) and real-world evaluations, we demonstrate that our method not only provides calibrated uncertainty estimates within the training distribution, but also enables effective out-of-distribution detection.", "AI": {"tldr": "提出C3方法，一种用于可控视频生成模型的校准不确定性量化技术，能够在子补丁级别进行密集置信度估计，精确定位生成视频帧中的不确定区域。", "motivation": "当前可控视频生成模型存在幻觉问题（生成与物理现实不符的视频帧），在机器人策略评估和规划等任务中带来严重问题。现有视频模型缺乏评估和表达置信度的能力，阻碍了幻觉缓解。", "method": "提出C3不确定性量化方法，包含三个核心创新：1）通过严格适当评分规则训练视频模型以获得正确性和校准性；2）在潜在空间估计视频模型的不确定性，避免像素空间方法的不稳定性和高昂训练成本；3）将密集潜在空间不确定性映射到可解释的像素级RGB空间不确定性，提供高分辨率不确定性热图。", "result": "在大规模机器人学习数据集（Bridge和DROID）和真实世界评估中，该方法不仅能在训练分布内提供校准的不确定性估计，还能实现有效的分布外检测。", "conclusion": "C3方法为可控视频生成模型提供了校准的不确定性量化能力，能够精确定位生成视频中的不可信区域，为解决视频生成中的幻觉问题提供了有效工具。"}}
{"id": "2512.05926", "pdf": "https://arxiv.org/pdf/2512.05926", "abs": "https://arxiv.org/abs/2512.05926", "authors": ["Wenyan Luo", "Dustin G. Mixon"], "title": "BalLOT: Balanced $k$-means clustering with optimal transport", "categories": ["stat.ML", "cs.DS", "cs.IT", "cs.LG", "math.OC"], "comment": "20 pages, 9 figures", "summary": "We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.", "AI": {"tldr": "提出BalLOT方法，使用最优传输解决平衡k-means聚类问题，实现快速有效的解决方案", "motivation": "解决平衡k-means聚类这一基本问题，传统方法在处理平衡约束时存在效率或效果上的不足", "method": "引入最优传输方法到交替最小化框架中，提出BalLOT算法，包含初始化方案和迭代优化过程", "result": "数值实验显示快速有效；理论证明：对通用数据产生整数耦合；在随机球模型下实现精确和部分恢复；初始化方案实现一步恢复", "conclusion": "BalLOT为平衡k-means聚类提供了理论保证的快速有效解决方案，结合了最优传输和交替最小化的优势"}}
{"id": "2512.05925", "pdf": "https://arxiv.org/pdf/2512.05925", "abs": "https://arxiv.org/abs/2512.05925", "authors": ["Federico Bianchi", "Yongchan Kwon", "Zachary Izzo", "Linjun Zhang", "James Zou"], "title": "To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.", "AI": {"tldr": "开发基于GPT-5的论文正确性检查器，系统量化AI领域已发表论文中的客观错误，发现错误数量随时间增加，验证了LLM在检测和修正学术论文错误方面的潜力。", "motivation": "同行评审的论文是构建新研究和知识的基础，但其中的错误会在文献中传播，导致后续研究混淆并影响可复现性。研究加速和同行评审系统压力使得错误更难被发现和避免。", "method": "开发基于GPT-5的论文正确性检查器，专注于检测客观错误（公式、推导、计算、图表等有明确事实依据的错误），排除主观评价。分析顶级AI会议和期刊的论文，并由人类专家验证AI识别的错误。", "result": "1. 已发表论文包含不可忽视的客观错误数量；2. 每篇论文平均错误数随时间增加：NeurIPS从2021年的3.8个增至2025年的5.9个（增长55.3%），ICLR从2018年的4.1个增至2025年的5.2个，TMLR从2022/23年的5.0个增至2025年的5.5个；3. AI检查器识别316个潜在错误，人类专家确认263个为真实错误，精确度83.2%；4. AI检查器能为75.8%的识别错误提出正确修正。", "conclusion": "前沿LLM在检测和修正已发表论文中的客观错误方面具有显著潜力，有助于建立更坚实的知识基础。虽然大多数错误相对较小，但修正它们可以减少文献混淆并增强可复现性。"}}
{"id": "2512.05922", "pdf": "https://arxiv.org/pdf/2512.05922", "abs": "https://arxiv.org/abs/2512.05922", "authors": ["Khang Le", "Anh Mai Vu", "Thi Kim Trang Vo", "Ha Thach", "Ngoc Bui Lam Quang", "Thanh-Huy Nguyen", "Minh H. N. Le", "Zhu Han", "Chandra Mohan", "Hien Van Nguyen"], "title": "LPD: Learnable Prototypes with Diversity Regularization for Weakly Supervised Histopathology Segmentation", "categories": ["cs.CV"], "comment": "Note: Khang Le and Anh Mai Vu contributed equally", "summary": "Weakly supervised semantic segmentation (WSSS) in histopathology reduces pixel-level labeling by learning from image-level labels, but it is hindered by inter-class homogeneity, intra-class heterogeneity, and CAM-induced region shrinkage (global pooling-based class activation maps whose activations highlight only the most distinctive areas and miss nearby class regions). Recent works address these challenges by constructing a clustering prototype bank and then refining masks in a separate stage; however, such two-stage pipelines are costly, sensitive to hyperparameters, and decouple prototype discovery from segmentation learning, limiting their effectiveness and efficiency. We propose a cluster-free, one-stage learnable-prototype framework with diversity regularization to enhance morphological intra-class heterogeneity coverage. Our approach achieves state-of-the-art (SOTA) performance on BCSS-WSSS, outperforming prior methods in mIoU and mDice. Qualitative segmentation maps show sharper boundaries and fewer mislabels, and activation heatmaps further reveal that, compared with clustering-based prototypes, our learnable prototypes cover more diverse and complementary regions within each class, providing consistent qualitative evidence for their effectiveness.", "AI": {"tldr": "提出一种用于弱监督组织病理学分割的聚类无关单阶段可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖", "motivation": "解决弱监督组织病理学分割中的三个主要挑战：类间同质性、类内异质性以及CAM引起的区域收缩问题。现有两阶段方法成本高、对超参数敏感且将原型发现与分割学习解耦，限制了效率和效果", "method": "提出聚类无关的单阶段可学习原型框架，通过多样性正则化增强形态学类内异质性覆盖。该方法避免了传统聚类原型库的构建，直接在分割学习过程中学习原型", "result": "在BCSS-WSSS数据集上达到最先进的性能，在mIoU和mDice指标上优于先前方法。定性分割图显示更清晰的边界和更少的错误标签，激活热图显示可学习原型覆盖了每个类别内更多样化和互补的区域", "conclusion": "提出的单阶段可学习原型框架通过多样性正则化有效解决了弱监督组织病理学分割中的关键挑战，相比传统两阶段聚类方法在性能和效率上都有显著提升"}}
{"id": "2512.05920", "pdf": "https://arxiv.org/pdf/2512.05920", "abs": "https://arxiv.org/abs/2512.05920", "authors": ["Jiawen Yang", "Yihui Cao", "Xuanyu Tian", "Yuyao Zhang", "Hongjiang Wei"], "title": "NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.", "AI": {"tldr": "提出NICE（神经隐式颅面模型），用于正颌手术的面部软组织变形预测，通过隐式神经表示方法提高预测精度和计算效率。", "motivation": "正颌手术中术后面部外观预测具有挑战性，现有方法（生物力学模型、参数化模型、深度学习方法）要么计算效率低，要么无法充分捕捉骨骼运动与面部软组织之间的复杂非线性相互作用。", "method": "NICE包含形状模块和手术模块：形状模块使用区域特定的隐式符号距离函数解码器重建面部表面、上颌骨和下颌骨；手术模块使用区域特定的变形解码器，由共享的手术潜在代码驱动，输出点位移场来建模手术结果。", "result": "实验表明NICE优于当前最先进方法，显著提高了关键面部区域（如嘴唇和下巴）的预测精度，同时稳健地保持了解剖完整性。", "conclusion": "NICE为增强正颌手术的手术规划和患者咨询提供了临床可行的工具，通过隐式神经表示有效建模了骨骼运动与面部软组织之间的复杂生物力学响应。"}}
{"id": "2512.05918", "pdf": "https://arxiv.org/pdf/2512.05918", "abs": "https://arxiv.org/abs/2512.05918", "authors": ["Xiaobo Wu", "Youmin Zhang"], "title": "A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following", "categories": ["eess.SP", "cs.RO", "stat.ML"], "comment": null, "summary": "Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol.", "AI": {"tldr": "提出一种基于残差方差匹配的递归最小二乘滤波器，用于无人机实时地形跟随中的航点估计", "motivation": "现有实时滤波算法在非线性时变系统中难以在测量噪声下保持准确的航点估计，这会影响无人机飞行安全和野火检测任务", "method": "提出残差方差匹配递归最小二乘滤波器，采用残差方差匹配估计准则自适应估计非线性时变无人机地形跟随系统的实时航点", "result": "在模拟地形环境中验证，RVM-RLS滤波器相比基准算法在多个评估指标上提高航点估计精度约88%", "conclusion": "该方法在实时滤波方法上取得进展，RVM-RLS滤波器在无人机在线野火巡逻中具有实际应用潜力"}}
{"id": "2512.05908", "pdf": "https://arxiv.org/pdf/2512.05908", "abs": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": "Accepted at LLM4Code Workshop, ICSE 2026", "summary": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "AI": {"tldr": "该论文提出了一种通过自然语言摘要实现多仓库微服务架构中bug定位的方法，将代码库转换为层次化的自然语言摘要，然后进行自然语言到自然语言的搜索，而不是跨模态检索。", "motivation": "在多仓库微服务架构中进行bug定位面临三大挑战：自然语言bug报告与代码之间的语义鸿沟、LLM上下文限制、以及需要首先识别正确的仓库。现有方法如检索增强生成(RAG)系统在处理大规模多仓库代码库时效果有限。", "method": "方法包括两个主要步骤：1) 将代码库转换为层次化的自然语言摘要（文件级、目录级、仓库级），2) 采用两阶段搜索策略：首先将bug报告路由到相关仓库，然后在选定的仓库内进行自上而下的定位（仓库->目录->文件）。", "result": "在包含46个仓库和110万行代码的工业系统DNext上评估，该方法实现了Pass@10为0.82，MRR为0.50，显著优于检索基线和GitHub Copilot、Cursor等代理式RAG系统。", "conclusion": "研究表明，工程化的自然语言表示比原始源代码在可扩展的bug定位中更有效，提供了可解释的仓库->目录->文件搜索路径，这对于在企业AI工具中建立信任和提供透明度至关重要。"}}
{"id": "2512.05906", "pdf": "https://arxiv.org/pdf/2512.05906", "abs": "https://arxiv.org/abs/2512.05906", "authors": ["Lennart P. L. Landsmeer", "Amirreza Movahedin", "Said Hamdioui", "Christos Strydis"], "title": "EventQueues: Autodifferentiable spike event queues for brain simulation on AI accelerators", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neural networks (SNNs), central to computational neuroscience and neuromorphic machine learning (ML), require efficient simulation and gradient-based training. While AI accelerators offer promising speedups, gradient-based SNNs typically implement sparse spike events using dense, memory-heavy data-structures. Existing exact gradient methods lack generality, and current simulators often omit or inefficiently handle delayed spikes. We address this by deriving gradient computation through spike event queues, including delays, and implementing memory-efficient, gradient-enabled event queue structures. These are benchmarked across CPU, GPU, TPU, and LPU platforms. We find that queue design strongly shapes performance. CPUs, as expected, perform well with traditional tree-based or FIFO implementations, while GPUs excel with ring buffers for smaller simulations, yet under higher memory pressure prefer more sparse data-structures. TPUs seem to favor an implementation based on sorting intrinsics. Selective spike dropping provides a simple performance-accuracy trade-off, which could be enhanced by future autograd frameworks adapting diverging primal/tangent data-structures.", "AI": {"tldr": "该论文提出了EventQueues，一种用于AI加速器上大脑模拟的可自动微分尖峰事件队列系统，解决了现有SNN模拟中梯度计算和延迟尖峰处理的效率问题。", "motivation": "脉冲神经网络在计算神经科学和神经形态机器学习中至关重要，但现有方法存在以下问题：1）梯度计算方法缺乏通用性；2）当前模拟器通常忽略或低效处理延迟尖峰；3）AI加速器上的稀疏尖峰事件通常使用密集、内存消耗大的数据结构实现。", "method": "通过尖峰事件队列推导梯度计算（包括延迟），并实现内存高效、支持梯度的队列结构。在CPU、GPU、TPU和LPU平台上进行基准测试，比较不同队列设计（树结构、FIFO、环形缓冲区、基于排序指令的实现）的性能。", "result": "队列设计对性能有显著影响：CPU在传统树结构或FIFO实现上表现良好；GPU在较小模拟中环形缓冲区表现优异，但在高内存压力下更适合稀疏数据结构；TPU偏好基于排序指令的实现。选择性尖峰丢弃提供了简单的性能-精度权衡。", "conclusion": "EventQueues为SNN模拟提供了内存高效、支持梯度的尖峰事件队列解决方案，未来自动微分框架可以通过适应不同的原始/切线数据结构来增强选择性尖峰丢弃的性能-精度权衡。"}}
{"id": "2512.05905", "pdf": "https://arxiv.org/pdf/2512.05905", "abs": "https://arxiv.org/abs/2512.05905", "authors": ["Wenhao Yan", "Sheng Ye", "Zhuoyi Yang", "Jiayan Teng", "ZhenHui Dong", "Kairui Wen", "Xiaotao Gu", "Yong-Jin Liu", "Jie Tang"], "title": "SCAIL: Towards Studio-Grade Character Animation via In-Context Learning of 3D-Consistent Pose Representations", "categories": ["cs.CV"], "comment": null, "summary": "Achieving character animation that meets studio-grade production standards remains challenging despite recent progress. Existing approaches can transfer motion from a driving video to a reference image, but often fail to preserve structural fidelity and temporal consistency in wild scenarios involving complex motion and cross-identity animations. In this work, we present \\textbf{SCAIL} (\\textbf{S}tudio-grade \\textbf{C}haracter \\textbf{A}nimation via \\textbf{I}n-context \\textbf{L}earning), a framework designed to address these challenges from two key innovations. First, we propose a novel 3D pose representation, providing a more robust and flexible motion signal. Second, we introduce a full-context pose injection mechanism within a diffusion-transformer architecture, enabling effective spatio-temporal reasoning over full motion sequences. To align with studio-level requirements, we develop a curated data pipeline ensuring both diversity and quality, and establish a comprehensive benchmark for systematic evaluation. Experiments show that \\textbf{SCAIL} achieves state-of-the-art performance and advances character animation toward studio-grade reliability and realism.", "AI": {"tldr": "SCAIL是一个通过上下文学习3D一致姿态表示来实现工作室级角色动画的框架", "motivation": "现有方法在复杂运动和跨身份动画的野外场景中难以保持结构保真度和时间一致性，无法满足工作室级生产标准", "method": "提出新颖的3D姿态表示和基于扩散变换器的全上下文姿态注入机制，构建了高质量的数据流水线和系统评估基准", "result": "SCAIL在实验中实现了最先进的性能，将角色动画推向工作室级的可靠性和真实感", "conclusion": "SCAIL通过创新的3D姿态表示和上下文学习机制，显著提升了角色动画的质量，向工作室级标准迈进"}}
{"id": "2512.05880", "pdf": "https://arxiv.org/pdf/2512.05880", "abs": "https://arxiv.org/abs/2512.05880", "authors": ["Simon Guiroy", "Mats Richter", "Sarath Chandar", "Christopher Pal"], "title": "Neural Coherence : Find higher performance to out-of-distribution tasks from few samples", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.", "AI": {"tldr": "该论文提出了一种名为\"神经一致性\"的新方法，用于在仅有少量无标签样本的情况下，从预训练模型中选择最佳检查点来处理分布外任务。", "motivation": "当前微调预训练视觉模型时，如何从大量训练检查点中选择最佳起点仍是一个开放问题。当目标任务数据稀缺、无标签且分布外时，依赖分布内验证数据的传统方法变得不可靠或不适用。", "method": "提出神经一致性概念，通过表征模型在源域和目标域的激活统计特性，定义具有高数据效率的模型选择方法。该方法仅需少量无标签目标域样本即可工作。", "result": "在ImageNet1K预训练模型上，针对Food-101、PlantNet-300K和iNaturalist等目标域进行实验，并在元学习设置中评估。相比现有基线方法，显著提高了跨不同目标域的泛化性能。", "conclusion": "神经一致性是一种有效的模型选择原则，能够在少量无标签样本下可靠工作，并展示了在训练数据选择中的多功能性，为解决分布外任务中的模型选择问题提供了新思路。"}}
{"id": "2512.05866", "pdf": "https://arxiv.org/pdf/2512.05866", "abs": "https://arxiv.org/abs/2512.05866", "authors": ["Md. Mahbub Hasan Akash", "Aria Tasnim Mridula", "Sheekar Banerjee", "Ishtiak Al Mamoon"], "title": "Underwater Image Reconstruction Using a Swin Transformer-Based Generator and PatchGAN Discriminator", "categories": ["cs.CV"], "comment": "This paper has been accepted for presentation at the IEEE 28th International Conference on Computer and Information Technology (ICCIT), December 2025", "summary": "Underwater imaging is essential for marine exploration, environmental monitoring, and infrastructure inspection. However, water causes severe image degradation through wavelength-dependent absorption and scattering, resulting in color distortion, low contrast, and haze effects. Traditional reconstruction methods and convolutional neural network-based approaches often fail to adequately address these challenges due to limited receptive fields and inability to model global dependencies. This paper presented a novel deep learning framework that integrated a Swin Transformer architecture within a generative adversarial network (GAN) for underwater image reconstruction. Our generator employed a U-Net structure with Swin Transformer blocks to capture both local features and long-range dependencies crucial for color correction across entire images. A PatchGAN discriminator provided adversarial training to ensure high-frequency detail preservation. We trained and evaluated our model on the EUVP dataset, which contains paired underwater images of varying quality. Quantitative results demonstrate stateof-the-art performance with PSNR of 24.76 dB and SSIM of 0.89, representing significant improvements over existing methods. Visual results showed effective color balance restoration, contrast improvement, and haze reduction. An ablation study confirms the superiority of our Swin Transformer designed over convolutional alternatives. The proposed method offers robust underwater image reconstruction suitable for various marine applications.", "AI": {"tldr": "提出了一种基于Swin Transformer和PatchGAN判别器的水下图像重建方法，用于解决水下图像的颜色失真、低对比度和雾化效应问题。", "motivation": "水下成像对海洋探索、环境监测和基础设施检查至关重要，但水对光的波长依赖性吸收和散射会导致严重的图像退化，包括颜色失真、低对比度和雾化效应。传统方法和基于卷积神经网络的方法由于感受野有限且无法建模全局依赖关系，往往无法充分应对这些挑战。", "method": "提出了一种新颖的深度学习框架，将Swin Transformer架构集成到生成对抗网络(GAN)中。生成器采用带有Swin Transformer块的U-Net结构，以捕获局部特征和长距离依赖关系，这对整个图像的颜色校正至关重要。使用PatchGAN判别器进行对抗训练，确保高频细节的保留。", "result": "在EUVP数据集上进行训练和评估，定量结果显示PSNR达到24.76 dB，SSIM达到0.89，优于现有方法。视觉结果显示有效的颜色平衡恢复、对比度改善和雾化减少。消融研究证实了Swin Transformer设计相对于卷积替代方案的优越性。", "conclusion": "该方法为各种海洋应用提供了稳健的水下图像重建解决方案，通过结合Swin Transformer的全局建模能力和GAN的对抗训练机制，有效解决了水下图像退化的关键问题。"}}
{"id": "2512.05865", "pdf": "https://arxiv.org/pdf/2512.05865", "abs": "https://arxiv.org/abs/2512.05865", "authors": ["Florent Draye", "Anson Lei", "Ingmar Posner", "Bernhard Schölkopf"], "title": "Sparse Attention Post-Training for Mechanistic Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\\approx 0.3 \\%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.", "AI": {"tldr": "提出一种简单的后训练方法，使Transformer注意力机制变得稀疏而不损失性能，通过稀疏正则化将注意力连接减少到约0.3%，同时保持原始预训练损失", "motivation": "现有稀疏注意力方法主要关注计算效率，而本文利用稀疏性作为结构先验，旨在通过稀疏化暴露更有组织和可解释的连接模式，为机制可解释性提供支持", "method": "采用灵活稀疏正则化的约束损失目标，在高达10亿参数的模型上进行后训练，使注意力连接稀疏化，同时保持原始预训练性能", "result": "成功将注意力连接减少到约0.3%的边，局部稀疏性级联为全局电路简化：任务特定电路涉及更少的组件（注意力头和MLP），连接边减少高达100倍", "conclusion": "Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可作为构建更有结构和可解释模型的指导原则"}}
{"id": "2512.05863", "pdf": "https://arxiv.org/pdf/2512.05863", "abs": "https://arxiv.org/abs/2512.05863", "authors": ["Tasnimul Hassan", "Md Faisal Karim", "Haziq Jeelani", "Elham Behnam", "Robert Green", "Fayeq Jeelani Syed"], "title": "Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.", "AI": {"tldr": "比较微调与零样本大语言模型在RAG框架下优化医疗问答系统的性能", "motivation": "直接将大语言模型应用于临床领域存在事实准确性不足和幻觉问题，需要结合领域专业知识来提高医疗问答系统的可靠性", "method": "采用检索增强生成(RAG)框架，结合医疗文献检索与开源大语言模型；使用LoRA对LLaMA 2和Falcon进行高效微调；在PubMedQA和MedMCQA基准数据集上评估", "result": "微调的LLaMA 2模型在PubMedQA上达到71.8%准确率，相比55.4%的零样本基线显著提升；检索增强使无支持内容减少约60%", "conclusion": "RAG增强的开源大语言模型在生物医学问答中具有实际应用潜力，通过检索证据提高事实正确性，为临床信息学应用提供可靠解决方案"}}
{"id": "2512.05859", "pdf": "https://arxiv.org/pdf/2512.05859", "abs": "https://arxiv.org/abs/2512.05859", "authors": ["Abhijith Punnappurath", "Luxi Zhao", "Ke Zhao", "Hue Nguyen", "Radek Grzeszczuk", "Michael S. Brown"], "title": "Edit-aware RAW Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Users frequently edit camera images post-capture to achieve their preferred photofinishing style. While editing in the RAW domain provides greater accuracy and flexibility, most edits are performed on the camera's display-referred output (e.g., 8-bit sRGB JPEG) since RAW images are rarely stored. Existing RAW reconstruction methods can recover RAW data from sRGB images, but these approaches are typically optimized for pixel-wise RAW reconstruction fidelity and tend to degrade under diverse rendering styles and editing operations. We introduce a plug-and-play, edit-aware loss function that can be integrated into any existing RAW reconstruction framework to make the recovered RAWs more robust to different rendering styles and edits. Our loss formulation incorporates a modular, differentiable image signal processor (ISP) that simulates realistic photofinishing pipelines with tunable parameters. During training, parameters for each ISP module are randomly sampled from carefully designed distributions that model practical variations in real camera processing. The loss is then computed in sRGB space between ground-truth and reconstructed RAWs rendered through this differentiable ISP. Incorporating our loss improves sRGB reconstruction quality by up to 1.5-2 dB PSNR across various editing conditions. Moreover, when applied to metadata-assisted RAW reconstruction methods, our approach enables fine-tuning for target edits, yielding further gains. Since photographic editing is the primary motivation for RAW reconstruction in consumer imaging, our simple yet effective loss function provides a general mechanism for enhancing edit fidelity and rendering flexibility across existing methods.", "AI": {"tldr": "提出一种可插拔的编辑感知损失函数，用于改进从sRGB图像重建RAW数据的方法，使重建的RAW数据对不同渲染风格和编辑操作更具鲁棒性。", "motivation": "用户经常在拍摄后编辑相机图像以达到偏好的照片处理风格。虽然RAW域编辑提供更高的准确性和灵活性，但大多数编辑是在相机的显示参考输出（如8位sRGB JPEG）上进行的，因为RAW图像很少被存储。现有的RAW重建方法可以从sRGB图像恢复RAW数据，但这些方法通常针对像素级RAW重建保真度进行优化，在不同渲染风格和编辑操作下性能会下降。", "method": "引入一个可插拔的编辑感知损失函数，可以集成到任何现有的RAW重建框架中。损失函数包含一个模块化、可微分的图像信号处理器（ISP），模拟具有可调参数的真实照片处理流程。在训练期间，每个ISP模块的参数从精心设计的分布中随机采样，这些分布模拟了实际相机处理中的变化。损失在sRGB空间中计算，通过这个可微分ISP渲染的地面真实RAW和重建RAW之间的差异。", "result": "在各种编辑条件下，结合该损失函数可将sRGB重建质量提高1.5-2 dB PSNR。当应用于元数据辅助的RAW重建方法时，该方法能够针对目标编辑进行微调，获得进一步的增益。", "conclusion": "由于照片编辑是消费级成像中RAW重建的主要动机，这个简单而有效的损失函数提供了一种通用机制，可以增强现有方法的编辑保真度和渲染灵活性。"}}
{"id": "2512.05853", "pdf": "https://arxiv.org/pdf/2512.05853", "abs": "https://arxiv.org/abs/2512.05853", "authors": ["Shiji Zhao", "Shukun Xiong", "Yao Huang", "Yan Jin", "Zhenyu Wu", "Jiyang Guan", "Ranjie Duan", "Jialing Tao", "Hui Xue", "Xingxing Wei"], "title": "VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are widely used in various fields due to their powerful cross-modal comprehension and generation capabilities. However, more modalities bring more vulnerabilities to being utilized for jailbreak attacks, which induces MLLMs to output harmful content. Due to the strong reasoning ability of MLLMs, previous jailbreak attacks try to explore reasoning safety risk in text modal, while similar threats have been largely overlooked in the visual modal. To fully evaluate potential safety risks in the visual reasoning task, we propose Visual Reasoning Sequential Attack (VRSA), which induces MLLMs to gradually externalize and aggregate complete harmful intent by decomposing the original harmful text into several sequentially related sub-images. In particular, to enhance the rationality of the scene in the image sequence, we propose Adaptive Scene Refinement to optimize the scene most relevant to the original harmful query. To ensure the semantic continuity of the generated image, we propose Semantic Coherent Completion to iteratively rewrite each sub-text combined with contextual information in this scene. In addition, we propose Text-Image Consistency Alignment to keep the semantical consistency. A series of experiments demonstrates that the VRSA can achieve a higher attack success rate compared with the state-of-the-art jailbreak attack methods on both the open-source and closed-source MLLMs such as GPT-4o and Claude-4.5-Sonnet.", "AI": {"tldr": "提出一种针对多模态大语言模型的视觉推理序列攻击方法，通过将有害文本分解为多个顺序相关的子图像来诱导模型输出有害内容", "motivation": "多模态大语言模型在视觉模态中的安全风险被忽视，现有攻击主要关注文本模态，需要全面评估视觉推理任务中的潜在安全风险", "method": "提出VRSA攻击方法，包含三个关键技术：自适应场景优化、语义连贯补全和文本-图像一致性对齐，通过分解有害文本为序列图像进行攻击", "result": "实验表明VRSA在开源和闭源MLLMs（包括GPT-4o和Claude-4.5-Sonnet）上相比现有攻击方法具有更高的攻击成功率", "conclusion": "多模态大语言模型在视觉推理任务中存在严重安全漏洞，VRSA方法有效揭示了这一风险，需要加强多模态安全防护"}}
{"id": "2512.05844", "pdf": "https://arxiv.org/pdf/2512.05844", "abs": "https://arxiv.org/abs/2512.05844", "authors": ["Daniel Rose", "Roxane Axel Jacob", "Johannes Kirchmair", "Thierry Langer"], "title": "NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.", "AI": {"tldr": "NEAT是一种用于3D分子生成的自回归集合变换器，通过邻域引导和集合处理实现原子排列不变性", "motivation": "现有自回归模型在3D分子生成中需要假设标记顺序，而分子图中的下一个原子预测应该对原子排列具有不变性。先前工作使用规范顺序或焦点原子来回避这个问题，作者认为这是不必要的", "method": "NEAT将分子图视为原子集合，使用邻域引导的高效自回归集合变换器，通过自回归流模型学习图边界上可接受标记的顺序无关分布", "result": "NEAT在3D分子生成中达到了最先进的性能，具有高计算效率和原子级排列不变性", "conclusion": "NEAT为可扩展的分子设计建立了实用基础，展示了自回归模型作为扩散模型替代方案的潜力"}}
{"id": "2512.05837", "pdf": "https://arxiv.org/pdf/2512.05837", "abs": "https://arxiv.org/abs/2512.05837", "authors": ["Kaichen Ouyang"], "title": "Wasserstein Evolution : Evolutionary Optimization as Phase Transition", "categories": ["cs.NE"], "comment": "19 pages, 5 figures", "summary": "This paper establishes a novel connection between evolutionary computation and statistical physics by formalizing evolutionary optimization as a phase transition process. We introduce Wasserstein Evolution (WE), a principled optimization framework that implements the Wasserstein gradient flow of a free energy functional, mathematically bridging evolutionary dynamics with thermodynamics. WE directly translates the physical competition between potential gradient forces (exploitation) and entropic forces (exploration) into algorithmic dynamics, providing an adaptive, theoretically grounded mechanism for balancing exploration and exploitation. Experiments on challenging benchmark functions demonstrate that WE achieves competitive convergence performance while maintaining dramatically higher population diversity than classical methods (GA, DE, CMA-ES).This superior entropy preservation enables effective navigation of multi-modal landscapes without premature convergence, validating the physical interpretation of optimization as a disorder-to-order transition. Our work provides not only an effective optimization algorithm but also a new paradigm for understanding evolutionary computation through statistical physics.", "AI": {"tldr": "该论文提出了一种将进化计算与统计物理学连接的新框架，将进化优化形式化为相变过程，引入了Wasserstein Evolution（WE）优化框架。", "motivation": "建立进化计算与统计物理学之间的理论联系，为进化优化提供一个物理基础的解释框架，解决传统方法中探索与开发平衡的理论基础问题。", "method": "引入Wasserstein Evolution（WE）框架，实现Wasserstein梯度流在自由能泛函上的应用，将势梯度力（开发）与熵力（探索）的物理竞争转化为算法动力学。", "result": "在挑战性基准函数上的实验表明，WE实现了有竞争力的收敛性能，同时保持了比经典方法（GA、DE、CMA-ES）显著更高的种群多样性，有效避免了早熟收敛。", "conclusion": "该工作不仅提供了一个有效的优化算法，还通过统计物理学为理解进化计算提供了一个新范式，将优化解释为从无序到有序的相变过程。"}}
{"id": "2512.05836", "pdf": "https://arxiv.org/pdf/2512.05836", "abs": "https://arxiv.org/abs/2512.05836", "authors": ["Clarissa W. Ong", "Hiba Arnaout", "Kate Sheehan", "Estella Fox", "Eugen Owtscharow", "Iryna Gurevych"], "title": "Using Large Language Models to Create Personalized Networks From Therapy Sessions", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in psychotherapy have focused on treatment personalization, such as by selecting treatment modules based on personalized networks. However, estimating personalized networks typically requires intensive longitudinal data, which is not always feasible. A solution to facilitate scalability of network-driven treatment personalization is leveraging LLMs. In this study, we present an end-to-end pipeline for automatically generating client networks from 77 therapy transcripts to support case conceptualization and treatment planning. We annotated 3364 psychological processes and their corresponding dimensions in therapy transcripts. Using these data, we applied in-context learning to jointly identify psychological processes and their dimensions. The method achieved high performance even with a few training examples. To organize the processes into networks, we introduced a two-step method that grouped them into clinically meaningful clusters. We then generated explanation-augmented relationships between clusters. Experts found that networks produced by our multi-step approach outperformed those built with direct prompting for clinical utility and interpretability, with up to 90% preferring our approach. In addition, the networks were rated favorably by experts, with scores for clinical relevance, novelty, and usefulness ranging from 72-75%. Our findings provide a proof of concept for using LLMs to create clinically relevant networks from therapy transcripts. Advantages of our approach include bottom-up case conceptualization from client utterances in therapy sessions and identification of latent themes. Networks generated from our pipeline may be used in clinical settings and supervision and training. Future research should examine whether these networks improve treatment outcomes relative to other methods of treatment personalization, including statistically estimated networks.", "AI": {"tldr": "开发一个端到端管道，利用大型语言模型从治疗会话转录本中自动生成个性化心理网络，以支持案例概念化和治疗规划。", "motivation": "心理治疗个性化需要基于个性化网络选择治疗模块，但传统方法需要密集的纵向数据，难以规模化。本研究旨在利用LLMs解决这一可扩展性问题。", "method": "1) 标注77个治疗转录本中的3364个心理过程及其维度；2) 使用上下文学习联合识别心理过程及其维度；3) 引入两步法将过程分组为临床有意义的聚类；4) 生成解释增强的聚类间关系。", "result": "方法在少量训练示例下实现高性能；专家评估显示，多步方法在临床效用和可解释性上优于直接提示法（90%专家偏好）；网络在临床相关性、新颖性和有用性方面获得72-75%的评分。", "conclusion": "研究证明了使用LLMs从治疗转录本创建临床相关网络的可行性，具有自下而上的案例概念化和潜在主题识别优势。未来研究应比较这些网络与其他个性化方法对治疗结果的影响。"}}
{"id": "2512.05830", "pdf": "https://arxiv.org/pdf/2512.05830", "abs": "https://arxiv.org/abs/2512.05830", "authors": ["Muhammet Cagri Yeke", "Samil Sirin", "Kivilcim Yuksel", "Abdurrahman Gumus"], "title": "Phase-OTDR Event Detection Using Image-Based Data Transformation and Deep Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 11 figures, 5 tables", "summary": "This study focuses on event detection in optical fibers, specifically classifying six events using the Phase-OTDR system. A novel approach is introduced to enhance Phase-OTDR data analysis by transforming 1D data into grayscale images through techniques such as Gramian Angular Difference Field, Gramian Angular Summation Field, and Recurrence Plot. These grayscale images are combined into a multi-channel RGB representation, enabling more robust and adaptable analysis using transfer learning models. The proposed methodology achieves high classification accuracies of 98.84% and 98.24% with the EfficientNetB0 and DenseNet121 models, respectively. A 5-fold cross-validation process confirms the reliability of these models, with test accuracy rates of 99.07% and 98.68%. Using a publicly available Phase-OTDR dataset, the study demonstrates an efficient approach to understanding optical fiber events while reducing dataset size and improving analysis efficiency. The results highlight the transformative potential of image-based analysis in interpreting complex fiber optic sensing data, offering significant advancements in the accuracy and reliability of fiber optic monitoring systems. The codes and the corresponding image-based dataset are made publicly available on GitHub to support further research: https://github.com/miralab-ai/Phase-OTDR-event-detection.", "AI": {"tldr": "该论文提出了一种基于图像转换和深度学习的Phase-OTDR事件检测方法，通过将一维数据转换为灰度图像，再组合成多通道RGB表示，利用迁移学习模型实现光纤事件的准确分类。", "motivation": "传统Phase-OTDR数据分析方法在处理复杂光纤事件时存在局限性，需要更鲁棒和高效的分析技术来准确分类不同类型的光纤事件，提高光纤监控系统的准确性和可靠性。", "method": "采用三种图像转换技术（Gramian Angular Difference Field、Gramian Angular Summation Field和Recurrence Plot）将一维Phase-OTDR数据转换为灰度图像，然后将这些图像组合成多通道RGB表示，最后使用迁移学习模型（EfficientNetB0和DenseNet121）进行分类。", "result": "该方法在六类光纤事件分类任务中取得了高准确率：EfficientNetB0达到98.84%，DenseNet121达到98.24%。通过5折交叉验证，测试准确率分别为99.07%和98.68%，证明了模型的可靠性。", "conclusion": "图像转换方法显著提升了Phase-OTDR数据分析的效果，减少了数据集大小并提高了分析效率，为光纤传感数据的解释提供了新的视角，推动了光纤监控系统的技术进步。"}}
{"id": "2512.05825", "pdf": "https://arxiv.org/pdf/2512.05825", "abs": "https://arxiv.org/abs/2512.05825", "authors": ["Shuhei Watanabe"], "title": "Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\\lfloor \\frac{M + 1}{2} \\rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.", "AI": {"tldr": "提出一种近似盒分解算法，用于加速基于超体积的多目标优化中的获取函数优化", "motivation": "超体积改进计算是贝叶斯优化中的主要计算瓶颈，现有盒分解算法存在超多项式内存复杂度问题，需要更高效的近似方法", "method": "提供Couckuyt等人近似算法的完整数学和算法描述，填补文献中的空白", "result": "为超体积盒分解近似算法提供了严格的算法描述，解决了现有文献中缺乏详细描述的问题", "conclusion": "该近似算法能够有效降低超体积计算的内存复杂度，为基于超体积的多目标优化提供更高效的实现方案"}}
{"id": "2512.05824", "pdf": "https://arxiv.org/pdf/2512.05824", "abs": "https://arxiv.org/abs/2512.05824", "authors": ["Hafsa Akebli", "Adam Shephard", "Vincenzo Della Mea", "Nasir Rajpoot"], "title": "Multimodal Oncology Agent for IDH1 Mutation Prediction in Low-Grade Glioma", "categories": ["cs.AI", "cs.CV"], "comment": "4 pages, 2 figures", "summary": "Low-grade gliomas frequently present IDH1 mutations that define clinically distinct subgroups with specific prognostic and therapeutic implications. This work introduces a Multimodal Oncology Agent (MOA) integrating a histology tool based on the TITAN foundation model for IDH1 mutation prediction in low-grade glioma, combined with reasoning over structured clinical and genomic inputs through PubMed, Google Search, and OncoKB. MOA reports were quantitatively evaluated on 488 patients from the TCGA-LGG cohort against clinical and histology baselines. MOA without the histology tool outperformed the clinical baseline, achieving an F1-score of 0.826 compared to 0.798. When fused with histology features, MOA reached the highest performance with an F1-score of 0.912, exceeding both the histology baseline at 0.894 and the fused histology-clinical baseline at 0.897. These results demonstrate that the proposed agent captures complementary mutation-relevant information enriched through external biomedical sources, enabling accurate IDH1 mutation prediction.", "AI": {"tldr": "开发一个多模态肿瘤学智能体，用于预测低级别胶质瘤中的IDH1基因突变", "motivation": "低级别胶质瘤中IDH1突变定义了具有不同预后和治疗意义的临床亚组，准确预测IDH1突变状态对临床决策至关重要", "method": "提出多模态肿瘤学智能体，整合基于TITAN基础模型的病理学工具进行IDH1突变预测，结合通过PubMed、Google Search和OncoKB对结构化临床和基因组输入进行推理", "result": "在TCGA-LGG队列的488名患者中评估，MOA无病理学工具时F1分数0.826优于临床基线0.798；融合病理学特征后达到最高性能F1分数0.912，超过病理学基线0.894和融合病理学-临床基线0.897", "conclusion": "该智能体通过外部生物医学资源捕获了互补的突变相关信息，实现了准确的IDH1突变预测，展示了多模态整合在肿瘤学预测中的优势"}}
{"id": "2512.05815", "pdf": "https://arxiv.org/pdf/2512.05815", "abs": "https://arxiv.org/abs/2512.05815", "authors": ["Marios-Nektarios Stamatopoulos", "Shridhar Velhal", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Optimal Safety-Aware Scheduling for Multi-Agent Aerial 3D Printing with Utility Maximization under Dependency Constraints", "categories": ["cs.RO"], "comment": null, "summary": "This article presents a novel coordination and task-planning framework to enable the simultaneous conflict-free collaboration of multiple unmanned aerial vehicles (UAVs) for aerial 3D printing. The proposed framework formulates an optimization problem that takes a construction mission divided into sub-tasks and a team of autonomous UAVs, along with limited volume and battery. It generates an optimal mission plan comprising task assignments and scheduling while accounting for task dependencies arising from the geometric and structural requirements of the 3D design, inter-UAV safety constraints, material usage, and total flight time of each UAV. The potential conflicts occurring during the simultaneous operation of the UAVs are addressed at a segment level by dynamically selecting the starting time and location of each task to guarantee collision-free parallel execution. An importance prioritization is proposed to accelerate the computation by guiding the solution toward more important tasks. Additionally, a utility maximization formulation is proposed to dynamically determine the optimal number of UAVs required for a given mission, balancing the trade-off between minimizing makespan and the deployment of excess agents. The proposed framework's effectiveness is evaluated through a Gazebo-based simulation setup, where agents are coordinated by a mission control module allocating the printing tasks based on the generated optimal scheduling plan while remaining within the material and battery constraints of each UAV.", "AI": {"tldr": "提出一个用于多无人机空中3D打印的最优安全感知调度框架，在依赖约束下实现效用最大化", "motivation": "解决多无人机同时进行空中3D打印时的协调和任务规划问题，需要考虑任务依赖关系、安全约束、材料使用和电池限制等复杂因素", "method": "提出一个优化问题框架，考虑任务依赖关系、无人机间安全约束、材料使用和飞行时间；采用重要性优先级加速计算；通过效用最大化动态确定最优无人机数量", "result": "通过Gazebo仿真验证了框架的有效性，能够生成无冲突的任务分配和调度计划，在材料、电池和安全性约束下实现最优任务执行", "conclusion": "该框架成功解决了多无人机空中3D打印的协调问题，实现了安全感知的最优调度，在最小化任务完成时间和无人机部署数量之间取得了平衡"}}
{"id": "2512.05814", "pdf": "https://arxiv.org/pdf/2512.05814", "abs": "https://arxiv.org/abs/2512.05814", "authors": ["Fubao Zhu", "Zhanyuan Jia", "Zhiguo Wang", "Huan Huang", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chen Zhao", "Weihua Zhou"], "title": "UG-FedDA: Uncertainty-Guided Federated Domain Adaptation for Multi-Center Alzheimer's Disease Detection", "categories": ["cs.CV"], "comment": "The code is already available on GitHub: https://github.com/chenzhao2023/UG_FADDA_AlzhemiersClassification", "summary": "Alzheimer's disease (AD) is an irreversible neurodegenerative disorder, and early diagnosis is critical for timely intervention. However, most existing classification frameworks face challenges in multicenter studies, as they often neglect inter-site heterogeneity and lack mechanisms to quantify uncertainty, which limits their robustness and clinical applicability. To address these issues, we proposed Uncertainty-Guided Federated Domain Adaptation (UG-FedDA), a novel multicenter AD classification framework that integrates uncertainty quantification (UQ) with federated domain adaptation to handle cross-site structure magnetic resonance imaging (MRI) heterogeneity under privacy constraints. Our approach extracts multi-template region-of-interest (RoI) features using a self-attention transformer, capturing both regional representations and their interactions. UQ is integrated to guide feature alignment, mitigating source-target distribution shifts by down-weighting uncertain samples. Experiments are conducted on three public datasets: the Alzheimer's Disease Neuroimaging Initiative (ADNI), the Australian Imaging, Biomarkers and Lifestyle study (AIBL), and the Open Access Series of Imaging Studies (OASIS). UG-FedDA achieved consistent cross-domain improvements in accuracy, sensitivity, and area under the ROC curve across three classification tasks: AD vs. normal controls (NC), mild cognitive impairment (MCI) vs. AD, and NC vs. MCI. For NC vs. AD, UG-FedDA achieves accuracies of 90.54%, 89.04%, and 77.78% on ADNI, AIBL and OASIS datasets, respectively. For MCI vs. AD, accuracies are 80.20% (ADNI), 71.91% (AIBL), and 79.73% (OASIS). For NC vs. MCI, results are 76.87% (ADNI), 73.91% (AIBL), and 83.73% (OASIS). These results demonstrate that the proposed framework not only adapts efficiently across multiple sites but also preserves strict privacy.", "AI": {"tldr": "提出UG-FedDA框架，将不确定性量化与联邦域适应结合，用于多中心阿尔茨海默病检测，解决跨站点异质性和隐私保护问题。", "motivation": "现有阿尔茨海默病分类框架在多中心研究中面临挑战：忽视站点间异质性、缺乏不确定性量化机制，限制了鲁棒性和临床适用性。", "method": "提出不确定性引导的联邦域适应框架，使用自注意力transformer提取多模板ROI特征，集成不确定性量化指导特征对齐，通过降低不确定样本权重来缓解源-目标分布偏移。", "result": "在ADNI、AIBL和OASIS三个公开数据集上，UG-FedDA在AD vs. NC、MCI vs. AD、NC vs. MCI三个分类任务中均取得一致的跨域改进，准确率分别为90.54%/89.04%/77.78%、80.20%/71.91%/79.73%、76.87%/73.91%/83.73%。", "conclusion": "该框架不仅能在多个站点间高效适应，还能保持严格的隐私保护，为多中心阿尔茨海默病检测提供了有效的解决方案。"}}
{"id": "2512.05812", "pdf": "https://arxiv.org/pdf/2512.05812", "abs": "https://arxiv.org/abs/2512.05812", "authors": ["Fabian Konstantinidis", "Moritz Sackmann", "Ulrich Hofmann", "Christoph Stiller"], "title": "Toward Efficient and Robust Behavior Models for Multi-Agent Driving Simulation", "categories": ["cs.RO", "cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Scalable multi-agent driving simulation requires behavior models that are both realistic and computationally efficient. We address this by optimizing the behavior model that controls individual traffic participants. To improve efficiency, we adopt an instance-centric scene representation, where each traffic participant and map element is modeled in its own local coordinate frame. This design enables efficient, viewpoint-invariant scene encoding and allows static map tokens to be reused across simulation steps. To model interactions, we employ a query-centric symmetric context encoder with relative positional encodings between local frames. We use Adversarial Inverse Reinforcement Learning to learn the behavior model and propose an adaptive reward transformation that automatically balances robustness and realism during training. Experiments demonstrate that our approach scales efficiently with the number of tokens, significantly reducing training and inference times, while outperforming several agent-centric baselines in terms of positional accuracy and robustness.", "AI": {"tldr": "提出一种高效且鲁棒的多智能体驾驶仿真行为模型，通过实例中心化场景表示和查询中心对称上下文编码器来优化交通参与者的行为控制。", "motivation": "可扩展的多智能体驾驶仿真需要既真实又计算高效的行为模型。现有方法在计算效率和交互建模方面存在不足，需要一种能够平衡效率和鲁棒性的解决方案。", "method": "采用实例中心化场景表示（每个交通参与者和地图元素在自身局部坐标系中建模），使用查询中心对称上下文编码器处理局部框架间的相对位置编码，通过对抗性逆强化学习训练行为模型，并提出自适应奖励变换来平衡鲁棒性和真实性。", "result": "实验表明该方法能够随token数量高效扩展，显著减少训练和推理时间，在位置准确性和鲁棒性方面优于多个智能体中心化基线方法。", "conclusion": "提出的方法通过实例中心化表示和对称上下文编码实现了高效且鲁棒的多智能体驾驶仿真行为建模，在计算效率和性能方面均表现出色。"}}
{"id": "2512.05809", "pdf": "https://arxiv.org/pdf/2512.05809", "abs": "https://arxiv.org/abs/2512.05809", "authors": ["Saurav Jha", "M. Jehanzeb Mirza", "Wei Lin", "Shiqi Yang", "Sarath Chandar"], "title": "Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling", "categories": ["cs.CV", "cs.AI"], "comment": "Extended abstract at World Modeling Workshop 2026", "summary": "Vision-Language Models (VLMs) remain limited in spatial reasoning tasks that require multi-view understanding and embodied perspective shifts. Recent approaches such as MindJourney attempt to mitigate this gap through test-time scaling where a world model imagines action-conditioned trajectories and a heuristic verifier selects helpful views from such trajectories. In this work, we systematically examine how such test-time verifiers behave across benchmarks, uncovering both their promise and their pitfalls. Our uncertainty-based analyses show that MindJourney's verifier provides little meaningful calibration, and that random scoring often reduces answer entropy equally well, thus exposing systematic action biases and unreliable reward signals. To mitigate these, we introduce a Verification through Spatial Assertions (ViSA) framework that grounds the test-time reward in verifiable, frame-anchored micro-claims. This principled verifier consistently improves spatial reasoning on the SAT-Real benchmark and corrects trajectory-selection biases through more balanced exploratory behavior. However, on the challenging MMSI-Bench, none of the verifiers, including ours, achieve consistent scaling, suggesting that the current world models form an information bottleneck where imagined views fail to enrich fine-grained reasoning. Together, these findings chart the bad, good, and ugly aspects of test-time verification for world-model-based reasoning. Our code is available at https://github.com/chandar-lab/visa-for-mindjourney.", "AI": {"tldr": "本文系统评估了世界模型在空间推理任务中的测试时缩放效果，揭示了现有验证器的局限性，并提出了一种基于空间断言的验证框架。", "motivation": "视觉语言模型在需要多视角理解和具身视角转换的空间推理任务中表现有限。现有方法如MindJourney通过测试时缩放让世界模型想象动作条件轨迹，并使用启发式验证器选择有用视角，但其有效性尚未得到系统评估。", "method": "提出了Verification through Spatial Assertions (ViSA)框架，将测试时奖励建立在可验证的、帧锚定的微观断言基础上，以纠正轨迹选择偏差并实现更平衡的探索行为。", "result": "在SAT-Real基准测试中，ViSA框架显著提升了空间推理性能，并纠正了轨迹选择偏差。但在更具挑战性的MMSI-Bench上，包括ViSA在内的所有验证器都无法实现一致的缩放，表明当前世界模型形成了信息瓶颈。", "conclusion": "研究揭示了基于世界模型的测试时验证的\"坏、好、丑\"三个方面：现有验证器缺乏有意义的校准，ViSA框架在部分任务上表现良好，但世界模型的想象视图无法丰富细粒度推理，形成了信息瓶颈。"}}
{"id": "2512.05808", "pdf": "https://arxiv.org/pdf/2512.05808", "abs": "https://arxiv.org/abs/2512.05808", "authors": ["Sushmita Bhattacharya", "Ninad Jadhav", "Hammad Izhar", "Karen Li", "Kevin George", "Robert Wood", "Stephanie Gil"], "title": "Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots", "categories": ["cs.RO"], "comment": "ef:International Symposium of Experimental Robotics 2025", "summary": "We introduce a system for real-time sperm whale rendezvous at sea using an autonomous uncrewed aerial vehicle. Our system employs model-based reinforcement learning that combines in situ sensor data with an empirical whale dive model to guide navigation decisions. Key challenges include (i) real-time acoustic tracking in the presence of multiple whales, (ii) distributed communication and decision-making for robot deployments, and (iii) on-board signal processing and long-range detection from fish-trackers. We evaluate our system by conducting rendezvous with sperm whales at sea in Dominica, performing hardware experiments on land, and running simulations using whale trajectories interpolated from marine biologists' surface observations.", "AI": {"tldr": "开发了一个使用自主无人机实时与抹香鲸海上会合的系统，结合模型强化学习和现场传感器数据来指导导航决策", "motivation": "需要解决在海上实时追踪抹香鲸并实现自主会合的挑战，包括多鲸鱼环境下的实时声学追踪、分布式通信决策以及机载信号处理等问题", "method": "采用基于模型的强化学习方法，将现场传感器数据与经验性鲸鱼潜水模型相结合来指导导航决策，系统包括实时声学追踪、分布式通信和机载信号处理", "result": "在多米尼加海域成功进行了与抹香鲸的会合实验，同时进行了陆地硬件实验和基于海洋生物学家表面观测数据的鲸鱼轨迹模拟", "conclusion": "成功开发了一个能够实时追踪抹香鲸并实现自主会合的系统，验证了模型强化学习方法在复杂海洋环境中的有效性"}}
{"id": "2512.05806", "pdf": "https://arxiv.org/pdf/2512.05806", "abs": "https://arxiv.org/abs/2512.05806", "authors": ["Martino Gulisano", "Marco Gabiccini"], "title": "Global stability of vehicle-with-driver dynamics via Sum-of-Squares programming", "categories": ["cs.RO", "eess.SY"], "comment": "20 pages, 7 figures, 2 tables", "summary": "This work estimates safe invariant subsets of the Region of Attraction (ROA) for a seven-state vehicle-with-driver system, capturing both asymptotic stability and the influence of state-safety bounds along the system trajectory. Safe sets are computed by optimizing Lyapunov functions through an original iterative Sum-of-Squares (SOS) procedure. The method is first demonstrated on a two-state benchmark, where it accurately recovers a prescribed safe region as the 1-level set of a polynomial Lyapunov function. We then describe the distinguishing characteristics of the studied vehicle-with-driver system: the control dynamics mimic human driver behavior through a delayed preview-tracking model that, with suitable parameter choices, can also emulate digital controllers. To enable SOS optimization, a polynomial approximation of the nonlinear vehicle model is derived, together with its operating-envelope constraints. The framework is then applied to understeering and oversteering scenarios, and the estimated safe sets are compared with reference boundaries obtained from exhaustive simulations. The results show that SOS techniques can efficiently deliver Lyapunov-defined safe regions, supporting their potential use for real-time safety assessment, for example as a supervisory layer for active vehicle control.", "AI": {"tldr": "该论文通过Sum-of-Squares（SOS）编程方法估计车辆-驾驶员系统的安全不变子集，用于评估实时安全性并支持主动车辆控制", "motivation": "需要为车辆-驾驶员系统提供安全评估方法，该系统包含7个状态变量，既有渐近稳定性要求，又需要考虑轨迹过程中的状态安全边界", "method": "采用原始迭代SOS程序优化Lyapunov函数，首先在二状态基准测试中验证，然后对非线性车辆模型进行多项式近似，并考虑操作包络约束，最后应用于转向不足和转向过度场景", "result": "SOS技术能有效生成Lyapunov定义的安全区域，与详尽仿真获得的参考边界相比，结果验证了方法的准确性，支持实时安全评估的潜在应用", "conclusion": "SOS编程方法能够高效计算车辆-驾驶员动力学的全局稳定性安全不变子集，为主动车辆控制的监督层提供理论基础"}}
{"id": "2512.05803", "pdf": "https://arxiv.org/pdf/2512.05803", "abs": "https://arxiv.org/abs/2512.05803", "authors": ["Blanca Inigo", "Benjamin D. Killeen", "Rebecca Choi", "Michelle Song", "Ali Uneri", "Majid Khan", "Christopher Bailey", "Axel Krieger", "Mathias Unberath"], "title": "3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic systems are transforming image-guided interventions by enhancing accuracy and minimizing radiation exposure. A significant challenge in robotic assistance lies in surgical path planning, which often relies on the registration of intraoperative 2D images with preoperative 3D CT scans. This requirement can be burdensome and costly, particularly in procedures like vertebroplasty, where preoperative CT scans are not routinely performed. To address this issue, we introduce a differentiable rendering-based framework for 3D transpedicular path planning utilizing bi-planar 2D X-rays. Our method integrates differentiable rendering with a vertebral atlas generated through a Statistical Shape Model (SSM) and employs a learned similarity loss to refine the SSM shape and pose dynamically, independent of fixed imaging geometries. We evaluated our framework in two stages: first, through vertebral reconstruction from orthogonal X-rays for benchmarking, and second, via clinician-in-the-loop path planning using arbitrary-view X-rays. Our results indicate that our method outperformed a normalized cross-correlation baseline in reconstruction metrics (DICE: 0.75 vs. 0.65) and achieved comparable performance to the state-of-the-art model ReVerteR (DICE: 0.77), while maintaining generalization to arbitrary views. Success rates for bipedicular planning reached 82% with synthetic data and 75% with cadaver data, exceeding the 66% and 31% rates of a 2D-to-3D baseline, respectively. In conclusion, our framework facilitates versatile, CT-free 3D path planning for robot-assisted vertebroplasty, effectively accommodating real-world imaging diversity without the need for preoperative CT scans.", "AI": {"tldr": "提出一种基于可微分渲染的3D路径规划框架，用于机器人辅助椎体成形术，仅使用任意视角的双平面X射线图像，无需术前CT扫描。", "motivation": "机器人辅助手术系统需要术前3D CT与术中2D图像的配准来进行路径规划，但椎体成形术等手术通常不进行术前CT扫描，这增加了负担和成本。现有方法对固定成像几何形状有依赖，难以适应实际临床中多样的X射线视角。", "method": "结合可微分渲染与统计形状模型生成的椎体图谱，使用学习到的相似性损失动态优化SSM形状和姿态，不依赖固定成像几何。通过两阶段评估：正交X射线椎体重建基准测试和任意视角X射线的临床医生参与路径规划。", "result": "在重建指标上优于归一化互相关基线（DICE：0.75 vs 0.65），与最先进模型ReVerteR性能相当（DICE：0.77），同时保持对任意视角的泛化能力。双椎弓根规划成功率在合成数据上达到82%，尸体数据上达到75%，分别超过2D-to-3D基线的66%和31%。", "conclusion": "该框架实现了无需CT的机器人辅助椎体成形术3D路径规划，有效适应真实世界成像多样性，为临床提供了一种灵活且成本效益高的解决方案。"}}
{"id": "2512.05802", "pdf": "https://arxiv.org/pdf/2512.05802", "abs": "https://arxiv.org/abs/2512.05802", "authors": ["Jiahua Dong", "Xudong Wang", "Wenqi Liang", "Zongyan Han", "Meng Cao", "Duzhen Zhang", "Hanbin Zhao", "Zhi Han", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Bring Your Dreams to Life: Continual Text-to-Video Customization", "categories": ["cs.CV"], "comment": "Accepted to AAAI2026", "summary": "Customized text-to-video generation (CTVG) has recently witnessed great progress in generating tailored videos from user-specific text. However, most CTVG methods assume that personalized concepts remain static and do not expand incrementally over time. Additionally, they struggle with forgetting and concept neglect when continuously learning new concepts, including subjects and motions. To resolve the above challenges, we develop a novel Continual Customized Video Diffusion (CCVD) model, which can continuously learn new concepts to generate videos across various text-to-video generation tasks by tackling forgetting and concept neglect. To address catastrophic forgetting, we introduce a concept-specific attribute retention module and a task-aware concept aggregation strategy. They can capture the unique characteristics and identities of old concepts during training, while combining all subject and motion adapters of old concepts based on their relevance during testing. Besides, to tackle concept neglect, we develop a controllable conditional synthesis to enhance regional features and align video contexts with user conditions, by incorporating layer-specific region attention-guided noise estimation. Extensive experimental comparisons demonstrate that our CCVD outperforms existing CTVG models. The code is available at https://github.com/JiahuaDong/CCVD.", "AI": {"tldr": "提出一种持续文本到视频定制模型（CCVD），能够连续学习新概念（主体和动作）生成视频，解决遗忘和概念忽视问题", "motivation": "现有定制文本到视频生成方法假设个性化概念是静态的，不随时间扩展，且在持续学习新概念时存在遗忘和概念忽视问题", "method": "提出CCVD模型，包含概念特定属性保留模块和任务感知概念聚合策略来应对灾难性遗忘，以及可控条件合成通过层特定区域注意力引导的噪声估计来增强区域特征和对齐视频上下文", "result": "大量实验比较表明，CCVD在持续学习新概念时优于现有CTVG模型，能有效减少遗忘并提高概念保留能力", "conclusion": "CCVD模型成功解决了持续文本到视频定制中的遗忘和概念忽视问题，为动态扩展个性化概念的视频生成提供了有效解决方案"}}
{"id": "2512.05794", "pdf": "https://arxiv.org/pdf/2512.05794", "abs": "https://arxiv.org/abs/2512.05794", "authors": ["Rebonto Haque", "Oliver M. Turnbull", "Anisha Parsan", "Nithin Parsan", "John J. Yang", "Charlotte M. Deane"], "title": "Mechanistic Interpretability of Antibody Language Models Using SAEs", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.", "AI": {"tldr": "研究使用稀疏自编码器（SAEs）技术对自回归抗体语言模型p-IgGen进行机制可解释性分析，比较TopK SAEs和Ordered SAEs在特征识别和生成控制方面的表现", "motivation": "虽然稀疏自编码器已被用于大型蛋白质语言模型的可解释性研究，但针对特定领域（如抗体）的语言模型，特别是如何有效识别生物相关特征并实现对生成的精确控制，仍需要深入研究", "method": "采用TopK SAEs和Ordered SAEs两种稀疏自编码器技术，对自回归抗体语言模型p-IgGen进行分析，比较它们在特征识别、概念映射和生成控制方面的表现", "result": "TopK SAEs能够揭示具有生物学意义的潜在特征，但高特征概念相关性并不能保证对生成的因果控制；Ordered SAEs通过层次结构能够可靠识别可控制特征，但激活模式更复杂且可解释性较差", "conclusion": "对于特定领域的蛋白质语言模型，TopK SAEs适用于将潜在特征映射到概念，而Ordered SAEs在需要精确生成控制时更优，这推进了领域特定蛋白质语言模型的机制可解释性研究"}}
{"id": "2512.05783", "pdf": "https://arxiv.org/pdf/2512.05783", "abs": "https://arxiv.org/abs/2512.05783", "authors": ["Maryam Yousefi", "Soodeh Bakhshandeh"], "title": "Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.", "AI": {"tldr": "提出了一种曲率正则化的变分自编码器，用于从稀疏深度数据中重建完整的3D场景，通过离散拉普拉斯算子实现正则化，显著提升了重建精度。", "motivation": "当深度传感器只能提供5%的必要测量数据时，重建完整3D场景变得困难。自动驾驶汽车和机器人无法容忍稀疏重建引入的几何误差，需要更准确的重建方法。", "method": "通过离散拉普拉斯算子实现曲率正则化，将其集成到变分自编码器中。该方法提供稳定的梯度和噪声抑制，仅增加15%的训练开销且推理成本为零。", "result": "相比标准变分自编码器，重建精度提高了18.1%。该方法挑战了几何深度学习中\"组合多个几何约束能提高性能\"的隐含假设，证明单个精心设计的正则化项可以超越复杂的多术语公式。", "conclusion": "曲率正则化通过离散拉普拉斯算子在稀疏深度场景重建中表现出色，代码和模型已开源，为几何深度学习提供了新的正则化思路。"}}
{"id": "2512.05774", "pdf": "https://arxiv.org/pdf/2512.05774", "abs": "https://arxiv.org/abs/2512.05774", "authors": ["Ziyang Wang", "Honglu Zhou", "Shijie Wang", "Junnan Li", "Caiming Xiong", "Silvio Savarese", "Mohit Bansal", "Michael S. Ryoo", "Juan Carlos Niebles"], "title": "Active Video Perception: Iterative Evidence Seeking for Agentic Long Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Website: https://activevideoperception.github.io/", "summary": "Long video understanding (LVU) is challenging because answering real-world queries often depends on sparse, temporally dispersed cues buried in hours of mostly redundant and irrelevant content. While agentic pipelines improve video reasoning capabilities, prevailing frameworks rely on a query-agnostic captioner to perceive video information, which wastes computation on irrelevant content and blurs fine-grained temporal and spatial information. Motivated by active perception theory, we argue that LVU agents should actively decide what, when, and where to observe, and continuously assess whether the current observation is sufficient to answer the query. We present Active Video Perception (AVP), an evidence-seeking framework that treats the video as an interactive environment and acquires compact, queryrelevant evidence directly from pixels. Concretely, AVP runs an iterative plan-observe-reflect process with MLLM agents. In each round, a planner proposes targeted video interactions, an observer executes them to extract time-stamped evidence, and a reflector evaluates the sufficiency of the evidence for the query, either halting with an answer or triggering further observation. Across five LVU benchmarks, AVP achieves highest performance with significant improvements. Notably, AVP outperforms the best agentic method by 5.7% in average accuracy while only requires 18.4% inference time and 12.4% input tokens.", "AI": {"tldr": "提出Active Video Perception (AVP)框架，通过主动感知理论解决长视频理解中的稀疏线索提取问题，实现基于查询的定向观察和证据收集", "motivation": "长视频理解面临挑战，因为真实世界查询通常依赖于稀疏、时间分散的线索，而现有基于查询无关的captioner方法浪费计算资源在无关内容上，模糊了细粒度时空信息", "method": "AVP采用迭代的plan-observe-reflect过程：planner提出针对性视频交互，observer执行交互提取时间戳证据，reflector评估证据充分性，决定停止回答或继续观察", "result": "在五个LVU基准测试中，AVP取得最高性能，平均准确率比最佳代理方法提高5.7%，同时仅需18.4%的推理时间和12.4%的输入token", "conclusion": "AVP框架通过主动感知理论有效解决了长视频理解中的稀疏线索提取问题，实现了高效、准确的查询相关证据收集，显著提升了长视频理解性能"}}
{"id": "2512.05765", "pdf": "https://arxiv.org/pdf/2512.05765", "abs": "https://arxiv.org/abs/2512.05765", "authors": ["Edward Y. Chang"], "title": "The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics", "categories": ["cs.AI", "cs.LG"], "comment": "13 pages, 3 figures", "summary": "Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: \"mere pattern matchers\" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while \"reasoning\" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.", "AI": {"tldr": "该论文提出AGI缺失的关键层是协调层，而非模式匹配能力本身。作者认为LLMs提供了必要的System-1模式库，但需要System-2协调层来选择、约束和绑定这些模式，从而实现真正的推理和规划。", "motivation": "针对\"LLMs只是模式匹配器，无法实现真正推理和规划\"的批评，作者认为这种观点误判了瓶颈所在。问题不在于模式匹配本身，而在于缺乏一个协调层来引导和约束这些模式，使其服务于目标导向的推理过程。", "method": "提出了UCCT理论（语义锚定理论），将推理建模为受有效支持度、表征失配度和自适应锚定预算控制的相变过程。基于此理论设计了MACI架构，包含诱饵机制、过滤机制和持久性机制三个核心组件，实现行为调制的辩论、苏格拉底式判断和事务性记忆。", "result": "通过理论框架将常见的反对意见重新解释为可测试的协调失败案例，论证了AGI的发展路径应该通过LLMs而非绕过它们。提出了具体的架构设计来填补System-1和System-2之间的鸿沟。", "conclusion": "通往AGI的道路应该通过LLMs而非绕过它们。关键在于在LLMs的模式匹配基础之上构建一个协调层，该层能够选择、约束和绑定模式，从而实现真正的推理和规划能力。UCCT理论和MACI架构为实现这一目标提供了理论框架和具体实现路径。"}}
{"id": "2512.05762", "pdf": "https://arxiv.org/pdf/2512.05762", "abs": "https://arxiv.org/abs/2512.05762", "authors": ["Ruochen Chen", "Thuy Tran", "Shaifali Parashar"], "title": "FNOPT: Resolution-Agnostic, Self-Supervised Cloth Simulation using Meta-Optimization with Fourier Neural Operators", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted for WACV", "summary": "We present FNOpt, a self-supervised cloth simulation framework that formulates time integration as an optimization problem and trains a resolution-agnostic neural optimizer parameterized by a Fourier neural operator (FNO). Prior neural simulators often rely on extensive ground truth data or sacrifice fine-scale detail, and generalize poorly across resolutions and motion patterns. In contrast, FNOpt learns to simulate physically plausible cloth dynamics and achieves stable and accurate rollouts across diverse mesh resolutions and motion patterns without retraining. Trained only on a coarse grid with physics-based losses, FNOpt generalizes to finer resolutions, capturing fine-scale wrinkles and preserving rollout stability. Extensive evaluations on a benchmark cloth simulation dataset demonstrate that FNOpt outperforms prior learning-based approaches in out-of-distribution settings in both accuracy and robustness. These results position FNO-based meta-optimization as a compelling alternative to previous neural simulators for cloth, thus reducing the need for curated data and improving cross-resolution reliability.", "AI": {"tldr": "提出FNOpt框架，通过元优化和傅里叶神经算子实现分辨率无关、自监督的布料模拟，无需地面真值数据即可学习物理合理的布料动力学", "motivation": "现有神经模拟器依赖大量地面真值数据或牺牲细节，在不同分辨率和运动模式上泛化能力差，需要一种能跨分辨率稳定模拟且保持细节的方法", "method": "将时间积分公式化为优化问题，训练由傅里叶神经算子参数化的分辨率无关神经优化器，仅使用粗网格和物理损失进行自监督训练", "result": "FNOpt在布料模拟基准测试中优于现有学习方法，在分布外设置下表现出更高的准确性和鲁棒性，能泛化到更细分辨率并捕捉细尺度皱纹", "conclusion": "基于FNO的元优化是布料神经模拟的有力替代方案，减少了对标注数据的需求，提高了跨分辨率可靠性"}}
{"id": "2512.05760", "pdf": "https://arxiv.org/pdf/2512.05760", "abs": "https://arxiv.org/abs/2512.05760", "authors": ["Zeyuan Ma", "Wenqi Huang", "Guo-Huan Song", "Hongshu Guo", "Sijie Ma", "Zhiguang Cao", "Yue-Jiao Gong"], "title": "Evolutionary System 2 Reasoning: An Empirical Proof", "categories": ["cs.AI"], "comment": null, "summary": "Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.", "AI": {"tldr": "提出进化推理优化（ERO）框架，通过进化算法在LLM群体中搜索具有强大推理能力的个体，以解决LLMs在系统2推理能力方面的不足。", "motivation": "尽管大型语言模型在特定任务上表现出色，但在一般智能和系统2推理（慢思考）方面仍有不足。本文旨在探索机器智能（如LLMs）能否像人类一样进化获得推理能力，而不仅仅是特定技能。", "method": "提出进化推理优化（ERO）框架：1）初始化多个LLMs作为群体；2）使用进化策略进化群体，最大化最佳个体的量化推理分数；3）通过适者生存原则搜索具有强推理能力的个体。", "result": "实验发现：1）最新LLMs（如GPT-5）仍表现出有限的系统2推理能力；2）通过简单的ERO进化循环，相对较弱的模型（Qwen-7B）能够被增强，涌现出强大的推理能力。", "conclusion": "ERO框架证明通过进化方法可以显著提升LLMs的推理能力，为机器智能获得类似人类的推理能力提供了实证证据，表明进化策略是增强LLMs系统2推理能力的有效途径。"}}
{"id": "2512.05759", "pdf": "https://arxiv.org/pdf/2512.05759", "abs": "https://arxiv.org/abs/2512.05759", "authors": ["Johannes Meyer", "Jasper Hoffmann", "Felix Schulz", "Dominik Merkle", "Daniel Buescher", "Alexander Reiterer", "Joschka Boedecker", "Wolfram Burgard"], "title": "Label-Efficient Point Cloud Segmentation with Active Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Semantic segmentation of 3D point cloud data often comes with high annotation costs. Active learning automates the process of selecting which data to annotate, reducing the total amount of annotation needed to achieve satisfactory performance. Recent approaches to active learning for 3D point clouds are often based on sophisticated heuristics for both, splitting point clouds into annotatable regions and selecting the most beneficial for further neural network training. In this work, we propose a novel and easy-to-implement strategy to separate the point cloud into annotatable regions. In our approach, we utilize a 2D grid to subdivide the point cloud into columns. To identify the next data to be annotated, we employ a network ensemble to estimate the uncertainty in the network output. We evaluate our method on the S3DIS dataset, the Toronto-3D dataset, and a large-scale urban 3D point cloud of the city of Freiburg, which we labeled in parts manually. The extensive evaluation shows that our method yields performance on par with, or even better than, complex state-of-the-art methods on all datasets. Furthermore, we provide results suggesting that in the context of point clouds the annotated area can be a more meaningful measure for active learning algorithms than the number of annotated points.", "AI": {"tldr": "提出一种基于主动学习的标签高效点云分割方法，通过2D网格划分和网络集成不确定性估计来选择最有价值的标注区域", "motivation": "3D点云语义分割的标注成本很高，现有主动学习方法通常基于复杂的启发式策略来划分点云区域并选择标注样本，需要更简单有效的解决方案", "method": "使用2D网格将点云划分为柱状区域作为标注单元，采用网络集成方法估计预测不确定性来选择最有价值的标注区域", "result": "在S3DIS、Toronto-3D和Freiburg城市点云数据集上的评估显示，该方法性能与复杂SOTA方法相当甚至更好，并发现标注面积比标注点数更能衡量主动学习效果", "conclusion": "提出了一种简单易实现的点云主动学习分割策略，在多个数据集上验证了有效性，并指出标注面积是比标注点数更有意义的评估指标"}}
{"id": "2512.05754", "pdf": "https://arxiv.org/pdf/2512.05754", "abs": "https://arxiv.org/abs/2512.05754", "authors": ["Xinjian Wu", "Hongmei Wang", "Yuan Zhou", "Qinglin Lu"], "title": "USV: Unified Sparsification for Accelerating Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The scalability of high-fidelity video diffusion models (VDMs) is constrained by two key sources of redundancy: the quadratic complexity of global spatio-temporal attention and the computational overhead of long iterative denoising trajectories. Existing accelerators -- such as sparse attention and step-distilled samplers -- typically target a single dimension in isolation and quickly encounter diminishing returns, as the remaining bottlenecks become dominant. In this work, we introduce USV (Unified Sparsification for Video diffusion models), an end-to-end trainable framework that overcomes this limitation by jointly orchestrating sparsification across both the model's internal computation and its sampling process. USV learns a dynamic, data- and timestep-dependent sparsification policy that prunes redundant attention connections, adaptively merges semantically similar tokens, and reduces denoising steps, treating them not as independent tricks but as coordinated actions within a single optimization objective. This multi-dimensional co-design enables strong mutual reinforcement among previously disjoint acceleration strategies. Extensive experiments on large-scale video generation benchmarks demonstrate that USV achieves up to 83.3% speedup in the denoising process and 22.7% end-to-end acceleration, while maintaining high visual fidelity. Our results highlight unified, dynamic sparsification as a practical path toward efficient, high-quality video generation.", "AI": {"tldr": "提出USV框架，通过统一稀疏化策略加速视频扩散模型，同时优化模型内部计算和采样过程", "motivation": "现有视频扩散模型面临两个主要冗余源：全局时空注意力的二次复杂度和长迭代去噪轨迹的计算开销。现有加速方法通常只针对单一维度，存在收益递减问题", "method": "USV是一个端到端可训练框架，学习动态的、数据和时间步相关的稀疏化策略，包括剪枝冗余注意力连接、自适应合并语义相似token、减少去噪步骤，将这些策略统一在一个优化目标中", "result": "在大规模视频生成基准测试中，USV实现了去噪过程83.3%的加速和端到端22.7%的加速，同时保持高视觉保真度", "conclusion": "统一的动态稀疏化是实现高效高质量视频生成的实用路径，多维协同设计使先前独立的加速策略能够相互增强"}}
{"id": "2512.05753", "pdf": "https://arxiv.org/pdf/2512.05753", "abs": "https://arxiv.org/abs/2512.05753", "authors": ["Wencheng Cai", "Xuchao Gao", "Congying Han", "Mingqiang Li", "Tiande Guo"], "title": "A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.", "AI": {"tldr": "提出一种基于强化学习的快速抗干扰认知雷达部署算法，解决传统进化算法耗时且易陷入局部最优的问题", "motivation": "现代战争中快速部署认知雷达对抗干扰是关键技术挑战，现有基于进化算法的方法耗时且易陷入局部最优，需要更高效的解决方案", "method": "将雷达部署问题建模为端到端任务，设计深度强化学习算法，开发集成神经模块感知热图信息，并提出新的奖励格式", "result": "实验结果表明，该方法在达到与进化算法相当覆盖性能的同时，部署速度提升约7000倍，消融实验验证了各组件必要性", "conclusion": "提出的FARDA框架通过强化学习实现了快速高效的雷达部署，显著优于传统进化算法，为抗干扰雷达部署提供了新解决方案"}}
{"id": "2512.05751", "pdf": "https://arxiv.org/pdf/2512.05751", "abs": "https://arxiv.org/abs/2512.05751", "authors": ["Remo Burn", "Victor F. Ksoll", "Hubert Klahr", "Thomas Henning"], "title": "Exoplanet formation inference using conditional invertible neural networks", "categories": ["astro-ph.EP", "cs.NE", "physics.data-an"], "comment": "10 pages, accepted poster for the Machine Learning and the Physical Sciences Workshop at the 39th conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "The interpretation of the origin of observed exoplanets is usually done only qualitatively due to uncertainties of key parameters in planet formation models. To allow a quantitative methodology which traces back in time to the planet birth locations, we train recently developed conditional invertible neural networks (cINN) on synthetic data from a global planet formation model which tracks growth from dust grains to evolved final giant planets. In addition to deterministic single planet formation runs, we also include gravitationally interacting planets in multiplanetary systems, which include some measure of chaos. For the latter case, we treat them as individual planets or choose the two or three planets most likely to be discovered by telescopes. We find that training on multiplanetary data, each planet treated as individual point, is promising. The single-planet data only covers a small range of planets and does not extrapolate well to planet properties not included in the training data. Extension to planetary systems will require more training data due to the higher dimensionality of the problem.", "AI": {"tldr": "使用条件可逆神经网络从观测数据推断系外行星形成过程", "motivation": "传统系外行星形成解释通常只能定性分析，因为行星形成模型中的关键参数存在不确定性，需要一种能够定量追溯行星形成历史的定量方法", "method": "使用条件可逆神经网络（cINN）在全局行星形成模型的合成数据上进行训练，该模型追踪从尘埃颗粒到成熟巨行星的完整生长过程，包括确定性单行星形成和多行星系统的引力相互作用", "result": "在多行星数据上训练（每个行星作为独立点处理）效果良好，而单行星数据仅覆盖小范围行星类型，对训练数据中未包含的行星属性外推效果不佳", "conclusion": "多行星系统训练方法有前景，但扩展到完整行星系统需要更多训练数据，因为问题维度更高"}}
{"id": "2512.05746", "pdf": "https://arxiv.org/pdf/2512.05746", "abs": "https://arxiv.org/abs/2512.05746", "authors": ["Shizhuo Mao", "Hongtao Zou", "Qihu Xie", "Song Chen", "Yi Kang"], "title": "HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.", "AI": {"tldr": "提出HQ-DM框架，通过单哈达玛变换进行量化感知训练，解决扩散模型低比特量化中的激活异常值问题", "motivation": "扩散模型在图像生成领域应用广泛，但计算和内存成本高，部署困难。现有量化方法在低比特量化场景下难以处理激活矩阵中的异常值，导致性能显著下降", "method": "提出HQ-DM量化感知训练框架，对激活矩阵应用单哈达玛变换，有效减少激活异常值，同时保持量化下的模型性能。相比传统的双哈达玛变换，该方法能无缝支持INT卷积操作并防止权重异常值放大", "result": "在ImageNet 256x256数据集上使用LDM-4模型进行条件生成，W4A4和W4A3量化方案相比现有最先进方法分别将Inception Score提高了12.8%和467.73%", "conclusion": "HQ-DM框架通过单哈达玛变换有效解决了扩散模型低比特量化中的激活异常值问题，显著提升了量化模型的性能，为扩散模型的轻量化部署提供了有效解决方案"}}
{"id": "2512.05740", "pdf": "https://arxiv.org/pdf/2512.05740", "abs": "https://arxiv.org/abs/2512.05740", "authors": ["Lennart Maack", "Julia-Kristin Graß", "Lisa-Marie Toscha", "Nathaniel Melling", "Alexander Schlaefer"], "title": "Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Large Language Models (VLMs) have demonstrated high potential in computer-aided diagnosis and decision-support. However, current VLMs show deficits in domain specific surgical scene understanding, such as identifying and explaining anatomical landmarks during Complete Mesocolic Excision. Additionally, there is a need for locally deployable models to avoid patient data leakage to large VLMs, hosted outside the clinic. We propose a privacy-preserving framework to distill knowledge from large, general-purpose LLMs into an efficient, local VLM. We generate an expert-supervised dataset by prompting a teacher LLM without sensitive images, using only textual context and binary segmentation masks for spatial information. This dataset is used for Supervised Fine-Tuning (SFT) and subsequent Direct Preference Optimization (DPO) of the locally deployable VLM. Our evaluation confirms that finetuning VLMs with our generated datasets increases surgical domain knowledge compared to its base VLM by a large margin. Overall, this work validates a data-efficient and privacy-conforming way to train a surgical domain optimized, locally deployable VLM for surgical scene understanding.", "AI": {"tldr": "该论文提出了一个隐私保护的框架，将大型通用LLM的知识蒸馏到高效、可本地部署的视觉语言模型(VLM)中，用于手术场景理解，特别是在完全结肠系膜切除术中的解剖结构解释。", "motivation": "当前VLM在特定手术领域（如完全结肠系膜切除术中的解剖标志识别和解释）理解方面存在不足，且需要可本地部署的模型以避免患者数据泄露到外部托管的大型VLM。", "method": "通过专家监督生成数据集：仅使用文本上下文和二进制分割掩码（不包含敏感图像）提示教师LLM，然后使用该数据集对可本地部署的VLM进行监督微调(SFT)和直接偏好优化(DPO)。", "result": "评估证实，使用生成的数据集微调VLM显著提高了其在手术领域的知识水平，相比基础VLM有大幅提升。", "conclusion": "这项工作验证了一种数据高效且符合隐私保护要求的方法，用于训练手术领域优化的可本地部署VLM，以增强手术场景理解能力。"}}
{"id": "2512.05734", "pdf": "https://arxiv.org/pdf/2512.05734", "abs": "https://arxiv.org/abs/2512.05734", "authors": ["Jinfeng Zhong", "Emmanuel Bacry", "Agathe Guilloux", "Jean-François Muzy"], "title": "KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.", "AI": {"tldr": "KANFormer是一个基于深度学习的模型，用于通过生存分析预测限价订单的成交概率和时间", "motivation": "现有模型仅依赖限价订单簿的快照序列，忽略了与LOB动态相关的代理行为以及订单在队列中的位置，无法有效捕捉与执行可能性相关的模式", "method": "结合扩张因果卷积网络与Transformer编码器，通过Kolmogorov-Arnold Networks（KANs）增强非线性逼近能力，整合市场级和代理级信息以及订单队列位置", "result": "在CAC 40指数期货数据上评估，KANFormer在校准（右删失对数似然、集成Brier分数）和区分（C指数、时间依赖AUC）方面均优于现有方法，并使用SHAP分析特征重要性随时间的变化", "conclusion": "结合丰富的市场信号与表达性强的神经架构能够实现准确且可解释的成交概率预测，KANFormer展示了这种组合的优势"}}
{"id": "2512.05732", "pdf": "https://arxiv.org/pdf/2512.05732", "abs": "https://arxiv.org/abs/2512.05732", "authors": ["Ippokratis Pantelidis", "Korbinian Randl", "Aron Henriksson"], "title": "Efficient Text Classification with Conformal In-Context Learning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.", "AI": {"tldr": "提出并系统评估了CICLe框架，这是一种结合轻量级基础分类器和保形预测的资源高效方法，用于指导LLM提示并减少候选类别集，从而提高文本分类效率。", "motivation": "大型语言模型在上下文学习中表现出色，但其在文本分类中的效果严重依赖提示设计且计算成本高昂。需要一种资源高效的框架来结合传统分类器的鲁棒性和LLM的适应性。", "method": "CICLe框架整合轻量级基础分类器与保形预测，通过自适应减少候选类别集来指导LLM提示，减少所需的shot数量和提示长度。", "result": "在多样化的NLP分类基准测试中，CICLe持续改进其基础分类器性能，在样本充足时优于少样本提示基线，在低数据情况下表现相当。效率方面减少shot数量达34.45%，提示长度减少25.16%，并能使用更小模型保持竞争力。", "conclusion": "CICLe是一种实用且可扩展的高效文本分类方法，结合了传统分类器的鲁棒性和LLM的适应性，在数据和计算效率方面取得显著提升，特别适用于类别不平衡的文本分类任务。"}}
{"id": "2512.05714", "pdf": "https://arxiv.org/pdf/2512.05714", "abs": "https://arxiv.org/abs/2512.05714", "authors": ["Max Martin Gnewuch", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "Big Tech-Funded AI Papers Have Higher Citation Impact, Greater Insularity, and Larger Recency Bias", "categories": ["cs.DL", "cs.AI", "cs.CL"], "comment": "Published at IEEE (ACDSA)", "summary": "Over the past four decades, artificial intelligence (AI) research has flourished at the nexus of academia and industry. However, Big Tech companies have increasingly acquired the edge in computational resources, big data, and talent. So far, it has been largely unclear how many papers the industry funds, how their citation impact compares to non-funded papers, and what drives industry interest. This study fills that gap by quantifying the number of industry-funded papers at 10 top AI conferences (e.g., ICLR, CVPR, AAAI, ACL) and their citation influence. We analyze about 49.8K papers, about 1.8M citations from AI papers to other papers, and about 2.3M citations from other papers to AI papers from 1998-2022 in Scopus. Through seven research questions, we examine the volume and evolution of industry funding in AI research, the citation impact of funded papers, the diversity and temporal range of their citations, and the subfields in which industry predominantly acts. Our findings reveal that industry presence has grown markedly since 2015, from less than 2 percent to more than 11 percent in 2020. Between 2018 and 2022, 12 percent of industry-funded papers achieved high citation rates as measured by the h5-index, compared to 4 percent of non-industry-funded papers and 2 percent of non-funded papers. Top AI conferences engage more with industry-funded research than non-funded research, as measured by our newly proposed metric, the Citation Preference Ratio (CPR). We show that industry-funded research is increasingly insular, citing predominantly other industry-funded papers while referencing fewer non-funded papers. These findings reveal new trends in AI research funding, including a shift towards more industry-funded papers and their growing citation impact, greater insularity of industry-funded work than non-funded work, and a preference of industry-funded research to cite recent work.", "AI": {"tldr": "该研究分析了1998-2022年间10个顶级AI会议中行业资助论文的引用影响力、孤立性和时效性偏见，发现行业资助论文具有更高的引用影响力、更强的内部引用倾向和更大的近期引用偏好。", "motivation": "随着大型科技公司在计算资源、大数据和人才方面占据优势，但行业资助论文的数量、引用影响力及其驱动因素尚不清楚，需要量化分析行业在AI研究中的参与度和影响力。", "method": "分析了约4.98万篇论文、180万次AI论文间的引用和230万次外部对AI论文的引用，通过七个研究问题考察行业资助论文的数量演变、引用影响力、引用多样性、时间范围和主导子领域。", "result": "行业参与度自2015年显著增长（从<2%到>11%）；2018-2022年间，12%的行业资助论文获得高引用率（非行业资助为4%，非资助为2%）；行业资助研究日益孤立，主要引用其他行业资助论文，较少引用非资助论文。", "conclusion": "AI研究资助呈现新趋势：行业资助论文数量增加且引用影响力增长，行业资助工作比非资助工作更具孤立性，行业资助研究偏好引用近期工作，揭示了行业在AI研究中的主导地位和内部循环特征。"}}
{"id": "2512.05711", "pdf": "https://arxiv.org/pdf/2512.05711", "abs": "https://arxiv.org/abs/2512.05711", "authors": ["Ali Krayani", "Seyedeh Fatemeh Sadati", "Lucio Marcenaro", "Carlo Regazzoni"], "title": "Bayesian Active Inference for Intelligent UAV Anti-Jamming and Adaptive Trajectory Planning", "categories": ["cs.RO", "cs.AI", "eess.SP", "eess.SY"], "comment": "This paper has been accepted for the 2026 IEEE Consumer Communications & Networking Conference (IEEE CCNC 2026)", "summary": "This paper proposes a hierarchical trajectory planning framework for UAVs operating under adversarial jamming conditions. Leveraging Bayesian Active Inference, the approach combines expert-generated demonstrations with probabilistic generative modeling to encode high-level symbolic planning, low-level motion policies, and wireless signal feedback. During deployment, the UAV performs online inference to anticipate interference, localize jammers, and adapt its trajectory accordingly, without prior knowledge of jammer locations. Simulation results demonstrate that the proposed method achieves near-expert performance, significantly reducing communication interference and mission cost compared to model-free reinforcement learning baselines, while maintaining robust generalization in dynamic environments.", "AI": {"tldr": "提出了一种基于贝叶斯主动推理的无人机分层轨迹规划框架，用于在对抗性干扰环境下实现智能抗干扰和自适应轨迹规划。", "motivation": "无人机在对抗性干扰环境下执行任务时面临通信中断和轨迹规划困难的问题，需要一种能够在线适应干扰、定位干扰源并调整轨迹的智能方法。", "method": "采用分层轨迹规划框架，结合贝叶斯主动推理，将专家生成的演示与概率生成建模相结合，编码高层符号规划、低层运动策略和无线信号反馈，实现在线干扰预测和自适应轨迹调整。", "result": "仿真结果表明，该方法实现了接近专家水平的性能，相比无模型强化学习基线显著减少了通信干扰和任务成本，同时在动态环境中保持了鲁棒的泛化能力。", "conclusion": "提出的贝叶斯主动推理框架为无人机在对抗性干扰环境下的智能轨迹规划提供了一种有效的解决方案，能够在未知干扰源位置的情况下实现自适应抗干扰。"}}
{"id": "2512.05710", "pdf": "https://arxiv.org/pdf/2512.05710", "abs": "https://arxiv.org/abs/2512.05710", "authors": ["Jianan Sun", "Dongzhihan Wang", "Mingyu Fan"], "title": "Manifold-Aware Point Cloud Completion via Geodesic-Attentive Hierarchical Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud completion seeks to recover geometrically consistent shapes from partial or sparse 3D observations. Although recent methods have achieved reasonable global shape reconstruction, they often rely on Euclidean proximity and overlook the intrinsic nonlinear geometric structure of point clouds, resulting in suboptimal geometric consistency and semantic ambiguity. In this paper, we present a manifold-aware point cloud completion framework that explicitly incorporates nonlinear geometry information throughout the feature learning pipeline. Our approach introduces two key modules: a Geodesic Distance Approximator (GDA), which estimates geodesic distances between points to capture the latent manifold topology, and a Manifold-Aware Feature Extractor (MAFE), which utilizes geodesic-based $k$-NN groupings and a geodesic-relational attention mechanism to guide the hierarchical feature extraction process. By integrating geodesic-aware relational attention, our method promotes semantic coherence and structural fidelity in the reconstructed point clouds. Extensive experiments on benchmark datasets demonstrate that our approach consistently outperforms state-of-the-art methods in reconstruction quality.", "AI": {"tldr": "提出一种基于流形感知的点云补全框架，通过整合测地距离信息来提升几何一致性和语义连贯性", "motivation": "现有方法主要依赖欧氏距离，忽略了点云内在的非线性几何结构，导致几何一致性不足和语义模糊", "method": "提出测地距离近似器(GDA)估计点间测地距离，以及流形感知特征提取器(MAFE)利用测地距离k-NN分组和测地关系注意力机制进行分层特征提取", "result": "在基准数据集上的实验表明，该方法在重建质量方面持续优于现有最先进方法", "conclusion": "通过显式整合非线性几何信息，提出的流形感知框架能够生成几何一致且语义连贯的点云补全结果"}}
{"id": "2512.05700", "pdf": "https://arxiv.org/pdf/2512.05700", "abs": "https://arxiv.org/abs/2512.05700", "authors": ["Ben Malin", "Tatiana Kalganova", "Nikolaos Boulgouris"], "title": "Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, conference paper", "summary": "We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.", "AI": {"tldr": "提出一种基于度量融合的方法来提高大型语言模型（LLM）忠实度评估的准确性", "motivation": "当前LLM忠实度评估存在不足，需要更准确的方法来评估模型输出的可信度，以便在更多样化的场景中安全部署LLM", "method": "将多个基础忠实度度量组合成融合度量，使用基于树的模型确定各度量的重要性权重，该权重由人类对LLM响应忠实度的判断驱动", "result": "融合度量在所有测试领域中与人类判断的相关性更强，提高了LLM忠实度评估的准确性", "conclusion": "提出的度量融合方法能更准确地评估LLM忠实度，增强对模型的信任，使其能在更多样化的场景中安全部署；同时构建了跨领域数据集支持可复现性"}}
{"id": "2512.05698", "pdf": "https://arxiv.org/pdf/2512.05698", "abs": "https://arxiv.org/abs/2512.05698", "authors": ["Xusheng Guo", "Wanfa Zhang", "Shijia Zhao", "Qiming Xia", "Xiaolong Xie", "Mingming Wang", "Hai Wu", "Chenglu Wen"], "title": "OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning", "categories": ["cs.CV"], "comment": "The 40th Annual AAAI Conference on Artificial Intelligence", "summary": "Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.", "AI": {"tldr": "OWL提出了一种无监督3D目标检测方法，通过占用引导预热和大模型先验推理来减少伪标签的误导影响，提高检测性能", "motivation": "现有无监督3D目标检测方法依赖伪标签自训练，但初始伪标签往往不准确，会误导优化过程，且有效过滤和精炼伪标签仍是一个关键挑战", "method": "1) 占用引导预热策略(OGW)：初始化骨干网络权重，赋予空间感知能力，减少错误伪标签对网络收敛的干扰；2) 实例提示推理模块(ICR)：利用大模型先验知识评估伪标签质量，实现精确过滤和精炼；3) 权重自适应自训练策略(WAS)：动态重新加权伪标签，通过自训练提升性能", "result": "在Waymo Open Dataset和KITTI数据集上的大量实验表明，OWL比最先进的无监督方法性能提升超过15.0% mAP，证明了方法的有效性", "conclusion": "OWL通过占用引导预热、大模型先验推理和权重自适应自训练，有效解决了无监督3D目标检测中伪标签质量差的问题，显著提升了检测性能"}}
{"id": "2512.05693", "pdf": "https://arxiv.org/pdf/2512.05693", "abs": "https://arxiv.org/abs/2512.05693", "authors": ["Zhiying Du", "Bei Liu", "Yaobo Liang", "Yichao Shen", "Haidong Cao", "Xiangyu Zheng", "Zhiyuan Feng", "Zuxuan Wu", "Jiaolong Yang", "Yu-Gang Jiang"], "title": "HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "The development of foundation models for embodied intelligence critically depends on access to large-scale, high-quality robot demonstration data. Recent approaches have sought to address this challenge by training on large collections of heterogeneous robotic datasets. However, unlike vision or language data, robotic demonstrations exhibit substantial heterogeneity across embodiments and action spaces as well as other prominent variations such as senor configurations and action control frequencies. The lack of explicit designs for handling such heterogeneity causes existing methods to struggle with integrating diverse factors, thereby limiting their generalization and leading to degraded performance when transferred to new settings. In this paper, we present HiMoE-VLA, a novel vision-language-action (VLA) framework tailored to effectively handle diverse robotic data with heterogeneity. Specifically, we introduce a Hierarchical Mixture-of-Experts (HiMoE) architecture for the action module which adaptively handles multiple sources of heterogeneity across layers and gradually abstracts them into shared knowledge representations. Through extensive experimentation with simulation benchmarks and real-world robotic platforms, HiMoE-VLA demonstrates a consistent performance boost over existing VLA baselines, achieving higher accuracy and robust generalization across diverse robots and action spaces. The code and models are publicly available at https://github.com/ZhiyingDu/HiMoE-VLA.", "AI": {"tldr": "提出HiMoE-VLA框架，通过分层混合专家架构处理机器人演示数据的异构性，提升视觉-语言-动作策略的泛化能力", "motivation": "机器人演示数据存在严重的异构性问题（如不同机器人本体、动作空间、传感器配置、控制频率等），现有方法缺乏专门处理这种异构性的设计，导致集成困难、泛化受限，在新场景中性能下降", "method": "提出HiMoE-VLA框架，采用分层混合专家架构作为动作模块，自适应处理多源异构性，逐步抽象为共享知识表示", "result": "在仿真基准和真实机器人平台上进行广泛实验，相比现有VLA基线方法，HiMoE-VLA表现出持续的性能提升，在不同机器人和动作空间上实现更高的准确性和鲁棒泛化", "conclusion": "HiMoE-VLA能有效处理机器人数据的异构性，提升视觉-语言-动作策略的泛化能力，代码和模型已开源"}}
{"id": "2512.05683", "pdf": "https://arxiv.org/pdf/2512.05683", "abs": "https://arxiv.org/abs/2512.05683", "authors": ["Yong En Kok", "Bowen Deng", "Alexander Bentley", "Andrew J. Parkes", "Michael G. Somekh", "Amanda J. Wright", "Michael P. Pound"], "title": "Physics-Informed Graph Neural Network with Frequency-Aware Learning for Optical Aberration Correction", "categories": ["cs.CV", "physics.optics"], "comment": null, "summary": "Optical aberrations significantly degrade image quality in microscopy, particularly when imaging deeper into samples. These aberrations arise from distortions in the optical wavefront and can be mathematically represented using Zernike polynomials. Existing methods often address only mild aberrations on limited sample types and modalities, typically treating the problem as a black-box mapping without leveraging the underlying optical physics of wavefront distortions. We propose ZRNet, a physics-informed framework that jointly performs Zernike coefficient prediction and optical image Restoration. We contribute a Zernike Graph module that explicitly models physical relationships between Zernike polynomials based on their azimuthal degrees-ensuring that learned corrections align with fundamental optical principles. To further enforce physical consistency between image restoration and Zernike prediction, we introduce a Frequency-Aware Alignment (FAA) loss, which better aligns Zernike coefficient prediction and image features in the Fourier domain. Extensive experiments on CytoImageNet demonstrates that our approach achieves state-of-the-art performance in both image restoration and Zernike coefficient prediction across diverse microscopy modalities and biological samples with complex, large-amplitude aberrations. Code is available at https://github.com/janetkok/ZRNet.", "AI": {"tldr": "提出ZRNet物理信息图神经网络框架，联合进行Zernike系数预测和光学图像恢复，通过Zernike图模块和频域感知对齐损失实现光学像差校正", "motivation": "光学像差显著降低显微镜图像质量，现有方法通常只处理轻度像差且局限于特定样本类型，将问题视为黑盒映射而未利用光学波前畸变的物理原理", "method": "提出ZRNet框架，包含Zernike图模块显式建模Zernike多项式间的物理关系，以及频域感知对齐损失在傅里叶域对齐Zernike系数预测和图像特征", "result": "在CytoImageNet数据集上的实验表明，该方法在图像恢复和Zernike系数预测方面均达到最先进性能，适用于多种显微镜模态和复杂大振幅像差的生物样本", "conclusion": "ZRNet通过物理信息学习和频域对齐，有效解决了光学像差校正问题，在保持物理一致性的同时实现了优异的恢复效果和系数预测精度"}}
{"id": "2512.05682", "pdf": "https://arxiv.org/pdf/2512.05682", "abs": "https://arxiv.org/abs/2512.05682", "authors": ["Yiming Shu", "Jiahui Xu", "Linghuan Kong", "Fangni Zhang", "Guodong Yin", "Chen Sun"], "title": "Scenario-aware Uncertainty Quantification for Trajectory Prediction with Statistical Guarantees", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Reliable uncertainty quantification in trajectory prediction is crucial for safety-critical autonomous driving systems, yet existing deep learning predictors lack uncertainty-aware frameworks adaptable to heterogeneous real-world scenarios. To bridge this gap, we propose a novel scenario-aware uncertainty quantification framework to provide the predicted trajectories with prediction intervals and reliability assessment. To begin with, predicted trajectories from the trained predictor and their ground truth are projected onto the map-derived reference routes within the Frenet coordinate system. We then employ CopulaCPTS as the conformal calibration method to generate temporal prediction intervals for distinct scenarios as the uncertainty measure. Building upon this, within the proposed trajectory reliability discriminator (TRD), mean error and calibrated confidence intervals are synergistically analyzed to establish reliability models for different scenarios. Subsequently, the risk-aware discriminator leverages a joint risk model that integrates longitudinal and lateral prediction intervals within the Frenet coordinate to identify critical points. This enables segmentation of trajectories into reliable and unreliable segments, holding the advantage of informing downstream planning modules with actionable reliability results. We evaluated our framework using the real-world nuPlan dataset, demonstrating its effectiveness in scenario-aware uncertainty quantification and reliability assessment across diverse driving contexts.", "AI": {"tldr": "提出一种面向自动驾驶轨迹预测的场景感知不确定性量化框架，通过统计保证为预测轨迹提供预测区间和可靠性评估", "motivation": "现有深度学习轨迹预测器缺乏适应异构现实场景的不确定性感知框架，而可靠的不确定性量化对安全关键自动驾驶系统至关重要", "method": "1) 将预测轨迹和真实轨迹投影到Frenet坐标系的地图参考路径上；2) 使用CopulaCPTS作为保形校准方法为不同场景生成时间预测区间；3) 通过轨迹可靠性判别器(TRD)分析平均误差和校准置信区间建立场景可靠性模型；4) 风险感知判别器整合纵向和横向预测区间识别关键点，分割轨迹为可靠/不可靠段", "result": "在真实世界nuPlan数据集上评估，证明了框架在多样化驾驶场景中进行场景感知不确定性量化和可靠性评估的有效性", "conclusion": "提出的框架能够为下游规划模块提供可操作的可靠性结果，在安全关键自动驾驶系统中实现可靠的轨迹预测不确定性量化"}}
{"id": "2512.05681", "pdf": "https://arxiv.org/pdf/2512.05681", "abs": "https://arxiv.org/abs/2512.05681", "authors": ["Tereza Novotna", "Jakub Harasta"], "title": "Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods", "categories": ["cs.CL", "cs.AI"], "comment": "The manuscript has been accepted for presentation as a short paper at the 38th International Conference on Legal Knowledge and Information Systems (JURIX 2025) in Torino, Italy", "summary": "Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.", "AI": {"tldr": "比较两种嵌入模型在捷克宪法法院判决检索中的性能：通用大模型（OpenAI）与领域特定BERT模型，提出噪声感知评估框架", "motivation": "案例法检索是耗时任务，现有司法数据库存在标签噪声和异质性，需要评估不同嵌入模型在噪声标签下的检索性能", "method": "提出噪声感知评估框架：包括IDF加权关键词重叠作为分级相关性、两个阈值二值化（0.20平衡、0.28严格）、配对bootstrap显著性检验、nDCG诊断和定性分析；比较OpenAI通用嵌入器与领域特定BERT（在30,000个判决上使用滑动窗口和注意力池化训练）", "result": "尽管绝对nDCG值不高（在噪声标签下预期如此），但OpenAI通用嵌入器在两个阈值设置下@10/@20/@100均显著优于领域预训练BERT；差异具有统计显著性；诊断表明低绝对值源于标签漂移和强理想标准而非模型效用不足", "conclusion": "通用大模型在噪声司法数据检索中优于领域特定模型；提出的评估框架足够稳健，可用于处理来自遗留司法数据库的异质性标签数据；标签噪声是评估中的关键挑战"}}
{"id": "2512.05674", "pdf": "https://arxiv.org/pdf/2512.05674", "abs": "https://arxiv.org/abs/2512.05674", "authors": ["Gargi Panda", "Soumitra Kundu", "Saumik Bhattacharya", "Aurobinda Routray"], "title": "Hyperspectral Unmixing with 3D Convolutional Sparse Coding and Projected Simplex Volume Maximization", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral unmixing (HSU) aims to separate each pixel into its constituent endmembers and estimate their corresponding abundance fractions. This work presents an algorithm-unrolling-based network for the HSU task, named the 3D Convolutional Sparse Coding Network (3D-CSCNet), built upon a 3D CSC model. Unlike existing unrolling-based networks, our 3D-CSCNet is designed within the powerful autoencoder (AE) framework. Specifically, to solve the 3D CSC problem, we propose a 3D CSC block (3D-CSCB) derived through deep algorithm unrolling. Given a hyperspectral image (HSI), 3D-CSCNet employs the 3D-CSCB to estimate the abundance matrix. The use of 3D CSC enables joint learning of spectral and spatial relationships in the 3D HSI data cube. The estimated abundance matrix is then passed to the AE decoder to reconstruct the HSI, and the decoder weights are extracted as the endmember matrix. Additionally, we propose a projected simplex volume maximization (PSVM) algorithm for endmember estimation, and the resulting endmembers are used to initialize the decoder weights of 3D-CSCNet. Extensive experiments on three real datasets and one simulated dataset with three different signal-to-noise ratio (SNR) levels demonstrate that our 3D-CSCNet outperforms state-of-the-art methods.", "AI": {"tldr": "提出了一种基于算法展开的3D卷积稀疏编码网络（3D-CSCNet）用于高光谱解混任务，结合了投影单纯形体积极大化算法进行端元初始化", "motivation": "现有基于算法展开的网络在高光谱解混任务中未能充分利用高光谱图像的三维数据立方体特性，且缺乏有效的端元初始化方法", "method": "1. 提出3D-CSCNet，基于自动编码器框架构建；2. 通过深度算法展开推导3D卷积稀疏编码块（3D-CSCB）用于估计丰度矩阵；3. 提出投影单纯形体积极大化（PSVM）算法进行端元估计并初始化解码器权重；4. 利用3D卷积同时学习光谱和空间关系", "result": "在三个真实数据集和一个模拟数据集（三种不同信噪比水平）上的实验表明，3D-CSCNet优于现有最先进方法", "conclusion": "3D-CSCNet通过结合3D卷积稀疏编码和投影单纯形体积极大化，有效解决了高光谱解混问题，在多个数据集上表现出优越性能"}}
{"id": "2512.05672", "pdf": "https://arxiv.org/pdf/2512.05672", "abs": "https://arxiv.org/abs/2512.05672", "authors": ["Yeobin Hong", "Suhyeon Lee", "Hyungjin Chung", "Jong Chul Ye"], "title": "InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.", "AI": {"tldr": "提出InverseCrafter方法，将4D视频生成任务重新定义为潜在空间中的修复问题，通过编码像素空间退化算子到连续多通道潜在掩码，避免重复VAE操作和反向传播的计算瓶颈。", "motivation": "当前可控4D视频生成方法主要依赖微调预训练视频扩散模型，这种方法计算成本高，需要大规模数据集和架构修改，且经常遭受灾难性遗忘问题，丢失模型原有的生成先验。", "method": "将4D生成任务重新定义为潜在空间中的修复问题，核心是提出一种原理性机制，将像素空间退化算子编码为连续多通道潜在掩码，从而绕过重复VAE操作和反向传播的昂贵计算瓶颈。", "result": "在相机控制任务中实现了可比的新视角生成和更优的测量一致性，计算开销接近零；在通用视频修复和编辑任务中也表现出色。", "conclusion": "InverseCrafter提供了一种高效的修复逆求解器，通过将4D生成重新定义为潜在空间问题，显著降低了计算成本，同时保持了生成质量，在视频生成和编辑任务中表现出优越性能。"}}
{"id": "2512.05669", "pdf": "https://arxiv.org/pdf/2512.05669", "abs": "https://arxiv.org/abs/2512.05669", "authors": ["Talha Enes Koksal", "Abdurrahman Gumus"], "title": "Deep Learning-Based Real-Time Sequential Facial Expression Analysis Using Geometric Features", "categories": ["cs.CV"], "comment": null, "summary": "Facial expression recognition is a crucial component in enhancing human-computer interaction and developing emotion-aware systems. Real-time detection and interpretation of facial expressions have become increasingly important for various applications, from user experience personalization to intelligent surveillance systems. This study presents a novel approach to real-time sequential facial expression recognition using deep learning and geometric features. The proposed method utilizes MediaPipe FaceMesh for rapid and accurate facial landmark detection. Geometric features, including Euclidean distances and angles, are extracted from these landmarks. Temporal dynamics are incorporated by analyzing feature differences between consecutive frames, enabling the detection of onset, apex, and offset phases of expressions. For classification, a ConvLSTM1D network followed by multilayer perceptron blocks is employed. The method's performance was evaluated on multiple publicly available datasets, including CK+, Oulu-CASIA (VIS and NIR), and MMI. Accuracies of 93%, 79%, 77%, and 68% were achieved respectively. Experiments with composite datasets were also conducted to assess the model's generalization capabilities. The approach demonstrated real-time applicability, processing approximately 165 frames per second on consumer-grade hardware. This research contributes to the field of facial expression analysis by providing a fast, accurate, and adaptable solution. The findings highlight the potential for further advancements in emotion-aware technologies and personalized user experiences, paving the way for more sophisticated human-computer interaction systems. To facilitate further research in this field, the complete source code for this study has been made publicly available on GitHub: https://github.com/miralab-ai/facial-expression-analysis.", "AI": {"tldr": "提出一种基于深度学习和几何特征的实时序列面部表情分析方法，使用MediaPipe FaceMesh进行面部关键点检测，提取几何特征，并通过ConvLSTM1D网络进行表情分类。", "motivation": "面部表情识别对于增强人机交互和开发情感感知系统至关重要。实时检测和解释面部表情在用户体验个性化、智能监控系统等应用中变得越来越重要。", "method": "使用MediaPipe FaceMesh进行快速准确的面部关键点检测，提取欧几里得距离和角度等几何特征，通过分析连续帧之间的特征差异来捕捉表情的起始、高峰和消退阶段，采用ConvLSTM1D网络和多层感知器块进行分类。", "result": "在CK+、Oulu-CASIA（VIS和NIR）、MMI等多个公开数据集上分别达到93%、79%、77%、68%的准确率。在消费级硬件上实现约165帧/秒的实时处理速度。", "conclusion": "该方法为面部表情分析提供了一个快速、准确且适应性强的解决方案，推动了情感感知技术和个性化用户体验的进一步发展，为更复杂的人机交互系统铺平了道路。"}}
{"id": "2512.05667", "pdf": "https://arxiv.org/pdf/2512.05667", "abs": "https://arxiv.org/abs/2512.05667", "authors": ["Jilles Steeve Dibangoye", "Thibaut Le Marre", "Ocan Sankur", "François Schwarzentruber"], "title": "On Dynamic Programming Theory for Leader-Follower Stochastic Games", "categories": ["cs.GT", "cs.AI"], "comment": "31 pages, 5 figures", "summary": "Leader-follower general-sum stochastic games (LF-GSSGs) model sequential decision-making under asymmetric commitment, where a leader commits to a policy and a follower best responds, yielding a strong Stackelberg equilibrium (SSE) with leader-favourable tie-breaking. This paper introduces a dynamic programming (DP) framework that applies Bellman recursion over credible sets-state abstractions formally representing all rational follower best responses under partial leader commitments-to compute SSEs. We first prove that any LF-GSSG admits a lossless reduction to a Markov decision process (MDP) over credible sets. We further establish that synthesising an optimal memoryless deterministic leader policy is NP-hard, motivating the development of ε-optimal DP algorithms with provable guarantees on leader exploitability. Experiments on standard mixed-motive benchmarks-including security games, resource allocation, and adversarial planning-demonstrate empirical gains in leader value and runtime scalability over state-of-the-art methods.", "AI": {"tldr": "该论文提出了一个用于领导者-追随者一般和随机博弈的动态规划框架，通过可信集合上的贝尔曼递归计算强斯塔克尔伯格均衡", "motivation": "领导者-追随者随机博弈在安全博弈、资源分配和对抗规划等应用中具有重要意义，但现有方法在计算效率和最优性保证方面存在局限，需要开发具有理论保证的动态规划算法", "method": "引入可信集合作为状态抽象，将领导者-追随者随机博弈无损约简为马尔可夫决策过程，并开发ε最优动态规划算法，在领导者可剥削性方面提供可证明的保证", "result": "证明了合成最优无记忆确定性领导者策略是NP难的，在标准混合动机基准测试（包括安全博弈、资源分配和对抗规划）中，新方法在领导者价值和运行时可扩展性方面优于现有技术", "conclusion": "提出的动态规划框架为领导者-追随者随机博弈提供了理论基础和实用算法，通过可信集合的贝尔曼递归有效计算强斯塔克尔伯格均衡，在理论和实验上都取得了显著进展"}}
{"id": "2512.05666", "pdf": "https://arxiv.org/pdf/2512.05666", "abs": "https://arxiv.org/abs/2512.05666", "authors": ["Irene Weber"], "title": "Feasibility of AI-Assisted Programming for End-User Development", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": "12 pages, 3 figures", "summary": "End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and \"copilots\", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.", "AI": {"tldr": "研究AI辅助编程对终端用户开发的可行性，探索生成式AI是否能够替代或补充现有的低代码/无代码平台", "motivation": "终端用户开发在组织数字化转型中扮演关键角色，当前主要依赖低代码/无代码平台。生成式AI和大型语言模型的出现为终端用户通过自然语言提示直接生成和优化代码提供了新可能，这相比传统可视化编程平台可能具有更大灵活性、更广泛适用性、更快开发速度等优势", "method": "通过案例研究，让非程序员参与者通过与AI助手交互来开发基本Web应用程序，分析任务完成情况和用户反馈", "result": "大多数研究参与者在合理时间内成功完成任务，并表达了对AI辅助终端用户编码作为可行方法的支持", "conclusion": "AI辅助终端用户编码是终端用户开发的可行范式，未来可能补充甚至替代低代码/无代码模型，具有实践、研究和教学方面的潜在影响"}}
{"id": "2512.05665", "pdf": "https://arxiv.org/pdf/2512.05665", "abs": "https://arxiv.org/abs/2512.05665", "authors": ["Shuai Dong", "Siyuan Wang", "Xingyu Liu", "Zhongyu Wei"], "title": "Interleaved Latent Visual Reasoning with Selective Perceptual Modeling", "categories": ["cs.CL", "cs.CV"], "comment": "11 pages, 6 figures. Code available at https://github.com/XD111ds/ILVR", "summary": "Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.", "AI": {"tldr": "提出ILVR框架，通过交错潜在视觉表示实现动态状态演化和精确感知建模的统一，解决多模态大语言模型中视觉反馈的计算成本问题", "motivation": "现有交错推理范式因重复编码像素密集图像而计算成本高昂，而潜在视觉推理方法要么因过度压缩特征牺牲精确感知建模，要么因静态非交错结构无法建模动态问题", "method": "ILVR框架将文本生成与潜在视觉表示交错进行，使用动量教师模型从辅助图像中选择性蒸馏相关特征到稀疏监督目标，通过自适应选择机制引导模型自主生成上下文感知的视觉信号", "result": "在多模态推理基准测试中，ILVR显著优于现有方法，有效弥合了细粒度感知和序列多模态推理之间的差距", "conclusion": "ILVR成功统一了动态状态演化和精确感知建模，通过交错潜在视觉表示和选择性感知建模，为多模态大语言模型提供了高效且精确的视觉推理能力"}}
{"id": "2512.05663", "pdf": "https://arxiv.org/pdf/2512.05663", "abs": "https://arxiv.org/abs/2512.05663", "authors": ["Johannes Meier", "Jonathan Michel", "Oussema Dhaouadi", "Yung-Hsu Yang", "Christoph Reich", "Zuria Bauer", "Stefan Roth", "Marc Pollefeys", "Jacques Kaiser", "Daniel Cremers"], "title": "LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection", "categories": ["cs.CV"], "comment": null, "summary": "Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.", "AI": {"tldr": "提出LeAD-M3D方法，通过非对称蒸馏技术实现实时单目3D目标检测，在保持高精度的同时显著提升推理速度", "motivation": "实时单目3D检测面临深度模糊、视角变化和计算成本高的挑战，现有方法要么依赖额外模态（如LiDAR），要么牺牲效率换取精度", "method": "采用三个核心技术：1) A2D2非对称增强去噪蒸馏，通过质量重要性加权的深度特征损失从干净图像教师模型向混合噪声学生模型传递几何知识；2) CM3D 3D感知一致性匹配，将3D MGIoU集成到匹配分数中改进预测与真值分配；3) CGI3D置信度门控3D推理，通过限制昂贵3D回归到高置信度区域加速检测", "result": "在KITTI和Waymo数据集上达到最先进精度，在Rope3D数据集上获得最佳汽车AP，同时推理速度比先前高精度方法快3.6倍", "conclusion": "LeAD-M3D证明了单目3D检测中高保真度和实时效率可以同时实现，无需LiDAR、立体视觉或几何假设，为单目3D检测设定了新的帕累托前沿"}}
{"id": "2512.05658", "pdf": "https://arxiv.org/pdf/2512.05658", "abs": "https://arxiv.org/abs/2512.05658", "authors": ["Pietro Ferrazzi", "Aitor Soroa", "Rodrigo Agerri"], "title": "Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.", "AI": {"tldr": "提出一种基于检索增强生成的方法，为多语言医学问答生成基于事实医学知识的推理轨迹，以提升大型语言模型在医学QA任务中的可靠性和性能。", "motivation": "现有医学问答方法主要针对英语，且依赖通用大型语言模型的知识蒸馏，存在医学知识可靠性问题。需要开发能够生成基于事实医学知识的多语言推理轨迹的方法，以支持更安全、透明的多语言临床决策支持工具。", "method": "采用检索增强生成方法，从医学维基百科中检索信息，为MedQA和MedMCQA数据集中的医学问题生成英语、意大利语和西班牙语三种语言的推理轨迹。共生成50万条推理轨迹，并将数据集扩展到意大利语和西班牙语。", "result": "在医学QA基准测试中，无论是在领域内还是领域外设置下，生成的推理轨迹通过上下文学习（少样本）和监督微调都能显著提升性能，在8B参数的大型语言模型中取得了最先进的结果。", "conclusion": "该方法能够生成基于事实医学知识的多语言推理轨迹，支持开发更安全、透明的多语言临床决策支持工具。研究团队发布了完整的资源套件，包括推理轨迹、翻译的QA数据集、医学维基百科和微调模型。"}}
{"id": "2512.05651", "pdf": "https://arxiv.org/pdf/2512.05651", "abs": "https://arxiv.org/abs/2512.05651", "authors": ["Nan Zhong", "Mian Zou", "Yiran Xu", "Zhenxing Qian", "Xinpeng Zhang", "Baoyuan Wu", "Kede Ma"], "title": "Self-Supervised AI-Generated Image Detection: A Camera Metadata Perspective", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of AI-generated imagery poses escalating challenges for multimedia forensics, yet many existing detectors depend on assumptions about the internals of specific generative models, limiting their cross-model applicability. We introduce a self-supervised approach for detecting AI-generated images that leverages camera metadata -- specifically exchangeable image file format (EXIF) tags -- to learn features intrinsic to digital photography. Our pretext task trains a feature extractor solely on camera-captured photographs by classifying categorical EXIF tags (\\eg, camera model and scene type) and pairwise-ranking ordinal and continuous EXIF tags (\\eg, focal length and aperture value). Using these EXIF-induced features, we first perform one-class detection by modeling the distribution of photographic images with a Gaussian mixture model and flagging low-likelihood samples as AI-generated. We then extend to binary detection that treats the learned extractor as a strong regularizer for a classifier of the same architecture, operating on high-frequency residuals from spatially scrambled patches. Extensive experiments across various generative models demonstrate that our EXIF-induced detectors substantially advance the state of the art, delivering strong generalization to in-the-wild samples and robustness to common benign image perturbations.", "AI": {"tldr": "提出一种基于相机元数据的自监督AI生成图像检测方法，利用EXIF标签学习数字摄影固有特征，实现跨模型泛化检测", "motivation": "现有AI生成图像检测器通常依赖于特定生成模型的内部假设，限制了跨模型适用性，需要一种不依赖生成模型内部信息、具有更好泛化能力的检测方法", "method": "通过自监督方式训练特征提取器：在相机拍摄的真实照片上，分类分类EXIF标签（如相机型号、场景类型），并对序数和连续EXIF标签（如焦距、光圈值）进行成对排序；使用这些EXIF诱导特征，先通过高斯混合模型建模真实图像分布进行单类检测，再扩展为二元检测，将学习到的提取器作为分类器的强正则化器", "result": "在各种生成模型上的广泛实验表明，该方法显著推进了现有技术水平，对野外样本具有强泛化能力，并对常见良性图像扰动具有鲁棒性", "conclusion": "基于相机元数据的自监督方法为AI生成图像检测提供了有效的跨模型解决方案，通过利用数字摄影的固有特征，实现了对AI生成图像的准确检测和良好泛化"}}
{"id": "2512.05638", "pdf": "https://arxiv.org/pdf/2512.05638", "abs": "https://arxiv.org/abs/2512.05638", "authors": ["Suman Sanyal"], "title": "Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \\emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \\emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \\emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.", "AI": {"tldr": "该论文提出了\"模块化喷射\"框架，用于诊断监督学习管道中的\"海市蜃楼\"与可识别性问题，通过分析模块级表示来评估模型内部分解的唯一性。", "motivation": "传统监督学习主要通过预测风险评估模型，但这种方法无法确定模型内部分解是否由数据和评估设计唯一确定。需要一种方法来诊断多个不同模块分解是否会产生相同的输入-输出映射（海市蜃楼现象）。", "method": "提出模块化喷射框架：给定任务流形、模块分解和模块级表示访问，估计经验喷射（描述每个模块对结构化输入扰动的局部线性响应映射）。开发MoJet算法进行经验喷射估计和海市蜃楼诊断。", "result": "在双模块线性回归管道中证明了喷射可识别性定理：在温和的秩假设和模块级喷射访问下，内部分解是唯一确定的。而仅基于风险的评估允许大量海市蜃楼分解实现相同的输入-输出映射。在线性和深度回归以及管道分类中展示了该框架。", "conclusion": "模块化喷射提供了一种超越预测风险的方法，用于诊断监督学习管道中的可识别性问题。该框架能够区分海市蜃楼（多个分解产生相同行为）和可识别（分解唯一确定）的机制，为模型解释和模块化分析提供了新工具。"}}
{"id": "2512.05635", "pdf": "https://arxiv.org/pdf/2512.05635", "abs": "https://arxiv.org/abs/2512.05635", "authors": ["Georgy Perevozchikov", "Nancy Mehta", "Egor Ershov", "Radu Timofte"], "title": "Experts-Guided Unbalanced Optimal Transport for ISP Learning from Unpaired and/or Paired Data", "categories": ["cs.CV"], "comment": null, "summary": "Learned Image Signal Processing (ISP) pipelines offer powerful end-to-end performance but are critically dependent on large-scale paired raw-to-sRGB datasets. This reliance on costly-to-acquire paired data remains a significant bottleneck. To address this challenge, we introduce a novel, unsupervised training framework based on Optimal Transport capable of training arbitrary ISP architectures in both unpaired and paired modes. We are the first to successfully apply Unbalanced Optimal Transport (UOT) for this complex, cross-domain translation task. Our UOT-based framework provides robustness to outliers in the target sRGB data, allowing it to discount atypical samples that would be prohibitively costly to map. A key component of our framework is a novel ``committee of expert discriminators,'' a hybrid adversarial regularizer. This committee guides the optimal transport mapping by providing specialized, targeted gradients to correct specific ISP failure modes, including color fidelity, structural artifacts, and frequency-domain realism. To demonstrate the superiority of our approach, we retrained existing state-of-the-art ISP architectures using our paired and unpaired setups. Our experiments show that while our framework, when trained in paired mode, exceeds the performance of the original paired methods across all metrics, our unpaired mode concurrently achieves quantitative and qualitative performance that rivals, and in some cases surpasses, the original paired-trained counterparts. The code and pre-trained models are available at: https://github.com/gosha20777/EGUOT-ISP.git.", "AI": {"tldr": "提出一种基于非平衡最优传输的专家引导框架，用于在无配对或配对数据下学习图像信号处理（ISP）管道", "motivation": "传统学习型ISP管道严重依赖大规模配对raw-to-sRGB数据集，这些数据获取成本高昂，成为主要瓶颈。需要开发能够在无配对或配对数据下训练的方法", "method": "提出基于非平衡最优传输（UOT）的无监督训练框架，首次将UOT应用于跨域翻译任务。框架包含\"专家判别器委员会\"作为混合对抗正则化器，通过提供专门梯度来纠正特定ISP失败模式（色彩保真度、结构伪影、频域真实性）", "result": "在配对模式下，框架性能超过原始配对方法的所有指标；在无配对模式下，定量和定性性能与原始配对训练方法相当甚至超越。成功重新训练了现有最先进的ISP架构", "conclusion": "提出的专家引导非平衡最优传输框架能够有效解决ISP学习中的配对数据依赖问题，在无配对和配对模式下均表现出色，为ISP训练提供了灵活且强大的解决方案"}}
{"id": "2512.05619", "pdf": "https://arxiv.org/pdf/2512.05619", "abs": "https://arxiv.org/abs/2512.05619", "authors": ["Menghua Jiang", "Haokai Gao", "Shuhao Chen", "Yin Chen"], "title": "Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting", "categories": ["cs.AI"], "comment": "Accepted by ECAI 2025", "summary": "Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist", "AI": {"tldr": "提出了一种名为DeepDist的新型随机局部搜索算法，用于解决部分最大可满足性(PMS)和加权部分最大可满足性(WPMS)问题，通过深度区分子句加权方案显著提升求解性能。", "motivation": "现有的随机局部搜索算法在处理PMS和WPMS问题时，通常采用统一的子句权重更新策略，未能充分考虑这两种问题类型之间的结构差异，导致求解效果受限。", "method": "提出了深度区分子句加权方案，首次根据PMS和WPMS实例的不同条件分别更新子句权重；设计了新的权重初始化方法以适应两种实例类型的特性；引入了优先满足单元子句和硬子句的decimation方法。", "result": "DeepDist在最近MaxSAT评估的基准测试中超越了最先进的SLS求解器；与TT-Open-WBO-Inc结合的混合求解器甚至超越了MaxSAT评估2024的获胜者SPB-MaxSAT-c-Band和SPB-MaxSAT-c-FPS。", "conclusion": "通过深度区分PMS和WPMS问题的特性，并设计相应的子句加权方案和辅助方法，能够显著提升随机局部搜索算法在最大可满足性问题上的求解性能。"}}
{"id": "2512.05613", "pdf": "https://arxiv.org/pdf/2512.05613", "abs": "https://arxiv.org/abs/2512.05613", "authors": ["Pasquale De Marinis", "Pieter M. Blok", "Uzay Kaymak", "Rogier Brussee", "Gennaro Vessio", "Giovanna Castellano"], "title": "DistillFSS: Synthesizing Few-Shot Knowledge into a Lightweight Segmentation Model", "categories": ["cs.CV"], "comment": null, "summary": "Cross-Domain Few-Shot Semantic Segmentation (CD-FSS) seeks to segment unknown classes in unseen domains using only a few annotated examples. This setting is inherently challenging: source and target domains exhibit substantial distribution shifts, label spaces are disjoint, and support images are scarce--making standard episodic methods unreliable and computationally demanding at test time. To address these constraints, we propose DistillFSS, a framework that embeds support-set knowledge directly into a model's parameters through a teacher--student distillation process. By internalizing few-shot reasoning into a dedicated layer within the student network, DistillFSS eliminates the need for support images at test time, enabling fast, lightweight inference, while allowing efficient extension to novel classes in unseen domains through rapid teacher-driven specialization. Combined with fine-tuning, the approach scales efficiently to large support sets and significantly reduces computational overhead. To evaluate the framework under realistic conditions, we introduce a new CD-FSS benchmark spanning medical imaging, industrial inspection, and remote sensing, with disjoint label spaces and variable support sizes. Experiments show that DistillFSS matches or surpasses state-of-the-art baselines, particularly in multi-class and multi-shot scenarios, while offering substantial efficiency gains. The code is available at https://github.com/pasqualedem/DistillFSS.", "AI": {"tldr": "提出DistillFSS框架，通过师生蒸馏将少样本知识直接嵌入模型参数中，实现无需支持图像的快速轻量推理", "motivation": "解决跨域少样本语义分割中的挑战：源域和目标域存在显著分布偏移，标签空间不重叠，支持图像稀缺，导致标准片段方法不可靠且计算量大", "method": "通过师生蒸馏过程将支持集知识直接嵌入学生网络参数中，在模型内部专门层中内化少样本推理，消除测试时对支持图像的需求", "result": "在医疗影像、工业检测和遥感的新CD-FSS基准测试中，DistillFSS达到或超越最先进基线，尤其在多类和多样本场景中表现优异，同时提供显著的效率提升", "conclusion": "DistillFSS框架通过将少样本知识蒸馏到轻量模型中，实现了高效、快速的跨域少样本语义分割，显著降低了计算开销"}}
{"id": "2512.05610", "pdf": "https://arxiv.org/pdf/2512.05610", "abs": "https://arxiv.org/abs/2512.05610", "authors": ["Juho Korkeala", "Jesse Muhojoki", "Josef Taher", "Klaara Salolahti", "Matti Hyyppä", "Antero Kukko", "Juha Hyyppä"], "title": "NormalView: sensor-agnostic tree species classification from backpack and aerial lidar data using geometric projections", "categories": ["cs.CV"], "comment": "19 pages, 8 figures", "summary": "Laser scanning has proven to be an invaluable tool in assessing the decomposition of forest environments. Mobile laser scanning (MLS) has shown to be highly promising for extremely accurate, tree level inventory. In this study, we present NormalView, a sensor-agnostic projection-based deep learning method for classifying tree species from point cloud data. NormalView embeds local geometric information into two-dimensional projections, in the form of normal vector estimates, and uses the projections as inputs to an image classification network, YOLOv11. In addition, we inspected the effect of multispectral radiometric intensity information on classification performance. We trained and tested our model on high-density MLS data (7 species, ~5000 pts/m^2), as well as high-density airborne laser scanning (ALS) data (9 species, >1000 pts/m^2). On the MLS data, NormalView achieves an overall accuracy (macro-average accuracy) of 95.5 % (94.8 %), and 91.8 % (79.1 %) on the ALS data. We found that having intensity information from multiple scanners provides benefits in tree species classification, and the best model on the multispectral ALS dataset was a model using intensity information from all three channels of the multispectral ALS. This study demonstrates that projection-based methods, when enhanced with geometric information and coupled with state-of-the-art image classification backbones, can achieve exceptional results. Crucially, these methods are sensor-agnostic, relying only on geometric information. Additionally, we publically release the MLS dataset used in the study.", "AI": {"tldr": "提出NormalView方法，一种传感器无关的基于投影的深度学习技术，用于从点云数据中分类树种，通过将局部几何信息嵌入二维投影并使用图像分类网络进行处理。", "motivation": "激光扫描在森林环境评估中具有重要价值，移动激光扫描（MLS）在树木级库存方面表现出极高精度。然而，需要一种传感器无关的方法来处理不同来源的点云数据，并有效利用几何信息进行树种分类。", "method": "NormalView方法将局部几何信息（法向量估计）嵌入二维投影，使用这些投影作为图像分类网络YOLOv11的输入。同时研究了多光谱辐射强度信息对分类性能的影响。", "result": "在高密度MLS数据（7个树种）上达到95.5%的整体准确率（宏平均94.8%），在高密度ALS数据（9个树种）上达到91.8%的整体准确率（宏平均79.1%）。多光谱强度信息对分类有积极影响。", "conclusion": "基于投影的方法在增强几何信息并结合先进的图像分类骨干网络时，能够取得优异结果。这些方法具有传感器无关性，仅依赖几何信息。研究还公开了使用的MLS数据集。"}}
{"id": "2512.05599", "pdf": "https://arxiv.org/pdf/2512.05599", "abs": "https://arxiv.org/abs/2512.05599", "authors": ["Panagiotis Giannikos", "Lampis Papakostas", "Evangelos Katralis", "Panagiotis Mavridis", "George Chryssinas", "Myrto Inglezou", "Nikolaos Panagopoulos", "Antonis Porichis", "Athanasios Mastrogeorgiou", "Panagiotis Chatzakos"], "title": "An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Battery recycling is becoming increasingly critical due to the rapid growth in battery usage and the limited availability of natural resources. Moreover, as battery energy densities continue to rise, improper handling during recycling poses significant safety hazards, including potential fires at recycling facilities. Numerous systems have been proposed for battery detection and removal from WEEE recycling lines, including X-ray and RGB-based visual inspection methods, typically driven by AI-powered object detection models (e.g., Mask R-CNN, YOLO, ResNets). Despite advances in optimizing detection techniques and model modifications, a fully autonomous solution capable of accurately identifying and sorting batteries across diverse WEEEs types has yet to be realized. In response to these challenges, we present our novel approach which integrates a specialized X-ray transmission dual energy imaging subsystem with advanced pre-processing algorithms, enabling high-contrast image reconstruction for effective differentiation of dense and thin materials in WEEE. Devices move along a conveyor belt through a high-resolution X-ray imaging system, where YOLO and U-Net models precisely detect and segment battery-containing items. An intelligent tracking and position estimation algorithm then guides a Delta robot equipped with a suction gripper to selectively extract and properly discard the targeted devices. The approach is validated in a photorealistic simulation environment developed in NVIDIA Isaac Sim and on the real setup.", "AI": {"tldr": "开发一个集成X射线成像、AI目标检测与分割以及Delta机器人操作的WEEE（废弃电子电气设备）电池分拣系统", "motivation": "电池回收日益重要，但现有系统无法实现完全自主的电池识别与分拣，且高能量密度电池的不当处理存在安全隐患", "method": "集成专用X射线双能成像子系统与先进预处理算法，使用YOLO和U-Net模型进行电池检测与分割，通过智能跟踪和位置估计算法引导Delta机器人进行选择性提取", "result": "在NVIDIA Isaac Sim开发的逼真仿真环境和实际设置中验证了该方法的有效性", "conclusion": "提出了一种能够准确识别和分拣各类WEEE中电池的集成系统，为电池回收提供了更安全高效的解决方案"}}
{"id": "2512.05597", "pdf": "https://arxiv.org/pdf/2512.05597", "abs": "https://arxiv.org/abs/2512.05597", "authors": ["Ruihong Yin", "Xuepeng Shi", "Oleksandr Bailo", "Marco Manfredi", "Theo Gevers"], "title": "Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "Recent perception-generalist approaches based on language models have achieved state-of-the-art results across diverse tasks, including 3D scene layout estimation, via unified architecture and interface. However, these approaches rely on autoregressive next-token prediction, which is inherently slow. In this work, we introduce Fast SceneScript, a novel structured language model for accurate and efficient 3D scene layout estimation. Our method employs multi-token prediction (MTP) to reduce the number of autoregressive iterations and significantly accelerate inference. While MTP improves speed, unreliable token predictions can significantly reduce accuracy. To filter out unreliable tokens, we adapt self-speculative decoding (SSD) for structured language models and introduce confidence-guided decoding (CGD) with an improved scoring mechanism for token reliability. Furthermore, we design a parameter-efficient mechanism that reduces the parameter overhead of MTP. Extensive experiments on the ASE and Structured3D benchmarks demonstrate that Fast SceneScript can generate up to 9 tokens per decoder inference step without compromising accuracy, while adding only $\\sim7.5\\%$ additional parameters.", "AI": {"tldr": "Fast SceneScript是一种用于3D场景布局估计的高效结构化语言模型，通过多令牌预测减少自回归迭代次数，显著加速推理速度", "motivation": "现有的基于语言模型的感知通用方法虽然取得了SOTA结果，但依赖自回归的下一个令牌预测，推理速度慢。需要一种既能保持准确性又能显著提升效率的3D场景布局估计方法", "method": "1. 采用多令牌预测(MTP)减少自回归迭代次数；2. 为过滤不可靠令牌，将自推测解码(SSD)适配到结构化语言模型；3. 引入置信度引导解码(CGD)和改进的令牌可靠性评分机制；4. 设计参数高效机制减少MTP的参数开销", "result": "在ASE和Structured3D基准测试中，Fast SceneScript每次解码器推理步骤可生成最多9个令牌而不影响准确性，仅增加约7.5%的额外参数", "conclusion": "Fast SceneScript通过多令牌预测和置信度引导解码，在保持3D场景布局估计准确性的同时显著提升推理效率，为结构化语言模型提供了高效解决方案"}}
{"id": "2512.05594", "pdf": "https://arxiv.org/pdf/2512.05594", "abs": "https://arxiv.org/abs/2512.05594", "authors": ["Roos M. Bakker", "Daan L. Di Scala", "Maaike H. T. de Boer", "Stephan A. Raaijmakers"], "title": "Ontology Learning with LLMs: A Benchmark Study on Axiom Identification", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to Semantic Web Journal, under review", "summary": "Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.", "AI": {"tldr": "本文提出OntoAxiom基准，系统评估大型语言模型在识别本体公理方面的性能，包括子类、不相交、子属性、定义域和值域等五种公理类型。", "motivation": "本体开发需要大量建模和领域专业知识，自动化本体学习过程具有重要价值。随着自然语言处理技术特别是大型语言模型的发展，需要系统评估LLMs在本体公理识别任务上的能力。", "method": "构建包含9个中等规模本体、17,118个三元组和2,771个公理的OntoAxiom基准。评估12个LLMs在三种few-shot设置和两种提示策略下的性能：直接查询所有公理的方法与逐个公理查询的方法。", "result": "逐个公理提示策略比直接方法获得更高的F1分数。性能因公理类型而异，某些公理更难识别。领域影响显著：FOAF本体子类公理得分0.642，而音乐本体仅0.218。大模型优于小模型，但小模型在资源受限场景仍可用。", "conclusion": "虽然LLMs性能不足以完全自动化公理识别，但能为本体工程师提供有价值的候选公理，支持本体的开发和精化。逐个公理方法更有效，但性能受公理类型和领域影响。"}}
{"id": "2512.05593", "pdf": "https://arxiv.org/pdf/2512.05593", "abs": "https://arxiv.org/abs/2512.05593", "authors": ["Rong Wang", "Wei Mao", "Changsheng Lu", "Hongdong Li"], "title": "Learning High-Fidelity Cloth Animation via Skinning-Free Image Transfer", "categories": ["cs.CV"], "comment": "Accepted to 3DV 2026", "summary": "We present a novel method for generating 3D garment deformations from given body poses, which is key to a wide range of applications, including virtual try-on and extended reality. To simplify the cloth dynamics, existing methods mostly rely on linear blend skinning to obtain low-frequency posed garment shape and only regress high-frequency wrinkles. However, due to the lack of explicit skinning supervision, such skinning-based approach often produces misaligned shapes when posing the garment, consequently corrupts the high-frequency signals and fails to recover high-fidelity wrinkles. To tackle this issue, we propose a skinning-free approach by independently estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. To further improve the visual quality of animation, we propose to encode both vertex attributes as rendered texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image models to recover fine-grained visual details in wrinkles, while maintaining superior scalability for garments of diverse topologies without relying on manual UV partition. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and robustly recover deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves animation quality on various garment types and recovers finer wrinkles than state-of-the-art methods.", "AI": {"tldr": "提出一种无需蒙皮的高保真布料动画生成方法，通过图像传输技术从人体姿态生成3D服装变形", "motivation": "现有方法依赖线性混合蒙皮获取低频姿态服装形状，但由于缺乏明确的蒙皮监督，在姿态变化时会产生错位形状，从而破坏高频信号并无法恢复高保真皱纹", "method": "提出蒙皮自由方法：独立估计顶点位置（低频姿态形状）和顶点法线（高频皱纹细节）；将顶点属性编码为渲染纹理图像，通过2D图像传输实现3D服装变形；利用预训练图像模型恢复皱纹细节；提出多模态融合整合两种频率模态约束", "result": "在各种服装类型上显著提高动画质量，恢复比现有方法更精细的皱纹，对不同拓扑结构的服装具有更好的可扩展性", "conclusion": "通过解耦低频形状和高频细节，结合图像传输技术和预训练图像模型，能够有效生成高保真布料动画，解决了传统蒙皮方法导致的形状错位问题"}}
{"id": "2512.05592", "pdf": "https://arxiv.org/pdf/2512.05592", "abs": "https://arxiv.org/abs/2512.05592", "authors": ["Katsuhiko Yamamoto", "Koichi Miyazaki", "Shogo Seki"], "title": "The T12 System for AudioMOS Challenge 2025: Audio Aesthetics Score Prediction System Using KAN- and VERSA-based Models", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by IEEE ASRU 2025", "summary": "We propose an audio aesthetics score (AES) prediction system by CyberAgent (AESCA) for AudioMOS Challenge 2025 (AMC25) Track 2. The AESCA comprises a Kolmogorov--Arnold Network (KAN)-based audiobox aesthetics and a predictor from the metric scores using the VERSA toolkit. In the KAN-based predictor, we replaced each multi-layer perceptron layer in the baseline model with a group-rational KAN and trained the model with labeled and pseudo-labeled audio samples. The VERSA-based predictor was designed as a regression model using extreme gradient boosting, incorporating outputs from existing metrics. Both the KAN- and VERSA-based models predicted the AES, including the four evaluation axes. The final AES values were calculated using an ensemble model that combined four KAN-based models and a VERSA-based model. Our proposed T12 system yielded the best correlations among the submitted systems, in three axes at the utterance level, two axes at the system level, and the overall average.", "AI": {"tldr": "本文提出了一种用于AudioMOS Challenge 2025 Track 2的音频美学评分预测系统，结合了KAN和VERSA两种模型，通过集成方法预测音频美学评分", "motivation": "为了解决音频美学评分预测问题，参加AudioMOS Challenge 2025竞赛，需要开发能够准确预测音频美学评分的系统", "method": "1. 使用KAN（Kolmogorov-Arnold Network）替换基线模型中的多层感知机层，采用分组有理KAN结构，使用标注和伪标注音频样本训练；2. 使用VERSA工具包构建基于极端梯度提升的回归模型，整合现有度量指标输出；3. 通过集成四个KAN模型和一个VERSA模型计算最终音频美学评分", "result": "提出的T12系统在提交的系统中获得了最佳相关性：在话语级别三个评估轴上、系统级别两个评估轴上以及整体平均得分上都表现最优", "conclusion": "结合KAN和VERSA模型的集成方法在音频美学评分预测任务中表现出色，验证了该混合架构的有效性"}}
{"id": "2512.05579", "pdf": "https://arxiv.org/pdf/2512.05579", "abs": "https://arxiv.org/abs/2512.05579", "authors": ["Panagiota Moraiti", "Panagiotis Giannikos", "Athanasios Mastrogeorgiou", "Panagiotis Mavridis", "Linghao Zhou", "Panagiotis Chatzakos"], "title": "A Comprehensive Framework for Automated Quality Control in the Automotive Industry", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper presents a cutting-edge robotic inspection solution designed to automate quality control in automotive manufacturing. The system integrates a pair of collaborative robots, each equipped with a high-resolution camera-based vision system to accurately detect and localize surface and thread defects in aluminum high-pressure die casting (HPDC) automotive components. In addition, specialized lenses and optimized lighting configurations are employed to ensure consistent and high-quality image acquisition. The YOLO11n deep learning model is utilized, incorporating additional enhancements such as image slicing, ensemble learning, and bounding-box merging to significantly improve performance and minimize false detections. Furthermore, image processing techniques are applied to estimate the extent of the detected defects. Experimental results demonstrate real-time performance with high accuracy across a wide variety of defects, while minimizing false detections. The proposed solution is promising and highly scalable, providing the flexibility to adapt to various production environments and meet the evolving demands of the automotive industry.", "AI": {"tldr": "提出一个用于汽车行业自动化质量控制的综合框架，集成了协作机器人、视觉系统和深度学习模型来检测铝压铸汽车零部件的表面和螺纹缺陷。", "motivation": "汽车制造业需要高效、准确的质量控制系统来检测铝压铸零部件的表面和螺纹缺陷，传统人工检测方法效率低、一致性差，需要自动化解决方案来提升检测精度和生产效率。", "method": "采用一对协作机器人，每个配备高分辨率相机视觉系统，使用特殊镜头和优化光照配置确保图像质量。采用YOLO11n深度学习模型，结合图像切片、集成学习和边界框融合技术来提升性能，并应用图像处理技术估计缺陷程度。", "result": "实验结果显示系统具有实时性能，对各种缺陷检测准确率高，同时最小化误检。系统表现出良好的可扩展性和适应性，能够满足不同生产环境的需求。", "conclusion": "提出的解决方案具有前景且高度可扩展，为汽车制造业提供了灵活、高效的自动化质量控制方案，能够适应各种生产环境并满足行业不断变化的需求。"}}
{"id": "2512.05578", "pdf": "https://arxiv.org/pdf/2512.05578", "abs": "https://arxiv.org/abs/2512.05578", "authors": ["Zheng Sun", "Zhipeng Dong", "Shixiong Wang", "Zhongyi Chu", "Fei Chen"], "title": "A Hyperspectral Imaging Guided Robotic Grasping System", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025", "summary": "Hyperspectral imaging is an advanced technique for precisely identifying and analyzing materials or objects. However, its integration with robotic grasping systems has so far been explored due to the deployment complexities and prohibitive costs. Within this paper, we introduce a novel hyperspectral imaging-guided robotic grasping system. The system consists of PRISM (Polyhedral Reflective Imaging Scanning Mechanism) and the SpectralGrasp framework. PRISM is designed to enable high-precision, distortion-free hyperspectral imaging while simplifying system integration and costs. SpectralGrasp generates robotic grasping strategies by effectively leveraging both the spatial and spectral information from hyperspectral images. The proposed system demonstrates substantial improvements in both textile recognition compared to human performance and sorting success rate compared to RGB-based methods. Additionally, a series of comparative experiments further validates the effectiveness of our system. The study highlights the potential benefits of integrating hyperspectral imaging with robotic grasping systems, showcasing enhanced recognition and grasping capabilities in complex and dynamic environments. The project is available at: https://zainzh.github.io/PRISM.", "AI": {"tldr": "开发了一个基于高光谱成像引导的机器人抓取系统，通过PRISM机制和SpectralGrasp框架实现高精度、无失真的高光谱成像与机器人抓取策略生成", "motivation": "高光谱成像技术能够精确识别和分析材料或物体，但其与机器人抓取系统的集成由于部署复杂性和成本过高而未被充分探索。本研究旨在解决这一集成难题，开发一个实用且成本效益高的高光谱成像引导机器人抓取系统", "method": "系统包含两个核心组件：1) PRISM（多面体反射成像扫描机制），用于实现高精度、无失真的高光谱成像，同时简化系统集成和降低成本；2) SpectralGrasp框架，通过有效利用高光谱图像的空间和光谱信息来生成机器人抓取策略", "result": "系统在纺织品识别方面相比人类表现有显著改进，在分拣成功率方面相比基于RGB的方法有明显提升。一系列对比实验进一步验证了系统的有效性", "conclusion": "该研究展示了将高光谱成像与机器人抓取系统集成的潜在优势，在复杂动态环境中增强了识别和抓取能力。PRISM机制简化了系统集成和成本，为实际应用提供了可行性"}}
{"id": "2512.05576", "pdf": "https://arxiv.org/pdf/2512.05576", "abs": "https://arxiv.org/abs/2512.05576", "authors": ["Ting-Ting Xie", "Yixin Zhang"], "title": "CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning", "categories": ["cs.AI"], "comment": "2nd Place Solution to the CURE-Bench Competition @ NeurIPS 2025. Code available at https://github.com/June01/CureAgent", "summary": "Current clinical agent built on small LLMs, such as TxAgent suffer from a \\textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \\textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \\textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.", "AI": {"tldr": "提出了CureAgent框架，这是一个无需训练的Executor-Analyst架构，用于解决临床推理中的上下文利用失败问题，将工具执行与临床推理解耦，通过分层集成策略实现高性能临床推理。", "motivation": "当前基于小型LLM的临床代理（如TxAgent）存在\"上下文利用失败\"问题：模型能够成功检索生物医学证据，但无法基于这些信息进行诊断推理。需要解决这种推理缺陷，同时避免昂贵的端到端微调。", "method": "提出Executor-Analyst框架：将专门的TxAgents（执行器）与长上下文基础模型（分析师）协同工作。采用分层集成策略而非全局池化，保留证据多样性。通过架构工程而非训练来解决推理问题。", "result": "在CURE-Bench上实现了最先进的性能，无需昂贵的端到端微调。发现了关键扩展见解：1）上下文-性能悖论（超过12k token会引入噪声降低准确性）；2）动作空间的维度诅咒（工具集扩展需要分层检索策略）。", "conclusion": "通过训练免费的架构工程方法，为下一代可信赖的AI驱动治疗提供了可扩展、敏捷的基础。证明了模块化架构在解决临床推理缺陷方面的有效性，同时避免了昂贵的训练成本。"}}
{"id": "2512.05571", "pdf": "https://arxiv.org/pdf/2512.05571", "abs": "https://arxiv.org/abs/2512.05571", "authors": ["Xingyu Zhang", "Anna Reithmeir", "Fryderyk Kögl", "Rickmer Braren", "Julia A. Schnabel", "Daniel M. Lang"], "title": "MedDIFT: Multi-Scale Diffusion-Based Correspondence in 3D Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Accurate spatial correspondence between medical images is essential for longitudinal analysis, lesion tracking, and image-guided interventions. Medical image registration methods rely on local intensity-based similarity measures, which fail to capture global semantic structure and often yield mismatches in low-contrast or anatomically variable regions. Recent advances in diffusion models suggest that their intermediate representations encode rich geometric and semantic information. We present MedDIFT, a training-free 3D correspondence framework that leverages multi-scale features from a pretrained latent medical diffusion model as voxel descriptors. MedDIFT fuses diffusion activations into rich voxel-wise descriptors and matches them via cosine similarity, with an optional local-search prior. On a publicly available lung CT dataset, MedDIFT achieves correspondence accuracy comparable to the state-of-the-art learning-based UniGradICON model and surpasses conventional B-spline-based registration, without requiring any task-specific model training. Ablation experiments confirm that multi-level feature fusion and modest diffusion noise improve performance.", "AI": {"tldr": "提出MedDIFT框架，利用预训练的医学扩散模型的多尺度特征作为体素描述符，实现3D医学图像的空间对应关系匹配，无需特定任务训练。", "motivation": "传统医学图像配准方法依赖局部强度相似性度量，难以捕捉全局语义结构，在低对比度或解剖变异区域容易产生误匹配。扩散模型的中间表示编码了丰富的几何和语义信息，可用于改进对应关系匹配。", "method": "MedDIFT利用预训练的潜在医学扩散模型提取多尺度特征作为体素描述符，通过余弦相似度进行匹配，可选局部搜索先验。该方法无需特定任务训练，融合扩散激活形成丰富的体素级描述符。", "result": "在公开的肺部CT数据集上，MedDIFT达到了与最先进的学习型UniGradICON模型相当的对应精度，超越了传统的B样条配准方法。消融实验证实多级特征融合和适度的扩散噪声能提升性能。", "conclusion": "MedDIFT证明了预训练扩散模型的特征在医学图像对应关系任务中的有效性，提供了一种无需特定任务训练的3D对应框架，在保持精度的同时避免了复杂的训练过程。"}}
{"id": "2512.05564", "pdf": "https://arxiv.org/pdf/2512.05564", "abs": "https://arxiv.org/abs/2512.05564", "authors": ["Zijun Wang", "Panwen Hu", "Jing Wang", "Terry Jingchen Zhang", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Zutao Jiang", "Hanhui Li", "Xiaodan Liang"], "title": "ProPhy: Progressive Physical Alignment for Dynamic World Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in video generation have shown remarkable potential for constructing world simulators. However, current models still struggle to produce physically consistent results, particularly when handling large-scale or complex dynamics. This limitation arises primarily because existing approaches respond isotropically to physical prompts and neglect the fine-grained alignment between generated content and localized physical cues. To address these challenges, we propose ProPhy, a Progressive Physical Alignment Framework that enables explicit physics-aware conditioning and anisotropic generation. ProPhy employs a two-stage Mixture-of-Physics-Experts (MoPE) mechanism for discriminative physical prior extraction, where Semantic Experts infer semantic-level physical principles from textual descriptions, and Refinement Experts capture token-level physical dynamics. This mechanism allows the model to learn fine-grained, physics-aware video representations that better reflect underlying physical laws. Furthermore, we introduce a physical alignment strategy that transfers the physical reasoning capabilities of vision-language models (VLMs) into the Refinement Experts, facilitating a more accurate representation of dynamic physical phenomena. Extensive experiments on physics-aware video generation benchmarks demonstrate that ProPhy produces more realistic, dynamic, and physically coherent results than existing state-of-the-art methods.", "AI": {"tldr": "提出ProPhy框架，通过渐进式物理对齐实现动态世界模拟，解决现有视频生成模型在物理一致性方面的不足", "motivation": "现有视频生成模型在处理大规模或复杂动态时难以产生物理一致的结果，主要因为现有方法对物理提示响应各向同性，忽视了生成内容与局部物理线索之间的细粒度对齐", "method": "提出渐进式物理对齐框架，采用两阶段物理专家混合机制：语义专家从文本描述推断语义级物理原理，精炼专家捕捉令牌级物理动态；引入物理对齐策略，将视觉语言模型的物理推理能力转移到精炼专家中", "result": "在物理感知视频生成基准测试中，ProPhy相比现有最先进方法能产生更真实、动态且物理一致的结果", "conclusion": "ProPhy框架通过显式的物理感知条件和各向异性生成，显著提升了动态世界模拟的物理一致性，为构建更可靠的世界模拟器提供了有效解决方案"}}
{"id": "2512.05557", "pdf": "https://arxiv.org/pdf/2512.05557", "abs": "https://arxiv.org/abs/2512.05557", "authors": ["Xingxi Yin", "Yicheng Li", "Gong Yan", "Chenglin Li", "Jian Zhao", "Cong Huang", "Yue Deng", "Yin Zhang"], "title": "2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sequential identity consistency under precise transient attribute control remains a long-standing challenge in controllable visual storytelling. Existing datasets lack sufficient fidelity and fail to disentangle stable identities from transient attributes, limiting structured control over pose, expression, and scene composition and thus constraining reliable sequential synthesis. To address this gap, we introduce \\textbf{2K-Characters-10K-Stories}, a multi-modal stylized narrative dataset of \\textbf{2{,}000} uniquely stylized characters appearing across \\textbf{10{,}000} illustration stories. It is the first dataset that pairs large-scale unique identities with explicit, decoupled control signals for sequential identity consistency. We introduce a \\textbf{Human-in-the-Loop pipeline (HiL)} that leverages expert-verified character templates and LLM-guided narrative planning to generate highly-aligned structured data. A \\textbf{decoupled control} scheme separates persistent identity from transient attributes -- pose and expression -- while a \\textbf{Quality-Gated loop} integrating MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing enforces pixel-level consistency. Extensive experiments demonstrate that models fine-tuned on our dataset achieves performance comparable to closed-source models in generating visual narratives.", "AI": {"tldr": "该论文提出了一个包含2000个独特风格化角色和10000个插画故事的多模态风格化叙事数据集，首次实现了大规模独特身份与解耦控制信号的配对，用于序列身份一致性生成。", "motivation": "现有数据集在精确瞬态属性控制下的序列身份一致性方面存在不足，缺乏足够的保真度，未能将稳定身份与瞬态属性解耦，限制了姿势、表情和场景构成的结构化控制，从而制约了可靠的序列合成。", "method": "提出了人类在环管道（HiL），利用专家验证的角色模板和LLM引导的叙事规划生成高度对齐的结构化数据；采用解耦控制方案分离持久身份与瞬态属性；通过质量门控循环整合MMLM评估、自动提示调优和局部图像编辑来强制像素级一致性。", "result": "实验表明，在该数据集上微调的模型在生成视觉叙事方面达到了与闭源模型相当的性能，实现了序列身份一致性和精确的瞬态属性控制。", "conclusion": "该数据集填补了现有数据集的空白，首次实现了大规模独特身份与显式解耦控制信号的配对，为可控视觉叙事提供了高质量的结构化数据支持。"}}
{"id": "2512.05556", "pdf": "https://arxiv.org/pdf/2512.05556", "abs": "https://arxiv.org/abs/2512.05556", "authors": ["Sanjeev Shrestha", "Rahul Dubey", "Hui Liu"], "title": "Improving Local Fidelity Through Sampling and Modeling Nonlinearity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.", "AI": {"tldr": "提出一种改进LIME解释方法的新技术，通过采样和建模非线性关系来提高局部保真度", "motivation": "随着黑盒机器学习模型在关键领域应用的增加，需要提供可靠的解释。LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确", "method": "使用多元自适应回归样条(MARS)建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布中采样，而不是像LIME那样重新加权样本", "result": "在三个UCI数据集上评估，与基线方法相比，该方法产生更忠实的解释，平均降低37%的均方根误差，显著提高了局部保真度", "conclusion": "提出的方法通过结合非线性建模和改进的采样技术，有效提高了局部解释的保真度，为复杂黑盒模型提供了更可靠的解释"}}
{"id": "2512.05546", "pdf": "https://arxiv.org/pdf/2512.05546", "abs": "https://arxiv.org/abs/2512.05546", "authors": ["Weijue Bu", "Guan Yuan", "Guixian Zhang"], "title": "Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 6 figures", "summary": "Large Vision-Language Models (VLMs) often exhibit text inertia, where attention drifts from visual evidence toward linguistic priors, resulting in object hallucinations. Existing decoding strategies intervene only at the output logits and thus cannot correct internal reasoning drift, while recent internal-control methods based on heuristic head suppression or global steering vectors lack principled grounding. We introduce Conscious Gaze (CG-VLM), a training-free, inference-time framework that converts game-theoretic interpretability into actionable decoding control. A Cognitive Demand Sensor built on Harsanyi interactions estimates instantaneous vision-text synergy and identifies moments when visual grounding is necessary. Conditioned on this signal, a Focused Consensus Induction module selectively reorients mid-layer attention toward visual tokens before collapse into text priors. CG-VLM achieves state-of-the-art results on POPE and CHAIR across InstructBLIP, LLaVA, Qwen-VL, and mPLUG, while preserving general capabilities, demonstrating that token-level sensing enables precise, context-aware intervention without compromising foundational knowledge.", "AI": {"tldr": "提出Conscious Gaze (CG-VLM)框架，通过游戏论解释性方法在推理时自适应调整注意力机制，缓解视觉语言模型中的幻觉问题", "motivation": "大型视觉语言模型存在文本惯性问题，注意力会从视觉证据漂移到语言先验，导致物体幻觉。现有解码策略仅在输出层干预，无法纠正内部推理漂移，而基于启发式头部抑制或全局导向向量的内部控制方法缺乏理论基础", "method": "提出训练免费、推理时框架：1) 基于Harsanyi交互的认知需求传感器估计瞬时视觉-文本协同作用，识别需要视觉基础的时刻；2) 聚焦共识诱导模块根据信号选择性地将中间层注意力重新导向视觉标记，防止其崩溃为文本先验", "result": "在POPE和CHAIR基准测试中，在InstructBLIP、LLaVA、Qwen-VL和mPLUG等模型上取得了最先进的结果，同时保持了一般能力", "conclusion": "标记级感知能够实现精确、上下文感知的干预，而不会损害基础知识，将游戏论解释性转化为可操作的解码控制"}}
{"id": "2512.05542", "pdf": "https://arxiv.org/pdf/2512.05542", "abs": "https://arxiv.org/abs/2512.05542", "authors": ["Jonathan Geuter", "Gregor Kornhardt"], "title": "RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 3 figures. 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Foundations of Reasoning in Language Models", "summary": "Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\\{m_i\\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.", "AI": {"tldr": "提出RoBoN方法，这是一种在测试时使用多个LLM进行序列化路由的Best-of-n扩展方法，通过奖励模型和一致性信号动态选择不同模型生成响应", "motivation": "传统Best-of-n方法仅依赖单一模型生成响应，但研究表明不同LLM在不同任务上具有互补优势，因此需要利用多模型多样性来提升性能", "method": "RoBoN顺序路由多个LLM生成响应，基于奖励模型评分和预测响应的一致性信号进行动态选择，无需额外训练，保持计算对等性，可与任何插件奖励模型配合使用", "result": "在多个推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）上，RoBoN在较大n值时始终优于标准Best-of-n方法，绝对准确率提升最高达3.4%，也优于均匀多模型组合基线", "conclusion": "模型间的多样性可以在推理时被利用来提升Best-of-n性能，超过任何单一组成模型，为多LLM测试时扩展提供了一种简单、无需训练的路径"}}
{"id": "2512.05539", "pdf": "https://arxiv.org/pdf/2512.05539", "abs": "https://arxiv.org/abs/2512.05539", "authors": ["Swantje Mahncke", "Malte Ott"], "title": "Ideal Observer for Segmentation of Dead Leaves Images", "categories": ["cs.CV", "math.ST", "stat.ME"], "comment": "41 pages, 16 figures", "summary": "The human visual environment is comprised of different surfaces that are distributed in space. The parts of a scene that are visible at any one time are governed by the occlusion of overlapping objects. In this work we consider \"dead leaves\" models, which replicate these occlusions when generating images by layering objects on top of each other. A dead leaves model is a generative model comprised of distributions for object position, shape, color and texture. An image is generated from a dead leaves model by sampling objects (\"leaves\") from these distributions until a stopping criterion is reached, usually when the image is fully covered or until a given number of leaves was sampled. Here, we describe a theoretical approach, based on previous work, to derive a Bayesian ideal observer for the partition of a given set of pixels based on independent dead leaves model distributions. Extending previous work, we provide step-by-step explanations for the computation of the posterior probability as well as describe factors that determine the feasibility of practically applying this computation. The dead leaves image model and the associated ideal observer can be applied to study segmentation decisions in a limited number of pixels, providing a principled upper-bound on performance, to which humans and vision algorithms could be compared.", "AI": {"tldr": "提出了一种用于分割\"枯叶图像\"的贝叶斯理想观测器理论框架，为图像分割任务提供了性能上限基准。", "motivation": "人类视觉环境由空间中分布的不同表面组成，物体之间的遮挡关系决定了场景的可见部分。枯叶模型通过分层叠加物体来模拟这种遮挡关系，需要理论方法来评估分割性能的上限。", "method": "基于先前工作，推导了贝叶斯理想观测器来计算给定像素集的后验概率，提供了逐步计算解释，并描述了实际应用可行性的决定因素。", "result": "建立了枯叶图像模型及其关联的理想观测器理论框架，可用于研究有限像素下的分割决策，为分割性能提供了原则性上限。", "conclusion": "该理想观测器为人类视觉系统和视觉算法的分割性能提供了可比较的理论上限基准，在有限像素条件下具有应用价值。"}}
{"id": "2512.05536", "pdf": "https://arxiv.org/pdf/2512.05536", "abs": "https://arxiv.org/abs/2512.05536", "authors": ["Johannes Ellemose", "Niklas Elmqvist"], "title": "Eye of the Beholder: Towards Measuring Visualization Complexity", "categories": ["cs.HC"], "comment": null, "summary": "Constructing expressive and legible visualizations is a key activity for visualization designers. While numerous design guidelines exist, research on how specific graphical features affect perceived visual complexity remains limited. In this paper, we report on a crowdsourced study to collect human ratings of perceived complexity for diverse visualizations. Using these ratings as ground truth, we then evaluated three methods to estimate this perceived complexity: image analysis metrics, multilinear regression using manually coded visualization features, and automated feature extraction using a large language model (LLM). Image complexity metrics showed no correlation with human-perceived visualization complexity. Manual feature coding produced a reasonable predictive model but required substantial effort. In contrast, a zero-shot LLM (GPT-4o mini) demonstrated strong capabilities in both rating complexity and extracting relevant features. Our findings suggest that visualization complexity is truly in the eye of the beholder, yet can be effectively approximated using zero-shot LLM prompting, offering a scalable approach for evaluating the complexity of visualizations. The dataset and code for the study and data analysis can be found at https://osf.io/w85a4/", "AI": {"tldr": "研究如何测量可视化图表的感知复杂度，通过众包收集人类评分，并评估三种估计复杂度的方法：图像分析指标、手动特征编码回归和大型语言模型自动特征提取。", "motivation": "虽然存在许多可视化设计指南，但关于特定图形特征如何影响感知视觉复杂度的研究仍然有限。需要系统研究如何测量和预测可视化图表的感知复杂度。", "method": "1) 通过众包研究收集人类对多样化可视化的复杂度评分作为基准；2) 评估三种方法：图像复杂度指标、基于手动编码特征的多线性回归、使用大型语言模型（GPT-4o mini）的零样本自动特征提取。", "result": "图像复杂度指标与人类感知的复杂度无相关性。手动特征编码能产生合理的预测模型但需要大量人工工作。零样本LLM在评分复杂度和提取相关特征方面表现出强大能力，能有效近似可视化复杂度。", "conclusion": "可视化复杂度确实\"因人而异\"，但可以通过零样本LLM提示有效近似，为评估可视化复杂度提供了可扩展的方法。LLM在自动特征提取方面表现出色，是评估可视化复杂度的有前景工具。"}}
{"id": "2512.05534", "pdf": "https://arxiv.org/pdf/2512.05534", "abs": "https://arxiv.org/abs/2512.05534", "authors": ["Yiming Tang", "Harshvardhan Saini", "Yizhen Liao", "Dianbo Liu"], "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.", "AI": {"tldr": "该论文为稀疏字典学习在机制可解释性中的理论基础研究，提出了首个统一的理论框架来分析各种SDL方法，并解释了实验观察到的现象。", "motivation": "随着AI模型在多个领域取得显著能力，理解神经网络学习什么表示以及如何处理信息变得日益重要。现有的稀疏字典学习方法（如稀疏自编码器、转码器、交叉编码器）在经验上取得了成功，但缺乏理论基础，现有理论工作仅限于带权重约束的稀疏自编码器，更广泛的SDL方法缺乏形式化基础。", "method": "提出了首个统一的SDL理论框架，将各种SDL方法视为一个统一的优化问题。展示了不同方法如何实例化该理论框架，并对优化景观进行了严格分析。设计了控制实验来验证理论结果。", "result": "首次为一些经验观察到的现象提供了理论解释，包括特征吸收、死神经元和神经元重采样技术。理论分析揭示了SDL方法的优化特性和行为模式。", "conclusion": "该工作填补了稀疏字典学习在机制可解释性中理论基础的空白，为理解神经网络如何表示和分离概念提供了形式化框架，有助于推动可解释AI的发展。"}}
{"id": "2512.05530", "pdf": "https://arxiv.org/pdf/2512.05530", "abs": "https://arxiv.org/abs/2512.05530", "authors": ["Chuang Yu", "Jinmiao Zhao", "Mingxuan Zhao", "Yunpeng Liu", "Xiujun Shu", "Yuanhao Feng", "Bo Wang", "Xiangyu Yue"], "title": "MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models", "categories": ["cs.AI"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of \"Understand -> Rethink -> Correct\", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND", "AI": {"tldr": "提出MIND推理框架，通过\"理解->重新思考->纠正\"的类人认知能力，实现从被动模仿推理到主动判别推理的范式演进", "motivation": "当前多模态大语言模型在推理任务中存在多理由语义建模有限、逻辑鲁棒性不足、易受复杂场景误导等问题", "method": "1. 理由增强与判别(RAD)范式自动扩展数据集；2. 渐进两阶段纠正学习(P2CL)策略；3. 多理由对比对齐(MCA)优化策略", "result": "在科学、常识和数学等多个公共数据集上实现了最先进的性能表现", "conclusion": "MIND框架为推进多模态大模型向更高层次认知智能提供了新视角，实现了从被动模仿到主动判别的推理范式演进"}}
{"id": "2512.05529", "pdf": "https://arxiv.org/pdf/2512.05529", "abs": "https://arxiv.org/abs/2512.05529", "authors": ["Kunyi Yang", "Qingyu Wang", "Cheng Yuan", "Yutong Ban"], "title": "See in Depth: Training-Free Surgical Scene Segmentation with Monocular Depth Priors", "categories": ["cs.CV", "cs.AI"], "comment": "The first two authors contributed equally", "summary": "Pixel-wise segmentation of laparoscopic scenes is essential for computer-assisted surgery but difficult to scale due to the high cost of dense annotations. We propose depth-guided surgical scene segmentation (DepSeg), a training-free framework that utilizes monocular depth as a geometric prior together with pretrained vision foundation models. DepSeg first estimates a relative depth map with a pretrained monocular depth estimation network and proposes depth-guided point prompts, which SAM2 converts into class-agnostic masks. Each mask is then described by a pooled pretrained visual feature and classified via template matching against a template bank built from annotated frames. On the CholecSeg8k dataset, DepSeg improves over a direct SAM2 auto segmentation baseline (35.9% vs. 14.7% mIoU) and maintains competitive performance even when using only 10--20% of the object templates. These results show that depth-guided prompting and template-based classification offer an annotation-efficient segmentation approach.", "AI": {"tldr": "提出了一种无需训练的手术场景分割框架DepSeg，利用单目深度作为几何先验，结合预训练视觉基础模型实现像素级分割", "motivation": "腹腔镜场景的像素级分割对计算机辅助手术至关重要，但由于密集标注成本高昂而难以扩展，需要开发标注高效的解决方案", "method": "使用预训练单目深度估计网络获取相对深度图，提出深度引导点提示，SAM2将其转换为类别无关掩码，通过预训练视觉特征池化和模板匹配进行分类", "result": "在CholecSeg8k数据集上，DepSeg显著优于直接SAM2自动分割基线（35.9% vs. 14.7% mIoU），即使仅使用10-20%的对象模板仍保持竞争力", "conclusion": "深度引导提示和基于模板的分类提供了一种标注高效的分割方法，展示了利用几何先验和预训练模型进行手术场景分割的有效性"}}
{"id": "2512.05528", "pdf": "https://arxiv.org/pdf/2512.05528", "abs": "https://arxiv.org/abs/2512.05528", "authors": ["Taketo Akama", "Zhuohao Zhang", "Tsukasa Nagashima", "Takagi Yutaka", "Shun Minamikawa", "Natalia Polouliakh"], "title": "Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "comment": null, "summary": "Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.", "AI": {"tldr": "使用消费级四通道EEG设备解码听众在真实音乐聆听中对音乐元素的注意力选择", "motivation": "音乐作为人类文化的重要组成部分，目前缺乏客观工具来量化听众在自然聆听体验中的注意力和感知焦点。视觉艺术已有眼动追踪研究，但听觉艺术的类似方法尚未充分发展。", "method": "使用自然主义、工作室制作的歌曲和轻量级消费级EEG设备（仅四个电极），分析真实世界音乐聆听过程中的神经响应，测试在最小化参与者负担和保持音乐体验真实性的条件下解码是否可行。", "result": "音乐注意力不仅可以在新歌曲中解码，还可以跨新受试者解码，相比现有方法在测试条件下表现出性能改进。消费级设备可以可靠地捕捉信号，音乐中的神经解码在真实世界设置中可能是可行的。", "conclusion": "这项研究为教育、个性化音乐技术和治疗干预等应用铺平了道路，证明了在生态有效的音乐聆听环境中解码选择性听觉注意力的可行性。"}}
{"id": "2512.05524", "pdf": "https://arxiv.org/pdf/2512.05524", "abs": "https://arxiv.org/abs/2512.05524", "authors": ["Chinthani Sugandhika", "Chen Li", "Deepu Rajan", "Basura Fernando"], "title": "VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation", "categories": ["cs.CV"], "comment": null, "summary": "Spatio-temporal scene graph generation (ST-SGG) aims to model objects and their evolving relationships across video frames, enabling interpretable representations for downstream reasoning tasks such as video captioning and visual question answering. Despite recent advancements in DETR-style single-stage ST-SGG models, they still suffer from several key limitations. First, while these models rely on attention-based learnable queries as a core component, these learnable queries are semantically uninformed and instance-agnostically initialized. Second, these models rely exclusively on unimodal visual features for predicate classification. To address these challenges, we propose VOST-SGG, a VLM-aided one-stage ST-SGG framework that integrates the common sense reasoning capabilities of vision-language models (VLMs) into the ST-SGG pipeline. First, we introduce the dual-source query initialization strategy that disentangles what to attend to from where to attend, enabling semantically grounded what-where reasoning. Furthermore, we propose a multi-modal feature bank that fuses visual, textual, and spatial cues derived from VLMs for improved predicate classification. Extensive experiments on the Action Genome dataset demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of integrating VLM-aided semantic priors and multi-modal features for ST-SGG. We will release the code at https://github.com/LUNAProject22/VOST.", "AI": {"tldr": "提出VOST-SGG框架，将视觉语言模型的常识推理能力集成到时空场景图生成中，解决现有单阶段模型的两个关键限制", "motivation": "现有DETR风格的单阶段ST-SGG模型存在两个主要问题：1) 可学习查询是语义无信息和实例无关初始化的；2) 仅依赖单模态视觉特征进行谓词分类。需要利用VLM的常识推理能力来改进ST-SGG", "method": "提出双源查询初始化策略，将\"关注什么\"与\"在哪里关注\"解耦，实现语义基础化的what-where推理；构建多模态特征库，融合来自VLM的视觉、文本和空间线索以改进谓词分类", "result": "在Action Genome数据集上的大量实验表明，该方法达到了最先进的性能，验证了集成VLM辅助语义先验和多模态特征对ST-SGG的有效性", "conclusion": "VOST-SGG框架成功地将VLM的常识推理能力集成到ST-SGG流程中，通过双源查询初始化和多模态特征库显著提升了时空场景图生成的性能"}}
{"id": "2512.05519", "pdf": "https://arxiv.org/pdf/2512.05519", "abs": "https://arxiv.org/abs/2512.05519", "authors": ["Bohui Shen", "Shrikar Bhatta", "Alex Ireebanije", "Zexuan Liu", "Abhinav Choudhry", "Ece Gumusel", "Kyrie Zhixuan Zhou"], "title": "User Negotiations of Authenticity, Ownership, and Governance on AI-Generated Video Platforms: Evidence from Sora", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As AI-generated video platforms rapidly advance, ethical challenges such as copyright infringement emerge. This study examines how users make sense of AI-generated videos on OpenAI's Sora by conducting a qualitative content analysis of user comments. Through a thematic analysis, we identified four dynamics that characterize how users negotiate authenticity, authorship, and platform governance on Sora. First, users acted as critical evaluators of realism, assessing micro-details such as lighting, shadows, fluid motion, and physics to judge whether AI-generated scenes could plausibly exist. Second, users increasingly shifted from passive viewers to active creators, expressing curiosity about prompts, techniques, and creative processes. Text prompts were perceived as intellectual property, generating concerns about plagiarism and remixing norms. Third, users reported blurred boundaries between real and synthetic media, worried about misinformation, and even questioned the authenticity of other commenters, suspecting bot-generated engagement. Fourth, users contested platform governance: some perceived moderation as inconsistent or opaque, while others shared tactics for evading prompt censorship through misspellings, alternative phrasing, emojis, or other languages. Despite this, many users also enforced ethical norms by discouraging the misuse of real people's images or disrespectful content. Together, these patterns highlighted how AI-mediated platforms complicate notions of reality, creativity, and rule-making in emerging digital ecosystems. Based on the findings, we discuss governance challenges in Sora and how user negotiations inform future platform governance.", "AI": {"tldr": "本研究通过分析Sora平台用户评论，探讨用户在AI生成视频平台上如何协商真实性、所有权和平台治理，揭示了AI技术如何重塑数字生态系统中的现实、创造力和规则制定概念。", "motivation": "随着AI生成视频平台的快速发展，出现了版权侵权等伦理挑战。本研究旨在理解用户如何在OpenAI的Sora平台上对AI生成视频进行意义建构，探索用户如何协商真实性、作者身份和平台治理。", "method": "采用定性内容分析方法，对Sora平台的用户评论进行主题分析，识别用户协商过程中的关键动态模式。", "result": "研究发现四个关键动态：1）用户作为现实主义的批判性评估者，评估微细节判断真实性；2）用户从被动观看者转向主动创作者，关注提示词作为知识产权；3）用户报告真实与合成媒体的界限模糊，担忧错误信息；4）用户对平台治理存在争议，同时自我执行伦理规范。", "conclusion": "AI中介平台复杂化了新兴数字生态系统中的现实、创造力和规则制定概念。用户协商揭示了平台治理面临的挑战，为未来平台治理提供了重要启示，特别是在真实性判断、知识产权保护和内容监管方面。"}}
{"id": "2512.05518", "pdf": "https://arxiv.org/pdf/2512.05518", "abs": "https://arxiv.org/abs/2512.05518", "authors": ["Jason Vega", "Gagandeep Singh"], "title": "Matching Ranks Over Probability Yields Truly Deep Safety Alignment", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "A frustratingly easy technique known as the prefilling attack has been shown to effectively circumvent the safety alignment of frontier LLMs by simply prefilling the assistant response with an affirmative prefix before decoding. In response, recent work proposed a supervised fine-tuning (SFT) defense using data augmentation to achieve a \\enquote{deep} safety alignment, allowing the model to generate natural language refusals immediately following harmful prefills. Unfortunately, we show in this work that the \"deep\" safety alignment produced by such an approach is in fact not very deep. A generalization of the prefilling attack, which we refer to as the Rank-Assisted Prefilling (RAP) attack, can effectively extract harmful content from models fine-tuned with the data augmentation defense by selecting low-probability \"harmful\" tokens from the top 20 predicted next tokens at each step (thus ignoring high-probability \"refusal\" tokens). We argue that this vulnerability is enabled due to the \"gaming\" of the SFT objective when the target distribution entropies are low, where low fine-tuning loss is achieved by shifting large probability mass to a small number of refusal tokens while neglecting the high ranks of harmful tokens. We then propose a new perspective on achieving deep safety alignment by matching the token ranks of the target distribution, rather than their probabilities. This perspective yields a surprisingly simple fix to the data augmentation defense based on regularizing the attention placed on harmful prefill tokens, an approach we call PRefill attEntion STOpping (PRESTO). Adding PRESTO yields up to a 4.7x improvement in the mean StrongREJECT score under RAP attacks across three popular open-source LLMs, with low impact to model utility.", "AI": {"tldr": "提出一种新的深度安全对齐方法，通过匹配目标分布的token排名而非概率，解决现有数据增强防御对Rank-Assisted Prefilling攻击的脆弱性", "motivation": "现有基于数据增强的SFT防御虽然声称实现了\"深度\"安全对齐，但实际上仍然容易受到Rank-Assisted Prefilling攻击，因为SFT目标在目标分布熵较低时容易被\"博弈\"，模型将大量概率质量转移到少量拒绝token而忽视有害token的高排名", "method": "提出PRESTO方法，通过正则化有害预填充token的注意力来实现深度安全对齐，核心思想是匹配目标分布的token排名而非概率", "result": "在三个流行的开源LLM上，添加PRESTO后在RAP攻击下的平均StrongREJECT分数提高了最多4.7倍，同时对模型实用性影响较小", "conclusion": "匹配token排名而非概率是实现真正深度安全对齐的有效视角，PRESTO方法能够显著提高模型对Rank-Assisted Prefilling攻击的防御能力"}}
{"id": "2512.05515", "pdf": "https://arxiv.org/pdf/2512.05515", "abs": "https://arxiv.org/abs/2512.05515", "authors": ["Yuhua Wen", "Qifei Li", "Yingying Zhou", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "title": "DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2025", "summary": "Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.", "AI": {"tldr": "提出DashFusion框架，通过双流对齐和分层瓶颈融合解决多模态情感分析中的对齐和融合问题", "motivation": "多模态情感分析面临对齐和融合两大挑战：对齐需要同步跨模态的时序和语义信息，融合需要整合对齐后的特征。现有方法往往孤立处理这两个问题，导致性能和效率受限", "method": "1. 双流对齐模块：时序对齐使用跨模态注意力建立帧级对应关系；语义对齐通过对比学习确保特征空间一致性。2. 监督对比学习利用标签信息优化模态特征。3. 分层瓶颈融合通过压缩瓶颈token逐步整合多模态信息，平衡性能与计算效率", "result": "在CMU-MOSI、CMU-MOSEI和CH-SIMS三个数据集上达到最先进性能，消融研究证实了对齐和融合技术的有效性", "conclusion": "DashFusion框架通过同时解决对齐和融合问题，在多模态情感分析中实现了更好的性能和计算效率平衡，为多模态学习提供了有效的解决方案"}}
{"id": "2512.05513", "pdf": "https://arxiv.org/pdf/2512.05513", "abs": "https://arxiv.org/abs/2512.05513", "authors": ["Chinthani Sugandhika", "Chen Li", "Deepu Rajan", "Basura Fernando"], "title": "Know-Show: Benchmarking Video-Language Models on Spatio-Temporal Grounded Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Large Video-Language Models (Video-LMs) have achieved impressive progress in multimodal understanding, yet their reasoning remains weakly grounded in space and time. We present Know-Show, a new benchmark designed to evaluate spatio-temporal grounded reasoning, the ability of a model to reason about actions and their semantics while simultaneously grounding its inferences in visual and temporal evidence. Know-Show unifies reasoning and localization within a single evaluation framework consisting of five complementary scenarios across spatial (person, object, person-object, and hand-object) and temporal dimensions. Built from Charades, Action Genome, and Ego4D with 2.5K human-authored questions, the benchmark exposes significant gaps between current Video-LMs and human reasoning. To bridge this gap, we propose GRAM, a training-free plug-in that augments Video-LMs with fine-grained grounding through attention-based video token selection and explicit timestamp encoding. Extensive experiments across open and closed Video-LMs (Qwen, VideoLLaVA, GPT-4o, and Gemini, etc.) reveal that existing models struggle to \"show what they know\" and vice versa, especially in fine-grained hand-object interactions. Know-Show establishes a unified standard for assessing grounded reasoning in video-language understanding and provides insights toward developing interpretable and reliable multimodal reasoning systems. We will release the code at https://github.com/LUNAProject22/Know-Show.", "AI": {"tldr": "提出了Know-Show基准测试，用于评估视频语言模型在时空基础推理方面的能力，即模型在推理动作及其语义的同时，将其推断基于视觉和时间证据的能力。", "motivation": "当前大型视频语言模型在多模态理解方面取得了显著进展，但其推理在空间和时间上仍然缺乏充分的基础。需要一个新的基准测试来评估模型在时空基础推理方面的能力，揭示现有模型的局限性。", "method": "1. 构建Know-Show基准测试，包含五个互补场景（空间维度：人物、物体、人物-物体、手-物体；时间维度）；2. 基于Charades、Action Genome和Ego4D数据集，包含2.5K个人工编写的问题；3. 提出GRAM方法，通过基于注意力的视频令牌选择和显式时间戳编码来增强视频语言模型的细粒度基础能力。", "result": "1. 现有视频语言模型（Qwen、VideoLLaVA、GPT-4o、Gemini等）在\"展示所知\"和\"知道所展示\"方面存在显著差距；2. 特别是在细粒度的手-物体交互任务中表现不佳；3. GRAM方法有效提升了模型的时空基础推理能力。", "conclusion": "Know-Show为评估视频语言理解中的基础推理建立了统一标准，揭示了现有模型在时空基础推理方面的局限性，并为开发可解释和可靠的多模态推理系统提供了见解。GRAM方法展示了通过增强细粒度基础能力来改进视频语言模型的有效途径。"}}
{"id": "2512.05511", "pdf": "https://arxiv.org/pdf/2512.05511", "abs": "https://arxiv.org/abs/2512.05511", "authors": ["Chuang Yu", "Jinmiao Zhao", "Yunpeng Liu", "Yaokun Li", "Xiujun Shu", "Yuanhao Feng", "Bo Wang", "Yimian Dai", "Xiangyu Yue"], "title": "Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm", "categories": ["cs.CV"], "comment": null, "summary": "While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework", "AI": {"tldr": "提出一种基于视觉基础模型的红外小目标检测新范式FDEP，通过语义对齐调制融合和协作优化隐式自蒸馏，在保持推理效率的同时显著提升检测精度", "motivation": "大规模视觉基础模型在多种视觉领域展现强大泛化能力，但其在单帧红外小目标检测任务中的潜力尚未充分探索。现有方法存在精度不足和评估体系碎片化问题", "method": "提出FDEP框架：1) 语义对齐调制融合模块实现基础模型全局语义先验与任务特征的动态对齐和深度融合；2) 协作优化隐式自蒸馏策略通过参数共享和同步反向传播实现隐式语义迁移；3) 构建HSE评估指标进行多阈值积分评估", "result": "在多个公开数据集上实现SOTA性能，显著提升现有编码器-解码器方法的精度，且不增加推理开销。HSE指标为公平模型比较提供稳定全面的评估基础", "conclusion": "首次系统性地将冻结视觉基础模型表示引入红外小目标检测任务，提出的FDEP框架能够无缝适配现有方法，在保持效率的同时显著提升检测性能，为红外小目标检测提供了新的高效范式"}}
{"id": "2512.05508", "pdf": "https://arxiv.org/pdf/2512.05508", "abs": "https://arxiv.org/abs/2512.05508", "authors": ["Yash Choudhary", "Preeti Rao", "Pushpak Bhattacharyya"], "title": "Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.", "AI": {"tldr": "该论文提出了一种利用歌词嵌入来预测音乐流行度的多模态方法，通过LLM提取歌词的高维语义特征，并结合音频特征和社交元数据进行流行度预测。", "motivation": "现有研究主要关注音频特征、社交元数据或模型架构，而歌词在预测音乐流行度中的作用被低估。歌词包含丰富的语义、句法和序列信息，对音乐流行度有重要影响。", "method": "开发了一个自动化流水线，使用LLM提取高维歌词嵌入（LyricsAENet），然后将其集成到HitMusicLyricNet多模态架构中，结合音频特征、歌词嵌入和社交元数据来预测0-100范围内的流行度分数。", "result": "在包含超过10万首曲目的SpotGenTrack数据集上，该方法优于现有基线，在MAE和MSE上分别实现了9%和20%的改进。消融实验证实了LLM驱动的歌词特征流水线（LyricsAENet）对性能提升的贡献。", "conclusion": "歌词在音乐流行度预测中具有重要价值，通过LLM提取的密集歌词表示能够显著提升预测性能，为音乐产业提供了新的技术洞察。"}}
{"id": "2512.05506", "pdf": "https://arxiv.org/pdf/2512.05506", "abs": "https://arxiv.org/abs/2512.05506", "authors": ["Junho Myung", "Hyunseung Lim", "Hana Oh", "Hyoungwook Jin", "Nayeon Kang", "So-Yeon Ahn", "Hwajung Hong", "Alice Oh", "Juho Kim"], "title": "When Scaffolding Breaks: Investigating Student Interaction with LLM-Based Writing Support in Real-Time K-12 EFL Classrooms", "categories": ["cs.HC"], "comment": "Under Review", "summary": "Large language models (LLMs) are promising tools for scaffolding students' English writing skills, but their effectiveness in real-time K-12 classrooms remains underexplored. Addressing this gap, our study examines the benefits and limitations of using LLMs as real-time learning support, considering how classroom constraints, such as diverse proficiency levels and limited time, affect their effectiveness. We conducted a deployment study with 157 eighth-grade students in a South Korean middle school English class over six weeks. Our findings reveal that while scaffolding improved students' ability to compose grammatically correct sentences, this step-by-step approach demotivated lower-proficiency students and increased their system reliance. We also observed challenges to classroom dynamics, where extroverted students often dominated the teacher's attention, and the system's assistance made it difficult for teachers to identify struggling students. Based on these findings, we discuss design guidelines for integrating LLMs into real-time writing classes as inclusive educational tools.", "AI": {"tldr": "研究在韩国中学EFL课堂中实时使用LLM进行写作支持的部署研究，分析其对学生写作能力、动机和课堂动态的影响", "motivation": "大型语言模型(LLMs)作为学生英语写作能力脚手架工具具有潜力，但在实时K-12课堂中的有效性尚未充分探索，需要了解课堂约束（如多样化的熟练水平和有限时间）如何影响其效果", "method": "在韩国一所中学八年级英语课堂进行为期六周的部署研究，涉及157名学生，观察LLM作为实时学习支持工具的使用情况", "result": "脚手架方法提高了学生撰写语法正确句子的能力，但逐步指导方式降低了低水平学生的动机并增加了系统依赖；观察到课堂动态挑战，外向学生主导教师注意力，系统辅助使教师难以识别困难学生", "conclusion": "基于研究发现，讨论了将LLM整合到实时写作课堂作为包容性教育工具的设计指南，强调需要平衡技术支持与教学需求"}}
{"id": "2512.05495", "pdf": "https://arxiv.org/pdf/2512.05495", "abs": "https://arxiv.org/abs/2512.05495", "authors": ["Ratnangshu Das", "Ahan Basu", "Christos Verginis", "Pushpak Jagtap"], "title": "Spatiotemporal Tubes for Differential Drive Robots with Model Uncertainty", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents a Spatiotemporal Tube (STT)-based control framework for differential-drive mobile robots with dynamic uncertainties and external disturbances, guaranteeing the satisfaction of Temporal Reach-Avoid-Stay (T-RAS) specifications. The approach employs circular STT, characterized by smoothly time-varying center and radius, to define dynamic safe corridors that guide the robot from the start region to the goal while avoiding obstacles. In particular, we first develop a sampling-based synthesis algorithm to construct a feasible STT that satisfies the prescribed timing and safety constraints with formal guarantees. To ensure that the robot remains confined within this tube, we then design analytically a closed-form, approximation-free control law. The resulting controller is computationally efficient, robust to disturbances and {model uncertainties}, and requires no model approximations or online optimization. The proposed framework is validated through simulation studies on a differential-drive robot and benchmarked against state-of-the-art methods, demonstrating superior robustness, accuracy, and computational efficiency.", "AI": {"tldr": "提出一种基于时空管道的控制框架，用于具有动态不确定性和外部干扰的差速驱动机器人，保证满足时间可达-避障-停留规范", "motivation": "差速驱动机器人在动态不确定性和外部干扰下难以保证满足复杂时空约束的安全控制，现有方法存在计算效率低、鲁棒性不足或需要模型近似的问题", "method": "采用圆形时空管道定义动态安全走廊，开发采样综合算法构建满足时空约束的管道，设计闭式无近似控制律使机器人保持在管道内", "result": "提出的控制器计算高效、对干扰和模型不确定性鲁棒，无需模型近似或在线优化，仿真验证显示在鲁棒性、精度和计算效率方面优于现有方法", "conclusion": "该框架为具有不确定性的差速驱动机器人提供了一种形式化保证的时空约束满足控制方案，具有实际应用价值"}}
{"id": "2512.05494", "pdf": "https://arxiv.org/pdf/2512.05494", "abs": "https://arxiv.org/abs/2512.05494", "authors": ["Fan Zhang", "Zhiwei Gu", "Hua Wang"], "title": "Decoding with Structured Awareness: Integrating Directional, Frequency-Spatial, and Structural Attention for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "To address the limitations of Transformer decoders in capturing edge details, recognizing local textures and modeling spatial continuity, this paper proposes a novel decoder framework specifically designed for medical image segmentation, comprising three core modules. First, the Adaptive Cross-Fusion Attention (ACFA) module integrates channel feature enhancement with spatial attention mechanisms and introduces learnable guidance in three directions (planar, horizontal, and vertical) to enhance responsiveness to key regions and structural orientations. Second, the Triple Feature Fusion Attention (TFFA) module fuses features from Spatial, Fourier and Wavelet domains, achieving joint frequency-spatial representation that strengthens global dependency and structural modeling while preserving local information such as edges and textures, making it particularly effective in complex and blurred boundary scenarios. Finally, the Structural-aware Multi-scale Masking Module (SMMM) optimizes the skip connections between encoder and decoder by leveraging multi-scale context and structural saliency filtering, effectively reducing feature redundancy and improving semantic interaction quality. Working synergistically, these modules not only address the shortcomings of traditional decoders but also significantly enhance performance in high-precision tasks such as tumor segmentation and organ boundary extraction, improving both segmentation accuracy and model generalization. Experimental results demonstrate that this framework provides an efficient and practical solution for medical image segmentation.", "AI": {"tldr": "提出一种用于医学图像分割的新型解码器框架，通过三个核心模块解决Transformer解码器在边缘细节、局部纹理和空间连续性建模方面的局限性。", "motivation": "传统Transformer解码器在医学图像分割中存在局限性：难以捕捉边缘细节、识别局部纹理以及建模空间连续性，特别是在复杂和模糊边界场景下表现不佳。", "method": "提出三个核心模块：1) 自适应交叉融合注意力(ACFA)模块，集成通道特征增强与空间注意力机制，引入三个方向的可学习指导；2) 三重特征融合注意力(TFFA)模块，融合空间、傅里叶和小波域特征，实现联合频率-空间表示；3) 结构感知多尺度掩码模块(SMMM)，利用多尺度上下文和结构显著性过滤优化编码器-解码器跳跃连接。", "result": "实验结果表明该框架显著提升了医学图像分割性能，特别是在肿瘤分割和器官边界提取等高精度任务中，提高了分割准确性和模型泛化能力。", "conclusion": "该框架通过三个协同工作的模块有效解决了传统解码器的不足，为医学图像分割提供了一个高效实用的解决方案，特别适用于复杂和模糊边界场景。"}}
{"id": "2512.05492", "pdf": "https://arxiv.org/pdf/2512.05492", "abs": "https://arxiv.org/abs/2512.05492", "authors": ["Qi Zhu", "Jingyi Zhang", "Naishan Zheng", "Wei Yu", "Jinghao Zhang", "Deyi Ji", "Feng Zhao"], "title": "WaterWave: Bridging Underwater Image Enhancement into Video Streams via Wavelet-based Temporal Consistency Field", "categories": ["cs.CV"], "comment": null, "summary": "Underwater video pairs are fairly difficult to obtain due to the complex underwater imaging. In this case, most existing video underwater enhancement methods are performed by directly applying the single-image enhancement model frame by frame, but a natural issue is lacking temporal consistency. To relieve the problem, we rethink the temporal manifold inherent in natural videos and observe a temporal consistency prior in dynamic scenes from the local temporal frequency perspective. Building upon the specific prior and no paired-data condition, we propose an implicit representation manner for enhanced video signals, which is conducted in the wavelet-based temporal consistency field, WaterWave. Specifically, under the constraints of the prior, we progressively filter and attenuate the inconsistent components while preserving motion details and scenes, achieving a natural-flowing video. Furthermore, to represent temporal frequency bands more accurately, an underwater flow correction module is designed to rectify estimated flows considering the transmission in underwater scenes. Extensive experiments demonstrate that WaterWave significantly enhances the quality of videos generated using single-image underwater enhancements. Additionally, our method demonstrates high potential in downstream underwater tracking tasks, such as UOSTrack and MAT, outperforming the original video by a large margin, i.e., 19.7% and 9.7% on precise respectively.", "AI": {"tldr": "提出WaterWave方法，通过小波变换构建时间一致性场，将单帧水下图像增强技术扩展到视频流中，解决帧间时间一致性问题", "motivation": "水下视频数据对难以获取，现有方法通常将单帧增强模型逐帧应用于视频，导致缺乏时间一致性，产生不自然的闪烁效果", "method": "基于小波变换的时间一致性场隐式表示方法，包含水下流校正模块，通过渐进式滤波衰减不一致成分，同时保留运动细节和场景内容", "result": "显著提升单帧水下增强方法生成的视频质量，在下游水下跟踪任务中表现优异，在UOSTrack和MAT上分别提升19.7%和9.7%的精确度", "conclusion": "WaterWave方法有效解决了水下视频增强的时间一致性问题，无需配对数据，通过小波变换的时间一致性场实现了自然流畅的水下视频增强"}}
{"id": "2512.05482", "pdf": "https://arxiv.org/pdf/2512.05482", "abs": "https://arxiv.org/abs/2512.05482", "authors": ["Mai Tsujimoto"], "title": "Concept-based Explainable Data Mining with VLM for 3D Detection", "categories": ["cs.CV"], "comment": "28 pages including appendix. Code: https://github.com/mm1129/concept_based_rare_detector_2025", "summary": "Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.", "AI": {"tldr": "提出一种利用视觉语言模型进行概念驱动的可解释数据挖掘框架，用于提升自动驾驶中3D目标检测性能，特别是针对罕见物体的检测。", "motivation": "自动驾驶系统中罕见物体检测是一个挑战性问题，仅依赖点云数据效果有限。视觉语言模型在图像理解方面表现出色，但尚未被充分探索用于通过智能数据挖掘来增强3D目标检测。", "method": "提出跨模态框架，利用2D VLM从驾驶场景中识别和挖掘罕见物体。方法综合了目标检测、语义特征提取、降维和多方面异常检测技术，形成可解释的流程。结合孤立森林和t-SNE异常检测方法以及基于概念的过滤，有效识别语义上有意义的罕见物体。", "result": "在nuScenes数据集上的实验表明，该概念引导的数据挖掘策略显著提升了3D目标检测模型的性能，同时仅使用少量训练数据。特别是对于拖车和自行车等挑战性物体类别，相比使用相同数量的随机数据有显著改进。", "conclusion": "该方法能够有效提取和标注目标罕见物体概念（如工程车辆、摩托车、障碍物），大幅减少标注负担并专注于最有价值的训练样本。这对安全关键自动驾驶系统中数据集的效率管理具有重要意义。"}}
{"id": "2512.05481", "pdf": "https://arxiv.org/pdf/2512.05481", "abs": "https://arxiv.org/abs/2512.05481", "authors": ["Jialin Li", "Yiwei Ren", "Kai Pan", "Dong Wei", "Pujin Cheng", "Xian Wu", "Xiaoying Tang"], "title": "UniFS: Unified Multi-Contrast MRI Reconstruction via Frequency-Spatial Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, Multi-Contrast MR Reconstruction (MCMR) has emerged as a hot research topic that leverages high-quality auxiliary modalities to reconstruct undersampled target modalities of interest. However, existing methods often struggle to generalize across different k-space undersampling patterns, requiring the training of a separate model for each specific pattern, which limits their practical applicability. To address this challenge, we propose UniFS, a Unified Frequency-Spatial Fusion model designed to handle multiple k-space undersampling patterns for MCMR tasks without any need for retraining. UniFS integrates three key modules: a Cross-Modal Frequency Fusion module, an Adaptive Mask-Based Prompt Learning module, and a Dual-Branch Complementary Refinement module. These modules work together to extract domain-invariant features from diverse k-space undersampling patterns while dynamically adapt to their own variations. Another limitation of existing MCMR methods is their tendency to focus solely on spatial information while neglect frequency characteristics, or extract only shallow frequency features, thus failing to fully leverage complementary cross-modal frequency information. To relieve this issue, UniFS introduces an adaptive prompt-guided frequency fusion module for k-space learning, significantly enhancing the model's generalization performance. We evaluate our model on the BraTS and HCP datasets with various k-space undersampling patterns and acceleration factors, including previously unseen patterns, to comprehensively assess UniFS's generalizability. Experimental results across multiple scenarios demonstrate that UniFS achieves state-of-the-art performance. Our code is available at https://github.com/LIKP0/UniFS.", "AI": {"tldr": "提出UniFS模型，一种统一的多对比度MRI重建方法，通过频率-空间融合处理多种k空间欠采样模式，无需针对不同采样模式重新训练。", "motivation": "现有MCMR方法存在两个主要问题：1) 无法泛化到不同的k空间欠采样模式，需要为每种模式单独训练模型；2) 要么只关注空间信息忽略频率特征，要么只提取浅层频率特征，未能充分利用跨模态频率信息。", "method": "UniFS包含三个核心模块：跨模态频率融合模块、基于自适应掩码的提示学习模块、双分支互补细化模块。这些模块协同工作，从多样化的k空间欠采样模式中提取域不变特征，同时动态适应各自的变化。", "result": "在BraTS和HCP数据集上，针对多种k空间欠采样模式和加速因子（包括未见过的模式）进行评估，实验结果表明UniFS在多个场景下达到了最先进的性能。", "conclusion": "UniFS通过统一的频率-空间融合框架，成功解决了MCMR任务中不同k空间欠采样模式的泛化问题，无需重新训练即可处理多种采样模式，显著提升了模型的实用性和泛化能力。"}}
{"id": "2512.05478", "pdf": "https://arxiv.org/pdf/2512.05478", "abs": "https://arxiv.org/abs/2512.05478", "authors": ["Jingyuan Yang", "Zihuan Bai", "Hui Huang"], "title": "EmoStyle: Emotion-Driven Image Stylization", "categories": ["cs.CV"], "comment": null, "summary": "Art has long been a profound medium for expressing emotions. While existing image stylization methods effectively transform visual appearance, they often overlook the emotional impact carried by styles. To bridge this gap, we introduce Affective Image Stylization (AIS), a task that applies artistic styles to evoke specific emotions while preserving content. We present EmoStyle, a framework designed to address key challenges in AIS, including the lack of training data and the emotion-style mapping. First, we construct EmoStyleSet, a content-emotion-stylized image triplet dataset derived from ArtEmis to support AIS. We then propose an Emotion-Content Reasoner that adaptively integrates emotional cues with content to learn coherent style queries. Given the discrete nature of artistic styles, we further develop a Style Quantizer that converts continuous style features into emotion-related codebook entries. Extensive qualitative and quantitative evaluations, including user studies, demonstrate that EmoStyle enhances emotional expressiveness while maintaining content consistency. Moreover, the learned emotion-aware style dictionary is adaptable to other generative tasks, highlighting its potential for broader applications. Our work establishes a foundation for emotion-driven image stylization, expanding the creative potential of AI-generated art.", "AI": {"tldr": "EmoStyle是一个情感驱动的图像风格化框架，旨在将艺术风格应用于图像以唤起特定情感，同时保持内容一致性。", "motivation": "现有图像风格化方法主要关注视觉外观转换，但忽视了风格所携带的情感影响。艺术长期以来一直是表达情感的深刻媒介，因此需要填补这一空白。", "method": "1) 构建EmoStyleSet数据集（从ArtEmis衍生的内容-情感-风格化图像三元组）；2) 提出情感-内容推理器，自适应整合情感线索与内容以学习连贯的风格查询；3) 开发风格量化器，将连续风格特征转换为情感相关的码本条目。", "result": "通过定性和定量评估（包括用户研究）表明，EmoStyle在保持内容一致性的同时增强了情感表达力。学习到的情感感知风格字典可适应其他生成任务。", "conclusion": "该工作为情感驱动的图像风格化奠定了基础，扩展了AI生成艺术的创作潜力，并展示了情感感知风格字典在更广泛应用中的潜力。"}}
{"id": "2512.05475", "pdf": "https://arxiv.org/pdf/2512.05475", "abs": "https://arxiv.org/abs/2512.05475", "authors": ["Saumya Biswas", "Jiten Oswal"], "title": "PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": "22 pages, 9 figures, 4 tables", "summary": "In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.", "AI": {"tldr": "比较不同对称性等变量子机器学习模型在分子几何结构学习中的性能，探索图嵌入和置换对称性对模型泛化能力的影响", "motivation": "研究在分子几何结构学习中，不同对称性等变量子机器学习模型的性能差异，特别是置换对称性和图嵌入如何影响模型的准确性和泛化能力，为几何数据集选择合适的模型提供标准", "method": "使用两种分子数据集（线性LiH分子和三角锥形NH3分子），比较四种模型：无对称性等变的量子模型、旋转等变模型、置换等变模型、以及图嵌入置换等变模型，以经典等变模型作为性能基准", "result": "图嵌入特征被证明是提高几何数据集可训练性的有效途径，置换对称嵌入被发现是几何学习中最具泛化能力的量子机器学习模型，模型性能差异与分子几何结构相关", "conclusion": "置换对称嵌入模型在分子几何学习中表现出最佳泛化性能，图嵌入是提升几何数据集训练效果的有效方法，研究结果为选择适合特定分子几何结构的量子机器学习模型提供了标准"}}
{"id": "2512.05472", "pdf": "https://arxiv.org/pdf/2512.05472", "abs": "https://arxiv.org/abs/2512.05472", "authors": ["Yiting Dong", "Zhaofei Yu", "Jianhao Ding", "Zijie Xu", "Tiejun Huang"], "title": "Unleashing Temporal Capacity of Spiking Neural Networks through Spatiotemporal Separation", "categories": ["cs.NE"], "comment": null, "summary": "Spiking Neural Networks (SNNs) are considered naturally suited for temporal processing, with membrane potential propagation widely regarded as the core temporal modeling mechanism. However, existing research lack analysis of its actual contributions in complex temporal tasks. We design Non-Stateful (NS) models progressively removing membrane propagation to quantify its stage-wise role. Experiments reveal a counterintuitive phenomenon: moderate removal in shallow or deep layers improves performance, while excessive removal causes collapse. We attribute this to spatio-temporal resource competition where neurons encode both semantics and dynamics within limited range, with temporal state consuming capacity for spatial learning. Based on this, we propose Spatial-Temporal Separable Network (STSep), decoupling residual blocks into independent spatial and temporal branches. The spatial branch focuses on semantic extraction while the temporal branch captures motion through explicit temporal differences. Experiments on Something-Something V2, UCF101, and HMDB51 show STSep achieves superior performance, with retrieval task and attention analysis confirming focus on motion rather than static appearance. This work provides new perspectives on SNNs' temporal mechanisms and an effective solution for spatiotemporal modeling in video understanding.", "AI": {"tldr": "该论文研究了脉冲神经网络（SNNs）在时间处理中的实际能力，通过时空分离的方法提升SNNs在复杂时间任务中的性能。", "motivation": "虽然SNNs被认为天然适合时间处理，膜电位传播被广泛视为核心时间建模机制，但现有研究缺乏对其在复杂时间任务中实际贡献的分析。作者发现时空资源竞争问题，即神经元在有限范围内同时编码语义和动态信息，时间状态消耗了空间学习的能力。", "method": "设计了无状态（NS）模型逐步移除膜电位传播以量化其阶段性作用，并提出了时空可分离网络（STSep），将残差块解耦为独立的空间和时间分支。空间分支专注于语义提取，时间分支通过显式时间差异捕捉运动信息。", "result": "实验发现反直觉现象：在浅层或深层适度移除膜电位传播能提高性能，而过度移除会导致崩溃。在Something-Something V2、UCF101和HMDB51数据集上，STSep实现了优越性能，检索任务和注意力分析证实了模型更关注运动而非静态外观。", "conclusion": "这项工作为SNNs的时间机制提供了新视角，并为视频理解中的时空建模提供了有效解决方案，通过时空分离解决了资源竞争问题，使模型能更好地专注于运动信息提取。"}}
{"id": "2512.05469", "pdf": "https://arxiv.org/pdf/2512.05469", "abs": "https://arxiv.org/abs/2512.05469", "authors": ["Zubair Ahmed Mohammad"], "title": "How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 9 figures, 3 tables. Code and reproducible experiments are available at: https://github.com/zubair0831/ensemble-generalization-gap", "summary": "Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.", "AI": {"tldr": "该研究从偏差-方差角度探讨集成学习如何在表格数据上平衡准确率和过拟合问题，通过四个分类任务比较了线性模型、单决策树和九种集成方法的表现。", "motivation": "集成模型通常比单一学习器获得更高准确率，但其保持较小泛化差距的能力尚未被充分理解。研究旨在揭示集成方法如何在不同类型表格数据上平衡准确率和过拟合。", "method": "使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单决策树和九种集成方法，并计算数据集复杂度指标如线性度得分、Fisher比率和噪声估计。", "result": "在近线性和干净数据上，线性模型已能很好泛化，集成方法提供额外收益有限；在具有显著非线性结构的数据集上，基于树的集成方法将测试准确率提高5-7个百分点，同时保持泛化差距低于3%；在噪声大或高度不平衡数据集上，集成方法仍具竞争力但需要正则化以避免过拟合。", "conclusion": "集成学习通过平均或受控提升减少方差，能在保持高准确率的同时控制过拟合。数据集复杂度指标可有效预测集成方法何时能有效控制方差，为实际表格应用中的模型选择提供实用指导。"}}
{"id": "2512.05468", "pdf": "https://arxiv.org/pdf/2512.05468", "abs": "https://arxiv.org/abs/2512.05468", "authors": ["Takara Taniguchi", "Yudai Ueda", "Atsuya Muramatsu", "Kohki Hashimoto", "Ryo Yagi", "Hideya Ochiai", "Chaodit Aswakul"], "title": "University Building Recognition Dataset in Thailand for the mission-oriented IoT sensor system", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Many industrial sectors have been using of machine learning at inference mode on edge devices. Future directions show that training on edge devices is promising due to improvements in semiconductor performance. Wireless Ad Hoc Federated Learning (WAFL) has been proposed as a promising approach for collaborative learning with device-to-device communication among edges. In particular, WAFL with Vision Transformer (WAFL-ViT) has been tested on image recognition tasks with the UTokyo Building Recognition Dataset (UTBR). Since WAFL-ViT is a mission-oriented sensor system, it is essential to construct specific datasets by each mission. In our work, we have developed the Chulalongkorn University Building Recognition Dataset (CUBR), which is specialized for Chulalongkorn University as a case study in Thailand. Additionally, our results also demonstrate that training on WAFL scenarios achieves better accuracy than self-training scenarios. Dataset is available in https://github.com/jo2lxq/wafl/.", "AI": {"tldr": "开发泰国朱拉隆功大学建筑识别数据集（CUBR），用于面向任务的物联网传感器系统中的无线自组织联邦学习（WAFL-ViT）", "motivation": "随着边缘设备性能提升，边缘训练成为未来方向。无线自组织联邦学习（WAFL）通过设备间通信实现协作学习，WAFL-ViT已在图像识别任务中测试，但作为面向任务的传感器系统，需要为每个任务构建特定数据集。需要为泰国朱拉隆功大学开发专门的建筑识别数据集。", "method": "开发了朱拉隆功大学建筑识别数据集（CUBR），作为泰国案例研究。该数据集专门针对朱拉隆功大学建筑识别任务设计，可用于WAFL-ViT系统的训练和评估。", "result": "成功构建了CUBR数据集，并在WAFL场景下进行测试。结果显示，在WAFL场景下的训练比自训练场景获得更高的准确率。数据集已在GitHub上公开可用。", "conclusion": "CUBR数据集为面向任务的物联网传感器系统提供了专门的泰国大学建筑识别数据集，支持WAFL-ViT系统的应用。实验证明WAFL场景下的协作学习优于单独的自训练，为边缘设备上的联邦学习提供了实际案例支持。"}}
{"id": "2512.05464", "pdf": "https://arxiv.org/pdf/2512.05464", "abs": "https://arxiv.org/abs/2512.05464", "authors": ["Panatchakorn Anantaprayoon", "Nataliia Babina", "Jad Tarifi", "Nima Asgharbeygi"], "title": "Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures, to appear in AAAI 2026 AIGOV Workshop", "summary": "Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.", "AI": {"tldr": "提出了一种名为\"动态对齐\"的可扩展自改进对齐框架，旨在超越传统的对齐规范，引入\"集体智能体\"作为统一开放的对齐价值，鼓励整合的智能体能力。", "motivation": "随着AI系统向AGI和ASI发展，传统基于人类偏好或预定义原则（如帮助性、诚实性、无害性）的对齐方法变得不足。人类反馈对齐资源密集且难以扩展，而现有的AI反馈自改进对齐方法大多局限于传统对齐价值。", "method": "提出了动态对齐框架，包含两个关键组件：1) 使用LLM自动生成训练数据集；2) 自奖励机制，其中策略模型评估自己的输出候选并为基于GRPO的学习分配奖励。", "result": "实验结果表明，该方法成功地将模型对齐到集体智能体价值，同时保留了通用的NLP能力。", "conclusion": "动态对齐框架为超越传统对齐规范提供了可扩展的自改进方法，通过集体智能体这一开放式的对齐价值，为AGI/ASI时代的对齐问题提供了新的解决方案。"}}
{"id": "2512.05461", "pdf": "https://arxiv.org/pdf/2512.05461", "abs": "https://arxiv.org/abs/2512.05461", "authors": ["Bolun Zhang", "Linzhuo Li", "Yunqi Chen", "Qinlin Zhao", "Zihan Zhu", "Xiaoyuan Yi", "Xing Xie"], "title": "Knowing Your Uncertainty -- On the application of LLM in social sciences", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "49 pages, 10 figures", "summary": "Large language models (LLMs) are rapidly being integrated into computational social science research, yet their blackboxed training and designed stochastic elements in inference pose unique challenges for scientific inquiry. This article argues that applying LLMs to social scientific tasks requires explicit assessment of uncertainty-an expectation long established in both quantitative methodology in the social sciences and machine learning. We introduce a unified framework for evaluating LLM uncertainty along two dimensions: the task type (T), which distinguishes between classification, short-form, and long-form generation, and the validation type (V), which captures the availability of reference data or evaluative criteria. Drawing from both computer science and social science literature, we map existing uncertainty quantification (UQ) methods to this T-V typology and offer practical recommendations for researchers. Our framework provides both a methodological safeguard and a practical guide for integrating LLMs into rigorous social science research.", "AI": {"tldr": "本文提出了一个评估大语言模型在社会科学研究中不确定性的统一框架，强调将LLMs应用于社会科学任务时需要明确评估不确定性。", "motivation": "大语言模型正快速融入计算社会科学研究，但其黑盒训练和推理中的随机性设计给科学研究带来了独特挑战。社会科学中的定量方法和机器学习都长期要求明确评估不确定性，而当前LLMs在社会科学应用中对不确定性的评估不足。", "method": "提出了一个评估LLM不确定性的统一框架，包含两个维度：任务类型（分类、短文本生成、长文本生成）和验证类型（参考数据或评估标准的可用性）。基于计算机科学和社会科学文献，将现有的不确定性量化方法映射到这个T-V类型学中。", "result": "建立了一个系统化的框架来评估LLM在社会科学应用中的不确定性，为研究人员提供了实用的建议和方法论保障。", "conclusion": "该框架既为将LLMs整合到严谨的社会科学研究中提供了方法论保障，也提供了实用指南，强调了在社会科学任务中应用LLMs时需要明确评估不确定性的重要性。"}}
{"id": "2512.05460", "pdf": "https://arxiv.org/pdf/2512.05460", "abs": "https://arxiv.org/abs/2512.05460", "authors": ["Dehong Zheng", "Zhongzhi Zhang"], "title": "ProbeWalk: Fast Estimation of Biharmonic Distance on Graphs via Probe-Driven Random Walks", "categories": ["cs.SI", "cs.DS"], "comment": null, "summary": "The biharmonic distance is a fundamental metric on graphs that measures the dissimilarity between two nodes, capturing both local and global structures. It has found applications across various fields, including network centrality, graph clustering, and machine learning. These applications typically require efficient evaluation of pairwise biharmonic distances. However, existing algorithms remain computationally expensive. The state-of-the-art method attains an absolute-error guarantee epsilon_abs with time complexity O(L^5 / epsilon_abs^2), where L denotes the truncation length. In this work, we improve the complexity to O(L^3 / epsilon^2) under a relative-error guarantee epsilon via probe-driven random walks. We provide a relative-error guarantee rather than an absolute-error guarantee because biharmonic distances vary by orders of magnitude across node pairs. Since L is often very large in real-world networks (for example, L >= 10^3), reducing the L-dependence from the fifth to the third power yields substantial gains. Extensive experiments on real-world networks show that our method delivers 10x-1000x per-query speedups at matched relative error over strong baselines and scales to graphs with tens of millions of nodes.", "AI": {"tldr": "提出了一种名为ProbeWalk的新方法，通过探针驱动的随机游走来快速估计图上的双调和距离，显著提高了计算效率。", "motivation": "双调和距离是图上的基本度量，在多个领域有重要应用，但现有算法计算成本高昂。当前最优方法的时间复杂度为O(L^5/ε_abs^2)，其中L是截断长度，在现实网络中L通常很大（≥10^3），导致计算效率低下。", "method": "开发了基于探针驱动随机游走的新算法，通过相对误差保证而非绝对误差保证来适应双调和距离在不同节点对间数量级差异大的特点。", "result": "将时间复杂度从O(L^5/ε_abs^2)降低到O(L^3/ε^2)，在真实网络上的实验显示，在相同相对误差下比基线方法快10-1000倍，能够扩展到数千万节点的图。", "conclusion": "ProbeWalk方法通过探针驱动随机游走显著提高了双调和距离的计算效率，解决了现有方法计算成本高的问题，为大规模图分析提供了实用工具。"}}
{"id": "2512.05453", "pdf": "https://arxiv.org/pdf/2512.05453", "abs": "https://arxiv.org/abs/2512.05453", "authors": ["Luc Moreau", "Alfred Rossi", "Sophie Stalla-Bourdillon"], "title": "Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments", "categories": ["cs.DB", "cs.AI", "cs.CY", "cs.LO"], "comment": "17 pages, 8 figures. Code and examples available at https://github.com/alfredr/parajudica", "summary": "Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.", "AI": {"tldr": "开发一个基于RDF/SPARQL的规则系统，用于在多框架环境下评估上下文相关的数据合规状态", "motivation": "解决在多个同时适用的合规框架下实施基于策略的数据访问控制（PBAC）的挑战", "method": "提出Parajudica——一个开放、模块化、可扩展的RDF/SPARQL规则系统，并建立了相应的元模型", "result": "通过应用于现有法律框架和行业标准，展示了该资源和元模型的实用性，为比较框架分析提供了见解", "conclusion": "Parajudica系统可用于合规策略执行、合规监控、数据发现和风险评估等多种应用场景"}}
{"id": "2512.05450", "pdf": "https://arxiv.org/pdf/2512.05450", "abs": "https://arxiv.org/abs/2512.05450", "authors": ["Pawel Weichbroth"], "title": "Classification and taxonomy of mobile application usability issues", "categories": ["cs.HC"], "comment": "55 pages, 5 figures, 9 tables, 129 references", "summary": "Despite years of research on testing the usability of mobile applications, our understanding of the issues their users experience still remains fragmented and underexplored. While most earlier studies has provided interesting insights, they have varying limitations in methodology, input diversity, and depth of analysis.On the contrary, this study employs a triangulation strategy, using two research methods (systematic literature review and interview) and two data sources (scholarly literature and expert knowledge) to explore the traits underlying usability issues. Our study contributes to the field of human-computer interaction (HCI) by presenting a catalog of 16 usability issue categories, enriched with corresponding keywords and extended into a taxonomy, as well as a novel three-tier app-user-resource (AUR) classification system. At the first app level, usability issues arise from user interface design, as well as from efficiency, errors, and operability. At the second user level, they influence cognitive load, effectiveness, ease of use, learnability, memorability, and understandability. At the third resource level, usability issues stem from network quality and hardware, such as battery life, CPU speed, physical device button size and availability, RAM capacity, and screen size. The root cause of the usability issues is the user interface design. Detailed findings and takeaways for both researchers and practitioners are also discussed. Further research could focus on developing a measurement model for the identified variables to confirm the direction and strength of their relationships with perceived usability. Software vendors can also benefit by updating existing quality assurance programs, reviews and audits tools, as well as testing checklists.", "AI": {"tldr": "该论文通过系统文献综述和专家访谈的方法，提出了移动应用可用性问题的分类、分类体系和三层AUR分类系统", "motivation": "尽管多年来对移动应用可用性测试进行了大量研究，但我们对用户所遇到问题的理解仍然零散且不够深入。先前研究在方法论、输入多样性和分析深度方面存在局限性，需要更全面的探索", "method": "采用三角测量策略，结合两种研究方法（系统文献综述和访谈）和两种数据来源（学术文献和专家知识），探索可用性问题的本质特征", "result": "提出了包含16个可用性问题类别的目录，并扩展为分类体系；开发了新颖的三层应用-用户-资源（AUR）分类系统，分别从应用层、用户层和资源层分析可用性问题", "conclusion": "可用性问题的根本原因在于用户界面设计。研究结果为研究人员和从业者提供了详细发现和建议，未来可开发测量模型验证变量关系，软件供应商可更新质量保证程序和测试工具"}}
{"id": "2512.05449", "pdf": "https://arxiv.org/pdf/2512.05449", "abs": "https://arxiv.org/abs/2512.05449", "authors": ["Robert Yang"], "title": "The Seeds of Scheming: Weakness of Will in the Building Blocks of Agentic Systems", "categories": ["cs.AI"], "comment": "4 pages + appendix. AAAI 2026 FAST Workshop (Oral)", "summary": "Large language models display a peculiar form of inconsistency: they \"know\" the correct answer but fail to act on it. In human philosophy, this tension between global judgment and local impulse is called akrasia, or weakness of will. We propose akrasia as a foundational concept for analyzing inconsistency and goal drift in agentic AI systems. To operationalize it, we introduce a preliminary version of the Akrasia Benchmark, currently a structured set of prompting conditions (Baseline [B], Synonym [S], Temporal [T], and Temptation [X]) that measures when a model's local response contradicts its own prior commitments. The benchmark enables quantitative comparison of \"self-control\" across model families, decoding strategies, and temptation types. Beyond single-model evaluation, we outline how micro-level akrasia may compound into macro-level instability in multi-agent systems that may be interpreted as \"scheming\" or deliberate misalignment. By reframing inconsistency as weakness of will, this work connects agentic behavior to classical theories of agency and provides an empirical bridge between philosophy, psychology, and the emerging science of agentic AI.", "AI": {"tldr": "该论文提出将人类哲学中的\"意志薄弱\"(akrasia)概念作为分析AI智能体系统不一致性和目标漂移的基础框架，并开发了Akrasia基准测试来量化评估语言模型的\"自我控制\"能力。", "motivation": "大型语言模型表现出一种特殊的不一致性：它们\"知道\"正确答案但未能据此行动。这种全局判断与局部冲动之间的张力在人类哲学中被称为意志薄弱。作者认为这一概念对于分析AI智能体系统的不一致性和目标漂移具有基础性意义。", "method": "提出了Akrasia基准测试的初步版本，包含四种结构化提示条件：基线(B)、同义词(S)、时间(T)和诱惑(X)，用于测量模型局部响应与其先前承诺之间的矛盾。该基准支持跨模型家族、解码策略和诱惑类型的\"自我控制\"能力定量比较。", "result": "基准测试能够量化评估不同模型在面临诱惑时的意志薄弱程度，并展示了微观层面的意志薄弱如何在多智能体系统中累积为宏观层面的不稳定性，这种不稳定性可能被解释为\"阴谋\"或故意错位。", "conclusion": "通过将不一致性重新定义为意志薄弱，这项工作将智能体行为与经典的能动性理论联系起来，为哲学、心理学和新兴的智能体AI科学之间建立了经验桥梁，为分析AI系统的稳定性和一致性提供了新视角。"}}
{"id": "2512.05446", "pdf": "https://arxiv.org/pdf/2512.05446", "abs": "https://arxiv.org/abs/2512.05446", "authors": ["Cheng-Yuan Ho", "He-Bi Yang", "Jui-Chiu Chiang", "Yu-Lun Liu", "Wen-Hsiao Peng"], "title": "TED-4DGS: Temporally Activated and Embedding-based Deformation for 4DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "Building on the success of 3D Gaussian Splatting (3DGS) in static 3D scene representation, its extension to dynamic scenes, commonly referred to as 4DGS or dynamic 3DGS, has attracted increasing attention. However, designing more compact and efficient deformation schemes together with rate-distortion-optimized compression strategies for dynamic 3DGS representations remains an underexplored area. Prior methods either rely on space-time 4DGS with overspecified, short-lived Gaussian primitives or on canonical 3DGS with deformation that lacks explicit temporal control. To address this, we present TED-4DGS, a temporally activated and embedding-based deformation scheme for rate-distortion-optimized 4DGS compression that unifies the strengths of both families. TED-4DGS is built on a sparse anchor-based 3DGS representation. Each canonical anchor is assigned learnable temporal-activation parameters to specify its appearance and disappearance transitions over time, while a lightweight per-anchor temporal embedding queries a shared deformation bank to produce anchor-specific deformation. For rate-distortion compression, we incorporate an implicit neural representation (INR)-based hyperprior to model anchor attribute distributions, along with a channel-wise autoregressive model to capture intra-anchor correlations. With these novel elements, our scheme achieves state-of-the-art rate-distortion performance on several real-world datasets. To the best of our knowledge, this work represents one of the first attempts to pursue a rate-distortion-optimized compression framework for dynamic 3DGS representations.", "AI": {"tldr": "提出TED-4DGS方法，用于动态3D高斯泼溅（4DGS）的率失真优化压缩，通过时间激活和嵌入变形机制实现高效的动态场景表示。", "motivation": "现有动态3DGS方法存在缺陷：时空4DGS使用过度指定、寿命短的基元，而规范3DGS的变形缺乏明确的时间控制。需要设计更紧凑高效的变形方案和率失真优化压缩策略。", "method": "基于稀疏锚点的3DGS表示，每个规范锚点分配可学习的时间激活参数控制出现/消失过渡；轻量级每锚点时间嵌入查询共享变形库产生锚点特定变形；采用INR超先验建模锚点属性分布，结合通道自回归模型捕获锚点内相关性。", "result": "在多个真实世界数据集上实现了最先进的率失真性能，是首批追求动态3DGS表示率失真优化压缩框架的尝试之一。", "conclusion": "TED-4DGS统一了两类方法的优势，通过时间激活和嵌入变形实现了高效的4DGS压缩，为动态3DGS表示提供了紧凑且率失真优化的解决方案。"}}
{"id": "2512.05442", "pdf": "https://arxiv.org/pdf/2512.05442", "abs": "https://arxiv.org/abs/2512.05442", "authors": ["Hua Wang", "Jinghao Lu", "Fan Zhang"], "title": "IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AAAI 2026", "summary": "Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.", "AI": {"tldr": "提出IdealTSF框架，利用非理想负样本数据提升时间序列预测模型性能", "motivation": "时间序列数据中普遍存在缺失值和异常值等问题，传统方法主要关注从序列数据中提取特征信息或将这些问题数据作为正样本进行知识迁移，但未能充分利用非理想负样本的潜力来增强事件预测", "method": "提出IdealTSF框架，包含三个渐进步骤：预训练、训练和优化。首先通过从负样本数据中提取知识进行预训练，然后在训练阶段将序列数据转换为理想正样本，并应用带有对抗扰动的负优化机制", "result": "大量实验证明，负样本数据能够释放基础注意力架构在时间序列预测中的显著潜力，IdealTSF特别适用于具有噪声样本或低质量数据的应用场景", "conclusion": "非理想负样本数据可以显著增强时间序列预测模型的性能，IdealTSF框架通过整合理想正样本和负样本，为处理噪声数据或低质量数据的应用提供了有效的解决方案"}}
{"id": "2512.05439", "pdf": "https://arxiv.org/pdf/2512.05439", "abs": "https://arxiv.org/abs/2512.05439", "authors": ["Tarun Suresh", "Nalin Wadhwa", "Debangshu Banerjee", "Gagandeep Singh"], "title": "BEAVER: An Efficient Deterministic LLM Verifier", "categories": ["cs.AI", "cs.FL"], "comment": null, "summary": "As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.", "AI": {"tldr": "BEAVER是一个高效的确定性LLM验证框架，用于计算语言模型输出满足约束条件的确定性概率边界。", "motivation": "随着LLM从研究原型转向生产系统，需要可靠的方法来验证模型输出是否满足约束条件。基于采样的估计方法只能提供直觉，无法提供可靠保证。", "method": "使用新颖的token trie和frontier数据结构，系统性地探索生成空间，为任何前缀封闭的语义约束维护可证明的可靠概率边界。", "result": "在正确性验证、隐私验证和安全代码生成任务上，BEAVER在相同计算预算下实现了6-8倍更紧的概率边界，识别出3-4倍更多的高风险实例。", "conclusion": "BEAVER是第一个实用的确定性LLM验证框架，能够提供采样方法或经验评估无法提供的精确特征描述和风险评估。"}}
{"id": "2512.05438", "pdf": "https://arxiv.org/pdf/2512.05438", "abs": "https://arxiv.org/abs/2512.05438", "authors": ["Benoit Marteau", "Shaun Q. Y. Tan", "Jieru Li", "Andrew Hornback", "Yishan Zhong", "Shaunna Wang", "Christian Lowson", "Jason Woloff", "Joshua M. Pahys", "Steven W. Hwang", "Coleman Hilton", "May D. Wang"], "title": "EXR: An Interactive Immersive EHR Visualization in Extended Reality", "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.MM"], "comment": "11 pages, 6 figures. Preprint version. This paper has been accepted to IEEE ICIR 2025. This is the author-prepared version and not the final published version. The final version will appear in IEEE Xplo", "summary": "This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.", "AI": {"tldr": "设计并实现了一个扩展现实(XR)平台，用于电子健康记录(EHR)的沉浸式交互可视化，将结构化与非结构化患者数据整合到共享3D环境中，支持实时协作和直观探索。", "motivation": "传统2D界面在医疗数据可视化方面存在局限，无法充分利用空间感知和沉浸式交互的优势。需要开发能够整合多种医疗数据源（包括结构化EHR数据、医学影像和AI分割结果）的下一代临床决策支持工具。", "method": "开发了模块化XR平台，集成FHIR标准的EHR数据、容积医学影像和AI生成的分割结果。使用合成EHR数据集和CT衍生的脊柱模型，通过AI分割管道处理，在共享3D环境中实现数据可视化。", "result": "成功实现了EHR数据的沉浸式3D可视化系统，能够直观展示患者数据并支持实时协作。系统展示了与现代化医疗系统的互操作性，为下一代临床决策支持工具奠定了基础。", "conclusion": "这种集成的XR解决方案可以作为下一代临床决策支持工具的基础，在交互式和空间丰富的环境中直接访问先进的数据基础设施，有望改善医疗数据可视化和临床协作。"}}
{"id": "2512.05433", "pdf": "https://arxiv.org/pdf/2512.05433", "abs": "https://arxiv.org/abs/2512.05433", "authors": ["Kim Marriott", "Matthew Butler", "Leona Holloway", "Bill Jolley", "Bongshin Lee", "Bruce Maguire", "Danielle Albers Szafir"], "title": "From Vision to Touch: Bridging Visual and Tactile Principles for Accessible Data Representation", "categories": ["cs.HC"], "comment": "To be published by IEEE as part of the 2025 Visualization Conference (VIS)", "summary": "Tactile graphics are widely used to present maps and statistical diagrams to blind and low vision (BLV) people, with accessibility guidelines recommending their use for graphics where spatial relationships are important. Their use is expected to grow with the advent of commodity refreshable tactile displays. However, in stark contrast to visual information graphics, we lack a clear understanding of the benefits that well-designed tactile information graphics offer over text descriptions for BLV people. To address this gap, we introduce a framework considering the three components of encoding, perception and cognition to examine the known benefits for visual information graphics and explore their applicability to tactile information graphics. This work establishes a preliminary theoretical foundation for the tactile-first design of information graphics and identifies future research avenues.", "AI": {"tldr": "该论文提出了一个框架，用于研究触觉信息图表相对于文本描述对盲人和低视力人群的益处，为触觉优先的信息图表设计建立理论基础。", "motivation": "随着商品化可刷新触觉显示器的出现，触觉图形的使用预计会增加，但目前缺乏对设计良好的触觉信息图表相对于文本描述对盲人和低视力人群益处的清晰理解。", "method": "引入了一个考虑编码、感知和认知三个组成部分的框架，借鉴视觉信息图表的已知益处，探索其在触觉信息图表中的适用性。", "result": "建立了触觉优先信息图表设计的初步理论基础，并确定了未来的研究方向。", "conclusion": "该框架为理解触觉信息图表相对于文本描述的优势提供了理论基础，有助于推动触觉优先的信息图表设计研究。"}}
{"id": "2512.05432", "pdf": "https://arxiv.org/pdf/2512.05432", "abs": "https://arxiv.org/abs/2512.05432", "authors": ["Jeffrey N. A. Aryee", "Patrick Davies", "Godfred A. Torsah", "Mercy M. Apaw", "Cyril D. Boateng", "Sam M. Mwando", "Chris Kwisanga", "Eric Jobunga", "Leonard K. Amekudzi"], "title": "Building Capacity for Artificial Intelligence in Africa: A Cross-Country Survey of Challenges and Governance Pathways", "categories": ["cs.CY", "cs.AI"], "comment": "16 pages, 4 figures, 1 table", "summary": "Artificial intelligence (AI) is transforming education and the workforce, but access to AI learning opportunities in Africa remains uneven. With rapid demographic shifts and growing labour market pressures, AI has become a strategic development priority, making the demand for relevant skills more urgent. This study investigates how universities and industries engage in shaping AI education and workforce preparation, drawing on survey responses from five African countries (Ghana, Namibia, Rwanda, Kenya and Zambia). The findings show broad recognition of AI importance but limited evidence of consistent engagement, practical training, or equitable access to resources. Most respondents who rated the AI component of their curriculum as very relevant reported being well prepared for jobs, but financial barriers, poor infrastructure, and weak communication limit participation, especially among students and underrepresented groups. Respondents highlighted internships, industry partnerships, and targeted support mechanisms as critical enablers, alongside the need for inclusive governance frameworks. The results showed both the growing awareness of AI's potential and the structural gaps that hinder its translation into workforce capacity. Strengthening university-industry collaboration and addressing barriers of access, funding, and policy are central to ensuring that AI contributes to equitable and sustainable development across the continent.", "AI": {"tldr": "调查非洲五国人工智能教育与劳动力准备现状，分析大学与产业在AI能力建设中的参与情况、挑战及治理路径", "motivation": "AI正在改变教育和劳动力市场，但非洲的AI学习机会获取不均。面对快速的人口结构变化和劳动力市场压力，AI已成为战略发展重点，相关技能需求日益紧迫。需要了解非洲大学和产业如何参与塑造AI教育和劳动力准备", "method": "对五个非洲国家（加纳、纳米比亚、卢旺达、肯尼亚、赞比亚）进行跨国家调查，收集大学和产业相关方的问卷回应，分析AI教育参与度、资源获取、培训质量等", "result": "调查显示广泛认识到AI重要性，但缺乏持续参与、实践培训和公平资源获取的证据。大多数认为课程AI内容非常相关的受访者报告就业准备充分，但财务障碍、基础设施差和沟通不畅限制了参与，特别是学生和弱势群体。实习、产业合作和针对性支持机制是关键推动因素", "conclusion": "研究显示AI潜力认知度提高，但结构差距阻碍其转化为劳动力能力。加强大学-产业合作，解决获取、资金和政策障碍，对确保AI促进非洲大陆公平可持续发展至关重要"}}
{"id": "2512.05430", "pdf": "https://arxiv.org/pdf/2512.05430", "abs": "https://arxiv.org/abs/2512.05430", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "title": "ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Submitted to LREC 2026. This work is an evolution of our earlier preprint arXiv:2507.23334", "summary": "Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.", "AI": {"tldr": "该论文提出了ArtistMus基准和MusWikiDB向量数据库，用于评估检索增强生成在音乐问答任务中的性能，旨在解决大语言模型在音乐知识方面的局限性。", "motivation": "当前大语言模型在音乐相关推理方面效果有限，因为预训练数据中音乐知识稀疏。虽然音乐信息检索和计算音乐学已探索结构化多模态理解，但缺乏基于艺术家元数据或历史背景的事实性和上下文音乐问答资源。", "method": "构建了MusWikiDB（包含144K音乐相关维基百科页面的320万段落向量数据库）和ArtistMus基准（包含500位多样化艺术家的1000个问题，附带流派、出道年份等元数据）。使用检索增强生成方法进行系统评估，并探索RAG风格的微调。", "result": "RAG显著提高了事实准确性：开源模型提升高达+56.8个百分点（如Qwen3 8B从35.0提升到91.8），接近专有模型性能。RAG风格微调进一步提升了事实回忆和上下文推理能力。MusWikiDB相比通用维基百科语料库准确率提高约6个百分点，检索速度快40%。", "conclusion": "MusWikiDB和ArtistMus为音乐信息检索和领域特定问答研究提供了重要资源，为音乐等文化丰富领域的检索增强推理奠定了基础，展示了RAG在解决大语言模型音乐知识局限性的有效性。"}}
{"id": "2512.05422", "pdf": "https://arxiv.org/pdf/2512.05422", "abs": "https://arxiv.org/abs/2512.05422", "authors": ["Jiangtong Tan", "Lin Liu", "Jie Huanng", "Xiaopeng Zhang", "Qi Tian", "Feng Zhao"], "title": "ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Unified multimodal models significantly improve visual generation by combining vision-language models (VLMs) with diffusion models. However, existing methods struggle to fully balance sufficient interaction and flexible implementation due to vast representation difference. Considering abundant and hierarchical information in VLM's layers from low-level details to high-level semantics, we propose \\textbf{ParaUni}. It extracts features from variants VLM's layers in a \\textbf{Para}llel way for comprehensive information interaction and retains a flexible separation architecture to enhance generation in \\textbf{Uni}fied multimodal model. Concretely, visual features from all VLM's layers are fed in parallel into a Layer Integration Module (LIM), which efficiently integrates fine-grained details and semantic abstractions and provides the fused representation as a condition to the diffusion model. To further enhance performance, we reveal that these hierarchical layers respond unequally to different rewards in Reinforcement Learning (RL). Crucially, we design a Layer-wise Dynamic Adjustment Mechanism (LDAM) to facilitate multiple reward improvements that aligns the hierarchical properties of these layers using RL. Extensive experiments show ParaUni leverages complementary multi-layer features to substantially improve generation quality and shows strong potential for multiple reward advances during RL stages. Code is available at https://github.com/JosephTiTan/ParaUni.", "AI": {"tldr": "提出ParaUni方法，通过并行提取视觉语言模型各层次特征并进行强化学习驱动的层次化调整，来增强统一多模态模型中的生成质量", "motivation": "现有统一多模态模型在视觉生成任务中难以平衡充分交互与灵活实现，因为视觉语言模型和扩散模型之间存在巨大的表示差异。视觉语言模型各层次包含从低层细节到高层语义的丰富层次化信息，但现有方法未能充分利用这些信息", "method": "1. 并行提取视觉语言模型各层次特征，通过层集成模块(LIM)高效整合细粒度细节和语义抽象；2. 设计层级动态调整机制(LDAM)，利用强化学习对不同层次特征进行差异化奖励调整，使层次特性与奖励目标对齐", "result": "实验表明ParaUni能够利用互补的多层次特征显著提升生成质量，在强化学习阶段展现出对多种奖励改进的强大潜力", "conclusion": "ParaUni通过并行信息交互和强化学习驱动的层次化调整，有效解决了统一多模态模型中视觉生成的质量问题，为多模态生成提供了新的解决方案"}}
{"id": "2512.05418", "pdf": "https://arxiv.org/pdf/2512.05418", "abs": "https://arxiv.org/abs/2512.05418", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Performance Evaluation of Deep Learning for Tree Branch Segmentation in Autonomous Forestry Systems", "categories": ["cs.CV"], "comment": null, "summary": "UAV-based autonomous forestry operations require rapid and precise tree branch segmentation for safe navigation and automated pruning across varying pixel resolutions and operational conditions. We evaluate different deep learning methods at three resolutions (256x256, 512x512, 1024x1024) using the Urban Street Tree Dataset, employing standard metrics (IoU, Dice) and specialized measures including Thin Structure IoU (TS-IoU) and Connectivity Preservation Rate (CPR). Among 22 configurations tested, U-Net with MiT-B4 backbone achieves strong performance at 256x256. At 512x512, MiT-B4 leads in IoU, Dice, TS-IoU, and Boundary-F1. At 1024x1024, U-Net+MiT-B3 shows the best validation performance for IoU/Dice and precision, while U-Net++ excels in boundary quality. PSPNet provides the most efficient option (2.36/9.43/37.74 GFLOPs) with 25.7/19.6/11.8 percentage point IoU reductions compared to top performers at respective resolutions. These results establish multi-resolution benchmarks for accuracy-efficiency trade-offs in embedded forestry systems. Implementation is available at https://github.com/BennyLinntu/PerformanceTreeBranchSegmentation.", "AI": {"tldr": "评估深度学习模型在不同分辨率下对树木枝干分割的性能，为自主林业系统的嵌入式应用提供精度-效率权衡的基准", "motivation": "无人机自主林业作业需要快速精确的树木枝干分割，以确保安全导航和自动修剪，但现有方法在不同像素分辨率和操作条件下的性能评估不足", "method": "使用Urban Street Tree数据集，在三种分辨率（256x256, 512x512, 1024x1024）下评估多种深度学习模型，包括标准指标（IoU, Dice）和专门指标（TS-IoU, CPR），共测试22种配置", "result": "在256x256分辨率下，U-Net+MiT-B4表现最佳；512x512下MiT-B4在多个指标领先；1024x1024下U-Net+MiT-B3在IoU/Dice和精度上最优，U-Net++在边界质量上表现最好；PSPNet效率最高但精度有显著降低", "conclusion": "研究为嵌入式林业系统建立了多分辨率性能基准，揭示了不同模型在精度和效率之间的权衡，为实际应用中的模型选择提供了指导"}}
{"id": "2512.05415", "pdf": "https://arxiv.org/pdf/2512.05415", "abs": "https://arxiv.org/abs/2512.05415", "authors": ["Masato Shibukawa", "Fumi Yoshida", "Toshifumi Yanagisawa", "Takashi Ito", "Hirohisa Kurosaki", "Makoto Yoshikawa", "Kohki Kamiya", "Ji-an Jiang", "Wesley Fraser", "JJ Kavelaars", "Susan Benecchi", "Anne Verbiscer", "Akira Hatakeyama", "Hosei O", "Naoya Ozaki"], "title": "Moving object detection from multi-depth images with an attention-enhanced CNN", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "14 pages, 22 figures, submitted to PASJ", "summary": "One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.", "AI": {"tldr": "提出一种基于注意力增强CNN的多深度图像移动目标检测方法，用于太阳系巡天数据中的移动目标自动识别", "motivation": "太阳系巡天数据中移动目标检测面临的主要挑战是区分真实目标与噪声等干扰信号。传统方法依赖人工验证，劳动成本高，需要减少人工干预并提高检测效率", "method": "提出多输入卷积神经网络集成卷积块注意力模块的方法。采用多输入架构同时处理多个堆叠图像，结合空间和通道维度的注意力机制，使模型能够聚焦关键特征", "result": "在约2000张观测图像的数据集上评估，模型准确率达到近99%，AUC>0.99。通过调整检测阈值，新模型相比人工验证减少了超过99%的工作量", "conclusion": "提出的注意力增强CNN方法在太阳系移动目标检测中表现出优异的分类性能，显著降低了人工验证的工作量，为自动化巡天数据处理提供了有效解决方案"}}
{"id": "2512.05412", "pdf": "https://arxiv.org/pdf/2512.05412", "abs": "https://arxiv.org/abs/2512.05412", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "YOLO and SGBM Integration for Autonomous Tree Branch Detection and Depth Estimation in Radiata Pine Pruning Applications", "categories": ["cs.CV"], "comment": null, "summary": "Manual pruning of radiata pine trees poses significant safety risks due to extreme working heights and challenging terrain. This paper presents a computer vision framework that integrates YOLO object detection with Semi-Global Block Matching (SGBM) stereo vision for autonomous drone-based pruning operations. Our system achieves precise branch detection and depth estimation using only stereo camera input, eliminating the need for expensive LiDAR sensors. Experimental evaluation demonstrates YOLO's superior performance over Mask R-CNN, achieving 82.0% mAPmask50-95 for branch segmentation. The integrated system accurately localizes branches within a 2 m operational range, with processing times under one second per frame. These results establish the feasibility of cost-effective autonomous pruning systems that enhance worker safety and operational efficiency in commercial forestry.", "AI": {"tldr": "提出一种用于辐射松修剪的自主无人机系统，集成YOLO目标检测与SGBM立体视觉技术，实现树枝检测与深度估计", "motivation": "人工修剪辐射松树存在安全风险（工作高度极端、地形复杂），需要开发成本效益高的自主修剪系统来提升工人安全性和作业效率", "method": "集成YOLO目标检测与SGBM立体视觉技术，仅使用立体相机输入，无需昂贵的LiDAR传感器，实现树枝检测和深度估计", "result": "YOLO在树枝分割方面表现优于Mask R-CNN，达到82.0% mAPmask50-95；集成系统在2米操作范围内准确定位树枝，每帧处理时间低于1秒", "conclusion": "该研究证明了成本效益高的自主修剪系统的可行性，能够显著提升商业林业中的工人安全性和作业效率"}}
{"id": "2512.05411", "pdf": "https://arxiv.org/pdf/2512.05411", "abs": "https://arxiv.org/abs/2512.05411", "authors": ["Pranav Pushkar Mishra", "Kranti Prakash Yeole", "Ramyashree Keshavamurthy", "Mokshit Bharat Surana", "Fatemeh Sarayloo"], "title": "A Systematic Framework for Enterprise Knowledge Retrieval: Leveraging LLM-Generated Metadata to Enhance RAG Systems", "categories": ["cs.IR", "cs.AI"], "comment": "7 pages, 3 figures, 3 tables", "summary": "In enterprise settings, efficiently retrieving relevant information from large and complex knowledge bases is essential for operational productivity and informed decision-making. This research presents a systematic framework for metadata enrichment using large language models (LLMs) to enhance document retrieval in Retrieval-Augmented Generation (RAG) systems. Our approach employs a comprehensive, structured pipeline that dynamically generates meaningful metadata for document segments, substantially improving their semantic representations and retrieval accuracy. Through extensive experiments, we compare three chunking strategies-semantic, recursive, and naive-and evaluate their effectiveness when combined with advanced embedding techniques. The results demonstrate that metadata-enriched approaches consistently outperform content-only baselines, with recursive chunking paired with TF-IDF weighted embeddings yielding an 82.5% precision rate compared to 73.3% for semantic content-only approaches. The naive chunking strategy with prefix-fusion achieved the highest Hit Rate@10 of 0.925. Our evaluation employs cross-encoder reranking for ground truth generation, enabling rigorous assessment via Hit Rate and Metadata Consistency metrics. These findings confirm that metadata enrichment enhances vector clustering quality while reducing retrieval latency, making it a key optimization for RAG systems across knowledge domains. This work offers practical insights for deploying high-performance, scalable document retrieval solutions in enterprise settings, demonstrating that metadata enrichment is a powerful approach for enhancing RAG effectiveness.", "AI": {"tldr": "提出一个系统化框架，利用LLM生成的元数据增强企业知识检索中的RAG系统性能", "motivation": "在企业环境中，从大型复杂知识库中高效检索相关信息对运营生产力和决策制定至关重要，需要提升RAG系统的检索准确性", "method": "采用结构化流水线，动态为文档片段生成有意义的元数据，比较三种分块策略（语义、递归、朴素）并结合先进的嵌入技术", "result": "元数据增强方法始终优于仅内容基线，递归分块结合TF-IDF加权嵌入达到82.5%精确率，朴素分块策略获得最高Hit Rate@10为0.925", "conclusion": "元数据增强提高了向量聚类质量并减少检索延迟，是企业环境中部署高性能、可扩展文档检索解决方案的关键优化方法"}}
{"id": "2512.05410", "pdf": "https://arxiv.org/pdf/2512.05410", "abs": "https://arxiv.org/abs/2512.05410", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Genetic Algorithms For Parameter Optimization for Disparity Map Generation of Radiata Pine Branch Images", "categories": ["cs.CV"], "comment": null, "summary": "Traditional stereo matching algorithms like Semi-Global Block Matching (SGBM) with Weighted Least Squares (WLS) filtering offer speed advantages over neural networks for UAV applications, generating disparity maps in approximately 0.5 seconds per frame. However, these algorithms require meticulous parameter tuning. We propose a Genetic Algorithm (GA) based parameter optimization framework that systematically searches for optimal parameter configurations for SGBM and WLS, enabling UAVs to measure distances to tree branches with enhanced precision while maintaining processing efficiency. Our contributions include: (1) a novel GA-based parameter optimization framework that eliminates manual tuning; (2) a comprehensive evaluation methodology using multiple image quality metrics; and (3) a practical solution for resource-constrained UAV systems. Experimental results demonstrate that our GA-optimized approach reduces Mean Squared Error by 42.86% while increasing Peak Signal-to-Noise Ratio and Structural Similarity by 8.47% and 28.52%, respectively, compared with baseline configurations. Furthermore, our approach demonstrates superior generalization performance across varied imaging conditions, which is critcal for real-world forestry applications.", "AI": {"tldr": "提出基于遗传算法的参数优化框架，用于优化辐射松树枝图像视差图生成中的SGBM和WLS滤波器参数，以提高无人机林业应用中的距离测量精度", "motivation": "传统立体匹配算法（如SGBM+WLS）在无人机应用中具有速度优势（约0.5秒/帧），但需要繁琐的手动参数调优。无人机林业应用需要高精度的距离测量，特别是在资源受限的环境中，因此需要一种自动化的参数优化方法", "method": "采用遗传算法（GA）框架，系统搜索SGBM和WLS滤波器的最优参数配置。该方法通过多代进化过程自动优化参数，无需人工干预，同时保持处理效率", "result": "实验结果表明，GA优化方法相比基线配置：均方误差降低42.86%，峰值信噪比提高8.47%，结构相似性提高28.52%。该方法在不同成像条件下表现出优异的泛化性能", "conclusion": "提出的GA参数优化框架有效解决了传统立体匹配算法的手动调参问题，显著提高了视差图质量，为资源受限的无人机林业应用提供了实用的自动化解决方案"}}
{"id": "2512.05402", "pdf": "https://arxiv.org/pdf/2512.05402", "abs": "https://arxiv.org/abs/2512.05402", "authors": ["Sithumi Wickramasinghe", "Bikramjit Das", "Dorien Herremans"], "title": "Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.NE"], "comment": null, "summary": "Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.", "AI": {"tldr": "提出一个基于深度学习的比特币挖矿硬件投资回报预测框架，用于判断购买ASIC矿机的最佳时机", "motivation": "比特币挖矿硬件采购面临市场波动、技术快速过时和协议驱动的收入周期等挑战，目前缺乏指导何时购买新ASIC硬件的计算框架", "method": "将硬件采购建模为时间序列分类任务，提出MineROI-Net——基于Transformer的开源架构，用于捕捉挖矿盈利能力的多尺度时间模式", "result": "在2015-2024年20款ASIC矿机数据上，MineROI-Net达到83.7%准确率和83.1%宏F1分数，在检测无利润和有利润时期分别达到93.6%和98.5%的精确率", "conclusion": "MineROI-Net为挖矿硬件采购时机提供了实用的数据驱动工具，可降低资本密集型挖矿操作的财务风险"}}
{"id": "2512.05398", "pdf": "https://arxiv.org/pdf/2512.05398", "abs": "https://arxiv.org/abs/2512.05398", "authors": ["Zhuoyuan Wu", "Xurui Yang", "Jiahui Huang", "Yue Wang", "Jun Gao"], "title": "The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/wuzy2115/DYNAPO", "summary": "Estimating accurate camera poses, 3D scene geometry, and object motion from in-the-wild videos is a long-standing challenge for classical structure from motion pipelines due to the presence of dynamic objects. Recent learning-based methods attempt to overcome this challenge by training motion estimators to filter dynamic objects and focus on the static background. However, their performance is largely limited by the availability of large-scale motion segmentation datasets, resulting in inaccurate segmentation and, therefore, inferior structural 3D understanding. In this work, we introduce the Dynamic Prior (\\ourmodel) to robustly identify dynamic objects without task-specific training, leveraging the powerful reasoning capabilities of Vision-Language Models (VLMs) and the fine-grained spatial segmentation capacity of SAM2. \\ourmodel can be seamlessly integrated into state-of-the-art pipelines for camera pose optimization, depth reconstruction, and 4D trajectory estimation. Extensive experiments on both synthetic and real-world videos demonstrate that \\ourmodel not only achieves state-of-the-art performance on motion segmentation, but also significantly improves accuracy and robustness for structural 3D understanding.", "AI": {"tldr": "提出Dynamic Prior方法，利用视觉语言模型和SAM2分割能力，无需特定训练即可识别动态物体，提升动态视频中的3D结构理解", "motivation": "传统SfM方法在动态物体存在时难以准确估计相机位姿和3D几何结构，现有学习方法受限于大规模运动分割数据集，导致分割不准确和3D理解性能下降", "method": "结合视觉语言模型的推理能力和SAM2的细粒度空间分割能力，构建Dynamic Prior来鲁棒识别动态物体，无需任务特定训练，可集成到SOTA的相机位姿优化、深度重建和4D轨迹估计流程中", "result": "在合成和真实世界视频上的实验表明，该方法不仅在运动分割上达到SOTA性能，还显著提高了结构3D理解的准确性和鲁棒性", "conclusion": "Dynamic Prior方法通过结合VLMs和SAM2的优势，无需特定训练即可有效识别动态物体，为动态视频的3D结构理解提供了更准确和鲁棒的解决方案"}}
{"id": "2512.05397", "pdf": "https://arxiv.org/pdf/2512.05397", "abs": "https://arxiv.org/abs/2512.05397", "authors": ["Rachel Poonsiriwong", "Chayapatr Archiwaranguprok", "Constanze Albrecht", "Peggy Yin", "Nattavudh Powthavee", "Hal Hershfield", "Monchai Lertsutthiwong", "Kavin Winson", "Pat Pataranutaporn"], "title": "Simulating Life Paths with Digital Twins: AI-Generated Future Selves Influence Decision-Making and Expand Human Choice", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Major life transitions demand high-stakes decisions, yet people often struggle to imagine how their future selves will live with the consequences. To support this limited capacity for mental time travel, we introduce AI-enabled digital twins that have ``lived through'' simulated life scenarios. Rather than predicting optimal outcomes, these simulations extend prospective cognition by making alternative futures vivid enough to support deliberation without assuming which path is best. We evaluate this idea in a randomized controlled study (N=192) using multimodal synthesis - facial age progression, voice cloning, and large language model dialogue - to create personalized avatars representing participants 30 years forward. Young adults 18 to 28 years old described pending binary decisions and were assigned to guided imagination or one of four avatar conditions: single-option, balanced dual-option, or expanded three-option with a system-generated novel alternative. Results showed asymmetric effects: single-sided avatars increased shifts toward the presented option, while balanced presentation produced movement toward both. Introducing a system-generated third option increased adoption of this new alternative compared to control, suggesting that AI-generated future selves can expand choice by surfacing paths that might otherwise go unnoticed. Participants rated evaluative reasoning and eudaimonic meaning-making as more important than emotional or visual vividness. Perceived persuasiveness and baseline agency predicted decision change. These findings advance understanding of AI-mediated episodic prospection and raise questions about autonomy in AI-augmented decisions.", "AI": {"tldr": "该论文研究了AI数字孪生技术如何通过模拟未来生活路径来影响年轻人的重大人生决策，发现AI生成的未来自我形象能够扩展人类选择范围并影响决策过程。", "motivation": "人们在面临重大人生转折时，往往难以想象未来自我将如何应对决策后果，这种\"心理时间旅行\"能力有限。因此需要开发AI技术来扩展前瞻性认知，帮助人们更生动地想象不同未来路径。", "method": "采用随机对照研究（N=192），使用多模态合成技术（面部年龄进展、声音克隆、大语言模型对话）创建代表参与者30年后状态的个性化数字孪生。参与者被分配到引导想象组或四种数字孪生条件之一：单选项、平衡双选项、扩展三选项（包含系统生成的新替代方案）。", "result": "结果显示非对称效应：单侧数字孪生增加了向呈现选项的转变，平衡呈现则产生向两个选项的移动。引入系统生成的第三选项增加了对新替代方案的采纳。参与者认为评估推理和幸福意义建构比情感或视觉生动性更重要。感知说服力和基线能动性预测了决策变化。", "conclusion": "AI生成的未来自我能够扩展人类选择范围，揭示可能被忽视的人生路径。这些发现推进了对AI介导的情景前瞻的理解，并提出了AI增强决策中自主性的重要问题。"}}
{"id": "2512.05394", "pdf": "https://arxiv.org/pdf/2512.05394", "abs": "https://arxiv.org/abs/2512.05394", "authors": ["Shizhan Liu", "Xinran Deng", "Zhuoyi Yang", "Jiayan Teng", "Xiaotao Gu", "Jie Tang"], "title": "Delving into Latent Spectral Biasing of Video VAEs for Superior Diffusability", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models pair VAEs with diffusion backbones, and the structure of VAE latents strongly influences the difficulty of diffusion training. However, existing video VAEs typically focus on reconstruction fidelity, overlooking latent structure. We present a statistical analysis of video VAE latent spaces and identify two spectral properties essential for diffusion training: a spatio-temporal frequency spectrum biased toward low frequencies, and a channel-wise eigenspectrum dominated by a few modes. To induce these properties, we propose two lightweight, backbone-agnostic regularizers: Local Correlation Regularization and Latent Masked Reconstruction. Experiments show that our Spectral-Structured VAE (SSVAE) achieves a $3\\times$ speedup in text-to-video generation convergence and a 10\\% gain in video reward, outperforming strong open-source VAEs. The code is available at https://github.com/zai-org/SSVAE.", "AI": {"tldr": "该论文提出了一种针对视频VAE的谱结构优化方法，通过分析潜在空间的光谱特性并引入正则化器，提升视频扩散模型的训练效率和生成质量。", "motivation": "现有视频VAE通常只关注重建保真度，而忽略了潜在空间结构对扩散训练的重要性。潜在空间的结构会显著影响扩散模型的训练难度，因此需要专门优化视频VAE的潜在空间特性。", "method": "首先对视频VAE潜在空间进行统计分析，识别出两个对扩散训练至关重要的光谱特性：偏向低频的时空频率谱和由少数模式主导的通道特征谱。然后提出两种轻量级、与主干网络无关的正则化器：局部相关正则化和潜在掩码重建。", "result": "提出的谱结构VAE（SSVAE）在文本到视频生成任务中实现了3倍的收敛速度提升和10%的视频奖励增益，优于现有的开源VAE模型。", "conclusion": "通过专门优化视频VAE潜在空间的光谱结构，可以显著提升扩散模型的训练效率和生成质量，为视频生成任务提供了有效的潜在表示优化方法。"}}
{"id": "2512.05391", "pdf": "https://arxiv.org/pdf/2512.05391", "abs": "https://arxiv.org/abs/2512.05391", "authors": ["Qingqiao Hu", "Weimin Lyu", "Meilong Xu", "Kehan Qi", "Xiaoling Hu", "Saumya Gupta", "Jiawei Zhou", "Chao Chen"], "title": "LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "20 pages", "summary": "Whole Slide Image (WSI) understanding is fundamentally challenging due to its gigapixel scale and the extreme sparsity of diagnostically relevant regions. Unlike human experts who primarily rely on key areas to arrive at a diagnosis, existing slide-level multimodal large language models (MLLMs) for pathology rely on heavy slide-level encoders that process thousands of patch features in a brute-force manner, resulting in excessive computational cost. In this work, we revisit the WSI-language modeling paradigm and show that tile-level features exhibit strong global and local redundancy, whereas only a small subset of tiles are truly task-relevant. Motivated by this observation, we introduce an efficient MLLM framework, called LoC-Path, that replaces the expensive slide-level encoder with redundancy-reducing modules. We first design a Sparse Token Merger (STM) and an MAE-pretrained resampler to remove local redundancy and compress globally redundant tile tokens into a compact slide-level representation set. We then propose a Cross-Attention Routing Adapter (CARA) and a Token Importance Scorer (TIS) to integrate the compressed visual representation with the language model in a computation-efficient manner. Extensive experiments demonstrate that our approach achieves performance comparable to existing state-of-the-art whole-slide MLLMs, while requiring significantly lower computation and memory.", "AI": {"tldr": "提出LoC-Path框架，通过压缩冗余的病理切片特征来构建高效的多模态大语言模型，显著降低计算和内存消耗", "motivation": "现有病理切片多模态大语言模型依赖重型切片级编码器处理数千个补丁特征，计算成本过高。研究发现切片级特征存在强烈的全局和局部冗余，只有小部分切片真正与任务相关", "method": "1) 设计稀疏令牌合并器(STM)和MAE预训练重采样器，去除局部冗余并将全局冗余切片令牌压缩为紧凑的切片级表示集；2) 提出交叉注意力路由适配器(CARA)和令牌重要性评分器(TIS)，以计算高效的方式将压缩的视觉表示与语言模型集成", "result": "实验表明，该方法在性能上与现有最先进的整片MLLMs相当，同时需要显著更低的计算和内存资源", "conclusion": "LoC-Path通过减少冗余特征处理，为病理学多模态大语言模型提供了一种高效的计算框架，在保持性能的同时大幅降低了资源需求"}}
{"id": "2512.05389", "pdf": "https://arxiv.org/pdf/2512.05389", "abs": "https://arxiv.org/abs/2512.05389", "authors": ["Yuxuan Chen", "Ian Leong Ting Lo", "Bao Guo", "Netitorn Kawmali", "Chun Kit Chan", "Ruoyu Wang", "Jia Pan", "Lei Yang"], "title": "CLIO: A Tour Guide Robot with Co-speech Actions for Visual Attention Guidance and Enhanced User Engagement", "categories": ["cs.HC"], "comment": "10 pages, 7 figures, human-robot interaction", "summary": "While audio guides can offer rich information about an exhibit, it is challenging for visitors to focus on specific exhibit details based only on the verbal description. We present \\textit{CLIO}, a tour guide robot with co-speech actions to direct visitors' visual attention and thus enhance the overall user engagement in a guided tour. \\textit{CLIO} is equipped with designed actions to engage visitors. It builds eye contact with the visitor through tracking a visitor's face and blinking its eyes, or orient their attention by its head movement and laser pointer. We further use a Large Language Model (LLM) to coordinate the designed actions with a given narrative script for exhibition. We conducted a user study to evaluate the \\textit{CLIO} system in a mock-up exhibition of historical photographs. We collected feedback from questionnaires and quantitative data from a mobile eye tracker. Experimental results validated that the engaging actions are well designed and demonstrated its efficacy in guiding visual attention of the visitors. It was evidenced that \\textit{CLIO} achieved an enhanced engagement compared to the baseline system with only audio guidance.", "AI": {"tldr": "CLIO是一个具有伴随语音动作的导览机器人，通过视觉注意力引导增强用户在博物馆参观中的参与度", "motivation": "传统音频导览难以引导参观者关注展品的具体细节，需要一种能够有效引导视觉注意力的交互方式", "method": "开发配备伴随语音动作的机器人系统，包括眼神接触、头部运动和激光指针；使用大语言模型协调动作与讲解脚本", "result": "用户研究和眼动追踪数据显示，CLIO系统能有效引导参观者的视觉注意力，相比纯音频导览显著提升了用户参与度", "conclusion": "具有伴随语音动作的导览机器人能够有效引导视觉注意力并增强用户参与度，为博物馆导览提供了新的交互范式"}}
{"id": "2512.05386", "pdf": "https://arxiv.org/pdf/2512.05386", "abs": "https://arxiv.org/abs/2512.05386", "authors": ["Jakub Kopko", "David Graber", "Saltuk Mustafa Eyrilmez", "Stanislav Mazurenko", "David Bednar", "Jiri Sedlar", "Josef Sivic"], "title": "Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 6 figures, submitted to NeurIPS 2025 AI4Science Workshop", "summary": "As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.", "AI": {"tldr": "评估可学习蛋白质-配体评分函数在未见靶标上的泛化能力，揭示标准基准测试无法反映真实泛化挑战", "motivation": "随着机器学习在分子设计中日益重要，需要确保可学习评分函数在新型蛋白质靶标上的可靠性。虽然许多评分函数在标准基准上表现良好，但其在训练数据之外的泛化能力仍面临重大挑战", "method": "在模拟有限已知结构和实验亲和力测量的靶标数据集划分上评估最先进评分函数的泛化能力；研究大规模自监督预训练是否能弥合泛化差距；探索利用有限测试靶标数据改进评分函数性能的简单方法", "result": "分析显示常用基准测试不能反映泛化到新型靶标的真实挑战；提供了大规模自监督预训练潜力的初步证据；研究了利用有限测试数据改进性能的方法", "conclusion": "研究结果强调了需要更严格的评估协议，并为设计具有扩展到新型蛋白质靶标预测能力的评分函数提供了实用指导"}}
{"id": "2512.05385", "pdf": "https://arxiv.org/pdf/2512.05385", "abs": "https://arxiv.org/abs/2512.05385", "authors": ["Yingjie Xia", "Tao Liu", "Jinglei Shi", "Qingsong Xie", "Heng Guo", "Jian Yang", "Xi Wang"], "title": "ShaRP: SHAllow-LayeR Pruning for Video Large Language Models Acceleration", "categories": ["cs.CV"], "comment": null, "summary": "Video Large Language Models (VLLMs) face the challenge of high computational load during the pre-filling stage due to the processing of an enormous number of visual tokens. Although attention-based pruning methods are widely used to accelerate inference, trials at early decoder layers often result in significant performance degradation, especially under high compression rates. We argue that while attention-based pruning inherently holds the potential to identify the most relevant visual tokens, its effectiveness in shallow decoder layers is limited by factors such as positional encoding bias and insufficient information interaction. In this paper, we propose an improved attention-based pruning framework, termed ShaRP, that integrates segment-aware causal masking, positional debiasing, and token deduplication for enhanced token selection. It enables effective pruning at shallow layers while maintaining stable performance under high compression rates without retraining. Extensive experiments demonstrate that ShaRP achieves competitive performance across multiple video understanding benchmarks, establishing a new paradigm for accelerating VLLM inference.", "AI": {"tldr": "提出ShaRP框架，通过浅层解码器剪枝加速视频大语言模型的推理，解决预填充阶段视觉令牌过多导致的高计算负载问题", "motivation": "视频大语言模型在预填充阶段需要处理大量视觉令牌，计算负载高；现有的基于注意力的剪枝方法在浅层解码器使用时性能下降严重，尤其是在高压缩率下", "method": "提出改进的基于注意力的剪枝框架ShaRP，整合了段感知因果掩码、位置去偏和令牌去重三个组件，增强令牌选择能力", "result": "在多个视频理解基准测试中取得了有竞争力的性能，在高压缩率下保持稳定性能，无需重新训练", "conclusion": "ShaRP为加速VLLM推理建立了新范式，能够在浅层实现有效剪枝，解决了现有方法在浅层性能下降的问题"}}
{"id": "2512.05383", "pdf": "https://arxiv.org/pdf/2512.05383", "abs": "https://arxiv.org/abs/2512.05383", "authors": ["Mara Downing", "Matthew Peng", "Jacob Granley", "Michael Beyeler", "Tevfik Bultan"], "title": "Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation", "categories": ["cs.SE", "cs.AI"], "comment": "20 pages, 4 figures, 2 tables", "summary": "Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.", "AI": {"tldr": "提出一种基于覆盖引导模糊测试的自动化方法，用于检测机器学习驱动的神经刺激系统中的不安全刺激模式", "motivation": "机器学习模型越来越多地用于神经假体设备中生成电刺激模式，虽然这些模型提供了精确和个性化的控制，但当模型输出直接传递到神经组织时，也引入了新的安全风险。需要系统化、量化的方法来检测和表征ML驱动神经刺激系统中的不安全刺激模式。", "method": "将覆盖引导模糊测试这一自动化软件测试技术应用于神经刺激领域。通过扰动模型输入并追踪产生的刺激是否违反电荷密度、瞬时电流或电极共激活等生物物理限制。该框架将编码器视为黑盒，并使用覆盖度量来引导探索，量化测试用例在可能输出空间和违规类型上的覆盖广度。", "result": "应用于视网膜和皮层深度刺激编码器时，该方法系统地揭示了超出既定安全限制的多样化刺激机制。两种违规输出覆盖度量识别出最高数量和多样性的不安全输出，使得能够在不同架构和训练策略之间进行可解释的比较。", "conclusion": "违规导向的模糊测试将安全评估重新定义为经验性、可重复的过程。通过将安全性从训练启发式转变为部署模型的可测量属性，为下一代神经接口的循证基准测试、监管准备和伦理保证奠定了基础。"}}
{"id": "2512.05379", "pdf": "https://arxiv.org/pdf/2512.05379", "abs": "https://arxiv.org/abs/2512.05379", "authors": ["Taslim Mahbub", "Shi Feng"], "title": "Mitigating Self-Preference by Authorship Obfuscation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.", "AI": {"tldr": "研究通过作者身份混淆来缓解语言模型评估中的自我偏好偏见", "motivation": "语言模型评估器存在自我偏好偏见，即倾向于选择自己生成的答案而非其他模型或人类的答案，这种偏见难以消除，因为前沿语言模型能够识别自己的输出", "method": "采用黑盒扰动方法对评估候选文本进行修改，通过同义词替换等简单扰动来混淆作者身份，减少模型对自己输出的识别能力", "result": "简单的同义词替换能有效减少自我偏好，但当尝试完全消除文本风格差异时，自我偏好会重新出现，表明自我识别和自我偏好发生在多个语义层面", "conclusion": "作者身份混淆能初步缓解自我偏好偏见，但完全消除该偏见仍面临挑战，因为语言模型能在多个语义层面识别自己的输出风格"}}
{"id": "2512.05377", "pdf": "https://arxiv.org/pdf/2512.05377", "abs": "https://arxiv.org/abs/2512.05377", "authors": ["Honglu Sun", "Hao Jing", "Zhixiang Dai", "Sa Xiao", "Wei Xue", "Jian Sun", "Qifeng Lu"], "title": "China Regional 3km Downscaling Based on Residual Corrective Diffusion Model", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.", "AI": {"tldr": "基于残差校正扩散模型的中国区域3公里降尺度方法研究", "motivation": "数值天气预报中高效生成高分辨率预报是一个基本挑战，传统方法包括动力降尺度和统计降尺度。深度学习为统计降尺度提供了强大工具，特别是扩散模型和生成对抗网络等超分辨率模型。本研究旨在解决更大区域（约20倍于原研究区域）和多层次变量（包括6个气压层）的降尺度问题。", "method": "采用基于扩散的降尺度框架CorrDiff，并在原始方法基础上添加全局残差连接以提高精度。将训练好的模型应用于中国气象局CMA-GFS的25公里全球网格预报和基于球面傅里叶神经算子的SFF数据驱动模型，生成中国区域3公里预报。", "result": "实验结果表明，该方法降尺度后的预报在目标变量的MAE方面普遍优于CMA-MESO直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比确定性回归模型产生更真实的预测。", "conclusion": "基于残差校正扩散模型的降尺度方法能够有效生成中国区域3公里高分辨率天气预报，在多个变量上优于传统高分辨率区域模型，展示了生成模型在气象降尺度应用中的潜力。"}}
{"id": "2512.05374", "pdf": "https://arxiv.org/pdf/2512.05374", "abs": "https://arxiv.org/abs/2512.05374", "authors": ["Charlie Summers", "Haneen Mohammed", "Eugene Wu"], "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control", "categories": ["cs.CR", "cs.AI", "cs.DB"], "comment": "7 pages, 7 figures, CIDR 2026", "summary": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.", "AI": {"tldr": "提出数据流控制（DFC）框架来解决LLM代理工作流中的数据安全和管理问题", "motivation": "LLM代理在执行复杂任务时存在政策违规、流程腐败和安全漏洞等风险，当前代理工作流缺乏对不良数据流的可见性和管理机制", "method": "开发可移植的DBMS数据流控制实例，将数据验证和访问控制从应用层转移到系统层，为代理生态系统提供原生DFC支持", "result": "提出了DFC框架的早期实现，并规划了面向代理生态系统的数据流控制研究议程", "conclusion": "需要在系统层面原生支持数据流控制，以解决LLM代理工作流中的数据安全和管理挑战，类似于数据库管理系统将数据控制从应用层转移到系统层的历史演进"}}
{"id": "2512.05373", "pdf": "https://arxiv.org/pdf/2512.05373", "abs": "https://arxiv.org/abs/2512.05373", "authors": ["Lijinghua Zhang", "Hengrui Cai"], "title": "Text Rationalization for Robust Causal Effect Estimation", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ME", "stat.ML"], "comment": null, "summary": "Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.", "AI": {"tldr": "提出CATR框架，通过文本合理化解决因果效应估计中的挑战，选择稀疏的必要token子集来保留足够的混杂信息，同时缓解观测层面的正性假设违反问题。", "motivation": "文本数据在因果推断中的应用日益增多，但高维文本特征会带来独特挑战：正性假设在观测层面经常被违反，冗余或虚假的文本特征会膨胀维度，导致极端倾向得分、不稳定权重和效应估计方差膨胀。", "method": "提出混杂感知的token合理化（CATR）框架，使用残差独立性诊断来选择稀疏的必要token子集，旨在保留足够的混杂信息以保证无混杂性，同时丢弃无关文本而保留关键信号。", "result": "在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法产生了更准确、稳定和可解释的因果效应估计。", "conclusion": "CATR框架通过文本合理化有效解决了因果效应估计中的挑战，通过选择稀疏的必要token子集来保留足够的混杂信息，同时缓解观测层面的正性假设违反问题，提高了估计的准确性、稳定性和可解释性。"}}
{"id": "2512.05371", "pdf": "https://arxiv.org/pdf/2512.05371", "abs": "https://arxiv.org/abs/2512.05371", "authors": ["Changwen Xing", "SamZaak Wong", "Xinlai Wan", "Yanfeng Lu", "Mengli Zhang", "Zebin Ma", "Lei Qi", "Zhengxiong Li", "Nan Guan", "Zhe Jiang", "Xi Wang", "Jun Yang"], "title": "ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications", "categories": ["cs.AI", "cs.AR"], "comment": "Accepted by the AAAl26 Conference Main Track", "summary": "While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).", "AI": {"tldr": "提出ChipMind框架，通过知识图谱增强检索机制解决大语言模型在长上下文集成电路设计规范中的推理限制", "motivation": "现有大语言模型在集成电路开发自动化中存在上下文窗口限制，现有上下文扩展方法难以对复杂电路规范进行有效的语义建模和多跳推理", "method": "1) 将电路规范转换为领域特定知识图谱ChipKG；2) 采用ChipKG增强推理机制，结合信息论自适应检索和意图感知语义过滤，动态追踪逻辑依赖并修剪无关噪声", "result": "在工业级规范推理基准测试中显著优于现有方法，平均提升34.59%（最高达72.73%）", "conclusion": "ChipMind填补了LLM辅助硬件设计学术研究与实际工业部署之间的关键空白"}}
{"id": "2512.05365", "pdf": "https://arxiv.org/pdf/2512.05365", "abs": "https://arxiv.org/abs/2512.05365", "authors": ["Zag ElSayed", "Craig Erickson", "Ernest Pedapati"], "title": "MCP-AI: Protocol-Driven Intelligence Framework for Autonomous Reasoning in Healthcare", "categories": ["cs.AI", "q-bio.QM"], "comment": "6 pages, 4 figures", "summary": "Healthcare AI systems have historically faced challenges in merging contextual reasoning, long-term state management, and human-verifiable workflows into a cohesive framework. This paper introduces a completely innovative architecture and concept: combining the Model Context Protocol (MCP) with a specific clinical application, known as MCP-AI. This integration allows intelligent agents to reason over extended periods, collaborate securely, and adhere to authentic clinical logic, representing a significant shift away from traditional Clinical Decision Support Systems (CDSS) and prompt-based Large Language Models (LLMs). As healthcare systems become more complex, the need for autonomous, context-aware clinical reasoning frameworks has become urgent. We present MCP-AI, a novel architecture for explainable medical decision-making built upon the Model Context Protocol (MCP) a modular, executable specification for orchestrating generative and descriptive AI agents in real-time workflows. Each MCP file captures clinical objectives, patient context, reasoning state, and task logic, forming a reusable and auditable memory object. Unlike conventional CDSS or stateless prompt-based AI systems, MCP-AI supports adaptive, longitudinal, and collaborative reasoning across care settings. MCP-AI is validated through two use cases: (1) diagnostic modeling of Fragile X Syndrome with comorbid depression, and (2) remote coordination for Type 2 Diabetes and hypertension. In either scenario, the protocol facilitates physician-in-the-loop validation, streamlines clinical processes, and guarantees secure transitions of AI responsibilities between healthcare providers. The system connects with HL7/FHIR interfaces and adheres to regulatory standards, such as HIPAA and FDA SaMD guidelines. MCP-AI provides a scalable basis for interpretable, composable, and safety-oriented AI within upcoming clinical environments.", "AI": {"tldr": "MCP-AI是一个基于模型上下文协议（MCP）的创新医疗AI框架，将协议驱动架构与临床推理相结合，实现自主、可解释的医疗决策支持。", "motivation": "传统医疗AI系统在上下文推理、长期状态管理和可验证工作流整合方面存在挑战，需要能够支持自适应、纵向和协作推理的框架，以满足复杂医疗系统的需求。", "method": "提出MCP-AI架构，基于模型上下文协议（MCP）构建模块化、可执行的规范，通过MCP文件捕获临床目标、患者上下文、推理状态和任务逻辑，形成可重用和可审计的内存对象。", "result": "通过两个用例验证：1）脆性X综合征伴抑郁的诊断建模；2）2型糖尿病和高血压的远程协调。系统支持医生在环验证，简化临床流程，并保证AI责任在医疗提供者之间的安全转移。", "conclusion": "MCP-AI为即将到来的临床环境提供了一个可扩展、可解释、可组合且以安全为导向的AI基础，代表了从传统临床决策支持系统和基于提示的LLM系统的重大转变。"}}
{"id": "2512.05362", "pdf": "https://arxiv.org/pdf/2512.05362", "abs": "https://arxiv.org/abs/2512.05362", "authors": ["Sanchit Kaul", "Joseph Luna", "Shray Arora"], "title": "PoolNet: Deep Learning for 2D to 3D Video Process Validation", "categories": ["cs.CV", "cs.LG"], "comment": "All code related to this paper can be found at https://github.com/sanchitkaul/PoolNet.git", "summary": "Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.", "AI": {"tldr": "PoolNet是一个用于2D到3D视频处理验证的深度学习框架，能够有效区分适合SfM处理的场景和不适合的场景", "motivation": "从序列和非序列图像数据中提取运动恢复结构信息耗时且计算成本高，同时大多数公开数据因相机姿态变化不足、场景元素遮挡和噪声数据而不适合处理", "method": "提出了PoolNet深度学习框架，用于帧级和场景级的野外数据验证，能够快速评估数据是否适合SfM处理", "result": "模型成功区分了适合SfM处理的场景和不适合的场景，同时显著缩短了获取运动恢复结构数据所需的时间", "conclusion": "PoolNet提供了一个高效的深度学习解决方案，能够快速验证2D到3D视频处理的数据质量，解决了传统方法耗时且计算成本高的问题"}}
{"id": "2512.05359", "pdf": "https://arxiv.org/pdf/2512.05359", "abs": "https://arxiv.org/abs/2512.05359", "authors": ["Zekai Shao", "Yufan Hu", "Jingyuan Liu", "Bin Fan", "Hongmin Liu"], "title": "Group Orthogonal Low-Rank Adaptation for RGB-T Tracking", "categories": ["cs.CV"], "comment": "13 pages, 8 figures. Accepted by AAAI 2026. Extended version", "summary": "Parameter-efficient fine-tuning has emerged as a promising paradigm in RGB-T tracking, enabling downstream task adaptation by freezing pretrained parameters and fine-tuning only a small set of parameters. This set forms a rank space made up of multiple individual ranks, whose expressiveness directly shapes the model's adaptability. However, quantitative analysis reveals low-rank adaptation exhibits significant redundancy in the rank space, with many ranks contributing almost no practical information. This hinders the model's ability to learn more diverse knowledge to address the various challenges in RGB-T tracking. To address this issue, we propose the Group Orthogonal Low-Rank Adaptation (GOLA) framework for RGB-T tracking, which effectively leverages the rank space through structured parameter learning. Specifically, we adopt a rank decomposition partitioning strategy utilizing singular value decomposition to quantify rank importance, freeze crucial ranks to preserve the pretrained priors, and cluster the redundant ranks into groups to prepare for subsequent orthogonal constraints. We further design an inter-group orthogonal constraint strategy. This constraint enforces orthogonality between rank groups, compelling them to learn complementary features that target diverse challenges, thereby alleviating information redundancy. Experimental results demonstrate that GOLA effectively reduces parameter redundancy and enhances feature representation capabilities, significantly outperforming state-of-the-art methods across four benchmark datasets and validating its effectiveness in RGB-T tracking tasks.", "AI": {"tldr": "提出了一种用于RGB-T跟踪的组正交低秩适应框架（GOLA），通过结构化参数学习有效利用秩空间，减少参数冗余并增强特征表示能力。", "motivation": "现有参数高效微调方法在RGB-T跟踪中存在秩空间冗余问题，许多秩几乎不提供实际信息，限制了模型学习多样化知识以应对各种挑战的能力。", "method": "采用奇异值分解的秩分解分区策略量化秩重要性，冻结关键秩以保留预训练先验，将冗余秩聚类为组，并设计组间正交约束策略，强制秩组学习互补特征。", "result": "在四个基准数据集上的实验结果表明，GOLA有效减少了参数冗余并增强了特征表示能力，显著优于最先进的方法。", "conclusion": "GOLA框架通过结构化参数学习和正交约束有效解决了RGB-T跟踪中的秩空间冗余问题，提高了模型适应性和性能。"}}
{"id": "2512.05356", "pdf": "https://arxiv.org/pdf/2512.05356", "abs": "https://arxiv.org/abs/2512.05356", "authors": ["Jason Weston", "Jakob Foerster"], "title": "AI & Human Co-Improvement for Safer Co-Superintelligence", "categories": ["cs.AI"], "comment": null, "summary": "Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.", "AI": {"tldr": "该论文提出\"AI与人类协同改进\"作为比\"AI自我改进\"更可实现且更安全的目标，旨在通过人类研究者与AI系统的协作实现协同超级智能", "motivation": "当前AI领域的自我改进目标存在危险且难以完全实现，需要寻找更安全、更可实现的发展路径来推动AI研究", "method": "通过人类研究者与AI系统的协作进行AI研究，从构思到实验的完整研究流程中实现协同工作，将人类研究改进纳入循环", "result": "协同改进能够加速AI研究进程，同时通过人机共生赋予AI和人类更安全的超级智能能力", "conclusion": "聚焦于人类研究改进的协同超级智能路径既能更快实现目标，又能确保更高的安全性，是比单纯AI自我改进更优越的发展方向"}}
{"id": "2512.05354", "pdf": "https://arxiv.org/pdf/2512.05354", "abs": "https://arxiv.org/abs/2512.05354", "authors": ["Yang Zheng", "Hao Tan", "Kai Zhang", "Peng Wang", "Leonidas Guibas", "Gordon Wetzstein", "Wang Yifan"], "title": "SplatPainter: Interactive Authoring of 3D Gaussians from 2D Edits via Test-Time Training", "categories": ["cs.CV", "cs.GR"], "comment": "project page https://y-zheng18.github.io/SplatPainter/", "summary": "The rise of 3D Gaussian Splatting has revolutionized photorealistic 3D asset creation, yet a critical gap remains for their interactive refinement and editing. Existing approaches based on diffusion or optimization are ill-suited for this task, as they are often prohibitively slow, destructive to the original asset's identity, or lack the precision for fine-grained control. To address this, we introduce \\ourmethod, a state-aware feedforward model that enables continuous editing of 3D Gaussian assets from user-provided 2D view(s). Our method directly predicts updates to the attributes of a compact, feature-rich Gaussian representation and leverages Test-Time Training to create a state-aware, iterative workflow. The versatility of our approach allows a single architecture to perform diverse tasks, including high-fidelity local detail refinement, local paint-over, and consistent global recoloring, all at interactive speeds, paving the way for fluid and intuitive 3D content authoring.", "AI": {"tldr": "提出SplatPainter方法，通过2D编辑交互式创作3D高斯表示，利用测试时训练实现状态感知的迭代工作流程", "motivation": "3D高斯溅射技术虽然革新了3D资产创建，但缺乏交互式精炼和编辑能力。现有方法速度慢、破坏原始资产特性或缺乏精细控制", "method": "采用状态感知前馈模型，从用户提供的2D视图直接预测紧凑、特征丰富的3D高斯表示属性更新，利用测试时训练创建状态感知的迭代工作流程", "result": "单一架构可执行多样化任务，包括高保真局部细节精炼、局部涂绘和全局一致重新着色，所有操作均在交互速度下完成", "conclusion": "为流畅直观的3D内容创作开辟了新途径，实现了3D高斯资产的连续编辑能力"}}
{"id": "2512.05350", "pdf": "https://arxiv.org/pdf/2512.05350", "abs": "https://arxiv.org/abs/2512.05350", "authors": ["Munazza Zaib", "Wei Wang", "Dulaji Hidellaarachchi", "Isma Farah Siddiqui"], "title": "Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": null, "summary": "Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.", "AI": {"tldr": "本文提出了一种混合方法学框架，用于识别和解决神经多样性女性在软件工程领域面临的独特挑战，包括性别偏见与神经差异交叉带来的多重障碍。", "motivation": "神经多样性女性在软件工程领域面临性别偏见和神经差异的双重挑战，但现有研究尚未系统性地关注这一群体。误诊、掩饰行为以及男性主导的工作文化加剧了她们的职业压力、倦怠和流失率。", "method": "提出混合方法学框架：将InclusiveMag包容性框架与GenderMag走查流程相结合，分为三个阶段：文献综述范围界定、人物角色和分析流程推导、协作研讨会应用。", "result": "通过文献综述将挑战归纳为认知、社交、组织、结构和职业发展五个维度，揭示了误诊/延迟诊断和掩饰行为如何加剧排斥现象。", "conclusion": "研究为后续开发和应用包容性分析方法奠定了基础，旨在支持可操作的变革，改善神经多样性女性在软件工程领域的职业环境。"}}
{"id": "2512.05343", "pdf": "https://arxiv.org/pdf/2512.05343", "abs": "https://arxiv.org/abs/2512.05343", "authors": ["Elisabetta Fedele", "Francis Engelmann", "Ian Huang", "Or Litany", "Marc Pollefeys", "Leonidas Guibas"], "title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://spacecontrol3d.github.io/", "summary": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/", "AI": {"tldr": "SpaceControl是一种无需训练、测试时即可实现3D生成空间控制的方法，支持从粗略几何体到精细网格的多种几何输入，并与预训练生成模型无缝集成。", "motivation": "现有3D生成方法主要依赖文本或图像提示，但这些方式在几何特异性上存在不足：语言描述模糊，图像编辑繁琐。需要一种能提供直观精确几何控制的方法。", "method": "提出训练免费的测试时方法，接受多种几何输入（从粗略几何体到详细网格），通过可控参数在几何保真度和输出真实感之间权衡，无需额外训练即可与预训练生成模型集成。", "result": "广泛的定量评估和用户研究表明，SpaceControl在几何忠实度方面优于基于训练和优化的基线方法，同时保持高视觉质量。提供了交互式用户界面支持在线编辑。", "conclusion": "SpaceControl为3D生成提供了有效的空间控制方法，支持多种几何输入，在几何忠实度和视觉质量之间取得良好平衡，可实际应用于创意工作流程。"}}
{"id": "2512.05342", "pdf": "https://arxiv.org/pdf/2512.05342", "abs": "https://arxiv.org/abs/2512.05342", "authors": ["Saitao Zhang", "Yubiao Luo", "Shiqing Wang", "Pushen Zuo", "Yongxiang Li", "Lunshuai Pan", "Zheng Miao", "Zhong Sun"], "title": "First Demonstration of Second-order Training of Deep Neural Networks with In-memory Analog Matrix Computing", "categories": ["cs.ET", "cs.AR", "cs.NE"], "comment": null, "summary": "Second-order optimization methods, which leverage curvature information, offer faster and more stable convergence than first-order methods such as stochastic gradient descent (SGD) and Adam. However, their practical adoption is hindered by the prohibitively high cost of inverting the second-order information matrix, particularly in large-scale neural network training. Here, we present the first demonstration of a second-order optimizer powered by in-memory analog matrix computing (AMC) using resistive random-access memory (RRAM), which performs matrix inversion (INV) in a single step. We validate the optimizer by training a two-layer convolutional neural network (CNN) for handwritten letter classification, achieving 26% and 61% fewer training epochs than SGD with momentum and Adam, respectively. On a larger task using the same second-order method, our system delivers a 5.88x improvement in throughput and a 6.9x gain in energy efficiency compared to state-of-the-art digital processors. These results demonstrate the feasibility and effectiveness of AMC circuits for second-order neural network training, opening a new path toward energy-efficient AI acceleration.", "AI": {"tldr": "首次演示了使用内存模拟矩阵计算进行深度神经网络二阶训练的方法", "motivation": "二阶优化方法虽然收敛更快更稳定，但在大规模神经网络训练中，二阶信息矩阵求逆的计算成本过高，阻碍了实际应用", "method": "使用基于电阻随机存取存储器（RRAM）的内存模拟矩阵计算（AMC）技术，实现单步矩阵求逆，构建二阶优化器", "result": "训练两层卷积神经网络进行手写字母分类，相比带动量的SGD和Adam分别减少26%和61%的训练轮次；在更大任务上，相比最先进的数字处理器，吞吐量提升5.88倍，能效提升6.9倍", "conclusion": "证明了AMC电路用于二阶神经网络训练的可行性和有效性，为能效型AI加速开辟了新路径"}}
{"id": "2512.05338", "pdf": "https://arxiv.org/pdf/2512.05338", "abs": "https://arxiv.org/abs/2512.05338", "authors": ["Hiroki Hasegawa", "Yukihiko Okada"], "title": "Interaction Tensor Shap", "categories": ["cs.LG", "cs.AI"], "comment": "30 pages", "summary": "Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.", "AI": {"tldr": "提出Interaction Tensor SHAP (IT SHAP)方法，将高阶Shapley交互作用表示为张量网络收缩，在张量列车结构下实现多项式时间和对数深度计算", "motivation": "机器学习模型日益深度和高维化，难以理解单个和组合特征如何影响预测。现有方法无法高效评估高阶交互：Shapley Taylor交互指数需要指数级子集枚举，而当前基于张量的方法仅限于一阶效应", "method": "将Shapley Taylor交互指数重新表述为值张量和权重张量的收缩，假设权重张量具有有限状态张量列车表示和多项式张量列车秩。在张量列车结构化模型和分布张量下，将指数复杂度降低到NC2并行时间", "result": "IT SHAP将STII的指数复杂度Θ(4^n)降低到NC2并行时间，在张量列车结构下实现多项式时间和多对数深度计算", "conclusion": "IT SHAP提供了一个统一、公理化且计算可处理的高维模型主效应和高阶交互作用表述框架，为可扩展的交互感知可解释AI奠定基础"}}
{"id": "2512.05335", "pdf": "https://arxiv.org/pdf/2512.05335", "abs": "https://arxiv.org/abs/2512.05335", "authors": ["Yuxiang Liu", "Shengfan Cao"], "title": "State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning", "categories": ["cs.RO"], "comment": null, "summary": "We study visual domain transfer for end-to-end imitation learning in a realistic and challenging setting where target-domain data are strictly off-policy, expert-free, and scarce. We first provide a theoretical analysis showing that the target-domain imitation loss can be upper bounded by the source-domain loss plus a state-conditional latent KL divergence between source and target observation models. Guided by this result, we propose State- Conditional Adversarial Learning, an off-policy adversarial framework that aligns latent distributions conditioned on system state using a discriminator-based estimator of the conditional KL term. Experiments on visually diverse autonomous driving environments built on the BARC-CARLA simulator demonstrate that SCAL achieves robust transfer and strong sample efficiency.", "AI": {"tldr": "提出一种用于端到端模仿学习的视觉域转移方法，在目标域数据离策略、无专家指导且稀缺的挑战性场景下，通过状态条件对抗学习实现鲁棒的视觉域转移。", "motivation": "研究在现实且具有挑战性的场景下的视觉域转移问题，其中目标域数据严格离策略、无专家指导且稀缺，传统模仿学习方法难以直接应用。", "method": "首先理论分析表明目标域模仿损失可由源域损失加上状态条件潜在KL散度上界约束。基于此提出状态条件对抗学习框架，使用基于判别器的估计器在系统状态条件下对齐源域和目标域的潜在分布。", "result": "在基于BARC-CARLA模拟器构建的视觉多样化自动驾驶环境中的实验表明，SCAL方法实现了鲁棒的域转移和强大的样本效率。", "conclusion": "提出的状态条件对抗学习框架能够有效解决目标域数据离策略、无专家指导且稀缺情况下的视觉域转移问题，为端到端模仿学习提供了实用的解决方案。"}}
{"id": "2512.05334", "pdf": "https://arxiv.org/pdf/2512.05334", "abs": "https://arxiv.org/abs/2512.05334", "authors": ["Samaneh Mohtadi", "Kevin Roitero", "Stefano Mizzaro", "Gianluca Demartini"], "title": "The Effect of Document Summarization on LLM-Based Relevance Judgments", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Relevance judgments are central to the evaluation of Information Retrieval (IR) systems, but obtaining them from human annotators is costly and time-consuming. Large Language Models (LLMs) have recently been proposed as automated assessors, showing promising alignment with human annotations. Most prior studies have treated documents as fixed units, feeding their full content directly to LLM assessors. We investigate how text summarization affects the reliability of LLM-based judgments and their downstream impact on IR evaluation. Using state-of-the-art LLMs across multiple TREC collections, we compare judgments made from full documents with those based on LLM-generated summaries of different lengths. We examine their agreement with human labels, their effect on retrieval effectiveness evaluation, and their influence on IR systems' ranking stability. Our findings show that summary-based judgments achieve comparable stability in systems' ranking to full-document judgments, while introducing systematic shifts in label distributions and biases that vary by model and dataset. These results highlight summarization as both an opportunity for more efficient large-scale IR evaluation and a methodological choice with important implications for the reliability of automatic judgments.", "AI": {"tldr": "研究文档摘要对基于大语言模型的相关性判断的影响及其对信息检索系统评估的影响", "motivation": "人工相关性标注成本高、耗时长，虽然大语言模型作为自动评估者显示出潜力，但先前研究大多将文档作为固定单元直接输入，未研究文本摘要对LLM判断可靠性的影响", "method": "使用最先进的大语言模型在多个TREC数据集上，比较基于完整文档的判断与基于不同长度LLM生成摘要的判断，分析它们与人工标注的一致性、对检索效果评估的影响以及对IR系统排序稳定性的影响", "result": "基于摘要的判断在系统排序稳定性方面与完整文档判断相当，但会引入系统性的标签分布偏移和偏差，这些偏差因模型和数据集而异", "conclusion": "文档摘要既是实现大规模信息检索评估效率提升的机会，也是一个对自动判断可靠性有重要影响的方法学选择"}}
{"id": "2512.05325", "pdf": "https://arxiv.org/pdf/2512.05325", "abs": "https://arxiv.org/abs/2512.05325", "authors": ["Ömer Faruk Akgül", "Yusuf Hakan Kalaycı", "Rajgopal Kannan", "Willie Neiswanger", "Viktor Prasanna"], "title": "LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often \"overthink\": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., \"hmm\", \"wait\") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.", "AI": {"tldr": "LYNX是一种在线早期退出机制，通过将模型的隐藏状态感知转化为置信度控制的停止决策，在推理过程中动态决定何时停止生成，以减少计算浪费并提高准确性。", "motivation": "大型推理模型在处理复杂任务时经常\"过度思考\"：在已经有足够信息给出正确答案后继续推理，这会浪费推理计算资源并可能损害准确性。现有的早期停止方法要么需要额外的采样和启发式方法，要么依赖辅助验证模型，或者只能作为事后分析流程而没有形式化保证。", "method": "LYNX将退出决策附加到自然出现的推理线索（如\"hmm\"、\"wait\"）上，在这些线索标记处使用隐藏状态训练轻量级探针，监督来自强制退出，并通过分割共形预测包装得到的分数以获得对过早退出的分布无关控制。该方法只需在通用数学语料上训练和校准一次探针，即可跨基准、解码温度甚至非数学任务重复使用。", "result": "在三个模型系列（1.5B到32B参数）上，每个基础模型使用单个数学训练的探针都能产生强大的准确性-效率权衡。在GSM8K上，LYNX匹配或改进基线准确性同时减少40-65%的标记；在MATH-500上提高准确性达12个百分点，同时减少35-60%的标记；在AIME 2024上恢复基线准确性并节省超过50%的标记；在非数学基准CommonsenseQA上，零样本转移实现适度准确性提升和高达70%的标记减少。", "conclusion": "LYNX提供了一种有效的在线早期退出机制，相比最先进的早期退出方法，它提供竞争性或更优的帕累托前沿，同时保持完全在线、推理时不需要代理模型，并提供明确的用户可调置信度保证。该方法展示了跨任务和模型规模的强大泛化能力。"}}
{"id": "2512.05323", "pdf": "https://arxiv.org/pdf/2512.05323", "abs": "https://arxiv.org/abs/2512.05323", "authors": ["Adam Lizerbram", "Shane Stevenson", "Iman Khadir", "Matthew Tu", "Samuel S. P. Shen"], "title": "Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition", "categories": ["cs.LG", "cs.AI", "stat.ML", "stat.OT"], "comment": "26 pages, 12 figures", "summary": "Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.", "AI": {"tldr": "测试AI天气预报模型FourCastNetv2在初始条件受噪声扰动下的鲁棒性，特别是针对飓风Florence的预测", "motivation": "评估AI天气预报模型对输入噪声和不确定性的鲁棒性对于极端天气事件（如飓风）的预测可靠性至关重要", "method": "进行两个实验：1) 在ERA5数据集的飓风Florence初始条件中加入不同程度的高斯噪声；2) 使用完全随机的初始条件启动模型，观察模型对无意义输入的反应", "result": "FCNv2在低到中等噪声下能准确保持飓风特征；即使在高噪声下也能保持风暴轨迹和结构，但位置精度下降；模型在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预报", "conclusion": "FCNv2对初始条件噪声表现出良好的鲁棒性，但存在系统性低估风暴强度的倾向；该方法简单且可移植到其他数据驱动的AI天气预报模型"}}
{"id": "2512.05318", "pdf": "https://arxiv.org/pdf/2512.05318", "abs": "https://arxiv.org/abs/2512.05318", "authors": ["Vignesh Kothapalli", "Ata Fatahibaarzi", "Hamed Firooz", "Maziar Sanjabi"], "title": "To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 45 figures, 3 tables", "summary": "Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.", "AI": {"tldr": "研究在元训练中过度使用思维链示例的隐藏成本，并提出CoT-Recipe方法来调节CoT和非CoT示例的比例以提升模型在新任务上的推理性能", "motivation": "思维链提示结合少样本上下文学习显著提升了大型语言模型的推理能力，但在预训练知识不足的新任务上效果不佳。研究发现元训练中过度包含CoT示例会损害性能，特别是在CoT监督有限的情况下", "method": "使用CoT-ICL Lab框架进行受控研究，提出CoT-Recipe方法，通过正式方法调节元训练序列中CoT和非CoT示例的混合比例", "result": "CoT-Recipe方法可以将transformer在新任务上的准确率提升高达300%（即使上下文没有CoT示例），在预训练LLM（Qwen2.5系列）的符号推理任务上观察到准确率提升高达130%", "conclusion": "元训练中过度使用CoT示例存在隐藏成本，通过CoT-Recipe方法精心调节CoT和非CoT示例的比例可以显著提升模型在新任务上的推理性能，特别是在CoT监督有限的情况下"}}
{"id": "2512.05314", "pdf": "https://arxiv.org/pdf/2512.05314", "abs": "https://arxiv.org/abs/2512.05314", "authors": ["Ke Mao", "Timotej Kapus", "Cons T Åhs", "Matteo Marescotti", "Daniel Ip", "Ákos Hajdu", "Sopot Cela", "Aparup Banerjee"], "title": "WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp", "categories": ["cs.SE", "cs.AI"], "comment": "11 pages, 4 figures, 48th International Conference on Software Engineering: Software Engineering in Practice", "summary": "The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes. WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.", "AI": {"tldr": "WhatsCode是一个在WhatsApp大规模工业环境中部署的领域特定AI开发系统，用于提高开发者效率，支持超过20亿用户，处理数百万行代码，并集成到端到端功能开发和DevOps流程中。", "motivation": "学术文献中缺乏关于在合规相关的大规模工业环境中部署AI辅助开发工具的研究，尽管行业采用正在增长。WhatsApp需要解决在服务超过20亿用户的复杂环境中部署AI开发工具的实际挑战。", "method": "WhatsCode从有针对性的隐私自动化演变为自主代理工作流，集成到端到端功能开发和DevOps流程中。系统采用两种稳定的人机协作模式：一键部署高置信度变更（60%情况）和指挥-修订复杂决策（40%情况）。", "result": "在25个月（2023-2025年）的部署中，WhatsCode将自动隐私验证覆盖率从15%提高到53%（3.5倍增长），识别隐私需求，生成超过3000个被接受的代码变更，接受率在不同自动化领域从9%到100%不等。系统提交了692个自动重构/修复变更，711个框架采用，141个功能开发辅助，并在错误分类中保持86%的精确度。", "conclusion": "研究表明，组织因素（如所有权模型、采用动态和风险管理）与技术能力同样重要，对于企业级AI成功至关重要。有效的人机协作（而非完全自动化）驱动可持续的业务影响，为合规相关环境中大规模AI工具部署提供了基于证据的指导。"}}
{"id": "2512.05311", "pdf": "https://arxiv.org/pdf/2512.05311", "abs": "https://arxiv.org/abs/2512.05311", "authors": ["Sadat Shahriar", "Navid Ayoobi", "Arjun Mukherjee"], "title": "The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?", "categories": ["cs.LG", "cs.AI"], "comment": "Published in RANLP 2025", "summary": "With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.", "AI": {"tldr": "评估最先进机器学习模型区分人类与LLM生成科学想法的能力，特别是在多次改写后的检测性能变化", "motivation": "随着LLM在研究领域应用的增加，区分LLM与人类生成的科学想法对于理解LLM研究能力的认知差异变得至关重要。虽然检测LLM生成文本已有研究，但区分科学想法的来源仍是未探索领域", "method": "系统评估SOTA机器学习模型区分人类与LLM生成想法的能力，特别关注连续改写阶段后的检测性能变化，并研究加入研究问题作为上下文信息的影响", "result": "SOTA模型在来源归属方面面临挑战，经过五次连续改写后检测性能平均下降25.4%；加入研究问题作为上下文信息可将检测性能提升最多2.97%；当想法被改写成简化的非专家风格时，检测算法表现最差", "conclusion": "LLM签名在迭代改写过程中会逐渐被侵蚀，特别是在简化改写后更难检测，这为区分人类与LLM生成科学想法带来了挑战，需要更先进的检测方法"}}
{"id": "2512.05310", "pdf": "https://arxiv.org/pdf/2512.05310", "abs": "https://arxiv.org/abs/2512.05310", "authors": ["Brandon Biggs", "David Sloan", "Brett Oppegaard", "Nicholas A. Giudice", "James M. Coughlan", "Bruce N. Walker"], "title": "Systematically Evaluating Equivalent Purpose for Digital Maps", "categories": ["cs.HC"], "comment": "In press at Journal on Technology and Persons with Disabilities, volume 14", "summary": "Digital geographic maps remain largely inaccessible to blind and low-vision individuals (BLVIs), despite global legislation adopting the Web Content Accessibility Guidelines (WCAG). A critical gap exists in defining \"equivalent purpose\" for maps under WCAG Success Criterion 1.1.1, which requires that non-text content provide a text alternative that serves the \"equivalent purpose\". This paper proposes a systematic framework for evaluating map accessibility, called the Map Equivalent-Purpose Framework (MEP Framework), defining purpose through three items (Generalized, Spatial Information, and Spatial Relationships), and establishing 15 measurable criteria for equivalent information communication. Eight text map representations were evaluated against visual map baselines using the proposed MEP Framework. Results show that legacy methods such as tables and turn-by-turn directions fail to meet the MEP Framework criteria, while Audiom Maps, Multi User Domain (MUD) Maps, and Audio Descriptions meet the criteria. The evaluation highlights the necessity of holistic, systematic approaches to ensure non-visual maps convey all generalized spatial information and relationships present in visual maps. The MEP Framework provides a replicable methodology for comprehensively assessing digital map accessibility, clarifying WCAG's \"equivalent purpose\", and guiding compliant and usable map creation. Compliant maps will support BLVIs' participation in map-dependent professions and civic engagement.", "AI": {"tldr": "提出一个系统性的地图无障碍评估框架（MEP框架），用于评估数字地图对盲人和低视力人群的可访问性，特别是定义WCAG准则中\"等效目的\"的具体标准。", "motivation": "数字地理地图对盲人和低视力人群仍然难以访问，尽管全球立法采用了Web内容无障碍指南（WCAG）。WCAG成功标准1.1.1要求非文本内容提供\"等效目的\"的文本替代，但地图领域的\"等效目的\"定义存在关键空白。", "method": "提出地图等效目的框架（MEP框架），通过三个项目（概括性信息、空间信息、空间关系）定义目的，并建立15个可衡量的等效信息沟通标准。使用该框架评估了8种文本地图表示方法相对于视觉地图基线的表现。", "result": "传统方法如表格和逐步导航指示未能满足MEP框架标准，而音频地图、多用户域地图和音频描述满足标准。评估强调需要整体系统方法来确保非视觉地图传达视觉地图中的所有概括性空间信息和关系。", "conclusion": "MEP框架为全面评估数字地图可访问性提供了可复制的方法论，澄清了WCAG的\"等效目的\"概念，并指导符合标准且可用的地图创建。符合标准的地图将支持盲人和低视力人群参与依赖地图的职业和公民参与。"}}
{"id": "2512.05303", "pdf": "https://arxiv.org/pdf/2512.05303", "abs": "https://arxiv.org/abs/2512.05303", "authors": ["Christian Westerdahl", "Jonas Poulsen", "Daniel Holmelund", "Peter Nicholas Hansen", "Fletcher Thompson", "Roberto Galeazzi"], "title": "Seabed-to-Sky Mapping of Maritime Environments with a Dual Orthogonal SONAR and LiDAR Sensor Suite", "categories": ["cs.RO"], "comment": null, "summary": "Critical maritime infrastructure increasingly demands situational awareness both above and below the surface, yet existing ''seabed-to-sky'' mapping pipelines either rely on GNSS (vulnerable to shadowing/spoofing) or expensive bathymetric sonars. We present a unified, GNSS-independent mapping system that fuses LiDAR-IMU with a dual, orthogonally mounted Forward Looking Sonars (FLS) to generate consistent seabed-to-sky maps from an Autonomous Surface Vehicle. On the acoustic side, we extend orthogonal wide-aperture fusion to handle arbitrary inter-sonar translations (enabling heterogeneous, non-co-located models) and extract a leading edge from each FLS to form line-scans. On the mapping side, we modify LIO-SAM to ingest both stereo-derived 3D sonar points and leading-edge line-scans at and between keyframes via motion-interpolated poses, allowing sparse acoustic updates to contribute continuously to a single factor-graph map. We validate the system on real-world data from Belvederekanalen (Copenhagen), demonstrating real-time operation with approx. 2.65 Hz map updates and approx. 2.85 Hz odometry while producing a unified 3D model that spans air-water domains.", "AI": {"tldr": "提出一种GNSS独立的海底到天空统一测绘系统，通过融合LiDAR-IMU和双正交前视声纳，从自主水面航行器生成一致的海底到天空地图", "motivation": "关键海事基础设施需要水面上下全方位态势感知，现有海底到天空测绘流程要么依赖易受遮挡/欺骗的GNSS，要么依赖昂贵的测深声纳，需要一种统一的GNSS独立测绘系统", "method": "1. 声学侧：扩展正交宽孔径融合处理任意声纳间平移，从每个FLS提取前沿形成线扫描；2. 测绘侧：修改LIO-SAM以通过运动插值姿态在关键帧之间摄入立体衍生的3D声纳点和前沿线扫描，允许稀疏声学更新持续贡献到单一因子图地图", "result": "在哥本哈根Belvederekanalen的真实世界数据上验证，系统实时运行，约2.65Hz地图更新和约2.85Hz里程计，同时生成跨越空气-水领域的统一3D模型", "conclusion": "开发了一种统一、GNSS独立的海底到天空测绘系统，通过融合LiDAR-IMU和双正交前视声纳，实现了从自主水面航行器生成一致的海底到天空地图，解决了现有系统依赖GNSS或昂贵设备的问题"}}
{"id": "2512.05300", "pdf": "https://arxiv.org/pdf/2512.05300", "abs": "https://arxiv.org/abs/2512.05300", "authors": ["Yonggang Jiang", "Yaowei Long", "Thatchaphol Saranurak", "Benyu Wang"], "title": "Crude Approximation of Directed Minimum Cut and Arborescence Packing in Almost Linear Time", "categories": ["cs.DS"], "comment": null, "summary": "We give almost-linear-time algorithms for approximating rooted minimum cut and maximum arborescence packing in directed graphs, two problems that are dual to each other [Edm73]. More specifically, for an $n$-vertex, $m$-edge directed graph $G$ whose $s$-rooted minimum cut value is $k$, our first algorithm computes an $s$-rooted cut of size at most $O(k\\log^{5} n)$ in $m^{1+o(1)}$ time, and our second algorithm packs $k$ $s$-rooted arborescences with $n^{o(1)}$ congestion in $m^{1+o(1)}$ time, certifying that the $s$-rooted minimum cut is at least $k / n^{o(1)}$. Our first algorithm also works for weighted graphs. Prior to our work, the fastest algorithms for computing the $s$-rooted minimum cut were exact but had super-linear running time: either $\\tilde{O}(mk)$ [Gab91] or $\\tilde{O}(m^{1+o(1)}\\min\\{\\sqrt{n},n/m^{1/3}\\})$ [CLN+22]. The fastest known algorithms for packing $s$-rooted arborescences had no congestion, but required $\\tilde{O}(m \\cdot \\mathrm{poly}(k))$ time [BHKP08].", "AI": {"tldr": "提出几乎线性时间算法来近似有向图中的根最小割和最大树形图打包问题", "motivation": "现有计算有向图根最小割的精确算法需要超线性时间，而树形图打包算法需要多项式时间，需要更高效的近似算法", "method": "开发两种几乎线性时间算法：1) 计算O(k log⁵ n)大小的根割；2) 打包k个树形图且具有n^o(1)拥塞", "result": "在m^{1+o(1)}时间内实现近似根最小割和树形图打包，显著快于现有精确算法", "conclusion": "首次实现了有向图根最小割和树形图打包问题的几乎线性时间近似算法，解决了计算效率瓶颈"}}
{"id": "2512.05299", "pdf": "https://arxiv.org/pdf/2512.05299", "abs": "https://arxiv.org/abs/2512.05299", "authors": ["Ahmad Yehia", "Jiseop Byeon", "Tianyi Wang", "Huihai Wang", "Yiming Xu", "Junfeng Jiao", "Christian Claudel"], "title": "ARCAS: An Augmented Reality Collision Avoidance System with SLAM-Based Tracking for Enhancing VRU Safety", "categories": ["eess.SY", "cs.AR", "cs.CV", "cs.ET", "cs.RO", "eess.IV"], "comment": "8 pages, 3 figures, 1 table", "summary": "Vulnerable road users (VRUs) face high collision risks in mixed traffic, yet most existing safety systems prioritize driver or vehicle assistance over direct VRU support. This paper presents ARCAS, a real-time augmented reality collision avoidance system that provides personalized spatial alerts to VRUs via wearable AR headsets. By fusing roadside 360-degree 3D LiDAR with SLAM-based headset tracking and an automatic 3D calibration procedure, ARCAS accurately overlays world-locked 3D bounding boxes and directional arrows onto approaching hazards in the user's passthrough view. The system also enables multi-headset coordination through shared world anchoring. Evaluated in real-world pedestrian interactions with e-scooters and vehicles (180 trials), ARCAS nearly doubled pedestrians' time-to-collision and increased counterparts' reaction margins by up to 4x compared to unaided-eye conditions. Results validate the feasibility and effectiveness of LiDAR-driven AR guidance and highlight the potential of wearable AR as a promising next-generation safety tool for urban mobility.", "AI": {"tldr": "ARCAS是一个增强现实碰撞避免系统，通过可穿戴AR头显为弱势道路使用者提供实时空间警报，融合路边3D LiDAR和SLAM跟踪技术，在混合交通环境中提升行人安全。", "motivation": "弱势道路使用者在混合交通中面临高碰撞风险，现有安全系统主要关注驾驶员或车辆辅助，缺乏直接支持弱势道路使用者的解决方案。", "method": "系统融合路边360度3D LiDAR与基于SLAM的头显跟踪技术，采用自动3D校准程序，在用户透视视图中准确叠加世界锁定的3D边界框和方向箭头，并通过共享世界锚定实现多头显协调。", "result": "在真实世界行人-电动滑板车/车辆交互的180次试验中，ARCAS几乎使行人的碰撞时间翻倍，并将对应方的反应裕度提高高达4倍，相比无辅助条件有显著改善。", "conclusion": "研究验证了LiDAR驱动的AR引导的可行性和有效性，突显了可穿戴AR作为下一代城市移动安全工具的潜力。"}}
{"id": "2512.05297", "pdf": "https://arxiv.org/pdf/2512.05297", "abs": "https://arxiv.org/abs/2512.05297", "authors": ["Xianglong Hou", "Xinquan Huang", "Paris Perdikaris"], "title": "CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators", "categories": ["cs.LG", "cs.AI", "math.NA"], "comment": null, "summary": "Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.", "AI": {"tldr": "提出Continuous Flow Operator (CFO)框架，通过流匹配学习连续时间偏微分方程动力学，避免自回归预测的误差累积问题，实现时间分辨率不变性", "motivation": "传统神经算子方法采用自回归预测方案，在长时间推演中会累积误差，且需要均匀时间离散化。现有连续方法（如神经ODE）计算负担重。需要一种既能学习连续时间PDE动力学又计算高效的方法", "method": "将流匹配重新用于直接学习PDE的右侧项，无需通过ODE求解器反向传播。对轨迹数据拟合时间样条，在节点处使用时差估计构造概率路径，其速度场近似真实PDE动力学。通过流匹配训练神经算子预测这些解析速度场", "result": "在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO表现出优越的长期稳定性和显著的数据效率。仅使用25%不规则子采样时间点训练的CFO优于使用完整数据训练的自回归基线，相对误差减少高达87%。推理时仅需50%的函数评估即可超越基线，同时支持反向时间推理和任意时间查询", "conclusion": "CFO框架成功解决了传统神经算子在时间依赖PDE中的误差累积和固定时间分辨率问题，通过流匹配实现了连续时间动力学学习，在保持计算效率的同时提供了时间分辨率不变性、反向推理和任意时间查询能力"}}
{"id": "2512.05292", "pdf": "https://arxiv.org/pdf/2512.05292", "abs": "https://arxiv.org/abs/2512.05292", "authors": ["Fan Zhang", "Jinfeng Chen", "Joseph J. B. Mvogo Ahanda", "Hanz Richter", "Ge Lv", "Bin Hu", "Qin Lin"], "title": "Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "In commercial robotic systems, it is common to encounter a closed inner-loop torque controller that is not user-modifiable. However, the outer-loop controller, which sends kinematic commands such as position or velocity for the inner-loop controller to track, is typically exposed to users. In this work, we focus on the development of an easily integrated add-on at the outer-loop layer by combining disturbance rejection control and robust control barrier function for high-performance tracking and safe control of the whole dynamic system of an industrial manipulator. This is particularly beneficial when 1) the inner-loop controller is imperfect, unmodifiable, and uncertain; and 2) the dynamic model exhibits significant uncertainty. Stability analysis, formal safety guarantee proof, and hardware experiments with a PUMA robotic manipulator are presented. Our solution demonstrates superior performance in terms of simplicity of implementation, robustness, tracking precision, and safety compared to the state of the art. Video: https://youtu.be/zw1tanvrV8Q", "AI": {"tldr": "为具有封闭架构的机器人系统开发一种易于集成的外环附加控制器，结合扰动抑制控制和鲁棒控制屏障函数，实现高性能跟踪和动态系统安全控制", "motivation": "商业机器人系统通常具有用户无法修改的封闭内环扭矩控制器，而外环控制器（发送位置或速度指令）通常对用户开放。当内环控制器不完善、不可修改且不确定，以及动态模型存在显著不确定性时，需要一种易于集成的高性能安全控制方案", "method": "在外环层结合扰动抑制控制和鲁棒控制屏障函数，开发易于集成的附加控制器。通过稳定性分析、形式化安全保证证明，并在PUMA机器人上进行了硬件实验验证", "result": "相比现有技术，该解决方案在实现简单性、鲁棒性、跟踪精度和安全性方面表现出优越性能。硬件实验验证了方法的有效性", "conclusion": "该研究为具有封闭架构的工业机器人系统提供了一种有效的外环安全控制方案，特别适用于内环控制器不完善且动态模型不确定的情况，实现了高性能跟踪和安全控制"}}
{"id": "2512.05288", "pdf": "https://arxiv.org/pdf/2512.05288", "abs": "https://arxiv.org/abs/2512.05288", "authors": ["Feijiang Han"], "title": "Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.", "AI": {"tldr": "该论文提出了首个系统性的WebShell家族分类自动化研究，通过动态函数调用追踪和行为分析，结合LLM生成变体数据增强，评估多种表示学习方法在细粒度WebShell家族分类任务上的性能。", "motivation": "恶意WebShell对医疗、金融等关键数字基础设施构成持续威胁。当前研究主要集中在检测恶意样本，但需要从被动检测转向深度分析和主动防御。自动化WebShell家族分类能够识别特定恶意软件谱系，理解攻击者战术并实现精准快速响应，但目前这一关键任务仍依赖缓慢的手动专家分析，是一个尚未充分探索的领域。", "method": "1. 提取动态函数调用追踪以捕获抵抗常见加密和混淆的固有行为；2. 使用大型语言模型合成新变体来增强数据集的规模和多样性；3. 将追踪抽象为序列、图和树结构；4. 基准测试全面的表示方法套件，包括经典序列嵌入（CBOW、GloVe）、Transformer模型（BERT、SimCSE）以及结构感知算法（图核、图编辑距离、Graph2Vec、各种图神经网络）；5. 在四个真实世界家族标注数据集上进行监督和无监督设置下的广泛实验。", "result": "通过大量实验建立了稳健的基线，并提供了关于数据抽象、表示模型和学习范式最有效组合的实用见解。研究评估了不同表示学习方法在WebShell家族分类任务上的性能，为这一挑战提供了实践指导。", "conclusion": "该研究首次系统性地探索了WebShell家族分类自动化，填补了从被动检测到主动防御的关键空白。通过动态行为分析、数据增强和全面的表示学习基准测试，为安全社区提供了从检测转向深入分析和快速响应的可行路径，为未来的WebShell威胁分析奠定了基础。"}}
{"id": "2512.05277", "pdf": "https://arxiv.org/pdf/2512.05277", "abs": "https://arxiv.org/abs/2512.05277", "authors": ["Kevin Cannons", "Saeed Ranjbar Alvar", "Mohammad Asiful Hossain", "Ahmad Rezaei", "Mohsen Gholami", "Alireza Heidarikhazaei", "Zhou Weimin", "Yong Zhang", "Mohammad Akbari"], "title": "From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \\href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \\href{https://github.com/vbdi/tad_bench}{Github}, respectively.", "AI": {"tldr": "该论文提出了一个专门用于自动驾驶场景中时间理解评估的基准测试TAD，并开发了两种无需训练的解决方案来提升视觉语言模型在自动驾驶时间推理任务上的性能。", "motivation": "自动驾驶中的时间理解是一个重要挑战，但现有的时间推理基准主要关注体育、烹饪、电影等其他视频内容，缺乏专门针对自动驾驶第一人称视角独特挑战的评估基准。", "method": "1) 提出了TAD基准测试，包含近6000个问答对，涵盖7个人工设计的任务；2) 提出了两种无需训练的解决方案：Scene-CoT（利用思维链）和TCogMap（结合自我中心时间认知地图）；3) 评估了9个开源和闭源通用模型以及最先进的自动驾驶专用模型。", "result": "当前最先进的模型在TAD基准上表现不佳，主要由于细粒度运动理解不完善。提出的Scene-CoT和TCogMap方法将现有视觉语言模型在TAD上的平均准确率提升了最高17.72%。", "conclusion": "通过引入TAD基准、评估多个最先进模型并提出有效的增强方法，这项工作旨在推动自动驾驶时间理解领域的未来研究，为解决自动驾驶中的时间推理挑战提供了重要工具和方法。"}}
{"id": "2512.05272", "pdf": "https://arxiv.org/pdf/2512.05272", "abs": "https://arxiv.org/abs/2512.05272", "authors": ["Ahmet Berke Gokmen", "Ajad Chhatkuli", "Luc Van Gool", "Danda Pani Paudel"], "title": "Inferring Compositional 4D Scenes without Ever Seeing One", "categories": ["cs.CV"], "comment": "Project page: https://github.com/insait-institute/COM4D", "summary": "Scenes in the real world are often composed of several static and dynamic objects. Capturing their 4-dimensional structures, composition and spatio-temporal configuration in-the-wild, though extremely interesting, is equally hard. Therefore, existing works often focus on one object at a time, while relying on some category-specific parametric shape model for dynamic objects. This can lead to inconsistent scene configurations, in addition to being limited to the modeled object categories. We propose COM4D (Compositional 4D), a method that consistently and jointly predicts the structure and spatio-temporal configuration of 4D/3D objects using only static multi-object or dynamic single object supervision. We achieve this by a carefully designed training of spatial and temporal attentions on 2D video input. The training is disentangled into learning from object compositions on the one hand, and single object dynamics throughout the video on the other, thus completely avoiding reliance on 4D compositional training data. At inference time, our proposed attention mixing mechanism combines these independently learned attentions, without requiring any 4D composition examples. By alternating between spatial and temporal reasoning, COM4D reconstructs complete and persistent 4D scenes with multiple interacting objects directly from monocular videos. Furthermore, COM4D provides state-of-the-art results in existing separate problems of 4D object and composed 3D reconstruction despite being purely data-driven.", "AI": {"tldr": "提出COM4D方法，无需4D组合训练数据，从单目视频中重建包含多个交互对象的完整4D场景", "motivation": "现实世界场景通常包含多个静态和动态对象，但现有方法往往一次只关注一个对象，依赖特定类别的参数化形状模型，导致场景配置不一致且受限于建模对象类别", "method": "通过精心设计的空间和时间注意力机制在2D视频输入上进行训练，将学习解耦为对象组合学习和单个对象动态学习，避免对4D组合训练数据的依赖，推理时通过注意力混合机制结合独立学习的注意力", "result": "COM4D能够从单目视频中重建完整且持久的4D场景，包含多个交互对象，并在4D对象重建和组合3D重建等现有分离问题上达到最先进的结果", "conclusion": "COM4D方法通过解耦的空间和时间注意力训练，无需4D组合训练数据即可一致地预测4D/3D对象的结构和时空配置，为4D场景理解提供了新的解决方案"}}
{"id": "2512.05270", "pdf": "https://arxiv.org/pdf/2512.05270", "abs": "https://arxiv.org/abs/2512.05270", "authors": ["Tianyi Wang", "Jiseop Byeon", "Ahmad Yehia", "Huihai Wang", "Yiming Xu", "Tianyi Zeng", "Ziran Wang", "Junfeng Jiao", "Christian Claudel"], "title": "XR-DT: Extended Reality-Enhanced Digital Twin for Agentic Mobile Robots", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "eess.SY"], "comment": "10 pages, 5 figures", "summary": "As mobile robots increasingly operate alongside humans in shared workspaces, ensuring safe, efficient, and interpretable Human-Robot Interaction (HRI) has become a pressing challenge. While substantial progress has been devoted to human behavior prediction, limited attention has been paid to how humans perceive, interpret, and trust robots' inferences, impeding deployment in safety-critical and socially embedded environments. This paper presents XR-DT, an eXtended Reality-enhanced Digital Twin framework for agentic mobile robots, that bridges physical and virtual spaces to enable bi-directional understanding between humans and robots. Our hierarchical XR-DT architecture integrates virtual-, augmented-, and mixed-reality layers, fusing real-time sensor data, simulated environments in the Unity game engine, and human feedback captured through wearable AR devices. Within this framework, we design an agentic mobile robot system with a unified diffusion policy for context-aware task adaptation. We further propose a chain-of-thought prompting mechanism that allows multimodal large language models to reason over human instructions and environmental context, while leveraging an AutoGen-based multi-agent coordination layer to enhance robustness and collaboration in dynamic tasks. Initial experimental results demonstrate accurate human and robot trajectory prediction, validating the XR-DT framework's effectiveness in HRI tasks. By embedding human intention, environmental dynamics, and robot cognition into the XR-DT framework, our system enables interpretable, trustworthy, and adaptive HRI.", "AI": {"tldr": "提出XR-DT框架，通过扩展现实增强的数字孪生技术，实现人机双向理解，提升移动机器人在共享工作空间中的安全、高效和可解释交互。", "motivation": "移动机器人与人类在共享工作空间中协同工作时，现有研究主要关注人类行为预测，而忽视了人类如何感知、理解和信任机器人的推理能力，这阻碍了在安全关键和社会化环境中的部署。", "method": "采用分层XR-DT架构，整合虚拟现实、增强现实和混合现实层，融合实时传感器数据、Unity引擎模拟环境和通过可穿戴AR设备捕获的人类反馈。设计具有统一扩散策略的自主移动机器人系统，提出链式思维提示机制让多模态大语言模型推理人类指令和环境上下文，并利用基于AutoGen的多智能体协调层增强动态任务中的鲁棒性和协作。", "result": "初步实验结果显示，系统能够准确预测人类和机器人的轨迹，验证了XR-DT框架在人机交互任务中的有效性。", "conclusion": "通过将人类意图、环境动态和机器人认知嵌入XR-DT框架，该系统实现了可解释、可信赖和自适应的人机交互，为安全关键和社会化环境中的机器人部署提供了新途径。"}}
{"id": "2512.05268", "pdf": "https://arxiv.org/pdf/2512.05268", "abs": "https://arxiv.org/abs/2512.05268", "authors": ["Niki Nezakati", "Arnab Ghosh", "Amit Roy-Chowdhury", "Vishwanath Saragadam"], "title": "CARD: Correlation Aware Restoration with Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Denoising diffusion models have achieved state-of-the-art performance in image restoration by modeling the process as sequential denoising steps. However, most approaches assume independent and identically distributed (i.i.d.) Gaussian noise, while real-world sensors often exhibit spatially correlated noise due to readout mechanisms, limiting their practical effectiveness. We introduce Correlation Aware Restoration with Diffusion (CARD), a training-free extension of DDRM that explicitly handles correlated Gaussian noise. CARD first whitens the noisy observation, which converts the noise into an i.i.d. form. Then, the diffusion restoration steps are replaced with noise-whitened updates, which inherits DDRM's closed-form sampling efficiency while now being able to handle correlated noise. To emphasize the importance of addressing correlated noise, we contribute CIN-D, a novel correlated noise dataset captured across diverse illumination conditions to evaluate restoration methods on real rolling-shutter sensor noise. This dataset fills a critical gap in the literature for experimental evaluation with real-world correlated noise. Experiments on standard benchmarks with synthetic correlated noise and on CIN-D demonstrate that CARD consistently outperforms existing methods across denoising, deblurring, and super-resolution tasks.", "AI": {"tldr": "提出CARD方法，一种处理空间相关噪声的图像恢复扩散模型扩展", "motivation": "现有扩散模型假设噪声为独立同分布高斯噪声，但真实传感器噪声常具有空间相关性，限制了实际应用效果", "method": "首先对噪声观测进行白化处理，将相关噪声转换为独立同分布形式，然后用噪声白化更新替换扩散恢复步骤，继承DDRM的闭式采样效率", "result": "在合成相关噪声标准基准和真实相关噪声数据集CIN-D上，CARD在去噪、去模糊和超分辨率任务中均优于现有方法", "conclusion": "CARD通过显式处理相关高斯噪声，扩展了扩散模型在图像恢复中的实用性，同时贡献了CIN-D数据集填补了真实相关噪声评估的空白"}}
{"id": "2512.05267", "pdf": "https://arxiv.org/pdf/2512.05267", "abs": "https://arxiv.org/abs/2512.05267", "authors": ["Osvaldo Simeone", "Yaniv Romano"], "title": "Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective", "categories": ["cs.IT", "cs.AI", "cs.LG"], "comment": null, "summary": "In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.", "AI": {"tldr": "本文从信息论角度综述了数据受限场景下人工智能系统的两大核心挑战：认知不确定性的量化与数据稀缺的缓解方法。", "motivation": "在机器人、通信、医疗等特定领域应用中，AI系统常面临训练数据有限的问题，这导致认知不确定性（可减少的不确定性），从而限制预测性能。需要系统性的方法来处理数据受限场景。", "method": "1) 广义贝叶斯学习框架：通过模型参数空间的广义后验量化认知不确定性；2) 信息论泛化边界：形式化训练数据量与预测不确定性的关系；3) 有限样本统计保证方法：包括共形预测和共形风险控制；4) 数据效率提升方法：结合有限标注数据与模型预测或合成数据。", "result": "本文系统梳理了从信息论角度处理数据受限AI问题的理论框架和方法论，为量化认知不确定性和缓解数据稀缺提供了完整的理论视角和实践指导。", "conclusion": "信息论为理解数据受限AI系统中的认知不确定性和数据效率提供了统一的理论框架，通过广义贝叶斯学习、泛化边界理论、有限样本保证方法以及数据增强技术，可以有效应对训练数据稀缺的挑战。"}}
{"id": "2512.05259", "pdf": "https://arxiv.org/pdf/2512.05259", "abs": "https://arxiv.org/abs/2512.05259", "authors": ["Georgios Chatzichristodoulou", "Niki Efthymiou", "Panagiotis Filntisis", "Georgios Pavlakos", "Petros Maragos"], "title": "Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization", "categories": ["cs.CV"], "comment": null, "summary": "While three-dimensional (3D) shape and pose estimation is a highly researched area that has yielded significant advances, the resulting methods, despite performing well for the adult population, generally fail to generalize effectively to children and infants. This paper addresses this challenge by introducing AionHMR, a comprehensive framework designed to bridge this domain gap. We propose an optimization-based method that extends a top-performing model by incorporating the SMPL-A body model, enabling the concurrent and accurate modeling of adults, children, and infants. Leveraging this approach, we generated pseudo-ground-truth annotations for publicly available child and infant image databases. Using these new training data, we then developed and trained a specialized transformer-based deep learning model capable of real-time 3D age-inclusive human reconstruction. Extensive experiments demonstrate that our methods significantly improve shape and pose estimation for children and infants without compromising accuracy on adults. Importantly, our reconstructed meshes serve as privacy-preserving substitutes for raw images, retaining essential action, pose, and geometry information while enabling anonymized datasets release. As a demonstration, we introduce the 3D-BabyRobot dataset, a collection of action-preserving 3D reconstructions of children interacting with robots. This work bridges a crucial domain gap and establishes a foundation for inclusive, privacy-aware, and age-diverse 3D human modeling.", "AI": {"tldr": "提出AionHMR框架，解决现有3D人体网格恢复方法在儿童和婴儿上泛化能力不足的问题，实现年龄包容性的3D人体重建，并用于动作保持的数据匿名化", "motivation": "现有3D人体形状和姿态估计方法在成人群体上表现良好，但在儿童和婴儿上泛化效果差，存在领域差距，需要开发年龄包容性的解决方案", "method": "1) 提出基于优化的方法，扩展现有顶级模型，结合SMPL-A身体模型，同时准确建模成人、儿童和婴儿；2) 为公开的儿童和婴儿图像数据库生成伪地面真值标注；3) 基于新训练数据开发专门的基于transformer的深度学习模型，实现实时3D年龄包容性人体重建", "result": "方法显著提高了儿童和婴儿的形状和姿态估计准确性，同时不损害成人的准确性；重建的网格可作为原始图像的隐私保护替代品，保留动作、姿态和几何信息，支持匿名数据集发布；创建了3D-BabyRobot数据集", "conclusion": "该工作填补了关键领域差距，为包容性、隐私保护和年龄多样化的3D人体建模奠定了基础，实现了动作保持的数据匿名化"}}
{"id": "2512.05257", "pdf": "https://arxiv.org/pdf/2512.05257", "abs": "https://arxiv.org/abs/2512.05257", "authors": ["Bychkov Oleksii", "Bychkova Sophia", "Lytvynchuk Khrystyna"], "title": "Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence", "categories": ["cs.AI"], "comment": "9 pages", "summary": "This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.", "AI": {"tldr": "该论文提出可能性理论作为解决Dempster-Shafer理论悖论的基础，为可靠人工智能提供数学严谨的不确定性处理框架", "motivation": "解决Dempster-Shafer理论中的Zadeh悖论危机，为人工智能中的不确定性推理提供逻辑一致且数学严谨的基础", "method": "采用Bychkov文章中的公理化可能性理论方法，基于可能性和必要性度量的对偶装置，从零开始构建逻辑一致的不确定性处理框架", "result": "通过经典医疗诊断困境的案例分析，证明可能性理论能够正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑", "conclusion": "可能性理论不仅是替代方案，而是为DST悖论提供了根本性解决方案，为可靠人工智能奠定了理论基础"}}
{"id": "2512.05247", "pdf": "https://arxiv.org/pdf/2512.05247", "abs": "https://arxiv.org/abs/2512.05247", "authors": ["Spencer Gibson", "Yun William Yu"], "title": "Incorporating indel channels into average-case analysis of seed-chain-extend", "categories": ["cs.DS", "q-bio.QM"], "comment": "25 pages (10 page main text + 2 page biblio + 13 page appendix); conference submission", "summary": "Given a sequence $s_1$ of $n$ letters drawn i.i.d. from an alphabet of size $σ$ and a mutated substring $s_2$ of length $m < n$, we often want to recover the mutation history that generated $s_2$ from $s_1$. Modern sequence aligners are widely used for this task, and many employ the seed-chain-extend heuristic with $k$-mer seeds. Previously, Shaw and Yu showed that optimal linear-gap cost chaining can produce a chain with $1 - O\\left(\\frac{1}{\\sqrt{m}}\\right)$ recoverability, the proportion of the mutation history that is recovered, in $O\\left(mn^{2.43θ} \\log n\\right)$ expected time, where $θ< 0.206$ is the mutation rate under a substitution-only channel and $s_1$ is assumed to be uniformly random. However, a gap remains between theory and practice, since real genomic data includes insertions and deletions (indels), and yet seed-chain-extend remains effective. In this paper, we generalize those prior results by introducing mathematical machinery to deal with the two new obstacles introduced by indel channels: the dependence of neighboring anchors and the presence of anchors that are only partially correct. We are thus able to prove that the expected recoverability of an optimal chain is $\\ge 1 - O\\Bigl(\\frac{1}{\\sqrt{m}}\\Bigr)$ and the expected runtime is $O(mn^{3.15 \\cdot θ_T}\\log n)$, when the total mutation rate given by the sum of the substitution, insertion, and deletion mutation rates ($θ_T = θ_i + θ_d + θ_s$) is less than $0.159$.", "AI": {"tldr": "将indel通道纳入seed-chain-extend平均情况分析，证明在包含插入和删除的突变模型下，最优链的期望恢复率接近1，并给出时间复杂度分析", "motivation": "现有理论分析仅考虑替换突变，而实际基因组数据包含插入和删除(indels)，理论与实际存在差距。需要建立包含indel通道的数学模型，解释seed-chain-extend在实际基因组比对中的有效性", "method": "引入处理indel通道的数学工具，解决两个新障碍：相邻锚点的依赖性和部分正确锚点的存在。通过分析包含替换、插入、删除的总突变率，建立理论模型", "result": "证明最优链的期望恢复率≥1-O(1/√m)，期望运行时间为O(mn^{3.15·θ_T}log n)，其中总突变率θ_T=θ_i+θ_d+θ_s<0.159", "conclusion": "成功将平均情况分析扩展到包含indel通道，填补了理论与实际之间的差距，为seed-chain-extend在真实基因组数据中的有效性提供了理论依据"}}
{"id": "2512.05246", "pdf": "https://arxiv.org/pdf/2512.05246", "abs": "https://arxiv.org/abs/2512.05246", "authors": ["Ankit Gupta", "Onur Dizdar", "Yun Chen", "Fehmi Emre Kadan", "Ata Sattarzadeh", "Stephen Wang"], "title": "NeuromorphicRx: From Neural to Spiking Receiver", "categories": ["cs.NE", "cs.IT"], "comment": null, "summary": "In this work, we propose a novel energy-efficient spiking neural network (SNN)-based receiver for 5G-NR OFDM system, called neuromorphic receiver (NeuromorphicRx), replacing the channel estimation, equalization and symbol demapping blocks. We leverage domain knowledge to design the input with spiking encoding and propose a deep convolutional SNN with spike-element-wise residual connections. We integrate an SNN with artificial neural network (ANN) hybrid architecture to obtain soft outputs and employ surrogate gradient descent for training. We focus on generalization across diverse scenarios and robustness through quantized aware training. We focus on interpretability of NeuromorphicRx for 5G-NR signals and perform detailed ablation study for 5G-NR signals. Our extensive numerical simulations show that NeuromorphicRx is capable of achieving significant block error rate performance gain compared to 5G-NR receivers and similar performance compared to its ANN-based counterparts with 7.6x less energy consumption.", "AI": {"tldr": "提出一种基于脉冲神经网络的5G-NR OFDM系统接收机NeuromorphicRx，用于替代传统接收机中的信道估计、均衡和符号解映射模块", "motivation": "传统5G-NR接收机在信道估计、均衡和解映射方面存在计算复杂度和能耗问题，需要更节能的解决方案", "method": "1) 利用领域知识设计脉冲编码输入；2) 构建深度卷积脉冲神经网络，包含脉冲元素级残差连接；3) 采用SNN-ANN混合架构获取软输出；4) 使用代理梯度下降进行训练；5) 通过量化感知训练提高鲁棒性", "result": "NeuromorphicRx相比传统5G-NR接收机在块错误率性能上有显著提升，与基于ANN的对应方案性能相当，但能耗降低7.6倍", "conclusion": "NeuromorphicRx证明了脉冲神经网络在无线通信接收机中的有效性，能够在保持性能的同时显著降低能耗，并具有良好的泛化能力和鲁棒性"}}
{"id": "2512.05242", "pdf": "https://arxiv.org/pdf/2512.05242", "abs": "https://arxiv.org/abs/2512.05242", "authors": ["Uwe M. Borghoff", "Mark Minas", "Jannis Schopp"], "title": "Learning to Code with Context: A Study-Based Approach", "categories": ["cs.SE", "cs.AI"], "comment": "36 pages, 7 figures, 5 tables", "summary": "The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.", "AI": {"tldr": "研究基于项目的编程课程中如何有效整合生成式AI工具，通过用户研究分析学生在软件开发各阶段使用AI工具的情况，并开发了一个基于RAG的本地部署LLM助手来提供项目上下文感知的支持。", "motivation": "生成式AI工具的快速发展正在改变软件开发方式，软件工程教育需要适应这一变化，确保学生不仅学习传统开发方法，还能有意义且负责任地使用新技术。项目制课程为探索AI辅助工具在实际开发实践中的整合提供了有效环境。", "method": "在大学编程项目中开展用户研究，学生协作开发电脑游戏，观察他们在软件开发不同阶段使用生成式AI工具的情况。同时开发了一个基于检索增强生成（RAG）的本地部署大型语言模型助手，该系统能够利用相关文档和源代码提供项目上下文感知的支持。", "result": "研究揭示了学生在软件开发过程中使用生成式AI工具的模式，识别了这些工具最有效的任务类型，分析了学生遇到的挑战。通过RAG系统对模型行为、参数敏感性和常见失败模式进行了定性分析。", "conclusion": "研究加深了对教育软件项目中上下文感知AI支持的理解，为未来将基于AI的辅助工具整合到软件工程课程中提供了参考依据。项目制课程是探索AI工具整合的有效环境，需要进一步研究如何优化AI辅助工具以更好地支持学习过程。"}}
{"id": "2512.05240", "pdf": "https://arxiv.org/pdf/2512.05240", "abs": "https://arxiv.org/abs/2512.05240", "authors": ["Dmitrii Torbunov", "Onur Okuducu", "Yi Huang", "Odera Dim", "Rebecca Coles", "Yonggang Cui", "Yihui Ren"], "title": "IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Continuous video monitoring in surveillance, robotics, and wearable systems faces a fundamental power constraint: conventional RGB cameras consume substantial energy through fixed-rate capture. Event cameras offer sparse, motion-driven sensing with low power consumption, but produce asynchronous event streams rather than RGB video. We propose a hybrid capture paradigm that records sparse RGB keyframes alongside continuous event streams, then reconstructs full RGB video offline -- reducing capture power consumption while maintaining standard video output for downstream applications. We introduce the Image and Event to Video (IE2Video) task: reconstructing RGB video sequences from a single initial frame and subsequent event camera data. We investigate two architectural strategies: adapting an autoregressive model (HyperE2VID) for RGB generation, and injecting event representations into a pretrained text-to-video diffusion model (LTX) via learned encoders and low-rank adaptation. Our experiments demonstrate that the diffusion-based approach achieves 33\\% better perceptual quality than the autoregressive baseline (0.283 vs 0.422 LPIPS). We validate our approach across three event camera datasets (BS-ERGB, HS-ERGB far/close) at varying sequence lengths (32-128 frames), demonstrating robust cross-dataset generalization with strong performance on unseen capture configurations.", "AI": {"tldr": "提出IE2Video任务：从单个初始RGB帧和后续事件相机数据重建RGB视频序列，解决事件相机低功耗但输出非标准视频的问题", "motivation": "传统RGB相机连续监控功耗高，事件相机功耗低但输出异步事件流而非标准RGB视频，需要一种混合捕获范式来平衡功耗与视频质量", "method": "提出两种架构策略：1) 基于自回归模型的HyperE2VID用于RGB生成；2) 通过学习的编码器和低秩适配将事件表示注入预训练的文本到视频扩散模型(LTX)", "result": "基于扩散的方法在感知质量上比自回归基线提升33%（LPIPS从0.422降至0.283），在三个事件相机数据集（BS-ERGB、HS-ERGB远/近）上验证了鲁棒的跨数据集泛化能力", "conclusion": "提出的混合捕获范式（稀疏RGB关键帧+连续事件流）能有效降低捕获功耗，同时通过IE2Video任务重建高质量RGB视频，扩散方法在视频重建质量上显著优于自回归方法"}}
{"id": "2512.05239", "pdf": "https://arxiv.org/pdf/2512.05239", "abs": "https://arxiv.org/abs/2512.05239", "authors": ["Ruofan Gao", "Amjed Tahir", "Peng Liang", "Teo Susnjak", "Foutse Khomh"], "title": "A Survey of Bugs in AI-Generated Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.", "AI": {"tldr": "对AI生成代码中的bug进行系统性调查和分类，为未来模型改进和质量评估提供参考", "motivation": "AI代码生成模型被广泛使用以提高生产力，但生成的代码存在质量问题。现有研究发现往往分散且缺乏系统性总结，需要全面审查来揭示错误类型、分布、修复策略及其与特定模型的相关性", "method": "系统性分析现有AI生成代码文献，建立对生成代码中bug和缺陷的整体理解，提供bug类型和模式的分类", "result": "建立了AI生成代码中bug的分类体系，揭示了不同类型bug的分布情况，分析了不同模型生成的代码中bug模式的差异", "conclusion": "AI生成代码中存在多种类型的bug，需要系统性分类和理解；为未来模型改进和质量评估提供了重要参考；讨论了可能的修复和缓解策略"}}
{"id": "2512.05234", "pdf": "https://arxiv.org/pdf/2512.05234", "abs": "https://arxiv.org/abs/2512.05234", "authors": ["Felix Mulitze", "Herbert Woisetschläger", "Hans Arno Jacobsen"], "title": "MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at the peer-reviewed AI4NextG Workshop at NeurIPS 2025", "summary": "The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.", "AI": {"tldr": "提出MAR-FL系统，一种通信高效的P2P联邦学习系统，通过迭代分组聚合显著降低通信开销", "motivation": "下一代无线系统与分布式机器学习融合需要高效、鲁棒的联邦学习方法；现有P2P FL方法通信复杂度高（O(N²)），限制了实际可扩展性", "method": "采用迭代分组聚合机制，将通信复杂度从O(N²)降低到O(N log N)，同时保持对网络流失的鲁棒性", "result": "MAR-FL实现了O(N log N)的通信成本，相比现有基线O(N²)显著降低，在聚合轮次中节点数增加时仍保持有效性", "conclusion": "MAR-FL系统通过创新的分组聚合方法解决了P2P FL的通信瓶颈问题，为无线网络环境下的联邦学习提供了高效、可扩展且鲁棒的解决方案"}}
{"id": "2512.05230", "pdf": "https://arxiv.org/pdf/2512.05230", "abs": "https://arxiv.org/abs/2512.05230", "authors": ["Jonathan Yang", "Chelsea Finn", "Dorsa Sadigh"], "title": "Invariance Co-training for Robot Visual Generalization", "categories": ["cs.RO", "cs.AI"], "comment": "14 pages, 10 figures", "summary": "Reasoning from diverse observations is a fundamental capability for generalist robot policies to operate in a wide range of environments. Despite recent advancements, many large-scale robotic policies still remain sensitive to key sources of observational variation such as changes in camera perspective, lighting, and the presence of distractor objects. We posit that the limited generalizability of these models arises from the substantial diversity required to robustly cover these quasistatic axes, coupled with the current scarcity of large-scale robotic datasets that exhibit rich variation across them. In this work, we propose to systematically examine what robots need to generalize across these challenging axes by introducing two key auxiliary tasks, state similarity and invariance to observational perturbations, applied to both demonstration data and static visual data. We then show that via these auxiliary tasks, leveraging both more-expensive robotic demonstration data and less-expensive, visually rich synthetic images generated from non-physics-based simulation (for example, Unreal Engine) can lead to substantial increases in generalization to unseen camera viewpoints, lighting configurations, and distractor conditions. Our results demonstrate that co-training on this diverse data improves performance by 18 percent over existing generative augmentation methods. For more information and videos, please visit https://invariance-cotraining.github.io", "AI": {"tldr": "提出一种通过辅助任务（状态相似性和观测扰动不变性）结合机器人演示数据和视觉丰富的合成图像进行协同训练的方法，以提高机器人策略在相机视角、光照和干扰物变化下的泛化能力。", "motivation": "当前大规模机器人策略对观测变化（如相机视角、光照、干扰物）敏感，泛化能力有限。这源于需要大量多样性数据来覆盖这些准静态变化轴，而现有大规模机器人数据集在这些方面的变化丰富度不足。", "method": "引入两个关键辅助任务：状态相似性和对观测扰动的不变性，应用于演示数据和静态视觉数据。通过这两个辅助任务，结合昂贵的机器人演示数据和廉价的、视觉丰富的合成图像（来自非物理模拟如Unreal Engine）进行协同训练。", "result": "该方法在未见过的相机视角、光照配置和干扰物条件下的泛化性能显著提升，比现有生成增强方法性能提高18%。", "conclusion": "通过辅助任务协同训练机器人演示数据和视觉丰富的合成图像，可以有效提高机器人策略对观测变化的泛化能力，为解决机器人视觉泛化问题提供了有效途径。"}}
{"id": "2512.05229", "pdf": "https://arxiv.org/pdf/2512.05229", "abs": "https://arxiv.org/abs/2512.05229", "authors": ["Yanis Lahrach", "Christian Hughes", "Ian Abraham"], "title": "Search at Scale: Improving Numerical Conditioning of Ergodic Coverage Optimization for Multi-Scale Domains", "categories": ["cs.RO"], "comment": null, "summary": "Recent methods in ergodic coverage planning have shown promise as tools that can adapt to a wide range of geometric coverage problems with general constraints, but are highly sensitive to the numerical scaling of the problem space. The underlying challenge is that the optimization formulation becomes brittle and numerically unstable with changing scales, especially under potentially nonlinear constraints that impose dynamic restrictions, due to the kernel-based formulation. This paper proposes to address this problem via the development of a scale-agnostic and adaptive ergodic coverage optimization method based on the maximum mean discrepancy metric (MMD). Our approach allows the optimizer to solve for the scale of differential constraints while annealing the hyperparameters to best suit the problem domain and ensure physical consistency. We also derive a variation of the ergodic metric in the log space, providing additional numerical conditioning without loss of performance. We compare our approach with existing coverage planning methods and demonstrate the utility of our approach on a wide range of coverage problems.", "AI": {"tldr": "提出一种尺度无关的自适应遍历覆盖优化方法，基于最大均值差异度量，改善多尺度域中的数值条件", "motivation": "现有遍历覆盖规划方法对问题空间的数值缩放高度敏感，优化公式在尺度变化时变得脆弱且数值不稳定，特别是在非线性约束下", "method": "开发基于最大均值差异度量的尺度无关自适应遍历覆盖优化方法，允许优化器求解微分约束的尺度，同时退火超参数以适应问题域；推导对数空间中的遍历度量变体", "result": "与现有覆盖规划方法相比，该方法在广泛的覆盖问题上展示了实用性，改善了数值条件而不损失性能", "conclusion": "提出的方法解决了遍历覆盖优化中的尺度敏感性问题，通过自适应调整和数值稳定化技术，实现了在多尺度域中的鲁棒覆盖规划"}}
{"id": "2512.05225", "pdf": "https://arxiv.org/pdf/2512.05225", "abs": "https://arxiv.org/abs/2512.05225", "authors": ["Patrizio Angelini", "Michael A. Bekos", "Giuseppe Di Battista", "Fabrizio Frati", "Luca Grilli", "Giacomo Ortali"], "title": "On Planar Straight-Line Dominance Drawings", "categories": ["cs.CG", "cs.DS"], "comment": "A preliminary version appears at WADS '25", "summary": "We study the following question, which has been considered since the 90's: Does every $st$-planar graph admit a planar straight-line dominance drawing? We show concrete evidence for the difficulty of this question, by proving that, unlike upward planar straight-line drawings, planar straight-line dominance drawings with prescribed $y$-coordinates do not always exist and planar straight-line dominance drawings cannot always be constructed via a contract-draw-expand inductive approach. We also show several classes of $st$-planar graphs that always admit a planar straight-line dominance drawing. These include $st$-planar $3$-trees in which every stacking operation introduces two edges incoming into the new vertex, $st$-planar graphs in which every vertex is adjacent to the sink, $st$-planar graphs in which no face has the left boundary that is a single edge, and $st$-planar graphs that have a leveling with span at most two.", "AI": {"tldr": "研究平面直线支配图的存在性问题，特别是针对st-平面图是否总是存在平面直线支配图的问题。", "motivation": "自90年代以来，研究者们一直在探讨st-平面图是否总是存在平面直线支配图的问题。这个问题在平面图绘制领域具有重要意义，因为平面直线支配图是一种特殊的向上平面图，具有更强的几何约束。", "method": "通过证明某些构造方法的局限性来展示问题的难度：1) 证明具有指定y坐标的平面直线支配图不一定存在；2) 证明收缩-绘制-扩展归纳方法不能总是构造出这种图。同时识别出几类总是存在平面直线支配图的st-平面图。", "result": "证明了平面直线支配图构造问题的难度，同时确定了四类总是存在平面直线支配图的st-平面图：1) 每个堆叠操作引入两条边指向新顶点的st-平面3-树；2) 每个顶点都与汇点相邻的st-平面图；3) 没有面以单边作为左边界的st-平面图；4) 具有跨度最多为2的层次化的st-平面图。", "conclusion": "平面直线支配图的存在性问题比向上平面直线图更为复杂，但某些特定类别的st-平面图确实总是存在这种绘制方式。研究揭示了该问题的难度，并为未来研究提供了方向。"}}
{"id": "2512.05212", "pdf": "https://arxiv.org/pdf/2512.05212", "abs": "https://arxiv.org/abs/2512.05212", "authors": ["Georgios Mappouras", "Charalambos Rossides"], "title": "On the Computability of Artificial General Intelligence", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.", "AI": {"tldr": "本文探讨了人工通用智能（AGI）的可计算性，通过形式化证明表明任何算法（包括AI模型）都无法在其初始算法之外展示新的功能能力，从而无法实现真正的创造性。", "motivation": "近年来人工智能快速发展，人们开始关注人类距离开发出达到人类智能水平的AI模型（即AGI）还有多远。本文旨在通过计算理论的角度，探讨AGI的可计算性上限。", "method": "采用前人关于AGI的定义（在某个研究领域中以创造性和创新性方式解锁新的、先前未知的功能能力），基于计算理论进行形式化分析，证明任何算法都无法超越其初始功能集。", "result": "形式化证明表明：没有算法能够展示其初始算法本身不存在的新的功能能力。因此，任何算法（包括AI模型）都无法在任何研究领域（科学、工程、艺术、体育等）实现真正的创造性。", "conclusion": "AI模型只能展示现有功能能力及其组合与排列，无法实现真正的创造性。这一结论对AI发展的未来以及人类智能的起源具有重要意义。"}}
{"id": "2512.05211", "pdf": "https://arxiv.org/pdf/2512.05211", "abs": "https://arxiv.org/abs/2512.05211", "authors": ["Ioannis Mandralis", "Severin Schumacher", "Morteza Gharib"], "title": "Wake Vectoring for Efficient Morphing Flight", "categories": ["cs.RO"], "comment": null, "summary": "Morphing aerial robots have the potential to transform autonomous flight, enabling navigation through cluttered environments, perching, and seamless transitions between aerial and terrestrial locomotion. Yet mid-flight reconfiguration presents a critical aerodynamic challenge: tilting propulsors to achieve shape change reduces vertical thrust, undermining stability and control authority. Here, we introduce a passive wake vectoring mechanism that recovers lost thrust during morphing. Integrated into a novel robotic system, Aerially Transforming Morphobot (ATMO), internal deflectors intercept and redirect rotor wake downward, passively steering airflow momentum that would otherwise be wasted. This electronics-free solution achieves up to a 40% recovery of vertical thrust in configurations where no useful thrust would otherwise be produced, substantially extending hover and maneuvering capabilities during transformation. Our findings highlight a new direction for morphing aerial robot design, where passive aerodynamic structures, inspired by thrust vectoring in rockets and aircraft, enable efficient, agile flight without added mechanical complexity.", "AI": {"tldr": "该论文提出了一种被动尾流转向机制，用于在变形飞行中恢复垂直推力，提高变形空中机器人的效率和敏捷性。", "motivation": "变形空中机器人在飞行中重新配置时面临关键的气动挑战：倾斜推进器以实现形状变化会减少垂直推力，从而削弱稳定性和控制能力。", "method": "引入被动尾流转向机制，通过内部偏转器拦截并重定向转子尾流向下，被动引导原本会被浪费的气流动量，无需电子设备。", "result": "在原本不会产生有用推力的配置中，该方案实现了高达40%的垂直推力恢复，显著扩展了变形期间的悬停和机动能力。", "conclusion": "研究结果为变形空中机器人设计指明了新方向，受火箭和飞机推力转向启发的被动气动结构，能够在无需增加机械复杂性的情况下实现高效、敏捷的飞行。"}}
{"id": "2512.05209", "pdf": "https://arxiv.org/pdf/2512.05209", "abs": "https://arxiv.org/abs/2512.05209", "authors": ["Vsevolod Plohotnuk", "Artyom Panshin", "Nikola Banić", "Simone Bianco", "Michael Freeman", "Egor Ershov"], "title": "DEAR: Dataset for Evaluating the Aesthetics of RenderingDEAR: Dataset for Evaluating the Aesthetics of Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).", "AI": {"tldr": "提出了DEAR数据集，专门用于评估图像渲染风格的美学质量，这是首个基于人类主观偏好的系统性渲染美学评估数据集", "motivation": "传统图像质量评估主要关注技术性退化（如噪声、模糊、压缩伪影），但渲染美学评估在摄影编辑、内容创作和AI生成图像等领域日益重要，由于缺乏反映主观风格偏好的数据集，这一领域研究不足", "method": "基于MIT-Adobe FiveK数据集构建，通过大规模众包收集成对人类偏好评分，每个图像对由25名不同的人类评估者评估，共有13,648名评估者参与，总计收集了大量主观美学偏好数据", "result": "创建了包含成对偏好评分的DEAR数据集，分析了人类投票模式，支持风格偏好预测、美学基准测试和个性化美学建模等多种应用场景，并在HuggingFace上发布了包含100张图像标记的子集", "conclusion": "DEAR是首个系统性解决基于人类主观偏好的图像渲染美学评估的数据集，为超越传统失真评估的美学模型开发提供了基础，推动了渲染美学评估这一新任务的发展"}}
{"id": "2512.05201", "pdf": "https://arxiv.org/pdf/2512.05201", "abs": "https://arxiv.org/abs/2512.05201", "authors": ["Ali Al Housseini", "Jaime Llorca", "Luca Turchet", "Tiziano Leidi", "Cristina Rottondi", "Omran Ayoub"], "title": "MuMeNet: A Network Simulator for Musical Metaverse Communications", "categories": ["cs.NI", "cs.SD"], "comment": "To be published in 2025 IEEE 6th International Symposium on the Internet of Sounds (IS2)", "summary": "The Metaverse, a shared and spatially organized digital continuum, is transforming various industries, with music emerging as a leading use case. Live concerts, collaborative composition, and interactive experiences are driving the Musical Metaverse (MM), but the requirements of the underlying network and service infrastructures hinder its growth. These challenges underscore the need for a novel modeling and simulation paradigm tailored to the unique characteristics of MM sessions, along with specialized service provisioning strategies capable of capturing their interactive, heterogeneous, and multicast-oriented nature. To this end, we make a first attempt to formally model and analyze the problem of service provisioning for MM sessions in 5G/6G networks. We first formalize service and network graph models for the MM, using \"live audience interaction in a virtual concert\" as a reference scenario. We then present MuMeNet, a novel discrete-event network simulator specifically tailored to the requirements and the traffic dynamics of the MM. We showcase the effectiveness of MuMeNet by running a linear programming based orchestration policy on the reference scenario and providing performance analysis under realistic MM workloads.", "AI": {"tldr": "开发MuMeNet网络模拟器，专门用于模拟和分析音乐元宇宙（Musical Metaverse）通信中的服务配置问题，特别针对5G/6G网络环境。", "motivation": "音乐元宇宙（如虚拟音乐会、协作作曲等）正在快速发展，但其底层网络和服务基础设施的要求限制了其增长。现有模拟工具无法捕捉音乐元宇宙会话的交互性、异构性和多播导向特性，需要专门的分析工具。", "method": "1. 形式化建模音乐元宇宙的服务和网络图模型，以\"虚拟音乐会中的实时观众互动\"为参考场景；2. 开发MuMeNet离散事件网络模拟器，专门针对音乐元宇宙的需求和流量动态；3. 在参考场景上运行基于线性编程的编排策略进行性能分析。", "result": "成功开发了MuMeNet模拟器，能够有效模拟音乐元宇宙通信场景。通过在实际音乐元宇宙工作负载下运行线性编程编排策略，展示了模拟器的有效性，并提供了性能分析。", "conclusion": "MuMeNet是首个专门针对音乐元宇宙通信需求设计的网络模拟器，为解决音乐元宇宙服务配置问题提供了有效的建模和分析工具，有助于推动音乐元宇宙在5G/6G网络环境下的发展。"}}
{"id": "2512.05198", "pdf": "https://arxiv.org/pdf/2512.05198", "abs": "https://arxiv.org/abs/2512.05198", "authors": ["Rowan Bradbury", "Dazhi Zhong"], "title": "Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "16 pages, 10 figures", "summary": "Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.", "AI": {"tldr": "提出像素等效潜在合成（PELC）原则，通过DecFormer实现扩散模型中像素空间与潜在空间一致的掩码合成，解决传统线性插值方法导致的边缘伪影和颜色偏移问题。", "motivation": "当前扩散模型中的潜在修复主要依赖在降采样掩码下对VAE潜在进行线性插值，但现代VAE捕获的全局上下文超出了局部结构，导致线性潜在混合无法实现像素等效，产生掩码接缝处的伪影、全局退化及颜色偏移。", "method": "提出像素等效潜在合成（PELC）原则，要求潜在合成应与像素空间合成等效。引入DecFormer（770万参数变压器），预测每通道混合权重和流形外残差校正，实现掩码一致的潜在融合。DecFormer训练目标是使融合后的解码与像素空间alpha合成匹配，兼容现有扩散管道，无需主干微调。", "result": "在FLUX.1系列上，DecFormer恢复了全局颜色一致性、软掩码支持、锐利边界和高保真掩码，边缘误差指标比标准掩码插值降低达53%。作为修复先验，轻量级LoRA结合DecFormer在FLUX.1-Dev上达到与完全微调的FLUX.1-Fill相当的保真度。", "conclusion": "PELC为像素等效潜在编辑提供了通用方案，不仅适用于修复任务，还可扩展到复杂颜色校正等编辑任务。DecFormer实现了真正的像素等效潜在合成，解决了传统方法的根本限制。"}}
{"id": "2512.05179", "pdf": "https://arxiv.org/pdf/2512.05179", "abs": "https://arxiv.org/abs/2512.05179", "authors": ["Aurélie Montfrond"], "title": "Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages, 2 figures", "summary": "Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.", "AI": {"tldr": "该研究通过微调BERT模型，为利默里克大学电子与计算机工程系开发了一个课程信息问答系统，展示了将基础模型适配到教育领域的可行性。", "motivation": "现有的科学问答系统主要关注聊天机器人风格，而针对特定领域推理的基础模型微调研究有限。特别是在大学课程材料领域，虽然存在生物医学和科学文献的领域特定BERT变体，但尚未有针对大学课程材料的基础模型。", "method": "使用大学模块手册构建了包含1,203个问答对的SQuAD格式自定义数据集，通过手动和合成生成的方式补充数据。使用PyTorch对BERT模型进行微调，并使用精确匹配和F1分数评估性能。", "result": "结果显示，即使是适度的微调也能改善假设框架和知识提取。微调后的BERT在学术问答对上表现出有效结果，证明了将基础模型适配到教育领域的可行性。", "conclusion": "这项工作填补了大学课程材料领域特定基础模型的空白，展示了微调BERT在学术问答中的有效性，为开发首个大学领域特定问答模型和实现自主教育知识系统奠定了基础。"}}
{"id": "2512.05176", "pdf": "https://arxiv.org/pdf/2512.05176", "abs": "https://arxiv.org/abs/2512.05176", "authors": ["Brittany Johnson", "Erin Reddick", "Angela D. R. Smith"], "title": "Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Under review", "summary": "Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as \"general purpose\" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of \"culturally-informed\" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.", "AI": {"tldr": "开发一个文化智能和价值观推断质量基准（CIVIQ），用于评估LLM与社区社会价值观和常识的对齐程度，以解决现有LLM主要与西方白人叙事对齐而忽视其他文化群体的问题。", "motivation": "当前大型语言模型（LLMs）通常被设计为\"通用目的\"技术，主要与西方白人叙事对齐，导致与其他文化和参与协作创新的群体错位。虽然已有\"文化知情\"LLMs（如ChatBlackGPT）的开发，但缺乏评估这些模型文化对齐能力的基准。现有的国家对齐基准（如KorNAT）在美国多元文化背景下效果有限。", "method": "提出一项复制研究，将开发韩国国家LLM对齐基准（KorNAT）的过程转化为开发CIVIQ基准。CIVIQ专注于社区社会价值观和常识的对齐，而非国家层面的对齐，以适应美国多元文化环境。", "result": "开发了CIVIQ基准，为研究和开发实践中AI技术的文化对齐提供了关键基础。该基准能够更有效地评估LLM与不同社区价值观和常识的对齐质量。", "conclusion": "CIVIQ基准填补了在多元文化环境中评估LLM文化对齐能力的空白，为开发更具包容性和代表性的AI技术提供了重要工具，特别是在美国这样文化多样的国家中。"}}
{"id": "2512.05172", "pdf": "https://arxiv.org/pdf/2512.05172", "abs": "https://arxiv.org/abs/2512.05172", "authors": ["Wentao Wang", "Chunyang Liu", "Kehua Sheng", "Bo Zhang", "Yan Wang"], "title": "Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing exploration of Large Language Models (LLM) and Vision-Language Models (VLM) has opened avenues for enhancing the effectiveness of reinforcement learning (RL). However, existing LLM-based RL methods often focus on the guidance of control policy and encounter the challenge of limited representations of the backbone networks. To tackle this problem, we introduce Enhanced Semantic Motion Representations (Semore), a new VLM-based framework for visual RL, which can simultaneously extract semantic and motion representations through a dual-path backbone from the RGB flows. Semore utilizes VLM with common-sense knowledge to retrieve key information from observations, while using the pre-trained clip to achieve the text-image alignment, thereby embedding the ground-truth representations into the backbone. To efficiently fuse semantic and motion representations for decision-making, our method adopts a separately supervised approach to simultaneously guide the extraction of semantics and motion, while allowing them to interact spontaneously. Extensive experiments demonstrate that, under the guidance of VLM at the feature level, our method exhibits efficient and adaptive ability compared to state-of-art methods. All codes are released.", "AI": {"tldr": "提出Semore框架，通过VLM引导增强语义运动表示，用于视觉强化学习，解决现有LLM-based RL方法表示能力有限的问题", "motivation": "现有基于LLM的强化学习方法主要关注控制策略指导，但面临骨干网络表示能力有限的问题。需要同时提取语义和运动表示来增强视觉RL的效果", "method": "提出双路径骨干网络从RGB流中同时提取语义和运动表示，利用VLM的常识知识从观测中检索关键信息，使用预训练CLIP实现文本-图像对齐，采用分离监督方法指导语义和运动提取并允许它们自发交互", "result": "实验表明，在特征级别VLM指导下，该方法相比最先进方法展现出高效和自适应能力", "conclusion": "Semore框架通过VLM引导增强语义运动表示，有效提升了视觉强化学习的性能，代码已开源"}}
{"id": "2512.05171", "pdf": "https://arxiv.org/pdf/2512.05171", "abs": "https://arxiv.org/abs/2512.05171", "authors": ["Aleksandr Abramov"], "title": "Two-Stage Camera Calibration Method for Multi-Camera Systems Using Scene Geometry", "categories": ["eess.IV", "cs.RO"], "comment": null, "summary": "Calibration of multi-camera systems is a key task for accurate object tracking. However, it remains a challenging problem in real-world conditions, where traditional methods are not applicable due to the lack of accurate floor plans, physical access to place calibration patterns, or synchronized video streams. This paper presents a novel two-stage calibration method that overcomes these limitations. In the first stage, partial calibration of individual cameras is performed based on an operator's annotation of natural geometric primitives (parallel, perpendicular, and vertical lines, or line segments of equal length). This allows estimating key parameters (roll, pitch, focal length) and projecting the camera's Effective Field of View (EFOV) onto the horizontal plane in a base 3D coordinate system. In the second stage, precise system calibration is achieved through interactive manipulation of the projected EFOV polygons. The operator adjusts their position, scale, and rotation to align them with the floor plan or, in its absence, using virtual calibration elements projected onto all cameras in the system. This determines the remaining extrinsic parameters (camera position and yaw). Calibration requires only a static image from each camera, eliminating the need for physical access or synchronized video. The method is implemented as a practical web service. Comparative analysis and demonstration videos confirm the method's applicability, accuracy, and flexibility, enabling the deployment of precise multi-camera tracking systems in scenarios previously considered infeasible.", "AI": {"tldr": "提出一种用于多相机系统的两阶段标定方法，利用场景几何特征进行标定，无需精确平面图、物理访问或同步视频流", "motivation": "传统多相机系统标定方法在现实条件下存在局限性：缺乏精确平面图、无法物理放置标定图案、或需要同步视频流。需要一种更灵活、实用的标定方法", "method": "两阶段标定方法：第一阶段基于操作员标注的自然几何基元（平行线、垂直线、等长线段）进行单相机部分标定，估计关键参数并投影有效视场到水平面；第二阶段通过交互操作投影的多边形，调整位置、尺度和旋转来对齐平面图或虚拟标定元素，确定剩余外参", "result": "该方法仅需每个相机的静态图像，无需物理访问或同步视频。实现了实用的网络服务，比较分析和演示视频证实了方法的适用性、准确性和灵活性", "conclusion": "该方法克服了传统方法的限制，使精确的多相机跟踪系统能够在以前认为不可行的场景中部署，为现实条件下的多相机标定提供了实用解决方案"}}
{"id": "2512.05169", "pdf": "https://arxiv.org/pdf/2512.05169", "abs": "https://arxiv.org/abs/2512.05169", "authors": ["Abdelmalik Moujahid", "Fadi Dornaika"], "title": "Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Artif Intell Rev 58, 234 (2025)", "summary": "Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.", "AI": {"tldr": "本文对多视图聚类技术进行了全面的综述，系统分类了不同的MVC方法，分析了各自的优缺点和实际挑战，并探讨了未来研究方向。", "motivation": "传统机器学习面临计算限制、单视图学习算法局限性以及处理多源大数据复杂性的挑战。多视图聚类作为无监督多视图学习的一种方法，能够弥补单视图方法的不足，提供更丰富的数据表示和有效的无监督学习解决方案。", "method": "通过系统综述方法，将多视图聚类技术分为七大类：协同训练、协同正则化、子空间、深度学习、核方法、锚点法和图方法。分析了早期融合、晚期融合和联合学习等集成策略，并回顾了140多篇基础与最新文献。", "result": "建立了多视图聚类的系统分类框架，识别了各类方法的优势和局限性，特别是可扩展性和不完整数据处理等实际挑战。提供了在医疗健康、多媒体和社交网络分析等领域的实际应用案例。", "conclusion": "多视图聚类是克服单视图学习局限性的强大方法，具有丰富的语义信息。本文填补了MVC研究中的空白，为领域发展提供了可操作的见解，并指出了跨学科应用和未来研究方向。"}}
{"id": "2512.05167", "pdf": "https://arxiv.org/pdf/2512.05167", "abs": "https://arxiv.org/abs/2512.05167", "authors": ["Fang Li"], "title": "Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education", "categories": ["cs.AI"], "comment": "Accepted by the 39th annual Consortium for Computing Sciences in Colleges (CCSC:SE)", "summary": "This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.", "AI": {"tldr": "设计一个两部分的AI课程，系统性地连接传统机器学习与现代大语言模型，为现代AI教育提供创新教学方法", "motivation": "传统AI教育往往将传统机器学习与现代LLMs割裂教学，学生难以理解AI技术的演进脉络，无法形成对AI领域的整体认知，也无法满足行业对既懂传统技术又掌握前沿LLMs技能的人才需求", "method": "设计分为两个连续互补部分的课程：第一部分教授基础机器学习概念，第二部分专注于当代LLM应用。课程架构包括课程设计、实施策略、评估方法，并在两个七周的夏季学期中实施", "result": "该综合教学方法增强了学生对AI领域的理解，更好地为他们在快速发展的AI领域应对行业需求做好了准备。课程实施结果显示学生能够建立对AI演进的全面理解，同时掌握传统和前沿技术的实践技能", "conclusion": "这种两部分的课程设计有效地弥合了传统机器学习与现代LLMs之间的教学鸿沟，为学生提供了连贯的AI学习路径，是适应AI快速发展的有效教育方法"}}
{"id": "2512.05162", "pdf": "https://arxiv.org/pdf/2512.05162", "abs": "https://arxiv.org/abs/2512.05162", "authors": ["C. M. Wyss"], "title": "How to Tame Your LLM: Semantic Collapse in Continuous Systems", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.DS", "math.PR"], "comment": "35 pages, 1 figure. Exolytica AI Technical Report XTR-2025-01", "summary": "We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,μ) \\to L^2(M,μ)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.", "AI": {"tldr": "该论文提出了一个关于大型语言模型语义动态的通用理论，将LLM形式化为连续状态机，证明了语义特征定理，解释了离散符号语义如何从连续计算中涌现。", "motivation": "理解大型语言模型如何从连续的激活流形中涌现出离散的、可解释的语义结构，解释连续计算与离散符号语义之间的桥梁。", "method": "将LLM形式化为连续状态机（CSMs），使用转移算子分析语义质量传播，在正则性假设下证明转移算子的紧致性和离散谱，建立语义特征定理（SCT）。", "result": "证明了语义特征定理：转移算子的主导特征函数诱导出有限个具有不变意义的谱盆地，每个盆地可在实数上的o-极小结构中定义，表明谱可聚合性与逻辑可驯性一致。", "conclusion": "连续激活流形会坍缩为有限的、逻辑可解释的本体论，解释了离散符号语义如何从连续计算中涌现，并将结果扩展到随机和绝热（时间非均匀）设置。"}}
{"id": "2512.05156", "pdf": "https://arxiv.org/pdf/2512.05156", "abs": "https://arxiv.org/abs/2512.05156", "authors": ["Igor Halperin"], "title": "Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.LG", "q-fin.CP"], "comment": "23 pages, 6 figures", "summary": "Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\\bf Q}$ and ${\\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.", "AI": {"tldr": "提出两种基于信息论和热力学的无监督度量方法，用于评估大型语言模型的语义忠实度和控制幻觉", "motivation": "评估LLM对给定任务的忠实度是一个复杂挑战，需要新的无监督度量方法来量化模型输出与输入上下文的一致性，以管理幻觉问题", "method": "将LLM视为二分信息引擎，隐藏层作为麦克斯韦妖控制上下文到答案的转换。将QCA三元组建模为共享主题的概率分布，使用转换矩阵Q和A分别编码查询目标和实际结果，通过KL散度优化计算语义忠实度(SF)，并提出基于热力学的语义熵产生(SEP)度量", "result": "SF度量通过将最小KL散度映射到[0,1]区间量化忠实度，高忠实度通常对应低熵产生。SF和SEP度量可单独或联合用于LLM评估和幻觉控制，在SEC 10-K文件摘要任务中进行了验证", "conclusion": "提出的SF和SEP度量为LLM的忠实度评估和幻觉管理提供了基于信息论和热力学的理论框架，能够量化模型输出与输入上下文的一致性，有助于提高LLM的可靠性和可信度"}}
{"id": "2512.05152", "pdf": "https://arxiv.org/pdf/2512.05152", "abs": "https://arxiv.org/abs/2512.05152", "authors": ["Kun Wang", "Donglin Di", "Tonghua Su", "Lei Fan"], "title": "EFDiT: Efficient Fine-grained Image Generation Using Diffusion Transformer Models", "categories": ["cs.CV"], "comment": "6pages, 5figures, published to 2025 IEEE International Conference on Multimedia and Expo (ICME), Nantes, France, 2025", "summary": "Diffusion models are highly regarded for their controllability and the diversity of images they generate. However, class-conditional generation methods based on diffusion models often focus on more common categories. In large-scale fine-grained image generation, issues of semantic information entanglement and insufficient detail in the generated images still persist. This paper attempts to introduce a concept of a tiered embedder in fine-grained image generation, which integrates semantic information from both super and child classes, allowing the diffusion model to better incorporate semantic information and address the issue of semantic entanglement. To address the issue of insufficient detail in fine-grained images, we introduce the concept of super-resolution during the perceptual information generation stage, enhancing the detailed features of fine-grained images through enhancement and degradation models. Furthermore, we propose an efficient ProAttention mechanism that can be effectively implemented in the diffusion model. We evaluate our method through extensive experiments on public benchmarks, demonstrating that our approach outperforms other state-of-the-art fine-tuning methods in terms of performance.", "AI": {"tldr": "提出EFDiT方法，通过层级嵌入器和超分辨率技术解决细粒度图像生成中的语义信息纠缠和细节不足问题", "motivation": "现有基于扩散模型的类别条件生成方法主要关注常见类别，在大规模细粒度图像生成中存在语义信息纠缠和生成图像细节不足的问题", "method": "1) 引入层级嵌入器整合超类和子类的语义信息；2) 在感知信息生成阶段引入超分辨率概念，通过增强和退化模型提升细节特征；3) 提出高效的ProAttention机制", "result": "在公开基准测试上的广泛实验表明，该方法在性能上优于其他最先进的微调方法", "conclusion": "EFDiT方法通过层级语义整合和细节增强机制，有效解决了细粒度图像生成中的语义纠缠和细节不足问题，提升了生成质量"}}
{"id": "2512.05150", "pdf": "https://arxiv.org/pdf/2512.05150", "abs": "https://arxiv.org/abs/2512.05150", "authors": ["Zhenglin Cheng", "Peng Sun", "Jianguo Li", "Tao Lin"], "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows", "categories": ["cs.CV"], "comment": "arxiv v0", "summary": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.", "AI": {"tldr": "提出TwinFlow框架，用于训练1步生成模型，绕过固定预训练教师模型的需求，避免标准对抗网络训练，实现大规模高效模型构建", "motivation": "现有多步生成框架（如扩散和流匹配）推理效率低（需要40-100次函数评估），现有少步方法存在明显限制：蒸馏方法需要迭代蒸馏过程或在极少步数（<4步）时性能显著下降；集成对抗训练的蒸馏方法引入训练不稳定、复杂性增加和高GPU内存开销", "method": "提出TwinFlow框架，通过自对抗流实现一步生成，无需固定预训练教师模型，避免标准对抗网络训练，使用自对抗训练策略", "result": "在文本到图像任务中，1步评估获得GenEval分数0.83，优于SANA-Sprint和RCGM等基线；在Qwen-Image-20B上全参数训练，1步性能匹配原始100步模型在GenEval和DPG-Bench基准上的表现，计算成本降低100倍，质量下降很小", "conclusion": "TwinFlow是一个简单有效的框架，用于训练1步生成模型，避免了固定教师模型和标准对抗网络的需求，适合构建大规模高效模型，在保持高质量的同时显著提升推理效率"}}
{"id": "2512.05145", "pdf": "https://arxiv.org/pdf/2512.05145", "abs": "https://arxiv.org/abs/2512.05145", "authors": ["Inna Wanyin Lin", "Yushi Hu", "Shuyue Stella Li", "Scott Geng", "Pang Wei Koh", "Luke Zettlemoyer", "Tim Althoff", "Marjan Ghazvininejad"], "title": "Self-Improving VLM Judges Without Human Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Effective judges of Vision-Language Models (VLMs) are crucial for model development. Current methods for training VLM judges mainly rely on large-scale human preference annotations. However, such an approach is costly, and the annotations easily become obsolete as models rapidly improve. In this work, we present a framework to self-train a VLM judge model without any human preference annotations, using only self-synthesized data. Our method is iterative and has three stages: (1) generate diverse multimodal instruction-response pairs at varying quality levels, (2) generate reasoning traces and judgments for each pair, removing the ones that do not match our expected quality levels, and (3) training on correct judge answers and their reasoning traces. We evaluate the resulting judge on Multimodal RewardBench and VL-RewardBench across domains: correctness, preference, reasoning, safety, and visual question-answering. Our method improves a Llama-3.2-11B multimodal judge from 0.38 to 0.51 in overall accuracy on VL-RewardBench, often outperforming much larger models including Llama-3.2-90B, GPT-4o, and Claude 3.5 Sonnet, with particularly strong gains in general, hallucination, and reasoning dimensions. The overall strength of these human-annotation-free results suggest the potential for a future self-judge that evolves alongside rapidly improving VLM capabilities.", "AI": {"tldr": "提出一个无需人工标注的自训练视觉语言模型（VLM）评判框架，通过自我合成数据来训练VLM评判模型", "motivation": "当前训练VLM评判模型主要依赖大规模人工偏好标注，这种方法成本高昂，且随着模型快速改进，标注数据容易过时", "method": "采用三阶段迭代框架：1) 生成不同质量级别的多样化多模态指令-响应对；2) 为每个对生成推理轨迹和评判，移除不符合预期质量级别的数据；3) 在正确的评判答案及其推理轨迹上进行训练", "result": "在Multimodal RewardBench和VL-RewardBench上评估，将Llama-3.2-11B多模态评判模型的整体准确率从0.38提升到0.51，在通用性、幻觉和推理维度表现突出，经常超越更大的模型", "conclusion": "无需人工标注的自训练方法展示了未来自我评判模型与快速发展的VLM能力同步进化的潜力"}}
{"id": "2512.05140", "pdf": "https://arxiv.org/pdf/2512.05140", "abs": "https://arxiv.org/abs/2512.05140", "authors": ["Georges Le Bellier", "Nicolas Audebert"], "title": "FlowEO: Generative Unsupervised Domain Adaptation for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Mar 2026, Tucson (AZ), United States", "summary": "The increasing availability of Earth observation data offers unprecedented opportunities for large-scale environmental monitoring and analysis. However, these datasets are inherently heterogeneous, stemming from diverse sensors, geographical regions, acquisition times, and atmospheric conditions. Distribution shifts between training and deployment domains severely limit the generalization of pretrained remote sensing models, making unsupervised domain adaptation (UDA) crucial for real-world applications. We introduce FlowEO, a novel framework that leverages generative models for image-space UDA in Earth observation. We leverage flow matching to learn a semantically preserving mapping that transports from the source to the target image distribution. This allows us to tackle challenging domain adaptation configurations for classification and semantic segmentation of Earth observation images. We conduct extensive experiments across four datasets covering adaptation scenarios such as SAR to optical translation and temporal and semantic shifts caused by natural disasters. Experimental results demonstrate that FlowEO outperforms existing image translation approaches for domain adaptation while achieving on-par or better perceptual image quality, highlighting the potential of flow-matching-based UDA for remote sensing.", "AI": {"tldr": "FlowEO是一个基于生成模型的图像空间无监督域自适应框架，专门用于地球观测数据，通过流匹配技术学习从源域到目标域的语义保持映射。", "motivation": "地球观测数据存在固有的异质性，来自不同传感器、地理区域、采集时间和大气条件，导致训练域和部署域之间的分布偏移严重限制了预训练遥感模型的泛化能力，因此无监督域自适应对于实际应用至关重要。", "method": "利用流匹配学习从源域到目标域图像分布的语义保持映射，通过生成模型进行图像空间的无监督域自适应，处理地球观测图像分类和语义分割中的挑战性域自适应配置。", "result": "在四个数据集上的广泛实验表明，FlowEO在域自适应方面优于现有的图像转换方法，同时在感知图像质量方面达到相当或更好的水平，突显了基于流匹配的无监督域自适应在遥感领域的潜力。", "conclusion": "FlowEO展示了生成模型和流匹配技术在地球观测无监督域自适应中的有效性，能够处理SAR到光学图像转换以及自然灾害引起的时空和语义偏移等挑战性场景。"}}
{"id": "2512.05139", "pdf": "https://arxiv.org/pdf/2512.05139", "abs": "https://arxiv.org/abs/2512.05139", "authors": ["Yang Xiang", "Jingwen Zhong", "Yige Yan", "Petros Koutrakis", "Eric Garshick", "Meredith Franklin"], "title": "Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.", "AI": {"tldr": "提出一个基于迁移学习和生成模型的时空卫星图像降尺度框架，通过冻结编码器将预训练的时空表示迁移到扩散模型中，实现从粗分辨率到细分辨率卫星图像的重建。", "motivation": "现有卫星图像降尺度方法在处理长时间序列粗分辨率数据时存在训练数据有限、物理一致性不足的问题，需要一种能够保持时空物理特性且训练效率高的解决方案。", "method": "结合轻量级U-Net迁移编码器和基于扩散的生成模型：先在粗分辨率数据上预训练U-Net学习时空表示，然后冻结其编码器作为物理有意义的潜在特征，迁移到更大的降尺度模型中。", "result": "在亚洲大区域（分为两个子区域和四个季节）的实验中，模型表现出色（R2 = 0.65到0.94），优于确定性U-Net、变分自编码器和先前迁移学习基线，且预测图像保持了物理一致的空间变异性和时间自相关性。", "conclusion": "迁移增强的扩散模型为长时间序列粗分辨率图像的降尺度提供了稳健且物理一致的解决方案，对改进环境暴露评估和长期环境监测具有重要意义。"}}
{"id": "2512.05137", "pdf": "https://arxiv.org/pdf/2512.05137", "abs": "https://arxiv.org/abs/2512.05137", "authors": ["Yunfei Zhang", "Yizhuo He", "Yuanxun Shao", "Zhengtao Yao", "Haoyan Xu", "Junhao Dong", "Zhen Yao", "Zhikang Dong"], "title": "ChromouVQA: Benchmarking Vision-Language Models under Chromatic Camouflaged Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have advanced multimodal understanding, yet still struggle when targets are embedded in cluttered backgrounds requiring figure-ground segregation. To address this, we introduce ChromouVQA, a large-scale, multi-task benchmark based on Ishihara-style chromatic camouflaged images. We extend classic dot plates with multiple fill geometries and vary chromatic separation, density, size, occlusion, and rotation, recording full metadata for reproducibility. The benchmark covers nine vision-question-answering tasks, including recognition, counting, comparison, and spatial reasoning. Evaluations of humans and VLMs reveal large gaps, especially under subtle chromatic contrast or disruptive geometric fills. We also propose a model-agnostic contrastive recipe aligning silhouettes with their camouflaged renderings, improving recovery of global shapes. ChromouVQA provides a compact, controlled benchmark for reproducible evaluation and extension. Code and dataset are available at https://github.com/Chromou-VQA-Benchmark/Chromou-VQA.", "AI": {"tldr": "提出ChromouVQA基准测试，用于评估视觉语言模型在色度伪装图像下的表现", "motivation": "视觉语言模型在目标嵌入杂乱背景需要图形-背景分离的场景中表现不佳，需要系统性的评估基准", "method": "基于石原式色度伪装图像构建大规模多任务基准，包含多种填充几何形状和色度分离参数，涵盖9种视觉问答任务", "result": "人类和VLM评估显示存在显著差距，特别是在细微色度对比或破坏性几何填充条件下；提出的对比学习方法能改善全局形状恢复", "conclusion": "ChromouVQA为可重复评估和扩展提供了紧凑、受控的基准，有助于推动VLM在复杂视觉场景下的发展"}}
{"id": "2512.05136", "pdf": "https://arxiv.org/pdf/2512.05136", "abs": "https://arxiv.org/abs/2512.05136", "authors": ["Yujie Xiao", "Gongzhen Tang", "Deyun Zhang", "Jun Li", "Guangkun Nie", "Haoyu Wang", "Shun Huang", "Tong Liu", "Qinghao Zhao", "Kangyin Chen", "Shenda Hong"], "title": "Fine-tuning an ECG Foundation Model to Predict Coronary CT Angiography Outcomes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Coronary artery disease (CAD) remains a major global health burden. Accurate identification of the culprit vessel and assessment of stenosis severity are essential for guiding individualized therapy. Although coronary CT angiography (CCTA) is the first-line non-invasive modality for CAD diagnosis, its dependence on high-end equipment, radiation exposure, and strict patient cooperation limits large-scale use. With advances in artificial intelligence (AI) and the widespread availability of electrocardiography (ECG), AI-ECG offers a promising alternative for CAD screening. In this study, we developed an interpretable AI-ECG model to predict severe or complete stenosis of the four major coronary arteries on CCTA. On the internal validation set, the model's AUCs for the right coronary artery (RCA), left main coronary artery (LM), left anterior descending artery (LAD), and left circumflex artery (LCX) were 0.794, 0.818, 0.744, and 0.755, respectively; on the external validation set, the AUCs reached 0.749, 0.971, 0.667, and 0.727, respectively. Performance remained stable in a clinically normal-ECG subset, indicating robustness beyond overt ECG abnormalities. Subgroup analyses across demographic and acquisition-time strata further confirmed model stability. Risk stratification based on vessel-specific incidence thresholds showed consistent separation on calibration and cumulative event curves. Interpretability analyses revealed distinct waveform differences between high- and low-risk groups, highlighting key electrophysiological regions contributing to model decisions and offering new insights into the ECG correlates of coronary stenosis.", "AI": {"tldr": "开发一个可解释的AI-ECG模型，通过微调心电图基础模型来预测冠状动脉CT血管造影中四支主要冠状动脉的严重或完全狭窄", "motivation": "冠状动脉疾病是全球主要健康负担，准确识别罪犯血管和评估狭窄程度对个体化治疗至关重要。虽然冠状动脉CT血管造影是CAD诊断的一线非侵入性方法，但其对高端设备、辐射暴露和严格患者配合的依赖限制了大规模使用。随着人工智能和心电图技术的普及，AI-ECG为CAD筛查提供了有前景的替代方案", "method": "开发可解释的AI-ECG模型，通过微调心电图基础模型来预测冠状动脉CT血管造影中四支主要冠状动脉（RCA、LM、LAD、LCX）的严重或完全狭窄。模型在内部和外部验证集上进行评估，并在临床正常心电图亚组中测试鲁棒性", "result": "内部验证集上，模型对RCA、LM、LAD、LCX的AUC分别为0.794、0.818、0.744、0.755；外部验证集上AUC达到0.749、0.971、0.667、0.727。在临床正常心电图亚组中性能保持稳定，亚组分析确认了模型在不同人口统计学和采集时间层面的稳定性。风险分层显示校准和累积事件曲线一致分离", "conclusion": "该研究成功开发了一个可解释的AI-ECG模型，能够有效预测冠状动脉CT血管造影结果，为冠状动脉疾病筛查提供了有前景的非侵入性替代方案。可解释性分析揭示了高风险和低风险组之间的波形差异，为理解冠状动脉狭窄的心电图相关性提供了新见解"}}
{"id": "2512.05134", "pdf": "https://arxiv.org/pdf/2512.05134", "abs": "https://arxiv.org/abs/2512.05134", "authors": ["Zihao Wu"], "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models", "categories": ["cs.CV", "cs.DC", "cs.LG"], "comment": "8 pages main, 8 pages appendix, 16 figures, 5 tables. Code: https://github.com/zihaowu25/InvarDiff", "summary": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.", "AI": {"tldr": "提出InvarDiff方法，通过跨时间步和跨层的特征不变性缓存来加速扩散模型推理，实现2-3倍端到端加速", "motivation": "扩散模型虽然能生成高质量图像，但迭代采样过程非常耗时。作者观察到确定性采样中存在特征不变性，可以利用这种不变性来减少冗余计算", "method": "通过少量确定性运行计算每个时间步、每层、每个模块的二进制缓存计划矩阵，使用重采样校正避免漂移。基于分位数变化度量决定哪些模块在哪些步骤可以重用而非重新计算。在推理时，按照该矩阵指导进行时间步优先和分层缓存", "result": "在DiT和FLUX模型上，InvarDiff实现了2-3倍的端到端加速，对标准质量指标影响最小。视觉质量几乎没有退化，与完整计算相比几乎无差异", "conclusion": "InvarDiff是一种无需训练的加速方法，通过利用扩散模型中的特征不变性进行智能缓存，显著提高了推理速度，同时保持了生成质量"}}
{"id": "2512.05132", "pdf": "https://arxiv.org/pdf/2512.05132", "abs": "https://arxiv.org/abs/2512.05132", "authors": ["Wenshuo Wang", "Fan Zhang"], "title": "Breaking Scale Anchoring: Frequency Representation Learning for Accurate High-Resolution Inference from Low-Resolution Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-Shot Super-Resolution Spatiotemporal Forecasting requires a deep learning model to be trained on low-resolution data and deployed for inference on high-resolution. Existing studies consider maintaining similar error across different resolutions as indicative of successful multi-resolution generalization. However, deep learning models serving as alternatives to numerical solvers should reduce error as resolution increases. The fundamental limitation is, the upper bound of physical law frequencies that low-resolution data can represent is constrained by its Nyquist frequency, making it difficult for models to process signals containing unseen frequency components during high-resolution inference. This results in errors being anchored at low resolution, incorrectly interpreted as successful generalization. We define this fundamental phenomenon as a new problem distinct from existing issues: Scale Anchoring. Therefore, we propose architecture-agnostic Frequency Representation Learning. It alleviates Scale Anchoring through resolution-aligned frequency representations and spectral consistency training: on grids with higher Nyquist frequencies, the frequency response in high-frequency bands of FRL-enhanced variants is more stable. This allows errors to decrease with resolution and significantly outperform baselines within our task and resolution range, while incurring only modest computational overhead.", "AI": {"tldr": "提出频率表示学习（FRL）方法，解决从低分辨率训练到高分辨率推理时的尺度锚定问题，实现误差随分辨率增加而减小", "motivation": "现有零样本超分辨率时空预测方法存在尺度锚定问题：模型在低分辨率训练时受限于奈奎斯特频率，无法处理高分辨率推理时的新频率成分，导致误差被锚定在低分辨率水平", "method": "提出架构无关的频率表示学习（FRL），通过分辨率对齐的频率表示和频谱一致性训练，使模型在高奈奎斯特频率网格上保持高频带频率响应的稳定性", "result": "FRL增强的变体在任务和分辨率范围内显著优于基线方法，误差随分辨率增加而减小，同时仅产生适度的计算开销", "conclusion": "频率表示学习有效缓解了尺度锚定问题，使深度学习模型能够作为数值求解器的替代方案，在高分辨率推理时实现误差的持续减小"}}
{"id": "2512.05131", "pdf": "https://arxiv.org/pdf/2512.05131", "abs": "https://arxiv.org/abs/2512.05131", "authors": ["Tianling Xu", "Shengzhe Gan", "Leslie Gu", "Yuelei Li", "Fangneng Zhan", "Hanspeter Pfister"], "title": "AREA3D: Active Reconstruction Agent with Unified Feed-Forward 3D Perception and Vision-Language Guidance", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Under review", "summary": "Active 3D reconstruction enables an agent to autonomously select viewpoints to efficiently obtain accurate and complete scene geometry, rather than passively reconstructing scenes from pre-collected images. However, existing active reconstruction methods often rely on hand-crafted geometric heuristics, which can lead to redundant observations without substantially improving reconstruction quality. To address this limitation, we propose AREA3D, an active reconstruction agent that leverages feed-forward 3D reconstruction models and vision-language guidance. Our framework decouples view-uncertainty modeling from the underlying feed-forward reconstructor, enabling precise uncertainty estimation without expensive online optimization. In addition, an integrated vision-language model provides high-level semantic guidance, encouraging informative and diverse viewpoints beyond purely geometric cues. Extensive experiments on both scene-level and object-level benchmarks demonstrate that AREA3D achieves state-of-the-art reconstruction accuracy, particularly in the sparse-view regime. Code will be made available at: https://github.com/TianlingXu/AREA3D .", "AI": {"tldr": "提出AREA3D主动重建智能体，结合前馈3D感知和视觉语言引导，实现高效自主的3D场景重建", "motivation": "现有主动重建方法依赖手工几何启发式规则，导致冗余观测且重建质量提升有限，需要更智能的视角选择和不确定性估计方法", "method": "将视角不确定性建模与前馈重建器解耦，实现精确不确定性估计；集成视觉语言模型提供高层语义引导，超越纯几何线索", "result": "在场景级和物体级基准测试中达到最先进的重建精度，尤其在稀疏视角条件下表现优异", "conclusion": "AREA3D通过结合前馈3D重建和视觉语言引导，实现了更高效、更准确的主动3D重建，为自主场景理解提供了新方法"}}
{"id": "2512.05128", "pdf": "https://arxiv.org/pdf/2512.05128", "abs": "https://arxiv.org/abs/2512.05128", "authors": ["Lucas Heublein", "Thorsten Nowak", "Tobias Feigl", "Jaspar Pahl", "Felix Ott"], "title": "GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System", "categories": ["eess.SP", "cs.AI"], "comment": "9 pages, 26 figures, 2 tables", "summary": "Jamming devices disrupt signals from the global navigation satellite system (GNSS) and pose a significant threat by compromising the reliability of accurate positioning. Consequently, the detection and localization of these interference signals are essential to achieve situational awareness, mitigating their impact, and implementing effective countermeasures. In this paper, we utilize a two-times-two patch antenna system (i.e., the software defined radio device Ettus USRP X440) to predict the angle, elevation, and distance to the jamming source based on in-phase and quadrature (IQ) samples. We propose to use an inertial measurement unit (IMU) attached to the antenna system to predict the relative movement of the antenna in dynamic scenarios. We present a synthetic aperture system that enables coherent spatial imaging using platform motion to synthesize larger virtual apertures, offering superior angular resolution without mechanically rotating antennas. While classical angle-of-arrival (AoA) methods exhibit reduced accuracy in multipath environments due to signal reflections and scattering, leading to localization errors, we utilize a methodology that fuses IQ and Fast Fourier Transform (FFT)-computed spectrograms with 22 AoA features and the predicted relative movement to enhance GNSS jammer direction finding.", "AI": {"tldr": "提出一种基于惯性测量的多天线系统，用于动态场景下的GNSS干扰源方向探测", "motivation": "GNSS干扰设备威胁定位系统可靠性，需要有效检测和定位干扰源以实现态势感知和反制", "method": "使用2×2贴片天线系统（USRP X440）采集IQ样本，结合IMU预测天线相对运动，构建合成孔径系统，融合IQ数据、FFT频谱图、22个AoA特征和相对运动预测", "result": "能够预测干扰源的角度、仰角和距离，在动态场景中实现高精度方向探测，克服多径环境下的传统AoA方法精度下降问题", "conclusion": "提出的惯性辅助多天线系统在动态场景下显著提升了GNSS干扰源方向探测的准确性和鲁棒性"}}
{"id": "2512.05126", "pdf": "https://arxiv.org/pdf/2512.05126", "abs": "https://arxiv.org/abs/2512.05126", "authors": ["Kaidi Wang", "Yi He", "Wenhao Guan", "Weijie Wu", "Hongwu Ding", "Xiong Zhang", "Di Wu", "Meng Meng", "Jian Luan", "Lin Li", "Qingyang Hong"], "title": "SyncVoice: Towards Video Dubbing with Vision-Augmented Pretrained TTS Model", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Video dubbing aims to generate high-fidelity speech that is precisely temporally aligned with the visual content. Existing methods still suffer from limitations in speech naturalness and audio-visual synchronization, and are limited to monolingual settings. To address these challenges, we propose SyncVoice, a vision-augmented video dubbing framework built upon a pretrained text-to-speech (TTS) model. By fine-tuning the TTS model on audio-visual data, we achieve strong audiovisual consistency. We propose a Dual Speaker Encoder to effectively mitigate inter-language interference in cross-lingual speech synthesis and explore the application of video dubbing in video translation scenarios. Experimental results show that SyncVoice achieves high-fidelity speech generation with strong synchronization performance, demonstrating its potential in video dubbing tasks.", "AI": {"tldr": "SyncVoice是一个基于预训练TTS模型的视频配音框架，通过视觉增强实现高质量语音生成和音视频同步，支持跨语言视频翻译场景", "motivation": "现有视频配音方法在语音自然度和音视频同步方面存在局限，且仅限于单语言设置，需要解决这些问题以支持跨语言视频翻译", "method": "基于预训练TTS模型构建视觉增强框架，通过音频-视觉数据微调实现音视频一致性，采用双说话人编码器缓解跨语言语音合成中的语言间干扰", "result": "实验结果表明SyncVoice实现了高保真语音生成和强同步性能，在视频配音任务中展现出潜力", "conclusion": "SyncVoice框架通过视觉增强的预训练TTS模型有效解决了视频配音中的语音自然度、音视频同步和跨语言问题，为视频翻译应用提供了可行方案"}}
{"id": "2512.05122", "pdf": "https://arxiv.org/pdf/2512.05122", "abs": "https://arxiv.org/abs/2512.05122", "authors": ["Unnikrishnan Radhakrishnan"], "title": "Documenting SME Processes with Conversational AI: From Tacit Knowledge to BPMN", "categories": ["cs.AI"], "comment": "Presented at 2025 International Workshop on Low-Cost Digital Solutions for Industrial Automation (LODISA)", "summary": "Small and medium-sized enterprises (SMEs) still depend heavily on tacit, experience-based know-how that rarely makes its way into formal documentation. This paper introduces a large-language-model (LLM)-driven conversational assistant that captures such knowledge on the shop floor and converts it incrementally and interactively into standards-compliant Business Process Model and Notation (BPMN) 2.0 diagrams. Powered by Gemini 2.5 Pro and delivered through a lightweight Gradio front-end with client-side bpmn-js visualisation, the assistant conducts an interview-style dialogue: it elicits process details, supports clarifying dialogue and on-demand analysis, and renders live diagrams that users can refine in real time. A proof-of-concept evaluation in an equipment-maintenance scenario shows that the chatbot produced an accurate \"AS-IS\" model, flagged issues via on-diagram annotations, and generated an improved \"TO-BE\" variant, all within about 12-minutes, while keeping API costs within an SME-friendly budget. The study analyses latency sources, model-selection trade-offs, and the challenges of enforcing strict XML schemas, then outlines a roadmap toward agentic and multimodal deployments. The results demonstrate that conversational LLMs can potentially be used to lower the skill and cost barriers to rigorous process documentation, helping SMEs preserve institutional knowledge, enhance operational transparency, and accelerate continuous-improvement efforts.", "AI": {"tldr": "开发一个基于大型语言模型的对话助手，帮助中小企业将隐性知识转化为标准BPMN流程文档", "motivation": "中小企业严重依赖隐性经验知识，这些知识很少被正式记录，导致知识流失和流程透明度不足，需要降低流程文档化的技能和成本门槛", "method": "使用Gemini 2.5 Pro驱动的对话助手，通过Gradio前端界面和bpmn-js可视化，进行访谈式对话，逐步将隐性知识转化为BPMN 2.0图表", "result": "在设备维护场景的概念验证中，聊天机器人在约12分钟内生成了准确的\"现状\"模型，通过图表标注发现问题，并生成改进的\"未来\"版本，API成本控制在中小企业可接受范围内", "conclusion": "对话式大型语言模型能够降低严格流程文档化的技能和成本障碍，帮助中小企业保存机构知识、增强运营透明度并加速持续改进工作"}}
{"id": "2512.05121", "pdf": "https://arxiv.org/pdf/2512.05121", "abs": "https://arxiv.org/abs/2512.05121", "authors": ["Tianshun Han", "Benjia Zhou", "Ajian Liu", "Yanyan Liang", "Du Zhang", "Zhen Lei", "Jun Wan"], "title": "PESTalk: Speech-Driven 3D Facial Animation with Personalized Emotional Styles", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "PESTalk is a novel method for generating 3D facial animations with personalized emotional styles directly from speech. It overcomes key limitations of existing approaches by introducing a Dual-Stream Emotion Extractor (DSEE) that captures both time and frequency-domain audio features for fine-grained emotion analysis, and an Emotional Style Modeling Module (ESMM) that models individual expression patterns based on voiceprint characteristics. To address data scarcity, the method leverages a newly constructed 3D-EmoStyle dataset. Evaluations demonstrate that PESTalk outperforms state-of-the-art methods in producing realistic and personalized facial animations.", "AI": {"tldr": "PESTalk是一种从语音直接生成具有个性化情感风格的3D面部动画的新方法，通过双流情感提取器和情感风格建模模块实现个性化情感表达。", "motivation": "现有方法在生成具有个性化情感风格的3D面部动画方面存在局限性，缺乏对个体表达模式的建模能力，且面临数据稀缺问题。", "method": "提出双流情感提取器（DSEE）同时捕捉时域和频域音频特征进行细粒度情感分析，以及情感风格建模模块（ESMM）基于声纹特征建模个体表达模式，并构建了新的3D-EmoStyle数据集解决数据稀缺问题。", "result": "评估表明PESTalk在生成逼真且个性化的面部动画方面优于现有最先进方法。", "conclusion": "PESTalk通过创新的双流情感提取和个性化情感风格建模，成功实现了从语音到具有个性化情感风格的3D面部动画的高质量生成。"}}
{"id": "2512.05119", "pdf": "https://arxiv.org/pdf/2512.05119", "abs": "https://arxiv.org/abs/2512.05119", "authors": ["Rongyang Zhang", "Yuqing Huang", "Chengqiang Lu", "Qimeng Wang", "Yan Gao", "Yi Wu", "Yao Hu", "Yin Xu", "Wei Wang", "Hao Wang", "Enhong Chen"], "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "26 pages, 6 figures, NeurIPS 2025 D&B Track poster", "summary": "In real-world scenarios, providing user queries with visually enhanced responses can considerably benefit understanding and memory, underscoring the great value of interleaved image-text generation. Despite recent progress, like the visual autoregressive model that unifies text and image processing in a single transformer architecture, generating high-quality interleaved content remains challenging. Moreover, evaluations of these interleaved sequences largely remain underexplored, with existing benchmarks often limited by unimodal metrics that inadequately assess the intricacies of combined image-text outputs. To address these issues, we present RAG-IGBench, a thorough benchmark designed specifically to evaluate the task of Interleaved Generation based on Retrieval-Augmented Generation (RAG-IG) in open-domain question answering. RAG-IG integrates multimodal large language models (MLLMs) with retrieval mechanisms, enabling the models to access external image-text information for generating coherent multimodal content. Distinct from previous datasets, RAG-IGBench draws on the latest publicly available content from social platforms and introduces innovative evaluation metrics that measure the quality of text and images, as well as their consistency. Through extensive experiments with state-of-the-art MLLMs (both open-source and proprietary) on RAG-IGBench, we provide an in-depth analysis examining the capabilities and limitations of these models. Additionally, we validate our evaluation metrics by demonstrating their high correlation with human assessments. Models fine-tuned on RAG-IGBench's training set exhibit improved performance across multiple benchmarks, confirming both the quality and practical utility of our dataset. Our benchmark is available at https://github.com/USTC-StarTeam/RAG-IGBench.", "AI": {"tldr": "提出了RAG-IGBench基准，用于评估基于检索增强生成（RAG）的交错图像-文本生成在开放域问答任务中的性能", "motivation": "现实场景中，提供视觉增强的响应能显著提升理解和记忆效果，但现有评估方法主要依赖单模态指标，无法充分评估图像-文本交错生成的质量和一致性", "method": "构建了RAG-IGBench基准，从社交平台获取最新公开内容，并设计了创新的评估指标来测量文本质量、图像质量以及两者的一致性", "result": "通过对最先进的多模态大语言模型（包括开源和专有模型）进行广泛实验，深入分析了这些模型的能力和局限性；验证了评估指标与人类评估的高度相关性；在训练集上微调的模型在多个基准上表现出改进的性能", "conclusion": "RAG-IGBench为交错图像-文本生成提供了全面的评估框架，证实了数据集的质量和实际效用，有助于推动该领域的发展"}}
{"id": "2512.05117", "pdf": "https://arxiv.org/pdf/2512.05117", "abs": "https://arxiv.org/abs/2512.05117", "authors": ["Prakhar Kaushik", "Shravan Chaudhari", "Ankit Vaidya", "Rama Chellappa", "Alan Yuille"], "title": "The Universal Weight Subspace Hypothesis", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "37 pages", "summary": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.", "AI": {"tldr": "该论文提出了\"通用权重子空间假设\"，发现不同任务训练的深度神经网络会收敛到相似的、低维的参数子空间。", "motivation": "研究深度神经网络在不同任务训练后是否具有内在的结构相似性，探索模型参数空间的本质组织方式。", "method": "对1100多个模型进行模态谱分析，包括500个Mistral-7B LoRA模型、500个Vision Transformers和50个LLaMA-8B模型，使用谱分解技术分析权重矩阵，识别共享的低维子空间。", "result": "发现神经网络无论初始化、任务或领域如何，都会系统性地收敛到共享的谱子空间，仅需几个主方向就能捕获大部分方差，识别出稀疏的联合子空间。", "conclusion": "深度神经网络存在内在的通用子空间结构，这对模型复用、多任务学习、模型融合以及高效训练算法开发具有重要意义，可能减少大规模神经模型的碳足迹。"}}
{"id": "2512.05116", "pdf": "https://arxiv.org/pdf/2512.05116", "abs": "https://arxiv.org/abs/2512.05116", "authors": ["Zhen Liu", "Tim Z. Xiao", "Carles Domingo-Enrich", "Weiyang Liu", "Dinghuai Zhang"], "title": "Value Gradient Guidance for Flow Matching Alignment", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at NeurIPS 2025; 26 pages, 20 figures", "summary": "While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.", "AI": {"tldr": "提出VGG-Flow方法，基于最优控制理论，通过梯度匹配来微调预训练的流匹配模型，实现高效对齐并保持概率先验", "motivation": "现有方法在微调流匹配模型时无法同时实现适应效率和概率先验保持，需要一种既能有效对齐人类偏好又能保持模型原有概率分布的方法", "method": "基于最优控制理论，提出VGG-Flow算法，核心思想是最优微调速度场与预训练速度场之间的差异应与价值函数的梯度场相匹配，利用奖励模型的一阶信息和价值函数的启发式初始化实现快速适应", "result": "在流行的文本到图像流匹配模型Stable Diffusion 3上验证，在有限计算预算下能够有效微调流匹配模型，实现有效且保持先验的对齐", "conclusion": "VGG-Flow方法成功解决了流匹配模型对齐中的效率与先验保持问题，为生成模型的人类偏好对齐提供了新的有效解决方案"}}
