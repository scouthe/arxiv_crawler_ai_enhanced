{"id": "2512.17909", "pdf": "https://arxiv.org/pdf/2512.17909", "abs": "https://arxiv.org/abs/2512.17909", "authors": ["Shilong Zhang", "He Zhang", "Zhifei Zhang", "Chongjian Ge", "Shuchen Xue", "Shaoteng Liu", "Mengwei Ren", "Soo Ye Kim", "Yuqian Zhou", "Qing Liu", "Daniil Pakhomov", "Kai Zhang", "Zhe Lin", "Ping Luo"], "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing", "categories": ["cs.CV"], "comment": "Project Page: https://jshilong.github.io/PS-VAE-PAGE/", "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.", "AI": {"tldr": "本文提出了一种框架，通过引入语义像素重建目标来调节潜在空间，使理解导向的编码器特征适合生成任务。", "motivation": "在采用高维表示编码器作为生成潜变量的趋势中，识别到两个主要障碍：缺乏紧凑正则化导致扩散模型产生离流形潜变量；弱像素级重建妨碍了生成器学习精确的细粒度几何和纹理。", "method": "引入了一种语义像素重建目标以调节潜在空间，并设计了一个统一的文字转图像（T2I）和图像编辑模型。该框架确保了在紧凑表示中压缩语义信息和细节，同时保持高图像重建质量。", "result": "展示了比现有方法更好的重构性能、更快的收敛速度以及显著提高的T2I和编辑任务表现。", "conclusion": "证明了通过适当调节，理解导向编码器可以被有效转化为强大的生成组件。"}}
{"id": "2512.17908", "pdf": "https://arxiv.org/pdf/2512.17908", "abs": "https://arxiv.org/abs/2512.17908", "authors": ["Ananta R. Bhattarai", "Helge Rhodin"], "title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.", "AI": {"tldr": "本文提出了一种测试时间自监督框架Re-Depth Anything，该框架通过重新照明预测的深度图来增强单目深度估计。", "motivation": "近年来，像DA-V2这样的基础模型在处理远离训练分布的真实世界图像时表现不佳。为此，我们引入了Re-Depth Anything以缩小领域差距，并提高真实场景中的深度准确性。", "method": "我们的方法通过将预测的深度图重新照明并与大规模2D扩散模型结合，在输入上直接执行无标签细化。此重新合成过程利用形状从阴影（SfS）线索，采用新颖的生成上下文和得分蒸馏采样（SDS）。为了防止优化崩溃，我们冻结编码器并仅更新中间嵌入。", "result": "在各种基准测试中，Re-Depth Anything相较于DA-V2显著提高了深度准确性和现实性。", "conclusion": "该研究展示了自监督的新途径，并增强了几何推理。"}}
{"id": "2512.17907", "pdf": "https://arxiv.org/pdf/2512.17907", "abs": "https://arxiv.org/abs/2512.17907", "authors": ["Byungjun Kim", "Taeksoo Kim", "Junyoung Lee", "Hanbyul Joo"], "title": "Dexterous World Models", "categories": ["cs.CV"], "comment": "Project Page: snuvclab.github.io/dwm", "summary": "Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes. Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics. Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.", "AI": {"tldr": "该论文提出了一种基于视频扩散的框架，能够根据静态3D场景和第一人称手部动作序列生成具有时空一致性的交互式视频。", "motivation": "当前的数字孪生模型缺乏动态变化的能力，无法实现人类与环境之间的实体互动。为此，研究者希望开发一种能够模拟这种互动的方法。", "method": "提出了一个基于场景和动作条件化的视频扩散框架（DWM），该框架可以生成第一人称视角下的手部运动序列在静态3D场景中的动态变化，并保持时空一致性。", "result": "实验结果表明，所提方法能够在维护相机与场景一致性的前提下实现如抓取、开启和移动物体等逼真且物理上合理的互动。", "conclusion": "该研究是基于视频扩散的交互式数字孪生模型的第一步，并能从第一人称视角动作中进行实体模拟。"}}
{"id": "2512.17902", "pdf": "https://arxiv.org/pdf/2512.17902", "abs": "https://arxiv.org/abs/2512.17902", "authors": ["Jonathon Fox", "William J Buchanan", "Pavlos Papadopoulos"], "title": "Adversarial Robustness of Vision in Open Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "ef:IEEE Access, 2025", "summary": "With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.", "AI": {"tldr": "研究了LLaVA和Llama Vision在视觉输入模式下对未定向PGD攻击的鲁棒性。", "motivation": "随着深度学习的发展，理解AI系统识别物体的方式变得越来越困难。因此，对手可能会通过添加未见过的元素来修改图像，从而混淆AI对实体的识别。", "method": "使用未定向PGD测试LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2，并在Visual Question Answering (VQA) v2数据集子集上进行实证评估。结果用标准VQA准确率指标量化。", "result": "结果显示，尽管基线准确率较低，Llama Vision在攻击下性能下降幅度较小，特别是在更高的扰动水平下表现优于LLaVA。", "conclusion": "研究证实了视觉模态是当代开放权重视觉语言模型(VLMs)的潜在攻击向量，并强调对抗鲁棒性不一定直接与标准基准性能相关，可能受底层架构和训练因素的影响。"}}
{"id": "2512.17901", "pdf": "https://arxiv.org/pdf/2512.17901", "abs": "https://arxiv.org/abs/2512.17901", "authors": ["Junyu Zhang", "Yifan Sun", "Tianang Leng", "Jingyan Shen", "Liu Ziyin", "Paul Pu Liang", "Huan Zhang"], "title": "When Reasoning Meets Its Laws", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "AI": {"tldr": "论文提出了推理法则(LoRe)框架，用于表征大型推理模型(LRM)中的内在推理模式，并通过LoRe-Bench基准测试验证了这些法则的单调性和组合性。", "motivation": "尽管大型推理模型在性能上表现出色，但它们的行为常常是非直观的，导致次优的推理能力。为了理论形式化期望的推理行为，提出了LoRe框架来表征内在模式。", "method": "论文首先提出计算法则并假设推理计算应与问题复杂性呈线性比例关系；进一步扩展了准确性法则，并通过单调性和组合性的性质评估这些假设；引入了用于系统测量这些属性的LoRe-Bench基准测试。还开发了一种有效的微调方法，以强制执行计算法和组合性。", "result": "评估结果表明大多数推理模型表现出合理的单调性但缺乏组合性。更好的遵守计算法则可显著提高在多个基准上的推理性能，并且不同属性之间的协同效应也被发现。", "conclusion": "通过LoRe框架的开发，论文揭示了大型推理模型中的内在模式并展示了强制执行这些模式可以改进推理性能的能力。"}}
{"id": "2512.17900", "pdf": "https://arxiv.org/pdf/2512.17900", "abs": "https://arxiv.org/abs/2512.17900", "authors": ["Vongani H. Maluleke", "Kie Horiuchi", "Lea Wilken", "Evonne Ng", "Jitendra Malik", "Angjoo Kanazawa"], "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/", "AI": {"tldr": "MAGNet是一种用于多智能体运动生成的统一自回归扩散框架，支持多种交互任务。", "motivation": "现有的运动生成方法大多特定于某些任务，难以泛化到灵活的多代理生成。提出了一种可以处理长时序、强代理间依赖和不同群体规模的新模型。", "method": "MAGNet引入了扩散强迫技术，在自回归去噪过程中显式地建模代理间的耦合关系，支持成对预测、伙伴填充和全多智能体运动生成。", "result": "该方法在双人基准测试中表现与专用方法相当，并能自然扩展到涉及三个或更多交互个体的多点场景。", "conclusion": "MAGNet提供了一种处理复杂社交互动的有效方式，可应用于广泛的交互任务。"}}
{"id": "2512.17898", "pdf": "https://arxiv.org/pdf/2512.17898", "abs": "https://arxiv.org/abs/2512.17898", "authors": ["Robin Schimmelpfennig", "Mark Díaz", "Vinodkumar Prabhakaran", "Aida Davani"], "title": "Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally", "categories": ["cs.AI"], "comment": null, "summary": "Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI's human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.", "AI": {"tldr": "本文通过在全球范围内进行的大规模跨国家实验，研究人类特征设计对AI系统交互中用户参与度和信任的影响。", "motivation": "探讨人类特质设计是否能增加用户与AI系统的互动中的投入和信任，以及这种影响是否会因文化差异而有所不同", "method": "通过对10个不同国家的3500名参与者进行实时开放式实验，评估了用户在评价AI的人类特征时的关注点，并测试特定设计选择如何影响跨文化的自我报告的信任水平。", "result": "发现当评价AI的人类特质时，用户更加注重交互中的实际线索而不是理论上的概念；人类特质的设计确实可以增加用户的拟人化感受，但这种效果并不总是提升参与度和信任感。设计的效果在不同文化中存在显著差异。", "conclusion": "挑战了关于人类特征的AI设计固有风险的观点，并揭示了一个复杂的、受文化影响的人机互动图景，表明需要采取更加定制化的治理策略来应对这一问题。"}}
{"id": "2512.17897", "pdf": "https://arxiv.org/pdf/2512.17897", "abs": "https://arxiv.org/abs/2512.17897", "authors": ["Tomer Borreda", "Fangqiang Ding", "Sanja Fidler", "Shengyu Huang", "Or Litany"], "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project page: https://radargen.github.io/", "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.", "AI": {"tldr": "RadarGen利用多视角相机图像生成逼真的汽车雷达点云。", "motivation": "现有的视觉数据集和模拟框架难以直接生成真实的雷达数据，该研究旨在通过图像引导的扩散模型来弥合这一差距，并提供一种跨感知模态统一生成性仿真的方法。", "method": "RadarGen使用高效的图像-隐式扩散技术，在鸟瞰视角下表示具有空间结构、雷达交叉截面和多普勒属性的雷达测量。结合预训练的基础模型提取深度、语义和运动线索，以引导生成过程趋向物理上合理的雷达模式。", "result": "在大规模驾驶数据上的评估表明，RadarGen捕捉到典型的雷达测量分布，并减少了与基于真实数据训练的感知模型之间的差距。", "conclusion": "RadarGen为跨感知模态统一生成性仿真提供了一种有前途的方法。"}}
{"id": "2512.17896", "pdf": "https://arxiv.org/pdf/2512.17896", "abs": "https://arxiv.org/abs/2512.17896", "authors": ["Xinru Wang", "Ming Yin", "Eunyee Koh", "Mustafa Doga Dogan"], "title": "XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows", "categories": ["cs.HC"], "comment": null, "summary": "As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users' needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.", "AI": {"tldr": "XAgen是一种可解释性工具，旨在帮助用户识别和纠正多代理工作流程中的错误。", "motivation": "随着大型语言模型驱动的多代理系统的广泛采用，系统可能以不透明的方式失败，使得用户难以观察、理解和修正问题。为了满足这些需求，设计了XAgen来支持不同AI专业知识水平的用户。", "method": "通过与12名从业者进行形式访谈，确定现有观测工具与用户需求之间的差距，基于此设计了XAgen工具，具有日志可视化、人机交互反馈和自动错误检测等核心功能。在8名参与者的研究中评估了XAgen的效果。", "result": "XAgen帮助用户更容易地定位失败点，并将其归因于特定的代理或步骤，从而迭代改进配置。", "conclusion": "XAgen为可解释性AI开发提供了以人为中心的设计指南，并突显了更上下文感知交互调试的机会。"}}
{"id": "2512.17893", "pdf": "https://arxiv.org/pdf/2512.17893", "abs": "https://arxiv.org/abs/2512.17893", "authors": ["Sven Benjamin Kožić", "Vinko Zlatić", "Fabio Franchini", "Salvatore Marco Giampaolo"], "title": "Exploring the Effect of Basis Rotation on NQS Performance", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.", "AI": {"tldr": "研究局部基旋转对神经量子态(NQS)性能的影响及其机制", "motivation": "理解NQS表现与所选基底的关系，揭示局部基旋转如何改变波函数在参数空间中的位置和优化效果", "method": "使用可解析的一维伊辛模型，通过计算量子费舍尔信息和福比尼-斯图迪距离来量化波函数移动情况；探究浅层架构(NQS)的训练结果，并引入分析旋转伊辛模型框架研究损失景观中信息几何障碍对NQS优化的影响", "result": "局部基旋转使精确波函数在参数空间内移动，增加其与初始化点的几何距离；浅层架构容易陷入鞍点区域，在铁磁情形下高曲率壁垒阻止了优化过程", "conclusion": "局部基旋转揭示了解决NQS性能的关键障碍，强调在可变训练中需考虑损失景观设计"}}
{"id": "2512.17892", "pdf": "https://arxiv.org/pdf/2512.17892", "abs": "https://arxiv.org/abs/2512.17892", "authors": ["Landon Taylor", "Joshua Jeppson", "Ahmed Irfan", "Lukas Buecherl", "Chris Myers", "Zhen Zhang"], "title": "Prefix Trees Improve Memory Consumption in Large-Scale Continuous-Time Stochastic Models", "categories": ["cs.DS", "cs.FL"], "comment": "21 pages, 2 figures", "summary": "Highly-concurrent system models with vast state spaces like Chemical Reaction Networks (CRNs) that model biological and chemical systems pose a formidable challenge to cutting-edge formal analysis tools. Although many symbolic approaches have been presented, transient probability analysis of CRNs, modeled as Continuous-Time Markov Chains (CTMCs), requires explicit state representation. For that purpose, current cutting-edge methods use hash maps, which boast constant average time complexity and linear memory complexity. However, hash maps often suffer from severe memory limitations on models with immense state spaces. To address this, we propose using prefix trees to store states for large, highly concurrent models (particularly CRNs) for memory savings. We present theoretical analyses and benchmarks demonstrating the favorability of prefix trees over hash maps for very large state spaces. Additionally, we propose using a Bounded Model Checking (BMC) pre-processing step to impose a variable ordering to further improve memory usage along with preliminary evaluations suggesting its effectiveness. We remark that while our work is motivated primarily by the challenges posed by CRNs, it is generalizable to all CTMC models.", "AI": {"tldr": "本文提出使用前缀树来存储连续时间马尔可夫链模型的状态以减少内存消耗，主要针对化学反应网络等大规模并发系统。", "motivation": "当前用于处理化学反应网络（CRNs）的高并发系统状态空间的方法存在严重的内存限制问题。为了克服这一挑战，本文提出了使用前缀树来存储状态，从而节省内存。", "method": "本文采用前缀树作为状态存储结构，并提出了一种基于有界模型检查的预处理步骤以优化变量排序并进一步提高内存利用率。", "result": "通过理论分析和基准测试证明了在非常大的状态空间中使用前缀树优于哈希表，同时提出了初步评估表明边界模型检测预处理的有效性。", "conclusion": "本文的研究虽然主要针对化学反应网络中的挑战，但其方法具有普适性，适用于所有连续时间马尔可夫链模型。"}}
{"id": "2512.17891", "pdf": "https://arxiv.org/pdf/2512.17891", "abs": "https://arxiv.org/abs/2512.17891", "authors": ["Kristoffer Wickstrøm", "Teresa Dorszewski", "Siyan Chen", "Michael Kampffmeyer", "Elisabeth Wetzer", "Robert Jenssen"], "title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training", "categories": ["cs.CV"], "comment": null, "summary": "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.", "AI": {"tldr": "本论文提出了一种将视觉变换器（ViT）转换为自我解释模型的方法，称为关键点计数分类器（KCCs），这种方法不需要重新训练。", "motivation": "现有的设计自解释模型的方法需要复杂的训练过程和特定的架构，使得这些方法变得不切实际。随着基于ViTs的基础模型的进步，这种不可行性变得更加明显。", "method": "该论文利用了最近的研究成果，即ViT能够自动且精确地在图像之间识别匹配的关键点，并在此基础上构建了一种易于解释的决策过程，这个过程可以直观地展示在输入中。", "result": "评估显示KCCs提高了人机交流效果，优于近期的一些基准方法。", "conclusion": "该论文认为KCCs为使基于ViT的基础模型更加透明和可靠迈出了重要一步。"}}
{"id": "2512.17883", "pdf": "https://arxiv.org/pdf/2512.17883", "abs": "https://arxiv.org/abs/2512.17883", "authors": ["Hye-Young Jo", "Mose Sakashita", "Aditi Mishra", "Ryo Suzuki", "Koichiro Niinuma", "Aakar Gupta"], "title": "Map2Video: Street View Imagery Driven AI Video Generation", "categories": ["cs.HC"], "comment": "15 pages, 19 figures", "summary": "AI video generation has lowered barriers to video creation, but current tools still struggle with inconsistency. Filmmakers often find that clips fail to match characters and backgrounds, making it difficult to build coherent sequences. A formative study with filmmakers highlighted challenges in shot composition, character motion, and camera control. We present Map2Video, a street view imagery-driven AI video generation tool grounded in real-world geographies. The system integrates Unity and ComfyUI with the VACE video generation model, as well as OpenStreetMap and Mapillary for street view imagery. Drawing on familiar filmmaking practices such as location scouting and rehearsal, Map2Video enables users to choose map locations, position actors and cameras in street view imagery, sketch movement paths, refine camera motion, and generate spatially consistent videos. We evaluated Map2Video with 12 filmmakers. Compared to an image-to-video baseline, it achieved higher spatial accuracy, required less cognitive effort, and offered stronger controllability for both scene replication and open-ended creative exploration.", "AI": {"tldr": "Map2Video是一款基于街景影像的AI视频生成工具，旨在解决现有视频生成工具中的不一致性问题。", "motivation": "现有的AI视频生成工具在视频片段与角色、背景的一致性上存在问题，导致难以构建连贯的序列。通过研究发现，拍摄角度、人物运动和摄像机控制是主要挑战。", "method": "Map2Video将Unity和ComfyUI集成在一起，并结合VACE视频生成模型及OpenStreetMap和Mapillary提供的街景影像数据，使用户能够选择地点定位演员和相机，设计动作路径并调整镜头移动以生成空间一致性的视频。", "result": "通过与12名电影制作人合作测试，相比图像转视频的基础线方法，Map2Video在提高空间准确性、减少认知负担以及提供场景再现和开放创意探索的控制方面表现出色。", "conclusion": "该研究提出了一种创新的方法来解决现有AI视频生成工具中的不一致性问题，并通过实际应用验证了其有效性和优越性。"}}
{"id": "2512.17882", "pdf": "https://arxiv.org/pdf/2512.17882", "abs": "https://arxiv.org/abs/2512.17882", "authors": ["Dominik Szczepaniak", "Monika Harvey", "Fani Deligianni"], "title": "Your Eyes Controlled the Game: Real-Time Cognitive Training Adaptation based on Eye-Tracking and Physiological Data in Virtual Reality", "categories": ["cs.HC"], "comment": null, "summary": "Cognitive training for sustained attention and working memory is vital across domains relying on robust mental capacity such as education or rehabilitation. Adaptive systems are essential, dynamically matching difficulty to user ability to maintain engagement and accelerate learning. Current adaptive systems often rely on simple performance heuristics or predict visual complexity and affect instead of cognitive load. This study presents the first implementation of real-time adaptive cognitive load control in Virtual Reality cognitive training based on eye-tracking and physiological data. We developed a bidirectional LSTM model with a self-attention mechanism, trained on eye-tracking and physiological (PPG, GSR) data from 74 participants. We deployed it in real-time with 54 participants across single-task (sustained attention) and dual-task (sustained attention + mental arithmetic) paradigms. Difficulty was adjusted dynamically based on participant self-assessment or model's real-time cognitive load predictions. Participants showed a tendency to estimate the task as too difficult, even though they were objectively performing at their best. Over the course of a 10-minute session, both adaptation methods converged at equivalent difficulty in single-task scenarios, with no significant differences in subjective workload or game performance. However, in the dual-task conditions, the model successfully pushed users to higher difficulty levels without performance penalties or increased frustration, highlighting a user tendency to underestimate capacity under high cognitive load. Findings indicate that machine learning models may provide more objective cognitive capacity assessments than self-directed approaches, mitigating subjective performance biases and enabling more effective training by pushing users beyond subjective comfort zones toward physiologically-determined optimal challenge levels.", "AI": {"tldr": "基于眼动和生理数据的实时认知负荷控制，进行虚拟现实中的认知训练适应性调整。", "motivation": "传统的自适应系统依赖简单的性能规则或预测视觉复杂性和情绪而非认知负载。本文研究旨在开发更客观的认知能力评估方法，以提高学习效率。", "method": "利用双向LSTM模型和自我注意机制处理眼动及生理（PPG，GSR）数据，从74名参与者处收集数据并训练模型；在54名参与者的单任务和双任务环境中部署实时调整难度的系统。", "result": "在10分钟的会话期间，两种适应方法在单任务情况下达到相同的难度水平，主观工作量或游戏性能没有显著差异。但在双任务条件下，模型能够将用户推向更高的难度而不会产生性能下降或增加挫败感。", "conclusion": "机器学习模型可能比自我定向的方法提供更客观的认知能力评估，从而减少主观偏见并促进在生理确定的最佳挑战水平上的训练效率提升。"}}
{"id": "2512.17878", "pdf": "https://arxiv.org/pdf/2512.17878", "abs": "https://arxiv.org/abs/2512.17878", "authors": ["Herlock Rahimi"], "title": "Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "26 pages, 1 figure", "summary": "Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics. A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.", "AI": {"tldr": "本文通过引入加权随机微分方程来实现Wasserstein-Fisher-Rao梯度流，以改进扩散模型的采样机制。", "motivation": "当前的连续生成式建模方法在处理非凸或多元分布时性能下降。为此，研究者试图利用信息几何工具增强采样方案。", "method": "通过引入显式的校正项，并结合Feynman-Kac表示法实现加权随机微分方程。", "result": "本文提供了初步但严格的WFR基于采样动力学的调查，澄清了其几何结构和算子理论。", "conclusion": "该方法为未来在理论上和技术上的进一步发展奠定了基础。"}}
{"id": "2512.17875", "pdf": "https://arxiv.org/pdf/2512.17875", "abs": "https://arxiv.org/abs/2512.17875", "authors": ["Haiwen Feng", "Long Lian", "Lisa Dunlap", "Jiahao Shu", "XuDong Wang", "Renhao Wang", "Trevor Darrell", "Alane Suhr", "Angjoo Kanazawa"], "title": "Visually Prompted Benchmarks Are Surprisingly Fragile", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.", "AI": {"tldr": "本文探讨了视觉提示基准测试中视觉标记变化对多模态模型性能的影响，并展示了如何通过调整这些细节来改变模型的排行榜位置。", "motivation": "现有视觉语言模型在评估时容易受到视觉提示设计中的微小变化影响，这表明当前的基准测试存在不稳定性和脆弱性。因此，研究者们希望揭示这种现象背后的机制，并提出改进方法以提高基准测试的可靠性。", "method": "通过改变视觉标记的颜色、大小以及其他低级推理选择（如JPEG压缩级别）来评估九个常见的开源和闭源VLM模型在两个视觉提示任务上的性能。此外，还创建了一个包含16种不同视觉标记变体的大规模视觉提示基准VPBench。", "result": "实验结果表明，细微的视觉提示变化可以显著影响模型的表现和排行榜排名；一些较小或较新的模型可以通过调整这些参数超越较大的专有模型。这强调了在评估多模态系统时需要更加关注细节设计的重要性。", "conclusion": "研究发现视觉语言模型对视觉提示的设计非常敏感，即使是看似无关的小变化也可能导致性能显著不同。为了提高基准测试的稳定性，研究人员提出了一种新的大型视觉提示基准VPBench，并提供了详细的分析工具以帮助未来的研究者更好地理解和评估这些模型。"}}
{"id": "2512.17873", "pdf": "https://arxiv.org/pdf/2512.17873", "abs": "https://arxiv.org/abs/2512.17873", "authors": ["Baohua Yan", "Qingyuan Liu", "Jennifer Kava", "Xuan Di"], "title": "InSPECT: Invariant Spectral Features Preservation of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.", "AI": {"tldr": "本文提出了一种新的扩散模型InSPECT，该模型在正向和逆向过程中保持不变的频谱特征，从而提高了生成图像的质量、多样性和计算效率。", "motivation": "现代扩散模型（DM）虽然达到了最先进的图像生成效果，但在数据扩散到白噪声并重新构造的过程中存在预测任务极其困难且计算上不可行的问题。为此，本文提出了一种新的方法以克服这一限制。", "method": "通过在正向过程中保持不变的频谱特征，并使傅里叶系数平滑地收敛到指定的随机噪音，从而实现了特征保存同时保持多样性和随机性。", "result": "实验表明，在CIFAR-10、Celeb-A和LSUN数据集上，InSPECT相比DDPM在10K迭代下平均FID降低了39.23%，IS提高了45.80%。这证明了保存不变特征能够提高生成图像的质量与多样性，并增强计算效率。", "conclusion": "这是首次尝试分析并保持扩散模型中的不变频谱特征，InSPECT展示了其在生成质量和多样性、计算效率以及收敛速率方面的显著优势"}}
{"id": "2512.17864", "pdf": "https://arxiv.org/pdf/2512.17864", "abs": "https://arxiv.org/abs/2512.17864", "authors": ["Balram Singh", "Ram Prakash Sharma", "Somnath Dey"], "title": "Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 12 figures", "summary": "Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.", "AI": {"tldr": "该论文提出了一种基于CBAM增强的CNN模型，用于植物叶片疾病的检测。", "motivation": "农作物病害威胁全球粮食安全，需要准确且可解释的方法进行疾病诊断。本文旨在开发一种透明、可靠的系统以提高农业智能诊断能力。", "method": "通过将Convolution Block Attention Module (CBAM) 集成到每个卷积层中，该模型改进了特征提取和病害定位功能，并在五个不同的植物病害数据集上进行了训练。", "result": "所提出的方法在多个评估指标下均优于现有技术，最高准确率达到98.87%，并展示了良好的泛化性能。", "conclusion": "本文的研究为农业智能诊断系统提供了可解释的人工智能应用实例。"}}
{"id": "2512.17853", "pdf": "https://arxiv.org/pdf/2512.17853", "abs": "https://arxiv.org/abs/2512.17853", "authors": ["Ran Gong", "Xiaohan Zhang", "Jinghuan Shang", "Maria Vittoria Minniti", "Jigarkumar Patel", "Valerio Pepe", "Riedana Yan", "Ahmet Gundogdu", "Ivan Kapelyukh", "Ali Abbas", "Xiaoqiang Yan", "Harsh Patel", "Laura Herlant", "Karl Schmeckpeper"], "title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning", "categories": ["cs.RO", "cs.AI"], "comment": "28 pages, 25 figures. The first four authors contributed equally", "summary": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .", "AI": {"tldr": "介绍了一种名为AnyTask的自动化框架，该框架通过大规模并行GPU仿真和基础模型生成多样化操作任务及机器人数据。", "motivation": "通用型机器学习受限于大量多样且高质量的数据收集，这些数据在现实世界中获取成本高昂。尽管仿真是扩大数据采集的一种有希望的方法，但相关的任务设计、场景生成、专家演示合成以及仿真实验到实际应用的转换仍然需要大量的手工劳动。", "method": "引入了三种AnyTask代理来生成专家演示：1）ViPR，一种具有VLM-in-the-loop并行细化的新颖任务和运动规划代理；2）ViPR-Eureka，一种基于生成密集奖励和LLM指导接触采样的强化学习代理；3）ViPR-RL，一种结合规划与学习的混合方法，在仅使用稀疏奖励的情况下共同产生高质量演示。", "result": "在仿真环境中对生成的数据进行行为克隆策略训练，并直接部署到实际机器人硬件上。策略能够泛化至新的物体姿态，在一系列现实世界的拾取放置、抽屉打开等任务中平均成功率为44%。", "conclusion": "该框架通过自动化工具减轻了数据收集和任务设计的人力负担，显著提高了从仿真到真实环境的迁移效果，并展示了在机器人学习领域的前景。"}}
{"id": "2512.17852", "pdf": "https://arxiv.org/pdf/2512.17852", "abs": "https://arxiv.org/abs/2512.17852", "authors": ["Mengkun Chen", "Sanidhya D. Tripathi", "James W. Tunnell"], "title": "Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Raman spectroscopy enables non-destructive, label-free molecular analysis with high specificity, making it a powerful tool for biomedical diagnostics. However, its application to biological tissues is challenged by inherently weak Raman scattering and strong fluorescence background, which significantly degrade signal quality. In this study, we present a simulation-driven denoising framework that combines a statistically grounded noise model with deep learning to enhance Raman spectra acquired under fluorescence-dominated conditions. We comprehensively modeled major noise sources. Based on this model, we generated biologically realistic Raman spectra and used them to train a cascaded deep neural network designed to jointly suppress stochastic detector noise and fluorescence baseline interference. To evaluate the performance of our approach, we simulated human skin spectra derived from real experimental data as a validation case study. Our results demonstrate the potential of physics-informed learning to improve spectral quality and enable faster, more accurate Raman-based tissue analysis.", "AI": {"tldr": "提出了一种基于模拟的深度学习框架，用于荧光主导条件下的拉曼光谱去噪", "motivation": "生物组织中的弱拉曼散射和强荧光背景严重影响了信号质量，限制了拉曼光谱在生物医学诊断中的应用。本研究旨在开发一个结合统计噪声模型与深度学习的方法以提升荧光背景下拉曼光谱的质量", "method": "通过生成基于真实数据的生物现实拉曼光谱来训练级联深度神经网络，该网络能同时抑制随机探测器噪声和荧光基线干扰", "result": "实验结果表明本方法能够显著提高拉曼光谱质量，并加速更准确的组织分析", "conclusion": "提出的模拟驱动学习框架有潜力改善信号质量和加快基于拉曼光谱的生物医学诊断"}}
{"id": "2512.17851", "pdf": "https://arxiv.org/pdf/2512.17851", "abs": "https://arxiv.org/abs/2512.17851", "authors": ["Sarah Rastegar", "Violeta Chatalbasheva", "Sieger Falkena", "Anuj Singh", "Yanbo Wang", "Tejas Gokhale", "Hamid Palangi", "Hadi Jamali-Rad"], "title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.", "AI": {"tldr": "本文介绍了一种名为InfSplign的方法，该方法在推理过程中通过调整噪声来提高文本到图像扩散模型的空间对齐。", "motivation": "现有的文本到图像扩散模型生成高质量图像的能力受限于训练数据缺乏细粒度空间监督以及文本嵌入无法编码空间语义的问题。为解决这些问题，本文提出了InfSplign以改进空间对齐。", "method": "InfSplign通过在去噪步骤中引入复合损失来调整噪声，使用不同层次的交叉注意力图来自基础解码器以确保准确的对象放置和平衡的对象存在。该方法轻量级、插件式兼容任何扩散模型骨干。", "result": "实验结果表明，在VISOR和T2I-CompBench上，InfSplign超越了现有的推理时间基线，并且甚至超过了基于微调的方法，建立了新的最佳性能（据我们所知）。", "conclusion": "InfSplign是一种在训练后无需额外调整即能显著提高文本到图像扩散模型空间对齐能力的轻量级方法。"}}
{"id": "2512.17850", "pdf": "https://arxiv.org/pdf/2512.17850", "abs": "https://arxiv.org/abs/2512.17850", "authors": ["Corey M. Abramson"], "title": "Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life", "categories": ["cs.CY", "cs.AI", "cs.HC", "stat.AP"], "comment": "CITE: Abramson, Corey M. (Forthcoming 2026). \"Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life.\" In Handbook of the Sociology of Aging, 2nd ed., edited by Markus H. Schafer, Dawn C. Carr, Jacqueline L. Angel, and Richard A. Settersten Jr", "summary": "This chapter demonstrates how computational social science (CSS) tools are extending and expanding research on aging. The depth and context from traditionally qualitative methods such as participant observation, in-depth interviews, and historical documents are increasingly employed alongside scalable data management, computational text analysis, and open-science practices. Machine learning (ML) and natural language processing (NLP), provide resources to aggregate and systematically index large volumes of qualitative data, identify patterns, and maintain clear links to in-depth accounts. Drawing on case studies of projects that examine later life--including examples with original data from the DISCERN study (a team-based ethnography of life with dementia) and secondary analyses of the American Voices Project (nationally representative interview)--the chapter highlights both uses and challenges of bringing CSS tools into more meaningful dialogue with qualitative aging research. The chapter argues such work has potential for (1) streamlining and augmenting existing workflows, (2) scaling up samples and projects, and (3) generating multi-method approaches to address important questions in new ways, before turning to practices useful for individuals and teams seeking to understand current possibilities or refine their workflow processes. The chapter concludes that current developments are not without peril, but offer potential for new insights into aging and the life course by broadening--rather than replacing--the methodological foundations of qualitative research.", "AI": {"tldr": "论文展示了计算社会科学工具如何与定性研究相结合，提高老龄化研究的效率和深度。", "motivation": "探索将计算机社会科学研究方法应用于老龄化领域的潜力和挑战。", "method": "通过案例分析展示机器学习、自然语言处理等技术在大规模定性数据分析中的应用。", "result": "研究表明这些工具可以扩展样本规模，增强现有工作流程，并生成多学科的方法来解决关键问题。", "conclusion": "尽管存在一些风险，但将计算社会科学与老龄化领域的定性研究相结合可以为理解生命周期提供新的视角。"}}
{"id": "2512.17846", "pdf": "https://arxiv.org/pdf/2512.17846", "abs": "https://arxiv.org/abs/2512.17846", "authors": ["Carlos Vélez García", "Miguel Cazorla", "Jorge Pomares"], "title": "Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines. PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected. We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\\% success, strongly outperforming prior methods that peak at 68\\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.", "AI": {"tldr": "本文提出了Planning as Descent (PaD)框架，用于离线目标条件强化学习任务中的轨迹合成。", "motivation": "现有的方法通常通过直接策略学习或显式规划来解决问题，但是这些方法容易出现训练和推理过程不一致的问题。PaD通过在能量景观中基于梯度下降进行轨迹优化来减少这种差异，并且可以在嘈杂的数据上提高性能。", "method": "PaD框架通过自监督目标重标定进行自我训练，学习一个目标条件的能量函数，该函数为可行的、符合目标的未来轨迹分配低能量。在推理阶段，多个轨迹候选方案基于不同的时间假设被优化，并选择平衡可行性和效率的低能耗计划。", "result": "实验表明，在狭窄专家演示数据集上进行训练时，PaD达到了95%的成功率，超过了先前方法68%的峰值成功率。此外，使用嘈杂、次优的数据进一步提高了成功概率和规划效率。", "conclusion": "本文的研究成果显示了学习评估并优化轨迹为离线无奖励规划提供了一种稳健的替代方案，而不是直接策略的学习方式"}}
{"id": "2512.17844", "pdf": "https://arxiv.org/pdf/2512.17844", "abs": "https://arxiv.org/abs/2512.17844", "authors": ["Rajni Dabas", "Samir Khuller", "Emilie Rivkin"], "title": "Capacitated Partition Vertex Cover and Partition Edge Cover", "categories": ["cs.DS"], "comment": null, "summary": "Our first focus is the Capacitated Partition Vertex Cover (C-PVC) problem in hypergraphs. In C-PVC, we are given a hypergraph with capacities on its vertices and a partition of the hyperedge set into $ω$ distinct groups. The objective is to select a minimum size subset of vertices that satisfies two main conditions: (1) in each group, the total number of covered hyperedges meets a specified threshold, and (2) the number of hyperedges assigned to any vertex respects its capacity constraint. A covered hyperedge is required to be assigned to a selected vertex that belongs to the hyperedge. This formulation generalizes classical Vertex Cover, Partial Vertex Cover, and Partition Vertex Cover. We investigate two primary variants: soft capacitated (multiple copies of a vertex are allowed) and hard capacitated (each vertex can be chosen at most once). Let $f$ denote the rank of the hypergraph. Our main contributions are: $(i)$ an $(f+1)$-approximation algorithm for the weighted soft-capacitated C-PVC problem, which runs in $n^{O(ω)}$ time, and $(ii)$ an $(f+ε)$-approximation algorithm for the unweighted hard-capacitated C-PVC problem, which runs in $n^{O(ω/ε)}$ time. We also study a natural generalization of the edge cover problem, the \\emph{Weighted Partition Edge Cover} (W-PEC) problem, where each edge has an associated weight, and the vertex set is partitioned into groups. For each group, the goal is to cover at least a specified number of vertices using incident edges, while minimizing the total weight of the selected edges. We present the first exact polynomial-time algorithm for the weighted case, improving runtime from $O(ωn^3)$ to $O(mn+n^2 \\log n)$ and simplifying the algorithmic structure over prior unweighted approaches.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.17843", "pdf": "https://arxiv.org/pdf/2512.17843", "abs": "https://arxiv.org/abs/2512.17843", "authors": ["Yueru Yan", "Tuc Nguyen", "Bo Su", "Melissa Lieffers", "Thai Le"], "title": "ShareChat: A Dataset of Chatbot Conversations in the Wild", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.", "AI": {"tldr": "分享聊天是一款大型跨平台语料库，包含了来自五大主流平台的142808次对话和超过66万个对话回合。", "motivation": "现有公共数据集忽略了模型的独特界面设计和能力，无法展示用户交互的真实情况。为了弥补这一不足，本文构建了一个包含不同平台原始特性如推理痕迹、来源链接等的大规模语料库。", "method": "收集来自ChatGPT等五个平台的聊天记录，并进行对话完整性和源引用行为分析以及时间趋势分析。", "result": "该数据集展示了用户与语言模型交互的真实情况，提供了超过142808次对话和66万个对话回合的数据。", "conclusion": "分享聊天为研究者提供了一个了解真实世界中用户-LLM交互的重要资源。"}}
{"id": "2512.17838", "pdf": "https://arxiv.org/pdf/2512.17838", "abs": "https://arxiv.org/abs/2512.17838", "authors": ["Roshan Kenia", "Xiaoman Zhang", "Pranav Rajpurkar"], "title": "ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges", "categories": ["cs.CV"], "comment": "https://github.com/rajpurkarlab/ReX-MLE", "summary": "Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.", "AI": {"tldr": "介绍了一个针对医学影像挑战的自主智能体基准测试ReX-MLE，评估了大型语言模型在复杂领域特定任务中的表现。", "motivation": "现有自治代理基准无法全面衡量医疗成像等复杂领域的技能，需要一个专门的框架来填补这一空白。", "method": "创建了一个包含20个挑战的医学影像问题库ReX-MLE，评估了自主智能体从数据预处理到模型训练的整体能力。测试了几种不同的智能体和LLM后端性能。", "result": "在真实计算和时间限制下，大多数自治代理的表现远低于人类专家水平，揭示出领域知识和技术上的瓶颈。", "conclusion": "ReX-MLE提供了一个评估医学影像任务中大型语言模型能力的基础，并指出了开发领域意识的自主AI系统的必要性。"}}
{"id": "2512.17819", "pdf": "https://arxiv.org/pdf/2512.17819", "abs": "https://arxiv.org/abs/2512.17819", "authors": ["Hannah Krahl", "Katrin Hartwig", "Ann-Kathrin Fischer", "Theodora Nikolakopoulou", "Guy Pires Cabritas", "Eva Ungeheuer", "Nina Gerber", "Alina Stöver"], "title": "Playful but Persuasive: Deceptive Designs and Advertising Strategies in Popular Mobile Apps for Children", "categories": ["cs.HC"], "comment": null, "summary": "Mobile gaming apps are woven into children's daily lives. Given their ongoing cognitive and emotional development, children are especially vulnerable and depend on designs that safeguard their well-being. When apps feature manipulative interfaces or heavy advertising, they may exert undue influence on young users, contributing to prolonged screen time, disrupted self-regulation, and accidental in-app purchases. In this study, we examined 20 popular, free-to-download children's apps in German-speaking regions to assess the prevalence of deceptive design patterns and advertising. Despite platform policies and EU frameworks like the General Data Protection Regulation and the Digital Services Act, every app contained interface manipulations intended to nudge, confuse, or pressure young users, averaging nearly six distinct deceptive patterns per app. Most also displayed high volumes of non-skippable ads, frequently embedded within core gameplay. These findings indicate a systemic failure of existing safeguards and call for stronger regulation, greater platform accountability, and child-centered design standards.", "AI": {"tldr": "研究分析了德国地区流行的儿童手机游戏应用程序中的欺骗性设计和广告策略，揭示其对儿童的影响。", "motivation": "探讨儿童应用中操纵性界面或大量广告可能带来的负面影响，包括过度使用屏幕时间、自我调节能力受损及意外的内购行为。", "method": "研究选取了20款受欢迎且免费下载的应用程序，在德国地区进行分析评估其欺骗性设计模式和广告策略的存在情况。", "result": "所有应用程序都包含旨在引导、混淆或施压年轻用户的界面操纵，平均每款应用具有近六种不同的欺骗性模式。大部分也显示大量不可跳过的广告，经常嵌入核心游戏玩法中。", "conclusion": "这些发现表明现有保障措施存在系统性失效问题，并呼吁采取更强有力的监管措施和平台责任以及以儿童为中心的设计标准。"}}
{"id": "2512.17817", "pdf": "https://arxiv.org/pdf/2512.17817", "abs": "https://arxiv.org/abs/2512.17817", "authors": ["Yue Li", "Qi Ma", "Runyi Yang", "Mengjiao Ma", "Bin Ren", "Nikola Popovic", "Nicu Sebe", "Theo Gevers", "Luc Van Gool", "Danda Pani Paudel", "Martin R. Oswald"], "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding", "categories": ["cs.CV"], "comment": null, "summary": "While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure. We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.", "AI": {"tldr": "该论文提出了一个名为Chorus的多教师预训练框架，用于从3D高斯场景编码中提取丰富的通用特征。", "motivation": "当前对于直接从3D高斯场景编码（3DGS）的原始数据中提取丰富通用特征的研究较为不足。因此，作者希望通过引入Chorus来填补这一空白。", "method": "Chorus利用共享的3D编码器和特定于教师的投影仪，通过知识蒸馏的方式从语言对齐、通才以及物体感知三种不同的教师模型中学习互补信号。", "result": "在广泛的任务上评估了Chorus的表现，包括开放词汇语义分割、实例分割等。此外，在仅支持点云数据集上的表现也优于基线方法。", "conclusion": "作者通过实验验证了Chorus的有效性，并提出了一个渲染和蒸馏的适应方案来促进跨领域微调。"}}
{"id": "2512.17814", "pdf": "https://arxiv.org/pdf/2512.17814", "abs": "https://arxiv.org/abs/2512.17814", "authors": ["Rolf Drechsler", "Qian Liu"], "title": "LLM-based Behaviour Driven Development for Hardware Design", "categories": ["cs.SE", "cs.AI", "cs.AR"], "comment": "7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025", "summary": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications. Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.", "AI": {"tldr": "本文研究了使用大型语言模型技术来支持硬件设计中的行为驱动开发。", "motivation": "测试和验证在硬件和系统设计中至关重要，但随着系统的增大其复杂性也随之增加。尽管行为驱动开发在软件工程中有很好的效果，在硬件设计中的应用仍然有限，主要原因是需要大量的手动努力将文本规范转换为精确的行为场景。大型语言模型的出现提供了自动化的可能。", "method": "本文探讨了使用大型语言模型技术来自动化从文本规格中导出精确行为场景的过程，以支持硬件设计中的行为驱动开发。", "result": "研究结果尚未具体展示，但通过介绍和讨论表明了这种方法在理论上是可行的，并且有可能减轻手动劳动负担。", "conclusion": "基于大型语言模型的行为驱动开发方法为解决硬件测试验证中复杂性问题提供了新的思路。"}}
{"id": "2512.17796", "pdf": "https://arxiv.org/pdf/2512.17796", "abs": "https://arxiv.org/abs/2512.17796", "authors": ["Yitong Wang", "Fangyun Wei", "Hongyang Zhang", "Bo Dai", "Yan Lu"], "title": "Animate Any Character in Any World", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://snowflakewang.github.io/AniX/", "summary": "Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.", "AI": {"tldr": "本文介绍了一种名为AniX的技术，允许用户通过自然语言指令控制角色在生成的环境中执行各种行为。", "motivation": "当前世界模型主要分为静态环境生成和可控实体模型。前者没有活动代理，后者仅限于单一实体的行为。作者希望通过结合两者的优点来创建一个更通用的角色动画系统。", "method": "AniX基于预训练视频生成器，通过条件自回归方法合成连贯的视频片段，并保持角色与环境的一致性。它支持用户指定的角色执行多种行为，包括移动和物体交互。", "result": "该模型在视觉质量、角色一致性、动作可控性和长期连贯性方面都表现出色。", "conclusion": "通过引入AniX技术，实现了更真实、更具互动性的环境中的角色动画，并且能够适应不同的角色和行为。"}}
{"id": "2512.17795", "pdf": "https://arxiv.org/pdf/2512.17795", "abs": "https://arxiv.org/abs/2512.17795", "authors": ["Binh Vu"], "title": "Intelligent Knowledge Mining Framework: Bridging AI Analysis and Trustworthy Preservation", "categories": ["cs.DL", "cs.AI", "cs.IR"], "comment": null, "summary": "The unprecedented proliferation of digital data presents significant challenges in access, integration, and value creation across all data-intensive sectors. Valuable information is frequently encapsulated within disparate systems, unstructured documents, and heterogeneous formats, creating silos that impede efficient utilization and collaborative decision-making. This paper introduces the Intelligent Knowledge Mining Framework (IKMF), a comprehensive conceptual model designed to bridge the critical gap between dynamic AI-driven analysis and trustworthy long-term preservation. The framework proposes a dual-stream architecture: a horizontal Mining Process that systematically transforms raw data into semantically rich, machine-actionable knowledge, and a parallel Trustworthy Archiving Stream that ensures the integrity, provenance, and computational reproducibility of these assets. By defining a blueprint for this symbiotic relationship, the paper provides a foundational model for transforming static repositories into living ecosystems that facilitate the flow of actionable intelligence from producers to consumers. This paper outlines the motivation, problem statement, and key research questions guiding the research and development of the framework, presents the underlying scientific methodology, and details its conceptual design and modeling.", "AI": {"tldr": "智能知识挖掘框架（IKMF）旨在通过AI驱动的分析与可信长期保存之间的桥梁，将静态数据仓库转变为动态的知识生态系统。", "motivation": "数字信息爆炸导致了数据孤岛和利用效率低下问题。因此提出IKMF来解决这些问题并促进跨领域协作决策。", "method": "该框架包括两个平行流：一个是从原始数据到机器可操作知识的系统化挖掘过程；另一个是确保完整性和可再现性的可信存档流。", "result": "论文提出了一个概念模型，定义了AI分析与长期保存之间的协同作用，并为开发此类框架提供了指导和科学方法论。", "conclusion": "通过提出IKMF的概念设计和建模方案，该研究为如何将静态数据仓库转变为动态知识生态系统提供了一个基础模型。"}}
{"id": "2512.17784", "pdf": "https://arxiv.org/pdf/2512.17784", "abs": "https://arxiv.org/abs/2512.17784", "authors": ["Ami Pandat", "Punna Rajasekhar", "G. Aravamuthan", "Gopika Vinod", "Rohit Shukla"], "title": "Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras", "categories": ["cs.CV"], "comment": null, "summary": "Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.", "AI": {"tldr": "本文提出了一种基于神经网络的混合畸变模型，用于CCTV摄像机的长距离深度估计。", "motivation": "传统的相机模型在长距离定位应用中受限于镜头非线性畸变的建模能力。为了提高远距离物体的三维定位性能，需要一种更好的畸变模型。", "method": "通过扩展传统畸变模型以包括高阶项，并使用神经网络进行残差校正，形成混合方法。", "result": "该框架在长距离对象本地化任务中表现出色，可以估计最大5公里范围内物体的三维位置并将其转换为GIS坐标进行可视化。", "conclusion": "提出的混合畸变模型有效地提高了CCTV摄像机的长距离定位性能，并提供了一种实用的方法来校准用于摄影测量应用中的远距离相机。"}}
{"id": "2512.17782", "pdf": "https://arxiv.org/pdf/2512.17782", "abs": "https://arxiv.org/abs/2512.17782", "authors": ["Arya Chavoshi", "Hassan Dashtian", "Naveen Sudharsan", "Dev Niyogi"], "title": "UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover", "categories": ["cs.CV"], "comment": null, "summary": "Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness. Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference. UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.", "AI": {"tldr": "UrbanDIFF是一种用于填补城市地表温度（LST）图像中云覆盖空缺的纯空间去噪扩散模型。", "motivation": "卫星获取的地表温度产品对于监测城市热岛效应至关重要，但由于频繁的云层遮挡，限制了其连续性。传统方法和基于深度学习的方法在处理大片或连续空缺时性能下降，因此引入一种新的去噪扩散模型以提高重建效果。", "method": "UrbanDIFF通过条件化静态城市结构信息（如建筑表面数据和数字高程模型），并在推理过程中利用监督像素引导细化步骤来保证与无云区域的一致性。该模型在NASA MODIS Terra LST数据上进行训练和评估。", "result": "实验结果表明，UrbanDIFF在各种条件下均优于插值基线方法，特别是在85%的云覆盖情况下取得了SSIM为0.89、RMSE为1.2 K及R^2为0.84的结果，并且随着云密度增加其性能下降较为缓慢。", "conclusion": "UrbanDIFF模型成功填补了城市地表温度图像中的云覆盖空缺，表现出色并具备在高覆盖率下保持稳定性的优势。"}}
{"id": "2512.17781", "pdf": "https://arxiv.org/pdf/2512.17781", "abs": "https://arxiv.org/abs/2512.17781", "authors": ["Yohanes Yudhi Adikusuma", "Qixing Huang", "Ying He"], "title": "LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.", "AI": {"tldr": "提出了LiteGE，一种轻量级的方法用于计算三维表面上的测地距离和非等距形状对应。", "motivation": "当前基于学习的方法虽然性能强大但依赖于大型三维骨干网络，导致高内存使用和延迟。因此，在交互式或资源受限环境中应用存在限制。", "method": "通过在关键体素上对无符号距离场样本进行主成分分析（PCA），构造轻量级、类别感知的形状描述符，以有效计算测地距离并支持稀疏点云输入。", "result": "LiteGE减少了内存使用和推理时间至现有神经方法的1/300，并在非等距形状匹配上达到目前网格基方法的1000倍速度提升。", "conclusion": "提出的方法成功解决了高容量网络依赖带来的问题，实现了高效、准确地计算测地距离及形状对应任务。"}}
{"id": "2512.17774", "pdf": "https://arxiv.org/pdf/2512.17774", "abs": "https://arxiv.org/abs/2512.17774", "authors": ["Saikat Roy", "Yannick Kirchhoff", "Constantin Ulrich", "Maximillian Rokuss", "Tassilo Wald", "Fabian Isensee", "Klaus Maier-Hein"], "title": "MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet", "AI": {"tldr": "本文提出了MedNeXt-v2，一种用于大规模监督表征学习的改进的三维ConvNeXt架构，在医疗图像分割任务中取得最先进的性能。", "motivation": "现有研究主要集中在增加数据集大小上，而忽视了基础网络是否适合大规模训练。本文旨在填补这一空白，并通过重新审视基于ConvNeXt的基础架构来改善大型预训练管道中的模型表现。", "method": "引入了一个改进的三维全局响应归一化模块，并结合深度、宽度和上下文扩展策略提升表征学习效果，同时在18k CT影像上进行大规模预训练并微调以适应多种医学图像分割任务。", "result": "MedNeXt-v2在六个具有挑战性的CT和MR数据集（包括144种结构）上的细调后表现最佳，并超越了七个公开发布的预训练模型，显示出显著的性能提升。此外还发现更强的基础网络能带来更好的结果，表征扩展对病理性分割尤为重要。", "conclusion": "实验结果表明MedNeXt-v2在大规模监督学习中是一种强大的基础网络，适用于三维医学图像分割任务，并且已将代码和预训练模型发布于官方nnUNet仓库。"}}
{"id": "2512.17773", "pdf": "https://arxiv.org/pdf/2512.17773", "abs": "https://arxiv.org/abs/2512.17773", "authors": ["Simon Giebenhain", "Tobias Kirschstein", "Liam Schoneveld", "Davide Davoli", "Zhe Chen", "Matthias Nießner"], "title": "Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://simongiebenhain.github.io/Pix2NPHM/ , Video: https://www.youtube.com/watch?v=MgpEJC5p1Ts", "summary": "Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.", "AI": {"tldr": "提出了一种从单张图片直接回归神经参数头模型（NPHM）参数的方法Pix2NPHM。", "motivation": "由于神经参数空间的表达性，基于网格的3D形态模型难以适应视觉输入。为此，该研究旨在开发一种能够利用这种复杂性的方法，以提高面部几何和表情重建的质量。", "method": "使用预先在几何预测任务上训练过的领域特定ViT作为骨干网络，通过混合3D数据集（包括超过10万次NPHM注册）进行监督，并利用大规模2D视频数据集中的法线估计作为伪真实几何。Pix2NPHM可以实现在交互帧率上的3D重建，并且在推理时间优化后可进一步提高几何保真度。", "result": "实现了前所未有的面部重建质量，能够在规模化的野外数据上运行。", "conclusion": "通过利用预训练的ViT和混合数据集进行监督，Pix2NPHM能够以交互帧率实现高质量的3D人脸重建，并在推理阶段进一步优化几何细节。"}}
{"id": "2512.17771", "pdf": "https://arxiv.org/pdf/2512.17771", "abs": "https://arxiv.org/abs/2512.17771", "authors": ["Dong Chen", "Zhengqing Hu", "Shixing Zhao", "Yibo Guo"], "title": "Easy Adaptation: An Efficient Task-Specific Knowledge Injection Method for Large Models in Resource-Constrained Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While the enormous parameter scale endows Large Models (LMs) with unparalleled performance, it also limits their adaptability across specific tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical approach for effectively adapting LMs to a diverse range of downstream tasks. However, existing PEFT methods face two primary challenges: (1) High resource cost. Although PEFT methods significantly reduce resource demands compared to full fine-tuning, it still requires substantial time and memory, making it impractical in resource-constrained environments. (2) Parameter dependency. PEFT methods heavily rely on updating a subset of parameters associated with LMs to incorporate task-specific knowledge. Yet, due to increasing competition in the LMs landscape, many companies have adopted closed-source policies for their leading models, offering access only via Application Programming Interface (APIs). Whereas, the expense is often cost-prohibitive and difficult to sustain, as the fine-tuning process of LMs is extremely slow. Even if small models perform far worse than LMs in general, they can achieve superior results on particular distributions while requiring only minimal resources. Motivated by this insight, we propose Easy Adaptation (EA), which designs Specific Small Models (SSMs) to complement the underfitted data distribution for LMs. Extensive experiments show that EA matches the performance of PEFT on diverse tasks without accessing LM parameters, and requires only minimal resources.", "AI": {"tldr": "该论文提出了一种新的方法，名为Easy Adaptation（EA），旨在解决大型模型在资源受限环境下适应特定任务时所面临的高成本和参数依赖性问题。", "motivation": "现有的PEFT方法尽管减少了全量微调的资源需求，但在资源限制环境中仍面临高昂的成本。此外，由于许多公司对其领先模型采取了封闭源策略，并仅通过API提供访问权限，这使得基于LM参数更新的任务特定知识注入变得困难且成本高昂。", "method": "论文提出了Easy Adaptation（EA），该方法利用特化的小型模型来补充大型模型在特定数据分布下的不足。具体来说，它设计了特定小型模型以适应大型模型的未充分拟合的数据集，并通过实验验证其性能。", "result": "广泛的实验证明，在不访问LM参数的情况下，Easy Adaptation（EA）可以达到与PEFT相同水平的表现，同时只需少量资源即可运行。", "conclusion": "论文表明，相对于现有的PEFT方法而言，Easy Adaptation（EA）不仅能够匹配甚至超越其性能表现，并且在资源利用方面更为高效。"}}
{"id": "2512.17769", "pdf": "https://arxiv.org/pdf/2512.17769", "abs": "https://arxiv.org/abs/2512.17769", "authors": ["Tanjim Taharat Aurpa", "Farzana Akter", "Md. Mehedi Hasan", "Shakil Ahmed", "Shifat Ara Rafiq", "Fatema Khan"], "title": "Bangla MedER: Multi-BERT Ensemble Approach for the Recognition of Bangla Medical Entity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical Entity Recognition (MedER) is an essential NLP task for extracting meaningful entities from the medical corpus. Nowadays, MedER-based research outcomes can remarkably contribute to the development of automated systems in the medical sector, ultimately enhancing patient care and outcomes. While extensive research has been conducted on MedER in English, low-resource languages like Bangla remain underexplored. Our work aims to bridge this gap. For Bangla medical entity recognition, this study first examined a number of transformer models, including BERT, DistilBERT, ELECTRA, and RoBERTa. We also propose a novel Multi-BERT Ensemble approach that outperformed all baseline models with the highest accuracy of 89.58%. Notably, it provides an 11.80% accuracy improvement over the single-layer BERT model, demonstrating its effectiveness for this task. A major challenge in MedER for low-resource languages is the lack of annotated datasets. To address this issue, we developed a high-quality dataset tailored for the Bangla MedER task. The dataset was used to evaluate the effectiveness of our model through multiple performance metrics, demonstrating its robustness and applicability. Our findings highlight the potential of Multi-BERT Ensemble models in improving MedER for Bangla and set the foundation for further advancements in low-resource medical NLP.", "AI": {"tldr": "本研究提出了一种针对孟加拉语医学实体识别的多BERT集成方法，该方法在基准模型中表现最佳。", "motivation": "目前对于英语以外的语言如孟加拉语中的医学实体识别的研究较少。本文旨在填补这一空白，并提升低资源语言医学自然语言处理的技术水平。", "method": "研究首先评估了包括BERT、DistilBERT、ELECTRA和RoBERTa在内的多种转换器模型，然后提出了一种多BERT集成方法来提高孟加拉语的医学实体识别准确性。", "result": "提出的多BERT集成方法在测试中获得了最高的89.58%准确率，并且比单一层次的BERT模型提高了11.80%。这些结果表明该方法的有效性和鲁棒性。", "conclusion": "研究展示了多BERT集成模型在低资源语言医学实体识别中的潜力，为未来的发展奠定了基础。"}}
{"id": "2512.17764", "pdf": "https://arxiv.org/pdf/2512.17764", "abs": "https://arxiv.org/abs/2512.17764", "authors": ["Kangchen Lv", "Mingrui Yu", "Shihefeng Wang", "Xiangyang Ji", "Xiang Li"], "title": "UniStateDLO: Unified Generative State Estimation and Tracking of Deformable Linear Objects Under Occlusion for Constrained Manipulation", "categories": ["cs.RO"], "comment": "The first two authors contributed equally. Project page: https://unistatedlo.github.io", "summary": "Perception of deformable linear objects (DLOs), such as cables, ropes, and wires, is the cornerstone for successful downstream manipulation. Although vision-based methods have been extensively explored, they remain highly vulnerable to occlusions that commonly arise in constrained manipulation environments due to surrounding obstacles, large and varying deformations, and limited viewpoints. Moreover, the high dimensionality of the state space, the lack of distinctive visual features, and the presence of sensor noises further compound the challenges of reliable DLO perception. To address these open issues, this paper presents UniStateDLO, the first complete DLO perception pipeline with deep-learning methods that achieves robust performance under severe occlusion, covering both single-frame state estimation and cross-frame state tracking from partial point clouds. Both tasks are formulated as conditional generative problems, leveraging the strong capability of diffusion models to capture the complex mapping between highly partial observations and high-dimensional DLO states. UniStateDLO effectively handles a wide range of occlusion patterns, including initial occlusion, self-occlusion, and occlusion caused by multiple objects. In addition, it exhibits strong data efficiency as the entire network is trained solely on a large-scale synthetic dataset, enabling zero-shot sim-to-real generalization without any real-world training data. Comprehensive simulation and real-world experiments demonstrate that UniStateDLO outperforms all state-of-the-art baselines in both estimation and tracking, producing globally smooth yet locally precise DLO state predictions in real time, even under substantial occlusions. Its integration as the front-end module in a closed-loop DLO manipulation system further demonstrates its ability to support stable feedback control in complex, constrained 3-D environments.", "AI": {"tldr": "UniStateDLO是一种用于在严重遮挡下进行变形线性对象感知的方法，包括单帧状态估计和跨帧跟踪。", "motivation": "为了克服现有视觉方法在受约束环境中面对障碍物、复杂形态变化及传感器噪声等挑战时的不足，提出了一种基于深度学习的完整感知管线UniStateDLO，以实现对变形线性对象（如电缆、绳索）的可靠识别与跟踪。", "method": "本文提出了一个利用扩散模型处理从部分点云中提取高维状态信息的任务框架。该框架能够解决初始遮挡、自遮挡以及由多个物体造成的遮挡问题，并且仅依赖于大规模合成数据集进行训练，具备零样本迁移能力。", "result": "实验结果表明，在模拟和真实场景下，UniStateDLO在估计与跟踪任务中超越了所有最新的基准方法，即使在大量遮挡条件下也能实时输出全局平滑局部精确的状态预测。", "conclusion": "该研究展示了UniStateDLO不仅能够有效支持闭环反馈控制下的复杂三维环境操作任务，而且具备强大的泛化能力。"}}
{"id": "2512.17759", "pdf": "https://arxiv.org/pdf/2512.17759", "abs": "https://arxiv.org/abs/2512.17759", "authors": ["Rahul Ravi", "Ruizhe Li", "Tarek Abdelfatah", "Stephen Chan", "Xin Chen"], "title": "Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.", "AI": {"tldr": "研究使用纵向对比增强磁共振成像（CE-MRI）和临床数据预测乳腺癌患者新辅助化疗的治疗反应。", "motivation": "开发机器学习模型，利用MRI图像特征和临床数据来预测乳腺癌患者的病理完全缓解率和5年无病生存状态。", "method": "研究包括肿瘤分割、图像配准、特征提取以及建模。通过图像配准方法从不同时间点的原始肿瘤部位提取并比较MRI影像特征，监测新辅助化疗过程中的瘤内变化。使用四个特征提取器（一个基于放射组学和三个基于深度学习），并与三种特征选择方法和四种机器学习模型相结合构建预测模型。", "result": "图像配准基特征提取法显著改善了预测模型的性能，在病理完全缓解率分类任务中，逻辑回归模型在放射组学特征上训练得到最佳表现，AUC为0.88，准确率为0.85；在无病生存状态分类中，AUC为0.78，准确率为0.72。", "conclusion": "研究表明图像配准方法显著改进了纵向特征学习性能，在预测病理完全缓解率和无病生存状态时表现更优。放射组学特征提取器比预训练的深度学习特征提取器更具效果且具有更好的可解释性。"}}
{"id": "2512.17756", "pdf": "https://arxiv.org/pdf/2512.17756", "abs": "https://arxiv.org/abs/2512.17756", "authors": ["Zhihan Zhou", "Daqian Shi", "Rui Song", "Lida Shi", "Xiaolei Diao", "Hao Xu"], "title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.", "AI": {"tldr": "提出AncientBench基准以评估大语言模型对古代字符的理解能力。", "motivation": "当前中文基准主要针对现代汉语和流传下来的古文，缺乏对发掘出来的古文的评测。为此，需要一个专门的基准来填补这一空白，提高大型语言模型在考古学中的应用。", "method": "AncientBench包含四个维度：字形理解、读音理解、意义理解和上下文理解，并设有十项任务如部首识别、同音词等，邀请考古学家进行实验评估并使用顶级大语言模型进行测试。", "result": "实验证明大型语言模型在古文中具有巨大潜力但与人类的表现仍有差距。", "conclusion": "该研究旨在推动大语言模型在考古学和古代汉语领域的应用和发展。"}}
{"id": "2512.17740", "pdf": "https://arxiv.org/pdf/2512.17740", "abs": "https://arxiv.org/abs/2512.17740", "authors": ["Amaia Sagasti", "Frederic Font"], "title": "When Pamplona sounds different: the soundscape transformation of San Fermin through intelligent acoustic sensors and a sound repository", "categories": ["cs.CY", "cs.SD"], "comment": "46 pages, 27 figures", "summary": "This study presents a use-case of a network of low-cost acoustic smart sensors deployed in the city of Pamplona to analyse changes in the urban soundscape during the San Fermin Festival. The sensors were installed in different areas of the city before, during, and after the event, capturing continuous acoustic data. Our analysis reveals a significant transformation in the city's sonic environment during the festive period: overall sound pressure levels increase significantly, soundscape patterns change, and the acoustic landscape becomes dominated by sounds associated with human activity. These findings highlight the potential of distributed smart acoustic monitoring systems to characterize the temporal dynamics of urban soundscapes and underscore how the large-scale event of San Fermin drastically reshapes the overall acoustic dynamics of the city of Pamplona. Additionally, to complement the objective measurements, a curated collection of real San Fermin sound recordings has been created and made publicly available, preserving the festival's unique sonic heritage.", "AI": {"tldr": "论文通过在潘普洛纳市部署低功耗声学智能传感器，研究圣费尔明节期间城市声音环境的变化。", "motivation": "通过分析大规模事件对城市音响动态的影响，并展示分布式智能声学监测系统的潜力。", "method": "安装不同区域的声学智能传感器，在节日前后连续采集声学数据。创建了独特的节日声音记录库，以保存圣费尔明节的声音遗产。", "result": "发现整体声音压力水平显著提高，声音景观由人类活动相关的声音主导。", "conclusion": "分布式智能声学监测系统可以表征城市声音环境的动态变化，并且大规模事件对城市音响影响巨大。"}}
{"id": "2512.17733", "pdf": "https://arxiv.org/pdf/2512.17733", "abs": "https://arxiv.org/abs/2512.17733", "authors": ["Jingmao Zhang", "Zhiting Zhao", "Yunqi Lin", "Jianghong Ma", "Tianjun Wei", "Haijun Zhang", "Xiaofeng Zhang"], "title": "Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Beyond user-item modeling, item-to-item relationships are increasingly used to enhance recommendation. However, common methods largely rely on co-occurrence, making them prone to item popularity bias and user attributes, which degrades embedding quality and performance. Meanwhile, although diversity is acknowledged as a key aspect of recommendation quality, existing research offers limited attention to it, with a notable lack of causal perspectives and theoretical grounding. To address these challenges, we propose Cadence: Diversity Recommendation via Causal Deconfounding of Co-purchase Relations and Counterfactual Exposure - a plug-and-play framework built upon LightGCN as the backbone, primarily designed to enhance recommendation diversity while preserving accuracy. First, we compute the Unbiased Asymmetric Co-purchase Relationship (UACR) between items - excluding item popularity and user attributes - to construct a deconfounded directed item graph, with an aggregation mechanism to refine embeddings. Second, we leverage UACR to identify diverse categories of items that exhibit strong causal relevance to a user's interacted items but have not yet been engaged with. We then simulate their behavior under high-exposure scenarios, thereby significantly enhancing recommendation diversity while preserving relevance. Extensive experiments on real-world datasets demonstrate that our method consistently outperforms state-of-the-art diversity models in both diversity and accuracy, and further validates its effectiveness, transferability, and efficiency over baselines.", "AI": {"tldr": "通过因果去混淆和反事实暴露，增强推荐系统的多样性与准确性", "motivation": "现有的基于共同购买关系的推荐方法易受商品流行度偏差和个人属性影响，导致嵌入质量下降。同时，现有研究较少从因果角度关注多样性的提升，因此提出了Cadence框架以解决这些问题", "method": "首先计算无偏不对称共同购买关系（UACR），构建去混淆的商品图；其次利用UACR识别用户潜在感兴趣但未接触的多样化商品类别，并模拟这些商品在高曝光度下的行为表现", "result": "实验证明，该方法在真实数据集上显著提高了多样性与准确性，优于现有最佳模型", "conclusion": "Cadence框架通过因果去混淆和反事实暴露有效提升了推荐系统的多样性和准确性，同时保持了良好的效率和可转移性"}}
{"id": "2512.17730", "pdf": "https://arxiv.org/pdf/2512.17730", "abs": "https://arxiv.org/abs/2512.17730", "authors": ["Yichen Jiang", "Mohammed Talha Alam", "Sohail Ahmed Khan", "Duc-Tien Dang-Nguyen", "Fakhri Karray"], "title": "AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.", "AI": {"tldr": "本文提出了一种参数高效的微调框架AdaptPrompt，以增强CLIP模型在深度伪造检测任务中的泛化能力。", "motivation": "随着图像生成技术的进步，高度逼真的合成媒体变得越来越普遍，这使得可靠的深度伪造检测变得更加困难。传统的深度伪造检测器往往针对特定的生成器进行训练，在面对未见过的模型时表现不佳。", "method": "本文引入了Diff-Gen数据集，并提出了AdaptPrompt框架，该框架通过联合学习任务特定的文本提示和视觉适配器来实现参数高效的迁移学习。此外，剪枝视觉编码器的最后一层Transformer块可以显著提高检测精度。", "result": "在广泛的测试集中，包括由GAN、扩散模型和商业工具生成的内容上，AdaptPrompt的表现优于现有的深度伪造检测方法。", "conclusion": "AdaptPrompt框架通过参数高效的微调实现了对多种合成内容的准确识别，并展示了其在少量样本下的泛化能力以及源归因任务上的出色性能。"}}
{"id": "2512.17726", "pdf": "https://arxiv.org/pdf/2512.17726", "abs": "https://arxiv.org/abs/2512.17726", "authors": ["Qian Zeng", "Yihui Wang", "Shu Yang", "Yingxue Xu", "Fengtao Zhou", "Jiabo Ma", "Dejia Cai", "Zhengyu Zhang", "Lijuan Qu", "Yu Wang", "Li Liang", "Hao Chen"], "title": "MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image", "categories": ["cs.CV"], "comment": "18 pages, 11 figures, 10 tables", "summary": "Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology", "AI": {"tldr": "MambaMIL+是一种新的多实例学习框架，旨在解决大规模计算病理学中长序列建模和空间上下文建模的挑战。", "motivation": "为了克服现有方法在处理全片图像时面临的长期依赖关系建模困难和内存遗忘问题，提出了MambaMIL+来改进对复杂病理图像的有效分析。", "method": "MambaMIL+通过引入重叠扫描、选择性条带位置编码器以及上下文令牌选择机制，提高了模型在处理大规模计算病理学数据时的性能。", "result": "实验结果表明，在20个基准测试中，MambaMIL+使用三种特征提取方法（ResNet-50，PLIP和CONCH）均达到最先进的表现。", "conclusion": "MambaMIL+通过改进空间上下文建模和长距离依赖关系，展示了其在大规模计算病理学中的有效性和鲁棒性。"}}
{"id": "2512.17724", "pdf": "https://arxiv.org/pdf/2512.17724", "abs": "https://arxiv.org/abs/2512.17724", "authors": ["Shaoyan Zhai", "Mohamed Abdel-Aty", "Chenzhu Wang", "Rodrigo Vena Garcia"], "title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses", "categories": ["cs.CV"], "comment": null, "summary": "The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.", "AI": {"tldr": "本文介绍了SAVeD数据集，这是一个针对ADAS车辆相关碰撞、险情和脱机事件的大规模视频数据集，旨在通过真实世界的案例分析改进驾驶行为研究。", "motivation": "现有的研究数据集主要局限于模拟环境或人类驾驶的数据，缺乏在风险条件下ADAS车辆的真实行为。因此，本文提出SAVeD数据集以填补这一空白。", "method": "SAVeD数据集由2,119个第一人称视频组成，涵盖了多种交通场景、光照和天气条件下的ADAS车辆操作情况，并提供了碰撞、规避动作以及脱机事件的帧级注释。此外，还提出了一个新的框架来计算动态物体的时间到碰撞（TTC）。", "result": "通过使用SAVeD数据集进行分析，展示了该数据集在复杂险情场景中改进模型性能的能力。", "conclusion": "SAVeD数据集填补了现有研究的空白，并且通过对真实世界风险情况的研究有助于提高ADAS系统的性能和安全性。"}}
{"id": "2512.17722", "pdf": "https://arxiv.org/pdf/2512.17722", "abs": "https://arxiv.org/abs/2512.17722", "authors": ["Paola Di Maio"], "title": "Digital and Web Forensics Model Cards, V1", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces a standardized model card framework specifically designed for digital and web forensics. Building upon established model card methodologies and recent work on abstract models for digital forensic analysis, this paper presents a web based framework that generates model cards specifically designed to represent knowledge in the forensic domain. The framework includes controlled vocabularies for classification, reasoning types, bias identification, and error categorization, along with a web-based generator tool to facilitate adoption. The paper describes the model card structure, presents the controlled vocabularies, and introduces the beta version of the generator tool, inviting community feedback to refine this emerging standard. Ultimately, the systemic risk is that that the anti fraud and digital and web forensics processes are controlled by the mobs.", "AI": {"tldr": "提出了一种标准化的模型卡片框架，专门用于数字和网络取证。", "motivation": "为了改进现有的数字取证方法，并通过一种基于Web的标准框架生成特定领域知识的模型卡片，来促进数字取证过程中的知识表示。", "method": "构建了一个包含分类、推理类型、偏见识别及错误分类等控制词汇表的模型卡片框架。同时提供了一款基于Web的生成工具以方便采纳和使用。", "result": "介绍了模型卡片结构，并展示了控制词汇，发布了该生成器工具的测试版，邀请社区反馈以便不断完善这一标准。", "conclusion": "此标准化模型卡框架有助于提升数字取证过程中的系统性风险管理。"}}
{"id": "2512.17717", "pdf": "https://arxiv.org/pdf/2512.17717", "abs": "https://arxiv.org/abs/2512.17717", "authors": ["Cheng Peng", "Zhuo Su", "Liao Wang", "Chen Guo", "Zhaohu Li", "Chengjiang Long", "Zheng Lv", "Jingxiang Sun", "Chenyangguang Zhang", "Yebin Liu"], "title": "FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation", "categories": ["cs.CV"], "comment": "Project page: https://pengc02.github.io/flexavatar", "summary": "We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.", "AI": {"tldr": "本文提出了FlexAvatar，这是一种灵活的大规模重建模型，能够从单个或稀疏图像中生成高保真度的3D头像，并支持详细的动态变形。", "motivation": "为了创建高质量、可动画化的3D头像而无需相机姿态或表情标签，该研究开发了一种新的方法以提高建模精度和现实感。", "method": "FlexAvatar使用带有结构化头部查询令牌的变压器重建模型，并引入了一个轻量级UNet解码器来生成实时详细的变形。此外还采用数据分布调整策略和10秒微调步骤，确保罕见表情细节的捕捉和身份特异性详细信息。", "result": "实验结果表明，FlexAvatar在3D一致性与动态现实感方面优于现有方法。", "conclusion": "研究提出了一种灵活的大规模重建模型FlexAvatar，能够高效地生成高质量、详细的可动画化3D头像。"}}
{"id": "2512.17708", "pdf": "https://arxiv.org/pdf/2512.17708", "abs": "https://arxiv.org/abs/2512.17708", "authors": ["Nils Wittek", "Anton Melnikov", "Bert Kaiser", "André Zimmermann"], "title": "Review of MEMS Speakers for Audio Applications", "categories": ["eess.AS", "cs.SD"], "comment": "37 pages, 6 figures", "summary": "Microelectromechanical systems (MEMS) speakers are compact, scalable alternatives to traditional voice coil speakers, promising improved sound quality through precise semiconductor manufacturing. This review provides an overview of the research landscape, including ultrasound pulse-based and thermoacoustic sound generation, classifying MEMS speakers by actuation principle: electrodynamic, piezoelectric, and electrostatic. A comparative analysis of performance indicators from 1990-2025 highlights the dominance of piezoelectric MEMS with direct air displacement, focusing on miniaturization and efficiency. The review outlines upcoming research challenges and identifies potential candidates for achieving full-spectrum audio performance. A focus on innovative approaches could lead to wideband adoption of MEMS-only speakers.", "AI": {"tldr": "本文综述了微机电系统(MEMS)扬声器的研究现状，比较分析了其性能指标，并展望未来研究挑战。", "motivation": "MEMS扬声器作为传统动圈式扬声器的紧凑型替代品，在半导体制造技术的支持下有望提供更优质的声音体验。", "method": "文章概述了1990年至2025年期间基于不同激励原理（如电动力、压电和静电）的MEMS扬声器研究，并对其性能指标进行了比较分析。", "result": "研究表明，直接空气位移式的压电MEMS扬声器在小型化与效率方面表现出色。", "conclusion": "展望未来的研究挑战并指出了实现全频段音频表现的潜在候选者，强调了创新方法对于推动MEMS扬声器广泛采用的重要性。"}}
{"id": "2512.17680", "pdf": "https://arxiv.org/pdf/2512.17680", "abs": "https://arxiv.org/abs/2512.17680", "authors": ["Ana Stankovic", "Mohamed Khalil Ben-Larbi", "Wolfgang H. Müller"], "title": "A Dual Quaternion based RRT* Path Planning Approach for Satellite Rendezvous and Docking", "categories": ["cs.RO"], "comment": "6 pages, CAMSAT 2025, This work has been accepted to IFAC", "summary": "This paper proposes a sampling-based motion planner that employs a dual quaternion representation to generate smooth, collision-free six-degree-of-freedom pose trajectories for satellite rendezvous and docking under keep-out zone constraints. The proposed planner integrates the dual quaternion algebra directly into an RRT* framework, thereby enabling natural screw motion interpolation in SE(3). The dual quaternion-based RRT* has been implemented in Python and demonstrated on a representative multi-obstacle scenario. A comparison with a standard RRT* using separate translation and quaternion steering highlights the enhanced pose continuity and obstacle avoidance of the proposed method. The present approach is purely kinematic in nature and does not take into account relative orbital dynamics. Consequently, the resulting path provides a preliminary estimate for a subsequent optimisation-based trajectory planner, which will refine the motion with dynamic constraints for the purpose of practical satellite rendezvous and docking missions.", "AI": {"tldr": "基于双四元数的RRT*路径规划方法用于卫星对接和捕获任务", "motivation": "提出一种新的采样基运动规划器，以生成平滑、无碰撞的六自由度姿态轨迹。该方法改进了相对轨道动力学下的轨迹规划问题，特别是考虑到保持区限制下六维空间中的姿态连续性和障碍物避让能力", "method": "采用双四元数表示直接集成到RRT*框架中，并在SE(3)中实现自然螺旋运动插值。与标准的基于分离平移和四元数引导的RRT*相比，该方法提高了姿态连续性并增强了障碍物避免性能", "result": "提出的基于双四元数的RRT*算法已被实施并在一个包含多个障碍物的代表性场景上进行了演示，展示了其在卫星对接任务中的优越性。结果表明它为后续优化轨迹规划提供了一个初步估计路径", "conclusion": "该方法成功地解决了保持区约束下的姿态连续性和避障问题，在实际应用中可以作为动力学优化前的一个初拟解决方案"}}
{"id": "2512.17678", "pdf": "https://arxiv.org/pdf/2512.17678", "abs": "https://arxiv.org/abs/2512.17678", "authors": ["Daphné Chopard", "Jorge da Silva Gonçalves", "Irene Cannistraci", "Thomas M. Sutter", "Julia E. Vogt"], "title": "You Only Train Once: Differentiable Subset Selection for Omics Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.", "AI": {"tldr": "提出了一种新的单细胞转录组数据基因子集选择框架YOTO，实现端到端的特征选择和预测。", "motivation": "大多数现有的特征选择方法存在多阶段流程或者依赖后处理特征归因的问题，导致选择与预测脱节。本文旨在开发一种直接整合两者的方法来改进生物标志物发现、提高可解释性和降低成本。", "method": "YOTO框架通过单一可微架构联合进行离散基因子集的选择和预测任务，在模型中预测任务指导哪些基因被选中，并且所学习的基因子集反过来塑造预测表示，形成闭合反馈回路以在训练过程中迭代优化选择与预测。此外，它利用多任务学习设计来跨相关目标共享表示。", "result": "在两个代表性的单细胞RNA测序数据集中评估YOTO，结果显示其性能优于现有方法，并且能够生成紧凑且有意义的基因子集。", "conclusion": "稀疏、端到端、多任务的基因子集选择不仅提高了预测性能，还为生物标志物发现和单细胞分析带来了进展。"}}
{"id": "2512.17675", "pdf": "https://arxiv.org/pdf/2512.17675", "abs": "https://arxiv.org/abs/2512.17675", "authors": ["Yudhistira Arief Wibowo"], "title": "An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.", "AI": {"tldr": "研究通过实证分析FFHQ超分辨率任务中的采样参数，探讨预训练扩散模型在条件下的性能优化。", "motivation": "提高单图像超分辨率的质量需要精心调整额外的超参数。该工作旨在识别影响性能的关键因素，并进行相应的调优。", "method": "进行了FFHQ超分辨率任务中的实证消融研究，分析了步骤大小和扩散步数对模型重建质量的影响。", "result": "发现条件步骤大小显著影响模型性能，最佳范围在[2.0, 3.0]之间。", "conclusion": "通过优化条件采样参数可以大幅改善预训练扩散模型的超分辨率结果。"}}
{"id": "2512.17673", "pdf": "https://arxiv.org/pdf/2512.17673", "abs": "https://arxiv.org/abs/2512.17673", "authors": ["Alexandre Personnic", "Mihai Bâce"], "title": "Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "12 pages, 5 figures, the code repository is available at https://gitlab.kuleuven.be/u0172623/ST-Gaze", "summary": "Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.", "AI": {"tldr": "本文提出了一种结合卷积神经网络和特定通道注意及自我注意模块的模型，用于视频中的注视点估计。", "motivation": "现有的注视点估算方法在捕捉图像帧内的空间关系以及多帧间的时间关系方面存在局限性，从而影响了性能。作者希望通过改进特征表示来提升性能。", "method": "作者提出了Spatio-Temporal Gaze Network（ST-Gaze），该模型结合CNN骨干网和特定通道注意及自我注意模块，优化眼部与面部特征融合，并通过时间传播捕捉帧间动态。", "result": "在EVE数据集上的实验表明，所提出的ST-Gaze模型实现了最先进的性能，无论是否进行人特异性调整。此外，消融研究进一步证明了其优越性。", "conclusion": "作者的研究结果为使用常见相机实现更稳健的视频注视点估算铺平了道路。"}}
{"id": "2512.17667", "pdf": "https://arxiv.org/pdf/2512.17667", "abs": "https://arxiv.org/abs/2512.17667", "authors": ["Yifei Cheng", "Yujia Zhu", "Baiyang Li", "Xinhao Deng", "Yitong Cai", "Yaochen Ren", "Qingyun Liu"], "title": "STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": "Accepted by IEEE INFOCOM 2026. Camera-ready version", "summary": "Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.", "AI": {"tldr": "STAR是一种零样本HTTPS网站指纹识别方法，通过学习加密流量和逻辑配置文件的联合嵌入空间来提高隐私保护。", "motivation": "现代HTTPS机制虽然增强了隐私保护，但仍易受基于加密流量模式的网站指纹攻击。现有的WF方法依赖于有监督的学习，并且无法处理未见过的网站。STAR旨在解决这些限制，通过零样本跨模态检索问题来提升性能。", "method": "STAR利用双编码器架构学习加密交通跟踪和爬取时间逻辑配置文件之间的联合嵌入空间。训练基于150K自动收集的交通-逻辑对，并使用对比和一致性目标以及结构感知增强。", "result": "在未见过的1600个网站上，STAR实现了87.9％的顶级准确性和0.963 AUC，在开放世界的检测中优于监督和少量样本基线。添加适配器后，仅使用每个站点四个标记跟踪进一步将TOP-5准确性提高到98.8％。", "conclusion": "分析表明，现代网络协议中存在的语义流量一致性是加密HTTPS流量的主要隐私风险。STAR及其数据集和代码已发布以支持可重复性和未来研究。"}}
{"id": "2512.17663", "pdf": "https://arxiv.org/pdf/2512.17663", "abs": "https://arxiv.org/abs/2512.17663", "authors": ["Antonios Antoniadis", "Denise Graafsma", "Ruben Hoeksma", "Maria Vlasiou"], "title": "Refining the Complexity Landscape of Speed Scaling: Hardness and Algorithms", "categories": ["cs.DS", "cs.CC", "cs.DM"], "comment": null, "summary": "We study the computational complexity of scheduling jobs on a single speed-scalable processor with the objective of capturing the trade-off between the (weighted) flow time and the energy consumption. This trade-off has been extensively explored in the literature through a number of problem formulations that differ in the specific job characteristics and the precise objective function. Nevertheless, the computational complexity of four important problem variants has remained unresolved and was explicitly identified as an open question in prior work. In this paper, we settle the complexity of these variants. More specifically, we prove that the problem of minimizing the objective of total (weighted) flow time plus energy is NP-hard for the cases of (i) unit-weight jobs with arbitrary sizes, and (ii)~arbitrary-weight jobs with unit sizes. These results extend to the objective of minimizing the total (weighted) flow time subject to an energy budget and hold even when the schedule is required to adhere to a given priority ordering. In contrast, we show that when a completion-time ordering is provided, the same problem variants become polynomial-time solvable. The latter result highlights the subtle differences between priority and completion orderings for the problem.", "AI": {"tldr": "论文研究了带权重流动时间和能耗约束的单处理器调度问题，解决了一些未解的NP难问题。", "motivation": "为了更好地理解在单一速度可调处理器上任务调度时流时间与能源消耗之间的权衡关系。", "method": "通过证明某些特定情况下的优化问题是NP难的，并展示当完成时间排序给定时这些问题是可以多项式时间内求解的。", "result": "证明了两种情况下（单位权重的工作和任意大小的工作，以及任意权重的工作和固定大小）最小化流动时间和能耗之和的问题是NP难的；同时也展示了在特定条件下该问题可以被多项式时间内解决。", "conclusion": "研究结果揭示了优先级排序与完成时间排序对于调度问题中的复杂性有着细微却重要的区别。"}}
{"id": "2512.17661", "pdf": "https://arxiv.org/pdf/2512.17661", "abs": "https://arxiv.org/abs/2512.17661", "authors": ["Yao Feng", "Chendong Xiang", "Xinyi Mao", "Hengkai Tan", "Zuyue Zhang", "Shuhe Huang", "Kaiwen Zheng", "Haitian Liu", "Hang Su", "Jun Zhu"], "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.", "AI": {"tldr": "本文提出了Vidarc，一种用于闭合循环控制的嵌入式视频扩散模型。", "motivation": "在数据稀缺的情况下，机器人手臂操作任务极具挑战性。现有的基于视频的方法通常不专门优化以适应特定于主体的闭环控制。", "method": "Vidarc通过预训练一个百万跨体体验集上的自回归视频扩散方法，并使用掩码反向动力学模型增强，实现快速、准确的闭环控制。", "result": "在真实世界部署中，Vidarc实现了至少高出15%的成功率和高达91%的延迟减少。它还展示了对以前未见过的机器人平台的强大泛化能力和错误修正能力。", "conclusion": "通过结合视频预测与动作相关的掩码，并利用实时反馈进行自回归生成缓存，Vidarc在闭合循环控制中表现出色，在各种任务和环境中都优于现有的方法。"}}
{"id": "2512.17655", "pdf": "https://arxiv.org/pdf/2512.17655", "abs": "https://arxiv.org/abs/2512.17655", "authors": ["Evangelos Sariyanidi", "Gokul Nair", "Lisa Yankowitz", "Casey J. Zampella", "Mohan Kashyap Pargi", "Aashvi Manakiwala", "Maya McNealis", "John D. Herrington", "Jeffrey Cohn", "Robert T. Schultz", "Birkan Tunc"], "title": "Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.", "AI": {"tldr": "Bitbox是一款开源工具箱，旨在通过视频分析人类行为，为心理学、精神病学和神经科学等领域的研究者提供便捷的计算方法。", "motivation": "现有的一些基于AI的行为视频分析工具需要专业软件栈，并且难以直接应用于假设驱动的研究。为了促进这些现代技术在行为科学研究中的应用，Bitbox应运而生。", "method": "Bitbox设计为一个模块化、可重复和易于解释的系统，它通过标准化接口从视频中提取高级的行为测量数据。核心模块使用面部、头部和身体处理程序进行测试并得到验证。", "result": "Bitbox使行为研究人员能够获得无需工程专业知识即可使用的强大且高级的行为指标，并向计算机科学家提供将方法传播到其影响最大的领域的方法。", "conclusion": "Bitbox预期会加速计算行为测量在行为、临床和心理健康研究中的整合，这是一项由社区驱动的持续演进的努力。"}}
{"id": "2512.17650", "pdf": "https://arxiv.org/pdf/2512.17650", "abs": "https://arxiv.org/abs/2512.17650", "authors": ["Zhongwei Zhang", "Fuchen Long", "Wei Li", "Zhaofan Qiu", "Wu Liu", "Ting Yao", "Tao Mei"], "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "categories": ["cs.CV", "cs.MM"], "comment": "Project page: https://zhw-zhang.github.io/ReCo-page/", "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "AI": {"tldr": "该论文提出了ReCo，一种用于指令视频编辑的新范式，通过在上下文生成过程中引入区域约束来改进视频编辑效果。", "motivation": "传统的方法难以指定编辑区域，并且会导致不准确的编辑区域和编辑与非编辑区域之间的令牌干扰问题。为解决这些问题，该论文提出了ReCo模型。", "method": "ReCo采用宽度拼接的方式将源视频和目标视频联合去噪。通过引入两个正则化项（即潜层和注意力正则化）来校准视频扩散学习，以此增加编辑区域的差异性并减少非编辑区域的影响。", "result": "实验结果显示，在四个主要基于指令的视频编辑任务中，ReCo均表现出优越性。", "conclusion": "通过引入区域约束，该模型有效解决了传统方法中存在的问题，并在多种视频编辑任务上实现了更好的效果。"}}
{"id": "2512.17640", "pdf": "https://arxiv.org/pdf/2512.17640", "abs": "https://arxiv.org/abs/2512.17640", "authors": ["Zhaolin Cai", "Huiyu Duan", "Zitong Xu", "Fan Li", "Zhi Liu", "Jing Liu", "Wei Shen", "Xiongkuo Min", "Guangtao Zhai"], "title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \\GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.", "AI": {"tldr": "提出了一种新的框架GRASP-HO，用于生成性的人体与物体交互检测，通过不同的认知导向多模态大型语言模型。", "motivation": "现有方法在处理未见过或模糊的交互时表现不佳；多模态大语言模型虽然具备丰富的世界知识，但与其脱节且难以微调。", "method": "设计了一种轻量级可学习的认知导向通道模块（CSC），将细粒度视觉证据注入冻结的多模态大型语言模型中进行有效推理。引入了混合引导策略来解决基于分类的人体与物体交互数据集和开放词汇生成模型之间的监督不匹配。", "result": "实验表明，该方法在闭集合性能上达到了最先进的水平，并且具备强大的零样本泛化能力。", "conclusion": "提出了一种统一的范式，无缝地桥接了判别感知和生成推理，实现了开世界人体与物体交互检测。"}}
{"id": "2512.17637", "pdf": "https://arxiv.org/pdf/2512.17637", "abs": "https://arxiv.org/abs/2512.17637", "authors": ["Anirban Majumdar", "Ritam Raha", "Rajarshi Roy", "David Parker", "Marta Kwiatkowska"], "title": "About Time: Model-free Reinforcement Learning with Timed Reward Machines", "categories": ["cs.AI", "cs.FL", "cs.LO"], "comment": null, "summary": "Reward specification plays a central role in reinforcement learning (RL), guiding the agent's behavior. To express non-Markovian rewards, formalisms such as reward machines have been introduced to capture dependencies on histories. However, traditional reward machines lack the ability to model precise timing constraints, limiting their use in time-sensitive applications. In this paper, we propose timed reward machines (TRMs), which are an extension of reward machines that incorporate timing constraints into the reward structure. TRMs enable more expressive specifications with tunable reward logic, for example, imposing costs for delays and granting rewards for timely actions. We study model-free RL frameworks (i.e., tabular Q-learning) for learning optimal policies with TRMs under digital and real-time semantics. Our algorithms integrate the TRM into learning via abstractions of timed automata, and employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search. Experimentally, we demonstrate that our algorithm learns policies that achieve high rewards while satisfying the timing constraints specified by the TRM on popular RL benchmarks. Moreover, we conduct comparative studies of performance under different TRM semantics, along with ablations that highlight the benefits of counterfactual-imagining.", "AI": {"tldr": "提出了一种新的时间奖励机（TRM）以在强化学习中表达非马尔可夫性奖励，同时引入了精确的时间约束。", "motivation": "现有奖励机制无法处理准确的时间限制，在某些应用中存在局限性。为解决此问题，本文提出了带有时间限制的奖励结构，旨在通过更丰富的奖励逻辑来优化策略学习。", "method": "研究了基于TRM的无模型强化学习框架，采用Q-学习算法，并结合时态自动机抽象和反事实想象启发式方法来提升搜索效率。", "result": "在实验中展示了该算法能够获得高收益并满足由TRM指定的时间约束，在流行RL基准上进行了性能比较研究，证明了其有效性。", "conclusion": "通过引入时间奖励机制（TRMs），可以有效地解决时间敏感应用中的强化学习问题，并且通过使用反事实想象启发式方法进一步提高了学习效率。"}}
{"id": "2512.17636", "pdf": "https://arxiv.org/pdf/2512.17636", "abs": "https://arxiv.org/abs/2512.17636", "authors": ["Mingyu Su", "Jian Guan", "Yuxian Gu", "Minlie Huang", "Hongning Wang"], "title": "Trust-Region Adaptive Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.", "AI": {"tldr": "提出了一种新的混合框架TRAPO，通过结合监督微调和强化学习来改进大型语言模型的复杂推理能力。", "motivation": "现有的两阶段方法（先进行监督微调再进行强化学习）存在模仿僵化、抑制探索以及导致遗忘的问题，限制了进一步提升的空间。为了克服这些缺点，TRAPO旨在通过在训练过程中交替应用监督微调和强化学习来统一外部指导与自我探索。", "method": "TRAPO框架通过优化专家前缀的SFT损失和模型自身完成部分的RL损失，实现了一种新颖的混合方法。该框架引入了信任区域监督微调（TrSFT）以稳定训练过程，并使用自适应前缀选择机制来基于实际效用分配专家指导。", "result": "实验结果表明，TRAPO在五个数学推理基准上均优于标准的监督微调、强化学习以及两阶段方法，并且超过了最近的状态-of-the-art技术。", "conclusion": "TRAPO框架提供了一种新的强大范式来增强大型语言模型的复杂推理能力，展示了其在实际应用中的优越性能。"}}
{"id": "2512.17629", "pdf": "https://arxiv.org/pdf/2512.17629", "abs": "https://arxiv.org/abs/2512.17629", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "title": "SCOPE: Sequential Causal Optimization of Process Interventions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.", "AI": {"tldr": "SCOPE提出了一种顺序因果优化过程干预的方法，用于在业务流程中推荐干预措施以优化关键绩效指标。", "motivation": "现有的PresPM方法要么只关注单个的干预决策，要么将多个独立的干预视作相互间没有影响，这导致它们无法准确地应对实际场景中的复杂性。此外，依赖模拟或数据增强的方法可能带来现实差距和偏差问题。", "method": "SCOPE使用逆向归纳法估计每个候选干预行动的影响，并将其从最终决策点传回起点。这种方法利用因果学习者直接使用观察数据进行训练，而无需构建过程近似用于强化学习。", "result": "实验结果表明，在现有合成数据集和基于实际日志的半合成数据集中，SCOPE在优化KPI方面始终优于现有的PresPM技术。", "conclusion": "SCOPE提供了一种能够直接处理多个干预措施相互影响的问题，并且能利用观察性数据的方法。同时，提出的新的半合成基准可以为未来的顺序PresPM研究提供参考。"}}
{"id": "2512.17621", "pdf": "https://arxiv.org/pdf/2512.17621", "abs": "https://arxiv.org/abs/2512.17621", "authors": ["Fengchun Liu", "Songhan Jiang", "Linghan Cai", "Ziyue Wang", "Yongbing Zhang"], "title": "PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology", "categories": ["cs.CV"], "comment": null, "summary": "While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.", "AI": {"tldr": "PathFLIP是一种用于全滑片图像(WSI)解释的新型框架，通过将整体描述分解为区域级子描述并生成文本条件下的区域嵌入以促进精确的语言视觉对齐。", "motivation": "现有的语言-视觉模型在处理大尺寸和空间异质性的WSI时存在挑战，难以捕捉细粒度的对应关系，从而影响其在下游任务中的性能。PathFLIP旨在解决这些问题，提供一种更精细、更具适应性的WSI解释方法。", "method": "PathFLIP将滑片级别的描述分解为区域级别的子描述，并生成文本条件下的区域嵌入以促进精确的语言视觉对齐。它使用大型语言模型(LLMs)来跟随不同的临床指令并适应各种诊断环境，展示了多种能力，包括分类、检索、病变定位和指令遵循。", "result": "PathFLIP在四个代表性基准上超过了现有的大规模病理学VLMs，并且需要更少的训练数据。", "conclusion": "PathFLIP提供了一种细粒度、指令感知的WSI解释方法，这为临床实践铺平了道路。"}}
{"id": "2512.17620", "pdf": "https://arxiv.org/pdf/2512.17620", "abs": "https://arxiv.org/abs/2512.17620", "authors": ["Di Wu", "Feng Yang", "Wenhui Zhao", "Jinwen Yu", "Pan Liao", "Benlian Xu", "Dingwen Zhang"], "title": "StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 4 figures. This work has been submitted to the IEEE for possible publication", "summary": "Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.", "AI": {"tldr": "StereoMV2D是一种结合了时序立体匹配的多视角三维目标检测框架，通过增强深度感知来提高查询先验的质量。", "motivation": "稀疏查询基于的多视图三维检测器虽然能高效集成多个图像中的对象特征，但单帧二维检测所固有的深度模糊性限制了三维查询生成的准确性。为了克服这个问题，研究人员提出了StereoMV2D框架。", "method": "该框架通过利用相邻帧中相同物体的跨时序视差来增强深度感知，并改进查询先验；同时，在感兴趣区域（RoIs）内高效执行所有计算。此外，一个动态置信度门控机制通过学习来自匹配矩阵和外观一致性的统计模式，自适应评估时序立体线索的可靠性。", "result": "在nuScenes和Argoverse2数据集上的广泛实验表明，StereoMV2D实现了优越的目标检测性能，并且没有显著增加计算负担。", "conclusion": "研究提出了一种新的框架StereoMV2D，它结合了时序立体匹配来增强多视角三维目标的检测精度。通过动态置信度门控机制确保其在对象外观和遮挡下的鲁棒性表现。"}}
{"id": "2512.17612", "pdf": "https://arxiv.org/pdf/2512.17612", "abs": "https://arxiv.org/abs/2512.17612", "authors": ["Alireza Samadifardheris", "Dirk H. J. Poot", "Florian Wiesinger", "Stefan Klein", "Juan A. Hernandez-Tamames"], "title": "Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution", "categories": ["cs.CV"], "comment": "This work has been submitted to IEEE TMI for possible publication", "summary": "High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.", "AI": {"tldr": "该论文提出了一种基于物理信息的自我监督框架，利用常规获得的高分辨率加权磁共振成像（wMRI）扫描作为指导来实现定量磁共振成像（qMRI）超分辨率。", "motivation": "长时间的数据采集使得高分辨率qMRI在临床上应用不足。该研究旨在通过结合临床常用的HR wMRI数据，无需HR qMRI地面真相标签来进行训练，以加速qMRI的获取和应用。", "method": "将超分辨问题形式化为贝叶斯最大后验概率推理，最小化两个差异：（1）从超级分辨率的qMRI图生成的高分辨率图像与通过前向信号模型获得的加权磁共振成像（wMRI）之间的差异；（2）低分辨率的qMRI和降采样的预测结果之间的差异。该方法利用常规采集的数据进行训练，无需HR qMRI监督。", "result": "实验结果显示T1加权图像主要改善了T1图谱，T2加权图像提高了T2图谱，并且结合指导能够同时优化所有参数。在独立获取的体内数据验证中，模型训练后的合成数据质量可与5分钟参考扫描相媲美。", "conclusion": "通过解耦HR qMRI需求，该框架实现了快速qMRI采集并利用常规临床图像进行增强，为将定量弛豫度测量整合到临床工作流程中提供了可行途径。"}}
{"id": "2512.17610", "pdf": "https://arxiv.org/pdf/2512.17610", "abs": "https://arxiv.org/abs/2512.17610", "authors": ["Denis Mikhailapov", "Vladimir Berikov"], "title": "Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR", "categories": ["cs.CV"], "comment": "7 pages, 5 figures, 1 listing", "summary": "Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy. These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection. However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs. This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.", "AI": {"tldr": "本文提出了一种基于额外旋转和翻转的半监督学习方法，用于具有多个输出的模型，在不假设模型响应的概率性质的情况下进行主动脉B型夹层的三维分割。", "motivation": "为了克服获取高质量标签数据的成本高和耗时问题，该研究利用了半监督学习方法。这种方法可以同时使用有标记的数据和无标记的数据来训练模型，并且特别适用于需要精确分割的医学影像应用领域。", "method": "通过引入额外旋转和翻转的方法对3D CTA图像进行处理，本文提出了一种新的半监督学习框架用于主动脉B型夹层的三维分割。这种技术允许在不假设模型响应的概率性质的情况下实现有效的训练。", "result": "该方法能够在有限标记数据的情况下提高多类别的医学影像分割精度，特别是在缺乏高质量标签时表现突出。", "conclusion": "通过将额外旋转和翻转引入半监督学习框架中，本文展示了如何有效利用少量标注数据进行主动脉B型夹层的三维分割。这种方法为其他需要高精度分割且难以获取大量标记数据的应用提供了新的可能性。"}}
{"id": "2512.17607", "pdf": "https://arxiv.org/pdf/2512.17607", "abs": "https://arxiv.org/abs/2512.17607", "authors": ["Zhaoqian Gao", "Min Yanga"], "title": "More Consistent Accuracy PINN via Alternating Easy-Hard Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.", "AI": {"tldr": "本文提出了一种结合硬优先和易优先训练策略的交替算法，以提高求解偏微分方程（PDE）的物理信息神经网络（PINN）的性能。", "motivation": "现有的PINNs培训方法存在明显的权衡和不一致的表现问题。为解决这一难题，本文提出了一种结合硬优先和易优先训练策略的方法，以提高其准确性与稳定性。", "method": "通过交替使用硬优先和易优先训练算法来组合两种方法的优势，并在不同类型的PDE上测试该方法的有效性。", "result": "所提出的交替训练法在具有陡峭梯度、非线性和高维度的PDE中表现出一致的高精度，相对L2误差主要在O(10^-5)到O(10^-6)范围内，超越了基线方法，并展示了更高的可靠性。", "conclusion": "这项工作提供了一种新的混合训练策略的设计思路来提升PINNs的性能和鲁棒性。"}}
{"id": "2512.17605", "pdf": "https://arxiv.org/pdf/2512.17605", "abs": "https://arxiv.org/abs/2512.17605", "authors": ["Svetlana Krasnova", "Emiliya Starikova", "Ilia Naletov", "Andrey Krylov", "Dmitry Sorokin"], "title": "MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.", "AI": {"tldr": "该论文提出了MGRegBench，这是一个用于乳腺X线摄影图像配准的公开基准数据集。", "motivation": "缺乏公共数据集和标准化基准阻碍了乳房影像注册技术的进步。为了解决这个问题，并推动相关研究的发展，作者创建了这个包含大量手动注释的数据集。", "method": "该论文评估了多种乳房X线摄影图像配准方法，包括经典的方法（ANTs），基于学习的方法（VoxelMorph，TransMorph）和隐式神经表示法（IDIR）。这些方法要么是从作者的实现中改编而来，要么是完全从头开始重新实现。", "result": "该数据集包含超过5000对影像，并且有100对包括了手动解剖标志和分割掩模。它首次提供了针对这个模式的大规模公共2D注册数据集以及详细的深度学习图像配准分析。", "conclusion": "论文发布了MGRegBench，为未来的研究提供了一个基础资源，以便进行公平的比较，并促进乳房影像配准技术的发展。"}}
{"id": "2512.17601", "pdf": "https://arxiv.org/pdf/2512.17601", "abs": "https://arxiv.org/abs/2512.17601", "authors": ["Zhaolin Cai", "Fan Li", "Ziwei Zheng", "Haixia Bi", "Lijun He"], "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.", "AI": {"tldr": "提出了一种基于多模态大型语言模型中注意力头的无微调视频异常检测方法HeadHunt-VAD。", "motivation": "当前基于MLLMs的传统无微调视频异常检测方法依赖文本生成，导致信息丢失、正常偏差及提示敏感性问题，难以捕捉细微的异常线索。为解决这些问题，提出了新的直接利用内部注意力头的方法。", "method": "提出HeadHunt-VAD框架，通过Robust Head Identification模块评估所有注意力头并选择稳定且具区分性的头部；这些头部特征用于训练轻量级异常评分器和时间定位器来实现高效准确的检测。", "result": "实验表明，在主要VAD基准上，HeadHunt-VAD优于其他无微调方法，并保持高效率。", "conclusion": "通过多模态大型语言模型中的头层探测解决了当前视频异常检测中面临的挑战，验证了其作为实际应用的强大和实用解决方案的有效性。"}}
{"id": "2512.17594", "pdf": "https://arxiv.org/pdf/2512.17594", "abs": "https://arxiv.org/abs/2512.17594", "authors": ["Tosin Ige", "Christopher Kiekintveld", "Aritran Piplai", "Asif Rahman", "Olukunle Kolade", "Sasidhar Kunapuli"], "title": "MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.", "AI": {"tldr": "提出了MAD-OOD框架，用于检测和分类未知恶意软件家族。", "motivation": "现有基于深度学习的恶意软件检测器依赖于封闭世界假设，在面对未见过的恶意软件家族时表现不佳。为了提高对未知恶意软件的识别能力，引入了MAD-OOD框架。", "method": "首先利用高斯判别分析建立类条件球形决策边界来分离已知和未知样本；第二阶段使用深度神经网络综合聚类预测、改进后的嵌入以及监督分类器输出以增强最终分类准确率。", "result": "在基准数据集上测试，MAD-OOD显著优于现有最佳方法，在未见过的恶意软件家族中达到AUC值高达0.911。", "conclusion": "提出的框架提供了一种可扩展、解释性强且统计基础良好的解决方案，适用于现实世界中的恶意软件检测和异常识别。"}}
{"id": "2512.17590", "pdf": "https://arxiv.org/pdf/2512.17590", "abs": "https://arxiv.org/abs/2512.17590", "authors": ["Malak Sadek", "Loraine Clarke", "Stefania Forlini", "Uta Hinrichs"], "title": "Digital Bricolage: Design Speculations for Embodied Approaches to Digitized Print-based Cultural Collections", "categories": ["cs.HC"], "comment": "ef:Presented at the 9th Workshop on Visualization for the Digital Humanities at the IEEE VIS Conference 2025", "summary": "COVID-related closures of public and academic libraries have underlined the importance of online platforms that provide access to digitized print-based collections. However, they also have highlighted the value of in-person handling of print artefacts for sensing and making sense of them. How do existing dominant digital platforms invite and/or discourage embodied forms of exploration and sense-making? What opportunities for embodied experience might we discover if we embrace the material qualities of print-based collections when designing interfaces for digital access? In this paper, we present findings from a speculative exercise where we invited creative professionals and experts in curating and handling access to collections to reflect on existing approaches to digitized print-based collections and to speculate about alternative design opportunities and modes of engagement. We argue for digital bricolage-a design approach that values working with materials that are \"on hand\" and embracing our ability to \"handle\" them in ways that foster both casual and curious exploration.", "AI": {"tldr": "论文探讨了通过数字平台访问印刷品收藏时，如何鼓励身体体验和探索的设计方法。", "motivation": "新冠疫情导致公共和学术图书馆关闭，强调了在线平台提供数字化印刷藏品访问的重要性。同时突显了亲自处理实物来感知和理解它们的价值。", "method": "邀请创意专业人士和收藏管理专家对现有数字印刷收藏的访问方式进行了反思，并提出了替代设计机会和互动模式的设想。", "result": "提出了一种名为“数字拼凑”的设计方法，这种方法重视使用手头材料并鼓励以促进随意和好奇探索的方式处理它们。", "conclusion": "通过采用数字拼凑的方法可以发现新的身体体验的机会，从而更好地理解和利用印刷品收藏。"}}
{"id": "2512.17586", "pdf": "https://arxiv.org/pdf/2512.17586", "abs": "https://arxiv.org/abs/2512.17586", "authors": ["Mahesh Keswani", "Raunak Bhattacharyya"], "title": "Learning Safe Autonomous Driving Policies Using Predictive Safety Representations", "categories": ["cs.LG", "cs.RO"], "comment": "8 pages, 4 figures. Submitted to ICRA 2026", "summary": "Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.", "AI": {"tldr": "本文研究了SRPL框架在真实世界自动驾驶场景中的应用，探讨其如何通过预测性安全表示来优化奖励与安全性之间的权衡。", "motivation": "自动驾驶需要同时满足性能和严格的安全要求，这造成了效率与保守政策之间的矛盾。SRPL框架旨在解决这一挑战，并且已经在控制环境中表现出潜力。本文动机在于验证该方法在真实世界场景中的有效性。", "method": "通过使用Waymo Open Motion Dataset（WOMD）和NuPlan数据集进行系统性实验，评估了SRPL框架的效果。分析了预测安全表示对提高观测噪声鲁棒性和跨数据集泛化能力的作用。", "result": "在WOMD和NuPlan数据集上的实验表明，SRPL方法在成功率（效应大小r=0.65-0.86）和成本降低（效应大小r=0.70-0.83）方面实现了统计显著的改进，并且其效果取决于策略优化器以及数据分布。此外，预测安全表示有助于提高观察噪声鲁棒性及跨数据集泛化能力。", "conclusion": "研究结果表明，在自动驾驶中使用预测安全表示可以增强SafeRL方法的安全性和性能权衡，展示了SRPL框架在实际应用中的潜力和价值。"}}
{"id": "2512.17585", "pdf": "https://arxiv.org/pdf/2512.17585", "abs": "https://arxiv.org/abs/2512.17585", "authors": ["N. A. Adarsh Pritam", "Jeba Shiney O", "Sanyam Jain"], "title": "SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench", "AI": {"tldr": "研究开发了SkinGenBench，用于评估生成模型和预处理方法对合成皮肤图像以及下游黑色素瘤诊断的影响。", "motivation": "探讨不同生成模型与预处理技术如何影响合成皮肤病理图的准确性及其在辅助黑色素瘤诊断中的应用效果。", "method": "采用HAM10000和MILK10K数据集，通过几何增强和高级去噪处理两种方式评估StyleGAN2-ADA及DDPM模型的表现，并结合感知与分布度量指标、特征空间分析以及五个下游分类器的性能进行综合评价。", "result": "结果表明生成架构选择对图像保真度和诊断效果的影响大于预处理复杂性；在FID和KID等量化指标中，StyleGAN2-ADA表现更佳，而扩散模型则产生更具差异性的样本。尽管去噪能略微提高合成质量，但对最终分类性能影响有限。值得注意的是，引入合成数据显著提升了黑色素瘤检测的精确度。", "conclusion": "研究证明了特定生成架构在皮肤病理图模拟和疾病诊断中的优越性；未来工作将探索进一步优化模型以更好地捕捉临床相关纹理特征并提升整体诊断效能。"}}
{"id": "2512.17584", "pdf": "https://arxiv.org/pdf/2512.17584", "abs": "https://arxiv.org/abs/2512.17584", "authors": ["Christian Cella", "Sole Ester Sonnino", "Marco Faroni", "Andrea Zanchettin", "Paolo Rocco"], "title": "Optimized Scheduling and Positioning of Mobile Manipulators in Collaborative Applications", "categories": ["cs.RO"], "comment": "Accepted at The IFAC Joint Conference on Computers, Cognition and Communication (J3C) 2025", "summary": "The growing integration of mobile robots in shared workspaces requires efficient path planning and coordination between the agents, accounting for safety and productivity. In this work, we propose a digital model-based optimization framework for mobile manipulators in human-robot collaborative environments, in order to determine the sequence of robot base poses and the task scheduling for the robot. The complete problem is treated as black-box, and Particle Swarm Optimization (PSO) is employed to balance conflicting Key-Performance Indicators (KPIs). We demonstrate improvements in cycle time, task sequencing, and adaptation to human presence in a collaborative box-packing scenario.", "AI": {"tldr": "提出了基于粒子群优化的数字模型框架，用于确定协作环境中移动机械臂的基础姿势序列和任务调度。", "motivation": "为了提高共享工作空间中移动机器人之间的路径规划效率和协调性，并在确保安全性的前提下提升生产力。", "method": "采用黑盒处理方式和粒子群优化算法来平衡关键性能指标，解决协作环境中移动机械臂的姿势序列和任务调度问题。", "result": "改善了循环时间、任务排序以及对人类存在情况下的适应性，在包装箱场景中取得了显著效果。", "conclusion": "所提出的框架在处理移动机械臂的基础姿势和任务调度优化方面展示了优越性能。"}}
{"id": "2512.17581", "pdf": "https://arxiv.org/pdf/2512.17581", "abs": "https://arxiv.org/abs/2512.17581", "authors": ["Annika Reinke", "Evangelia Christodoulou", "Sthuthi Sadananda", "A. Emre Kavur", "Khrystyna Faryna", "Daan Schouten", "Bennett A. Landman", "Carole Sudre", "Olivier Colliot", "Nick Heller", "Sophie Loizillon", "Martin Maška", "Maëlys Solal", "Arya Yazdan-Panah", "Vilma Bozgo", "Ömer Sümer", "Siem de Jong", "Sophie Fischer", "Michal Kozubek", "Tim Rädsch", "Nadim Hammoud", "Fruzsina Molnár-Gábor", "Steven Hicks", "Michael A. Riegler", "Anindo Saha", "et al. (12 additional authors not shown)"], "title": "Medical Imaging AI Competitions Lack Fairness", "categories": ["cs.CV"], "comment": "Submitted to Nature BME", "summary": "Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.", "AI": {"tldr": "评估医学影像AI竞赛的数据集是否具有公平性，包括多样性和可访问性。", "motivation": "探讨医学影像AI基准测试数据集的代表性和可用性问题，以确定其对临床相关性的支持程度。", "method": "系统研究了241个生物医学图像分析挑战中的458项任务，涵盖19种成像模式。", "result": "发现竞赛数据集中存在显著偏见，包括地理位置、模态和问题类型相关的偏差，并且受限的访问条件降低了可重复性和长期再利用性。", "conclusion": "当前基准测试生态系统存在根本性的公平性限制，导致领导板上的成功与临床相关性之间脱节。"}}
{"id": "2512.17579", "pdf": "https://arxiv.org/pdf/2512.17579", "abs": "https://arxiv.org/abs/2512.17579", "authors": ["Marco Faroni", "Alessio Spanò", "Andrea M. Zanchettin", "Paolo Rocco"], "title": "On Using Neural Networks to Learn Safety Speed Reduction in Human-Robot Collaboration: A Comparative Analysis", "categories": ["cs.RO"], "comment": "Accepted at IEEE Internation Conference on Emerging Technologies and Factory Automation (ETFA) 2025", "summary": "In Human-Robot Collaboration, safety mechanisms such as Speed and Separation Monitoring and Power and Force Limitation dynamically adjust the robot's speed based on human proximity. While essential for risk reduction, these mechanisms introduce slowdowns that makes cycle time estimation a hard task and impact job scheduling efficiency. Existing methods for estimating cycle times or designing schedulers often rely on predefined safety models, which may not accurately reflect real-world safety implementations, as these depend on case-specific risk assessments. In this paper, we propose a deep learning approach to predict the robot's safety scaling factor directly from process execution data. We analyze multiple neural network architectures and demonstrate that a simple feed-forward network effectively estimates the robot's slowdown. This capability is crucial for improving cycle time predictions and designing more effective scheduling algorithms in collaborative robotic environments.", "AI": {"tldr": "本文提出了一种基于深度学习的方法，通过过程执行数据预测机器人的安全减速因子。", "motivation": "现有方法依赖预定义的安全模型来估计循环时间和设计调度器，但这些模型可能无法准确反映实际情况。因此，需要更精确的实时安全机制。", "method": "本文分析了多种神经网络架构，并证明了一个简单的前馈网络能够有效地估算机器人的减速。", "result": "实验表明，所提出的深度学习方法可以提高循环时间预测精度并优化协作机器人环境中的调度算法。", "conclusion": "通过直接从过程执行数据中预测安全减速因子，该方法能显著改善循环时间的准确性，并促进更有效的任务调度。"}}
{"id": "2512.17578", "pdf": "https://arxiv.org/pdf/2512.17578", "abs": "https://arxiv.org/abs/2512.17578", "authors": ["Ge Wang", "Xing Liu", "Xin Yuan"], "title": "3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Video snapshot compressive imaging (SCI) captures dynamic scene sequences through a two-dimensional (2D) snapshot, fundamentally relying on optical modulation for hardware compression and the corresponding software reconstruction. While mainstream video SCI using random binary modulation has demonstrated success, it inevitably results in temporal aliasing during compression. One-hot modulation, activating only one sub-frame per pixel, provides a promising solution for achieving perfect temporal decoupling, thereby alleviating issues associated with aliasing. However, no algorithms currently exist to fully exploit this potential. To bridge this gap, we propose an algorithm specifically designed for one-hot masks. First, leveraging the decoupling properties of one-hot modulation, we transform the reconstruction task into a generative video inpainting problem and introduce a stochastic differential equation (SDE) of the forward process that aligns with the hardware compression process. Next, we identify limitations of the pure diffusion method for video SCI and propose a novel framework that combines one-step regression initialization with one-step diffusion refinement. Furthermore, to mitigate the spatial degradation caused by one-hot modulation, we implement a dual optical path at the hardware level, utilizing complementary information from another path to enhance the inpainted video. To our knowledge, this is the first work integrating diffusion into video SCI reconstruction. Experiments conducted on synthetic datasets and real scenes demonstrate the effectiveness of our method.", "AI": {"tldr": "提出了一种基于一次回归初始化和一次扩散细化的算法，用于解决视频快照压缩成像中的一次激活调制问题。", "motivation": "主流视频快照压缩成像使用随机二进制调制虽然有效，但会导致时域混叠。一热调制能彻底解耦时间分量，避免这些问题，但是缺乏对应的算法实现。", "method": "通过引入生成式视频插值问题和前向过程的随机微分方程来解决重建任务，并提出了一次回归初始化结合一次扩散细化的新框架。同时采用双光路硬件设计以增强插值视频。", "result": "实验表明该方法能够有效提高图像质量，解决了现有技术中的时域混叠和空间退化问题。", "conclusion": "这是首次将扩散模型应用于视频快照压缩成像重建的研究，并展示了其在合成数据集及真实场景上的优越表现。"}}
{"id": "2512.17573", "pdf": "https://arxiv.org/pdf/2512.17573", "abs": "https://arxiv.org/abs/2512.17573", "authors": ["Qilong Wang", "Xiaofan Ming", "Zhenyi Lin", "Jinwen Li", "Dongwei Ren", "Wangmeng Zuo", "Qinghua Hu"], "title": "RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \\url{https://github.com/stonecutter-21/roomeditor}.", "AI": {"tldr": "提出了一种名为RoomEditor++的参数共享扩散架构，用于高保真家具合成。", "motivation": "为了克服现有图像组合方法在实现高保真度的同时保持背景完整性的限制，本文提出了一个新的基准数据集和一种新的参数共享扩散架构。", "method": "设计了一个包含112,851个训练对和1,832个测试对的RoomBench++数据集。同时提出了一种名为RoomEditor++的参数共享双扩散骨干网络，适用于U-Net和DiT架构，统一了参考图像和背景图像的特征提取与插值过程。", "result": "实验表明，RoomEditor++在量化指标、定性评估和人类偏好研究中优于现有方法，并且对未见过的室内场景具有强大的泛化能力。", "conclusion": "通过参数共享机制，RoomEditor++能够提供精确的几何变换、纹理保留和平滑集成。它不仅提高了合成家具的质量，还展示了在广泛场景中的适应性。"}}
{"id": "2512.17570", "pdf": "https://arxiv.org/pdf/2512.17570", "abs": "https://arxiv.org/abs/2512.17570", "authors": ["Yikang Yue", "Yishu Yin", "Xuehai Qian"], "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake", "AI": {"tldr": "本文提出了一种新的SSD卸载训练系统GreedySnake，通过垂直调度和优化器步长重叠来提高大规模语言模型（LLM）的训练效率。", "motivation": "为了使大型语言模型的训练更加经济高效，SSD卸载训练提供了一个实用且有前景的方法。然而，现有的水平调度方法限制了系统的性能提升潜力。", "method": "GreedySnake采用垂直调度策略，在处理完一层的所有微批次后才继续进行下一层的操作，并通过在前向传递和优化器步骤之间重叠执行来减轻I/O瓶颈。", "result": "实验结果表明，与ZeRO-Infinity相比，GreedySnake在A100 GPU上实现了显著的训练吞吐量提升：对于GPT-65B模型，在单GPU上的性能提高了1.96倍，在四块GPU上提高了1.93倍；对于更大的GPT-175B模型，在单GPU上的性能提高了2.53倍。", "conclusion": "GreedySnake通过优化调度策略和重叠执行步骤，有效提升了SSD卸载训练系统的效率。"}}
{"id": "2512.17568", "pdf": "https://arxiv.org/pdf/2512.17568", "abs": "https://arxiv.org/abs/2512.17568", "authors": ["Kangchen Lv", "Mingrui Yu", "Yongyi Jia", "Chenyu Zhang", "Xiang Li"], "title": "Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation", "categories": ["cs.RO"], "comment": "The first two authors contributed equally. Project Website: https://kinematics-aware-diffusion-policy.github.io", "summary": "Whole-body control of robotic manipulators with awareness of full-arm kinematics is crucial for many manipulation scenarios involving body collision avoidance or body-object interactions, which makes it insufficient to consider only the end-effector poses in policy learning. The typical approach for whole-arm manipulation is to learn actions in the robot's joint space. However, the unalignment between the joint space and actual task space (i.e., 3D space) increases the complexity of policy learning, as generalization in task space requires the policy to intrinsically understand the non-linear arm kinematics, which is difficult to learn from limited demonstrations. To address this issue, this letter proposes a kinematics-aware imitation learning framework with consistent task, observation, and action spaces, all represented in the same 3D space. Specifically, we represent both robot states and actions using a set of 3D points on the arm body, naturally aligned with the 3D point cloud observations. This spatially consistent representation improves the policy's sample efficiency and spatial generalizability while enabling full-body control. Built upon the diffusion policy, we further incorporate kinematics priors into the diffusion processes to guarantee the kinematic feasibility of output actions. The joint angle commands are finally calculated through an optimization-based whole-body inverse kinematics solver for execution. Simulation and real-world experimental results demonstrate higher success rates and stronger spatial generalizability of our approach compared to existing methods in body-aware manipulation policy learning.", "AI": {"tldr": "该论文提出了一种基于扩散策略的全臂机器人操纵方法，通过在三维空间中保持任务、观测和动作的一致性来提高样本效率和空间泛化能力。", "motivation": "传统的机器人操作通常只考虑末端执行器的姿态，在关节空间学习动作增加了政策学习的复杂度。为了更好地处理涉及身体碰撞避免或物体-机器人体接触的任务，需要一个既能理解非线性臂动力学又能保持三维一致性表示的方法。", "method": "该方法使用扩散策略，并在机器人状态和动作中引入3D点表示法，优化了全臂逆运动学求解器以生成关节角度命令。通过这种方式保证输出动作的几何可行性，并提高了样本效率和空间泛化能力。", "result": "实验结果表明，在身体意识操纵政策学习方面，该方法相比现有技术具有更高的成功率和更强的空间泛化能力。", "conclusion": "提出的方法能够在保持三维一致性的前提下提高机器人操作的成功率和性能。"}}
{"id": "2512.17566", "pdf": "https://arxiv.org/pdf/2512.17566", "abs": "https://arxiv.org/abs/2512.17566", "authors": ["Mathilde Gajda Faanes", "David Bouget", "Asgeir S. Jakola", "Timothy R. Smith", "Vasileios K. Kavouridis", "Francesco Latini", "Margret Jensdottir", "Peter Milos", "Henrietta Nittby Redebrandt", "Rickard L. Sjöberg", "Rupavathana Mahesparan", "Lars Kjelsberg Pedersen", "Ole Solheim", "Ingerid Reinertsen"], "title": "A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.", "AI": {"tldr": "该论文提出了一种统一的FLAIR高信号分割模型，用于不同类型的中枢神经系统肿瘤和采集时间点。", "motivation": "T2加权液体衰减反转恢复（FLAIR）磁共振成像在脑瘤诊断、治疗规划和监测中至关重要。自动分割FLAIR高信号区对临床有重要意义。", "method": "利用注意力U-Net架构训练了5000张来自不同中心的FLAIR图像，构建了一个统一的FLAIR高信号分割模型，并与特定数据集模型进行了性能对比。", "result": "该模型在各种肿瘤类型和采集时间点上取得了平均Dice分数：术前脑膜瘤88.65%，术前转移瘤80.08%，术前胶质瘤90.92%、术后84.60%，以及术前低级别胶质瘤84.47%和术后61.27%。", "conclusion": "统一模型在各种肿瘤类型和采集时间点上实现了与特定数据集模型相当的分割性能，有助于临床应用推广。"}}
{"id": "2512.17562", "pdf": "https://arxiv.org/pdf/2512.17562", "abs": "https://arxiv.org/abs/2512.17562", "authors": ["Sujal Chondhekar", "Vasanth Murukuri", "Rushabh Vasani", "Sanika Goyal", "Rajshree Badami", "Anushree Rana", "Sanjana SN", "Karthik Pandia", "Sulabh Katiyar", "Neha Jagadeesh", "Sankalp Gulati"], "title": "When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Technical Report", "summary": "Speech enhancement methods are commonly believed to improve the performance of automatic speech recognition (ASR) in noisy environments. However, the effectiveness of these techniques cannot be taken for granted in the case of modern large-scale ASR models trained on diverse, noisy data. We present a systematic evaluation of MetricGAN-plus-voicebank denoising on four state-of-the-art ASR systems: OpenAI Whisper, NVIDIA Parakeet, Google Gemini Flash 2.0, Parrotlet-a using 500 medical speech recordings under nine noise conditions. ASR performance is measured using semantic WER (semWER), a normalized word error rate (WER) metric accounting for domain-specific normalizations. Our results reveal a counterintuitive finding: speech enhancement preprocessing degrades ASR performance across all noise conditions and models. Original noisy audio achieves lower semWER than enhanced audio in all 40 tested configurations (4 models x 10 conditions), with degradations ranging from 1.1% to 46.6% absolute semWER increase. These findings suggest that modern ASR models possess sufficient internal noise robustness and that traditional speech enhancement may remove acoustic features critical for ASR. For practitioners deploying medical scribe systems in noisy clinical environments, our results indicate that preprocessing audio with noise reduction techniques might not just be computationally wasteful but also be potentially harmful to the transcription accuracy.", "AI": {"tldr": "研究了语音增强技术对现代医学ASR系统的影响，发现其反而降低了识别准确率。", "motivation": "质疑传统观点即语音增强能提升ASR性能，并探究在复杂噪音环境中的真实效果。", "method": "使用MetricGAN-plus-voicebank算法处理500条医疗音频记录，在九种噪声条件下评估四个现代ASR系统的表现，采用语义WER作为评价指标。", "result": "发现所有测试配置中，语音增强使semWER上升1.1%到46.6%，表明传统降噪可能损害ASR所需的声学特征。", "conclusion": "建议在临床嘈杂环境中部署医学转录系统时，避免使用噪声减少技术，因为这可能会降低准确性并浪费计算资源。"}}
{"id": "2512.17560", "pdf": "https://arxiv.org/pdf/2512.17560", "abs": "https://arxiv.org/abs/2512.17560", "authors": ["M. Faroni", "A. Spano", "A. M. Zanchettin", "P. Rocco"], "title": "Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Ensuring human safety in collaborative robotics can compromise efficiency because traditional safety measures increase robot cycle time when human interaction is frequent. This paper proposes a safety-aware approach to mitigate efficiency losses without assuming prior knowledge of safety logic. Using a deep-learning model, the robot learns the relationship between system state and safety-induced speed reductions based on execution data. Our framework does not explicitly predict human motions but directly models the interaction effects on robot speed, simplifying implementation and enhancing generalizability to different safety logics. At runtime, the learned model optimizes task selection to minimize cycle time while adhering to safety requirements. Experiments on a pick-and-packaging scenario demonstrated significant reductions in cycle times.", "AI": {"tldr": "提出了一种基于深度学习的任务调度方法，以提高人机协作的安全性和效率", "motivation": "传统安全措施在人类频繁互动时会增加机器人循环时间，影响效率。该研究旨在减少这种效率损失而不依赖于预设的安全逻辑知识", "method": "利用执行数据训练一个深层模型来预测系统状态与速度减缓之间的关系，并据此优化任务选择以缩短循环时间同时满足安全性要求", "result": "在抓取和包装场景中的实验显示了显著的循环时间减少", "conclusion": "该方法通过深度学习直接建模交互效果，简化实现并提高不同安全逻辑下的泛化能力"}}
{"id": "2512.17559", "pdf": "https://arxiv.org/pdf/2512.17559", "abs": "https://arxiv.org/abs/2512.17559", "authors": ["Maliha Tabassum", "M Shamim Kaiser"], "title": "Towards Explainable Conversational AI for Early Diagnosis with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.", "AI": {"tldr": "本研究提出了一种基于大规模语言模型的诊断聊天机器人，旨在通过更透明和互动的方式提高早期疾病的诊断效率。", "motivation": "当前AI和深度学习在医疗领域的应用存在交互性和透明度不足的问题，影响了其在实际应用场景中的效果。因此，研究者希望通过开发一种更加透明且易于理解的人工智能系统来改善这一状况。", "method": "该论文利用GPT-4o、增强型生成技术以及可解释人工智能方法构建了一个对话式诊断助手，通过与患者互动来提取和标准化症状，并使用相似性匹配及自适应提问策略优先考虑潜在的疾病。同时采用链式思维提示技术增强了系统背后的推理过程透明度。", "result": "在对比传统机器学习模型如朴素贝叶斯、逻辑回归等方法后，基于大规模语言模型的新系统表现出色，达到了90%的整体准确率和100%的Top-3准确率。", "conclusion": "该研究展示了通过对话式AI辅助早期疾病诊断的巨大潜力，并且证明了这种方法在提高透明度和临床相关性方面具有显著优势。"}}
{"id": "2512.17553", "pdf": "https://arxiv.org/pdf/2512.17553", "abs": "https://arxiv.org/abs/2512.17553", "authors": ["Guglielmo Del Col", "Väinö Karjalainen", "Teemu Hakala", "Yibo Zhang", "Eija Honkavaara"], "title": "Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz. The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.", "AI": {"tldr": "本文提出了一种基于深度学习的自主导航框架，用于在密集森林中实现鲁棒飞行。", "motivation": "由于自然环境下的有限可见性、薄且不规则的障碍物以及感知退化等问题，无人机在密集森林中的自主导航非常具有挑战性。该研究旨在解决这些问题，提供更可靠的飞行方案。", "method": "通过引入语义增强深度编码和神经运动元评估来改进原有的sevae-ORACLE算法，并添加了横向控制、时间一致性机制、立体视觉惯性里程计解决方案以及实时安全层等模块以提升自主导航能力。此外还增加了深度细化阶段，优化GPU性能。", "result": "新方法在模拟高密度森林环境下的测试中表现出更高的成功率和更稳定的轨迹，在实际飞行任务中的表现也优于现有导航系统。", "conclusion": "该研究提出的方法显著提升了无人机在复杂自然环境中自主导航的可靠性和安全性。"}}
{"id": "2512.17547", "pdf": "https://arxiv.org/pdf/2512.17547", "abs": "https://arxiv.org/abs/2512.17547", "authors": ["Mehdi Hosseinzadeh", "Shin-Fang Chng", "Yi Xu", "Simon Lucey", "Ian Reid", "Ravi Garg"], "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting", "categories": ["cs.CV"], "comment": "Project page: https://m80hz.github.io/g3splat/", "summary": "3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).", "AI": {"tldr": "G3Splat是一种新的3D场景表示方法，通过引入几何先验来生成一致的3D高斯点云。", "motivation": "现有基于图像预测3D高斯的方法依赖于视图合成损失，但在无姿态下的泛化能力不足。因此需要解决自监督学习中的模糊性问题，以获得具有几何意义的一致表示。", "method": "G3Splat通过引入几何先验来克服在自监督中学习3D高斯点云时的模糊性，并依靠RE10K数据集进行训练。", "result": "该方法实现了高质量的重建、姿态估计和新颖视图合成，在ScanNet上的零样本泛化能力也优于现有技术。", "conclusion": "G3Splat在几何一致性重建、相对姿态估计以及新视角合成方面达到了最新水平，且其代码和预训练模型已公开。"}}
{"id": "2512.17545", "pdf": "https://arxiv.org/pdf/2512.17545", "abs": "https://arxiv.org/abs/2512.17545", "authors": ["Yunqi Gao", "Leyuan Liu", "Yuhan Li", "Changxin Gao", "Yuanyuan Liu", "Jingying Chen"], "title": "ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages,16 figures", "summary": "With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \\url{https://github.com/starVisionTeam/ClothHMR}.", "AI": {"tldr": "本文提出了一种从单张图像中恢复穿着不同衣物的人体3D网格的新方法ClothHMR。", "motivation": "当前的方法在处理穿着宽松衣物的3D人体网格恢复时表现不佳，因此提出使用服装定制和基于大型基础视觉模型的人体信息来提高估计能力。", "method": "该方法包括两个模块：服装定制（CT）和FHVM基的网格恢复（MR）。CT模块通过身体语义估计和边缘预测确保衣物贴合人体轮廓。MR模块则优化3D人体网格参数，使其与从基础视觉模型中推断出的人体信息一致。", "result": "实验结果表明，ClothHMR在基准数据集和野外图像上显著优于现有的先进方法，并且开发了一个由ClothHMR支持的在线时尚购物应用。", "conclusion": "ClothHMR能够准确恢复穿着不同衣物的人体3D网格，显示出强大的实用价值。"}}
{"id": "2512.17541", "pdf": "https://arxiv.org/pdf/2512.17541", "abs": "https://arxiv.org/abs/2512.17541", "authors": ["Qijian Tian", "Xin Tan", "Jiayu Ying", "Xuhong Wang", "Yuan Xie", "Lizhuang Ma"], "title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views", "categories": ["cs.CV"], "comment": "Project page: https://fangzhou2000.github.io/projects/fleg", "summary": "We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.", "AI": {"tldr": "FLEG是一种前馈网络，可以从任意视角重建带有语言嵌入的3D高斯分布。", "motivation": "现有的解决方案在结合前馈重构和高斯头部时面临固定输入视图和不足的3D训练数据的问题。作者提出了一种无需3D标注的2D到3D提升框架，以应对这些问题，并通过实例引导对比学习使二维语义与三维表示对齐。", "method": "FLEG利用大量视频数据进行训练，这些数据具有易于获得的2D实例信息和语言嵌入。为了减少密集视图带来的高内存和计算成本，还提出了一种几何-语义层次稀疏化策略。", "result": "实验显示，FLEG在多种相关任务中超越了现有方法，在生成准确的几何、高质量的外观以及与语言对齐的语义方面表现出色。", "conclusion": "通过使用视频数据和提出的框架及策略，FLEG能够从任意稀疏或密集视图前馈方式高效地重建带有语言嵌入的3D高斯分布。"}}
{"id": "2512.17534", "pdf": "https://arxiv.org/pdf/2512.17534", "abs": "https://arxiv.org/abs/2512.17534", "authors": ["Christian Lagemann", "Sajeda Mokbel", "Miro Gondrum", "Mario Rüttgers", "Jared Callaham", "Ludger Paehler", "Samuel Ahnert", "Nicholas Zolman", "Kai Lagemann", "Nikolaus Adams", "Matthias Meinke", "Wolfgang Schröder", "Jean-Christophe Loiseau", "Esther Lagemann", "Steven L. Brunton"], "title": "HydroGym: A Reinforcement Learning Platform for Fluid Dynamics", "categories": ["physics.flu-dyn", "cs.AI", "cs.LG"], "comment": null, "summary": "Modeling and controlling fluid flows is critical for several fields of science and engineering, including transportation, energy, and medicine. Effective flow control can lead to, e.g., lift increase, drag reduction, mixing enhancement, and noise reduction. However, controlling a fluid faces several significant challenges, including high-dimensional, nonlinear, and multiscale interactions in space and time. Reinforcement learning (RL) has recently shown great success in complex domains, such as robotics and protein folding, but its application to flow control is hindered by a lack of standardized benchmark platforms and the computational demands of fluid simulations. To address these challenges, we introduce HydroGym, a solver-independent RL platform for flow control research. HydroGym integrates sophisticated flow control benchmarks, scalable runtime infrastructure, and state-of-the-art RL algorithms. Our platform includes 42 validated environments spanning from canonical laminar flows to complex three-dimensional turbulent scenarios, validated over a wide range of Reynolds numbers. We provide non-differentiable solvers for traditional RL and differentiable solvers that dramatically improve sample efficiency through gradient-enhanced optimization. Comprehensive evaluation reveals that RL agents consistently discover robust control principles across configurations, such as boundary layer manipulation, acoustic feedback disruption, and wake reorganization. Transfer learning studies demonstrate that controllers learned at one Reynolds number or geometry adapt efficiently to new conditions, requiring approximately 50% fewer training episodes. The HydroGym platform is highly extensible and scalable, providing a framework for researchers in fluid dynamics, machine learning, and control to add environments, surrogate models, and control algorithms to advance science and technology.", "AI": {"tldr": "本文介绍了HydroGym，一种用于流体动力学控制研究的强化学习平台。", "motivation": "该论文旨在解决流体流动控制中的挑战性问题，并推动强化学习在复杂领域（如机器人技术和蛋白质折叠）的应用至流体动力学。由于缺乏标准化基准平台和计算需求高，流体模拟方面面临困难。", "method": "HydroGym集成了复杂的流体控制测试环境、可扩展的运行时基础设施以及最先进的RL算法。它提供非微分求解器以支持传统RL，并通过梯度增强优化提高样本效率。", "result": "该平台在42个验证环境中显示出RL代理可以发现一致且稳健的控制原理，转移学习研究显示控制器能够高效地适应新条件。", "conclusion": "HydroGym为流体动力学、机器学习和控制的研究人员提供了一个框架，并具有高度可扩展性和灵活性以支持进一步开发与应用。"}}
{"id": "2512.17532", "pdf": "https://arxiv.org/pdf/2512.17532", "abs": "https://arxiv.org/abs/2512.17532", "authors": ["Jiaqi Tang", "Jianmin Chen", "Wei Wei", "Xiaogang Xu", "Runtao Liu", "Xiangyu Wu", "Qipeng Xie", "Jiafei Wu", "Lei Zhang", "Qifeng Chen"], "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2026 Oral", "summary": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.", "AI": {"tldr": "提出了Robust-R1框架，旨在提高多模态语言模型在极端视觉退化情况下的鲁棒性。", "motivation": "现有的多模态大语言模型难以应对极端的现实世界中的视觉降级，导致它们的实际稳健性能不足。为了克服这些限制，作者提出了一种新的方法来显式建模视觉退化，并提高解释性和优化能力。", "method": "通过监督微调建立视觉降级感知基础，利用奖励驱动对齐准确感知降级参数，并根据降级强度调整动态推理深度尺度。引入了一个特殊的数据集，包含11K个样本，模拟了四个关键的现实世界视觉处理阶段，每个样本都带有结构化链路。", "result": "实验评估表明Robust-R1在R-Bench上的表现优于所有通用和鲁棒性基准，在MMMB、MMStar和RealWorldQA上也具有优越的抗降级性能。", "conclusion": "通过提出新的框架，显著提高了模型在极端视觉退化情况下的稳健性和抗干扰能力。"}}
{"id": "2512.17527", "pdf": "https://arxiv.org/pdf/2512.17527", "abs": "https://arxiv.org/abs/2512.17527", "authors": ["Muhammad Haris Khan"], "title": "SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate \"never-before-seen\" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.", "AI": {"tldr": "构建了一个名为SafeBench-Seq的基准和基线分类器，用于蛋白质危险筛查。", "motivation": "现有的基于基础模型的蛋白质设计方法存在生物安全风险，但缺乏简单可重现且在同源性控制下进行评估的基础序列水平危险筛查方案。", "method": "采用公共数据（SafeProtein危险蛋白和UniProt良性蛋白）构建基准分类器，并使用全局物理化学描述符和氨基酸组成作为特征。通过聚类排除训练集与测试集中序列重叠，提供置信区间并通过调整后的概率进行校准。", "result": "随机分割法相对于同源性聚类评估时会高估模型的稳健性；线性模型具有较好的校准性能，而树形集成模型则保持较高的Brier/ECE得分。SafeBench-Seq在CPU上运行且仅提供元数据，不发放有害序列。", "conclusion": "SafeBench-Seq提供了简单可重现的基础基准和基线分类器进行蛋白质危险筛查，并通过严格的评估确保生物安全性。"}}
{"id": "2512.17519", "pdf": "https://arxiv.org/pdf/2512.17519", "abs": "https://arxiv.org/abs/2512.17519", "authors": ["Muhammad Haris Khan"], "title": "Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.", "AI": {"tldr": "提出了K-OTG机制，通过秘密密钥访问控制来保护指令调优的语言模型。", "motivation": "为了防止未经授权的用户使用语言模型，并同时保持授权用户的性能。", "method": "引入了K-OTG机制，在训练阶段区分授权和未授权示例；在推理时应用正交变换隐藏状态，只有正确的密钥才能恢复原始基底，否则会扰乱输出。", "result": "实验表明，未经授权的用户无法使用模型，而授权用户的性能仅略有下降。此方法与LoRA良好兼容，并且能够有效地阻止未授权访问。", "conclusion": "K-OTG提供了一种实用的方法来防止未经授权的使用，并保持了授权用户的性能"}}
{"id": "2512.17517", "pdf": "https://arxiv.org/pdf/2512.17517", "abs": "https://arxiv.org/abs/2512.17517", "authors": ["Siemen Brussee", "Pieter A. Valkema", "Jurre A. J. Weijer", "Thom Doeleman", "Anne M. R. Schrader", "Jesper Kers"], "title": "PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology", "categories": ["cs.CV", "cs.LG", "cs.NE", "cs.SE", "q-bio.TO"], "comment": "14 Pages, 3 Figures, 2 Appendices", "summary": "We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL", "AI": {"tldr": "PathBench-MIL 是一个自动化的多实例学习框架，用于组织病理学中的模型训练和基准测试。", "motivation": "为了提高多实例学习在组织病理学应用中的效率和可重复性，提出了一种自动化、标准化的框架来简化实验过程并促进跨数据集和任务的一致性。", "method": "PathBench-MIL 自动化了端到端的 MIL 管道构建，并提供了包括预处理、特征提取以及 MIL 聚合在内的工具。此外，它还提供了一个统一配置系统和模块化的扩展功能，以支持快速实验并标准化结果。", "result": "通过集成各种多实例学习模型和特性提取器，PathBench-MIL 可进行可重复的基准测试，并且支持可视化工具以帮助研究者理解及优化他们的工作流程。", "conclusion": "PathBench-MIL 能够简化复杂的实验过程并促进跨数据集的一致性，同时它作为一个开放源代码项目，可供更多研究团队使用和贡献。"}}
{"id": "2512.17515", "pdf": "https://arxiv.org/pdf/2512.17515", "abs": "https://arxiv.org/abs/2512.17515", "authors": ["Mahsa Lavaei", "Zahra Abadi", "Salar Beigzad", "Alireza Maleki"], "title": "Resource-efficient medical image classification for edge devices", "categories": ["eess.IV", "cs.LG"], "comment": "Conference paper published in ICAMIDA 2025 (IEEE)", "summary": "Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.", "AI": {"tldr": "研究提出了一种在资源受限的边缘设备上高效进行医学图像分类的方法，通过模型量化技术降低计算和内存需求", "motivation": "在医疗保健中实现准确及时诊断的重要性推动了对深度学习模型在边缘设备上的部署需求。然而，这些设备由于计算和内存限制而面临挑战", "method": "研究采用模型量化方法（包括量化感知训练和后训练量化），以优化医学图像分类任务中的资源使用", "result": "实验结果显示，量化的模型不仅大幅降低了模型大小和推理延迟，还保持了临床可接受的诊断准确性", "conclusion": "这项工作为在远程及资源有限环境中部署AI驱动的医疗诊断技术提供了实际路径"}}
{"id": "2512.17514", "pdf": "https://arxiv.org/pdf/2512.17514", "abs": "https://arxiv.org/abs/2512.17514", "authors": ["Sairam VCR", "Rishabh Lalla", "Aveen Dayal", "Tejal Kulkarni", "Anuj Lalla", "Vineeth N Balasubramanian", "Muhammad Haris Khan"], "title": "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.", "AI": {"tldr": "该论文提出了一种增强特征空间中目标聚焦能力的方法，以改善源无关对象检测（SFOD）任务的表现。", "motivation": "当前的SFOD方法依赖于均值教师自标注，但域偏移会削弱模型保持强烈目标关注表征的能力，导致背景噪声中的高置信度激活，产生不可靠的伪标签。现有的工作主要改进这些伪标签，而忽略了增强特征空间本身的需求。", "method": "提出了一种称为FALCON-SFOD（基于Foundation Model对齐学习与抑制冗余和抗噪性）的框架，包括两个互补组件：SPAR（空间先验感知正则化），利用视觉基础模型的一般化能力来调节检测器特征空间；IRPL（不平衡敏感噪声鲁棒伪标签生成）。理论分析表明这些设计有助于更紧密地定位误差并减少分类错误。", "result": "FALCON-SFOD在多个SFOD基准测试中实现了具有竞争力的表现，增强了对象聚焦能力和适应性。", "conclusion": "通过增强特征空间中的目标聚焦能力，该方法克服了域偏移带来的挑战，并改进了源无关对象检测任务的性能。"}}
{"id": "2512.17505", "pdf": "https://arxiv.org/pdf/2512.17505", "abs": "https://arxiv.org/abs/2512.17505", "authors": ["Ufuk Asil", "Efendi Nasibov"], "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.", "AI": {"tldr": "本文提出了一种适用于无人机的视觉惯性里程计(VIO)方法，该方法结合了误差状态扩展卡尔曼滤波器(ESKF)和缩放无迹卡尔曼滤波器(SUKF)，以提高在复杂环境下的定位精度。", "motivation": "为了提升无人机在恶劣环境下进行准确姿态估计的能力，并确保传感器的可靠性。本文提出了一种新的混合方法，通过结合误差状态扩展卡尔曼滤波(ESKF)和缩放无迹卡尔曼滤波(SUKF)，来改进惯性测量单元(IMU)数据处理。", "method": "该系统采用一种新颖的四元数聚焦错误状态EKF/UKF(Qf-ES-EKF/UKF)架构。首先使用ESKF传播整个状态，然后应用SUKF步骤精确化姿态估计。同时根据图像熵、强度变化等指标动态评估视觉测量可靠性，并调整测量噪声协方差。", "result": "实验结果表明，在EuRoC MAV数据集上，该方法在挑战性场景中位置精度提高了49%，旋转精度提高了57%；与ESKF相比，具有约48%的计算效率提升。这些发现证明了所提出的方法能够有效平衡计算效率和估计准确性。", "conclusion": "本文提出的VIO方法通过采用新颖的Qf-ES-EKF/UKF架构，在复杂环境中显著提高了无人机的姿态估计性能，并在保证高精度的同时，大幅提升了计算效率"}}
{"id": "2512.17504", "pdf": "https://arxiv.org/pdf/2512.17504", "abs": "https://arxiv.org/abs/2512.17504", "authors": ["Hoiyeong Jin", "Hyojin Jang", "Jeongho Kim", "Junha Hyung", "Kinam Kim", "Dongjin Kim", "Huijin Choi", "Hyeonji Kim", "Jaegul Choo"], "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, project page: https://myyzzzoooo.github.io/InsertAnywhere/", "summary": "Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.", "AI": {"tldr": "InsertAnywhere是一种新的视频物体插入框架，它通过4D场景理解和扩散模型生成技术实现了几何一致性和光照一致性。", "motivation": "目前的视频生成方法在处理视频物体插入时存在对4D场景理解不足和难以处理遮挡及光影效果等问题。为此，提出了一个新的框架来解决这些问题。", "method": "该方法通过一个4D感知掩码生成模块重建场景几何结构，并传播用户指定的对象放置位置以保持时间一致性并避免遮挡不一致问题。然后使用扩散视频生成模型在插入对象及其周围环境变化（如光照和阴影）的同时进行合成。", "result": "实验结果显示，该框架能够产生几何上合理且视觉连贯的物体插入效果，在多种现实场景中优于现有研究及商业模型。", "conclusion": "InsertAnywhere提供了一种有效的视频物体插入方法，通过结合4D感知掩码生成和扩散模型来实现高质量的视频编辑。"}}
{"id": "2512.17499", "pdf": "https://arxiv.org/pdf/2512.17499", "abs": "https://arxiv.org/abs/2512.17499", "authors": ["Peshawa J. Muhammad Ali", "Navin Vincent", "Saman S. Abdulla", "Han N. Mohammed Fadhl", "Anders Blilie", "Kelvin Szolnoky", "Julia Anna Mielcarz", "Xiaoyi Ji", "Nita Mulliqi", "Abdulbasit K. Al-Talabani", "Kimmo Kartasalo"], "title": "Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort", "categories": ["cs.CV"], "comment": "40 pages, 8 figures, 11 tables", "summary": "Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer. Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium). Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa > 0.90) for all AI models and scanner pairs, including low-cost compact scanner. Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology. Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.", "AI": {"tldr": "评估AI模型在中东人群中诊断前列腺病理的准确性。", "motivation": "提高全球范围内病理AI系统的可接受度，特别是在未充分代表的人群中进行验证研究以实现最大收益。", "method": "收集并数字化339份来自伊拉克库尔德地区的前列腺活检样本，并评估任务特定的端到端AI模型和两个基础模型与病理科医师的一致性以及在三种扫描仪上的表现。", "result": "AI与病理学家之间的评分一致性接近于病理学家之间的一致性（Cohen的二次加权Kappa值为0.801 vs 0.799）。所有AI模型和扫描器组合的跨扫描器一致性都很高（二次加权Kappa大于0.90）。", "conclusion": "AI模型在前列腺组织病理学评估中表现出与病理科医师相当的表现。低成本紧凑型扫描仪可以支持验证研究，并允许实验室在有限样本量的情况下实现成本效益高的AI采纳。这是中东地区第一个公开可用的数字病理数据集，有助于进一步研究全球公平的人工智能病理学。"}}
{"id": "2512.17495", "pdf": "https://arxiv.org/pdf/2512.17495", "abs": "https://arxiv.org/abs/2512.17495", "authors": ["Rang Li", "Lei Li", "Shuhuai Ren", "Hao Tian", "Shuhao Gu", "Shicheng Li", "Zihao Yue", "Yudong Wang", "Wenhan Ma", "Zhe Yang", "Jingyuan Ma", "Zhifang Sui", "Fuli Luo"], "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.", "AI": {"tldr": "GroundingME是一个评估多模态大型语言模型在视觉定位任务中的基准测试，通过四个维度的挑战来揭示这些模型的能力差距。", "motivation": "当前的基准测试无法全面反映人类在复杂场景中处理视觉定位任务的能力。因此，引入了GroundingME以更好地衡量MLLMs的实际能力，并探索改进策略。", "method": "创建了一个包含1005个样本的数据集，这些样本分为四个维度进行评估：区分相似对象、理解空间关系、处理遮挡或小物体和识别无法定位的查询。通过自动生成与人工验证相结合的方法来构造数据集。", "result": "最好的模型在总体准确率方面仅达到45.1%，大多数模型在识别无法定位的任务上得分为0%，表明存在显著的能力差距。两种改进策略显示出一定的效果：思考轨迹选择最佳响应可提高复杂任务的准确性，而混合训练方法可以将无法定位查询的识别精度从0%提升至27.9%。", "conclusion": "GroundingME不仅揭示了当前MLLMs在视觉定位方面的局限性，而且也为未来研究提供了方向。"}}
{"id": "2512.17492", "pdf": "https://arxiv.org/pdf/2512.17492", "abs": "https://arxiv.org/abs/2512.17492", "authors": ["Oskar Kristoffersen", "Alba R. Sánchez", "Morten R. Hannemose", "Anders B. Dahl", "Dim P. Papadopoulos"], "title": "MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.", "AI": {"tldr": "本文提出了一种名为MMLANDMARKS的多模态地理空间理解基准，涵盖了高分辨率航拍图像、地面视角图像、文本信息和地理坐标等四种模态。", "motivation": "当前地理空间基准在多模式覆盖方面存在局限性，阻碍了相关领域的进展。因此，本文旨在通过引入MMLANDMARKS数据集来解决这一问题，以促进更广泛的地理空间理解。", "method": "研究构建了一个包含197k高分辨率航拍图像、329k地面视角图像以及相应文本信息和地理坐标的多模态数据集。该数据集中每种模式之间具有一对一的对应关系，适用于训练和评估多种地理空间任务模型。", "result": "通过使用CLIP启发的基线方法，在跨视图地面对卫星检索、地面及卫星定位等不同任务中展示了广泛的泛化能力和与现成的基础模型及专业最先进模型相比的竞争性能。", "conclusion": "本文证明了多模态数据集在实现广泛地理空间理解方面的必要性，并通过MMLANDMARKS数据集展示了其优势。"}}
{"id": "2512.17489", "pdf": "https://arxiv.org/pdf/2512.17489", "abs": "https://arxiv.org/abs/2512.17489", "authors": ["Muhammad Atif Butt", "Kai Wang", "Javier Vazquez-Corral", "Joost Van De Weijer"], "title": "LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Current text-to-image (T2I) models have demonstrated remarkable progress in creative image generation, yet they still lack precise control over scene illuminants, which is a crucial factor for content designers aiming to manipulate the mood, atmosphere, and visual aesthetics of generated images. In this paper, we present an illuminant personalization method named LumiCtrl that learns an illuminant prompt given a single image of an object. LumiCtrl consists of three basic components: given an image of the object, our method applies (a) physics-based illuminant augmentation along the Planckian locus to create fine-tuning variants under standard illuminants; (b) edge-guided prompt disentanglement using a frozen ControlNet to ensure prompts focus on illumination rather than structure; and (c) a masked reconstruction loss that focuses learning on the foreground object while allowing the background to adapt contextually, enabling what we call contextual light adaptation. We qualitatively and quantitatively compare LumiCtrl against other T2I customization methods. The results show that our method achieves significantly better illuminant fidelity, aesthetic quality, and scene coherence compared to existing personalization baselines. A human preference study further confirms strong user preference for LumiCtrl outputs. The code and data will be released upon publication.", "AI": {"tldr": "提出了一种名为LumiCtrl的方法，用于个性化文本到图像模型中的照明控制。", "motivation": "当前的文本到图像(T2I)模型在创意图像生成方面取得了显著进展，但仍缺乏对场景光源的精确控制。这种能力对于内容设计师来说至关重要，因为他们需要操纵生成图像的情绪、氛围和视觉美感。", "method": "LumiCtrl包括三个基本组成部分：基于物理原理的光源增强、边缘导向提示解耦以及掩码重构损失函数，以专注于前景对象的学习同时允许背景适应性地变化。", "result": "实验结果表明，与现有个性化方法相比，LumiCtrl在光源准确性、美学质量和场景一致性方面表现出显著优势。人类偏好研究表明用户更偏爱LumiCtrl的输出。", "conclusion": "通过结合物理学原理和先进的图像处理技术，LumiCtrl为文本到图像生成模型中的照明控制提供了新的解决方案，并且展示了良好的实际应用潜力。"}}
{"id": "2512.17488", "pdf": "https://arxiv.org/pdf/2512.17488", "abs": "https://arxiv.org/abs/2512.17488", "authors": ["Almustapha A. Wakili", "Adamu Hussaini", "Abubakar A. Musa", "Woosub Jung", "Wei Yu"], "title": "TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis", "categories": ["cs.CV", "cs.LG"], "comment": "IEEE Virtual Conference on Communications. 4-6 November 2025", "summary": "Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.", "AI": {"tldr": "提出了一种隐私保护的联邦学习框架TwinSegNet，用于准确和实时的大脑肿瘤分割。", "motivation": "当前深度学习方法依赖于集中化的数据收集，这引发了隐私问题并限制了不同机构之间的泛化能力。因此，开发一种既能保证隐私又能提供高精度分割的方法是必要的。", "method": "TwinSegNet结合了卷积编码器和视觉变换瓶颈来捕获局部和全局上下文，并通过个性化数字孪生在各机构内对其私有数据进行微调以形成其独特模型。该框架利用混合的ViT-UNet模型，实现了准确的大脑肿瘤分割。", "result": "TwinSegNet在包括BraTS 2019-2021在内的九个异构MRI数据集上进行了评估，Dice分数达到最高达0.90%，并且敏感性和特异性超过90%。该模型表现出了强大的非独立同分布（non-IID）机构间分配的鲁棒性。", "conclusion": "TwinSegNet提供了一种在多机构临床环境中实现大规模个性化分割的方法，同时遵守严格的数据保密要求，并且在保护隐私的同时不牺牲性能。"}}
{"id": "2512.17474", "pdf": "https://arxiv.org/pdf/2512.17474", "abs": "https://arxiv.org/abs/2512.17474", "authors": ["Ali Alsayegh", "Tariq Masood"], "title": "Zero-Shot Recognition of Dysarthric Speech Using Commercial Automatic Speech Recognition and Multimodal Large Language Models", "categories": ["eess.AS", "cs.SD"], "comment": null, "summary": "Voice-based human-machine interaction is a primary modality for accessing intelligent systems, yet individuals with dysarthria face systematic exclusion due to recognition performance gaps. Whilst automatic speech recognition (ASR) achieves word error rates (WER) below 5% on typical speech, performance degrades dramatically for dysarthric speakers. Multimodal large language models (MLLMs) offer potential for leveraging contextual reasoning to compensate for acoustic degradation, yet their zero-shot capabilities remain uncharacterised. This study evaluates eight commercial speech-to-text services on the TORGO dysarthric speech corpus: four conventional ASR systems (AssemblyAI, Whisper large-v3, Deepgram Nova-3, Nova-3 Medical) and four MLLM-based systems (GPT-4o, GPT-4o Mini, Gemini 2.5 Pro, Gemini 2.5 Flash). Evaluation encompasses lexical accuracy, semantic preservation, and cost-latency trade-offs. Results demonstrate severity-dependent degradation: mild dysarthria achieves 3-5% WER approaching typical-speech benchmarks, whilst severe dysarthria exceeds 49% WER across all systems. A verbatim-transcription prompt yields architecture-specific effects: GPT-4o achieves 7.36 percentage point WER reduction with consistent improvement across all tested speakers, whilst Gemini variants exhibit degradation. Semantic metrics indicate that communicative intent remains partially recoverable despite elevated lexical error rates. These findings establish empirical baselines enabling evidence-based technology selection for assistive voice interface deployment.", "AI": {"tldr": "本文研究了使用商用自动语音识别系统和多模态大型语言模型在TORGO语料库上对非典型言语（即患有失语症的人）进行零样本识别的表现。", "motivation": "当前的自动语音识别技术在处理典型的普通话语音时表现出色，但对于具有失语症状的用户来说，其准确性大大降低。为了填补这一空白并为辅助语音接口提供技术依据，本文评估了不同商用系统在这方面的性能。", "method": "研究使用八个不同的商用声纹到文本转换服务（包括四个传统的自动语音识别系统和四个基于多模态大型语言模型的系统）在TORGO语料库上进行测试，并从词汇准确度、语义保留和成本-延迟权衡等方面对它们的表现进行了评估。", "result": "研究结果表明，对于轻度失语症患者，错误率可降至3%-5%，接近普通话语音的标准；而对于重度失语症者，则所有系统都超过了49%的错误率。此外，在使用逐字转录提示的情况下，GPT-4o模型能够实现一致性的性能提升，而Gemini变体则表现较差。", "conclusion": "研究为辅助语音接口技术的选择提供了实证基准，并强调了多模态大型语言模型在处理失语症患者方面具有潜在的应用价值。"}}
{"id": "2512.17470", "pdf": "https://arxiv.org/pdf/2512.17470", "abs": "https://arxiv.org/abs/2512.17470", "authors": ["Dennis Gross", "Jørn Eirik Betten", "Helge Spieker"], "title": "Translating the Rashomon Effect to Sequential Decision-Making Tasks", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The Rashomon effect describes the phenomenon where multiple models trained on the same data produce identical predictions while differing in which features they rely on internally. This effect has been studied extensively in classification tasks, but not in sequential decision-making, where an agent learns a policy to achieve an objective by taking actions in an environment. In this paper, we translate the Rashomon effect to sequential decision-making. We define it as multiple policies that exhibit identical behavior, visiting the same states and selecting the same actions, while differing in their internal structure, such as feature attributions. Verifying identical behavior in sequential decision-making differs from classification. In classification, predictions can be directly compared to ground-truth labels. In sequential decision-making with stochastic transitions, the same policy may succeed or fail on any single trajectory due to randomness. We address this using formal verification methods that construct and compare the complete probabilistic behavior of each policy in the environment. Our experiments demonstrate that the Rashomon effect exists in sequential decision-making. We further show that ensembles constructed from the Rashomon set exhibit greater robustness to distribution shifts than individual policies. Additionally, permissive policies derived from the Rashomon set reduce computational requirements for verification while maintaining optimal performance.", "AI": {"tldr": "该论文将Rashomon效应从分类任务拓展到了顺序决策任务，定义了多个在环境中表现出相同行为的策略集，并通过形式验证方法证明其存在。", "motivation": "现有研究主要针对分类任务中的Rashomon效应，缺乏对顺序决策任务中此现象的研究。作者希望通过探索这种效应，能够提高模型的鲁棒性并减少计算需求。", "method": "定义了在相同环境中行为一致但内部结构不同的策略集；采用形式验证技术来比较各策略在环境中的完整概率行为。", "result": "实验表明Rashomon效应存在于顺序决策任务中，并且从该集合构建的模型更具分布迁移的鲁棒性，同时可以通过推导出的宽容策略减少计算需求。", "conclusion": "论文成功地将Rashomon效应拓展到了顺序决策领域，并展示了其在提高模型性能和降低计算成本方面的潜在价值。"}}
{"id": "2512.17462", "pdf": "https://arxiv.org/pdf/2512.17462", "abs": "https://arxiv.org/abs/2512.17462", "authors": ["Olivier Jeunen", "Schaun Wheeler"], "title": "Behavioural Effects of Agentic Messaging: A Case Study on a Financial Service Application", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": "To appear in the 48th European Conference on Information Retrieval (ECIR '26) Industry Track", "summary": "Marketing and product personalisation provide a prominent and visible use-case for the application of Information Retrieval methods across several business domains. Recently, agentic approaches to these problems have been gaining traction. This work evaluates the behavioural and retention effects of agentic personalisation on a financial service application's customer communication system during a 2025 national tax filing period. Through a two month-long randomised controlled trial, we compare an agentic messaging approach against a business-as-usual (BAU) rule-based campaign system, focusing on two primary outcomes: unsubscribe behaviour and conversion timing. Empirical results show that agent-led messaging reduced unsubscribe events by 21\\% ($\\pm 0.01$) relative to BAU and increased early filing behaviour in the weeks preceding the national deadline. These findings demonstrate how adaptive, user-level decision-making systems can modulate engagement intensity whilst improving long-term retention indicators.", "AI": {"tldr": "研究评估了代理个性化消息对金融应用用户行为和留存的影响。", "motivation": "探讨代理式个人化通信系统在提高用户体验、增加参与度与延长用户留存方面的潜力。", "method": "通过随机对照试验，将代理式消息方法与常规的规则基础系统进行比较，重点关注两种主要结果：取消订阅行为及转换时间。", "result": "结果显示，代理引导的消息减少了21%的退订事件，并促进了提前申报的行为。", "conclusion": "证明了适应性用户级决策系统的使用可以调节参与强度并改善长期留存指标。"}}
{"id": "2512.17461", "pdf": "https://arxiv.org/pdf/2512.17461", "abs": "https://arxiv.org/abs/2512.17461", "authors": ["Evangelos Pournaras"], "title": "Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This article shows how fair voting methods can be a catalyst for change in the way we make collective decisions, and how such change can promote long-awaited upgrades of democracy. Based on real-world evidence from democratic innovations in participatory budgeting, in Switzerland and beyond, I highlight a trilogy of key research results: Fair voting methods achieve to be (i) legitimacy incubator, (ii) novel impact accelerator and (iii) safeguard for risks of artificial intelligence (AI). Compared to majoritarian voting methods, combining expressive ballot formats (e.g. cumulative voting) with ballot aggregation methods that promote proportional representation (e.g. equal shares) results in more winners and higher (geographical) representation of citizens. Such fair voting methods are preferred and found fairer even by voters who do not win, while promoting stronger democratic values for citizens such as altruism and compromise. They also result in new resourceful ideas to put for voting, which are cost-effective and win, especially in areas of welfare, education and culture. Strikingly, fair voting methods are also more resilient to biases and inconsistencies of generative AI in emerging scenarios of AI voting assistance or AI representation of voters who would be likely to abstain. I also review the relevance of such upgrades for democracies in crisis, such as the one of Greece featured in the recent study of `Unmute Democracy'. Greek democracy can build stronger resilience via higher representation of citizens in democratic processes as well as democratic innovations in participation. Fair voting methods can be a catalyst for both endeavors.", "AI": {"tldr": "研究探讨了公平投票方法如何促进民主升级，特别是在参与预算、瑞士以及其他地区的民主创新中的应用。", "motivation": "通过展现公平投票方法在提升民主合法性、推动新颖影响加速器以及保障人工智能风险方面的潜力，旨在推进民主改革和增强民主韧性。", "method": "基于真实世界证据，结合累积投票等表达性投票格式与促进比例代表的选票汇总方法进行研究。同时探讨了这些方法在AI辅助投票中的表现。", "result": "公平投票方式相较于多数制投票方法能够产生更多赢家，并更好地实现公民地理代表性。这种方法甚至被未获胜利的选民认为更加公正，且促进了更强的民主价值观和新创意的有效采纳。", "conclusion": "公平投票方法可以作为催化剂推动民主升级与增强韧性。对于希腊等处于危机中的民主国家而言，通过提高公民在决策过程中的代表性和引入参与式创新，能够实现更强大的民主韧性。"}}
{"id": "2512.17459", "pdf": "https://arxiv.org/pdf/2512.17459", "abs": "https://arxiv.org/abs/2512.17459", "authors": ["Tobias Sautter", "Jan-Niklas Dihlmann", "Hendrik P. A. Lensch"], "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework", "categories": ["cs.CV"], "comment": "Project Page: https://3dregen.jdihlmann.com/", "summary": "Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements. Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.", "AI": {"tldr": "该论文提出了一种用于从单张图像生成室内场景的三维重建框架。", "motivation": "当前的三维场景生成方法存在对象分解不正确、空间关系不准确和背景缺失等问题，难以满足艺术家的需求。因此，研究者提出了一个能解决这些问题的新框架。", "method": "提出了一种结合多种模型（如资产检测、重构和放置）的组合性框架来处理从单张图像到三维纹理场景的重建任务，并使用生成模型来进行遮挡物体的推理与重构以实现场景级别的合理性。此外，还引入了基于地面平面估计的四自由度可微优化算法。", "result": "3D-RE-GEN在单一图像的室内场景三维重建中达到了最先进的性能，生成的结果是连贯且可以修改的，并为视觉效果和游戏开发提供了现实主义光照的基础。", "conclusion": "通过提出一种新的组合性框架3D-RE-GEN，在单张图像到三维纹理场景的重建任务上实现了更准确的对象分解、更好的空间关系以及完整的背景。"}}
{"id": "2512.17453", "pdf": "https://arxiv.org/pdf/2512.17453", "abs": "https://arxiv.org/abs/2512.17453", "authors": ["Henok Tenaw Moges", "Deshendran Moodley"], "title": "A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 5 figures, 2 tables. Accepted for presentation at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain", "summary": "We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.", "AI": {"tldr": "本文提出了一种轻量级的空间时间图神经网络（Lite-STGNN）用于长期多变量时间序列预测。", "motivation": "为了提高长期多变量时间序列预测的准确性，同时减少计算成本和训练时间。", "method": "该方法结合了基于分解的时间建模与可学习稀疏图结构的空间模块。时间模块采用趋势季节性分解，空间模块则通过低秩Top-K邻接矩阵和保守时域门控进行消息传递。", "result": "实验结果显示，在四个基准数据集上，Lite-STGNN在长达720步的预测中达到了最先进的准确性，并且比基于Transformer的方法更参数高效、训练速度更快。", "conclusion": "Lite-STGNN提供了一个紧凑、可解释和高效的框架，适用于长期多变量时间序列预测。"}}
{"id": "2512.17452", "pdf": "https://arxiv.org/pdf/2512.17452", "abs": "https://arxiv.org/abs/2512.17452", "authors": ["Yen-Chieh Huang", "Rui Fang", "Ming-Syan Chen", "Pi-Cheng Hsiu"], "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .", "AI": {"tldr": "该论文提出了一种称为Write-Gated KV的机制，用于在长上下文语言模型推理中优化关键值缓存管理。", "motivation": "当前长上下文LLM推理受限于二次注意力复杂度和线性KV缓存增长。现有方法通过后处理选择或驱逐来缓解问题，但忽视了根本低效：无差别地将数据写入持久内存。", "method": "该论文形式化了KV缓存管理为三个基本操作的因果系统：KV Admission、Selection和Eviction。Write-Gated KV是一种轻量级机制，它通过预测token的效用来学习何时将其写入缓存，从而过滤掉低效状态以保持紧凑的全局缓存。", "result": "与Llama模型相比，Write-Gated KV将内存使用减少46-57%，在prefill和decode阶段分别提供3.03-3.45倍和1.89-2.56倍的速度提升，并且几乎不影响准确性。同时兼容FlashAttention和paged-KV系统。", "conclusion": "学习何时写入是实现高效长上下文推理的一种合理且实用的方法。"}}
{"id": "2512.17450", "pdf": "https://arxiv.org/pdf/2512.17450", "abs": "https://arxiv.org/abs/2512.17450", "authors": ["Jon Muhovič", "Janez Perš"], "title": "MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.", "AI": {"tldr": "提出了一种新型的多模态海洋数据集MULTIAQUA，用于开发能够在各种条件下提取有用信息的方法。", "motivation": "无人水面车辆在操作中会遇到多种视觉情况，一些情况下仅使用彩色相机图像无法解决所有问题。为了改善这些状况下的场景解释质量，引入了MULTIAQUA数据集。", "method": "数据集中包含同步、校准和注释的多模态传感器采集的数据，并展示了几种多模态方法在夜间测试集上的表现。提出了训练策略以增强模型在近完全黑暗条件下的性能。", "result": "提出的方法可以在仅使用白天图像的情况下，通过简化数据获取与标注过程来提高多模态模型的稳健性。", "conclusion": "MULTIAQUA数据集及其配套的训练方法为改善无人水面车辆在复杂海洋环境中的视觉感知能力提供了有力支持。"}}
{"id": "2512.17446", "pdf": "https://arxiv.org/pdf/2512.17446", "abs": "https://arxiv.org/abs/2512.17446", "authors": ["Chunggi Lee", "Ut Gong", "Tica Lin", "Stefanie Zollmann", "Scott A Epsley", "Adam Petway", "Hanspeter Pfister"], "title": "VAIR: Visual Analytics for Injury Risk Exploration in Sports", "categories": ["cs.HC"], "comment": null, "summary": "Injury prevention in sports requires understanding how bio-mechanical risks emerge from movement patterns captured in real-world scenarios. However, identifying and interpreting injury prone events from raw video remains difficult and time-consuming. We present VAIR, a visual analytics system that supports injury risk analysis using 3D human motion reconstructed from sports video. VAIR combines pose estimation, bio-mechanical simulation, and synchronized visualizations to help users explore how joint-level risk indicators evolve over time. Domain experts can inspect movement segments through temporally aligned joint angles, angular velocity, and internal forces to detect patterns associated with known injury mechanisms. Through case studies involving Achilles tendon and Anterior cruciate ligament (ACL) injuries in basketball, we show that VAIR enables more efficient identification and interpretation of risky movements. Expert feedback confirms that VAIR improves diagnostic reasoning and supports both retrospective analysis and proactive intervention planning.", "AI": {"tldr": "该论文提出了一种名为VAIR的视觉分析系统，用于体育活动中受伤风险的探索。", "motivation": "在运动中预防伤害需要理解生物力学风险如何从实际场景中的动作模式中产生。然而，从原始视频中识别和解释潜在的伤害事件仍然困难且耗时。", "method": "VAIR结合了姿态估计、生物力学模拟和同步可视化技术来帮助用户探索随时间变化的关节层面的风险指标。", "result": "通过篮球运动员跟腱和前交叉韧带(ACL)损伤案例研究，结果表明VAIR能够更有效地识别和解释危险动作，并得到领域专家的认可。", "conclusion": "VAIR改进了诊断推理能力，支持对过往分析以及预防性干预规划。"}}
{"id": "2512.17445", "pdf": "https://arxiv.org/pdf/2512.17445", "abs": "https://arxiv.org/abs/2512.17445", "authors": ["Yun He", "Francesco Pittaluga", "Ziyu Jiang", "Matthias Zwicker", "Manmohan Chandraker", "Zaid Tasneem"], "title": "LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents", "categories": ["cs.CV"], "comment": "Project Page: https://yunhe24.github.io/langdrivectrl/", "summary": "LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.", "AI": {"tldr": "LangDriveCTRL是一个基于自然语言控制的框架，用于编辑现实世界的驾驶视频以合成多样化的交通场景。", "motivation": "为了创造更加真实和细致的驾驶环境编辑效果，提出了一个结合了显式三维场景分解与多代理系统的框架。", "method": "该系统利用场景图表示驾驶视频，并通过三个专门化代理人协同工作实现细粒度编辑：对象定位代理人根据自然语言描述确定目标物体节点；行为编辑代理人生成多个对象的轨迹；行为审查代理人迭代地检查并改进生成的轨迹。最后，使用视频扩散工具处理插入对象和显著视角变化引入的伪影。", "result": "定量分析显示，LangDriveCTRL在指令对齐方面接近于目前的最佳技术水平（SoTA）的两倍，并且在结构保持、照片逼真度以及交通现实感方面表现更优。", "conclusion": "LangDriveCTRL不仅支持单个物体节点编辑，还能从单一自然语言命令中执行多对象行为编辑，展示出强大的灵活性和适应性。"}}
{"id": "2512.17444", "pdf": "https://arxiv.org/pdf/2512.17444", "abs": "https://arxiv.org/abs/2512.17444", "authors": ["Javier Gonzalez-Ruiz", "Carlos Rodriguez-Pardo", "Iacopo Savelli", "Alice Di Bella", "Massimo Tavoni"], "title": "Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.NE", "econ.GN"], "comment": "Accepted to Energy and AI. Code available in https://github.com/jjgonzalez2491/MARLEY_V1", "summary": "Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.", "AI": {"tldr": "本论文提出了一种多智能体强化学习模型，用于评估长期电力市场设计对于实现碳减排目标的影响。", "motivation": "在向零排放经济转型的过程中，需要更先进的工具来帮助政策制定者和利益相关者设计、测试和评估长期的电力市场机制。现有的工具无法有效应对这一挑战。", "method": "论文采用了独立近端策略优化算法来模拟发电公司在批发市场中的投资决策行为，并通过广泛的超参数搜索确保模型在多智能体环境下的训练结果符合竞争性假设。", "result": "该模型应用于意大利电力系统的简化版本，展示了市场设计对于实现碳减排目标的重要性以及如何避免价格波动。", "conclusion": "提出的框架能够评估长期的电力市场，在这种市场上多个政策和市场机制同时起作用，并且参与者会根据脱碳路径进行响应和适应。"}}
{"id": "2512.17442", "pdf": "https://arxiv.org/pdf/2512.17442", "abs": "https://arxiv.org/abs/2512.17442", "authors": ["Jan Hutter", "Hua Chang Bakker", "Stan Fris", "Madelon Bernardy", "Yuanna Liu"], "title": "A Systematic Reproducibility Study of BSARec for Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "Jan Hutter, Hua Chang Bakker, Stan Fris, Madelon Bernardy contributed equally to this work", "summary": "In sequential recommendation (SR), the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests. To overcome this, BSARec augments the Transformer encoder with a frequency layer that rescales high-frequency components using the Fourier transform. However, the overall effectiveness of BSARec and the roles of its individual components have yet to be systematically validated. We reproduce BSARec and show that it outperforms other SR methods on some datasets. To empirically assess whether BSARec improves performance on high-frequency signals, we propose a metric to quantify user history frequency and evaluate SR methods across different user groups. We compare digital signal processing (DSP) techniques and find that the discrete wavelet transform (DWT) offer only slight improvements over Fourier transforms, and DSP methods provide no clear advantage over simple residual connections. Finally, we explore padding strategies and find that non-constant padding significantly improves recommendation performance, whereas constant padding hinders the frequency rescaler's ability to capture high-frequency signals.", "AI": {"tldr": "该论文系统地验证了BSARec模型在顺序推荐中的有效性，并探讨了其各个组件的作用。", "motivation": "Transformer模型的自我注意机制作为低通滤波器，限制了它捕捉短时用户兴趣的能力。为解决这个问题，BSARec引入频率层来提升高频信号捕获能力，但整体有效性和各部分作用需要进一步验证。", "method": "论文通过复现BSARec，并设计了一个衡量用户历史频率的指标来评估推荐方法在不同用户群体中的表现；对比了数字信号处理技术的效果；探讨了填充策略对模型性能的影响。", "result": "BSARec优于其他顺序推荐方法，特别是在某些数据集上。离散小波变换仅提供微弱改进，并不优于简单的残差连接。非常数填充策略显著提高推荐效果。", "conclusion": "该研究确认了BSARec在顺序推荐中的优越性，并指出频率层和填充策略对其性能的重要性。"}}
{"id": "2512.17436", "pdf": "https://arxiv.org/pdf/2512.17436", "abs": "https://arxiv.org/abs/2512.17436", "authors": ["Jiaze Li", "Jingyang Chen", "Yuxun Qu", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Shijie Xu", "Zhenru Lin", "Junyou Zhu", "Boshen Xu", "Wenhui Tan", "Pei Fu"], "title": "Xiaomi MiMo-VL-Miloco Technical Report", "categories": ["cs.CV"], "comment": null, "summary": "We open-source \\textbf{MiMo-VL-Miloco-7B} and its quantized variant \\textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \\href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.", "AI": {"tldr": "本论文介绍了小米MiMo-VL-Miloco模型及其在智能家居环境中的应用，该模型在手势识别、场景理解以及多模态推理方面表现出色。", "motivation": "为了提升智能家庭环境中视觉语言模型的理解能力和多模态推理能力，研究者开发了专门针对家居场景的MiMo-VL-Miloco-7B模型，并通过多种实验验证其性能优于现有的封闭源代码和开源基线。", "method": "该方法基于MiMo-VL-7B骨干网络构建了一个两阶段训练流程。首先采用监督微调，然后利用基于相对策略优化的强化学习进行进一步调整。此外还引入了链式思考监督以及预算感知推理，使模型在数据有效性的前提下高效地完成任务。", "result": "实验结果显示MiMo-VL-Miloco-7B在手势识别、家居场景理解和多个视频基准测试中均取得领先F1分数，并且比其他基线表现出色。", "conclusion": "通过目标化的家庭场景训练，不仅能增强活动和手势的理解能力，还能有效地进行文本推理并适度优化文档相关任务的性能。该模型及其工具包已公开发布以支持研究与实际部署。"}}
{"id": "2512.17435", "pdf": "https://arxiv.org/pdf/2512.17435", "abs": "https://arxiv.org/abs/2512.17435", "authors": ["Teng Wang", "Xinxin Zhao", "Wenzhe Cai", "Changyin Sun"], "title": "ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination", "categories": ["cs.RO"], "comment": "17 pages, 10 figures. arXiv admin note: text overlap with arXiv:2410.09874", "summary": "Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.", "AI": {"tldr": "本文提出了一种基于视觉语言模型的无地图导航框架ImagineNav++，通过场景想象和选择性焦点记忆机制实现高效的自主导航。", "motivation": "当前方法利用大型语言模型增强探索效率但受限于文本表示。作者旨在探讨视觉语言模型是否能通过仅使用RGB/RGB-D流进行无地图导航，并提出了一种基于场景想象的方法来改善空间感知与规划。", "method": "ImagineNav++包括未来视角想象模块和选择性焦点记忆机制，前者生成具有高探索潜力的语义有意义视点；后者则通过稀疏到密集框架整合关键帧观察结果构建长期空间推理记忆。", "result": "在开放词汇物体导航基准上，ImagineNav++取得了无地图设置下的SOTA性能，并且超过了大多数基于地图的方法。", "conclusion": "研究表明，场景想象和选择性焦点记忆机制对于视觉语言模型中的空间推理至关重要。"}}
{"id": "2512.17432", "pdf": "https://arxiv.org/pdf/2512.17432", "abs": "https://arxiv.org/abs/2512.17432", "authors": ["Georgios Simantiris", "Konstantinos Bacharidis", "Apostolos Papanikolaou", "Petros Giannakakis", "Costas Panagiotakis"], "title": "AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments", "categories": ["cs.CV"], "comment": "36 pages, 19 figures, 8 tables", "summary": "Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.", "AI": {"tldr": "构建了一个用于洪水环境语义分割和理解的全球航拍图像数据集AIFloodSense", "motivation": "现有洪水分割数据集地理范围有限，标注细节不足，难以开发稳健且通用的计算机视觉方法。本文旨在填补这一空白，提供一个全面、多样化的数据集以支持灾害响应和风险评估技术的发展", "method": "创建了一个包含470张高分辨率图像的数据集，涵盖230次不同洪水事件及六大陆的不同环境，提供了三项任务：图像分类（含环境类型识别等子任务）、语义分割以及视觉问答。使用最新架构建立了基准模型以展示数据集的复杂性和价值", "result": "展示了AIFloodSense在推进领域通用AI工具方面的潜力和重要性", "conclusion": "AIFloodSense为洪水检测和理解提供了丰富的资源，有助于开发更具适应性的灾难响应技术"}}
{"id": "2512.17425", "pdf": "https://arxiv.org/pdf/2512.17425", "abs": "https://arxiv.org/abs/2512.17425", "authors": ["Beatrice Luciani", "Katherine Lin Poggensee", "Heike Vallery", "Alex van den Berg", "Severin David Woernle", "Mostafa Mogharabi", "Stefano Dalla Gasperina", "Laura Marchal-Crespo"], "title": "Personalized Gait Patterns During Exoskeleton-Aided Training May Have Minimal Effect on User Experience. Insights from a Pilot Study", "categories": ["cs.RO"], "comment": null, "summary": "Robot-aided gait rehabilitation facilitates high-intensity and repeatable therapy. However, most exoskeletons rely on pre-recorded, non-personalized gait trajectories constrained to the sagittal plane, potentially limiting movement naturalness and user comfort. We present a data-driven gait personalization framework for an exoskeleton that supports multi-planar motion, including hip abduction/adduction and pelvic translation and rotation. Personalized trajectories to individual participants were generated using regression models trained on anthropometric, demographic, and walking speed data from a normative database. In a within-subject experiment involving ten unimpaired participants, these personalized trajectories were evaluated in regard to comfort, naturalness, and overall experience and compared against two standard patterns from the same database: one averaging all the trajectories, and one randomly selected. We did not find relevant differences across pattern conditions, despite all trajectories being executed with high accuracy thanks to a stiff position-derivative controller. We found, however, that pattern conditions in later trials were rated as more comfortable and natural than those in the first trial, suggesting that participants might have adapted to walking within the exoskeleton, regardless of the enforced gait pattern. Our findings highlight the importance of integrating subjective feedback when designing personalized gait controllers and accounting for user adaptation during experimentation.", "AI": {"tldr": "本文提出了一种基于数据的个性化步态框架，用于支持多平面运动的外骨骼，并通过实验评估了个性化步态模式对用户体验的影响。", "motivation": "目前大多数外骨骼依靠预录的非个性化的步态轨迹，可能限制自然性和用户舒适度。因此研究者希望通过个性化步态来改善用户体验。", "method": "使用回归模型基于参与者的人体测量、人口统计和行走速度数据从规范数据库生成个性化轨迹，并对十名无损参与者的个性化步态模式进行评估，与来自同一数据库的两种标准模式比较。", "result": "实验发现不同步态模式之间没有显著差异。但是，在后续试验中，所有条件下的步态模式被评价为更舒适、自然。", "conclusion": "研究结果表明，设计个性化步态控制器时整合主观反馈，并在实验期间考虑用户适应性非常重要。"}}
{"id": "2512.17419", "pdf": "https://arxiv.org/pdf/2512.17419", "abs": "https://arxiv.org/abs/2512.17419", "authors": ["Lilin Wang", "Lucas Ramalho", "Alan Celestino", "Phuc Anthony Pham", "Yu Liu", "Umang Kumar Sinha", "Andres Portillo", "Onassis Osunwa", "Gabriel Maduekwe"], "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.", "AI": {"tldr": "SWE-Bench++ 是一个从开源仓库生成可扩展软件工程基准的自动化框架。", "motivation": "现有的基准测试如 SWE-bench 受限于手动整理、静态数据集和 Python 虫修复任务，本研究旨在通过自动收集 GitHub 拉取请求来构建覆盖多语言和多样任务类型的开放源码基准。", "method": "SWE-Bench++ 包括四个阶段：程序化获取、环境合成、测试预言提取以及质量保证。最后一步是将强模型失败的实例转换为训练轨迹。", "result": "在初始基准中，包含来自3971个仓库的11133个实例，覆盖了11种语言。在其中一部分数据上的表现显示，最强的语言模型通过率在16.89%到36.20%之间不等。", "conclusion": "SWE-Bench++ 提供了一个可扩展、多语言的基准测试工具，用于评估和改进仓库级别的代码生成任务。"}}
{"id": "2512.17416", "pdf": "https://arxiv.org/pdf/2512.17416", "abs": "https://arxiv.org/abs/2512.17416", "authors": ["Martin Krebs", "Jan Obdržálek", "Vít Musil", "Tomáš Brázdil"], "title": "Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification", "categories": ["cs.CV"], "comment": "ef:2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)", "summary": "Deep neural networks are starting to show their worth in critical applications such as assisted cancer diagnosis. However, for their outputs to get accepted in practice, the results they provide should be explainable in a way easily understood by pathologists. A well-known and widely used explanation technique is occlusion, which, however, can take a long time to compute, thus slowing the development and interaction with pathologists. In this work, we set out to find a faster replacement for occlusion in a successful system for detecting prostate cancer. Since there is no established framework for comparing the performance of various explanation methods, we first identified suitable comparison criteria and selected corresponding metrics. Based on the results, we were able to choose a different explanation method, which cut the previously required explanation time at least by a factor of 10, without any negative impact on the quality of outputs. This speedup enables rapid iteration in model development and debugging and brings us closer to adopting AI-assisted prostate cancer detection in clinical settings. We propose that our approach to finding the replacement for occlusion can be used to evaluate candidate methods in other related applications.", "AI": {"tldr": "本文旨在寻找一种更快的替代方法来解释基于CNN的前列腺癌分类模型，从而加速开发和临床应用。", "motivation": "为了使深度学习在辅助癌症诊断中的结果被病理学家接受，需要提供易于理解的结果。然而，现有的遮挡技术计算时间长，影响了开发效率。", "method": "首先确定合适的比较标准，并选择相应的指标来评估各种解释方法的性能，最终选择了另一种解释方法，该方法将所需解释时间至少缩短了一个数量级。", "result": "新方法显著减少了所需的时间，并且没有对输出质量产生负面影响，促进了模型开发和调试过程中的快速迭代。", "conclusion": "本研究提出的方法可以用于评估其他相关应用中候选的替代遮挡技术。"}}
{"id": "2512.17412", "pdf": "https://arxiv.org/pdf/2512.17412", "abs": "https://arxiv.org/abs/2512.17412", "authors": ["Neil Urquhart", "Amir Rahimi", "Efstathios-Al. Tingas"], "title": "Optimisation of Aircraft Maintenance Schedules", "categories": ["cs.NE", "cs.AI"], "comment": "ef:In E. Hart, T. Horvath, Z. Tan, & S. Thomson (Eds.), Advances in Computational Intelligence Systems. Springer 2025", "summary": "We present an aircraft maintenance scheduling problem, which requires suitably qualified staff to be assigned to maintenance tasks on each aircraft. The tasks on each aircraft must be completed within a given turn around window so that the aircraft may resume revenue earning service. This paper presents an initial study based on the application of an Evolutionary Algorithm to the problem. Evolutionary Algorithms evolve a solution to a problem by evaluating many possible solutions, focusing the search on those solutions that are of a higher quality, as defined by a fitness function. In this paper, we benchmark the algorithm on 60 generated problem instances to demonstrate the underlying representation and associated genetic operators.", "AI": {"tldr": "该论文提出了利用进化算法优化飞机维护计划的问题。", "motivation": "通过合理安排具备资质的工作人员在规定的周转时间内完成每架飞机上的维护任务，以使飞机尽快恢复运营服务。", "method": "采用进化算法对生成的60个问题实例进行基准测试，展示其底层表示和相关遗传算子。", "result": "展示了基于进化算法优化飞机维护计划的结果。", "conclusion": "证明了该方法在解决飞机维护调度问题上的可行性和有效性。"}}
{"id": "2512.17411", "pdf": "https://arxiv.org/pdf/2512.17411", "abs": "https://arxiv.org/abs/2512.17411", "authors": ["Xingyu Feng"], "title": "Detection and Analysis of Sensitive and Illegal Content on the Ethereum Blockchain Using Machine Learning Techniques", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Blockchain technology, lauded for its transparent and immutable nature, introduces a novel trust model. However, its decentralized structure raises concerns about potential inclusion of malicious or illegal content. This study focuses on Ethereum, presenting a data identification and restoration algorithm. Successfully recovering 175 common files, 296 images, and 91,206 texts, we employed the FastText algorithm for sentiment analysis, achieving a 0.9 accuracy after parameter tuning. Classification revealed 70,189 neutral, 5,208 positive, and 15,810 negative texts, aiding in identifying sensitive or illicit information. Leveraging the NSFWJS library, we detected seven indecent images with 100% accuracy. Our findings expose the coexistence of benign and harmful content on the Ethereum blockchain, including personal data, explicit images, divisive language, and racial discrimination. Notably, sensitive information targeted Chinese government officials. Proposing preventative measures, our study offers valuable insights for public comprehension of blockchain technology and regulatory agency guidance. The algorithms employed present innovative solutions to address blockchain data privacy and security concerns.", "AI": {"tldr": "本文研究利用机器学习技术在以太坊区块链上检测和分析敏感及非法内容。", "motivation": "鉴于区块链的透明性和不可篡改性，其去中心化的结构引发了对潜在恶意或非法内容存在的担忧。因此，本研究旨在通过数据识别和恢复算法来发现并处理这些内容。", "method": "采用FastText算法进行文本情感分析，利用NSFWJS库检测不适当图片，并成功恢复了175个常见文件、296张图片和91,206条文本。经过参数调整后，FastText算法的准确率为0.9。", "result": "通过分类发现共有70,189条中性文本、5,208条正面文本和15,810条负面文本，并且检测到了七张不适当的图片。这些发现揭示了以太坊区块链上共存的有益与有害内容，包括个人数据、露骨图像、煽动性的语言及种族歧视。", "conclusion": "本研究不仅提出了针对区块链中敏感和非法信息处理的有效方法，还为公众理解区块链技术以及监管机构制定指导方针提供了有价值的见解。"}}
{"id": "2512.17396", "pdf": "https://arxiv.org/pdf/2512.17396", "abs": "https://arxiv.org/abs/2512.17396", "authors": ["Léo Butsanets", "Charles Corbière", "Julien Khlaut", "Pierre Manceron", "Corentin Dancette"], "title": "RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint, 23 pages, 12 figures, 7 tables", "summary": "In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.", "AI": {"tldr": "本文介绍了RadImageNet-VQA，这是一个用于放射学视觉问答（VQA）的大型CT和MRI数据集。", "motivation": "当前医疗领域的VQA数据集规模有限，主要集中在X光成像或生物医学插图上，并且容易出现基于文本的捷径。为此，本文提出了RadImageNet-VQA，以解决这些问题并推动放射学视觉问答的发展。", "method": "RadImageNet-VQA由专家精心策划的注释构建而成，包含750K图像配对和7.5M问题-答案样本。该数据集涵盖了三个关键任务：异常检测、解剖结构识别和病理诊断，并覆盖了八个解剖区域及97种病理类别。", "result": "实验表明，最先进的视觉语言模型在细粒度的病理学辨识方面仍然存在困难，特别是在开放问答设置中并且即使经过微调也是如此。此外，纯文本分析显示，在没有图像输入的情况下，这些模型的表现接近随机水平，这证明RadImageNet-VQA不受基于语言的捷径的影响。", "conclusion": "该数据集及其基准测试是公开可用的，并且能够推动放射学视觉问答的研究进展。"}}
{"id": "2512.17394", "pdf": "https://arxiv.org/pdf/2512.17394", "abs": "https://arxiv.org/abs/2512.17394", "authors": ["Zabir Al Nazi", "G M Shahariar", "Abrar Hossain", "Wei Peng"], "title": "Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?", "categories": ["cs.CL", "cs.CV", "cs.CY"], "comment": null, "summary": "Theory of Mind (ToM) -- the ability to attribute beliefs, desires, and emotions to others -- is fundamental for human social intelligence, yet remains a major challenge for artificial agents. Existing Vision-Language Models (VLMs) are increasingly applied in socially grounded tasks, but their capacity for cross-cultural ToM reasoning is largely unexplored. In this work, we introduce CulturalToM-VQA, a new evaluation benchmark containing 5095 questions designed to probe ToM reasoning across diverse cultural contexts through visual question answering. The dataset captures culturally grounded cues such as rituals, attire, gestures, and interpersonal dynamics, enabling systematic evaluation of ToM reasoning beyond Western-centric benchmarks. Our dataset is built through a VLM-assisted human-in-the-loop pipeline, where human experts first curate culturally rich images across traditions, rituals, and social interactions; a VLM then assist in generating structured ToM-focused scene descriptions, which are refined into question-answer pairs spanning a taxonomy of six ToM tasks and four graded complexity levels. The resulting dataset covers diverse theory of mind facets such as mental state attribution, false belief reasoning, non-literal communication, social norm violations, perspective coordination, and multi-agent reasoning.", "AI": {"tldr": "介绍了一种新的评估基准CulturalToM-VQA，用于探究视觉语言模型在跨文化理论思维推理方面的能力。", "motivation": "当前的视觉语言模型虽然在社会相关任务中得到广泛应用，但它们的跨文化理论思维推理能力尚未被充分研究。本工作旨在填补这一空白，并建立一个更广泛的评估标准以测试不同文化背景下的理论思维推理能力。", "method": "通过人类专家和视觉语言模型共同协作的方式创建了包含5095个问题的新数据集CulturalToM-VQA，这些问题涵盖了各种跨文化的线索以及六种不同的理论思维任务类型和四个渐进的复杂度级别。", "result": "新建立的数据集覆盖了从简单的心理状态归因到复杂的多代理推理等不同方面的理论思维能力。", "conclusion": "通过CulturalToM-VQA数据集，可以全面评估视觉语言模型在跨文化背景下的理论思维推理能力，并且能够推动该领域的进一步研究和发展。"}}
{"id": "2512.17390", "pdf": "https://arxiv.org/pdf/2512.17390", "abs": "https://arxiv.org/abs/2512.17390", "authors": ["Ezgi Dağtekin", "Ercan Erkalkan"], "title": "Psychological Factors Influencing University Students Trust in AI-Based Learning Assistants", "categories": ["cs.HC", "cs.CY"], "comment": "Conference paper. Appears in the Proceedings of the 2nd International Conference on Multidisciplinary Sciences and Technological Developments (ICMUSTED 2025), December 12 15, 2025, Türkiye", "summary": "Artificial intelligence (AI) based learning assistants and chatbots are increasingly integrated into higher education. While these tools are often evaluated in terms of technical performance, their successful and ethical use also depends on psychological factors such as trust, perceived risk, technology anxiety, and students general attitudes toward AI. This paper adopts a psychology oriented perspective to examine how university students form trust in AI based learning assistants. Drawing on recent literature in mental health, human AI interaction, and trust in automation, we propose a conceptual framework that organizes psychological predictors of trust into four groups: cognitive appraisals, affective reactions, social relational factors, and contextual moderators. A narrative review approach synthesizes empirical findings and derives research questions and hypotheses for future studies. The paper highlights that trust in AI is a psychological process shaped by individual differences and learning environments, with practical implications for instructors, administrators, and designers of educational AI systems.", "AI": {"tldr": "探讨大学生对基于AI的学习助手的信任因素", "motivation": "研究心理因素如何影响大学生对AI学习助手的信任，促进其在教育中的成功和伦理使用", "method": "采用心理学视角，结合心理健康、人机交互及自动化信任的相关文献，提出一个概念框架，并通过叙事回顾方法总结实证发现", "result": "确定了四个影响信任的心理预测因子：认知评估、情感反应、社交关系因素以及情境调节器", "conclusion": "强调信任是受个人差异和学习环境影响的心理过程，具有实际应用价值"}}
{"id": "2512.17376", "pdf": "https://arxiv.org/pdf/2512.17376", "abs": "https://arxiv.org/abs/2512.17376", "authors": ["Peixuan Zhang", "Shuchen Weng", "Jiajun Tang", "Si Li", "Boxin Shi"], "title": "Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors", "categories": ["cs.CV"], "comment": null, "summary": "Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.", "AI": {"tldr": "提出了一种将文本中的抽象情绪转化为图像中具体情绪的Affective Image Filter (AIF)任务。", "motivation": "希望通过视觉化的方式，更有效地在社交媒体上表达和传递情感。", "method": "提出了基于多模态变压器架构的AIF-B模型以及利用预训练大规模扩散模型生成先验知识的AIF-D模型。", "result": "实验结果表明所提出的AIF模型在内容一致性和情绪真实性方面优于现有方法，能够更有效地唤起特定情绪。", "conclusion": "讨论了AIF模型的价值和潜在应用前景"}}
{"id": "2512.17373", "pdf": "https://arxiv.org/pdf/2512.17373", "abs": "https://arxiv.org/abs/2512.17373", "authors": ["Zhengmian Hu"], "title": "Dialectics for Artificial Intelligence", "categories": ["cs.AI"], "comment": null, "summary": "Can artificial intelligence discover, from raw experience and without human supervision, concepts that humans have discovered? One challenge is that human concepts themselves are fluid: conceptual boundaries can shift, split, and merge as inquiry progresses (e.g., Pluto is no longer considered a planet). To make progress, we need a definition of \"concept\" that is not merely a dictionary label, but a structure that can be revised, compared, and aligned across agents. We propose an algorithmic-information viewpoint that treats a concept as an information object defined only through its structural relation to an agent's total experience. The core constraint is determination: a set of parts forms a reversible consistency relation if any missing part is recoverable from the others (up to the standard logarithmic slack in Kolmogorov-style identities). This reversibility prevents \"concepts\" from floating free of experience and turns concept existence into a checkable structural claim. To judge whether a decomposition is natural, we define excess information, measuring the redundancy overhead introduced by splitting experience into multiple separately described parts. On top of these definitions, we formulate dialectics as an optimization dynamics: as new patches of information appear (or become contested), competing concepts bid to explain them via shorter conditional descriptions, driving systematic expansion, contraction, splitting, and merging. Finally, we formalize low-cost concept transmission and multi-agent alignment using small grounds/seeds that allow another agent to reconstruct the same concept under a shared protocol, making communication a concrete compute-bits trade-off.", "AI": {"tldr": "通过原始经验而无需人类监督，人工智能是否可以发现人类已经发现的概念？", "motivation": "探讨人工智能能否独立于人工指导从纯数据中提取并理解概念。", "method": "提出一种算法信息观点，将概念定义为仅通过其与代理总经历的结构关系确定的信息对象，并引入辩证法作为优化动态过程。", "result": "该研究提供了一种形式化低成本的概念传输和多代理对齐的方法。", "conclusion": "利用小的基础/种子使得另一个代理能够重建相同的概念，在共享协议下，这使通信成为具体计算位元的权衡。"}}
{"id": "2512.17370", "pdf": "https://arxiv.org/pdf/2512.17370", "abs": "https://arxiv.org/abs/2512.17370", "authors": ["Deqing Liu", "Yinfeng Gao", "Deheng Qian", "Qichao Zhang", "Xiaoqing Ye", "Junyu Han", "Yupeng Zheng", "Xueyi Liu", "Zhongpu Xia", "Dawei Ding", "Yifeng Pan", "Dongbin Zhao"], "title": "TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.", "AI": {"tldr": "提出了一种基于专家接管数据的后优化框架TakeAD，以改善端到端自动驾驶系统的闭环性能。", "motivation": "现有端到端自动驾驶方法依赖于模仿学习，在训练和部署之间存在开环与闭环不匹配的问题。如何利用司机干预的数据来扩大模仿学习策略的能力是一个有价值的但尚未解决的挑战。", "method": "设计了一个高效的数据收集管道，用于获取专家接管数据，并结合迭代数据聚合（DAgger）和直接偏好优化（DPO），通过多次迭代增强策略在接管情况下的恢复能力。", "result": "实验表明，与纯模仿学习方法相比，该方法在Bench2Drive基准测试中表现出更有效的性能。逐步消融分析确认了每个组件的贡献。", "conclusion": "提出的TakeAD框架能够有效解决端到端自动驾驶系统中的开环和闭环不匹配问题，并通过利用专家接管数据来提高系统的整体性能。"}}
{"id": "2512.17356", "pdf": "https://arxiv.org/pdf/2512.17356", "abs": "https://arxiv.org/abs/2512.17356", "authors": ["Tingxiao Zhou", "Leying Zhang", "Zhengyang Chen", "Yanmin Qian"], "title": "Training Text-to-Speech Model with Purely Synthetic Data: Feasibility, Sensitivity, and Generalization Capability", "categories": ["cs.SD"], "comment": "14 pages, 5 figures, received by National Conference on Man-Machine Speech Communication (NCMMSC2025)", "summary": "The potential of synthetic data in text-to-speech (TTS) model training has gained increasing attention, yet its rationality and effectiveness require systematic validation. In this study, we systematically investigate the feasibility of using purely synthetic data for TTS training and explore how various factors--including text richness, speaker diversity, noise levels, and speaking styles--affect model performance. Our experiments reveal that increasing speaker and text diversity significantly enhances synthesis quality and robustness. Cleaner training data with minimal noise further improves performance. Moreover, we find that standard speaking styles facilitate more effective model learning. Our experiments indicate that models trained on synthetic data have great potential to outperform those trained on real data under similar conditions, due to the absence of real-world imperfections and noise.", "AI": {"tldr": "研究使用纯合成数据训练文本到语音模型的可行性及其对各种因素的敏感性和泛化能力。", "motivation": "验证合成数据在文本到语音模型训练中的合理性和有效性，探索影响模型性能的各种因素。", "method": "系统地调查了仅用合成数据进行TTS培训的可能性，并研究了文本丰富度、说话人多样性、噪声水平和讲话风格等因素的影响。", "result": "实验表明，增加说话者和文本的多样性可以显著提高合成质量；较干净的数据（噪音少）进一步改善性能。标准的讲话方式有助于更有效的模型学习。", "conclusion": "研究表明，在类似条件下使用纯合成数据训练的模型具有超越真实数据训练的潜力，原因是它避免了现实世界中的不完美和噪声。"}}
{"id": "2512.17354", "pdf": "https://arxiv.org/pdf/2512.17354", "abs": "https://arxiv.org/abs/2512.17354", "authors": ["Wisnu Uriawan", "Muhammad Aditya Hafizh Zahran", "Inayah Ayu Deswita", "Muhammad Ahsani Taqwim", "Ismail Muhammad Ahmadi", "Marvi Yoga Pratama"], "title": "Implementation of Augmented Reality as an Educational Tool for Practice in Early Childhood", "categories": ["cs.HC", "q-fin.CP"], "comment": "7 pages, 3 figures, 1 table", "summary": "Learning Wudhu for young children requires engaging and interactive media to foster a deep understanding of the worship procedures. This study aims to develop a Wudhu learning application based on Augmented Reality (AR) as an interactive and fun educational medium. The development method used includes the stages of needs analysis, system design, implementation, and testing using Black Box Testing. The system utilizes marker-based tracking to display 3D animations of Wudhu movements in real-time when the camera detects a marker on the printed media. The test results indicate that all main functions run well, and a limited trial on children aged 5-7 years showed an increase in learning interest and a better understanding of the Wudhu sequence. Thus, the application of AR technology is proven effective in improving the quality of basic worship instruction for young children.", "AI": {"tldr": "开发基于增强现实（AR）的洗手学习应用，用于儿童教育", "motivation": "培养儿童对礼拜仪式的理解需要吸引人的互动媒介", "method": "采用需求分析、系统设计、实施和黑盒测试的方法进行开发，使用标记跟踪显示3D动画", "result": "所有主要功能运行良好，在5-7岁儿童中试验表明提高了学习兴趣并更好地理解了洗手顺序", "conclusion": "AR技术证明对改善基本礼拜教育质量有效"}}
{"id": "2512.17352", "pdf": "https://arxiv.org/pdf/2512.17352", "abs": "https://arxiv.org/abs/2512.17352", "authors": ["Ivan Kralj", "Lodovico Giaretta", "Gordan Ježić", "Ivana Podnar Žarko", "Šarūnas Girdzijauskas"], "title": "Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "19 pages, 6 figures, 5 tables, journal", "summary": "Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.", "AI": {"tldr": "本文提出了一种自适应图剪枝算法，用于降低基于云边计算的时空图神经网络在交通预测中的通信开销，并引入了新的突发事件预测准确性指标。", "motivation": "现有时空图神经网络在智能交通系统中处理高频数据时产生大量通信开销，本文旨在通过动态过滤冗余节点特征来减少这种开销，并提高对突发交通状况的响应能力。", "method": "提出了一种自适应剪枝算法，根据模型性能调整修剪率；引入了Sudden Event Prediction Accuracy (SEPA)作为新评估指标；在半分布式环境进行实验测试。", "result": "实验证明，该方法可以在减少通信成本的同时保持预测精度，并且对于突发交通状况的响应性优于标准误差度量。", "conclusion": "自适应剪枝算法有效减少了时空图神经网络处理交通数据时所需的通信开销，同时保证了对紧急事件的高度敏感性和准确性。"}}
{"id": "2512.17350", "pdf": "https://arxiv.org/pdf/2512.17350", "abs": "https://arxiv.org/abs/2512.17350", "authors": ["Chenming Zhou", "Jiaan Wang", "Yu Li", "Lei Li", "Juan Cao", "Sheng Tang"], "title": "Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026", "summary": "The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.", "AI": {"tldr": "该论文提出了一种基于像素级别的映射预处理步骤来提高AI生成图像检测的泛化能力。", "motivation": "当前的AI图像检测器容易过度拟合特定来源的语义线索，而无法学习通用的生成特征。作者希望通过打破这些非本质的语义模式来提升检测器的泛化性能。", "method": "引入一种像素级别的映射预处理步骤以扰乱图像的像素值分布，并迫使检测器关注更具普遍性的高频痕迹。", "result": "实验显示，该方法显著提高了基于GAN和扩散模型生成器的检测器在跨生成器上的表现。", "conclusion": "通过打破语义线索，可以有效提高AI生成图像检测的泛化能力。"}}
{"id": "2512.17349", "pdf": "https://arxiv.org/pdf/2512.17349", "abs": "https://arxiv.org/abs/2512.17349", "authors": ["Xijie Huang", "Jinhan Li", "Tianyue Wu", "Xin Zhou", "Zhichao Han", "Fei Gao"], "title": "Flying in Clutter on Monocular RGB by Learning in 3D Radiance Fields with Domain Adaptation", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "Modern autonomous navigation systems predominantly rely on lidar and depth cameras. However, a fundamental question remains: Can flying robots navigate in clutter using solely monocular RGB images? Given the prohibitive costs of real-world data collection, learning policies in simulation offers a promising path. Yet, deploying such policies directly in the physical world is hindered by the significant sim-to-real perception gap. Thus, we propose a framework that couples the photorealism of 3D Gaussian Splatting (3DGS) environments with Adversarial Domain Adaptation. By training in high-fidelity simulation while explicitly minimizing feature discrepancy, our method ensures the policy relies on domain-invariant cues. Experimental results demonstrate that our policy achieves robust zero-shot transfer to the physical world, enabling safe and agile flight in unstructured environments with varying illumination.", "AI": {"tldr": "该论文提出了一种方法，使用单目RGB图像训练飞行机器人在复杂环境中导航。", "motivation": "现有的自主导航系统主要依赖于激光雷达和深度摄像头。然而，一个基本问题是：仅依靠单目RGB图像，飞行机器人能否在混乱环境中导航？由于真实世界数据收集成本高昂，学习策略的模拟提供了可能路径，但模拟到现实之间的感知差距成为瓶颈。", "method": "论文提出了一种结合3D高斯点阵和对抗域适应框架的方法。通过在高度逼真的仿真环境中学得不变特征来最小化特征差异，确保了策略依赖于领域通用线索。", "result": "实验结果表明，该策略实现了从模拟到现实的稳健零样本迁移，在各种光照条件下实现了安全敏捷飞行。", "conclusion": "本文方法为单目RGB图像驱动的安全和高效飞行提供了一种新途径，推动了在复杂环境中自主导航的研究。"}}
{"id": "2512.17343", "pdf": "https://arxiv.org/pdf/2512.17343", "abs": "https://arxiv.org/abs/2512.17343", "authors": ["Cuixin Yang", "Rongkang Dong", "Kin-Man Lam", "Yuhang Zhang", "Guoping Qiu"], "title": "Multi-level distortion-aware deformable network for omnidirectional image super-resolution", "categories": ["cs.CV"], "comment": null, "summary": "As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.", "AI": {"tldr": "提出了一种多级畸变感知可变形网络（MDDN）用于全景图像超分辨率。", "motivation": "现有的全景图像超分辨率方法存在采样范围和特征提取能力有限的问题，难以有效捕捉大区域的畸变模式。为此，提出了MDDN来扩大采样范围和感受野。", "method": "该网络由三个并行分支组成：可变形注意力机制（相当于膨胀率为1的路径）以及两个具有2和3膨胀率的可变形卷积。这些设计扩展了采样范围以包括更广泛的畸变模式，并通过低秩分解策略减少计算成本。", "result": "在公开数据集上的大量实验表明，MDDN优于现有方法，证明了其有效性和优越性。", "conclusion": "提出的MDDN能够有效处理全景图像超分辨率中的几何畸变问题，并且实验结果验证了其性能超越当前最佳的方法。"}}
{"id": "2512.17331", "pdf": "https://arxiv.org/pdf/2512.17331", "abs": "https://arxiv.org/abs/2512.17331", "authors": ["Shihang Li", "Zhiqiang Gong", "Minming Ye", "Yue Gao", "Wen Yao"], "title": "SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation", "categories": ["cs.CV"], "comment": "Submitted to ICASSP 2026", "summary": "Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.", "AI": {"tldr": "本论文提出了一种基于注意力引导的协同变形框架，用于高质量的人脸动画合成。", "motivation": "传统显式变形方法在准确运动传递和恢复缺失区域方面存在困难，而最近的基于注意力的方法虽然有效但复杂度高且几何基础弱。为了解决这些问题，提出了SynergyWarpNet模型。", "method": "该模型包括三个阶段：首先使用3D稠密光流进行粗略空间对齐；其次利用跨注意力机制从多个参考图像中补全遮挡或失真区域；最后采用自适应融合模块平衡结构对齐和视觉一致性。", "result": "在基准数据集上的全面评估显示该方法取得了最先进的性能。", "conclusion": "SynergyWarpNet框架通过结合显式变形和基于注意力的方法，实现了高保真的人脸动画合成。"}}
{"id": "2512.17326", "pdf": "https://arxiv.org/pdf/2512.17326", "abs": "https://arxiv.org/abs/2512.17326", "authors": ["Sander Moonemans", "Sebastiaan Ram", "Frédérique Meeuwsen", "Carlijn Lems", "Jeroen van der Laak", "Geert Litjens", "Francesco Ciompi"], "title": "Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Vision-language models (VLMs) have the potential to become co-pilots for pathologists. However, most VLMs either focus on small regions of interest within whole-slide images, provide only static slide-level outputs, or rely on data that is not publicly available, limiting reproducibility. Furthermore, training data containing WSIs paired with detailed clinical reports is scarce, restricting progress toward transparent and generalisable VLMs. We address these limitations with three main contributions. First, we introduce Polysome, a standardised tool for synthetic instruction generation. Second, we apply Polysome to the public HISTAI dataset, generating HISTAI-Instruct, a large whole-slide instruction tuning dataset spanning 24,259 slides and over 1.1 million instruction-response pairs. Finally, we use HISTAI-Instruct to train ANTONI-α, a VLM capable of visual-question answering (VQA). We show that ANTONI-α outperforms MedGemma on WSI-level VQA tasks of tissue identification, neoplasm detection, and differential diagnosis. We also compare the performance of multiple incarnations of ANTONI-α trained with different amounts of data. All methods, data, and code are publicly available.", "AI": {"tldr": "论文介绍了用于病理学的开放性管道和数据集，以支持全滑动图像视觉语言模型的研究。", "motivation": "为了克服当前大多数视觉语言模型存在的问题，如关注小区域、提供静态输出以及使用非公开数据，本文旨在通过引入Polysome工具来生成大量合成指令，并利用HISTAI-Instruct训练出更强大的视觉问答系统。", "method": "作者首先介绍了用于生成合成指令的标准化工具Polysome。然后利用该工具在公共HISTAI数据集上生成了HISTAI-Instruct，一个包含24,259张全滑动图像和超过1.1百万个指令响应对的数据集。最后使用此数据集训练了一个名为ANTONI-α的视觉语言模型，能够在组织识别、肿瘤检测及鉴别诊断任务中超越MedGemma。", "result": "结果表明，ANTONI-α在全滑动图像级别的视觉问答任务上优于MedGemma，并展示了不同规模的数据对模型性能的影响。", "conclusion": "通过提供一个开放的管道和数据集，作者为开发更强大、透明且通用的病理学辅助工具铺平了道路。"}}
{"id": "2512.17323", "pdf": "https://arxiv.org/pdf/2512.17323", "abs": "https://arxiv.org/abs/2512.17323", "authors": ["Jiyun Kong", "Jun-Hyuk Kim", "Jong-Seok Lee"], "title": "DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training", "categories": ["cs.CV"], "comment": null, "summary": "Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.", "AI": {"tldr": "提出了一种基于扩散的事件驱动单帧合成框架（DESSERT），利用残差训练来改善视频帧预测。", "motivation": "为了克服现有方法在动态场景中引入的孔洞和模糊问题，提出了新的框架以提高帧合成的质量。", "method": "使用预训练的稳定扩散模型进行残差训练，确保时间一致性。包含两个阶段：事件到残差对齐变分自编码器（ER-VAE）阶段以及基于事件数据条件的扩散模型去噪阶段。", "result": "实验表明该方法优于现有事件基重建、图像基视频帧预测、事件基视频帧预测和单边事件基视频帧插值方法，产生更清晰和时间上一致的帧合成。", "conclusion": "DESSERT框架通过残差训练提高视频帧预测质量，在动态场景中表现出色。"}}
{"id": "2512.17322", "pdf": "https://arxiv.org/pdf/2512.17322", "abs": "https://arxiv.org/abs/2512.17322", "authors": ["Jose Vargas Quiros", "Bart Liefers", "Karin van Garderen", "Jeroen Vermeulen", "Eyened Reading Center", "Caroline Klaver"], "title": "Rotterdam artery-vein segmentation (RAV) dataset", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology. Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools. Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information. Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings. Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.", "AI": {"tldr": "提供一套高质量的眼底彩色图像数据集，用于血管分析的机器学习算法开发和评估", "motivation": "支持眼科中血管分析的机器学习算法的发展和评估，提供多样性和高质的眼底图及其详细的动脉静脉分割标注", "method": "从鹿特丹研究项目中抽取眼底彩色图片，并使用定制界面进行详细注释，确保血管连接性并验证", "result": "数据集包含1024x1024像素PNG图像，包括原始RGB、对比增强和A/V掩码三种模式的图像", "conclusion": "此数据集为开发适用于临床的眼科机器学习工具提供了丰富的资源，有助于提高自动筛查与诊断疾病的准确性"}}
{"id": "2512.17321", "pdf": "https://arxiv.org/pdf/2512.17321", "abs": "https://arxiv.org/abs/2512.17321", "authors": ["Momina Liaqat Ali", "Muhammad Abid"], "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.", "AI": {"tldr": "提出一种模块化的神经符号控制框架，以解决大语言模型在连续控制中遇到的问题。", "motivation": "尽管大型语言模型已能有效应用于基于语言的控制任务，但其稳定性、收敛速度和幻觉动作等问题仍然存在。为了解决这些问题，并提高系统的可解释性、稳定性和泛化能力，提出了一种新的方法。", "method": "该研究采用了一个轻量级神经增量控制器执行连续空间中的有界增量操作，同时使用本地部署的语言模型进行符号任务的解读。通过实验对比了仅用语言模型控制、仅用神经网络控制和提议的方法（即LLM+DL框架）的表现。", "result": "与仅基于语言模型的方法相比，该方法显著提高了成功率和效率，平均减少了70%以上的步数，并且速度提高了8.83倍。同时证明了这种方法对语言模型的质量具有鲁棒性。", "conclusion": "研究表明，通过控制语言模型产生符号输出并分配未解释执行给神经控制器，可以创建更可靠、高效的基于语言的实体系统。"}}
{"id": "2512.17320", "pdf": "https://arxiv.org/pdf/2512.17320", "abs": "https://arxiv.org/abs/2512.17320", "authors": ["Lu Wei", "Yuta Nakashima", "Noa Garcia"], "title": "EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories", "categories": ["cs.CV"], "comment": "Under review", "summary": "The widespread adoption of text-to-image (T2I) generation has raised concerns about privacy, bias, and copyright violations. Concept erasure techniques offer a promising solution by selectively removing undesired concepts from pre-trained models without requiring full retraining. However, these methods are often evaluated on a limited set of concepts, relying on overly simplistic and direct prompts. To test the boundaries of concept erasure techniques, and assess whether they truly remove targeted concepts from model representations, we introduce EMMA, a benchmark that evaluates five key dimensions of concept erasure over 12 metrics. EMMA goes beyond standard metrics like image quality and time efficiency, testing robustness under challenging conditions, including indirect descriptions, visually similar non-target concepts, and potential gender and ethnicity bias, providing a socially aware analysis of method behavior. Using EMMA, we analyze five concept erasure methods across five domains (objects, celebrities, art styles, NSFW, and copyright). Our results show that existing methods struggle with implicit prompts (i.e., generating the erased concept when it is indirectly referenced) and visually similar non-target concepts (i.e., failing to generate non-targeted concepts resembling the erased one), while some amplify gender and ethnicity bias compared to the original model.", "AI": {"tldr": "EMMA是一个用于评估概念擦除技术的基准测试，涵盖广泛的语义指标和多个类别。", "motivation": "现有的概念擦除方法通常仅在有限的概念集上进行简单的直接提示测试，缺乏全面性和复杂性。为了更好地评价这些方法的有效性并检测潜在的社会偏见问题，引入了EMMA。", "method": "EMMA通过五种关键维度和十二项指标评估不同的概念擦除技术，包括图像质量、时间效率以及在间接描述等挑战条件下的鲁棒性测试，并分析五种不同领域的方法表现。", "result": "实验结果显示现有方法在处理隐含提示和视觉相似非目标概念时效果不佳，并且某些方法可能会放大性别和种族偏见。", "conclusion": "EMMA提供了一个全面的框架来评估和改善概念擦除技术，有助于开发更加可靠、公正的T2I模型。"}}
{"id": "2512.17319", "pdf": "https://arxiv.org/pdf/2512.17319", "abs": "https://arxiv.org/abs/2512.17319", "authors": ["Yunkai Dang", "Meiyi Zhu", "Donghao Wang", "Yizhuo Zhang", "Jiacheng Yang", "Qi Fan", "Yuekun Yang", "Wenbin Li", "Feng Miao", "Yang Gao"], "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR", "AI": {"tldr": "该论文介绍了RSHR-Bench，一个超高分辨率的遥感视觉理解和推理基准。", "motivation": "现有大多数远程感知基准依赖于低分辨率图像，有些高分辨率基准存在任务设计缺陷。引入新基准以准确评估视觉理解能力。", "method": "创建了一个包含5,329张超高清图片的数据集，并定义了四类任务：多项选择问答、开放式问答、图像描述和单图评估。", "result": "实验结果揭示了在超高分辨率场景中，现有模型性能差距显著。", "conclusion": "RSHR-Bench提供了对高分辨遥感数据的理解与推理能力的更准确评价。"}}
{"id": "2512.17316", "pdf": "https://arxiv.org/pdf/2512.17316", "abs": "https://arxiv.org/abs/2512.17316", "authors": ["Michael Merry", "Pat Riddle", "Jim Warren"], "title": "Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - \"we know it when we see it\". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \\textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.", "AI": {"tldr": "提出了一个测试模型固有可解释性的标准，使用图论进行模型的结构局部和全局解释。", "motivation": "当前对固有可解释性缺乏一致定义或测试方法。文章旨在提供一种适用全球的标准来衡量模型的固有可解释性。", "method": "利用图理论表示和分解模型以获得结构局部解释，再重组为全局解释；将这些解释作为注释，形成假设-证据结构以供不同解释方法使用。", "result": "提出了一个能够匹配现有对固有可解释性的直观理解的测试标准，并通过PREDICT模型证明了一个大规模回归模型可能不具备解释性而稀疏神经网络具备解释性。", "conclusion": "该工作为其他关于可解释性的研究提供了结构基础，同时提供了一种灵活但严格的测试方法，可以用于合规框架中。"}}
{"id": "2512.17313", "pdf": "https://arxiv.org/pdf/2512.17313", "abs": "https://arxiv.org/abs/2512.17313", "authors": ["SuBeen Lee", "GilHan Park", "WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.", "AI": {"tldr": "提出了辅助描述知识（ADK）框架，以提高视觉语言模型的少样本适应性。", "motivation": "现有的参数高效微调方法依赖于固定的手动提示，在处理语义复杂的类别时效果有限。因此需要一种新的方法来提供更丰富的文本表示而不增加计算开销。", "method": "通过大规模语言模型生成每个类别的描述性提示，构建辅助描述知识框架。该框架分为组合知识和实例特定知识两部分：前者为平均后的丰富语义表示；后者利用轻量级非参数注意力机制选择与给定图像最相关的描述。", "result": "实验表明ADK能够显著提升多种参数高效微调方法的性能，达到新的最佳水平。", "conclusion": "辅助描述知识框架作为一种无参数、即插即用组件，在视觉语言模型的少样本适应性中表现出色。"}}
{"id": "2512.17312", "pdf": "https://arxiv.org/pdf/2512.17312", "abs": "https://arxiv.org/abs/2512.17312", "authors": ["Qi Song", "Honglin Li", "Yingchen Yu", "Haoyi Zhou", "Lin Yang", "Song Bai", "Qi She", "Zilong Huang", "Yunqing Zhao"], "title": "CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent releases such as o3 highlight human-like \"thinking with images\" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.", "AI": {"tldr": "CodeDance是一种动态的工具集成多模态大型语言模型，用于可执行视觉推理。", "motivation": "现有的开源方法大多依赖于文本链、僵化可视模式或单步管道，这限制了灵活性、解释性和复杂任务上的迁移性。因此，需要一种新的方法来增强视觉推理的性能和通用性。", "method": "CodeDance采用可执行代码作为通用求解器来进行视觉推理，并通过引入平衡与适应性的工具调用奖励机制，以在探索效率之间取得平衡并减少工具过度使用。", "result": "实验结果表明，CodeDance不仅始终优于模式驱动和纯文本基线模型，在跨推理基准测试中（如视觉搜索、数学和图表问题），还超越了先进的闭源模型GPT-4o等。", "conclusion": "CodeDance展示了通过可执行代码进行视觉推理的有效性，并且在无需特定任务微调的情况下，表现出新颖的工具调用、未见组合及跨任务转移能力。"}}
{"id": "2512.17309", "pdf": "https://arxiv.org/pdf/2512.17309", "abs": "https://arxiv.org/abs/2512.17309", "authors": ["Asil Kaan Bozcuoglu", "Ziyuan Liu"], "title": "RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models", "categories": ["cs.RO"], "comment": null, "summary": "RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.", "AI": {"tldr": "本文提出了RecipeMasterLLM，一种基于大型语言模型的高阶规划器，用于自动生成符合RoboEarth标准知识图谱的动作本体论。", "motivation": "随着大型语言模型的发展，作者认为可以通过自动化方式改进罗博地球的知识获取过程。通过引入大规模语言模型来理解和生成与RoboEarth标准知识图谱一致的动作描述，可以显著提高机器人操作的效率和准确性。", "method": "RecipeMasterLLM利用经过微调的大规模语言模型自动生成动作本体论，并在检索增强生成阶段提供环境信息以提升上下文理解能力。该架构能够响应用户提示来生成符合标准知识图谱的动作描述。", "result": "通过使用RecipeMasterLLM，机器人可以更准确地理解和执行任务，提高了系统性能和适应性。", "conclusion": "本文展示了大型语言模型在自动化RoboEarth知识获取中的潜力，提出了RecipeMasterLLM架构，并强调了这种方法对于提高机器人操作效率的重要性。"}}
{"id": "2512.17308", "pdf": "https://arxiv.org/pdf/2512.17308", "abs": "https://arxiv.org/abs/2512.17308", "authors": ["Daksh Jain", "Aarya Jain", "Ashutosh Desai", "Avyakt Verma", "Ishan Bhanuka", "Pratik Narang", "Dhruv Kumar"], "title": "Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation", "categories": ["cs.AI", "cs.CL"], "comment": "Under Review", "summary": "Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.", "AI": {"tldr": "该论文研究了大型语言模型在宝可梦战斗中的应用，评估它们作为策略性对手的能力。", "motivation": "通过宝可梦战斗这一独特场景测试大型语言模型的战略决策能力，并探讨这些模型是否能在没有特定领域训练的情况下作为动态游戏对手发挥作用。", "method": "开发了一个基于回合的宝可梦对战系统，在此系统中，大型语言模型根据比赛状态而非预先编程逻辑选择动作。该框架涵盖了类型效果乘数、统计数据基础伤害计算以及多宝可梦团队管理等核心宝可梦机制。", "result": "通过跨多种架构系统的系统性评估，测量了胜利率、决策延迟、类型一致性精度和标记效率等方面的结果表明大型语言模型可以在没有特定领域训练的情况下作为动态游戏对手发挥作用。", "conclusion": "结果表明大型语言模型不仅可以作为策略性的游戏玩家，还可以参与内容创作。这为互动娱乐中的程序生成和自适应难度系统提供了潜在的应用前景。"}}
{"id": "2512.17306", "pdf": "https://arxiv.org/pdf/2512.17306", "abs": "https://arxiv.org/abs/2512.17306", "authors": ["Wenhao Yang", "Yu Xia", "Jinlong Huang", "Shiyin Lu", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang", "Yuanyu Wan", "Lijun Zhang"], "title": "Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.", "AI": {"tldr": "提出了一种名为DRIM的模型，该模型通过深度但可靠的多轮推理在用图像思考时实现改进的自我反思能力。", "motivation": "现有视觉语言模型（VLM）虽然展示了强大的推理能力，但在遇到错误推理路径时往往无法进行自我纠正。作者旨在解决这一问题。", "method": "DRIM模型通过一个三阶段流程：数据构建、冷启动SFT和RL来实现其目标。首先，基于高分辨率图像数据集构造具有挑战性的视觉问答对；其次，在SFT阶段收集工具轨迹作为初始引导数据；最后，在RL阶段引入冗余惩罚策略优化以激励模型发展自我反思推理。", "result": "DRIM在视觉理解基准测试中表现出优于现有方法的性能。", "conclusion": "该论文通过提出一种新模型实现了深度但可靠的多轮推理，解决了现有VLM模型无法自我纠正错误推理路径的问题。"}}
{"id": "2512.17303", "pdf": "https://arxiv.org/pdf/2512.17303", "abs": "https://arxiv.org/abs/2512.17303", "authors": ["Ankit Yadav", "Ta Duc Huy", "Lingqiao Liu"], "title": "EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance", "categories": ["cs.CV"], "comment": "26 pages", "summary": "In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.", "AI": {"tldr": "提出了一种名为EMAG的机制，用于改进扩散模型中的样本生成。", "motivation": "现有方法难以精确控制负样例的质量和难度，并且层选择往往是固定的。因此需要一种新的指导策略来提高生成效果。", "method": "EMAG通过在推理时修改注意力权重来增强扩散变压器模型的效果，使用基于统计的自适应层选择规则。", "result": "与传统的CFG方法相比，EMAG显著提高了样本质量和人类偏好分数（HPS）0.46。并且可以与其他高级指导技术结合以进一步改善HPS。", "conclusion": "EMAG提供了一种有效的改进机制，在不改变训练流程的情况下提高扩散模型的生成质量，并且可以与现有方法相结合，获得更好的结果。"}}
{"id": "2512.17302", "pdf": "https://arxiv.org/pdf/2512.17302", "abs": "https://arxiv.org/abs/2512.17302", "authors": ["Kyeongmin Yeo", "Yunhong Min", "Jaihoon Kim", "Minhyuk Sung"], "title": "MatLat: Material Latent Space for PBR Texture Generation", "categories": ["cs.CV"], "comment": "Project page: https://matlat-proj.github.io", "summary": "We propose a generative framework for producing high-quality PBR textures on a given 3D mesh. As large-scale PBR texture datasets are scarce, our approach focuses on effectively leveraging the embedding space and diffusion priors of pretrained latent image generative models while learning a material latent space, MatLat, through targeted fine-tuning. Unlike prior methods that freeze the embedding network and thus lead to distribution shifts when encoding additional PBR channels and hinder subsequent diffusion training, we fine-tune the pretrained VAE so that new material channels can be incorporated with minimal latent distribution deviation. We further show that correspondence-aware attention alone is insufficient for cross-view consistency unless the latent-to-image mapping preserves locality. To enforce this locality, we introduce a regularization in the VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions to maintain strong pixel-latent spatial correspondence. Ablation studies and comparison with previous baselines demonstrate that our framework improves PBR texture fidelity and that each component is critical for achieving state-of-the-art performance.", "AI": {"tldr": "提出了一种生成框架，用于在给定的3D网格上生成高质量的PBR纹理。", "motivation": "大规模PBR纹理数据集稀缺，现有的方法冻结嵌入网络导致分布偏差和一致性问题。因此，需要一种新的方法来优化PBR纹理质量。", "method": "通过微调预训练的VAE学习材料潜在空间MatLat，并引入局部性正则化以保持像素-潜在空间对应关系。", "result": "实验表明该框架提高了PBR纹理的真实性，且每个组件对于实现最佳性能至关重要。", "conclusion": "提出的生成框架在PBR纹理生成方面实现了最先进的性能。"}}
{"id": "2512.17299", "pdf": "https://arxiv.org/pdf/2512.17299", "abs": "https://arxiv.org/abs/2512.17299", "authors": ["Abdullah M. Zyarah", "Dhireesha Kudithipudi"], "title": "M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge", "categories": ["cs.LG", "cs.AI", "cs.ET"], "comment": null, "summary": "Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.", "AI": {"tldr": "M2RU是一种混合信号架构，用于在边缘设备上实现高效的时序处理和持续学习。", "motivation": "连续学习对于边缘平台来说具有挑战性，因为递归网络依赖于耗能的训练过程和频繁的数据移动，这不适合嵌入式部署。因此提出了M2RU以解决这些问题。", "method": "M2RU实现了一种名为minion递归单元的架构，它采用了加权位流技术和经验回放机制来处理多比特数字输入，并稳定在领域转变下的学习过程。", "result": "M2RU实现了15 GOPS功耗为48.62 mW，即312 GOPS/瓦特，在顺序MNIST和CIFAR-10任务中保持了与软件基准相差不超过5%的准确性。与CMOS数字设计相比，加速器提供了29倍的能量效率。", "conclusion": "M2RU作为一种可扩展且节能的技术平台，为实时适应边缘层面的时间智能提供支持，其预期工作寿命可达12.2年。"}}
{"id": "2512.17298", "pdf": "https://arxiv.org/pdf/2512.17298", "abs": "https://arxiv.org/abs/2512.17298", "authors": ["Fanpu Cao", "Yaofo Chen", "Zeng You", "Wei Luo", "Cen Chen"], "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration", "categories": ["cs.CV"], "comment": "Accepted for poster presentation at AAAI 2026", "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.", "AI": {"tldr": "本文提出了一种针对扩散变换器（DiTs）的动态特征缓存框架ProCache，以解决其高计算成本的问题。", "motivation": "现有的特征缓存方法在非均匀时间动态和错误累积方面存在局限性。作者通过分析DiT特征演化过程中去噪的变化情况，提出了新的解决方案以提高加速效果并减少质量退化。", "method": "ProCache包含两个核心组件：一个适应模型时间特性的约束感知缓存模式搜索模块；以及在缓存段中针对深块和高重要性令牌的选择性计算模块，从而最小化错误累积的开销。", "result": "实验表明，在PixArt-alpha和DiT上，ProCache能够实现高达1.96倍和2.90倍的加速，同时保持质量不下降。与以前基于缓存的方法相比，它表现出了显著的优势。", "conclusion": "通过引入新的约束感知特征缓存方法和选择性计算策略，ProCache有效解决了扩散变换器高计算成本的问题，并展示了优越的性能提升。"}}
{"id": "2512.17296", "pdf": "https://arxiv.org/pdf/2512.17296", "abs": "https://arxiv.org/abs/2512.17296", "authors": ["Wuyi Liu", "Le Jin", "Junxian Yang", "Yuanchao Yu", "Zishuo Peng", "Jinfeng Xu", "Xianzhi Li", "Jun Zhou"], "title": "Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\\\ via Self-Supervised Image Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.", "AI": {"tldr": "提出HiSIR-Net框架，用于高分辨率PCBA的像素级缺陷定位", "motivation": "解决PCBA自动检测中标签数据不足和微小缺陷问题，提高在复杂高分辨率图像中的异常定位准确性", "method": "设计包含选择性输入重建门(SIR-Gate)和区域优化补丁选择(ROPS)方案的自我监督重构框架，减少不相关重构错误，降低假阳性率", "result": "实验显示该方法在SIPCBA-500数据集及其他公开基准上的定位性能优越，并具有实用运行速度", "conclusion": "提出的方法能够有效进行高分辨率PCBA缺陷检测和像素级定位"}}
{"id": "2512.17293", "pdf": "https://arxiv.org/pdf/2512.17293", "abs": "https://arxiv.org/abs/2512.17293", "authors": ["June Young Yi", "Hyeongju Kim", "Juheon Lee"], "title": "Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track", "categories": ["cs.SD", "cs.AI"], "comment": "2 pages, preprint, This work has been submitted to the IEEE for possible publication. Submitted to ICASSP 2026 SPGC (WildSpoof Challenge, TTS track)", "summary": "This paper presents a lightweight text-to-speech (TTS) system developed for the WildSpoof Challenge TTS Track. Our approach fine-tunes the recently released open-weight TTS model, \\textit{Supertonic}\\footnote{\\url{https://github.com/supertone-inc/supertonic}}, with Self-Purifying Flow Matching (SPFM) to enable robust adaptation to in-the-wild speech. SPFM mitigates label noise by comparing conditional and unconditional flow matching losses on each sample, routing suspicious text--speech pairs to unconditional training while still leveraging their acoustic information. The resulting model achieves the lowest Word Error Rate (WER) among all participating teams, while ranking second in perceptual metrics such as UTMOS and DNSMOS. These findings demonstrate that efficient, open-weight architectures like Supertonic can be effectively adapted to diverse real-world speech conditions when combined with explicit noise-handling mechanisms such as SPFM.", "AI": {"tldr": "本文提出了一种轻量级的文本到语音(TTS)系统，通过自净化流匹配(SPFM)对开源模型Supertonic进行微调。", "motivation": "为了使TTS系统能够适应真实世界中的多变语境，在WildSpoof挑战赛中开发一种新的方法以提高系统的鲁棒性。", "method": "使用SPFM技术来减轻标签噪声，通过比较每个样本的条件和无条件流匹配损失，将可疑文本-语音对路由到无条件训练中。", "result": "该模型在所有参赛队伍中获得了最低的WER，并且在感知指标如UTMOS和DNSMOS方面排名第二。", "conclusion": "结合SPFM等显式噪声处理机制后，高效、开源架构如Supertonic可以有效适应各种真实世界的语音条件。"}}
{"id": "2512.17292", "pdf": "https://arxiv.org/pdf/2512.17292", "abs": "https://arxiv.org/abs/2512.17292", "authors": ["Cuixin Yang", "Rongkang Dong", "Kin-Man Lam"], "title": "Vision-Language Model Guided Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, into universal IR. Nevertheless, these methods fail to utilize the linguistic priors to ensure semantic coherence during the restoration process. To address this issue, in this paper, we propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which leverages the rich vision-language priors of VLMs, such as CLIP, to enhance IR performance through improved visual perception and semantic understanding. Our approach consists of two stages: VLM-based feature extraction and diffusion-based image restoration. In the first stage, we extract complementary visual and linguistic representations of input images by condensing the visual perception and high-level semantic priors through VLMs. Specifically, we align the embeddings of captions from low-quality and high-quality images using a cosine similarity loss with LoRA fine-tuning, and employ a degradation predictor to decompose degradation and clean image content embeddings. These complementary visual and textual embeddings are then integrated into a diffusion-based model via cross-attention mechanisms for enhanced restoration. Extensive experiments and ablation studies demonstrate that VLMIR achieves superior performance across both universal and degradation-specific IR tasks, underscoring the critical role of integrated visual and linguistic knowledge from VLMs in advancing image restoration capabilities.", "AI": {"tldr": "本文提出了基于视觉语言模型的图像修复框架(VLMIR)，以改善图像恢复过程中的视觉感知和语义理解。", "motivation": "当前方法难以有效利用视觉和语言知识，导致图像修复过程中存在语义不连贯的问题。为此，研究者引入了VLM来解决这些问题，提高图像修复的效果。", "method": "该方法分为两个阶段：一是使用CLIP等VL模型提取输入图片的互补视觉和文本表示；二是通过交叉注意机制将这些信息融入扩散模型以增强修复过程。", "result": "实验表明,VLMIR在通用以及特定退化类型的图像恢复任务上均表现出色，验证了整合视觉与语言知识对提升图像恢复能力的重要作用。", "conclusion": "本文提出的VLMIR框架通过利用视觉-语言先验提高了图像恢复的质量和效果。"}}
{"id": "2512.17289", "pdf": "https://arxiv.org/pdf/2512.17289", "abs": "https://arxiv.org/abs/2512.17289", "authors": ["G. M. Refatul Islam", "Safwan Shaheer", "Yaseen Nur", "Mohammad Rafid Hamid"], "title": "Subjective Question Generation and Answer Evaluation using NLP", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 5 figures, 2 tables, conference paper", "summary": "Natural Language Processing (NLP) is one of the most revolutionary technologies today. It uses artificial intelligence to understand human text and spoken words. It is used for text summarization, grammar checking, sentiment analysis, and advanced chatbots and has many more potential use cases. Furthermore, it has also made its mark on the education sector. Much research and advancements have already been conducted on objective question generation; however, automated subjective question generation and answer evaluation are still in progress. An automated system to generate subjective questions and evaluate the answers can help teachers assess student work and enhance the student's learning experience by allowing them to self-assess their understanding after reading an article or a chapter of a book. This research aims to improve current NLP models or make a novel one for automated subjective question generation and answer evaluation from text input.", "AI": {"tldr": "本文研究旨在改进现有的自然语言处理模型或创建新的模型，用于从文本输入中自动生成主观问题并评估答案。", "motivation": "自动化生成主观问题和回答评估的系统可以帮助教师评估学生作品，并通过让学生在阅读文章或书籍章节后自我检查理解情况来提升学习体验。", "method": "尚未具体说明方法，但研究方向是改进现有NLP模型或者创建新模型以实现从文本输入中自动生成主观问题及评价答案的目标。", "result": "未明确给出结果，推测可能涉及新型算法开发和相关性能测试。", "conclusion": "未来工作包括进一步优化模型和扩展应用范围，从而改善教育领域内的学习体验与评估机制。"}}
{"id": "2512.17281", "pdf": "https://arxiv.org/pdf/2512.17281", "abs": "https://arxiv.org/abs/2512.17281", "authors": ["Ioannis Stylianou", "Achintya kr. Sarkar", "Nauman Dawalatabad", "James Glass", "Zheng-Hua Tan"], "title": "LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "Robust Voice Activity Detection (VAD) remains a challenging task, especially under noisy, diverse, and unseen acoustic conditions. Beyond algorithmic development, a key limitation in advancing VAD research is the lack of large-scale, systematically controlled, and publicly available datasets. To address this, we introduce LibriVAD - a scalable open-source dataset derived from LibriSpeech and augmented with diverse real-world and synthetic noise sources. LibriVAD enables systematic control over speech-to-noise ratio, silence-to-speech ratio (SSR), and noise diversity, and is released in three sizes (15 GB, 150 GB, and 1.5 TB) with two variants (LibriVAD-NonConcat and LibriVAD-Concat) to support different experimental setups. We benchmark multiple feature-model combinations, including waveform, Mel-Frequency Cepstral Coefficients (MFCC), and Gammatone filter bank cepstral coefficients, and introduce the Vision Transformer (ViT) architecture for VAD. Our experiments show that ViT with MFCC features consistently outperforms established VAD models such as boosted deep neural network and convolutional long short-term memory deep neural network across seen, unseen, and out-of-distribution (OOD) conditions, including evaluation on the real-world VOiCES dataset. We further analyze the impact of dataset size and SSR on model generalization, experimentally showing that scaling up dataset size and balancing SSR noticeably and consistently enhance VAD performance under OOD conditions. All datasets, trained models, and code are publicly released to foster reproducibility and accelerate progress in VAD research.", "AI": {"tldr": "介绍LibriVAD数据集，用于评估不同的语音活动检测模型。", "motivation": "缺乏大规模、系统控制和公开可用的数据集是推进语音活动检测研究的关键限制。为了解决这一问题并促进算法的发展，提出了LibriVAD。", "method": "使用LibriSpeech和多样化的现实世界及合成噪音来源创建数据集，并通过不同特征模型组合进行基准测试，包括波形、梅尔频率倒谱系数（MFCC）以及伽玛通滤波器组倒谱系数。引入Vision Transformer（ViT）架构用于语音活动检测。", "result": "实验表明，在已见、未见和分布外条件下，使用MFCC特征的ViT模型始终优于传统的VAD模型，如增强深度神经网络和卷积长短时记忆深层神经网络，并在现实世界VOiCES数据集上进行评估。进一步分析了数据规模与SSR对模型泛化的影响。", "conclusion": "通过释放所有数据、训练好的模型以及代码来促进可重复性和加速VAD研究的进步，表明扩大数据集的大小并平衡SSR能够显著和一致地提高在分布外条件下的语音活动检测性能。"}}
{"id": "2512.17279", "pdf": "https://arxiv.org/pdf/2512.17279", "abs": "https://arxiv.org/abs/2512.17279", "authors": ["Zehui Lin", "Luyi Han", "Xin Wang", "Ying Zhou", "Yanming Zhang", "Tianyu Zhang", "Lingyun Bao", "Shandong Wu", "Dong Xu", "Tao Tan", "the UUSIC25 Challenge Consortium"], "title": "Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge", "categories": ["cs.CV"], "comment": "8 pages, 2 figures. Summary of the UUSIC25 Challenge held at MICCAI 2025. Extensive Supplementary Material (containing original team reports) is available in the \"ancillary files\" section", "summary": "IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems. OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation. DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization. OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory). RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges. CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.", "AI": {"tldr": "评估通用深度学习模型在多器官分类和分割任务中的诊断准确性和效率。", "motivation": "当前的超声波AI工具局限于单一任务，限制了其临床应用。为了提高实用性，需要开发一种能够执行多种任务的通用型AI系统。", "method": "开展UUSIC25挑战赛，在11644张图像（公共/私有）上训练算法，并通过独立的多中心测试集评估模型性能。主要评价指标包括Dice相似系数和ROC曲线下面积，同时考虑计算效率。", "result": "最佳模型SMART在五项分割任务中达到平均Dice相似系数0.854，在二元分类中的AUC为0.766。然而，在未知数据集上（乳腺癌分子分型）的性能显著下降，从内部验证的0.571降至外部中心的0.508。", "conclusion": "通用AI模型在多任务中表现出色，但需要解决域泛化问题以实现临床应用。"}}
{"id": "2512.17278", "pdf": "https://arxiv.org/pdf/2512.17278", "abs": "https://arxiv.org/abs/2512.17278", "authors": ["Guoping Cai", "Houjin Chen", "Yanfeng Li", "Jia Sun", "Ziwei Chen", "Qingzi Geng"], "title": "WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.", "AI": {"tldr": "WDFFU-Mamba模型用于乳腺肿瘤的超声图像分割，通过结合小波域增强和注意力融合方法提高准确性。", "motivation": "解决乳腺超声图像分割中的噪声、伪影等问题以实现更准确的肿瘤检测与诊断。", "method": "提出一种名为WDFFU-Mamba的新模型，在Mamba架构基础上加入WHF模块用于低级特征增强，并通过DAFF模块合并跳跃连接和语义特征，提升上下文一致性。", "result": "实验表明，WDFFU-Mamba在Dice系数和HD95等指标上优于现有方法，表现出色并具有良好的泛化能力。", "conclusion": "提出的WDFFU-Mamba模型提高了乳腺肿瘤超声图像分割的准确性与鲁棒性，在临床应用中有很大潜力。"}}
{"id": "2512.17270", "pdf": "https://arxiv.org/pdf/2512.17270", "abs": "https://arxiv.org/abs/2512.17270", "authors": ["Yongqi Li", "Hao Lang", "Fei Huang", "Tieyun Qian", "Yongbin Li"], "title": "Understanding Generalization in Role-Playing Models via Information Theory", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.", "AI": {"tldr": "通过信息理论提出一种新的度量标准R-EMID，用于评估角色扮演游戏模型（RPM）在不同分布下的表现下降，并探讨各种因素对RPM泛化能力的影响。", "motivation": "现有方法无法提供细粒度的诊断来解释分布变化如何影响RPM的表现。因此，缺乏正式框架来表征RPM的泛化行为。为解决这些问题，本文旨在提供一种新的度量标准和框架以更好地理解和提升角色扮演游戏模型（RPM）在不同环境下的表现。", "method": "引入基于推理的有效互信息差异(R-EMID)作为衡量RPM性能下降的新指标，并提出一种协同演化的强化学习框架来模拟用户、角色和对话上下文之间的关联，从而提高对对话响应生成概率的估计准确性。", "result": "通过使用R-EMID评估不同角色扮演游戏模型（RPM）的表现，发现用户变化是所有变动中风险最大的因素。同时证明了强化学习是最有效的提升RPM泛化能力的方法。", "conclusion": "本文提出了一种新的度量标准和框架来理解并提高角色扮演游戏模型（RPM）的泛化能力，展示了在面对分布变化时不同因素的影响，并表明了使用基于推理的有效互信息差异(R-EMID)评估表现的重要性。"}}
{"id": "2512.17268", "pdf": "https://arxiv.org/pdf/2512.17268", "abs": "https://arxiv.org/abs/2512.17268", "authors": ["Matthias Bentert", "Fedor v. Fomin", "Petr A. Golovach", "Souvik Saha", "Sanjay Seetharaman", "Anannya Upasana"], "title": "Line Cover and Related Problems", "categories": ["cs.CG", "cs.DS"], "comment": "Accepted to STACS 2026", "summary": "We study extensions of the classic \\emph{Line Cover} problem, which asks whether a set of $n$ points in the plane can be covered using $k$ lines. Line Cover is known to be NP-hard, and we focus on two natural generalizations. The first is \\textbf{Line Clustering}, where the goal is to find $k$ lines minimizing the sum of squared distances from the input points to their nearest line. The second is \\textbf{Hyperplane Cover}, which asks whether $n$ points in $\\mathbb{R}^d$ can be covered by $k$ hyperplanes. We also study the more general \\textbf{Projective Clustering} problem, which unifies both settings and has applications in machine learning, data analysis, and computational geometry. In this problem, one seeks $k$ affine subspaces of dimension $r$ that minimize the sum of squared distances from the given points in $\\mathbb{R}^d$ to the nearest subspace. Our results reveal notable differences in the parameterized complexity of these problems. While Line Cover is fixed-parameter tractable when parameterized by $k$, we show that Line Clustering is W[1]-hard with respect to $k$ and does not admit an algorithm with running time $n^{o(k)}$ unless the Exponential Time Hypothesis fails. Hyperplane Cover is NP-hard even for $d=2$, and prior work of Langerman and Morin [Discrete & Computational Geometry, 2005] showed that it is fixed-parameter tractable when parameterized by both $k$ and $d$. We complement this by proving that Hyperplane Cover is W[2]-hard when parameterized by $k$ alone. Finally, we present an algorithm for Projective Clustering running in $n^{O(dk(r+1))}$ time. This bound matches our lower bound for Line Clustering and generalizes the classic algorithm for $k$-Means Clustering ($r=0$) by Inaba, Katoh, and Imai [SoCG 1994].", "AI": {"tldr": "研究了经典平面点集直线覆盖问题的两个自然推广，分别是最小化平方距离和超平面覆盖，并提出了参数复杂度分析。", "motivation": "探讨了Line Cover问题及其扩展在数据处理、机器学习等领域的应用，特别是它们的参数复杂性差异。", "method": "通过研究不同条件下线簇、超平面覆盖及投影聚类等问题的算法性能和复杂性界限。", "result": "表明线簇问题是W[1]难解且不具有亚指数时间算法；超平面覆盖在仅参数化k的情况下是W[2]难解，但在同时参数化k和d时则是可固定参数求解的。提出了一个适用于投影聚类问题的新算法。", "conclusion": "揭示了各种几何覆盖与聚类问题之间的复杂性区别，并提供了解决这些问题的有效方法论框架。"}}
{"id": "2512.17267", "pdf": "https://arxiv.org/pdf/2512.17267", "abs": "https://arxiv.org/abs/2512.17267", "authors": ["Michael J. Ryan", "Yanzhe Zhang", "Amol Salunkhe", "Yi Chu", "Di Xu", "Diyi Yang"], "title": "AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating user-facing AI applications remains a central challenge, especially in open-ended domains such as travel planning, clinical note generation, or dialogue. The gold standard is user feedback (e.g., thumbs up/down) or behavioral signals (e.g., retention), but these are often scarce in prototypes and research projects, or too-slow to use for system optimization. We present AutoMetrics, a framework for synthesizing evaluation metrics under low-data constraints. AutoMetrics combines retrieval from MetricBank, a collection of 48 metrics we curate, with automatically generated LLM-as-a-Judge criteria informed by lightweight human feedback. These metrics are composed via regression to maximize correlation with human signal. AutoMetrics takes you from expensive measures to interpretable automatic metrics. Across 5 diverse tasks, AutoMetrics improves Kendall correlation with human ratings by up to 33.4% over LLM-as-a-Judge while requiring fewer than 100 feedback points. We show that AutoMetrics can be used as a proxy reward to equal effect as a verifiable reward. We release the full AutoMetrics toolkit and MetricBank to accelerate adaptive evaluation of LLM applications.", "AI": {"tldr": "该论文提出了一个框架AutoMetrics，用于在低数据条件下合成评估指标，通过结合从MetricBank检索的度量和自动生成的LLM作为裁判的标准来提高与人类评价的相关性。", "motivation": "在原型开发和研究项目中，用户反馈或行为信号通常稀缺且难以获取。为了优化系统性能并加速LLM应用程序的适应评估，需要一个能够快速生成可靠评估指标的方法。", "method": "AutoMetrics结合从MetricBank检索的度量标准与自动生成的LLM作为裁判的标准，通过线性回归来最大化与人类评价的相关性，并将昂贵的人类反馈转化为可解释的自动化度量。", "result": "在五个不同的任务上，AutoMetrics将Kendall相关系数提高了33.4%，并在需要少于100个反馈点的情况下实现了这一点。此外，它还展示了作为替代奖励的有效使用，效果与验证奖励相当。", "conclusion": "AutoMetrics能够加速适应评估并提高LLM应用程序的性能，其工具包和MetricBank已经开放以供更广泛应用。"}}
{"id": "2512.17266", "pdf": "https://arxiv.org/pdf/2512.17266", "abs": "https://arxiv.org/abs/2512.17266", "authors": ["Miru Hong", "Minho Lee", "Geonhee Jo", "Jae-Hee So", "Pascal Bauer", "Sang-Ki Ko"], "title": "ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework", "categories": ["cs.AI"], "comment": "8 pages, 2 figures, 7 tables. To appear in Hudl Performance Insights 2025", "summary": "Transfers play a pivotal role in shaping a football club's success, yet forecasting whether a transfer will succeed remains difficult due to the strong context-dependence of on-field performance. Existing evaluation practices often rely on static summary statistics or post-hoc value models, which fail to capture how a player's contribution adapts to a new tactical environment or different teammates. To address this gap, we introduce EventGPT, a player-conditioned, value-aware next-event prediction model built on a GPT-style autoregressive transformer. Our model treats match play as a sequence of discrete tokens, jointly learning to predict the next on-ball action's type, location, timing, and its estimated residual On-Ball Value (rOBV) based on the preceding context and player identity. A key contribution of this framework is the ability to perform counterfactual simulations. By substituting learned player embeddings into new event sequences, we can simulate how a player's behavioral distribution and value profile would change when placed in a different team or tactical structure. Evaluated on five seasons of Premier League event data, EventGPT outperforms existing sequence-based baselines in next-event prediction accuracy and spatial precision. Furthermore, we demonstrate the model's practical utility for transfer analysis through case studies-such as comparing striker performance across different systems and identifying stylistic replacements for specific roles-showing that our approach provides a principled method for evaluating transfer fit.", "AI": {"tldr": "本文提出了EventGPT模型，通过使用基于GPT的框架来预测球员在新球队中的表现。", "motivation": "现有的评估方法无法有效捕捉球员适应新战术环境的能力。因此，提出了一种新的预测模型以解决此问题。", "method": "EventGPT是一个基于GPT的自回归变压器模型，用于学习基于前序上下文和球员身份预测下一个球动作类型的任务，并具备反事实模拟能力。", "result": "在英超联赛的五季事件数据上进行评估时，EventGPT的表现优于现有的序列基线方法，在下一次事件预测准确性和空间精度方面表现出色。", "conclusion": "本文提出的模型提供了一种原则性的方式来评价转会适应性，并通过案例研究展示了其实际应用价值。"}}
{"id": "2512.17263", "pdf": "https://arxiv.org/pdf/2512.17263", "abs": "https://arxiv.org/abs/2512.17263", "authors": ["Dong Zifei", "Wu Wenjie", "Hao Jinkui", "Chen Tianqi", "Weng Ziqiao", "Zhou Bo"], "title": "AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning", "categories": ["cs.CV"], "comment": "20 pages, 12 figures, Preprint (under review at Medical Image Analysis)", "summary": "Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.", "AI": {"tldr": "提出AnyCXR框架，使用合成数据进行胸部X光图像的多器官分割", "motivation": "由于真实世界中注释稀缺和获取条件的巨大差异，对胸部X光图像的解剖结构进行鲁棒性分割仍然具有挑战性。", "method": "结合多阶段域随机化（MSDR）引擎生成高多样性的人体解剖合成放射影像，并使用条件联合标注正则化（CAR）策略利用不完整注释以保证解剖一致性。", "result": "AnyCXR在多种真实世界数据集上实现了强零样本泛化，提供了准确的解剖结构分割地图，支持下游临床任务。", "conclusion": "AnyCXR提供了一个可扩展和可靠的胸部X光分析基础，并通过减少标注负担提高了对各种成像条件的鲁棒性。"}}
{"id": "2512.17259", "pdf": "https://arxiv.org/pdf/2512.17259", "abs": "https://arxiv.org/abs/2512.17259", "authors": ["Abhivansh Gupta"], "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.", "AI": {"tldr": "提出了一种可验证优先架构，以确保LLM代理的可控性、可审计性和对部署者意图的忠实度。", "motivation": "随着基于LLM的代理变得更加自主和多模态，保证其可控性、可审计性和对部署者意图的忠实度变得至关重要。先前的研究表明，代理个性和工具访问显著影响行为偏差。", "method": "提出了一个集成运行时代理行动认证的技术架构，并嵌入了轻量级审核代理以持续验证意图与行为之间的差异；同时还制定了挑战响应验证协议用于高风险操作。引入了一套评测基准OPERA（可观察性、可信执行、红队攻击和认证）来衡量检测偏离的能力。", "result": "该方法将评估重点从如何预防行为偏差转变为快速可靠地发现并修复偏差。", "conclusion": "通过实时验证代理的行为，可以有效提升LLM系统对恶意操作的抵御能力，并确保其运行符合预期。"}}
{"id": "2512.17255", "pdf": "https://arxiv.org/pdf/2512.17255", "abs": "https://arxiv.org/abs/2512.17255", "authors": ["Quan Do", "Caroline Ahn", "Leah Bakst", "Michael Pascale", "Joseph T. McGuire", "Chantal E. Stern", "Michael E. Hasselmo"], "title": "From Priors to Predictions: Explaining and Visualizing Human Reasoning in a Graph Neural Network Framework", "categories": ["q-bio.NC", "cs.AI"], "comment": "44 pages, 7 figures, 3 suppl figures", "summary": "Humans excel at solving novel reasoning problems from minimal exposure, guided by inductive biases, assumptions about which entities and relationships matter. Yet the computational form of these biases and their neural implementation remain poorly understood. We introduce a framework that combines Graph Theory and Graph Neural Networks (GNNs) to formalize inductive biases as explicit, manipulable priors over structure and abstraction. Using a human behavioral dataset adapted from the Abstraction and Reasoning Corpus (ARC), we show that differences in graph-based priors can explain individual differences in human solutions. Our method includes an optimization pipeline that searches over graph configurations, varying edge connectivity and node abstraction, and a visualization approach that identifies the computational graph, the subset of nodes and edges most critical to a model's prediction. Systematic ablation reveals how generalization depends on specific prior structures and internal processing, exposing why human like errors emerge from incorrect or incomplete priors. This work provides a principled, interpretable framework for modeling the representational assumptions and computational dynamics underlying generalization, offering new insights into human reasoning and a foundation for more human aligned AI systems.", "AI": {"tldr": "该论文提出了一个结合图论和图神经网络的框架，用于解释并可视化人类推理过程。", "motivation": "人类能够从少量接触中解决新的推理问题，并受到归纳偏置和关于哪些实体和关系重要的假设引导。然而这些偏置的计算形式及其在大脑中的实现仍然不清楚。为此研究者提出了一个结合图论和图神经网络的框架来理解人类推理。", "method": "通过引入基于图形的先验，该方法使用了优化流水线搜索不同的图形配置，并且开发了一种可视化方法以识别对于模型预测最重要的计算图部分。", "result": "系统化消融实验揭示了概括依赖于特定的先前结构和内部处理，这解释了人类类似错误是如何从不正确的或不完备的先验中产生的。", "conclusion": "这项工作提供了一个原则性和可解释性的框架来建模推广的基本假设和计算动态过程，并提供了对人类推理的新见解，同时也为更符合人类的人工智能系统奠定了基础。"}}
{"id": "2512.17253", "pdf": "https://arxiv.org/pdf/2512.17253", "abs": "https://arxiv.org/abs/2512.17253", "authors": ["Yiren Song", "Cheng Liu", "Weijia Mao", "Mike Zheng Shou"], "title": "Mitty: Diffusion-based Human-to-Robot Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.", "AI": {"tldr": "Mitty是一种基于扩散的Transformer模型，用于从人类演示视频生成机器人执行视频。", "motivation": "现有方法在将人类演示转换为机器人执行时会损失信息和累积误差，影响时间和视觉一致性。因此，需要一种直接从人类演示视频学习的方法来实现可扩展且通用化的机器人学习。", "method": "Mitty基于预训练的扩散模型，利用强视觉-时间先验将人类演示压缩成条件令牌，并通过双向注意力与机器人的去噪令牌融合以生成执行视频。此外还开发了自动合成管道以缓解配对数据稀缺问题。", "result": "实验显示，Mitty在Human2Robot和EPIC-Kitchens上的表现优于现有方法，展现出强大的泛化能力，提供了可扩展机器人学习的新见解。", "conclusion": "Mitty通过直接从人类演示视频生成高质量的机器人执行视频，在减少信息损失的同时提高了模型性能。"}}
{"id": "2512.17251", "pdf": "https://arxiv.org/pdf/2512.17251", "abs": "https://arxiv.org/abs/2512.17251", "authors": ["Madhava Gaikwad"], "title": "AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: LOCK-LLM Work-shop, NeurIPS 2025", "summary": "Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.", "AI": {"tldr": "设计了一种名为AlignDP的混合隐私保护方法，用于大型语言模型的数据接口，以防止知识转移。", "motivation": "现有防御措施如水印和监控在数据泄露后才起作用。需要一种新的方法来阻止在训练过程中知识的传递，从而更好地保护大型语言模型的安全。", "method": "将罕见字段与非罕见字段分离处理：罕见字段通过PAC不区分性屏蔽实现零epsilon局部差分隐私；非罕见字段使用RAPPOR进行差分隐私化。一个全局聚合器负责组合和预算管理。这种两层设计隐藏了罕见事件，对频繁事件添加受控噪声。", "result": "证明了PAC扩展到全局聚合的限制，并为RAPPOR估计提供了边界分析；通过玩具模拟验证可行性：罕见类别保持隐蔽，而频繁类别可以以小误差恢复。", "conclusion": "AlignDP是一种有效的混合隐私保护方案，在大型语言模型中提供数据接口级别的知识保护，同时保证了实用性的权衡。"}}
{"id": "2512.17250", "pdf": "https://arxiv.org/pdf/2512.17250", "abs": "https://arxiv.org/abs/2512.17250", "authors": ["Ziyang Lin", "Zixuan Sun", "Sanhorn Chen", "Xiaoyang Chen", "Roy Zhao"], "title": "Accelerating Multi-modal LLM Gaming Performance via Input Prediction and Mishit Correction", "categories": ["cs.AI"], "comment": "UIUC 25 Fall CS 498", "summary": "Real-time sequential control agents are often bottlenecked by inference latency. Even modest per-step planning delays can destabilize control and degrade overall performance. We propose a speculation-and-correction framework that adapts the predict-then-verify philosophy of speculative execution to model-based control with TD-MPC2. At each step, a pretrained world model and latent-space MPC planner generate a short-horizon action queue together with predicted latent rollouts, allowing the agent to execute multiple planned actions without immediate replanning. When a new observation arrives, the system measures the mismatch between the encoded real latent state and the queued predicted latent. For small to moderate mismatch, a lightweight learned corrector applies a residual update to the speculative action, distilled offline from a replanning teacher. For large mismatch, the agent safely falls back to full replanning and clears stale action queues. We study both a gated two-tower MLP corrector and a temporal Transformer corrector to address local errors and systematic drift. Experiments on the DMC Humanoid-Walk task show that our method reduces the number of planning inferences from 500 to 282, improves end-to-end step latency by 25 percent, and maintains strong control performance with only a 7.1 percent return reduction. Ablation results demonstrate that speculative execution without correction is unreliable over longer horizons, highlighting the necessity of mismatch-aware correction for robust latency reduction.", "AI": {"tldr": "提出了一个预测和修正框架，以减少多模态LLM游戏中的推理延迟。", "motivation": "实时顺序控制代理经常受到推理延迟的限制。即使轻微的每步规划延迟也可能使控制不稳定并降低整体性能。", "method": "利用预训练的世界模型和潜在空间MPC计划器生成短时域动作队列及预测潜在轨迹，允许代理执行多个计划行动而无需立即重新规划。当新观察到来时，系统会测量实际潜在状态与预测状态之间的不匹配，并根据此差异应用轻量级修正。", "result": "实验表明该方法将规划推断次数从500次减少到282次，提高端到端步骤延迟25％，同时保持良好的控制性能。没有修正的预测执行在较长的时间范围内是不可靠的，说明了不匹配感知修正对于可靠延迟降低的重要性。", "conclusion": "该研究证明了利用预测和修正框架有效减少多模态LLM游戏中的推理延迟，并提高了整体控制效率。"}}
{"id": "2512.17247", "pdf": "https://arxiv.org/pdf/2512.17247", "abs": "https://arxiv.org/abs/2512.17247", "authors": ["Zahra Rahmani", "Hossein Sameti"], "title": "Incorporating Error Level Noise Embedding for Improving LLM-Assisted Robustness in Persian Speech Recognition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic Speech Recognition (ASR) systems suffer significant performance degradation in noisy environments, a challenge that is especially severe for low-resource languages such as Persian. Even state-of-the-art models such as Whisper struggle to maintain accuracy under varying signal-to-noise ratios (SNRs). This study presents a robust noise-sensitive ASR error correction framework that combines multiple hypotheses and noise-aware modeling. Using noisy Persian speech, we generate 5-best hypotheses from a modified Whisper-large decoder. Error Level Noise (ELN) is introduced as a representation that captures semantic- and token-level disagreement across hypotheses, quantifying the linguistic distortions caused by noise. ELN thus provides a direct measure of noise-induced uncertainty, enabling the LLM to reason about the reliability of each hypothesis during correction. Three models are evaluated: (1) a base LLaMA-2-7B model without fine-tuning, (2) a fine-tuned variant trained on text-only hypotheses, and (3) a noise-conditioned model integrating ELN embeddings at both sentence and word levels. Experimental results demonstrate that the ELN-conditioned model achieves substantial reductions in Word Error Rate (WER). Specifically, on the challenging Mixed Noise test set, the proposed Fine-tuned + ELN (Ours) model reduces the WER from a baseline of 31.10\\% (Raw Whisper) to 24.84\\%, significantly surpassing the Fine-tuned (No ELN) text-only baseline of 30.79\\%, whereas the original LLaMA-2-7B model increased the WER to 64.58\\%, demonstrating that it is unable to correct Persian errors on its own. This confirms the effectiveness of combining multiple hypotheses with noise-aware embeddings for robust Persian ASR in noisy real-world scenarios.", "AI": {"tldr": "本文提出了一种结合多假设和噪声感知模型的鲁棒性错误校正框架，以提高波斯语语音识别系统在嘈杂环境下的性能。", "motivation": "自动语音识别（ASR）系统在嘈杂环境中表现不佳，特别是在像波斯语这样的低资源语言中。即使最先进的模型也难以保持准确性，因此需要一种新的方法来改善这种状况。", "method": "利用带噪声的波斯语语音生成5个最佳假设，并引入错误级别噪声（ELN），捕捉不同假设之间的语义和标记级差异，量化噪声引起的语言扭曲。使用三个不同的模型进行评估：未经微调的基本LLaMA-2-7B模型、仅基于文本假设的微调变体以及整合了ELN嵌入的噪声条件化模型。", "result": "实验结果显示，引入了ELN条件化的模型在Mixed Noise测试集上的词错误率（WER）从基线31.10%降至24.84%，显著优于无ELN仅基于文本假设的微调基准模型30.79%。原始LLaMA-2-7B模型则将WER提升至64.58%，表明其无法独立校正波斯语错误。", "conclusion": "结合多个假设和噪声感知嵌入的方法能有效提高嘈杂环境下的鲁棒性波斯语音识别性能，为解决实际场景中的挑战提供了新思路。"}}
{"id": "2512.17241", "pdf": "https://arxiv.org/pdf/2512.17241", "abs": "https://arxiv.org/abs/2512.17241", "authors": ["Suraj Nukala", "Meera Sushma", "Leimin Tian", "Akansel Cosgun", "Dana Kulic"], "title": "A Service Robot's Guide to Interacting with Busy Customers", "categories": ["cs.RO", "cs.HC"], "comment": "Presented at ACRA 2025. 10 pages, 4 figures. Includes a user study (N=24) using the Temi robot evaluating speech, visual, and micromotion modalities", "summary": "The growing use of service robots in hospitality highlights the need to understand how to effectively communicate with pre-occupied customers. This study investigates the efficacy of commonly used communication modalities by service robots, namely, acoustic/speech, visual display, and micromotion gestures in capturing attention and communicating intention with a user in a simulated restaurant scenario. We conducted a two-part user study (N=24) using a Temi robot to simulate delivery tasks, with participants engaged in a typing game (MonkeyType) to emulate a state of busyness. The participants' engagement in the typing game is measured by words per minute (WPM) and typing accuracy. In Part 1, we compared non-verbal acoustic cue versus baseline conditions to assess attention capture during a single-cup delivery task. In Part 2, we evaluated the effectiveness of speech, visual display, micromotion and their multimodal combination in conveying specific intentions (correct cup selection) during a two-cup delivery task. The results indicate that, while speech is highly effective in capturing attention, it is less successful in clearly communicating intention. Participants rated visual as the most effective modality for intention clarity, followed by speech, with micromotion being the lowest ranked.These findings provide insights into optimizing communication strategies for service robots, highlighting the distinct roles of attention capture and intention communication in enhancing user experience in dynamic hospitality settings.", "AI": {"tldr": "研究探讨了服务机器人在忙碌顾客面前的交流效果，通过模拟餐厅场景中的送餐任务来评估语音、视觉展示和微动作手势这三种沟通方式的有效性。", "motivation": "随着服务机器人的广泛使用，在餐饮等动态环境中有效与忙碌客户进行沟通变得尤为重要。本文旨在了解不同通信模态如何吸引注意并传达意图。", "method": "研究分为两部分，利用Temi机器人模拟送餐任务，并让参与者在玩打字游戏的同时完成任务。通过比较非言语声学提示和基线条件来评估注意力捕捉效果；并在多杯任务中评测语音、视觉展示、微动作以及它们的组合传达意图的效果。", "result": "结果显示，虽然语音最能吸引注意，但在清晰传达意图方面不如视觉展示有效。参与者认为视觉是最有效的沟通模态，其次是语音，而微动作排名最低。", "conclusion": "研究发现为服务机器人优化沟通策略提供了见解，强调了捕捉注意力与明确传达意图在改善用户体验中的不同作用"}}
