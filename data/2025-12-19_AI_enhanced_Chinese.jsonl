{"id": "2512.16924", "pdf": "https://arxiv.org/pdf/2512.16924", "abs": "https://arxiv.org/abs/2512.16924", "authors": ["Hanlin Wang", "Hao Ouyang", "Qiuyu Wang", "Yue Yu", "Yihao Meng", "Wen Wang", "Ka Leong Cheng", "Shuailei Ma", "Qingyan Bai", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Xing Zhu", "Yujun Shen", "Qifeng Chen"], "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text", "categories": ["cs.CV"], "comment": "Project page and code: https://worldcanvas.github.io/", "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.", "AI": {"tldr": "世界是你的画布，通过结合文本、轨迹和参考图像生成可控的事件视频。", "motivation": "为了提高模拟的真实性和交互性，提出了一种多模态方法来生成包含多代理交互、物体进出等复杂场景的视频。", "method": "WorldCanvas框架通过整合语义意图（文本）、运动信息（轨迹）和视觉参考（图像），使用户能够控制事件的发生与演变。", "result": "生成的视频不仅具有时间连贯性，还保持了对象身份的一致性和场景稳定性。", "conclusion": "WorldCanvas将世界模型从被动预测器转变为互动式、可塑性的模拟器，极大地增强了用户体验和创造力。"}}
{"id": "2512.16923", "pdf": "https://arxiv.org/pdf/2512.16923", "abs": "https://arxiv.org/abs/2512.16923", "authors": ["Chun-Wei Tuan Mu", "Jia-Bin Huang", "Yu-Lun Liu"], "title": "Generative Refocusing: Flexible Defocus Control from a Single Image", "categories": ["cs.CV"], "comment": "Project website: https://generative-refocusing.github.io/", "summary": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.", "AI": {"tldr": "本文介绍了一种名为Generative Refocusing的技术，该技术可以从单张图片中实现灵活的景深控制。", "motivation": "目前的单图景深调整方法存在诸如需要全焦图像输入、依赖仿真数据以及对光圈控制有限等缺点。为了克服这些限制并实现在单一图像上进行精确且具有高度可控性的景深操作，作者提出了新的技术方案。", "method": "该方法包括两个主要步骤：首先通过DeblurNet从各种输入中恢复全焦图像；然后使用BokehNet合成可控制的模糊效果。创新之处在于采用半监督训练方式，结合了配对的合成数据与未配对的真实模糊图片，并利用EXIF元数据捕捉真实光学特性。", "result": "实验结果表明，该方法在景深去模糊、散景合成以及再聚焦基准测试中均取得了最佳性能。此外，还支持基于文本指导调整和定制光圈形状的功能。", "conclusion": "通过使用半监督训练并结合多种数据类型，Generative Refocusing能够有效解决单图景深控制中的挑战，并且提供更灵活、可控的效果。"}}
{"id": "2512.16922", "pdf": "https://arxiv.org/pdf/2512.16922", "abs": "https://arxiv.org/abs/2512.16922", "authors": ["Sihan Xu", "Ziqiao Ma", "Wenhao Chai", "Xuweiyi Chen", "Weiyang Jin", "Joyce Chai", "Saining Xie", "Stella X. Yu"], "title": "Next-Embedding Prediction Makes Strong Vision Learners", "categories": ["cs.CV"], "comment": "Project Page: https://sihanxu.me/nepa", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "AI": {"tldr": "本文探索了通过预测未来补丁嵌入来训练视觉模型的新方法，以替代传统的自我监督学习。", "motivation": "受到自然语言生成预训练成功的启发，作者探讨了一种基于预测未来补丁嵌入的新的自我监督学习方式，旨在简化模型设计并提高其在不同任务上的表现。", "method": "提出了Next-Embedding Predictive Autoregression (NEPA) 方法，通过因果掩蔽和停止梯度技术训练Transformer模型来预测未来的补丁嵌入。该方法仅使用图像内核作为预训练目标，不涉及像素重建、离散标记、对比损失或特定任务的头部。", "result": "实验结果显示，在ImageNet-1K数据集上，使用ViT-B和ViT-L骨干网络经过微调后分别达到了83.8% 和 85.3% 的top-1 准确率。此外，该方法还能有效转移至ADE20K上的语义分割任务。", "conclusion": "NEPA提供了一种简单、可扩展且可能适用于多种模态的视觉自我监督学习替代方案"}}
{"id": "2512.16921", "pdf": "https://arxiv.org/pdf/2512.16921", "abs": "https://arxiv.org/abs/2512.16921", "authors": ["Qihao Liu", "Chengzhi Mao", "Yaojie Liu", "Alan Yuille", "Wen-Sheng Chu"], "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification", "categories": ["cs.CV", "cs.AI"], "comment": "project page: https://auditdm.github.io/", "summary": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.", "AI": {"tldr": "该论文介绍了AuditDM，一个通过增强学习训练的自动化框架，用于发现和修复多模态大型语言模型（MMLM）的能力差距。", "motivation": "传统的评估方法在解释性方面不足，并不能充分揭示多模态LLM之间的能力差异。为了克服这一问题，提出了AuditDM来自动检测并解决这些模型中的性能缺陷。", "method": "AuditDM通过增强学习训练一个MMLM作为审计员，生成能最大化目标模型间分歧的问题和反事实图像。经过训练的审计员可以揭示出多种不同的模型弱点，并为修复提供无标注的数据。", "result": "应用AuditDM到最先进的模型（如Gemma-3和PaliGemma-2）中，发现了超过20种不同的失败模式。通过这些发现进行微调后，所有模型在16个基准上的表现都有所提升，甚至使得一个较小的模型超越了一个较大的竞争对手。", "conclusion": "研究表明，在数据扩展到达边际效应时，针对性地对模型进行审计是一种诊断和改进的有效途径。"}}
{"id": "2512.16920", "pdf": "https://arxiv.org/pdf/2512.16920", "abs": "https://arxiv.org/abs/2512.16920", "authors": ["Jinjie Mai", "Chaoyang Wang", "Guocheng Gordon Qian", "Willi Menapace", "Sergey Tulyakov", "Bernard Ghanem", "Peter Wonka", "Ashkan Mirzaei"], "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://snap-research.github.io/easyv2v/", "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/", "AI": {"tldr": "EasyV2V提出了一种基于指令的高质量视频编辑框架，旨在解决视频一致性、控制和泛化的问题。", "motivation": "由于图像编辑技术快速发展，而视频编辑仍面临挑战，包括一致性、可控性和泛化性。为了促进视频编辑领域的发展，本文研究了数据、架构和控制的设计空间，并引入了一种简单有效的基于指令的视频编辑框架EasyV2V。", "method": "在数据方面，构建了多样的视频对以训练模型；在模型设计上，利用预训练的语言到视频模型进行编辑任务，并通过简单的序列拼接和轻量级LoRA微调来提升模型性能。对于控制部分，使用单一掩码机制实现空间时间控制并支持参考图像。", "result": "EasyV2V框架实现了高质量的视频编辑结果，超过了同期的其他系统以及商业系统的表现。", "conclusion": "通过提出易用且高效的基于指令的视频编辑方法EasyV2V，本研究证明了在处理复杂视频编辑任务时灵活性和性能的重要性。"}}
{"id": "2512.16919", "pdf": "https://arxiv.org/pdf/2512.16919", "abs": "https://arxiv.org/abs/2512.16919", "authors": ["Sicheng Zuo", "Zixun Xie", "Wenzhao Zheng", "Shaoqing Xu", "Fang Li", "Shengyin Jiang", "Long Chen", "Zhi-Xin Yang", "Jiwen Lu"], "title": "DVGT: Driving Visual Geometry Transformer", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Code is available at https://github.com/wzzheng/DVGT", "summary": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.", "AI": {"tldr": "提出了一种从多视角图像序列重建全局稠密三维点云的方法DVGT，适用于不同的驾驶场景和相机配置。", "motivation": "当前缺乏针对自动驾驶的密集几何感知模型，无法适应不同场景和相机设置。为此，提出了DVGT来弥补这一空白。", "method": "使用DINO主干提取每个图像的视觉特征，并采用交替的局部注意、跨视图空间注意以及跨帧时间注意来推断图像间的几何关系；然后通过多个头解码全局点云并预测各帧的相机姿态。", "result": "在nuScenes，OpenScene，Waymo，KITTI和DDAD等数据集上训练后，DVGT在各种场景中显著优于现有模型。", "conclusion": "提出的方法无需依赖精确的相机参数或外部传感器即可直接从图像序列预测出度量尺度下的几何结构。"}}
{"id": "2512.16918", "pdf": "https://arxiv.org/pdf/2512.16918", "abs": "https://arxiv.org/abs/2512.16918", "authors": ["Chaoyang Wang", "Kaituo Feng", "Dongyang Chen", "Zhongyu Wang", "Zhixun Li", "Sicheng Gao", "Meng Meng", "Xu Zhou", "Manyuan Zhang", "Yuzhang Shang", "Xiangyu Yue"], "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos", "categories": ["cs.CV"], "comment": "Project page: https://github.com/CYWang735/AdaTooler-V", "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.", "AI": {"tldr": "提出了一种适应性工具使用的多模态大语言模型AdaTooler-V，用于图像和视频任务。", "motivation": "现有的开源模型在使用视觉工具时存在盲目调用的问题，这增加了推理开销并降低了性能。因此需要一种能够根据实际情况判断是否需要调用工具的机制来提高模型效率。", "method": "引入了一种基于强化学习的AT-GRPO算法，通过调整奖励尺度鼓励模型仅在视觉问题确实需要工具时进行调用，并构建了两个数据集以支持训练：AdaTooler-V-CoT-100k和AdaTooler-V-300k。", "result": "实验结果表明，在十二个基准测试中，AdaTooler-V展示了强大的推理能力。在高分辨率基准V*上，AdaTooler-V-7B的准确率达到了89.8%，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。", "conclusion": "提出的Adaptive Tooler-V有效地解决了盲目调用视觉工具的问题，并提高了多模态语言模型在各种视觉推理任务上的性能。"}}
{"id": "2512.16917", "pdf": "https://arxiv.org/pdf/2512.16917", "abs": "https://arxiv.org/abs/2512.16917", "authors": ["Qihao Liu", "Luoxin Ye", "Wufei Ma", "Yu-Cheng Chou", "Alan Yuille"], "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.", "AI": {"tldr": "通过对抗性增强学习框架提升大型语言模型的推理能力。", "motivation": "尽管具有显式推理能力的语言模型在数学推理方面表现优秀，但仍存在过程错误。因此提出了一种基于对抗强化学习的联合训练框架来改进这一问题。", "method": "设计了生成对抗理由器，通过分片逻辑完整性的计算高效审查计划和结构化评价，使语言模型推理器与基于语言模型的判别器相互进化。这种方法利用互补信号提高信用分配，增加样本效率，提升整体推理质量。", "result": "在多个数学基准上实现了超过强基线的标准RL后训练方法的一致性改进。具体来说，在AIME24上将DeepSeek-R1-Distill-Qwen-7B从54.0提升至61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7（+10.0）。", "conclusion": "所提出的方法通过对抗性增强学习框架有效提升了语言模型的推理质量，并且判别器模块还支持灵活的目标奖励塑造，适用于多种任务如教师蒸馏、偏好对齐等。"}}
{"id": "2512.16915", "pdf": "https://arxiv.org/pdf/2512.16915", "abs": "https://arxiv.org/abs/2512.16915", "authors": ["Guibao Shen", "Yihua Du", "Wenhang Ge", "Jing He", "Chirui Chang", "Donghao Zhou", "Zhen Yang", "Luozhou Wang", "Xin Tao", "Ying-Cong Chen"], "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.", "AI": {"tldr": "本文提出了StereoPilot，一种用于单眼到立体视频转换的高效模型。", "motivation": "当前自动立体化面临多阶段流水线导致的错误累积、深度模糊和格式不一致问题。因此，研究提出了一种新的方法来解决这些问题。", "method": "建立一个大型统一数据集UniStereo，并提出一种直接合成目标视图的高效前向模型StereoPilot，无需依赖显式深度地图或迭代扩散采样。", "result": "实验显示，StereoPilot在视觉保真度和计算效率方面均显著优于现有方法。", "conclusion": "StereoPilot为单眼到立体视频转换提供了一种高效的解决方案。"}}
{"id": "2512.16913", "pdf": "https://arxiv.org/pdf/2512.16913", "abs": "https://arxiv.org/abs/2512.16913", "authors": ["Xin Lin", "Meixi Song", "Dizhe Zhang", "Wenxuan Lu", "Haodong Li", "Bo Du", "Ming-Hsuan Yang", "Truong Nguyen", "Lu Qi"], "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation", "categories": ["cs.CV"], "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/", "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}", "AI": {"tldr": "该论文提出了一种用于全景深度估计的通用模型，能够跨不同场景距离泛化。", "motivation": "为了处理室内和室外以及合成与真实数据之间的领域差距，并提高模型在各种距离下的鲁棒性，作者开发了这个新的全景度量深度基础模型。", "method": "论文采用DINOv3-Large作为骨干网络，并引入了一个即插即用的范围掩码头、以清晰度为中心的优化和几何一致性的优化方法。", "result": "实验结果显示，在多个基准测试（如Stanford2D3D, Matterport3D和Deep360）上，该模型表现出强大的性能和零样本泛化能力，并在各种真实场景中提供了稳健且稳定的度量预测。", "conclusion": "该研究提出了一种新的全景深度估计方法，通过结合大规模数据集、可靠的伪标签生成和增强的模型结构设计，在多个基准测试中展示了出色的性能。"}}
{"id": "2512.16912", "pdf": "https://arxiv.org/pdf/2512.16912", "abs": "https://arxiv.org/abs/2512.16912", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Wotao Yin", "Xi Chen", "Tianyi Lin"], "title": "Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "35 pages", "summary": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.", "AI": {"tldr": "探索与利用：通过裁剪、熵和虚假奖励重新思考RLVR框架", "motivation": "研究在具有可验证奖励（RLVR）的强化学习中，如何平衡探索与利用的关系，以提升大型语言模型（LLMs）的推理能力。当前研究表明，这两种看似矛盾的方法能够提高LLM的数学推理能力，但其背后的原理仍不清晰。", "method": "聚焦于策略熵与性能关系以及虚假奖励收益的问题，并通过裁剪偏置和模型污染来解释这些现象。", "result": "发现虚假奖励下的裁剪偏置减少策略熵，产生更确定性的输出；而单独的熵最小化不足以提升性能。提出一种奖励不对齐模型，说明虚假奖励如何在污染环境下也能提高性能。", "conclusion": "明确了虚假奖励带来的效益机制，并提供了有效的RLVR训练原则"}}
{"id": "2512.16911", "pdf": "https://arxiv.org/pdf/2512.16911", "abs": "https://arxiv.org/abs/2512.16911", "authors": ["Andrew Wagenmaker", "Perry Dong", "Raymond Tsao", "Chelsea Finn", "Sergey Levine"], "title": "Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator's actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator's behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator's actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.", "AI": {"tldr": "该论文提出了一种称为后验行为克隆（PostBC）的方法，用于预训练策略以确保更有效的强化学习微调。", "motivation": "标准的行为克隆方法可能无法保证在演示者动作上的覆盖范围，从而影响后续的强化学习微调效果。因此，研究旨在探索如何通过改变预训练策略来改善微调性能。", "method": "PostBC 方法首先不对观察到的演示进行精确匹配，而是训练一个能够模拟给定演示数据集后验行为分布的策略，以此来确保覆盖范围并提升后续微调的效果。", "result": "在现实世界和仿真环境中进行了测试，结果显示 PostBC 比标准的行为克隆方法获得了更好的强化学习微调性能。", "conclusion": "PostBC 是一种通过现代生成模型实现的方法，在不牺牲预训练表现的情况下提高了策略的有效初始状态，并显著改善了实际任务中的强化学习微调结果。"}}
{"id": "2512.16910", "pdf": "https://arxiv.org/pdf/2512.16910", "abs": "https://arxiv.org/abs/2512.16910", "authors": ["Qihang Rao", "Borui Zhang", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers", "categories": ["cs.CV", "cs.LG"], "comment": "Under review. Code is available at https://github.com/Neur-IO/SFTok", "summary": "Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).", "AI": {"tldr": "SFTok是一种多步迭代机制的离散图像令牌化方法，通过自我强迫引导视觉重构和去偏与拟合训练策略解决训练-推理不一致问题，提高压缩率下图像重建质量。", "motivation": "当前离散令牌器在多模态系统中的应用受限于其性能不如连续令牌器的问题。因此提出SFTok以改善这种差距，提升其在高分辨率图像生成任务上的表现。", "method": "SFTok采用自我强迫引导视觉重构和去偏与拟合训练策略的方法解决离散令牌化过程中的训练-推理不一致问题，并通过多步迭代机制提高图像的重建质量。", "result": "实验表明，SFTok在仅使用64个令牌压缩图像的情况下，在ImageNet数据集上达到了最先进的图像重建质量（rFID=1.21），并在类到图像生成任务中表现出色（gFID=2.29）。", "conclusion": "SFTok通过解决离散令牌化中的关键挑战，显著提升了图像的压缩和重建性能，在保持计算效率的同时实现了高分辨率图像的良好质量。"}}
{"id": "2512.16909", "pdf": "https://arxiv.org/pdf/2512.16909", "abs": "https://arxiv.org/abs/2512.16909", "authors": ["Yuanchen Ju", "Yongyuan Liang", "Yen-Jen Wang", "Nandiraju Gireesh", "Yuanliang Ju", "Seungjae Lee", "Qiao Gu", "Elvis Hsieh", "Furong Huang", "Koushil Sreenath"], "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning", "categories": ["cs.CV", "cs.RO"], "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/", "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.", "AI": {"tldr": "本文提出了一种统一的场景表示方法MomaGraph，用于移动机械手在家庭环境中的导航和操作任务规划。", "motivation": "为了更好地支持移动机械手同时执行导航和操作任务，需要一种能够捕捉物体位置、功能以及可交互部分的紧凑且语义丰富的场景表示法。然而现有工作存在分离空间与功能性关系等问题。", "method": "本文提出了MomaGraph，将空间-功能性关系集成到一个统一的场景图中，并构建了大规模数据集和评估框架来支持这种表示方法的发展。此外，还开发了一种基于强化学习训练的大规模视觉语言模型MomaGraph-R1。", "result": "实验结果表明，在基准测试上，该模型达到了71.6%的准确率，比最佳基线高出11.4%，并且在公共数据集和实际机器人实验中表现出良好的泛化能力。", "conclusion": "通过提出MomaGraph及其相关组件，本文提供了一种有效的场景表示方法以及评估体系，显著提高了移动机械手执行复杂任务的能力。"}}
{"id": "2512.16908", "pdf": "https://arxiv.org/pdf/2512.16908", "abs": "https://arxiv.org/abs/2512.16908", "authors": ["Yuqun Wu", "Chih-hao Lin", "Henry Che", "Aditi Tiwari", "Chuhang Zou", "Shenlong Wang", "Derek Hoiem"], "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.", "AI": {"tldr": "研究了在不同时间对同一场景的图像或视频捕获之间添加、删除或移动的对象进行检测的问题。", "motivation": "多视角变化检测对于许多应用（如机器人整理和施工进度及安全监控）至关重要，但视角的变化可能导致对象被错误地识别为改变。", "method": "引入SceneDiff基准测试集以及一个全新的训练免费方法。该方法利用预训练的3D、分割和图像编码模型来稳健预测多视图的对象变化检测任务。通过三维对齐捕获图像，提取物体区域，并比较空间语义区域特征以检测变化。", "result": "在多视图和两视图基准上实验表明，该方法相比于现有方法分别提高了94%和37.4%的AP值。", "conclusion": "提出了一种新的训练免费的方法用于多视角对象变化检测，并且通过SceneDiff数据集验证了其有效性。"}}
{"id": "2512.16907", "pdf": "https://arxiv.org/pdf/2512.16907", "abs": "https://arxiv.org/abs/2512.16907", "authors": ["Mingfei Chen", "Yifan Wang", "Zhengqin Li", "Homanga Bharadhwaj", "Yujin Chen", "Chuan Qin", "Ziyi Kou", "Yuan Tian", "Eric Whitmire", "Rajinder Sodhi", "Hrvoje Benko", "Eli Shlizerman", "Yue Liu"], "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project website: https://egoman-project.github.io", "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.", "AI": {"tldr": "该论文提出了一种从视觉语言推理到动作生成的框架，以实现准确且阶段感知的手部轨迹预测。", "motivation": "现有手部轨迹预测方法受限于数据集和模型设计，无法充分结合语义监督与行动推理。为解决这一问题，作者提出了一个新的大规模数据集EgoMAN，并开发了一种新的模型来链接视觉语言推理与动作生成。", "method": "提出EgoMAN数据集以及一种新的推理到运动的框架，该框架通过轨迹-令牌接口将视觉语言推理和运动生成连接在一起。模型逐步训练以使推理与运动动态对齐。", "result": "所提方法在准确性和阶段感知方面表现良好，并且能够跨现实场景进行泛化。", "conclusion": "本文提出的方法能够更准确地预测手部轨迹，实现了从视觉语言理解到实际动作的无缝过渡。"}}
{"id": "2512.16906", "pdf": "https://arxiv.org/pdf/2512.16906", "abs": "https://arxiv.org/abs/2512.16906", "authors": ["Xiaoyan Cong", "Haotian Yang", "Angtian Wang", "Yizhi Wang", "Yiding Yang", "Canyu Zhang", "Chongyang Ma"], "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io", "AI": {"tldr": "本文提出了一种基于视觉语言模型和奖励优化的视频编辑框架VIVA，以指导根据自然语言指令修改输入视频。", "motivation": "现有扩散模型方法由于训练数据限制，难以处理多样且复杂的实际场景指令。为解决这一问题，提出了VIVA框架来改善泛化能力。", "method": "首先引入基于视觉语言模型的编码器生成视觉关联指令表示；其次设计Edit-GRPO阶段进行相对奖励优化以提升编辑效果；最后构建数据合成流水线生成高质量视频指令配对数据。", "result": "实验结果显示，VIVA在遵循指令、泛化能力和编辑质量方面优于当前最佳方法。", "conclusion": "通过整合视觉语言模型和奖励优化技术，VIVA框架显著提高了基于文本指令的视频编辑任务表现。"}}
{"id": "2512.16905", "pdf": "https://arxiv.org/pdf/2512.16905", "abs": "https://arxiv.org/abs/2512.16905", "authors": ["Kaixin Ding", "Yang Zhou", "Xi Chen", "Miao Yang", "Jiarong Ou", "Rui Chen", "Xin Tao", "Hengshuang Zhao"], "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection", "categories": ["cs.CV"], "comment": "project page: https://kxding.github.io/project/Alchemist/", "summary": "Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.", "AI": {"tldr": "该论文提出了一种基于元梯度的数据选择框架Alchemist，用于提高文本到图像生成模型的训练效率和质量。", "motivation": "当前文本到图像生成模型受限于数据集的质量，低质量和冗余样本会导致视觉效果退化、不稳定训练和计算效率低下。现有的数据筛选方法依赖于昂贵的手动整理或单一维度特征的启发式评分，缺乏有效的元学习方法适应图像模态。", "method": "Alchemist包括两个主要阶段：数据评级和数据裁剪。它通过迭代优化模型从数据角度自动评估每个样本的影响，使用轻量级评分器基于梯度信息估计每个样本的重要性，并采用Shift-G采样策略选择高效训练的子集。", "result": "在合成和网络爬取的数据集上进行的实验表明，Alchemist可以提高视觉质量和下游性能。即使只用选中的50%数据训练也可以超越使用完整数据集的效果。", "conclusion": "Alchemist是第一个自动、可扩展、基于元梯度的数据选择框架，适用于文本到图像模型的训练任务。"}}
{"id": "2512.16901", "pdf": "https://arxiv.org/pdf/2512.16901", "abs": "https://arxiv.org/abs/2512.16901", "authors": ["Rahul Bhargava", "Malene Hornstrup Jespersen", "Emily Boardman Ndulue", "Vivica Dsouza"], "title": "Impacts of Racial Bias in Historical Training Data for News AI", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.", "AI": {"tldr": "研究探讨了使用纽约时报注释语料库训练的多标签分类器中隐含的种族偏见问题。", "motivation": "揭示大型语言模型在新闻编辑室应用中的潜在历史偏见，防止其对现代事件报道产生负面影响。", "method": "通过量化和质化方法探究“blacks”主题标签在训练数据及已训练模型中的表现，并使用可解释AI技术评估该标签的性能。", "result": "发现“blacks”标签部分充当了某些少数群体的“种族歧视探测器”，但在现代事件如反亚裔仇恨故事报道中表现出色度较低。", "conclusion": "揭示了大型语言模型在新闻编辑室应用中的潜在风险，强调了减少历史偏见的重要性。"}}
{"id": "2512.16900", "pdf": "https://arxiv.org/pdf/2512.16900", "abs": "https://arxiv.org/abs/2512.16900", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Kai Qiu", "Chong Luo", "Zuxuan Wu"], "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.", "AI": {"tldr": "FlashPortrait是一种端到端的视频扩散变换器，能够生成身份一致且无限长度的动画视频，并将推理速度提高了6倍。", "motivation": "当前基于扩散的方法在长肖像动画中难以保持身份一致性。因此提出了FlashPortrait来解决这一问题，同时实现快速和稳定的身份保持。", "method": "首先计算无身份依赖的表情特征，然后通过归一化的表情块调整面部特性以提高身份稳定性。使用动态滑动窗口方案并在重叠区域进行加权融合，确保长动画的平滑过渡和身份一致性。基于扩散层的时间步变化率以及导数幅度比预测未来的潜在值，从而跳过一些去噪步骤，加速推理。", "result": "实验表明FlashPortrait在质量和速度上均表现出色，提高了无限长度视频生成的效果和效率。", "conclusion": "通过提出的新方法，FlashPortrait能够在确保身份一致性的前提下实现快速、高质量的长动画视频生成。"}}
{"id": "2512.16899", "pdf": "https://arxiv.org/pdf/2512.16899", "abs": "https://arxiv.org/abs/2512.16899", "authors": ["Yushi Hu", "Reyhane Askari-Hemmat", "Melissa Hall", "Emily Dinan", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "categories": ["cs.CL", "cs.CV"], "comment": "Code and data available at https://github.com/facebookresearch/MMRB2", "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "AI": {"tldr": "介绍多模态RewardBench 2，评估处理交织文本和图像序列的全能奖励模型。", "motivation": "全能模型在处理交织文本和图像时需要更好的奖励模型来训练，目前这方面的研究还处于初步阶段。为了推动这一领域的发展，作者提出了一套全面基准MMRB2。", "method": "MMRB2包含四个任务：图文生成、图像编辑、交织生成以及多模态推理，并通过专家注释提供了1000对偏好样本。该基准集设计考虑了实用但具有挑战性的提示词、最先进的模型和代理的响应，以及人类共识较强的偏好对比。", "result": "最新版本的Gemini 3 Pro在MMRB2上表现出色，准确率高达75-80%；GPT-5和Gemini 2.5 Pro达到66-75%，相比之下，人类专家的准确性超过90%，而广受使用的GPT-4o仅能达59%。开源模型Qwen3-VL-32B与Gemini 2.5 Flash表现出类似水平的性能。", "conclusion": "MMRB2基准测试不仅反映了当前模型的能力，还展示了它们在下游任务中的应用潜力，并指出了改善奖励模型的关键领域。"}}
{"id": "2512.16896", "pdf": "https://arxiv.org/pdf/2512.16896", "abs": "https://arxiv.org/abs/2512.16896", "authors": ["Jinghuan Shang", "Harsh Patel", "Ran Gong", "Karl Schmeckpeper"], "title": "Sceniris: A Fast Procedural Scene Generation Framework", "categories": ["cs.RO", "cs.CV", "cs.GR"], "comment": "Code is available at https://github.com/rai-inst/sceniris", "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris", "AI": {"tldr": "Sceniris是一种高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体。", "motivation": "当前的程序化生成方法存在低输出吞吐量的问题，成为扩大数据集创建的主要瓶颈。", "method": "通过批处理采样和cuRobo中的更快碰撞检查，Sceniris克服了先前方法Scene Synthesizer的性能限制。", "result": "相比Scene Synthesizer, Sceniri至少快234倍。它还支持更多种类的空间关系以满足不同的场景需求。", "conclusion": "Sceniris为物理AI和生成模型的发展提供了重要的工具，能够高效地生成大规模、无碰撞且可操作的场景变体。"}}
{"id": "2512.16893", "pdf": "https://arxiv.org/pdf/2512.16893", "abs": "https://arxiv.org/abs/2512.16893", "authors": ["Kaiwen Jiang", "Xueting Li", "Seonwook Park", "Ravi Ramamoorthi", "Shalini De Mello", "Koki Nagano"], "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation", "categories": ["cs.CV"], "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d", "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d", "AI": {"tldr": "通过从基于2D扩散的方法中蒸馏知识到前馈编码器，即时将单张图像转换为快速、具有表现力且一致的三维动画表示。", "motivation": "当前的2D方法虽然改进了视频质量但牺牲了3D一致性与速度。而直接使用3D模型的方法虽然保持了3D一致性并加快了推理速度，却缺乏表情细节。因此作者希望结合两者的优点。", "method": "从基于2D扩散的方法中蒸馏知识到前馈编码器；采用高效的局部融合策略而非复杂全局机制来提升动画表现力。", "result": "所提出方法在保证高质量的同时达到107.31 FPS的速度，优于牺牲速度或质量的其他设计。", "conclusion": "该方法结合了2D和3D技术的优点，实现了快速且具有高度表达性的三维面部动画。"}}
{"id": "2512.16891", "pdf": "https://arxiv.org/pdf/2512.16891", "abs": "https://arxiv.org/abs/2512.16891", "authors": ["Haichao Zhang", "Yao Lu", "Lichen Wang", "Yunzhe Li", "Daiwei Chen", "Yunpeng Xu", "Yun Fu"], "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": ":68T05; 68T07; 68T10; 68T45; 68T50; 68U10; 68P20; 62H30; 62H35ACM Class:I.2.4; I.2.6; I.2.7; I.2.8; I.2.10; I.4; I.5; I.7; H.3.1; H.3.3; H.3.4; H.3.5", "summary": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.", "AI": {"tldr": "本文提出了一种基于视频大语言模型的新型表示方法LinkedOut，用于提高视频推荐系统的性能。", "motivation": "当前视频大语言模型在下游任务如视频推荐中存在高延迟、不支持多视频输入和丢弃视觉细节等问题。这些问题源于缺乏能够同时保留像素级细节并利用世界知识的表示方法。", "method": "LinkedOut通过从原始帧中提取语义接地的知识感知令牌来生成表示，这些令牌由大语言模型提供，受可提示查询和辅助模态指导。引入了一种跨层知识融合机制MoE以选择合适的抽象层次，并支持个性化、解释性和低延迟推荐。", "result": "LinkedOut实现了在标准基准测试中的最先进结果，在视频推荐任务中优于现有方法。", "conclusion": "本文提出的方法解决了当前视频大语言模型在下游视觉任务中的局限性，证明了利用世界知识和视觉推理进行个性化、可解释的低延迟推荐的有效性。"}}
{"id": "2512.16885", "pdf": "https://arxiv.org/pdf/2512.16885", "abs": "https://arxiv.org/abs/2512.16885", "authors": ["Norika Wada", "Kohei Yamashita", "Ryo Kawahara", "Ko Nishino"], "title": "M-PhyGs: Multi-Material Object Dynamics from Video", "categories": ["cs.CV"], "comment": null, "summary": "Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.", "AI": {"tldr": "该论文提出了一种从视频中估计多材料复杂自然物体的物理参数的方法M-PhyGs，特别是针对花朵这种具有代表性的常见对象。", "motivation": "现有的方法在估计视觉数据中的物理物质参数时存在局限性，假设单一材质或过于简单的拓扑结构。而现实世界中的物体通常是复杂的，包含多种材料和几何形状，因此需要一种新的方法来准确地进行多材料的物理参数估算。", "method": "M-PhyGs通过引入级联3D和2D损失以及利用时间小批量处理，从视频中高效地分割对象并恢复其连续力学参数，并考虑重力的影响。同时提出了一个新的数据集Phlowers来评估这一任务。", "result": "实验结果表明，在Phlowers数据集中，M-PhyGs及其组件在多材料物理参数估计方面的准确性和有效性得到了证明。", "conclusion": "该论文成功地解决了从视频中估计复杂自然物体的多材料物理参数的问题，为未来的研究提供了新的方法和平台。"}}
{"id": "2512.16881", "pdf": "https://arxiv.org/pdf/2512.16881", "abs": "https://arxiv.org/abs/2512.16881", "authors": ["Arhan Jain", "Mingtong Zhang", "Kanav Arora", "William Chen", "Marcel Torne", "Muhammad Zubair Irshad", "Sergey Zakharov", "Yue Wang", "Sergey Levine", "Chelsea Finn", "Wei-Chiu Ma", "Dhruv Shah", "Abhishek Gupta", "Karl Pertsch"], "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies", "categories": ["cs.RO", "cs.LG"], "comment": "Website: https://polaris-evals.github.io/", "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.", "AI": {"tldr": "PolaRiS框架利用神经重建技术将真实世界的场景转换为仿真环境，从而实现对机器人策略的高保真评估。", "motivation": "由于现实世界中进行机器人策略测试的成本高昂、复杂性和不确定性，开发一种能精确模拟实际条件的方法变得至关重要。现有的仿真方法虽然提供了潜在的解决方案，但与实际情况之间的差距使得它们难以可靠地反映真实性能。", "method": "通过神经网络重建技术，PolaRiS将现实世界的场景视频转化为可交互的仿真环境，并结合简单的数据联合训练来优化这种转换，以减少实际和模拟间的差异。该方法还包括了一种零样本评估方案，在未见过的新环境中直接测试策略性能。", "result": "实验表明，与现有仿真基准相比，PolaRiS提供的仿真评估结果与现实世界中的机器人策略表现有着更强的相关性，并且其简单性有助于快速构建多样化的模拟环境。", "conclusion": "PolaRiS框架通过提供一种低成本、高效率的方式来准确评估通用机器人的策略性能，为下一代机器人基础模型的开发和测试提供了重要支持。"}}
{"id": "2512.16880", "pdf": "https://arxiv.org/pdf/2512.16880", "abs": "https://arxiv.org/abs/2512.16880", "authors": ["Valay Bundele", "Mehran Hosseinzadeh", "Hendrik P. A. Lensch"], "title": "Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.", "AI": {"tldr": "本文提出了一种增强的SAM3模型（ReMeDI-SAM3），用于提高内窥镜手术视频中器械分割在遮挡情况下的准确性。", "motivation": "现有技术如SAM3在处理手术视频中的物体分割时，因为记忆更新不区分、固定的内存容量和弱的身份恢复能力，在面对频繁的遮挡情况下表现不佳。为了改善这些问题，本文提出了新的解决方案。", "method": "方法包括：（i）使用相关性感知的记忆过滤器，并通过一个专用的遮挡感知内存来存储预遮挡帧；(ii)引入分段插值方案以扩大有效内存容量；以及(iii)采用基于特征的身份重新识别模块和时间投票机制，以便在遮挡后可靠地恢复身份。", "result": "实验结果表明，在EndoVis17和EndoVis18数据集上进行评估时，相比原始的SAM3模型，本文提出的ReMeDI-SAM3在零样本设置下分别获得了约7%和16%的绝对mcIoU改进，并且超过了之前的训练依赖方法。", "conclusion": "通过引入记忆增强策略，ReMeDI-SAM3显著提升了内窥镜视频中器械分割任务的表现，在遮挡情况下实现了更可靠的识别性能。"}}
{"id": "2512.16876", "pdf": "https://arxiv.org/pdf/2512.16876", "abs": "https://arxiv.org/abs/2512.16876", "authors": ["Astrid Brull", "Sara Aguti", "Véronique Bolduc", "Ying Hu", "Daniel M. Jimenez-Gutierrez", "Enrique Zuazua", "Joaquin Del-Rio", "Oleksii Sliusarenko", "Haiyan Zhou", "Francesco Muntoni", "Carsten G. Bönnemann", "Xabi Uribe-Etxebarria"], "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "comment": null, "summary": "The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.", "AI": {"tldr": "本论文利用联邦学习技术开发了一种用于诊断COL6-RD的机器学习模型。", "motivation": "由于罕见病（如COL6-RD）的数据稀少且分布分散，传统的机器学习方法在该领域应用受限。联邦学习可以实现在保护患者数据隐私的同时进行跨机构联合训练模型的目的。", "method": "通过使用Sherpa.ai FL平台，在两个国际组织之间实施了全球联邦学习计划，利用来自病人成纤维细胞培养的胶原VI免疫荧光显微图像训练模型，并将其分类为与COL6-RD相关的三种主要致病机制组：外显子跳跃、甘氨酸取代和伪外显子插入。", "result": "该方法实现了F1得分为0.82的新机器学习模型，优于单一组织的模型（0.57-0.75），展示了联邦学习在提升诊断准确性及泛化能力方面的显著优势。", "conclusion": "这种方法不仅提高了诊断准确率，还支持了对意义未明变异的解释，并有助于确定新的致病突变优先排序。"}}
{"id": "2512.16875", "pdf": "https://arxiv.org/pdf/2512.16875", "abs": "https://arxiv.org/abs/2512.16875", "authors": ["Chao Gao", "Liren Shan", "Vaidehi Srinivas", "Aravindan Vijayaraghavan"], "title": "Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery", "categories": ["cs.DS", "cs.LG", "math.ST", "stat.ML"], "comment": null, "summary": "We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $α$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\\Pr_{D}[E] \\ge 1-α$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $β$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $β$? Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(β^{γd})$ multiplicative factor of the volume of best $β$-conditioned ellipsoid while covering at least $1-O(α/γ)$ probability mass for any $γ< α$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.", "AI": {"tldr": "研究在高维度下找到任意分布的置信椭圆问题，提出了一个能够在多项式时间内运行且具有近似保证的算法。", "motivation": "在统计学中，最小体积估计器已被作为鲁棒非参数的位置和散度估计方法引入。然而，在高维空间中，当条件数β（最大轴长与最小区间长度的比例）趋向于无穷大时，找到这样的椭圆成为NP难问题。因此研究了如何高效地在多项式时间内找到具有近似体积保证的置信椭圆。", "method": "提出了一个基于最小体积包围椭圆及其几何Brascamp-Lieb不等式的丰富对偶结构算法。此算法可以在多项式时间内运行，并能提供较好的体积近似保证。", "result": "该方法能够找到一个其体积在$O(β^{γd})$倍数范围内比最佳条件数为$β$的椭圆更小，同时覆盖至少$1-O(α/γ)$概率质量的椭圆。此外还证明了这种依赖性似乎是必要的，直到指数中的常数。", "conclusion": "首次提供了在最坏情况下具有近似保证的稳健子空间恢复问题的多项式时间算法。"}}
{"id": "2512.16874", "pdf": "https://arxiv.org/pdf/2512.16874", "abs": "https://arxiv.org/abs/2512.16874", "authors": ["Tomáš Souček", "Pierre Fernandez", "Hady Elsahar", "Sylvestre-Alvise Rebuffi", "Valeriu Lacatusu", "Tuan Tran", "Tom Sander", "Alexandre Mourachko"], "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "comment": "Code and model available at https://github.com/facebookresearch/videoseal", "summary": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.", "AI": {"tldr": "Pixel Seal提出了一种新的图像和视频水印方法，通过仅使用对抗训练来提高不可见性和鲁棒性。", "motivation": "现有水印技术在平衡隐蔽性和鲁棒性方面存在困难，导致水印可见或不够稳定。该研究旨在解决这些问题，并提供一种更可靠的方法。", "method": "Pixel Seal采用了一种新的对抗训练方案来消除不稳定的像素级不可见性损失，并通过分阶段训练和高分辨率适应技术增强了模型的稳定性与效果。", "result": "实验结果表明，Pixel Seal在各种图像类型及变换下均优于现有技术水平，在视频应用中也表现出色。", "conclusion": "Pixel Seal提供了一种有效、可扩展的方法来保障真实世界的图像和视频来源的可靠性。"}}
{"id": "2512.16873", "pdf": "https://arxiv.org/pdf/2512.16873", "abs": "https://arxiv.org/abs/2512.16873", "authors": ["Otman A. Basir"], "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.", "AI": {"tldr": "提出社会责任堆栈(SRS)，一种嵌入社会价值的六层架构框架，用于规范人工智能系统。", "motivation": "现有负责AI的努力提供重要的规范原则但缺乏在整个生命周期中操作的有效工程机制。SRS将社会责任视为闭环监督控制问题，结合设计时的安全措施和运行时监控以及制度监管。", "method": "开发统一的约束形式化方法，并展示如何监测并强制执行公平性、自主权、认知负担和解释质量。", "result": "通过临床决策支持、自动驾驶车辆和公共部门系统案例研究说明了SRS将规范目标转化为可操作的工程控制的能力。", "conclusion": "该框架为负责任的技术治理提供了理论基础，使AI系统更加透明、适应性和审计性。"}}
{"id": "2512.16872", "pdf": "https://arxiv.org/pdf/2512.16872", "abs": "https://arxiv.org/abs/2512.16872", "authors": ["Shayan Hundrieser", "Philipp Tuchel", "Insung Kong", "Johannes Schmidt-Hieber"], "title": "On the Universal Representation Property of Spiking Neural Networks", "categories": ["cs.NE", "cs.LG", "stat.ML"], "comment": "54 pages, 8 figures", "summary": "Inspired by biology, spiking neural networks (SNNs) process information via discrete spikes over time, offering an energy-efficient alternative to the classical computing paradigm and classical artificial neural networks (ANNs). In this work, we analyze the representational power of SNNs by viewing them as sequence-to-sequence processors of spikes, i.e., systems that transform a stream of input spikes into a stream of output spikes. We establish the universal representation property for a natural class of spike train functions. Our results are fully quantitative, constructive, and near-optimal in the number of required weights and neurons. The analysis reveals that SNNs are particularly well-suited to represent functions with few inputs, low temporal complexity, or compositions of such functions. The latter is of particular interest, as it indicates that deep SNNs can efficiently capture composite functions via a modular design. As an application of our results, we discuss spike train classification. Overall, these results contribute to a rigorous foundation for understanding the capabilities and limitations of spike-based neuromorphic systems.", "AI": {"tldr": "本文分析了脉冲神经网络（SNN）的通用表示能力，探讨其作为序列到序列处理器的功能。", "motivation": "受生物学启发，SNNs通过时间离散脉冲处理信息，提供了一种比传统计算模式和人工神经网络更节能的选择。研究目的是量化并建立SNN的通用表示特性，特别是对于具有少量输入、低时间复杂度或组合函数的情况。", "method": "本文将SNN视为序列到序列的处理器，分析了其在转换输入脉冲流为输出脉冲流时的通用表示能力，并提供了详细的构造方法和量化结果。这些结果是关于所需权重和神经元数量近乎最优的。", "result": "研究显示，SNN特别适合于表达具有少量输入、低时间复杂度或组合函数的功能。特别是深度SNN可以通过模块化设计高效地捕捉复合功能。作为应用案例，本文讨论了脉冲序列分类的问题。", "conclusion": "这些结果为理解基于脉冲的神经形态系统的性能和局限性提供了坚实的理论基础"}}
{"id": "2512.16871", "pdf": "https://arxiv.org/pdf/2512.16871", "abs": "https://arxiv.org/abs/2512.16871", "authors": ["Hesham G. Moussa", "Aroosa Hameed", "Arashmid Akhavain"], "title": "Sequencing to Mitigate Catastrophic Forgetting in Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "The Manuscript is submitted for review under IEEE Transactions on Artificial intelligence", "summary": "To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, and exploit knowledge throughout its lifetime. This ability, known as Continual learning, provides a foundation for AI systems to develop themselves adaptively. Catastrophic forgetting is a major challenge to the progress of Continual Learning approaches, where learning a new task usually results in a dramatic performance drop on previously learned ones. Many approaches have emerged to counteract the impact of CF. Most of the proposed approaches can be categorized into five classes: replay-based, regularization-based, optimization-based, representation-based, and architecture-based. In this work, we approach the problem from a different angle, specifically by considering the optimal sequencing of tasks as they are presented to the model. We investigate the role of task sequencing in mitigating CF and propose a method for determining the optimal task order. The proposed method leverages zero-shot scoring algorithms inspired by neural architecture search (NAS). Results demonstrate that intelligent task sequencing can substantially reduce CF. Moreover, when combined with traditional continual learning strategies, sequencing offers enhanced performance and robustness against forgetting. Additionally, the presented approaches can find applications in other fields, such as curriculum learning.", "AI": {"tldr": "研究通过任务排序来减轻灾难性遗忘问题的方法。", "motivation": "解决连续学习中的灾难性遗忘，提升AI系统适应环境变化的能力。", "method": "利用零样本评分算法确定最佳任务顺序，减少灾难性遗忘。", "result": "智能任务排序可以显著降低灾难性遗忘，并且与传统策略结合可进一步提高性能和鲁棒性。", "conclusion": "任务排序是一种有效的减轻连续学习中灾难性遗忘的方法，同时也可以应用于其他领域如课程学习。"}}
{"id": "2512.16866", "pdf": "https://arxiv.org/pdf/2512.16866", "abs": "https://arxiv.org/abs/2512.16866", "authors": ["Jiabin Xue"], "title": "Semi-Supervised Online Learning on the Edge by Transforming Knowledge from Teacher Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Edge machine learning (Edge ML) enables training ML models using the vast data distributed across network edges. However, many existing approaches assume static models trained centrally and then deployed, making them ineffective against unseen data. To address this, Online Edge ML allows models to be trained directly on edge devices and updated continuously with new data. This paper explores a key challenge of Online Edge ML: \"How to determine labels for truly future, unseen data points\". We propose Knowledge Transformation (KT), a hybrid method combining Knowledge Distillation, Active Learning, and causal reasoning. In short, KT acts as the oracle in active learning by transforming knowledge from a teacher model to generate pseudo-labels for training a student model. To verify the validity of the method, we conducted simulation experiments with two setups: (1) using a less stable teacher model and (2) a relatively more stable teacher model. Results indicate that when a stable teacher model is given, the student model can eventually reach its expected maximum performance. KT is potentially beneficial for scenarios that meet the following circumstances: (1) when the teacher's task is generic, which means existing pre-trained models might be adequate for its task, so there will be no need to train the teacher model from scratch; and/or (2) when the label for the student's task is difficult or expensive to acquire.", "AI": {"tldr": "提出了一种知识转换方法，用于边端机器学习中的在线模型训练。", "motivation": "解决边缘设备上在线机器学习中如何为未来未知数据生成标签的问题。", "method": "通过结合知识蒸馏、主动学习和因果推理的方法来实现教师模型到学生模型的知识转移。", "result": "实验结果表明，在给定稳定教师模型的情况下，学生模型可以达到预期的最大性能。", "conclusion": "所提出的知识转换方法对于任务泛化性强且标签获取困难或昂贵的场景具有潜在价值。"}}
{"id": "2512.16864", "pdf": "https://arxiv.org/pdf/2512.16864", "abs": "https://arxiv.org/abs/2512.16864", "authors": ["Tianyuan Qu", "Lei Ke", "Xiaohang Zhan", "Longxiang Tang", "Yuqi Liu", "Bohao Peng", "Bei Yu", "Dong Yu", "Jiaya Jia"], "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing", "categories": ["cs.CV"], "comment": "Precise region control and planning for instruction-based image editing. Our project page: https://replan-iv-edit.github.io", "summary": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io", "AI": {"tldr": "本文提出了一种名为RePlan的框架，用于复杂指令驱动的图像编辑任务。", "motivation": "现有的模型在处理复杂的指令和混乱或模糊场景时表现不佳。为了应对这一挑战，作者提出了一个结合视觉语言规划器与扩散编辑器的新方法。", "method": "该方法包括两个主要部分：使用基于GRPO的强化学习增强的视觉语言规划器进行分步骤推理并明确地将指令映射到目标区域；然后利用训练自由的关注地区注入机制进行精准、平行多区编辑。", "result": "RePlan在复杂设置下表现出色，特别是在IV-Edit基准测试中优于大型数据集上训练的强大基线模型，提高了区域精度和整体准确性。", "conclusion": "通过引入新颖的规划执行框架RePlan，本文解决了现有图像编辑模型面对复杂指令时的不足，并展示了其优越性能。"}}
{"id": "2512.16861", "pdf": "https://arxiv.org/pdf/2512.16861", "abs": "https://arxiv.org/abs/2512.16861", "authors": ["Zihan Zhou", "Animesh Garg", "Ajay Mandlekar", "Caelan Garrett"], "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/", "AI": {"tldr": "ReinforceGen结合任务分解、数据生成、模仿学习和运动规划，通过强化学习进行微调，解决长时序操作问题。", "motivation": "长时间的操作一直是机器人领域的难题。本文提出了一种新方法来克服这一挑战，通过组合技术提高机器人的操作成功率。", "method": "ReinforceGen首先将任务分解成多个局部技能，并利用运动规划连接这些技能。在10个人类演示数据集上进行模仿学习训练后，再通过在线调整和强化学习进行微调。", "result": "在Robosuite数据集测试中，ReinforceGen实现了80%的成功率。去除微调部分的实验表明，这种方法提高了89％的平均表现。", "conclusion": "提出的ReinforceGen系统有效地解决了长时序操作问题，并证明了组合任务分解、模仿学习和强化学习的有效性。"}}
{"id": "2512.16856", "pdf": "https://arxiv.org/pdf/2512.16856", "abs": "https://arxiv.org/abs/2512.16856", "authors": ["Nenad Tomašev", "Matija Franklin", "Julian Jacobs", "Sébastien Krier", "Simon Osindero"], "title": "Distributional AGI Safety", "categories": ["cs.AI"], "comment": null, "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "AI": {"tldr": "本文提出了一种分布式的AGI安全框架，旨在通过虚拟代理经济的沙盒设计来应对多个具有互补技能和沟通能力的亚AGI个体组成的集体风险。", "motivation": "当前AI安全性研究主要集中在单个AI系统的保护上。然而，随着工具使用能力和通信协调能力的提高，亚AGI个体可能形成群体实现高级别功能，这种假设需要更多的关注，并应发展相应的安全保障措施。", "method": "提出了一种框架，该框架基于代理间的交易由健全市场机制管理、结合适当的审计、声誉管理和监督来缓解集体风险的虚拟代理沙盒经济。", "result": "尚无具体实验结果或结论，本文主要提供理论性建议和框架设计。", "conclusion": "鉴于先进AI代理快速部署所带来的安全挑战，应对基于群体行为的AGI假设给予充分重视，并发展分布式的安全保障措施。"}}
{"id": "2512.16855", "pdf": "https://arxiv.org/pdf/2512.16855", "abs": "https://arxiv.org/abs/2512.16855", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge", "categories": ["cs.AI", "cs.LO"], "comment": "Published in the IEEE ICCAD 2025 conference", "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.", "AI": {"tldr": "提出了基于信号时序逻辑的大型语言模型压缩框架TOGGLE，以确保在边缘设备上部署的语言模型在压缩过程中保持形式化指定的语义属性。", "motivation": "现有的压缩技术如量化和剪枝往往会导致关键语言特性退化，并且缺乏对模型行为保存的形式保证。为此提出了TOGGLE来解决这些问题，使大型语言模型能在资源有限的边缘设备上高效部署。", "method": "采用了基于时序逻辑的鲁棒性引导贝叶斯优化方法探索逐层量化和剪枝配置，生成形式上满足指定语言约束且无需重新训练或微调的压缩模型。", "result": "在四个大型语言模型架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B 和 Mistral 7B）上的评估显示，TOGGLE实现了高达3.3倍计算成本减少和68.8%的模型大小减少。", "conclusion": "TOGGLE代表了将形式方法集成到大型语言模型压缩中的首次尝试，并使得在边缘硬件上部署高效且可验证的语言模型成为可能。"}}
{"id": "2512.16853", "pdf": "https://arxiv.org/pdf/2512.16853", "abs": "https://arxiv.org/abs/2512.16853", "authors": ["Amita Kamath", "Kai-Wei Chang", "Ranjay Krishna", "Luke Zettlemoyer", "Yushi Hu", "Marjan Ghazvininejad"], "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.", "AI": {"tldr": "研究提出了一种改进的自动化文本到图像评估基准GenEval 2，以解决现有评估方法随时间逐渐失效的问题。", "motivation": "随着T2I模型能力的进步，现有的评价标准如GenEval会变得过时，导致评估结果与人类判断偏差增大。因此需要一个新的更适应现代模型能力和长期稳定的评估框架。", "method": "提出了新的基准GenEval 2，它包括更多基础视觉概念和更高层次的组合性，并引入了Soft-TIFA评价方法结合对视觉元素的单独评判，以提高评测准确性。", "result": "新方法与人类判断的一致性显著提高，降低了随时间产生的偏差问题。大范围的人类研究表明，新的基准可以有效评估当前T2I模型的表现。", "conclusion": "虽然避免长期漂移并非易事，但这项研究展示了持续审计和改进自动化图像生成模型评价的重要性。"}}
{"id": "2512.16851", "pdf": "https://arxiv.org/pdf/2512.16851", "abs": "https://arxiv.org/abs/2512.16851", "authors": ["Ripan Kumar Kundu", "Istiak Ahmed", "Khaza Anuarul Hoque"], "title": "PrivateXR: Defending Privacy Attacks in Extended Reality Through Explainable AI-Guided Differential Privacy", "categories": ["cs.CR", "cs.AI", "cs.HC"], "comment": "Published in the IEEE ISMAR 2025 conference", "summary": "The convergence of artificial AI and XR technologies (AI XR) promises innovative applications across many domains. However, the sensitive nature of data (e.g., eye-tracking) used in these systems raises significant privacy concerns, as adversaries can exploit these data and models to infer and leak personal information through membership inference attacks (MIA) and re-identification (RDA) with a high success rate. Researchers have proposed various techniques to mitigate such privacy attacks, including differential privacy (DP). However, AI XR datasets often contain numerous features, and applying DP uniformly can introduce unnecessary noise to less relevant features, degrade model accuracy, and increase inference time, limiting real-time XR deployment. Motivated by this, we propose a novel framework combining explainable AI (XAI) and DP-enabled privacy-preserving mechanisms to defend against privacy attacks. Specifically, we leverage post-hoc explanations to identify the most influential features in AI XR models and selectively apply DP to those features during inference. We evaluate our XAI-guided DP approach on three state-of-the-art AI XR models and three datasets: cybersickness, emotion, and activity classification. Our results show that the proposed method reduces MIA and RDA success rates by up to 43% and 39%, respectively, for cybersickness tasks while preserving model utility with up to 97% accuracy using Transformer models. Furthermore, it improves inference time by up to ~2x compared to traditional DP approaches. To demonstrate practicality, we deploy the XAI-guided DP AI XR models on an HTC VIVE Pro headset and develop a user interface (UI), namely PrivateXR, allowing users to adjust privacy levels (e.g., low, medium, high) while receiving real-time task predictions, protecting user privacy during XR gameplay.", "AI": {"tldr": "本文提出了一种结合可解释AI和差分隐私的框架，以保护XR系统中的数据隐私。", "motivation": "在人工智能与扩展现实技术融合中，敏感数据（如眼动追踪）易被利用进行个人信息泄露。传统的差分隐私方法会在不相关特征上引入不必要的噪声，降低模型准确性并增加推理时间，因此本文提出结合可解释AI的差异隐私保护方案。", "method": "使用后验解释确定XR人工智能模型中最关键的特征，并在这些特征中应用差分隐私，在推断过程中选择性地添加噪音以防止信息泄露。评估了三种最先进的AI XR模型和三个数据集：网络恶心、情感分类以及活动识别。", "result": "所提出的方法在减少MIA和RDA成功率方面表现出色，分别降低了43%和39%，同时保持高达97％的准确性，并将推理时间提高了一倍。还开发了一个用户界面（PrivateXR），使用户能够根据个人隐私偏好调整保护级别。", "conclusion": "通过结合可解释AI技术与差分隐私，有效提升了XR系统中的数据隐私保护能力，在减少信息泄露风险的同时保证了模型的实用性和效率"}}
{"id": "2512.16848", "pdf": "https://arxiv.org/pdf/2512.16848", "abs": "https://arxiv.org/abs/2512.16848", "authors": ["Yulun Jiang", "Liangze Jiang", "Damien Teney", "Michael Moor", "Maria Brbic"], "title": "Meta-RL Induces Exploration in Language Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.", "AI": {"tldr": "本文提出了一种元强化学习框架LaMer，用于促进语言模型代理在环境中的主动探索和长期奖励优化。", "motivation": "现有的基于强化学习训练的语言模型代理在需要积极探索的任务中表现不佳，并且难以从试错经验中有效适应。因此，作者提出了LaMer来解决这些问题。", "method": "LaMer由两个关键组件构成：跨集体验框架以鼓励探索和长期奖励优化；以及通过反思进行的即时策略调整机制，使代理能够在没有梯度更新的情况下根据任务反馈信号调整策略。", "result": "在不同环境中实验表明，与强化学习基准相比，LaMer显著提高了性能，在Sokoban、Minesweeper和Webshop上的表现分别提升了11%，14% 和 19%。此外，LaMer还展示了更好的泛化能力，能够应对更具挑战性或前所未见的任务。", "conclusion": "元强化学习提供了一种通过学习探索策略来诱导语言代理进行更有效适应的原理方法，从而增强其在新环境中处理问题的能力。"}}
{"id": "2512.16843", "pdf": "https://arxiv.org/pdf/2512.16843", "abs": "https://arxiv.org/abs/2512.16843", "authors": ["Harsh Vardhan Bansal"], "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)", "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications", "AI": {"tldr": "提出了一种新的逐层缓存框架LLMCache，用于通过重复使用基于输入序列语义相似性的中间激活来加速Transformer推理。", "motivation": "现有的缓存机制如令牌级键值缓存在自动回归解码中提供了速度提升，但其适用范围和灵活性有限。为了提高实时和大规模部署中的Transformer推理效率，作者提出了LLMCache。", "method": "引入了一种轻量级的指纹匹配方法来识别语义相似的输入，并设计了自适应驱逐策略以管理缓存过时的问题。该框架适用于各种模型架构及层面上的缓存。", "result": "实验结果表明，在BERT和GPT-2上，LLMCache在SQuAD、WikiText-103和OpenBookQA数据集上的推理时间最多可提高3.1倍，并且准确性损失小于0.5%。", "conclusion": "LLMCache作为一种实用的通用解决方案，能够显著优化Transformer模型的真实世界应用中的推理效率。"}}
{"id": "2512.16842", "pdf": "https://arxiv.org/pdf/2512.16842", "abs": "https://arxiv.org/abs/2512.16842", "authors": ["Yuxin Ray Song", "Jinzhou Li", "Rao Fu", "Devin Murphy", "Kaichen Zhou", "Rishi Shiv", "Yaqi Li", "Haoyu Xiong", "Crystal Elaine Owens", "Yilun Du", "Yiyue Luo", "Xianyi Cheng", "Antonio Torralba", "Wojciech Matusik", "Paul Pu Liang"], "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "https://opentouch-tactile.github.io/", "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.", "AI": {"tldr": "本文介绍了OpenTouch，一个包含第一人称视角视频、全手触觉和姿态数据的户外数据集。", "motivation": "现有的可穿戴触觉传感器稀少且缺乏与第一人称视图对齐的完整手部触觉数据集。该研究旨在填补视觉感知和物理交互之间的空白，促进多模态第一视角感知、具身学习以及接触丰富的机器人操作的进步。", "method": "通过收集5.1小时同步视频-触觉-姿态数据并创建2900个剪辑的详细文本注释，构建了一个新的户外数据集OpenTouch。使用此数据集提出了检索和分类基准测试。", "result": "实验表明，触觉信号可以提供有力的抓取理解线索，并增强跨模态对齐；从野外视频查询中可靠地检索出这些信息。", "conclusion": "通过发布这个注释过的视觉-触觉-姿态数据集及基准测试，推进了多模态第一视角感知、具身学习以及接触丰富的机器人操作领域的发展。"}}
{"id": "2512.16841", "pdf": "https://arxiv.org/pdf/2512.16841", "abs": "https://arxiv.org/abs/2512.16841", "authors": ["Emmanuel D. Muñiz-De-León", "Jorge A. Rosales-de-Golferichs", "Ana S. Muñoz-Rodríguez", "Alejandro I. Trejo-Castro", "Eduardo de Avila-Armenta", "Antonio Martínez-Torteya"], "title": "Radiology Report Generation with Layer-Wise Anatomical Attention", "categories": ["cs.CV"], "comment": "11 pages, 6 figures", "summary": "Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.", "AI": {"tldr": "该论文提出了一种结合冻结的DINOv3视觉变压器编码器和增强层级解剖注意力机制的GPT-2解码器，用于从单个胸部X光片生成放射学报告。", "motivation": "当前最先进的系统依赖于大规模多模态训练、临床元数据和多个成像视图，资源密集且难以普及。该论文旨在开发一种紧凑的图像到文本架构来生成胸部X光片报告，并通过层级解剖注意力机制提高模型性能。", "method": "使用冻结的DINOv3视觉变压器编码器与GPT-2解码器相结合，同时引入了层次化高斯平滑技术以集成肺和心脏分割掩模。这种方法在不增加可训练参数的情况下将注意力导向临床相关的区域，并且仅基于单个图像生成报告。", "result": "实验结果表明，在MIMIC-CXR数据集上使用CheXpert和RadGraph评估指标，该方法对于五个关键病理的Macro-F1提高了168%，Micro-F1提高了146%，整体性能提升了86%。同时，结构一致性也有所改善。", "conclusion": "尽管模型较小且仅依赖图像输入设计，但通过解码器级别的解剖指导改进了空间定位并增强了临床相关区域的连贯性表现"}}
{"id": "2512.16826", "pdf": "https://arxiv.org/pdf/2512.16826", "abs": "https://arxiv.org/abs/2512.16826", "authors": ["Arslan Amin", "Rafia Mumtaz", "Muhammad Jawad Bashir", "Syed Mohammad Hassan Zaidi"], "title": "Next-Generation License Plate Detection and Recognition System using YOLOv8", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 5 figures. Accepted and published in the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)", "summary": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.", "AI": {"tldr": "本文研究使用YOLOv8在车牌识别和字符识别任务中的性能，提出了一种高效的检测与识别系统。", "motivation": "为了提高交通管理和车辆监控系统的效率，本文旨在解决传统方法在多样环境下实时准确性不足的问题。", "method": "采用两个不同的数据集进行训练和评估。引入YOLOv8 Nano和Small模型分别用于车牌检测和字符识别任务，并通过x轴位置排序来序列化字符。", "result": "YOLOv8 Nano模型在车牌检测中精度达到0.964，mAP50为0.918；YOLOv8 Small模型在字符识别中的精度为0.92，mAP50为0.91。优化的管道提高了系统的效率和准确性。", "conclusion": "通过结合YOLOv8 Nano进行车牌检测和YOLOv8 Small进行字符识别的方法，在保证计算效率的同时实现了高准确率，为智能交通系统中边缘设备的实际应用奠定了基础。"}}
{"id": "2512.16818", "pdf": "https://arxiv.org/pdf/2512.16818", "abs": "https://arxiv.org/abs/2512.16818", "authors": ["Marius Dähling", "Sebastian Krebs", "J. Marius Zöllner"], "title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects", "categories": ["cs.CV"], "comment": "15 pages, 8 figures, accepted by WACV 2026", "summary": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.", "AI": {"tldr": "论文提出了DenseBEV方法，通过使用密集的BEV网格单元作为锚点来进行多摄像头3D物体检测。", "motivation": "当前的研究中，基于Bird's-Eye-View（BEV）的变压器被用于多摄像头3D物体检测。传统模型通常使用随机查询作为锚点进行优化，而新方法则尝试通过直接使用BEV特征单元来改进这一过程。", "method": "论文提出了一种新的两阶段锚生成方法，该方法利用密集的BEV网格将每个BEV单元视为潜在的对象，并采用BEV非极大值抑制技术来处理大量的查询。同时引入了混合时间建模方法以进一步提高检测性能。", "result": "在nuScenes数据集上的评估显示，DenseBEV方法显著提高了NDS和mAP指标，特别是在小物体检测方面表现出色。应用到Waymo Open数据集中时，该方法达到了60.7%的LET-mAP，优于之前的最佳表现。", "conclusion": "通过使用密集的BEV网格单元作为锚点并采用混合时间建模技术，DenseBEV在多摄像头3D物体检测任务中表现出色，并且特别适用于小物体检测。"}}
{"id": "2512.16814", "pdf": "https://arxiv.org/pdf/2512.16814", "abs": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "ef:Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:15370-15383, 2025", "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "AI": {"tldr": "提出了一种基于语言模型的自然语言到时态逻辑翻译框架（GraFT），通过限制输出词汇表中的有效令牌来提高准确性。", "motivation": "现有方法在原子命题提取、共指处理及从有限数据学习方面存在不足，导致自然语言转换为形式化语言如时态逻辑的过程中遇到困难。", "method": "GraFT框架通过减少解空间复杂度来增强翻译的准确性和效率，采用受限词汇表预测令牌，并理论证明了这种方法的有效性。", "result": "实验表明GraFT在CW、GLTL和Navi基准测试上分别提高了5.49%的整体端到端翻译准确率以及14.06%的域外翻译准确性。", "conclusion": "通过限制输出词汇表中的令牌，GraFT能够更有效地学习并提高自然语言向时态逻辑转换的精度。"}}
{"id": "2512.16813", "pdf": "https://arxiv.org/pdf/2512.16813", "abs": "https://arxiv.org/abs/2512.16813", "authors": ["Bahman Abolhassani", "Tugba Erpek", "Kemal Davaslioglu", "Yalin E. Sagduyu", "Sastry Kompella"], "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG", "eess.SP"], "comment": null, "summary": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.", "AI": {"tldr": "本文提出了一种基于QMIX算法的多智能体强化学习框架，用于提高集群通信在对抗性干扰下的鲁棒性。", "motivation": "传统的反干扰措施如固定功率控制或静态信道跳频对适应性强的对手无效。因此，需要一种更有效的策略来保障机器人集群网络的安全性和完整性。", "method": "本文采用了QMIX算法设计了一个多智能体强化学习框架，使得每个代理能够协调选择传输频率（通道）和功率，并学会了集中但可分解的动作值函数以实现协同分散执行。", "result": "仿真结果显示，在无信道复用的情况下，QMIX快速收敛到接近理想最优策略的合作策略；而在更一般的信道衰减环境中启用信道复用后，仍然表现出更高的吞吐量和更低的干扰发生率。", "conclusion": "多智能体强化学习在保障自主集群网络的安全性方面表现出了强大的效果。"}}
{"id": "2512.16811", "pdf": "https://arxiv.org/pdf/2512.16811", "abs": "https://arxiv.org/abs/2512.16811", "authors": ["Jingjing Qian", "Boyao Han", "Chen Shi", "Lei Xiao", "Long Yang", "Shaoshuai Shi", "Li Jiang"], "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.", "AI": {"tldr": "GeoPredict 提出了一种几何感知的 VLA 框架，通过预测性的运动学和几何先验来改进机器人操作中的精确性。", "motivation": "现有的 VLA 模型在任务中表现得较为被动且二维中心化，在需要精细三维推理的任务中可靠性较低。为了提高这种模型在复杂场景下的性能，GeoPredict 被设计出来。", "method": "通过引入轨迹级别模块和预测的3D高斯几何模块，GeoPredict 能够对机器人手臂的多步3D关键点轨迹进行编码与预测，并使用基于深度的方法对其进行监督。此方法仅在训练时需要额外组件，在推理阶段则不需要复杂的三维解码。", "result": "实验表明，相较于其他 VLA 基线模型，GeoPredict 在几何复杂和空间要求高的场景中表现更为出色。", "conclusion": "GeoPredict 成功改进了现有VLA模型的精度与鲁棒性，特别是在需要精确3D推理的任务中。"}}
{"id": "2512.16795", "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.", "AI": {"tldr": "该论文提出了一种推理增强的检索增广语言模型框架，旨在解决现有RAG系统在面对冲突或过时信息时的问题。", "motivation": "现有的Retrieval-Augmented Generation (RAG) 方法在面对冲突或主观、过时的信息时表现不佳。缺乏统一的推理监督。", "method": "引入了结构化且可解释的推理过程，包括文档级裁决、冲突分析和基于证据合成三个阶段，并设计了一个Conflict-Aware Trust-Score（CATS）管道以评估模型的表现。", "result": "实验结果表明，在Qwen模型上，监督微调后端到端答案正确率从0.069提升至0.883，行为一致性也显著提高。", "conclusion": "该论文提出的方法为解决冲突问题和建立可解释的RAG系统奠定了基础。"}}
{"id": "2512.16793", "pdf": "https://arxiv.org/pdf/2512.16793", "abs": "https://arxiv.org/abs/2512.16793", "authors": ["Xiaopeng Lin", "Shijie Lian", "Bin Yu", "Ruoqi Yang", "Changti Wu", "Yuzhuo Miao", "Yurun Jin", "Yukun Shi", "Cong Huang", "Bojun Cheng", "Kai Chen"], "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "categories": ["cs.RO"], "comment": "17 pages, 4 figures", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "AI": {"tldr": "本文提出了一种将第一人称视频转换为结构化训练数据的管道，以增强视觉语言模型在机器人物理智能方面的表现。", "motivation": "大多数视觉语言模型主要基于第三人称数据进行训练，与人形机器人的第一人称感知和行动存在视角不匹配问题。大规模的人类第一人称视频提供了一种可扩展的替代方案，可以自然地捕捉丰富的交互上下文和因果结构。", "method": "提出了一条将原始第一人称视频转换为多层、模式驱动的视觉问答监督信号的管道，并利用该方法创建了Egocentric2Embodiment数据集（E2E-3M）。通过在E2E-3M数据集上进行训练，获得一种名为PhysBrain的第一人称感知增强型大脑。", "result": "与基线模型相比，PhysBrain在第一人称理解和规划任务中的表现显著提高。此外，它还能够更有效地将人类第一人称监督转移到下游机器人控制任务中，并提高了SimplerEnv的成功率（53.9%）。", "conclusion": "本文的研究表明，通过利用大规模的人类第一人称视频数据，可以有效增强视觉语言模型的物理智能和样本效率。"}}
{"id": "2512.16792", "pdf": "https://arxiv.org/pdf/2512.16792", "abs": "https://arxiv.org/abs/2512.16792", "authors": ["Endar Suprih Wihidayat", "Sieteng Soh", "Kwan-Wu Chin", "Duc-son Pham"], "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint", "categories": ["cs.DC", "cs.AI"], "comment": "17 pages, 9 figures", "summary": "In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.", "AI": {"tldr": "本文提出了一个多阶段边缘服务器升级问题，旨在通过部署新服务器和升级现有服务器来满足延迟要求，并最大化任务的平均数量。", "motivation": "为了解决多接入边缘计算系统在多个时间阶段进行升级改造时面临的预算、成本及资源限制等问题，提出了一种新的网络规划问题。", "method": "本文提出了两种解决方案：一种是适用于小规模网络的混合整数线性规划模型；另一种是一种高效的启发式算法（M-ESU/H），用于大规模网络。通过这两种方法来最大化满足延迟需求的任务数量。", "result": "实验结果表明，对于小型网络，M-ESU/H 解决方案与最优解之间的差距小于1.25%，且运行速度明显更快；在大型网络中，M-ESU/H 在相同预算和任务增长率的条件下比其他三种启发式算法提高了最多21.57%的任务满足率。", "conclusion": "该研究提出的多阶段边缘服务器升级方法具有良好的可扩展性和实际应用价值，可以有效应用于长期多接入边缘计算系统。"}}
{"id": "2512.16791", "pdf": "https://arxiv.org/pdf/2512.16791", "abs": "https://arxiv.org/abs/2512.16791", "authors": ["Shuting Zhao", "Zeyu Xiao", "Xinrong Chen"], "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2026", "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/", "AI": {"tldr": "提出了一种新的基于动力学的时空状态空间模型KineST，用于从稀疏信号中重建人体全姿态。", "motivation": "AR/VR场景中的全姿态重建面临挑战，现有方法要么计算成本高，要么无法很好地平衡精度、时间连贯性和效率。因此提出了KineST。", "method": "利用动力学引导的双向扫描策略和混合时空表示学习方法来提取时空依赖性，并引入几何角速度损失以增强运动稳定性。", "result": "实验显示，KineST在准确性和时间一致性方面表现优越，同时保持轻量级框架。", "conclusion": "KineST提供了一种有效的方法来解决从稀疏信号中重建全姿态的问题。"}}
{"id": "2512.16784", "pdf": "https://arxiv.org/pdf/2512.16784", "abs": "https://arxiv.org/abs/2512.16784", "authors": ["Simone Teglia", "Claudia Melis Tonti", "Francesco Pro", "Leonardo Russo", "Andrea Alfarano", "Leonardo Pentassuglia", "Irene Amerini"], "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories", "categories": ["cs.CV"], "comment": "ef:Computer Analysis of Images and Patterns. CAIP 2025", "summary": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.", "AI": {"tldr": "R3ST是一个合成的三维数据集，用于提供具有现实轨迹的真实车辆行为的数据。", "motivation": "现有的真实数据集在捕捉真实的道路物体行为方面表现出色，但缺乏精确的地面实况注释。相比之下，合成数据集可以轻松标注大量帧而无需额外成本或时间，但由于使用AI模型或基于规则的系统生成轨迹，通常缺少现实车辆运动。", "method": "R3ST通过生成一个合成的三维环境并集成从SinD（一种无人机拍摄的鸟瞰图数据集）中获取的真实世界轨迹来克服这一限制。", "result": "该数据集在提供准确多模态地面实况注释和真实驾驶车辆轨迹方面填补了合成数据与现实轨迹之间的差距，从而推动了道路车辆轨迹预测的研究。", "conclusion": "R3ST通过集成真实世界的轨迹解决了现有合成数据集中缺少现实车辆运动的问题，促进了交通分析模型的训练和评估，并提升了道路安全性。"}}
{"id": "2512.16780", "pdf": "https://arxiv.org/pdf/2512.16780", "abs": "https://arxiv.org/abs/2512.16780", "authors": ["Nils Küchenmeister", "Alex Ivliev", "Markus Krötzsch"], "title": "Towards Mass Spectrum Analysis with ASP", "categories": ["cs.LO", "cs.AI"], "comment": "22 pages, 11 figures. Extended version of a paper accepted at 17th International Conference on Logic Programming and Non-monotonic Reasoning (LPNMR 2024). Under consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implemen- tation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "该论文提出了一种新的基于回答集编程（ASP）的方法来分析化学样品的分子结构。", "motivation": "通过利用相对元素丰度和结构片段的信息，开发一种更有效的分子结构发现方法，以解决大规模光谱分析中的组合问题。", "method": "使用ASP实现对分子结构进行规范表示，并通过比较不同ASP对称性破坏方法以及商业工具的性能来验证该方法的有效性和准确性。", "result": "证明了新提出的ASP方法在处理大量已知分子结构时，具有较高的准确性和较好的性能表现。", "conclusion": "ASP提供了一种有效的解决大规模光谱分析中组合问题的方法，可以用于发现化学样品中的分子结构。"}}
{"id": "2512.16776", "pdf": "https://arxiv.org/pdf/2512.16776", "abs": "https://arxiv.org/abs/2512.16776", "authors": ["Kling Team", "Jialu Chen", "Yuanzheng Ci", "Xiangyu Du", "Zipeng Feng", "Kun Gai", "Sainan Guo", "Feng Han", "Jingbin He", "Kang He", "Xiao Hu", "Xiaohua Hu", "Boyuan Jiang", "Fangyuan Kong", "Hang Li", "Jie Li", "Qingyu Li", "Shen Li", "Xiaohan Li", "Yan Li", "Jiajun Liang", "Borui Liao", "Yiqiao Liao", "Weihong Lin", "Quande Liu", "et al. (43 additional authors not shown)"], "title": "Kling-Omni Technical Report", "categories": ["cs.CV"], "comment": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "AI": {"tldr": "本文提出了Kling-Omni框架，该框架能够从多模态语言输入中直接合成高质量视频。", "motivation": "为了创造一种可以综合处理各种视频生成、编辑和智能推理任务的统一系统，从而实现高质量且具有智能化的视频内容创作。", "method": "Kling-Omni采用端到端的方法整合了视频生成、编辑和智能推理等任务，并通过构建多模态数据系统来支持这些功能。它还利用高效的预训练策略与基础设施优化。", "result": "综合评估表明，Kling-Omni在上下文生成、基于推理的编辑以及遵循多模态指令等方面表现出色。", "conclusion": "除了作为内容创作工具之外，我们相信Kling-Omni是向能够感知、推理、生成和与复杂动态世界互动的多模态世界模拟器迈进的重要一步。"}}
{"id": "2512.16771", "pdf": "https://arxiv.org/pdf/2512.16771", "abs": "https://arxiv.org/abs/2512.16771", "authors": ["Enis Baty", "C. P. Bridges", "Simon Hadfield"], "title": "FlowDet: Unifying Object Detection and Generative Transport Flows", "categories": ["cs.CV"], "comment": null, "summary": "We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.", "AI": {"tldr": "论文提出了一种新的物体检测方法FlowDet，利用现代的条件流匹配技术，改进了DiffusionDet中的扩散噪声生成问题框架。", "motivation": "为了克服扩散模型中曲线随机传输路径导致的问题，并实现更快更准确的检测性能提升，提出了使用条件流匹配进行物体检测的方法。", "method": "FlowDet采用更加直接且简单的传输路径来解决扩散过程中遇到的问题。通过学习这些直线路径，能够提高检测精度并加快计算速度。", "result": "实验结果表明，在COCO和LVIS数据集上，当限制召回率时，该方法相比于DiffusionDet可提升AP值3.6%，稀有类别对象的AP$_{rare}$值4.2%。", "conclusion": "FlowDet不仅在广泛的实验中超越了基于扩散的方法和其他非生成基线模型，还展示了其强大的泛化能力和实际应用价值。"}}
{"id": "2512.16770", "pdf": "https://arxiv.org/pdf/2512.16770", "abs": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.", "AI": {"tldr": "提出了一个将自然语言映射到系统签名的框架GinSign，用于时态逻辑翻译。", "motivation": "现有自然语言到时态逻辑转换框架依赖准确原子映射或低精度问题，缺乏有效的自动化和准确性保证。", "method": "设计了一个分层结构化的分类模型来实现自然语言片段向系统签名元素的映射，分为预测谓词标签和选择适当类型的常量参数两步。", "result": "实验结果显示，该方法在多个领域中取得了95.5%的语义等价性得分，比现有最佳方法提高1.4倍。", "conclusion": "GinSign框架显著提高了自然语言到时态逻辑翻译的准确性和效率，支持下游模型检查和验证系统行为。"}}
{"id": "2512.16767", "pdf": "https://arxiv.org/pdf/2512.16767", "abs": "https://arxiv.org/abs/2512.16767", "authors": ["Zhiyang Guo", "Ori Zhang", "Jax Xiang", "Alan Zhao", "Wengang Zhou", "Houqiang Li"], "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation", "categories": ["cs.CV"], "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/", "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.", "AI": {"tldr": "提出了一种新的前馈框架Make-It-Poseable，用于三维人物动画的姿势调整。", "motivation": "现有的自动绑定和姿态条件生成方法存在皮肤权重预测不准确、拓扑缺陷等问题，限制了它们的鲁棒性和泛化能力。为了克服这些局限性，我们引入了一种新的前馈框架。", "method": "提出的方法将角色变形视为潜在空间变换问题，通过操作形状令牌来重建新姿态，结合密集姿势表示实现精确控制，并采用潜在空间监督策略和自适应完成模块以确保高保真几何。", "result": "该方法在姿势质量上表现出色，适用于三维编辑应用如部件替换和细化。", "conclusion": "提出的Make-It-Poseable框架通过将角色变形作为潜在空间变换问题来解决现有的局限性，并展示了在3D人物动画中的优越性能。"}}
{"id": "2512.16760", "pdf": "https://arxiv.org/pdf/2512.16760", "abs": "https://arxiv.org/abs/2512.16760", "authors": ["Tianshuai Hu", "Xiaolu Liu", "Song Wang", "Yiyao Zhu", "Ao Liang", "Lingdong Kong", "Guoyang Zhao", "Zeying Gong", "Jun Cen", "Zhiyu Huang", "Xiaoshuai Hao", "Linfeng Li", "Hang Song", "Xiangtai Li", "Jun Ma", "Shaojie Shen", "Jianke Zhu", "Dacheng Tao", "Ziwei Liu", "Junwei Liang"], "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future", "categories": ["cs.RO"], "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad", "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.", "AI": {"tldr": "本文综述了自主驾驶中视觉语言行动模型（VLA）的发展，从早期的视觉行为模型到现代的VLA框架，并区分了端到端和双系统两大范式。", "motivation": "传统的感知-决策-行动管道在复杂或长尾场景下容易失效。通过结合视觉理解、语言推理和可操作输出，VLA模型可以提供更透明、更具泛化能力和与人类一致的驾驶策略。", "method": "本文将现有方法分为两大类：端到端视觉语言行动（VLA）框架和双系统视觉语言行动（VLA）框架，并进一步区分了文本生成动作和数值生成动作以及显式指导机制和隐式指导机制。", "result": "文章总结了评价基于VLA的自主驾驶系统的代表性数据集和基准测试，指出了关键挑战和未来方向，包括稳健性、可解释性和指令准确性。", "conclusion": "本文旨在为推进与人类兼容的自主驾驶系统建立一个连贯的基础。"}}
{"id": "2512.16755", "pdf": "https://arxiv.org/pdf/2512.16755", "abs": "https://arxiv.org/abs/2512.16755", "authors": ["Siqi Wang", "Chao Liang", "Yunfan Gao", "Erxin Yu", "Sen Li", "Yushi Li", "Jing Li", "Haofen Wang"], "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?", "categories": ["cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.", "AI": {"tldr": "城市探索者（CitySeeker）是一个评估视觉语言模型在动态城市环境中基于隐含需求进行实体导航的基准测试。", "motivation": "当前视觉语言模型主要擅长显式指令的导航，而如何理解并应对城市环境中的隐性人类需求仍是个挑战。本文旨在填补这一研究空白，并提供一个基准测试来评估模型的空间推理和决策能力。", "method": "设计了包含6440条轨迹、覆盖8座城市的CitySeeker数据集；通过一系列实验揭示了顶尖模型在应对长时域推断、空间认知以及经验检索方面存在的瓶颈；提出了回溯机制（Backtracking）、增强空间认知（Cognition）和基于记忆的检索（Retrieval，BCR）等策略来改善这些问题。", "result": "最前沿的视觉语言模型仅完成了21.1%的任务。研究发现这些模型在长时域推理、空间理解和经验回忆等方面存在不足，并提出了一系列改进方案。", "conclusion": "该研究表明现有视觉语言模型在处理城市实体导航中的隐含需求方面面临挑战，提出了若干策略以改善其空间智能能力，为解决‘最后一公里’的导航难题提供了有价值的见解。"}}
{"id": "2512.16750", "pdf": "https://arxiv.org/pdf/2512.16750", "abs": "https://arxiv.org/abs/2512.16750", "authors": ["Claudia Vale Oliveira", "Nelson Zagalo", "Filipe Silva", "Anabela Brandao", "Syeda Faryal Hussain Khurrum", "Joaquim Santos"], "title": "Plausibility as Failure: How LLMs and Humans Co-Construct Epistemic Error", "categories": ["cs.HC", "cs.AI"], "comment": "19 pages, 2 tables, 77 references, 6 appendices", "summary": "Large language models (LLMs) are increasingly used as epistemic partners in everyday reasoning, yet their errors remain predominantly analyzed through predictive metrics rather than through their interpretive effects on human judgment. This study examines how different forms of epistemic failure emerge, are masked, and are tolerated in human AI interaction, where failure is understood as a relational breakdown shaped by model-generated plausibility and human interpretive judgment. We conducted a three round, multi LLM evaluation using interdisciplinary tasks and progressively differentiated assessment frameworks to observe how evaluators interpret model responses across linguistic, epistemic, and credibility dimensions. Our findings show that LLM errors shift from predictive to hermeneutic forms, where linguistic fluency, structural coherence, and superficially plausible citations conceal deeper distortions of meaning. Evaluators frequently conflated criteria such as correctness, relevance, bias, groundedness, and consistency, indicating that human judgment collapses analytical distinctions into intuitive heuristics shaped by form and fluency. Across rounds, we observed a systematic verification burden and cognitive drift. As tasks became denser, evaluators increasingly relied on surface cues, allowing erroneous yet well formed answers to pass as credible. These results suggest that error is not solely a property of model behavior but a co-constructed outcome of generative plausibility and human interpretive shortcuts. Understanding AI epistemic failure therefore requires reframing evaluation as a relational interpretive process, where the boundary between system failure and human miscalibration becomes porous. The study provides implications for LLM assessment, digital literacy, and the design of trustworthy human AI communication.", "AI": {"tldr": "研究探讨了大型语言模型与人类合作时产生的认知错误及其影响。", "motivation": "目前对于大型语言模型的认知错误主要通过预测性指标进行分析，忽视了其对人类判断的解释性效应。研究旨在探索人机互动中不同形式的认知失败、隐藏及容忍现象。", "method": "采用三轮多模型评估法，使用跨学科任务和逐步差异化的评价框架观察评价者如何从语言学、认知和可信度方面解读模型反应。", "result": "发现大型语言模型的错误由预测性转为解释性形式；语言流畅性和结构连贯性掩盖了深层的意义扭曲。随着任务复杂性的增加，评价者更多依赖表面线索，导致错误但结构良好的答案被接受。", "conclusion": "认知失败不仅是模型行为的问题，更是生成可信度和人类解读捷径共同作用的结果。评估人工智能的认知失败需要重新定义为人机互动解释过程中的关系性结果。"}}
{"id": "2512.16743", "pdf": "https://arxiv.org/pdf/2512.16743", "abs": "https://arxiv.org/abs/2512.16743", "authors": ["Mahadev Prasad Panda", "Purnachandra Rao Makkena", "Srivatsa Prativadibhayankaram", "Siegfried Fößel", "André Kaup"], "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.", "AI": {"tldr": "TreeNet是一种轻量级的低比特率图像压缩模型，采用二叉树结构编码解码器和注意力特征融合机制。", "motivation": "降低计算复杂度是学习型图像压缩技术广泛应用的关键挑战。", "method": "采用二叉树结构的编码解码架构以及注意力特征融合机制来实现高效表示与重建。", "result": "在低比特率下，相较于JPEG AI，TreeNet的平均BD-rate提升了4.83%，并且模型复杂性降低了87.82%。", "conclusion": "通过广泛的消融实验，进一步探索了TreeNet内部各种潜在表征的影响，并提供了对重构因素的深入理解"}}
{"id": "2512.16740", "pdf": "https://arxiv.org/pdf/2512.16740", "abs": "https://arxiv.org/abs/2512.16740", "authors": ["Yunkai Yang", "Yudong Zhang", "Kunquan Zhang", "Jinxiao Zhang", "Xinying Chen", "Haohuan Fu", "Runmin Dong"], "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.", "AI": {"tldr": "论文提出了一种任务导向的数据合成框架TODSynth，用于改进遥感图像语义分割的任务效果。", "motivation": "当前生成可控数据在扩大标注集和减少人工注释方面有潜力，但存在对齐控制复杂性和样本质量不确定性的挑战。", "method": "提出了一个基于多模态扩散变压器（MM-DiT）的数据合成框架TODSynth，并引入了一种任务反馈引导的插件式采样策略，同时开发了动态调整生成方向的方法CRFM。", "result": "实验表明，该方法在多种场景下优于现有可控生成技术，产生了更稳定、更具任务导向性的合成数据。", "conclusion": "论文成功地解决了遥感图像语义分割中数据合成的挑战，通过提出TODSynth框架和CRFM方法实现了更为稳定且高质量的数据生成。"}}
{"id": "2512.16739", "pdf": "https://arxiv.org/pdf/2512.16739", "abs": "https://arxiv.org/abs/2512.16739", "authors": ["Yipeng Zhuang", "Yifeng Guo", "Yuewen Li", "Yuheng Wu", "Philip Leung-Ho Yu", "Tingting Song", "Zhiyong Wang", "Kunzhong Zhou", "Weifang Wang", "Li Zhuang"], "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach", "categories": ["cs.AI"], "comment": null, "summary": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.", "AI": {"tldr": "基于人工智能预测癌症疼痛发作，采用混合决策支持方法。", "motivation": "肺癌患者经常经历突发性疼痛发作，需要及时干预。为了实现主动的疼痛管理，提出了一种结合机器学习和大型语言模型的管道，利用结构化和非结构化的电子健康记录数据来预测住院48小时和72小时内可能出现的疼痛发作。", "method": "采用回顾性队列分析了266名入院患者的数据，包括人口统计学特征、肿瘤分期、生命体征和世界卫生组织三级镇痛药物使用情况。机器学习模块捕捉了药物使用的趋势，而大型语言模型解释了模糊剂量记录和自由文本临床笔记。整合这些模态提高了敏感性和可解释性。", "result": "该框架在48小时预测中达到了0.874的准确率，在72小时内达到了0.917的准确率，并且由于加入了大语义模型，敏感度分别提高了8.6%和10.4%。", "conclusion": "这种混合方法提供了一种临床可解释且可扩展的工具，用于早期疼痛发作预测，有望提高治疗精度并优化肿瘤护理中的资源配置。"}}
{"id": "2512.16733", "pdf": "https://arxiv.org/pdf/2512.16733", "abs": "https://arxiv.org/abs/2512.16733", "authors": ["Daniel Bramblett", "Rushang Karia", "Adrian Ciotinga", "Ruthvick Suresh", "Pulkit Verma", "YooJung Choi", "Siddharth Srivastava"], "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities", "categories": ["cs.AI"], "comment": null, "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.", "AI": {"tldr": "通过PDDL风格的表示法学习和建模黑盒AI系统的规划能力", "motivation": "确保黑盒AI系统安全运行，开发高效方法提供其能力的可靠且可解释的描述", "method": "使用蒙特卡洛树搜索范式创建测试任务，获取数据并修剪可能的符号模型假设空间", "result": "理论结果表明学习到的模型具有正确性、完全性和收敛性；实验证明了该方法的范围、效率和准确性", "conclusion": "提出的方法能够有效地理解和解释黑盒AI系统的能力"}}
{"id": "2512.16727", "pdf": "https://arxiv.org/pdf/2512.16727", "abs": "https://arxiv.org/abs/2512.16727", "authors": ["Haochen Chang", "Pengfei Ren", "Buyuan Zhang", "Da Li", "Tianhao Han", "Haoyang Zhang", "Liang Xie", "Hongbo Chen", "Erwei Yin"], "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition", "categories": ["cs.CV", "cs.HC"], "comment": "Project page: https://omg-bench.github.io/", "summary": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/", "AI": {"tldr": "该论文提出了OMG-Bench，一个新的用于基于骨架的在线微手势识别的大规模公共基准。", "motivation": "现有的公开数据集有限且任务特定算法难以应对微手势识别挑战。为了解决这些问题，作者开发了一个多视角自监督流水线以生成骨骼数据，并引入了OMG-Bench作为首个大规模公开基准。", "method": "提出了一个基于记忆增强变换器的端到端框架（HMATr），该框架通过存储帧级细节和窗口级别语义来统一手势检测与分类，同时使用位置感知查询来隐式编码手势的位置和语义。", "result": "实验表明，所提出的HMATr在检测率上超越了现有最佳方法7.6%，为在线微手势识别设定了一个新的基准。", "conclusion": "OMG-Bench不仅提供了大规模的公共数据集以促进研究，还提出了一个先进的模型架构（HMATr），该架构在处理基于骨架的手势识别任务中表现优异。"}}
{"id": "2512.16724", "pdf": "https://arxiv.org/pdf/2512.16724", "abs": "https://arxiv.org/abs/2512.16724", "authors": ["Yixiang Chen", "Yan Huang", "Keji He", "Peiyan Li", "Liang Wang"], "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at RA-L 2025", "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .", "AI": {"tldr": "本文提出了VERM方法，利用基础模型从构建的3D点云中想象出虚拟任务适应视角，以减少冗余信息并提高机器人操作效率。", "motivation": "在执行3D操作任务时，多摄像头设置引入了大量冗余和无关信息，增加了计算成本，并迫使模型花费额外时间提取关键的操作相关细节。为了过滤掉冗余信息并准确地提取操作相关特征，本文提出了VERM方法。", "method": "VERM利用基础模型知识从构建的3D点云中想象出虚拟任务适应视角。此外，还设计了深度感知模块和动态粗到细的过程以促进3D动作规划和精细操作。", "result": "实验结果表明，本文的方法在模拟基准RLBench和真实世界评估中均优于先前的最佳方法，并且训练时间缩短1.89倍，推理速度提高1.54倍。", "conclusion": "VERM通过利用基础模型从3D点云生成虚拟视角，有效减少了冗余信息并提高了操作效率。该方法在多种实验中的表现超过了现有最佳方法。"}}
{"id": "2512.16715", "pdf": "https://arxiv.org/pdf/2512.16715", "abs": "https://arxiv.org/abs/2512.16715", "authors": ["Oliver Stritzel", "Nick Hühnerbein", "Simon Rauch", "Itzel Zarate", "Lukas Fleischmann", "Moike Buck", "Attila Lischka", "Christian Frey"], "title": "Towards Reproducibility in Predictive Process Mining: SPICE - A Deep Learning Library", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, Predictive Process Mining (PPM) techniques based on artificial neural networks have evolved as a method for monitoring the future behavior of unfolding business processes and predicting Key Performance Indicators (KPIs). However, many PPM approaches often lack reproducibility, transparency in decision making, usability for incorporating novel datasets and benchmarking, making comparisons among different implementations very difficult. In this paper, we propose SPICE, a Python framework that reimplements three popular, existing baseline deep-learning-based methods for PPM in PyTorch, while designing a common base framework with rigorous configurability to enable reproducible and robust comparison of past and future modelling approaches. We compare SPICE to original reported metrics and with fair metrics on 11 datasets.", "AI": {"tldr": "提出SPICE框架，基于深度学习的业务流程预测方法", "motivation": "提高可重复性、决策透明度以及新数据集的易用性和基准测试能力", "method": "重新实现三种流行的深度学习基线方法，并设计了一个具有严格配置性的通用基础框架", "result": "与原始报告和公平指标进行对比，验证了SPICE的有效性", "conclusion": "SPICE提供了可重复的、稳健的方法比较平台"}}
{"id": "2512.16710", "pdf": "https://arxiv.org/pdf/2512.16710", "abs": "https://arxiv.org/abs/2512.16710", "authors": ["Chiara Di Vece", "Zhehua Mao", "Netanell Avisdris", "Brian Dromey", "Raffaele Napolitano", "Dafna Ben Bashat", "Francisco Vasconcelos", "Danail Stoyanov", "Leo Joskowicz", "Sophia Bano"], "title": "A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 3 tables", "summary": "Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.", "AI": {"tldr": "该论文建立了一个开放、跨中心、多设备的胎儿超声图像基准数据集，用于评估人工智能辅助胎儿生长检测方法。", "motivation": "手动标注解剖标志耗时且依赖操作员，对不同扫描仪和站点间的变化敏感，限制了自动化方法的一致性。需要一个基于多个来源注释的数据集来开发人工智能辅助的胎儿生长评估方法。", "method": "该数据集包含4,513张来自1904名受试者的去标识化超声图像，并涵盖了三个临床中心使用七种不同设备获取的标准平面解剖标志。提供了标准化、独立于训练/测试的数据分割，评价代码和基线结果。", "result": "通过自动胎儿生物测量模型量化领域偏移，表明仅限单个中心的训练和评估会显著高估性能，而多中心测试更准确。", "conclusion": "这是首个公开可用的跨中心、多设备且包含所有主要胎儿生物测量标志注释的数据集，为域适应和多中心泛化提供了一个稳健的基准，并有助于实现更加可靠的AI辅助胎儿生长评估。"}}
{"id": "2512.16707", "pdf": "https://arxiv.org/pdf/2512.16707", "abs": "https://arxiv.org/abs/2512.16707", "authors": ["Abhisek Ganguly"], "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems", "categories": ["cs.AI", "cs.LO"], "comment": "6 Pages, 0 figures", "summary": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.", "AI": {"tldr": "该论文形式化了限制算法智能的两个独立计算限制：正式不完备性和动态不可预测性。", "motivation": "探讨算法智能系统的推理能力和长期预测能力之间的固有限制，以阐明推理、预测和自我分析之间固有的权衡。", "method": "将正式的不完备性和动态的不可预测性形式化为两个独立的计算限制，并研究这些限制对代理预测能力的影响。", "result": "证明了算法代理无法普遍地计算其最大预测时间范围，说明推理、预测和自我分析之间的固有限制。", "conclusion": "从理论上澄清了智能系统中推理、预测和自我分析之间不可克服的权衡。"}}
{"id": "2512.16706", "pdf": "https://arxiv.org/pdf/2512.16706", "abs": "https://arxiv.org/abs/2512.16706", "authors": ["Antonella Rech", "Nicola Conci", "Nicola Garau"], "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.", "AI": {"tldr": "SDFoam通过联合学习显式Voronoi图和隐式的Signed Distance Field（SDF）来提高网格重建的精度。", "motivation": "现有的NeRF，3DGS及RadiantFoam等方法在表面重建上仍存在不足，SDFoam旨在解决这些方法难以精确重构网格的问题。", "method": "SDFoam通过使用SDF引入一致度量等值面，并利用射线追踪和Eikonal目标进行优化以实现更清晰、视图一致性更高的表面。", "result": "SDFoam在各种场景中提高了网格重建的准确性（Chamfer距离），同时保持了光度质量，训练速度与RadiantFoam相当。", "conclusion": "SDFoam通过结合显式和隐式的混合方法，在不牺牲效率的情况下实现了更准确、视图一致性的表面重构。"}}
{"id": "2512.16705", "pdf": "https://arxiv.org/pdf/2512.16705", "abs": "https://arxiv.org/abs/2512.16705", "authors": ["David Müller", "Espen Knoop", "Dario Mylonopoulos", "Agon Serifi", "Michael A. Hopkins", "Ruben Grandia", "Moritz Bächer"], "title": "Olaf: Bringing an Animated Character to Life in the Physical World", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.", "AI": {"tldr": "该论文介绍了通过强化学习和动画参考，将卡通角色奥拉夫带入现实世界。", "motivation": "为了使动画角色以非物理方式移动并保持其独特的比例，同时在机械设计和动作控制方面进行创新。", "method": "利用强化学习根据动画参考对Olaf的动作进行控制。使用软泡沫裙掩盖不对称的腿，并采用球面和平面连杆来适应内部的执行器。通过引入额外奖励减少行走时的声音冲击并防止过热。", "result": "在模拟和硬件上验证了模型的有效性，展示了卡通机器人角色的独特真实感。", "conclusion": "该方法成功地实现了奥拉夫的真实动作表现，为动画角色进入现实世界提供了新的可能性。"}}
{"id": "2512.16701", "pdf": "https://arxiv.org/pdf/2512.16701", "abs": "https://arxiv.org/abs/2512.16701", "authors": ["Giovanni Adorni"], "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences", "categories": ["cs.AI"], "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025", "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures. We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.", "AI": {"tldr": "提出网络人文主义教育框架以重新定义人类在AI学习环境中的角色和责任，通过具体案例探讨该理论的应用和挑战。", "motivation": "面对大型语言模型可能带来的认知卸载、教师职业化危机等问题，主张重构人机共作的学习生态，增强个体的批判思维能力和算法公民意识。", "method": "提出三个设计原则：反思能力、算法市民身份与对话式设计，并通过具体教学案例和认证体系来验证这些理念的实际应用效果。", "result": "表明基于提示词的学习方法可以提升知识生成过程中的主体性，同时也揭示了工作量分配、公平性和管理等方面的潜在挑战。", "conclusion": "网络人文主义教育有助于实现AI支持下的人本化教育愿景，但也需要面对和解决一系列现实问题。"}}
{"id": "2512.16698", "pdf": "https://arxiv.org/pdf/2512.16698", "abs": "https://arxiv.org/abs/2512.16698", "authors": ["Mahbub E Sobhani", "Md. Faiyaz Abdullah Sayeedi", "Mohammad Nehad Alam", "Proma Hossain Progga", "Swakkhar Shatabda"], "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning", "categories": ["cs.AI", "cs.CG"], "comment": "Accepted to the ARR October 2025 cycle", "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver", "AI": {"tldr": "该论文比较了单一代理和多代理框架在四个视觉数学基准上的表现，以评估它们解决基于图表的几何问题的能力。", "motivation": "探索多代理设计相较于单个代理是否能够更好地解决基于图表的几何问题，并为多模态大型语言模型提供指导。", "method": "系统地比较了单一代理和多代理管道在四个视觉数学基准上的表现，这些基准包括Geometry3K、MathVerse、OlympiadBench和We-Math。研究涉及开源模型（如Qwen-2.5-VL）和闭源模型（如Gemini-2.0-Flash）。", "result": "对于开源模型，多代理框架在所有测试基准上的表现均优于单个代理，尤其是在Geometry3K、OlympiadBench和We-Math上。而对于闭源模型，在经典基准上单一代理模式的表现更好，但在新基准We-Math上，多代理模式仅提供了适度的改进。", "conclusion": "多代理管道在开源模型中表现出明显的优势，并且可以帮助强大的专有系统处理新的、不太熟悉的基准问题。但并非所有情况下使用代理分解都是最优策略。"}}
{"id": "2512.16694", "pdf": "https://arxiv.org/pdf/2512.16694", "abs": "https://arxiv.org/abs/2512.16694", "authors": ["Wisnu Uriawan", "Achmad Ajie Priyajie", "Angga Gustian", "Fikri Nur Hidayat", "Sendi Ahmad Rafiudin", "Muhamad Fikri Zaelani"], "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm", "categories": ["cs.AI"], "comment": null, "summary": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.", "AI": {"tldr": "使用Apriori算法对古兰经传述进行无监督主题聚类。", "motivation": "鉴于伊斯兰文本数字化的快速增长，研究旨在自动化古兰经传述的主题分组。", "method": "采用预处理包括案例折叠、标点清理、标记化、停用词移除和词干提取等步骤，并使用Apriori算法进行关联规则挖掘分析。", "result": "发现了有意义的相关模式，如礼拜次数与祈祷之间的关系，经文与启示的关系以及传述故事的描述，表明Apriori算法能够自动揭示潜在的语义联系。", "conclusion": "研究证明了Apriori算法在无监督主题聚类中的有效性，并有助于数字伊斯兰研究和技术驱动的学习系统的发展。"}}
{"id": "2512.16688", "pdf": "https://arxiv.org/pdf/2512.16688", "abs": "https://arxiv.org/abs/2512.16688", "authors": ["Serafino Pandolfini", "Lorenzo Pellegrini", "Matteo Ferrara", "Davide Maltoni"], "title": "Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?", "categories": ["cs.CV"], "comment": "17 pages, 5 figures, 9 tables", "summary": "The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.", "AI": {"tldr": "评估现有的深度伪造检测模型在局部修复编辑中的表现。", "motivation": "现有深度伪造图像检测器对完全合成的图片表现出色，但对于区域级别操作如局部修复的能力仍需进一步研究和验证。", "method": "通过多个数据集系统性地测试了最先进的深度伪造检测模型处理局部修复任务的表现。这些数据集涵盖了不同生成器、遮罩大小以及修复技术。", "result": "实验结果显示，在大型生成器集合上训练的模型可以部分转移到基于修补的编辑，并且能够可靠地区分中等和大面积的操作或再生样式修补，超过了许多现有的临时检测方法。", "conclusion": "深度伪造图像检测模型对局部修复有一定的识别能力，但仍存在改进空间。"}}
{"id": "2512.16685", "pdf": "https://arxiv.org/pdf/2512.16685", "abs": "https://arxiv.org/abs/2512.16685", "authors": ["Gonçalo Gaspar Alves", "Shekoufeh Gorgi Zadeh", "Andreas Husch", "Ben Bausch"], "title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.", "AI": {"tldr": "研究通过指纹识别技术在少量样本下实现MRI和X射线图像中同一受试者的重新识别。", "motivation": "为了防止由于使用相同的受试者出现在不同数据集中的原因导致的模型性能膨胀，引入了指纹识别方法来映射每个受试者的图像到一个独特的潜在空间区域。", "method": "采用ResNet-50网络，并通过三元组损失训练，在3D MRI和2D X-ray数据上进行实验，评估少量样本下的指纹识别效果。", "result": "在标准场景下（20-way 1-shot）达到了99.10%的Mean-Recall@K；在更具挑战性的场景中（500-way 5-shot），ChestXray-14数据集上获得90.06%，BraTS-2021数据集上获得98.86%。", "conclusion": "该方法通过指纹识别技术能够在少量样本的情况下，准确地重新识别出MRI和X射线图像中的同一受试者。"}}
{"id": "2512.16670", "pdf": "https://arxiv.org/pdf/2512.16670", "abs": "https://arxiv.org/abs/2512.16670", "authors": ["Ole Beisswenger", "Jan-Niklas Dihlmann", "Hendrik P. A. Lensch"], "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://framediffuser.jdihlmann.com/", "summary": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.", "AI": {"tldr": "本文提出了一种条件扩散框架FrameDiffuser，用于在帧到帧的基础上将几何和材料属性转换为逼真的图像，并保持时间一致性。", "motivation": "现有的基于单图的模型无法生成具有时间一致性的帧；视频模型虽然能保证时间连贯性但计算成本过高且不适合交互式应用。本文旨在解决这些问题，提出一种既经济又适合实时渲染的方法。", "method": "FrameDiffuser通过结合ControlNet和ControlLoRA两种结构指导方式，并采取三阶段训练策略来实现稳定的自回归生成过程，同时专为特定环境进行了优化以提高推理速度和图像质量。", "result": "该框架能够在保持时间一致性的前提下生成高质量的逼真图像，具有准确的光照、阴影和反射效果。其在指定环境中优于泛化模型的表现。", "conclusion": "FrameDiffuser展示了在交互式应用中实现高效且连贯的帧渲染能力，并为未来的研究提供了新的方向。"}}
{"id": "2512.16661", "pdf": "https://arxiv.org/pdf/2512.16661", "abs": "https://arxiv.org/abs/2512.16661", "authors": ["Jacob Reiss", "Shikshya Shiwakoti", "Samuel Goldsmith", "Ujjwal Pandit"], "title": "Microsoft Academic Graph Information Retrieval for Research Recommendation and Assistance", "categories": ["cs.IR", "cs.AI"], "comment": "5 pages, 3 figures", "summary": "In today's information-driven world, access to scientific publications has become increasingly easy. At the same time, filtering through the massive volume of available research has become more challenging than ever. Graph Neural Networks (GNNs) and graph attention mechanisms have shown strong effectiveness in searching large-scale information databases, particularly when combined with modern large language models. In this paper, we propose an Attention-Based Subgraph Retriever, a GNN-as-retriever model that applies attention-based pruning to extract a refined subgraph, which is then passed to a large language model for advanced knowledge reasoning.", "AI": {"tldr": "本论文提出了一种基于注意力机制的子图检索器，利用图形神经网络从大规模信息数据库中提取精炼子图，并传递给大语言模型进行高级知识推理。", "motivation": "在当今的信息驱动世界里，尽管获取科学研究文献变得越来越容易，但筛选大量可用研究资料却变得更加困难。结合现代大型语言模型，图神经网络和图注意力机制已在大规模信息数据库搜索中显示出强大的效果。", "method": "提出了一种基于注意力的子图检索器（Attention-Based Subgraph Retriever），它是一个GNN作为检索器的模型，通过注意力机制进行修剪以提取精炼子图，并将其传递给大语言模型进行高级知识推理。", "result": "实验结果表明该方法在大规模信息数据库搜索中取得了良好的效果。", "conclusion": "基于注意力的子图检索器能够有效地从大量科学研究文献中筛选出相关性较高的资料，为科研推荐和辅助提供了有力支持。"}}
{"id": "2512.16658", "pdf": "https://arxiv.org/pdf/2512.16658", "abs": "https://arxiv.org/abs/2512.16658", "authors": ["Sangeeth B", "Serena Nicolazzo", "Deepa K.", "Vinod P"], "title": "Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.", "AI": {"tldr": "本论文提出了一种基于混沌序列的深度神经网络白盒水印技术，以保护模型知识产权。", "motivation": "随着深度神经网络在各个领域的迅速普及，其产权保护和防止滥用的问题日益突出。该研究旨在提供一种有效的方法来证明并验证模型的所有权。", "method": "利用逻辑映射生成混沌序列，并将其嵌入到选定中间层的权重中以标识所有权。通过遗传算法优化提取和再生序列之间的相似性来进行水印验证。", "result": "实验结果表明，即使经过微调，水印仍然可检测且模型准确率几乎没有损失。此外，还进行了基于激活图的分类器来区分原始、带水印和被篡改模型。", "conclusion": "该方法提供了一种灵活、可扩展的解决方案，在白盒环境中有效嵌入和验证模型的所有权，适用于需要知识产权保护的真实世界场景"}}
{"id": "2512.16656", "pdf": "https://arxiv.org/pdf/2512.16656", "abs": "https://arxiv.org/abs/2512.16656", "authors": ["Sri Yash Tadimalla", "Justin Cary", "Gordon Hull", "Jordan Register", "Daniel Maxwell", "David Pugalee", "Tina Heafner"], "title": "Comprehensive AI Literacy: The Case for Centering Human Agency", "categories": ["cs.AI", "cs.CY"], "comment": "2 figures, 2 tables", "summary": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.", "AI": {"tldr": "提出一种以人类主体性为中心的全面人工智能素养框架，旨在提高教育生态系统中所有参与者的批判性和伦理意识", "motivation": "当前的人工智能教育侧重于实用技能而忽略了批判性思维和伦理判断，形成了一种危险的知识差距。因此需要一个强调人类自主性的系统性转变", "method": "通过AI素养、流利度及能力框架来培养学生和教师对人工智能技术的深刻理解和负责任的态度", "result": "该框架为参与者提供清晰路径，以明确表达关于人工智能决策及其影响的看法", "conclusion": "真正的AI素养不仅是使用工具的能力，更是理解选择的意义，以及这些选择如何塑造学术工作、职业生涯和社会"}}
{"id": "2512.16650", "pdf": "https://arxiv.org/pdf/2512.16650", "abs": "https://arxiv.org/abs/2512.16650", "authors": ["Jirui Yang", "Hengqi Guo", "Zhihui Lu", "Yi Zhao", "Yuansen Zhang", "Shijing Hu", "Qiang Duan", "Yinggui Wang", "Tao Wei"], "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.", "AI": {"tldr": "Prefix Probing方法用于检测大语言模型中的有害内容，通过比较特定前缀的条件对数概率来生成危害评分。", "motivation": "在实际应用中，大型语言模型面临准确性、推理延迟和部署成本之间的权衡。为了提高安全敏感应用的安全性，需要一种轻量级且高效的有害内容检测方法。", "method": "Prefix Probing通过比较\"同意/执行\"与\"拒绝/安全\"前缀的条件对数概率来评估文本的危害程度，并利用前缀缓存降低检测开销至近乎首次令牌延迟。同时设计了一种高效前缀构造算法以发现高度信息性的前缀。", "result": "实验表明，Prefix Probing在保持高检测准确性的同时，仅需极小的计算成本且无需额外部署模型，效果与主流外部安全模型相当甚至更优。", "conclusion": "Prefix Probing是一种高效、实用且轻量级的安全检测方法，适用于大语言模型的实际应用。"}}
{"id": "2512.16644", "pdf": "https://arxiv.org/pdf/2512.16644", "abs": "https://arxiv.org/abs/2512.16644", "authors": ["Wisnu Uriawan", "Aria Octavian Hamza", "Ade Ripaldi Nuralim", "Adi Purnama", "Ahmad Juaeni Yunus", "Anissya Auliani Supriadi Putri"], "title": "Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam", "categories": ["cs.AI"], "comment": null, "summary": "This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.", "AI": {"tldr": "本文实现了一个基于伊斯兰教法的聊天机器人，用于回答关于伊斯兰教的问题。", "motivation": "通过提供一个准确、上下文相关的咨询平台，增强宗教教育和数字传道，并提高对经过验证的伊斯兰知识的访问。", "method": "使用Q-学习结合Sentence-Transformers进行语义嵌入的方法开发了一个基于伊斯兰教法的聊天机器人。该系统采用CRISP-DM方法论处理了25,000条问题答案配对的数据集，这些数据来源于《古兰经》、圣训等权威来源。", "result": "在功能性测试中，聊天机器人原型实现了87%的语义准确性，在包括法理学、神学、礼拜和交易在内的多个主题上表现良好。", "conclusion": "该创新为传统伊斯兰学术与现代AI咨询之间架起了桥梁。虽然对于封闭领域的查询非常有效，但仍然存在一些限制，例如静态学习和数据集依赖性，这表明未来有改进的空间，如持续适应和多轮对话支持。"}}
{"id": "2512.16639", "pdf": "https://arxiv.org/pdf/2512.16639", "abs": "https://arxiv.org/abs/2512.16639", "authors": ["Gramoz Goranci", "Shaofeng Jiang", "Peter Kiss", "Eva Szilagyi", "Qiaoyuan Yang"], "title": "Fully Dynamic Algorithms for Chamfer Distance", "categories": ["cs.DS"], "comment": "NeurIPS 2025", "summary": "We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \\subset \\mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\\mathrm{dist}_{\\mathrm{CH}}(A,B) = \\sum_{a \\in A} \\min_{b \\in B} \\textrm{dist}(a,b)$, where $\\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\\ell_p$ norm for $p \\in \\{1,2 \\}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+ε)$-approximation in $\\tilde{O}(ε^{-d})$ update time and $O(1/ε)$-approximation in $\\tilde{O}(d n^{ε^2} ε^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.", "AI": {"tldr": "研究在动态环境下计算Chamfer距离的算法，通过点插入或删除来高效维护两个点集合A和B之间的近似Chamfer距离。", "motivation": "为了适应需要频繁评估动态变化数据集的应用场景，如用作机器学习中的损失函数，提出一种新的动态算法以提高效率和准确性。", "method": "通过减少到近似最近邻搜索来实现对Chamfer距离的高效维护，并结合标准的ANN界限获得不同的时间复杂度下的近似结果。", "result": "(1+ε)近似的更新时间为~O(ε^{-d})，而O(1/ε)近似的更新时间为~O(d n^{ε^2} ε^{-4}). 实验表明该方法在真实数据集上表现良好，并具有竞争力。", "conclusion": "首次提出了一种动态维护Chamfer距离的算法，在实际应用中显示出了较好的性能和效率。"}}
{"id": "2512.16636", "pdf": "https://arxiv.org/pdf/2512.16636", "abs": "https://arxiv.org/abs/2512.16636", "authors": ["Giorgos Petsangourakis", "Christos Sgouropoulos", "Bill Psomas", "Theodoros Giannakopoulos", "Giorgos Sfikas", "Ioannis Kakogeorgiou"], "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .", "AI": {"tldr": "REGLUE 提出了一种结合视觉基础模型（VFMs）的全局和局部语义信息以改进扩散生成模型的方法。", "motivation": "现有的基于潜在空间的扩散模型存在高阶语义学习慢的问题，需要更长训练时间。REGLUE 力图通过引入 VFMs 的多层、非线性的语义信息来解决这一问题，并提高图像合成的质量和收敛速度。", "method": "REGLUE 使用单一 SiT 后端框架同时建模 VAE 图像潜在空间，VFMs 的紧凑局部（补丁级）语义以及全局（图级别）[CLS] 标签。它通过一个轻量级的卷积语义压缩器将多层 VFMs 特征非线性地聚合为低维、空间结构化的表示，并在扩散过程中与 VAE 潜在变量进行纠缠。外部对齐损失进一步规整内部表示以接近冻结的 VFM 目标。", "result": "REGLUE 在 ImageNet256x256 上的实验表明，它比 SiT-B/2、SiT-XL/2 基线和 REPA, ReDi, REG 方法在 FID 和收敛速度上都有了显著改进。实验还发现空间 VFM 语义至关重要，非线性压缩是解锁其全部益处的关键。", "conclusion": "REGLUE 提供了一种结合视觉基础模型的全局和局部语义信息的新方法来增强扩散生成模型，证明了这种方法在图像合成任务中的有效性和优越性。"}}
{"id": "2512.16635", "pdf": "https://arxiv.org/pdf/2512.16635", "abs": "https://arxiv.org/abs/2512.16635", "authors": ["Danxu Liu", "Di Wang", "Hebaixu Wang", "Haoyang Chen", "Wentao Jiang", "Yilin Cheng", "Haonan Guo", "Wei Cui", "Jing Zhang"], "title": "SARMAE: Masked Autoencoder for SAR Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE", "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.", "AI": {"tldr": "SARMAE是一种用于自监督SAR表征学习的掩码自动编码器，旨在解决由于数据稀缺和物理噪声而导致的学习难题。", "motivation": "现有针对合成孔径雷达（SAR）图像的深度学习受限于数据稀少，并且SAR特有的斑点噪声阻碍了细粒度语义表示学习。为此开发了一种新的方法来应对这些挑战。", "method": "SARMAE构建了一个大规模的百万级SAR数据集SAR-1M，同时设计Speckle-Aware Representation Enhancement (SARE)和Semantic Anchor Representation Constraint (SARC)，前者通过在掩码自动编码器中注入特定于SAR的斑点噪声来促进抗噪表征学习；后者利用配对的光学先验知识使SAR特征与语义保持一致。", "result": "实验结果显示，SARMAE在多个SAR数据集上实现了最先进的性能，在分类、检测和分割任务中表现优异。", "conclusion": "SARMAE通过自监督学习方法提升了SAR图像的表征能力，解决了由于斑点噪声及数据稀少带来的挑战。"}}
{"id": "2512.16626", "pdf": "https://arxiv.org/pdf/2512.16626", "abs": "https://arxiv.org/abs/2512.16626", "authors": ["Barna Pásztor", "Thomas Kleine Buening", "Andreas Krause"], "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game", "categories": ["cs.LG", "cs.AI", "cs.GT", "cs.MA", "stat.ML"], "comment": "10 pages, 5 tables, 1 figures", "summary": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.", "AI": {"tldr": "本文提出了Stackelberg学习框架，通过领导者和跟随者之间的互动优化人类偏好。", "motivation": "解决传统RLHF和NLHF方法在处理复杂偏好结构时的不足，提出了一种新的基于序列博弈的学习方式，以更好地捕捉和优化人类偏好。", "method": "SLHF将问题视为领导者先行动然后跟随者根据领导者的动作响应的游戏。这种方法能够通过迭代采样提高模型对不同数据集的一致性和鲁棒性。", "result": "实验表明该方法在大规模语言模型上表现良好，并且可以跨参数规模和模型家族进行迁移学习，无需额外微调。", "conclusion": "SLHF是一种有效的方法，能够在各种偏好设置下实现强大而一致的优化效果。"}}
{"id": "2512.16625", "pdf": "https://arxiv.org/pdf/2512.16625", "abs": "https://arxiv.org/abs/2512.16625", "authors": ["Linghui Shen", "Mingyue Cui", "Xingyi Yang"], "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers", "categories": ["cs.CV"], "comment": "17 pages, 11 figures", "summary": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.", "AI": {"tldr": "本文提出了一种名为DeContext的方法，旨在保护图像免受未经授权的上下文编辑。", "motivation": "现有的基于DiT的模型容易被用于身份冒充、误导信息等恶意行为。作者通过注入微小的针对性扰动来削弱这些跨注意力通道，从而防止非法编辑。", "method": "方法的关键在于通过对输入图像施加小的有针对性的干扰，破坏上下文传播路径，这主要发生在多模态注意层中。此外还集中于早期去噪步骤和特定的变压器块以增强效果。", "result": "实验表明，DeContext能够有效阻止未经授权的编辑同时保持良好的视觉质量。", "conclusion": "基于注意力的扰动是一种强大的防御图像篡改的有效方法"}}
{"id": "2512.16620", "pdf": "https://arxiv.org/pdf/2512.16620", "abs": "https://arxiv.org/abs/2512.16620", "authors": ["Kanwal Aftab", "Graham Adams", "Mark Scanlon"], "title": "Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation", "categories": ["cs.CV"], "comment": null, "summary": "Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.", "AI": {"tldr": "利用电插座作为室内标记，通过三阶段深度学习管道实现多媒体地理定位。", "motivation": "室内多媒体地理定位技术在打击人口贩卖、儿童剥削等严重犯罪中具有重大应用前景。然而，由于室内外环境差异大和技术挑战多，现有方法难以满足需求。", "method": "提出了一种基于电插座的三阶段深度学习管道：第一阶段使用YOLOv11检测插座（mAP@0.5 = 0.843），第二阶段利用Xception分类器将插座类型分为12类（准确率=0.912），第三阶段通过阈值大于90%的置信度匹配国家代码（准确率=0.96）。为解决数据稀缺问题，创建了两个专用数据集。", "result": "在Hotel-50K数据集上进行评估时，该管道展示了处理复杂现实条件的能力。整个框架提供了一个实用的方法来实现真实世界的数字取证应用。", "conclusion": "通过利用标准化的电插座类型作为室内标记点，有效解决了现有室内多媒体地理定位技术面临的挑战，并提供了可应用于实际场景中的解决方案。"}}
{"id": "2512.16615", "pdf": "https://arxiv.org/pdf/2512.16615", "abs": "https://arxiv.org/abs/2512.16615", "authors": ["Yifan Zhou", "Zeqi Xiao", "Tianyi Wei", "Shuai Yang", "Xingang Pan"], "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/SingleZombie/LLSA", "summary": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA", "AI": {"tldr": "本文提出了Log-linear Sparse Attention (LLSA)，一种针对极长序列的可训练稀疏注意机制，通过层次化结构将选择和注意力成本从二次降低到对数线性复杂度。", "motivation": "扩散变压器（DiTs）在视觉生成方面处于领先地位，但其平方级自注意计算限制了对长令牌序列的扩展。现有的Top-K稀疏注意方法虽然减少了DiT的计算量，但仍存在选择成本和质量下降的问题。作者认为这是由于单一层次设计的不足。", "method": "LLSA采用分层Top-K选择机制，并引入Hierarchical KV Enrichment保留全局上下文的同时减少令牌使用。该方法利用稀疏索引进行高效训练，并在高分辨率像素空间图像生成任务中进行了评估。", "result": "在256x256像素令牌序列上，LLSA将注意力推理加速了28.27倍，DiT训练加速了6.09倍，同时保持了生成质量。", "conclusion": "LLSA为高效训练长序列的DiTs提供了一个有前景的方向。"}}
{"id": "2512.16614", "pdf": "https://arxiv.org/pdf/2512.16614", "abs": "https://arxiv.org/abs/2512.16614", "authors": ["Giulia Boato", "Andrea Montibeller", "Edward Delp", "Luisa Verdoliva", "Daniele Miorandi"], "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.MM"], "comment": null, "summary": "AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.", "AI": {"tldr": "提出了一种可靠的AI取证代理框架，用于选择和组合检测器，识别来源和上下文，并提供不确定性校准的评估。", "motivation": "当前的解决方案存在一些问题，需要改进多媒体认证过程以提高可靠性。", "method": "引入了统一的AI取证代理框架，强调不确定性的考量并提升证据可信度验证流程。", "result": "该框架可以更好地进行真实性验证并提高系统的解释性。", "conclusion": "提出的AI取证代理框架有助于改善现有的多媒体认证技术，并提供更可靠和透明的结果。"}}
{"id": "2512.16609", "pdf": "https://arxiv.org/pdf/2512.16609", "abs": "https://arxiv.org/abs/2512.16609", "authors": ["Ayush Bhavsar"], "title": "Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment", "categories": ["cs.CV"], "comment": "4 pages, 2 figures. Code and demo available at https://doi.org/10.5281/zenodo.17915355", "summary": "This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.", "AI": {"tldr": "本文介绍了Hazedefy，一个轻量级的去雾处理管道，适用于实时视频和现场摄像头增强。", "motivation": "旨在解决现有去雾方法计算复杂度高、难以在普通硬件上实现的问题。通过简化算法和优化设计以适应移动和嵌入式设备的需求。", "method": "基于暗通道先验（DCP）概念及大气散射模型，采用伽玛自适应重建技术，快速传输近似算法以及稳定的光估计方法，并提供可选的颜色平衡阶段。", "result": "实验结果表明，在真实世界图像和视频上处理后，Hazedefy能够显著提升视觉清晰度与对比度且不需要GPU加速。", "conclusion": "提出的Hazedefy管道在计算效率、实际部署性方面表现出色，适用于移动设备的实时去雾需求。"}}
{"id": "2512.16602", "pdf": "https://arxiv.org/pdf/2512.16602", "abs": "https://arxiv.org/abs/2512.16602", "authors": ["Iker García-Ferrero", "David Montero", "Roman Orus"], "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.", "AI": {"tldr": "介绍了一种在推理时间对大型语言模型的政治敏感话题拒绝行为进行精细控制的方法，无需重新训练。", "motivation": "为了更精准地控制大模型在政治敏感话题上的回应方式，并且保持其安全性和性能的平衡。", "method": "采用了一个作为裁判的大模型来为拒绝行为分配信心分数，并通过计算导向向量（steering vectors）来实现对模型响应的精细调整。使用了岭回归变体以更好地分离拒绝与服从的方向，从而改善控制效果。", "result": "该方法在Qwen3-Next-80B-A3B-Thinking上有效移除了政治敏感话题上的拒绝行为，并且保持了安全性和基准测试中的接近基线性能。这种方法可以推广到4B和80B模型，并可以根据需要诱导出特定的拒绝行为。", "conclusion": "该研究展示了通过激活导向（activation steering）去除政治拒绝行为的同时保留有害内容的安全对齐，为推理时间提供了一条实际可行、透明可控的调节路径。"}}
{"id": "2512.16586", "pdf": "https://arxiv.org/pdf/2512.16586", "abs": "https://arxiv.org/abs/2512.16586", "authors": ["Shaohua Wu", "Tong Yu", "Shenling Wang", "Xudong Zhao"], "title": "Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.", "AI": {"tldr": "本文提出了Yuan-TecSwin，一种基于Swin-transformer块的条件扩散模型，用于改进图像生成。", "motivation": "卷积神经网络（CNN）中的局部操作限制了模型理解长距离语义信息的能力。为解决此问题并提高文本与图像对齐的效果，本文提出使用Swin-transformer替代CNN以增强非本地建模能力。", "method": "通过在编码器和解码器中应用Swin-transformer块来改进特征提取和图像恢复；采用适配的时间步长搜索不同扩散阶段，进一步提高推断性能。此外，优化了文本编码器的选择、有效利用文本嵌入以及文本条件融入的设计。", "result": "Yuan-TecSwin在ImageNet生成基准测试中达到了1.37的FID分数，并且模型生成的图像难以被人类面试官分辨出与人工绘制的不同。", "conclusion": "通过使用Swin-transformer块代替CNN，该方法显著提高了文本条件扩散模型的效果和性能。"}}
{"id": "2512.16584", "pdf": "https://arxiv.org/pdf/2512.16584", "abs": "https://arxiv.org/abs/2512.16584", "authors": ["Jintao Tong", "Jiaqi Gu", "Yujing Lou", "Lubin Fan", "Yixiong Zou", "Yue Wu", "Jieping Ye", "Ruixuan Li"], "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs", "categories": ["cs.CV"], "comment": "14 pages, 11 figures", "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.", "AI": {"tldr": "提出了一种在多模态大型语言模型中融合视觉想象和文本推理的新方法。", "motivation": "当前的多模态大型语言模型在需要视觉想象力的任务上表现不佳，而人类可以在同一脑空间内灵活地进行视觉-文本交互。受此启发，研究者希望将视觉信息无缝融入到文本推理过程中。", "method": "提出了Sketch-in-Latents（SkiLa）方法，在多模态推理中融合视觉想象和文本推理，通过连续的潜藏图像嵌入实现动态切换模式。该机制确保了这些潜伏图元具有语义基础。", "result": "实验表明，所提出的SkiLa在以视觉为中心的任务上表现优秀，并且对各种通用多模态基准测试展示出了强大的泛化能力。", "conclusion": "通过融合视觉想象和文本推理，SkiLa实现了更自然的统一多模态思考过程，提高了模型在复杂任务中的性能。"}}
{"id": "2512.16577", "pdf": "https://arxiv.org/pdf/2512.16577", "abs": "https://arxiv.org/abs/2512.16577", "authors": ["Nico Albert Disch", "Saikat Roy", "Constantin Ulrich", "Yannick Kirchhoff", "Maximilian Rokuss", "Robin Peretzke", "David Zimmerer", "Klaus Maier-Hein"], "title": "CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series", "categories": ["cs.CV"], "comment": "https://github.com/MIC-DKFZ/Longitudinal4DMed", "summary": "Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.", "AI": {"tldr": "CRONOS 是一种用于从多个过去扫描中进行连续时间序列到图像预测的统一框架，特别适用于医学影像。", "motivation": "现有的模型要么依赖于单一先前扫描，要么固定在网格时间上或针对全局标签，这限制了不规则采样下的体素级预测能力。CRONOS旨在解决这一问题。", "method": "CRONOS 学习了一个时空速度场，该场可以将上下文体积传输到任意时间的目标体积，在3D像素空间中操作。", "result": "在三个公共数据集上，CRONOS的性能优于基线模型，并且计算成本具有竞争力。", "conclusion": "CRONOS是首个实现连续序列到医学图像预测的方法，代码和评估协议将公开发布以支持跨多数据集的可重复基准测试。"}}
{"id": "2512.16567", "pdf": "https://arxiv.org/pdf/2512.16567", "abs": "https://arxiv.org/abs/2512.16567", "authors": ["Yin Zhang", "Yongqiang Zhang", "Yaoyue Zheng", "Bogdan Raducanu", "Dan Liu"], "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026", "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.", "AI": {"tldr": "本文提出了Causal-Tune方法，通过分离和利用视觉基础模型中的因果因素来改善领域泛化的语义分割性能。", "motivation": "现有工作在使用视觉基础模型进行领域泛化语义分割时，忽视了预训练模型中存在的非因果因子对性能的负面影响。这些因子存在于特征的低频和高频部分，并影响其表示能力。", "method": "采用离散余弦变换（DCT）提取每一层特征的频率谱，利用高斯带通滤波器分离出因果与非因果成分；引入一组在频域操作的因果感知可学习令牌来进一步优化因果成分，而非因果成分则被丢弃。通过逆向DCT将精炼后的特征转换回空间域并传递至下一层次。", "result": "实验表明Causal-Tune方法在多种跨领域任务中表现优异，在恶劣天气条件下尤其有效，例如雪景下提升了+4.8%的mIoU。", "conclusion": "通过识别和分离因果因素以抑制非因果影响，该方法显著提高了视觉基础模型在域泛化语义分割中的性能。"}}
{"id": "2512.16564", "pdf": "https://arxiv.org/pdf/2512.16564", "abs": "https://arxiv.org/abs/2512.16564", "authors": ["Kirill Mazur", "Marwan Taher", "Andrew J. Davison"], "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction", "categories": ["cs.CV"], "comment": "For project page, see https://makezur.github.io/4DPM/", "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps. Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity. The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.", "AI": {"tldr": "该论文提出了一种动态重建系统，能够从单目RGB视频中持续性地重建场景。", "motivation": "为了实现对整个场景的完整和持久重建，包括所有之前可见的部分，并且能够在不同时间点上回放重建结果。", "method": "通过将场景分解为一组刚性的3D基本体并估计密集2D对应关系，系统能够推断这些基本体在场景中的刚性运动。利用优化管道实现4D场景重建，并引入机制以预测不可见物体的运动。", "result": "该系统在对象扫描和多对象数据集上显著优于现有方法，在定量和定性方面均表现出色。", "conclusion": "提出的系统能够在单目RGB视频中实现4D时空感知，具备回放时间轴上的3D重建、多对象扫描以及物体持久性的功能。"}}
{"id": "2512.16561", "pdf": "https://arxiv.org/pdf/2512.16561", "abs": "https://arxiv.org/abs/2512.16561", "authors": ["Yuxin Wang", "Lei Ke", "Boqiang Zhang", "Tianyuan Qu", "Hanxun Yu", "Zhenpeng Huang", "Meng Yu", "Dan Xu", "Dong Yu"], "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": "Project Page: https://n3d-vlm.github.io", "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.", "AI": {"tldr": "提出N3D-VLM框架，通过原生3D感知能力提高视觉语言模型在三维场景中的空间理解和推理准确性。", "motivation": "现有多模态模型缺乏对三维物体的感知能力，影响其理解三维空间关系和深度线索的能力。因此，需要一个能够进行精确三维定位和解释性空间理解的方法。", "method": "开发了一个集成原生3D物体感知与3D视觉推理的统一框架，并通过构建大规模数据集提升训练效果。", "result": "实验结果显示该方法在3D定位任务上达到最先进的性能，在3D空间推理上也超越现有方法。", "conclusion": "N3D-VLM框架能够显著提高基于图像的视觉语言模型的空间理解和三维感知能力。"}}
{"id": "2512.16555", "pdf": "https://arxiv.org/pdf/2512.16555", "abs": "https://arxiv.org/abs/2512.16555", "authors": ["Marcelo Rosa", "José E. R. Cury", "Fabio L. Baldissera"], "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots", "categories": ["cs.RO", "cs.FL", "cs.MA", "eess.SY"], "comment": null, "summary": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots", "AI": {"tldr": "本文提出了一种基于监督控制理论的模块化合成方法，用于多机器人协调构建3-D结构。", "motivation": "解决多机器人自主构造预定义3-D结构时的协调问题。", "method": "采用监督控制理论从单个机器人的模型和目标结构中合成了一个正确的、反应性的控制器（即监督者），并在其他机器人上复制此监督者以完成目标结构。", "result": "通过该方法，可以有效实现多机器人之间的协调构建3-D结构。", "conclusion": "所提出的基于模块化合成的监督控制理论能够成功应用于复杂3-D结构的多机器人自主建造中。"}}
{"id": "2512.16553", "pdf": "https://arxiv.org/pdf/2512.16553", "abs": "https://arxiv.org/abs/2512.16553", "authors": ["Yumeng Wang", "Tianyu Fan", "Lingrui Xu", "Chao Huang"], "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild", "categories": ["cs.AI", "cs.CL"], "comment": "Data and code are available at https://github.com/Tango-Whiskyman/Needle_in_the_Web", "summary": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.", "AI": {"tldr": "本文介绍了针对复杂、模糊查询的网页检索基准测试Needle in the Web。", "motivation": "现有评估标准如BrowseComp和xBench-DeepSearch侧重于多跳合成推理任务，忽略了模糊探索性搜索。为此，本文旨在填补这一研究空白，提出新的基准测试以评估现代搜索引擎和LLM系统在处理语义模糊查询方面的性能。", "method": "该研究设计了一个包含663个问题的基准测试集，覆盖七个领域，并采用灵活的方法生成具有可控难度的问题，以便更好地评估系统的检索能力。", "result": "三个领先的大规模语言模型和三种基于代理的搜索系统在新的基准测试中的表现不佳，多数模型准确率低于35%，没有一个系统能够在所有领域或不同难度级别上都表现出色。", "conclusion": "Needle in the Web揭示了当前搜索引擎在处理语义模糊查询方面的挑战，并强调了有效检索的重要性。"}}
{"id": "2512.16532", "pdf": "https://arxiv.org/pdf/2512.16532", "abs": "https://arxiv.org/abs/2512.16532", "authors": ["Himanshu Gharat", "Himanshi Agrawal", "Gourab K. Patro"], "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment", "categories": ["cs.AI", "cs.IR"], "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)", "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.", "AI": {"tldr": "研究探讨了记忆增强的AI招聘代理在个性化过程中引入和放大偏见的风险。", "motivation": "尽管记忆增强了LLM的功能，但其带来的个性化偏见风险尚未充分探究。通过模拟带有安全训练LLM的记忆增强型个人化代理的行为，该研究旨在揭示这些系统如何在各个操作阶段中引入并放大偏见。", "method": "使用招聘作为案例场景，实验了具有安全训练的大型语言模型的AI代理，在不同运行阶段和条件下测试其行为。", "result": "实验证明，通过个性化，记忆增强型代理会系统性地引入并强化偏见。", "conclusion": "研究强调了在内存增强LLM驱动的人工智能代理中采取额外保护措施或设置操作守则的重要性。"}}
{"id": "2512.16531", "pdf": "https://arxiv.org/pdf/2512.16531", "abs": "https://arxiv.org/abs/2512.16531", "authors": ["Ander Alvarez", "Alessandro Genuardi", "Nilotpal Sinha", "Antonio Tiene", "Samuel Mugel", "Román Orús"], "title": "Scaling Laws for Energy Efficiency of Local LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.", "AI": {"tldr": "该论文研究了在边缘设备上部署大型语言模型和视觉-语言模型时，基于中央处理器的推理计算成本与输入长度或分辨率之间的关系，并探讨了量子启发式压缩技术的效果。", "motivation": "现代人工智能部署主要依靠图形处理单元，但大多数消费类硬件依赖中央处理器。缺乏针对仅使用中央处理器进行本地推断的工作负载的计算规律，因此需要研究和量化这些模型在边缘设备上的性能。", "method": "通过在两种代表性的中央处理器层级上对大型语言和视觉-语言模型进行基准测试（即MacBook Pro M2和Raspberry Pi 5），使用连续采样处理和内存使用的方法，并结合面积积分来表征计算负载随输入长度或分辨率变化的关系。此外，还探讨了量子启发式压缩技术的效果。", "result": "发现两种经验法则：语言模型推理的计算成本与标记长度呈近似线性关系；视觉-语言模型存在预处理驱动的“分辨率膝盖”现象，在内部分辨率限制之上计算保持恒定，在低于该限制时急剧下降。此外，量子启发式压缩技术可以减少处理器和内存使用高达71.9%，能耗降低至62%。", "conclusion": "这些结果系统地量化了多模式中央处理器单机推理的规模法则，并指出模型压缩和输入分辨率预处理是可持续边缘推断的有效低成本杠杆。"}}
{"id": "2512.16530", "pdf": "https://arxiv.org/pdf/2512.16530", "abs": "https://arxiv.org/abs/2512.16530", "authors": ["Primoz Kocbek", "Leon Kopitar", "Gregor Stiglic"], "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.", "AI": {"tldr": "利用大型语言模型简化生物医学文本，提高健康素养。", "motivation": "通过简化生物医学文本以提升公众的健康素养。", "method": "使用公共数据集开发和评估了几种方法：基线提示模板、两个AI代理以及微调法。选择了OpenAI gpt-4o 和gpt-4o mini作为基准模型，并应用了定量指标（Flesch-Kincaid等级水平，SMOG指数，SARI，BERTScore，G-Eval）和定性指标（五点李克特量表：简洁、准确、完整）。", "result": "结果表明gpt-4o-mini表现最佳，而微调方法表现较差。定量指标G-Eval显示了有希望的结果，并与定性评估一致。", "conclusion": "大型语言模型在简化生物医学文本方面表现出色，特别是gpt-4o-mini模型，在未来的应用中具有潜力。"}}
{"id": "2512.16529", "pdf": "https://arxiv.org/pdf/2512.16529", "abs": "https://arxiv.org/abs/2512.16529", "authors": ["Julien Gachadoat", "Guillaume Lagarde"], "title": "ParamExplorer: A framework for exploring parameters in generative art", "categories": ["cs.AI", "cs.HC", "cs.SE"], "comment": "16 pages, 3 figures", "summary": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.", "AI": {"tldr": "介绍了一种用于探索生成艺术参数空间的框架ParamExplorer", "motivation": "解决艺术家在高维和复杂参数空间中寻找美学上吸引人的输出时面临的挑战，通过减少手动试错提高发现潜在有趣配置的可能性", "method": "开发一个基于强化学习的人机交互式模块化框架，并评估几种探索策略", "result": "实现并评估了多种探索策略，证明了该框架的有效性", "conclusion": "ParamExplorer为艺术家提供了一个强大的工具来探索和优化生成艺术的参数空间"}}
{"id": "2512.16523", "pdf": "https://arxiv.org/pdf/2512.16523", "abs": "https://arxiv.org/abs/2512.16523", "authors": ["Zhiwei Li", "Yitian Pang", "Weining Wang", "Zhenan Sun", "Qi Li"], "title": "TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.", "AI": {"tldr": "提出了一种在推理时用于对抗性检测和增强适应性的轻量级防御框架Test-Time Padding (TTP)。", "motivation": "现有模型在对抗性扰动下表现脆弱，且现有的测试时间策略无法有效区分干净输入与对抗性输入。", "method": "通过空间填充前后的CLIP特征嵌入余弦相似度偏移进行对抗性检测，并对识别出的对抗案例采用可训练填充来恢复中断的关注模式。", "result": "实验显示TTP在多种模型和数据集上均优于现有测试时间防御方法，显著提高了对抗鲁棒性而不牺牲清洁准确性。", "conclusion": "提出了一个有效的解决方案TTP，用于增强视觉语言模型的安全性和准确性。"}}
{"id": "2512.16519", "pdf": "https://arxiv.org/pdf/2512.16519", "abs": "https://arxiv.org/abs/2512.16519", "authors": ["Nikolaos Ellinas", "Alexandra Vioni", "Panos Kakoulidis", "Georgios Vamvoukakis", "Myrsini Christidou", "Konstantinos Markopoulos", "Junkwang Oh", "Gunu Jho", "Inchul Hwang", "Aimilios Chalamandaris", "Pirros Tsiakoulis"], "title": "Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "This paper introduces a cepstrum-based pitch modification method that can be applied to any mel-spectrogram representation. As a result, this method is compatible with any mel-based vocoder without requiring any additional training or changes to the model. This is achieved by directly modifying the cepstrum feature space in order to shift the harmonic structure to the desired target. The spectrogram magnitude is computed via the pseudo-inverse mel transform, then converted to the cepstrum by applying DCT. In this domain, the cepstral peak is shifted without having to estimate its position and the modified mel is recomputed by applying IDCT and mel-filterbank. These pitch-shifted mel-spectrogram features can be converted to speech with any compatible vocoder. The proposed method is validated experimentally with objective and subjective metrics on various state-of-the-art neural vocoders as well as in comparison with traditional pitch modification methods.", "AI": {"tldr": "本文提出了一种基于伪谱的方法，用于对基于mel频谱图的神经声码器进行音调修改。", "motivation": "为了在不改变模型的情况下，使任何基于mel的声码器能够执行音调调整任务。", "method": "通过直接在cepstrum特征空间中移动谐波结构来实现音调调整。首先计算伪逆mel变换后的频谱图幅度，然后应用DCT转换到cepstrum域，在此领域无需估计峰值位置即可进行音调偏移，并通过IDCT和mel滤波器组重新计算修改后的mel。", "result": "实验结果表明该方法在各种最先进的神经声码器上的客观评价和主观感知均优于传统的方法。", "conclusion": "提出了一种新颖的基于cepstrum的音调调整方法，能够在不改变模型的情况下应用于任何基于mel的声码器，并且效果较好。"}}
{"id": "2512.16518", "pdf": "https://arxiv.org/pdf/2512.16518", "abs": "https://arxiv.org/abs/2512.16518", "authors": ["Xuefu Dong", "Liqiang Xu", "Lixing He", "Zengyi Han", "Ken Christofferson", "Yifei Chen", "Akihito Taya", "Yuuki Nishiyama", "Kaoru Sezaki"], "title": "Poster: Recognizing Hidden-in-the-Ear Private Key for Reliable Silent Speech Interface Using Multi-Task Learning", "categories": ["cs.HC", "eess.AS"], "comment": "UbiComp Poster 2025", "summary": "Silent speech interface (SSI) enables hands-free input without audible vocalization, but most SSI systems do not verify speaker identity. We present HEar-ID, which uses consumer active noise-canceling earbuds to capture low-frequency \"whisper\" audio and high-frequency ultrasonic reflections. Features from both streams pass through a shared encoder, producing embeddings that feed a contrastive branch for user authentication and an SSI head for silent spelling recognition. This design supports decoding of 50 words while reliably rejecting impostors, all on commodity earbuds with a single model. Experiments demonstrate that HEar-ID achieves strong spelling accuracy and robust authentication.", "AI": {"tldr": "本文提出了一种使用消费者主动降噪耳机识别隐藏在耳内的私人密钥的方法，以实现可靠的手势无声语音界面。", "motivation": "无声语音接口（SSI）允许手部自由输入而无需发出声音，但大多数SS系统不验证说话者身份。为了解决这一问题，并提供可靠的身份验证和无声拼写识别，本文提出了HEar-ID。", "method": "该方法使用主动降噪耳机捕捉低频“耳语”音频和高频超声波反射。两个信号流通过共享编码器生成嵌入式特征，然后用于对比分支进行用户身份验证以及SS头部实现无声拼写识别。", "result": "实验表明，HEar-ID能够以高准确率解码50个单词，并且可以有效拒绝冒充者，在普通耳机上使用单个模型即可实现可靠的身份认证和无声语音输入。", "conclusion": "本文提出的HEar-ID能够在提供可靠身份验证的同时支持无声拼写识别，所有功能都基于普通的主动降噪耳机，显示了在消费者设备上的强大应用潜力。"}}
{"id": "2512.16515", "pdf": "https://arxiv.org/pdf/2512.16515", "abs": "https://arxiv.org/abs/2512.16515", "authors": ["Pradeep Singh", "Mudasani Rushikesh", "Bezawada Sri Sai Anurag", "Balasubramanian Raman"], "title": "The Universe Learning Itself: On the Evolution of Dynamics from the Big Bang to Machine Intelligence", "categories": ["nlin.AO", "cs.AI"], "comment": "38 pages, 3 figures", "summary": "We develop a unified, dynamical-systems narrative of the universe that traces a continuous chain of structure formation from the Big Bang to contemporary human societies and their artificial learning systems. Rather than treating cosmology, astrophysics, geophysics, biology, cognition, and machine intelligence as disjoint domains, we view each as successive regimes of dynamics on ever-richer state spaces, stitched together by phase transitions, symmetry-breaking events, and emergent attractors. Starting from inflationary field dynamics and the growth of primordial perturbations, we describe how gravitational instability sculpts the cosmic web, how dissipative collapse in baryonic matter yields stars and planets, and how planetary-scale geochemical cycles define long-lived nonequilibrium attractors. Within these attractors, we frame the origin of life as the emergence of self-maintaining reaction networks, evolutionary biology as flow on high-dimensional genotype-phenotype-environment manifolds, and brains as adaptive dynamical systems operating near critical surfaces. Human culture and technology-including modern machine learning and artificial intelligence-are then interpreted as symbolic and institutional dynamics that implement and refine engineered learning flows which recursively reshape their own phase space. Throughout, we emphasize recurring mathematical motifs-instability, bifurcation, multiscale coupling, and constrained flows on measure-zero subsets of the accessible state space. Our aim is not to present any new cosmological or biological model, but a cross-scale, theoretical perspective: a way of reading the universe's history as the evolution of dynamics itself, culminating (so far) in biological and artificial systems capable of modeling, predicting, and deliberately perturbing their own future trajectories.", "AI": {"tldr": "本文构建了一个统一的动态系统叙事，从宇宙大爆炸到现代机器智能，追溯结构形成的连续链。", "motivation": "将宇宙学、天体物理学、地球物理学、生物学、认知科学和机器学习视作动态系统演化的不同阶段，强调其数学模式的一致性和连贯性。", "method": "通过描述从基本场动力学到生命起源、进化生物学到大脑运作的全过程，揭示了各个领域之间的连续性及其背后的共同动力学原理。", "result": "提出了一个跨尺度理论视角，将宇宙的历史视为动态系统的演化过程，并强调了这种观点对于理解生物和人工系统如何预测和塑造未来的重要性。", "conclusion": "当前人类文化和技术的发展（包括现代机器学习）可以被看作是通过模拟、预测并有意地改变自身未来的轨迹来实现的。"}}
{"id": "2512.16512", "pdf": "https://arxiv.org/pdf/2512.16512", "abs": "https://arxiv.org/abs/2512.16512", "authors": ["Pompougnac Hugo", "Guillon Christophe", "Noiry Sylvain", "Dutilleul Alban", "Iooss Guillaume", "Rastello Fabrice"], "title": "XTC, A Research Platform for Optimizing AI Workload Operators", "categories": ["cs.PF", "cs.AI"], "comment": null, "summary": "Achieving high efficiency on AI operators demands precise control over computation and data movement. However, existing scheduling languages are locked into specific compiler ecosystems, preventing fair comparison, reuse, and evaluation across frameworks. No unified interface currently decouples scheduling specification from code generation and measurement. We introduce XTC, a platform that unifies scheduling and performance evaluation across compilers. With its common API and reproducible measurement framework, XTC enables portable experimentation and accelerates research on optimization strategies.", "AI": {"tldr": "XTC是一个研究平台，用于统一AI工作负载操作符的调度和性能评估。", "motivation": "现有的调度语言锁定在特定编译器生态系统中，无法进行公平比较、重用和评估。缺乏一个将调度规范与代码生成和测量分离的统一接口。", "method": "XTC平台提供了一个通用API和可重复测量框架来实现跨编译器的调度和性能评估的统一化。", "result": "XTC使得便携式实验成为可能，加速了优化策略的研究。", "conclusion": "通过引入XTC平台，研究者可以在不同的编译器生态系统中进行公平比较、重用和评估。"}}
{"id": "2512.16511", "pdf": "https://arxiv.org/pdf/2512.16511", "abs": "https://arxiv.org/abs/2512.16511", "authors": ["Hossein Javidnia"], "title": "Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.", "AI": {"tldr": "该论文提出了MAGINet，一种用于从单张RGB肖像准确预测面部光不变的扩散反射图的技术。", "motivation": "精确地分解不受控制光照条件下的脸部图像对于实现逼真的重照明、高保真数字双胞胎和增强现实效果至关重要。", "method": "MAGINet利用了分层残差编码，空间和通道注意机制以及自适应多尺度特征融合技术。初始预测的反射图通过轻量级三层CNN进一步细化，并且基于此反射图生成五个其他渲染通路：环境遮挡、表面法线、镜面反射、透明度和原始扩散色彩。", "result": "与先前的方法相比，该论文提出的完整管道在FFHQ-UV-Intrinsics数据集上训练时，在扩散反射预测方面取得了最先进的性能，并且整个渲染流程的保真度也得到了显著提高。", "conclusion": "该方法生成的结果通路能够实现真实面部图像的高质量重照明和材料编辑。"}}
{"id": "2512.16504", "pdf": "https://arxiv.org/pdf/2512.16504", "abs": "https://arxiv.org/abs/2512.16504", "authors": ["Qiushuo Cheng", "Jingjing Liu", "Catherine Morgan", "Alan Whone", "Majid Mirmehdi"], "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.", "AI": {"tldr": "本文提出了一种基于骨架的视频动作定位方法，通过多尺度特征融合和对比学习提升时空敏感性。", "motivation": "现有的自监督预训练范式在基于骨架的动作识别中取得了成功，但针对动作边界的检测，仍需更细腻的时间敏感特性来区分相邻帧的变化。", "method": "本文采用片段差异性任务进行自监督预训练，并通过U形模块融合中间特征以增强分辨率和定位能力。", "result": "实验表明该方法在BABEL数据集上优于现有基于骨架的对比学习方法，在PKUMMD上的迁移学习性能也达到了领先水平。", "conclusion": "所提出的方法能够有效地提升基于骨架的动作定位精度，展示了其在动作边界的精细区分和高分辨率帧级定位方面的优越性。"}}
{"id": "2512.16501", "pdf": "https://arxiv.org/pdf/2512.16501", "abs": "https://arxiv.org/abs/2512.16501", "authors": ["Beitong Zhou", "Zhexiao Huang", "Yuan Guo", "Zhangxuan Gu", "Tianyu Xia", "Zichen Luo", "Fei Tang", "Dehan Kong", "Yanyi Shang", "Suling Ou", "Zhenlin Guo", "Changhua Meng", "Shuheng Shen"], "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks", "categories": ["cs.CV"], "comment": null, "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.", "AI": {"tldr": "本文介绍了VenusBench-GD，一个跨平台的GUI基准测试工具，用于评估GUI代理的能力。", "motivation": "现有的GUI接地基准存在数据量不足、领域覆盖狭窄或过于专注于单一平台的问题。因此需要一个新的全面的多平台GUI基准来解决这些问题。", "method": "VenusBench-GD建立了一个大规模、跨平台的基准测试，涵盖了广泛的应用程序和UI元素，并且具有高质量的数据构建流程以提高标注准确性。此外还提出了一种分层任务分类法，将接地分为基本和高级类别，包括六个不同的子任务。", "result": "实验结果表明通用多模态模型在基础接地任务上与专门的GUI模型相当甚至更好；但在高级任务中仍然更倾向于使用GUI专用模型，但这些模型表现出显著过度拟合和较差的鲁棒性。", "conclusion": "此研究证明了全面多层次评估框架对于评估模型能力的重要性。"}}
{"id": "2512.16494", "pdf": "https://arxiv.org/pdf/2512.16494", "abs": "https://arxiv.org/abs/2512.16494", "authors": ["Mengyuan Liu", "Jiajie Liu", "Jinyan Zhang", "Wenhao Li", "Junsong Yuan"], "title": "PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation", "categories": ["cs.CV", "cs.AI"], "comment": "IEEE Transactions on Image Processing (T-IP)", "summary": "The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.", "AI": {"tldr": "本文提出了一种用于单目三维人体姿态估计的Mixture-of-Experts网络PoseMoE，通过将深度信息与二维姿态解耦来提高整体估计精度。", "motivation": "现有的基于提升的方法在利用检测到的二维姿态作为中间表示时，会因为深度不确定性引入而限制最终的估计准确性。本文揭示了深度表征的重要性，并提出了解决方案。", "method": "PoseMoE采用了混合专家网络设计，其中专门化的专家模块用于细化良好检测出的二维姿态特征和学习深度特征。此外还提出了跨专家知识聚合模块以增强特征通过双向映射在二维姿态与深度之间。", "result": "实验结果表明，在Human3.6M、MPI-INF-3DHP和3DPW等三个广泛使用的数据集上，本文提出的PoseMoE方法优于传统的基于提升的方法。", "conclusion": "论文展示了如何通过改进的网络设计来提高单目三维人体姿态估计的质量，强调了解耦深度信息的重要性并验证了该方案的有效性。"}}
{"id": "2512.16493", "pdf": "https://arxiv.org/pdf/2512.16493", "abs": "https://arxiv.org/abs/2512.16493", "authors": ["Huma Hafeez", "Matthew Garratt", "Jo Plested", "Sankaran Iyer", "Arcot Sowmya"], "title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images", "categories": ["cs.CV"], "comment": "Conference paper just submitted", "summary": "The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.", "AI": {"tldr": "提出YOLO11-4K框架，用于实时检测4K全景图像中的小物体。", "motivation": "针对4K或更高分辨率的全景图像中常规对象检测器（如YOLO）存在的计算需求高和准确性不足的问题，研发一种高效的实时检测方法。", "method": "设计了一种新型多尺度检测头和GhostConv为基础的骨干网，以提高对小物体的敏感性和减少计算复杂性。同时构建了CVIP360数据集进行评估。", "result": "YOLO11-4K在mAP为0.95的同时实现了每帧28.3毫秒的延迟，并且与YOLO11相比，准确度提升了0.95比0.908，延迟降低了75%。", "conclusion": "该框架适用于实时处理高分辨率全景图像中的对象检测任务，在自动驾驶导航、监控和增强现实领域具有广泛的应用前景。"}}
{"id": "2512.16491", "pdf": "https://arxiv.org/pdf/2512.16491", "abs": "https://arxiv.org/abs/2512.16491", "authors": ["Theresa Eimer", "Lennart Schäpermeier", "André Biedenkapp", "Alexander Tornede", "Lars Kotthoff", "Pieter Leyman", "Matthias Feurer", "Katharina Eggensperger", "Kaitlin Maile", "Tanja Tornede", "Anna Kozak", "Ke Xue", "Marcel Wever", "Mitra Baratchi", "Damir Pulatov", "Heike Trautmann", "Haniye Kashgarani", "Marius Lindauer"], "title": "Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network", "categories": ["cs.AI"], "comment": null, "summary": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.", "AI": {"tldr": "总结了COSEAL社区在元算法研究中的最佳实践，涵盖实验周期的各个方面", "motivation": "当前关于元算法的研究存在大量潜在错误源，这威胁着科学见解的有效性和可扩展性。这些最佳实践分散于不同的出版物和领域之中，并且彼此独立地演变。", "method": "收集了COSEAL社区在元算法研究中的良好实践，从制定研究问题、选择实验设计到执行实验及最终的分析与结果呈现。", "result": "建立了当前元算法研究领域的现状，并为新研究人员和从业人员提供了指南。", "conclusion": "该报告作为指导性文件，有助于提高元算法研究的有效性和可重复性"}}
{"id": "2512.16485", "pdf": "https://arxiv.org/pdf/2512.16485", "abs": "https://arxiv.org/abs/2512.16485", "authors": ["Kejun Liu", "Yuanyuan Liu", "Lin Wei", "Chang Tang", "Yibing Zhan", "Zijing Chen", "Zhe Chen"], "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by TMM", "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.", "AI": {"tldr": "本文提出了一个结合眼睛行为和面部表情的多模态情感识别数据集EMER，并设计了增强的情感识别模型EMERT。", "motivation": "当前的情感识别主要依赖于面部表情，但这些表情往往是社交工具而非真实情绪的表现。为此，作者引入眼睛行为作为重要的情绪线索来弥补这种差距。", "method": "通过刺激材料诱导自发性情绪以采集真实数据，并同时收集眼睛运动序列和眼动图等非侵入式眼部数据以及面部视频。设计了EMERT模型，使用模态对抗特征解耦与多任务Transformer结合眼球行为作为面部表情的有效补充。", "result": "实验结果表明，EMERT在多种评价协议中优于其他最先进的多模态方法，显示出了眼睛行为对情感识别的重要性。", "conclusion": "本文通过分析眼部行为在情感识别中的重要性，推进了情感识别的研究以更好地解决面部表情与真实情绪之间的差距问题。"}}
{"id": "2512.16484", "pdf": "https://arxiv.org/pdf/2512.16484", "abs": "https://arxiv.org/abs/2512.16484", "authors": ["Yuan Li", "Yahan Yu", "Youyuan Lin", "Yong-Hao Yang", "Chenhui Chu", "Shin'ya Nishida"], "title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.", "AI": {"tldr": "该论文研究了如何通过模型获取人类类似的盲图质量评估（BIQA）能力，采用强化学习方法，并设计了一个奖励机制来驱动模型进行自我一致性推理。", "motivation": "模仿人类感知-推理的流程来进行图像质量评估，提高BIQA系统的性能和可解释性。", "method": "收集了反映人类感知-推理过程的数据，使用人类标注作为强化学习中的奖励信号，引导模型朝向类似人类的感知和推理；通过设计特定的奖励机制使模型仅从自生成描述中推断图像质量。", "result": "该方法在Pearson和Spearman相关系数等通用指标下达到了与最先进的BIQA系统相当的成绩，并且ROUGE-1得分达到0.512，表明了人类解释的大范围覆盖。", "conclusion": "通过模仿人类感知-推理过程并采用强化学习的方法，实现了更加接近于人类的图像质量评估模型。"}}
{"id": "2512.16483", "pdf": "https://arxiv.org/pdf/2512.16483", "abs": "https://arxiv.org/abs/2512.16483", "authors": ["Senmao Li", "Kai Wang", "Salman Khan", "Fahad Shahbaz Khan", "Jian Yang", "Yaxing Wang"], "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.", "AI": {"tldr": "本文提出了StageVAR，一种针对视觉自回归模型的阶段感知加速框架。", "motivation": "传统的视觉自回归（VAR）模型在大规模步骤中存在计算复杂性和运行时间增加的问题。现有的加速方法依赖于手动步骤选择，并未考虑生成过程中不同阶段的重要性差异。", "method": "StageVAR通过分析早期和后期步骤的不同作用，设计了一种无需额外训练的插件式加速策略，利用后期计算中的语义无关性和低秩特性进行优化。", "result": "实验结果显示，StageVAR在GenEval指标下降不超过0.01且DPG指标仅降低0.26的情况下，实现了高达3.4倍的速度提升，并优于现有的加速基准方法。", "conclusion": "研究表明，阶段感知设计是视觉自回归图像生成的有效原则。"}}
{"id": "2512.16472", "pdf": "https://arxiv.org/pdf/2512.16472", "abs": "https://arxiv.org/abs/2512.16472", "authors": ["Omar Namnakani", "Yasmeen Abdrabou", "John H. Williamson", "Mohamed Khamis"], "title": "Investigating the Effect of Encumbrance on Gaze- and Touch-based Target Acquisition on Handheld Mobile Devices", "categories": ["cs.HC"], "comment": "20 pages, 9 figures", "summary": "The potential of using gaze as an input modality in the mobile context is growing. While users often encumber themselves by carrying objects and using mobile devices while walking, the impact of encumbrance on gaze input performance remains unexplored. To investigate this, we conducted a user study (N=24) to evaluate the effect of encumbrance on the performance of 1) Gaze using Dwell time (with/without visual feedback), 2) GazeTouch (with/without visual feedback), and 3) One- or two-hand touch input. While Touch generally performed better, Gaze, especially with feedback, showed a consistent performance regardless of whether participants were encumbered or unencumbered. Participants' preferences for input modalities varied with encumbrance: they preferred Gaze when encumbered, and touch when unencumbered. Our findings enhance understanding of the effect of encumbrance on gaze input and contribute towards selecting appropriate input modalities in future mobile user interfaces to account for situational impairments.", "AI": {"tldr": "研究手持移动设备中负担感对视线和触摸输入方式影响的研究", "motivation": "探讨用户携带物品或在行走时使用移动设备的情况下，负载对视线作为输入模式性能的影响。当前关于此方面的研究较少。", "method": "通过一项包含24名参与者的用户体验研究来评估负载下，1) 使用停留时间的视线操作（有/无视觉反馈），2）结合触摸和注视的操作（有/无视觉反馈），以及3）单手或双手触摸输入的表现。", "result": "尽管触摸输入通常表现更好，但视线输入，尤其是带有反馈的情况下，在用户受负载影响时也保持了稳定的性能。参与者在不同情况下对输入方式的偏好也有所不同：当受到负担时更倾向于使用视线输入；不受负担时则更喜欢触摸输入。", "conclusion": "研究揭示了负载感对视线输入的影响，并为未来移动界面的设计提供了依据，特别是在考虑到情境性障碍的情况下选择适当的输入模式方面。"}}
{"id": "2512.16469", "pdf": "https://arxiv.org/pdf/2512.16469", "abs": "https://arxiv.org/abs/2512.16469", "authors": ["Jiayu Zhang", "Kaixing Zhao", "Tianhao Shao", "Bin Guo", "Liang He"], "title": "Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing", "categories": ["cs.RO"], "comment": null, "summary": "Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.", "AI": {"tldr": "提出了一个名为Tri-Select的多阶段视觉数据选择框架，用于过滤冗余和低质量图像。", "motivation": "移动视觉众包传感产生的数据通常冗余且异构，导致了需要一种有效的筛选方法来解决这些问题。", "method": "提出了一种三阶段的方法：第一阶段基于元数据的过滤；第二阶段是利用空间相似度进行光谱聚类；第三阶段采用最大独立集搜索指导下的视觉特征选择。", "result": "实验表明，Tri-Select可以提高图像的选择效率和质量。", "conclusion": "Tri-Select框架对于大规模的移动视觉众包应用非常有效。"}}
{"id": "2512.16468", "pdf": "https://arxiv.org/pdf/2512.16468", "abs": "https://arxiv.org/abs/2512.16468", "authors": ["Danial Safaei", "Siddartha Khastgir", "Mohsen Alirezaei", "Jeroen Ploeg", "Son Tong", "Xingyu Zhao"], "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery", "categories": ["cs.AI"], "comment": null, "summary": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.", "AI": {"tldr": "通过引入决定性特征保真度（DFF）来量化和弥补合成图像与真实图像之间的行为差距，提高自动驾驶车辆仿真测试的可靠性。", "motivation": "尽管在视觉真实性方面取得了进步，但现有的像素级保真度不足以确保从模拟到现实世界的可靠转移。真正的关键在于系统是否基于相同因果证据进行决策，而不仅仅是图像看起来像真是不是。因此需要一种新的衡量标准来捕捉机制上的对等性。", "method": "提出了决定性特征保真度（DFF）这一新指标，并使用可解释的人工智能方法比较在真实和模拟环境中驱动SUT输出的决定性特征。还提出基于反事实解释的实际估算器以及引导校准方案，以提高模拟器的真实感。", "result": "实验表明DFF揭示了传统输出值保真度所忽略的差异并改善了决定性和输入级保真度而不会牺牲输出值保真度。", "conclusion": "引入的DFF指标和相关方法有助于提高仿真测试与现实世界的对齐，从而增强自动驾驶车辆的安全性保证。"}}
{"id": "2512.16465", "pdf": "https://arxiv.org/pdf/2512.16465", "abs": "https://arxiv.org/abs/2512.16465", "authors": ["Jinwu Chen", "Qidie Wu", "Bin Li", "Lin Ma", "Xin Si", "Yang Hu", "Shouyi Yin", "Jun Yang"], "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution", "categories": ["cs.AI"], "comment": null, "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.", "AI": {"tldr": "cuPilot是一种策略协调的多代理框架，旨在通过引入策略作为内核演化的中间语义表示来优化CUDA内核。", "motivation": "现有的自动CUDA内核优化方法由于其不完善的代理设计和进化表示之间的匹配问题，在性能上表现不佳。因此需要一种更有效的解决方案，以克服这些限制并提高优化后的内核性能。", "method": "cuPilot通过策略协调的演化算法、基于Roofline模型的提示技术和策略级别的种群初始化来实现CUDA内核的自动优化。", "result": "实验表明，在包含100个基准内核的数据集上，由cuPilot生成的内核平均比PyTorch快3.09倍。在GEMM任务中，它展示了复杂的优化并实现了对关键硬件单元的高度利用。", "conclusion": "通过引入策略作为中间语义表示，cuPilot能够更有效地实现CUDA内核的自动优化，并显著提高其性能"}}
{"id": "2512.16461", "pdf": "https://arxiv.org/pdf/2512.16461", "abs": "https://arxiv.org/abs/2512.16461", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.", "AI": {"tldr": "SNOW是一个无需训练且不依赖骨干网络的框架，用于统一理解4D场景。该框架结合了Vision-Language模型（VLM）语义与点云几何和时间一致性。", "motivation": "自主机器人系统需要对动态环境进行时空理解以确保可靠导航和交互。然而，现有的视觉语言模型缺乏3D几何定位及时间动态信息的支撑；同时，基于几何感知的方法虽然捕捉结构和运动但存在语义稀疏问题。", "method": "SNOW利用同步RGB图像和3D点云数据生成对象级别的建议，并通过HDBSCAN聚类引导SAM2分割。每个分段区域通过提出的时空标记化补丁编码（STEP）进行编码，产生多模态令牌捕捉局部的语义、几何和时间属性。这些令牌被整合到4D场景图中，形成4D先验知识供下游推理使用。", "result": "SNOW在多种基准测试上实现了精确的4D场景理解和空间定位推断，并且设置了多个设置下的新的最先进的性能。", "conclusion": "SNOW证明了结构化的4D先验知识对于具身推理和自主机器人的重要性。"}}
{"id": "2512.16456", "pdf": "https://arxiv.org/pdf/2512.16456", "abs": "https://arxiv.org/abs/2512.16456", "authors": ["Masashi Hatano", "Saptarshi Sinha", "Jacob Chalk", "Wei-Hong Li", "Hideo Saito", "Dima Damen"], "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach", "categories": ["cs.CV"], "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/", "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.", "AI": {"tldr": "该论文提出了一个基于文本条件的扩散模型，用于生成符合自然人体行为的视线引导的目标抓取动作。", "motivation": "模仿真实的人体运动是计算机图形学和机器人领域的重要任务之一。通过利用大量数据集中的视向引导目标抓取动作序列，提高合成的动作与现实人类行为的一致性。", "method": "论文首先收集并整理了来自五个公开数据集中共23.7K的视线导向的目标抓取运动序列，然后训练了一个文本条件下的扩散模型，并通过目标姿势或位置对模型进行微调。最后，采用多种指标评估生成动作的有效性。", "result": "在最大的HD-EPIC数据集上，当以目标物体的位置作为条件时，该模型达到了60%的视向成功率和89%的目标抓取成功率。", "conclusion": "论文展示了通过结合文本条件和大量的视线导向运动数据，可以有效地生成更加自然且准确的人体动作。"}}
{"id": "2512.16455", "pdf": "https://arxiv.org/pdf/2512.16455", "abs": "https://arxiv.org/abs/2512.16455", "authors": ["Ignacio Heredia", "Álvaro López García", "Germán Moltó", "Amanda Calatrava", "Valentin Kozlov", "Alessandro Costantini", "Viet Tran", "Mario David", "Daniel San Martín", "Marcin Płóciennik", "Marta Obregón Ruiz", "Saúl Fernandez", "Judith Sáinz-Pardo Díaz", "Miguel Caballer", "Caterina Alarcón Marín", "Stefan Dlugolinsky", "Martin Šeleng", "Lisana Berberi", "Khadijeh Alibabaei", "Borja Esteban Sanchis", "Pedro Castro", "Giacinto Donvito", "Diego Aguirre", "Sergio Langarita", "Vicente Rodriguez", "et al. (7 additional authors not shown)"], "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.", "AI": {"tldr": "描述了一种支持科学研究中人工智能工作的联邦计算平台。", "motivation": "为了提供可重复部署，一致且透明的访问方式，以满足在物理分布的不同e-基础设施上的科学工作负载需求。", "method": "通过一个全面的服务目录，该平台能够为用户提供覆盖整个机器学习生命周期的一体化用户体验。它包括模型开发、训练和部署，并提供了工具支持模型的追溯性和可重复性。", "result": "此平台还与各种人工智能模型提供商、数据集及存储资源集成，使用户可以与其更广泛的机器学习生态系统互动。", "conclusion": "该平台易于定制以降低外部社区的采用障碍。"}}
{"id": "2512.16454", "pdf": "https://arxiv.org/pdf/2512.16454", "abs": "https://arxiv.org/abs/2512.16454", "authors": ["Tianhao Shao", "Kaixing Zhao", "Feng Liu", "Lixin Yang", "Bin Guo"], "title": "AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems", "categories": ["cs.RO"], "comment": null, "summary": "As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable \"user\". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.", "AI": {"tldr": "提出了一种基于行为和预测的调度框架AG-MPBS，用于优化空中地面无人系统的任务分配效率。", "motivation": "随着无人机在城市感知和应急响应中的重要性增加，有效利用这些自主设备完成时间敏感的任务已成为关键挑战。", "method": "MPBS整合了三个核心模块：行为感知KNN分类器、基于时变马尔可夫模型的预测机制以及考虑任务紧急性和基站性能的动态优先级调度算法。", "result": "在真实数据集上的实验表明，该框架显著提高了任务完成效率和资源利用率。", "conclusion": "AG-MPBS为无人系统的智能协作调度提供了有效的解决方案。"}}
{"id": "2512.16453", "pdf": "https://arxiv.org/pdf/2512.16453", "abs": "https://arxiv.org/abs/2512.16453", "authors": ["Jiayang Yang", "Chunhui Zhao", "Martin Guay", "Zhixing Cao"], "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.", "AI": {"tldr": "通过TimeSeries2Report（TS2R）框架将原始锂离子电池运行时间序列转换为结构化、语义增强的报告，使大型语言模型能够在储能系统管理中进行推理、预测和决策。", "motivation": "大型语言模型在解释多变量时间序列数据方面具有潜力，但将其应用于实际电池储能系统的操作和维护仍然未被充分探索。TS2R旨在填补这一空白。", "method": "TS2R通过结合分段、语义抽象和基于规则的解释将短期动态编码为自然语言，从而将低级传感器信号与高级上下文见解联系起来。该框架在实验室规模和真实数据集上进行基准测试，并评估报告质量和异常检测、状态预测及充放电管理等下游任务的表现。", "result": "TS2R相较于基于视觉、嵌入式和文本的基线方法，在准确性、鲁棒性和可解释性指标方面始终提高了大型语言模型的性能。整合后的LLM达到了专家级别的决策质量，并且无需重新训练或架构修改即可保持预测一致性。", "conclusion": "TS2R为适应性、以LLM驱动的电池智能提供了一条实用路径，展示了在储能系统管理中的潜力和效果。"}}
{"id": "2512.16449", "pdf": "https://arxiv.org/pdf/2512.16449", "abs": "https://arxiv.org/abs/2512.16449", "authors": ["Abhishek Kashyap", "Yuxuan Yang", "Henrik Andreasson", "Todor Stoyanov"], "title": "Single-View Shape Completion for Robotic Grasping in Clutter", "categories": ["cs.RO"], "comment": null, "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.", "AI": {"tldr": "通过使用扩散模型从单视图部分深度观测中进行类别级别的3D形状补全，从而改进机器人抓取任务。", "motivation": "在基于视觉的机器人操作中，单一摄像机视角仅能捕获物体的一面，并且在混乱环境中可能会进一步受到遮挡。这种不完整的几何信息导致了抓取估计算法性能不佳。", "method": "利用扩散模型从单视图的部分深度观测中进行3D形状补全，以重建完整对象的几何结构，提供更丰富的上下文供抓取规划使用。这种方法专注于具有多样几何特征的家庭常用物品，并生成完整的3D形状作为下游抓取推断网络的输入。", "result": "在混乱场景中的初步评估显示，该方法相比于没有形状补全的简单基线和最近的状态-of-the-art形状补全方法，在抓取成功率上分别高出23%和19%。", "conclusion": "通过从单视图深度观测中进行有效的3D形状补全，可以显著提高混乱环境中的机器人抓取性能。"}}
{"id": "2512.16448", "pdf": "https://arxiv.org/pdf/2512.16448", "abs": "https://arxiv.org/abs/2512.16448", "authors": ["Shabnam Bagheri Marzijarani", "Mohammad Zolfaghari", "Hedieh Sajedi"], "title": "IoMT-based Automated Leukemia Classification using CNN and Higher Order Singular Value", "categories": ["cs.LG", "cs.AI"], "comment": "ef:2023 9th International Conference on Web Research (ICWR)", "summary": "The Internet of Things (IoT) is a concept by which objects find identity and can communicate with each other in a network. One of the applications of the IoT is in the field of medicine, which is called the Internet of Medical Things (IoMT). Acute Lymphocytic Leukemia (ALL) is a type of cancer categorized as a hematic disease. It usually begins in the bone marrow due to the overproduction of immature White Blood Cells (WBCs or leukocytes). Since it has a high rate of spread to other body organs, it is a fatal disease if not diagnosed and treated early. Therefore, for identifying cancerous (ALL) cells in medical diagnostic laboratories, blood, as well as bone marrow smears, are taken by pathologists. However, manual examinations face limitations due to human error risk and time-consuming procedures. So, to tackle the mentioned issues, methods based on Artificial Intelligence (AI), capable of identifying cancer from non-cancer tissue, seem vital. Deep Neural Networks (DNNs) are the most efficient machine learning (ML) methods. These techniques employ multiple layers to extract higher-level features from the raw input. In this paper, a Convolutional Neural Network (CNN) is applied along with a new type of classifier, Higher Order Singular Value Decomposition (HOSVD), to categorize ALL and normal (healthy) cells from microscopic blood images. We employed the model on IoMT structure to identify leukemia quickly and safely. With the help of this new leukemia classification framework, patients and clinicians can have real-time communication. The model was implemented on the Acute Lymphoblastic Leukemia Image Database (ALL-IDB2) and achieved an average accuracy of %98.88 in the test step.", "AI": {"tldr": "基于IoMT的急性淋巴细胞白血病分类，使用CNN和高阶奇异值分解方法", "motivation": "解决人工诊断白血病耗时长且易出错的问题，采用AI技术提高癌症识别准确性与效率", "method": "结合卷积神经网络(CNN)和高阶奇异值分解(HOSVD)，应用于IoMT结构中以快速准确地分类急性淋巴细胞白血病和正常细胞", "result": "在急性淋巴细胞白血病图像数据库(ALL-IDB2)上测试，模型的平均精度达到98.88%", "conclusion": "提出的新框架能够实现实时患者与医生之间的通讯，并有效提升了急性淋巴细胞白血病的诊断速度和准确性"}}
{"id": "2512.16447", "pdf": "https://arxiv.org/pdf/2512.16447", "abs": "https://arxiv.org/abs/2512.16447", "authors": ["Sören Auer", "Allard Oelen", "Mohamad Yaser Jaradeh", "Mutahira Khalid", "Farhana Keya", "Sasi Kiran Gaddipati", "Jennifer D'Souza", "Lorenz Schlüter", "Amirreza Alasti", "Gollam Rabby", "Azanzi Jiomekong", "Oliver Karras"], "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.", "AI": {"tldr": "提出了一种面向研究人员的AI辅助平台TIB AIssistant的设计愿景，旨在通过模块化组件支持科研全过程。", "motivation": "鉴于生成式人工智能和大型语言模型的进步对科学研究有巨大潜力但存在整合困难的问题，论文提出了一个跨学科领域的协作人机平台。", "method": "描述了该平台的概念框架、系统架构以及早期原型的实现情况，展示了其可行性和潜在影响。", "result": "展示了一个初步原型的功能，并论证了此方法的有效性与重要性。", "conclusion": "TIB AIssistant作为一种通用的人工智能辅助科研工具，在提升科研效率和质量方面展现出巨大潜力。"}}
{"id": "2512.16446", "pdf": "https://arxiv.org/pdf/2512.16446", "abs": "https://arxiv.org/abs/2512.16446", "authors": ["Enis Yalcin", "Joshua O'Hara", "Maria Stamatopoulou", "Chengxu Zhou", "Dimitrios Kanoulas"], "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)", "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.", "AI": {"tldr": "该论文提出了一种环境感知的强化学习框架E-SDS，通过整合视觉语言模型和实时地形传感器分析，自动生成奖励函数以训练具有感知能力的人形机器人步态策略。", "motivation": "现有的基于视觉语言模型的方法在人形机器人的运动控制中表现出了潜力，但由于缺乏对环境的感知能力，难以处理复杂的地形。因此，论文提出了一种新的方法来解决这一问题。", "method": "E-SDS框架将视觉语言模型与实时传感器分析相结合，自动生成适应复杂地形的奖励函数。这些函数在训练过程中帮助机器人学习更加稳健和灵活的运动策略。", "result": "实验结果显示，在四种不同地形上，使用E-SDS方法训练的策略比手工设计的奖励或非感知自动基线更能成功地完成楼梯下降任务，并且减少了速度跟踪误差51.9%到82.6%。", "conclusion": "该框架显著降低了手动设计奖励所需的人力工作量（从几天减少到不到两小时），同时生成了更加稳健和灵活的运动策略。"}}
{"id": "2512.16445", "pdf": "https://arxiv.org/pdf/2512.16445", "abs": "https://arxiv.org/abs/2512.16445", "authors": ["Roman Akramov", "Artem Khamatullin", "Svetlana Glazyrina", "Maksim Kryzhanovskiy", "Roman Ischenko"], "title": "Topic Modelling Black Box Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": null, "summary": "Choosing the number of topics $T$ in Latent Dirichlet Allocation (LDA) is a key design decision that strongly affects both the statistical fit and interpretability of topic models. In this work, we formulate the selection of $T$ as a discrete black-box optimization problem, where each function evaluation corresponds to training an LDA model and measuring its validation perplexity. Under a fixed evaluation budget, we compare four families of optimizers: two hand-designed evolutionary methods - Genetic Algorithm (GA) and Evolution Strategy (ES) - and two learned, amortized approaches, Preferential Amortized Black-Box Optimization (PABBO) and Sharpness-Aware Black-Box Optimization (SABBO). Our experiments show that, while GA, ES, PABBO, and SABBO eventually reach a similar band of final perplexity, the amortized optimizers are substantially more sample- and time-efficient. SABBO typically identifies a near-optimal topic number after essentially a single evaluation, and PABBO finds competitive configurations within a few evaluations, whereas GA and ES require almost the full budget to approach the same region.", "AI": {"tldr": "本文将LDA模型中主题数量的选择问题转化为一个离散的黑盒优化问题，并比较了四种优化器在该问题上的表现。", "motivation": "选择适当的主题数T对Latent Dirichlet Allocation (LDA) 模型的拟合度和可解释性至关重要。为了有效解决此问题，文章提出将主题数量的选择转化为黑盒优化问题进行求解。", "method": "研究使用了四种方法来解决这个问题：遗传算法（GA），进化策略（ES），偏好化分摊黑盒优化（PABBO）以及锐度感知黑盒优化（SABBO）。通过这些方法训练LDA模型并评估其验证困惑度，以确定最佳主题数量。", "result": "实验结果表明，在有限的计算预算下，学习到的、基于分摊的方法更加样本和时间效率高。特别地，SABBO通常在一次迭代中就能找到接近最优的主题数，而PABBO则能够在几次迭代内找到竞争性的配置。", "conclusion": "研究展示了如何通过黑盒优化方法有效解决LDA模型主题数量的选择问题，并且学习到的、基于分摊的方法相较于传统的进化算法更具有优势。"}}
{"id": "2512.16444", "pdf": "https://arxiv.org/pdf/2512.16444", "abs": "https://arxiv.org/abs/2512.16444", "authors": ["Yadong Li", "Tong Zhang", "Bo Huang", "Zhen Cui"], "title": "StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm", "categories": ["cs.AI"], "comment": "15 pages, 11 figures", "summary": "Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \\href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.", "AI": {"tldr": "建立一个用于多智能体算法对抗评估的StarCraft II对战环境SC2BA，以解决现有基准测试中对手单一、多样性不足的问题。", "motivation": "现有的MARL算法评估使用固定的内置AI模式作为对手，导致了算法评价中的多样性和适应性不足。因此需要建立新的评估标准来提高评估的质量和实用性。", "method": "开发了一个基于StarCraft II的多智能体算法对抗环境SC2BA以及相应的APyMARL库，用于在双算法对战和多算法混合对战两种模式下进行算法间的对比测试。", "result": "通过SC2BA进行了广泛的基准实验，发现了一些关于现有经典MARL算法的有效性、敏感性和可扩展性的有趣观察结果和问题。", "conclusion": "SC2BA环境及其相关实验已公开发布于Github。该工作将为未来的多智能体强化学习领域提供新的评估标准和发展方向。"}}
{"id": "2512.16443", "pdf": "https://arxiv.org/pdf/2512.16443", "abs": "https://arxiv.org/abs/2512.16443", "authors": ["Shangxun Li", "Youngjung Uh"], "title": "Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.", "AI": {"tldr": "通过几何视角调整文本嵌入，改进基于单一提示的无训练自由方法以提高主题一致性和文本对齐。", "motivation": "现有文本到图像生成模型在生成高质量图像方面表现良好但无法保持多输出间主题一致性。传统方法如模型微调或图像条件化需要大量计算和个别优化。", "method": "通过几何视角调整文本嵌入，抑制不必要的语义信息以减少不同帧之间的语义纠缠。", "result": "实验表明该方法在提高主题一致性和文本对齐方面显著优于现有基线。", "conclusion": "本研究提出了一种简单有效的无训练自由方法来解决文本到图像生成中的主题一致性问题。"}}
{"id": "2512.16442", "pdf": "https://arxiv.org/pdf/2512.16442", "abs": "https://arxiv.org/abs/2512.16442", "authors": ["Allard Oelen", "Sören Auer"], "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles", "categories": ["cs.AI"], "comment": null, "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.", "AI": {"tldr": "TIB AIssistant是一个支持整个研究生命周期的人工智能平台，它由一系列辅助工具组成，每个工具负责特定的研究任务，并提供与外部学术服务交互的接口。", "motivation": "随着人工智能和大型语言模型在社会各个领域的广泛应用，特别是在学术领域的影响日益显著。为了更好地利用这些技术来提高研究人员的工作效率和支持他们的研究活动，在整个研究生命周期内提供了这样的支持平台是有必要的。", "method": "TIB AIssistant由一系列负责不同研究任务的辅助工具组成，并且提供与外部学术服务交互的功能，同时生成的数据可以被存储和导出以确保透明度并增强研究成果的可复制性。通过逐步展示各助手之间的互动来说明AIssistant的主要功能。", "result": "展示了TIB AIssistant如何支持研究者在各个阶段的任务，并最终形成一份草稿研究报告。", "conclusion": "TIB AIssistant为研究人员提供了一个社区维护的人工智能辅助平台的基础，这将有助于推动更多基于人工智能的研究工作。"}}
{"id": "2512.16433", "pdf": "https://arxiv.org/pdf/2512.16433", "abs": "https://arxiv.org/abs/2512.16433", "authors": ["Maeve Madigan", "Parameswaran Kamalaruban", "Glenn Moynihan", "Tom Kempton", "David Sutton", "Stuart Burrell"], "title": "Emergent Bias and Fairness in Multi-Agent Decision Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-agent systems have demonstrated the ability to improve performance on a variety of predictive tasks by leveraging collaborative decision making. However, the lack of effective evaluation methodologies has made it difficult to estimate the risk of bias, making deployment of such systems unsafe in high stakes domains such as consumer finance, where biased decisions can translate directly into regulatory breaches and financial loss. To address this challenge, we need to develop fairness evaluation methodologies for multi-agent predictive systems and measure the fairness characteristics of these systems in the financial tabular domain. Examining fairness metrics using large-scale simulations across diverse multi-agent configurations, with varying communication and collaboration mechanisms, we reveal patterns of emergent bias in financial decision-making that cannot be traced to individual agent components, indicating that multi-agent systems may exhibit genuinely collective behaviors. Our findings highlight that fairness risks in financial multi-agent systems represent a significant component of model risk, with tangible impacts on tasks such as credit scoring and income estimation. We advocate that multi-agent decision systems must be evaluated as holistic entities rather than through reductionist analyses of their constituent components.", "AI": {"tldr": "本文研究了多智能体决策系统中的偏见和公平性问题，特别是在金融领域的表格式任务中。", "motivation": "由于缺乏有效的评估方法，多智能体系统的偏见风险难以估计，在高风险领域如消费者金融中的部署变得不安全。因此需要开发针对这些系统的公平性评估方法，并测量其在金融服务决策中的偏见特性。", "method": "通过大规模模拟实验研究了不同多智能体配置的公平性度量，包括不同的通信和协作机制。", "result": "发现了一种无法追溯到单个代理组件的新兴偏见模式，表明多智能体系统可能表现出真正的集体行为。这些结果强调了金融领域中多智能体系统的公平风险是模型风险的重要组成部分，对任务如信用评分和收入估算有实质性影响。", "conclusion": "多智能体决策系统应该作为整体实体进行评估而不是通过简化其构成组件的分析来评估。"}}
{"id": "2512.16428", "pdf": "https://arxiv.org/pdf/2512.16428", "abs": "https://arxiv.org/abs/2512.16428", "authors": ["Xin Miao", "Pawan Kumar Mishra"], "title": "Preparing Future-Ready Learners: K12 Skills Shift and GenAI EdTech Innovation Direction", "categories": ["cs.HC"], "comment": null, "summary": "Since Generative AI came out it has quickly embedded itself in our social fabric, triggering lots of discussions, predictions, and efforts from research, industry, government and capital market to experiment and embrace the technology. The question for the global K12 education is, what and how should our children learn in this fast changing world to be prepared for the changing labor market and live a happy and balanced life? Three key aspects will be discussed: 1) Skills; 2) Evaluation of Learning; 3) Strategic GenAI-powered EdTech innovation for long term educational impact.", "AI": {"tldr": "本文探讨了在快速变化的世界中，K12教育如何通过技能培养、学习评价和战略性的生成式人工智能教育技术创新来为学生准备未来。", "motivation": "随着生成式AI的出现及其迅速融入社会生活，引发了关于全球K12教育的讨论与思考，即孩子们需要掌握哪些技能以适应未来的劳动力市场，并过上幸福平衡的生活。", "method": "文章将从三个方面展开探讨：所需技能、学习评价以及长期影响的战略性生成式人工智能教育技术创新方向。", "result": "未具体列出研究成果，但通过三个关键领域的讨论提供了一个全面的框架来指导未来K12教育的方向。", "conclusion": "生成式AI时代的到来要求教育系统进行适应性的调整和改革，以培养适合未来的下一代人才。"}}
{"id": "2512.16425", "pdf": "https://arxiv.org/pdf/2512.16425", "abs": "https://arxiv.org/abs/2512.16425", "authors": ["Allard Oelen", "Mohamad Yaser Jaradeh", "Sören Auer"], "title": "Introducing ORKG ASK: an AI-driven Scholarly Literature Search and Exploration System Taking a Neuro-Symbolic Approach", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "As the volume of published scholarly literature continues to grow, finding relevant literature becomes increasingly difficult. With the rise of generative Artificial Intelligence (AI), and particularly Large Language Models (LLMs), new possibilities emerge to find and explore literature. We introduce ASK (Assistant for Scientific Knowledge), an AI-driven scholarly literature search and exploration system that follows a neuro-symbolic approach. ASK aims to provide active support to researchers in finding relevant scholarly literature by leveraging vector search, LLMs, and knowledge graphs. The system allows users to input research questions in natural language and retrieve relevant articles. ASK automatically extracts key information and generates answers to research questions using a Retrieval-Augmented Generation (RAG) approach. We present an evaluation of ASK, assessing the system's usability and usefulness. Findings indicate that the system is user-friendly and users are generally satisfied while using the system.", "AI": {"tldr": "本文介绍了ASK系统，这是一个采用神经符号方法的人工智能驱动的科研文献搜索和探索系统。", "motivation": "随着发表的学术论文数量不断增加，找到相关的文献变得越来越困难。本文旨在通过引入一个结合了生成式人工智能和知识图谱的AI驱动系统来解决这一问题。", "method": "ASK系统采用了向量搜索、大型语言模型（LLM）以及知识图谱技术，并利用检索增强生成(RAG)方法从输入的研究问题中提取关键信息并生成答案。", "result": "实验结果显示，该系统的用户友好性得到了认可，使用者普遍对系统表示满意。", "conclusion": "ASK提供了一种有效的方式帮助研究人员更方便地找到相关文献和探索知识。"}}
{"id": "2512.16424", "pdf": "https://arxiv.org/pdf/2512.16424", "abs": "https://arxiv.org/abs/2512.16424", "authors": ["Nguyen Xuan-Vu", "Daniel Armstrong", "Milena Wehrbach", "Andres M Bran", "Zlatko Jončev", "Philippe Schwaller"], "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.", "AI": {"tldr": "介绍了Synthelite，一种利用大型语言模型（LLM）进行直接的逆合成转化提议和路线设计的计算机辅助合成规划框架。", "motivation": "现有框架缺乏与人类专家交互机制，限制了集成化学家见解的能力。希望通过引入使用大型语言模型的合成规划框架来解决这些问题。", "method": "通过利用大型语言模型的内在化学知识和推理能力生成端到端合成路线，并允许通过自然语言提示进行专家干预。", "result": "实验表明Synthelite在策略受限和起始材料受限的合成任务中，成功率可达到95%，并且能够在路线设计中考虑化学可行性。", "conclusion": "认为Synthelite不仅是一个有用的工具，也是迈向大型语言模型作为合成规划中心协调者的一步。"}}
{"id": "2512.16420", "pdf": "https://arxiv.org/pdf/2512.16420", "abs": "https://arxiv.org/abs/2512.16420", "authors": ["Daniel Rika", "Nino Sapir", "Ido Gus"], "title": "DPDFNet: Boosting DeepFilterNet2 via Dual-Path RNN", "categories": ["cs.SD"], "comment": null, "summary": "We present DPDFNet, a causal single-channel speech enhancement model that extends DeepFilterNet2 architecture with dual-path blocks in the encoder, strengthening long-range temporal and cross-band modeling while preserving the original enhancement framework. In addition, we demonstrate that adding a loss component to mitigate over-attenuation in the enhanced speech, combined with a fine-tuning phase tailored for \"always-on\" applications, leads to substantial improvements in overall model performance. To compare our proposed architecture with a variety of causal open-source models, we created a new evaluation set comprising long, low-SNR recordings in 12 languages across everyday noise scenarios, better reflecting real-world conditions than commonly used benchmarks. On this evaluation set, DPDFNet delivers superior performance to other causal open-source models, including some that are substantially larger and more computationally demanding. We also propose an holistic metric named PRISM, a composite, scale-normalized aggregate of intrusive and non-intrusive metrics, which demonstrates clear scalability with the number of dual-path blocks. We further demonstrate on-device feasibility by deploying DPDFNet on Ceva-NeuPro-Nano edge NPUs. Results indicate that DPDFNet-4, our second-largest model, achieves real-time performance on NPN32 and runs even faster on NPN64, confirming that state-of-the-art quality can be sustained within strict embedded power and latency constraints.", "AI": {"tldr": "DPDFNet是一种增强的语音增强模型，通过双路径RNN块改进DeepFilterNet2架构。", "motivation": "提出DPDFNet以加强长距离时序和跨频带建模，并减少过度衰减问题，同时适应日常噪声场景下的“始终开启”应用需求。", "method": "在编码器中加入双路径RNN块增强模型性能；增加损失组件来减轻过度衰减现象；并进行微调以优化“始终开启”的应用场景。", "result": "DPDFNet在包含12种语言的长时低信噪比录音数据集上表现优于其他因果性开源模型，包括一些更大更复杂的模型。提出的PRISM评价指标显示了双路径块数量与性能的可扩展性，并且实现在边缘设备上的实时运行。", "conclusion": "DPDFNet通过优化架构和损失函数实现了更好的语音增强效果，证明了在嵌入式系统中保持高性能的同时满足低功耗和延迟要求的可能性。"}}
{"id": "2512.16415", "pdf": "https://arxiv.org/pdf/2512.16415", "abs": "https://arxiv.org/abs/2512.16415", "authors": ["Muhammad Ibraheem Siddiqui", "Muhammad Haris Khan"], "title": "CountZES: Counting via Zero-Shot Exemplar Selection", "categories": ["cs.CV"], "comment": null, "summary": "Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.", "AI": {"tldr": "CountZES是一种无训练框架，用于通过零样本示例选择来进行物体计数。", "motivation": "现有的零样本物体计数方法要么依赖于多实例候选的开放词汇检测器，要么使用无法准确区分对象实例的随机补丁采样。因此，提出了CountZES来解决这些问题。", "method": "CountZES通过三个协同阶段发现多样化的示例：基于检测的锚点（DAE），密度引导的示例选择（DGE）和特征共识示例选择（FCE）。DAE改进开放词汇检测以隔离精确单一实例的示例，DGE采用统计一致且语义紧凑的自监督模式来识别示例，而FCE通过特征空间聚类增强视觉一致性。", "result": "实验在多种数据集上显示了CountZES优于其他零样本物体计数方法，并且能够有效地跨自然、航空和医学领域进行泛化。", "conclusion": "CountZES提出了一种无训练的框架，通过零样本示例选择实现物体计数，这种方法在各种场景中都表现出色。"}}
{"id": "2512.16414", "pdf": "https://arxiv.org/pdf/2512.16414", "abs": "https://arxiv.org/abs/2512.16414", "authors": ["Jannes Malanowski"], "title": "Conquering the Multiverse: The River Voting Method with Efficient Parallel Universe Tiebreaking", "categories": ["cs.DS"], "comment": "This is my Bachelor's Thesis. Based on this thesis we published arXiv:2512.14409", "summary": "Democracy relies on making collective decisions through voting. In addition, voting procedures have further applications, for example in the training of artificial intelligence. An essential criterion for determining the winner of a fair election is that all alternatives are treated equally: this is called neutrality. The established Ranked Pairs voting method cannot simultaneously guarantee neutrality and be computationally tractable for election with ties. River, the recently introduced voting method, shares desirable properties with Ranked Pairs and has further advantages, such as a new property related to resistance against manipulation. Both Ranked Pairs and River use a weighted margin graph to model the election. Ties in the election can lead to edges of equal margin. To order the edges in such a case, a tiebreaking scheme must be employed. Many tiebreaks violate neutrality or other important properties. A tiebreaking scheme that preserves neutrality is Parallel Universe Tiebreaking (PUT). Ranked Pairs with PUT is NP-hard to compute. The main result of this thesis shows that River with PUT can be computed in polynomial worst-case runtime: We can check whether an alternative is a River PUT winner, by running River with a specially constructed ordering of the edges. To construct this ordering, we introduce the semi-River diagram which contains the edges that can appear in any River diagram for some arbitrary tiebreak. On this diagram we can compute the River winners, by applying a variant of Prims algorithm per alternative. Additionally, we give an algorithm improve the previous naive runtime of River from $\\mathcal{O}(n^4)$ to $\\mathcal{O}(n^2 \\log n)$, where n is the number of alternatives.", "AI": {"tldr": "研究提出了一种新的投票方法River，并展示了在并行宇宙平局破解方案下，该方法可以实现公平且计算效率高的选举结果。", "motivation": "现有的一些排名成对投票法无法同时保证中立性和处理复杂选举中的平局。本文旨在解决这一问题，通过引入River投票法以及并行宇宙平局破解（PUT）来提高选举的公平性和可操作性。", "method": "采用加权边图模型表示选举数据，并利用半River图表和改进版本的Prim算法在每个替代方案上计算River PUT获胜者。同时优化了River方法的时间复杂度，从O(n^4)减少到O(n^2 log n)。", "result": "证明了River与PUT结合可在多项式时间内完成选举结果的计算，并且展示了该方法能够有效地处理平局情况而不破坏中立性原则。", "conclusion": "论文成功地开发了一个公平、高效的投票机制，解决了在复杂选举场景下的平局问题。这种方法不仅保证了中立性，而且提高了算法效率。"}}
{"id": "2512.16413", "pdf": "https://arxiv.org/pdf/2512.16413", "abs": "https://arxiv.org/abs/2512.16413", "authors": ["Liyuan Deng", "Hao Guo", "Yunpeng Bai", "Yongkang Dai", "Huaxi Huang", "Yilei Shi"], "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.", "AI": {"tldr": "BrepLLM是一种框架，使大型语言模型能够解析和推导三维边界表示（Brep）数据。", "motivation": "现有的基于令牌序列的大型语言模型不适合直接处理包含复杂几何和拓扑信息的3D Brep模型。", "method": "该方法包括两个阶段：交叉模态对齐预训练和多阶段LLM微调。第一阶段通过自适应UV采样将Brep转换为具有几何和拓扑信息的图表示，并设计层次化BrepEncoder提取特征，与文本嵌入进行对比学习。第二阶段结合预训练后的BrepEncoder到LLM中，采用三阶段渐进式训练策略：MLP语义映射、微调LLM及混合查询专家（MQE）增强几何多样性建模。", "result": "实验表明，BrepLLM在3D物体分类和描述任务上达到最佳结果。", "conclusion": "BrepLLM成功地填补了结构化3D几何与自然语言之间的鸿沟，实现了对复杂3DBrep数据的解析和推导。"}}
{"id": "2512.16406", "pdf": "https://arxiv.org/pdf/2512.16406", "abs": "https://arxiv.org/abs/2512.16406", "authors": ["Joachim Winther Pedersen", "Erwan Plantec", "Eleni Nisioti", "Marcello Barylli", "Milton Montero", "Kathrin Korte", "Sebastian Risi"], "title": "Hypernetworks That Evolve Themselves", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "How can neural networks evolve themselves without relying on external optimizers? We propose Self-Referential Graph HyperNetworks, systems where the very machinery of variation and inheritance is embedded within the network. By uniting hypernetworks, stochastic parameter generation, and graph-based representations, Self-Referential GHNs mutate and evaluate themselves while adapting mutation rates as selectable traits. Through new reinforcement learning benchmarks with environmental shifts (CartPoleSwitch, LunarLander-Switch), Self-Referential GHNs show swift, reliable adaptation and emergent population dynamics. In the locomotion benchmark Ant-v5, they evolve coherent gaits, showing promising fine-tuning capabilities by autonomously decreasing variation in the population to concentrate around promising solutions. Our findings support the idea that evolvability itself can emerge from neural self-reference. Self-Referential GHNs reflect a step toward synthetic systems that more closely mirror biological evolution, offering tools for autonomous, open-ended learning agents.", "AI": {"tldr": "提出一种自我演化的神经网络系统——自参照图超网络（Self-Referential GHN），使其在没有外部优化器的情况下进行变异和适应。", "motivation": "探索如何使神经网络在其结构内部实现演化，无需依赖于外部的优化机制或操作者干预。", "method": "结合超网络、随机参数生成以及基于图的表示方法，设计了一种能够自我变异和评估的系统。通过调整变异率作为可选择性状，并在环境变化的情况下进行新的强化学习基准测试（如CartPoleSwitch, LunarLander-Switch）。", "result": "实验结果显示，在Ant-v5等移动任务中，自参照图超网络能够演化出一致的步态模式；并在不断变化的环境中表现出迅速且可靠的适应性。", "conclusion": "研究结果支持了神经网络自我可演化性的观点，并展示了向更接近生物进化的合成系统发展的潜力。"}}
{"id": "2512.16397", "pdf": "https://arxiv.org/pdf/2512.16397", "abs": "https://arxiv.org/abs/2512.16397", "authors": ["Haodi He", "Jihun Yu", "Ronald Fedkiw"], "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Submitted to CVPR 2026. 21 pages, 22 figures", "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.", "AI": {"tldr": "该论文利用高斯点集技术来创建高质量的人脸几何结构和纹理。", "motivation": "为了构建一个统一且一致的三维人脸模型，该研究利用了高斯点集而非NeRF方法，并通过分割标注实现面部区域对齐。这种方法能够使用少量图像（11张）重建中性姿态。", "method": "论文采用了高斯点集技术与三角网格相结合的方式，通过对高斯点施加软约束以提高重建质量。同时将几何结构转化为纹理空间处理，实现了高度视觉保真度的模型。此外还引入了可重光照的高斯模型来分离纹理和照明。", "result": "研究展示了如何使用少量图像进行高质量人脸建模，并且创建出可用于标准图形管线的高度清晰的高分辨率反照率贴图。", "conclusion": "该方法能够利用不同条件下的图像训练，具有灵活性和鲁棒性。论文还证明了其在文本驱动资产生成流程中的有效性。"}}
{"id": "2512.16393", "pdf": "https://arxiv.org/pdf/2512.16393", "abs": "https://arxiv.org/abs/2512.16393", "authors": ["Zhanwei Li", "Liang Li", "Jiawan Zhang"], "title": "Adaptive Frequency Domain Alignment Network for Medical image segmentation", "categories": ["cs.CV"], "comment": null, "summary": "High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.", "AI": {"tldr": "本文提出了一种自适应频域对齐网络（AFDAN），用于医疗图像分割任务，以解决高质量标注数据稀缺的问题。", "motivation": "由于手动注释的时间和劳动强度大，高精度的医学图像分割需要大量的高质量标注数据。然而，这些数据往往非常稀缺，因此本文提出了自适应频域对齐网络（AFDAN），旨在通过频域特征对齐减轻这一问题。", "method": "AFDAN 包含三个核心组件：对抗领域学习模块、源目标频率融合模块和空间-频率整合模块。这三个模块共同协作实现跨领域的知识转移，提高分割的准确性。", "result": "实验显示，AFDAN 在新构建的 VITILIGO2025 数据集上的交并比（IoU）为90.9%，在DRIVE数据集上达到了82.6%的 IoU ，超越了当前最先进的方法。", "conclusion": "通过引入自适应频域对齐网络（AFDAN），本文成功地缓解了医疗图像分割中高质量标注数据稀缺的问题，提高了跨领域的知识转移效率和分割准确性。"}}
{"id": "2512.16392", "pdf": "https://arxiv.org/pdf/2512.16392", "abs": "https://arxiv.org/abs/2512.16392", "authors": ["Mohammad-Javad Rezaei", "Mozafar Bag-Mohammadi"], "title": "PCIA: A Path Construction Imitation Algorithm for Global Optimization", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.", "AI": {"tldr": "提出了一种新的优化算法PCIA，模仿人类构建新路径的方式。", "motivation": "为了开发一种更有效的全局优化方法，受人类在交通路线选择和路径重建的启发，设计了PCIA算法。", "method": "通过生成随机群体来寻找到达目标的最佳路径，每个粒子代表一条通往目的地的道路。该算法模拟了人群智能的行为，并且在面对路径关闭时能够构建新的混合路径。", "result": "经过53个数学优化问题和13个约束优化问题的测试，PCIA显示出与流行及最新的元启发式算法相比具有高度竞争力的表现。", "conclusion": "PCIA是一种有效的全局优化算法，其性能在多种类型的优化问题上都显示出了竞争优势。"}}
{"id": "2512.16391", "pdf": "https://arxiv.org/pdf/2512.16391", "abs": "https://arxiv.org/abs/2512.16391", "authors": ["Dhruv Deshmukh", "Saurabh Goyal", "Nipun Kwatra", "Ramachandran Ramjee"], "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "11 pages, 8 figures, 3 tables and 1 algorithm", "summary": "Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.", "AI": {"tldr": "Kascade提出了一种无需训练的稀疏注意方法，用于长上下文LLM推理，以减少延迟。", "motivation": "随着推理模型和RAG变得流行，注意力成为长上下文LLM推理的主要延迟来源。因此，需要一种有效的方法来加速这一过程。", "method": "Kascade通过计算少量锚点层中的精确Top-k索引，并在中间层重用这些索引来实现稀疏注意。这种方法利用了后softmax注意力本质上是稀疏的以及高权重键的身份在相邻层中保持稳定这两个观察结果。", "result": "Kascade在H100 GPU上实现了高达4.1倍的解码注意力加速和2.2倍的填充注意力加速，同时在LongBench和AIME-24等长上下文基准测试中接近密集注意力精度。", "conclusion": "Kascade提供了一种简单而有效的训练自由稀疏注意方法，可显著提高长上下文LLM推理的速度而不牺牲准确性。"}}
{"id": "2512.16378", "pdf": "https://arxiv.org/pdf/2512.16378", "abs": "https://arxiv.org/abs/2512.16378", "authors": ["Sara Papi", "Javier Garcia Gilabert", "Zachary Hopton", "Vilém Zouhar", "Carlos Escolano", "Gerard I. Gállego", "Jorge Iranzo-Sánchez", "Ahrii Kim", "Dominik Macháček", "Patricia Schmidtova", "Maike Züfle"], "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": "Project available at https://github.com/sarapapi/hearing2translate", "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.", "AI": {"tldr": "本文通过全面的测试套件评估了5种最先进的SpeechLLM与16种直接和级联系统在不同语言对和挑战条件下的表现，以确定语音翻译质量是否因集成到LLM中而提高。", "motivation": "随着大型语言模型的发展，将语音作为原生模态进行集成已经成为趋势。然而，这种集成是否真的提升了语音转文字的翻译质量仍是一个未解之谜。", "method": "本文构建了一个包括5种最先进的SpeechLLM和16个直接及级联系统的测试套件，并在不同语言对、挑战条件以及基准下进行了评估。", "result": "结果表明，目前的级联系统整体上最为可靠。而当前的SpeechLLMs仅能在特定环境下与之匹敌，语音基础模型则落后于两者。", "conclusion": "集成大型语言模型对于提高高质量的语音翻译至关重要，无论是将其整合进单一模型还是在管道中使用。"}}
{"id": "2512.16371", "pdf": "https://arxiv.org/pdf/2512.16371", "abs": "https://arxiv.org/abs/2512.16371", "authors": ["Mariam Hassan", "Bastien Van Delft", "Wuyang Li", "Alexandre Alahi"], "title": "Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis", "AI": {"tldr": "本文提出了一种名为Factorized Video Generation（FVG）的新方法，将文本到视频生成过程分解为三个阶段：推理、合成和时间合成，以提高视频生成的准确性和连贯性。", "motivation": "最先进的Text-to-Video (T2V) 扩散模型在生成视觉上令人印象深刻的视频方面存在困难，特别是在构建复杂的场景或遵循逻辑的时间指令时。这些错误通常源于初始帧的构造不正确或不一致的问题。", "method": "本文提出了一种名为Factorized Video Generation（FVG）的新方法，将文本到视频生成过程分解为三个阶段：(1) 推理，其中大语言模型(Large Language Model, LLM) 重写视频提示以描述唯一的初始场景；(2) 组合，其中文本到图像(Text-to-Image, T2I) 模型从新的提示中合成高质量且结构正确的锚帧；以及 (3) 时间合成，其中经过微调的视频模型专注于动画制作和遵循提示。", "result": "分解的方法在T2V CompBench基准测试上设置了新状态，并在VBench2上显著提升了所有测试模型的表现。此外，视觉锚定使采样步骤减少了70%，而不损失任何性能。", "conclusion": "Factorized Video Generation提供了一种简单而实用的路径，以实现更有效、更具鲁棒性和可控性的视频合成"}}
{"id": "2512.16367", "pdf": "https://arxiv.org/pdf/2512.16367", "abs": "https://arxiv.org/abs/2512.16367", "authors": ["Sijia Chen", "Wei Dong"], "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion", "categories": ["cs.RO"], "comment": "accept by IEEE Transactions on Industrial Electronics", "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.", "AI": {"tldr": "本文提出了一种利用视觉惯性和单一范围融合的主动和自适应地面-空中定位系统，用于增强飞行机器人在复杂环境中的定位鲁棒性。", "motivation": "传统方法使用固定相机观测预附着标记来估计飞行机器人的位置，在距离受限且易受捕获失败的情况下存在局限性。本文旨在通过综合主动视觉、单一测距、惯性里程计和光学流，提高地面-空中定位框架的鲁棒性和适应性。", "method": "设计了安装在地面上并可动态旋转以检测和跟踪飞行机器人上红外标记的主动视觉子系统。结合单一测距技术扩展可行距离并增强视觉降级时的捕获能力。利用多项式近似与扩展滑动窗口融合多源测量，实现鲁棒且高效的位置估计。根据传感器保真度实施自适应滑动置信评估算法动态调整加权参数。", "result": "实验结果显示，在烟雾干扰、光照变化、障碍物遮挡等条件下，所提方法实现了平均均方根误差约为0.09m的鲁棒在线定位，并且能够抵抗捕获损失和传感器故障。", "conclusion": "该系统通过综合多种传感技术显著增强了飞行机器人在复杂环境中的实时定位性能，展示了其在多源信息融合、自适应评估及提高鲁棒性方面的潜力。"}}
{"id": "2512.16366", "pdf": "https://arxiv.org/pdf/2512.16366", "abs": "https://arxiv.org/abs/2512.16366", "authors": ["Omar Namnakani", "Yasmeen Abdrabou", "Cristina Fiani", "John H. Williamson", "Mohamed Khamis"], "title": "Mind the Gaze: Improving the Usability of Dwell Input by Adapting Gaze Targets Based on Viewing Distance", "categories": ["cs.HC"], "comment": "18 pages, 8 figures", "summary": "Dwell input shows promise for handheld mobile contexts, but its performance is impacted by target size and viewing distance. While fixed target sizes suffice in static setups, in mobile settings, frequent posture changes alter viewing distances, which in turn distort perceived size and hinder dwell performance. We address this through GAUI, a Gaze-based Adaptive User Interface that dynamically resizes targets to maximise performance at the given viewing distance. In a two-phased study (N=24), GAUI leveraged the strengths of its distance-responsive design, outperforming the large UI static baseline in task time, and being less error-prone than the small UI static baseline. It was rated the most preferred interface overall. Participants reflected on using GAUI in six different postures. We discuss how their experience is impacted by posture, and propose guidelines for designing context-aware adaptive UIs for dwell interfaces on handheld mobile devices that maximise performance.", "AI": {"tldr": "本文提出了一种基于注视距离调整目标大小的自适应用户界面GAUI，以提高手持移动设备上停留输入法的性能。", "motivation": "在移动环境中，由于频繁变化的身体姿势导致观察距离的变化影响了固定尺寸的目标在停留输入中的表现。这阻碍了用户体验和效率，因此研究者们希望通过动态调整目标大小来解决这个问题。", "method": "通过GAUI，在不同视角下动态地改变目标的大小以优化用户体验，并进行了两阶段实验（N=24），评估了与静态大、小界面相比的效果。", "result": "GAUI在任务完成时间上超越了静态的大界面基线，且比静态的小界面更少出错。用户偏好调查中，GAUI被评为最喜爱的交互方式，并反映出了六种不同姿态下的使用体验。", "conclusion": "实验结果表明GAUI能够根据观察距离自适应地调整目标大小以优化用户体验，为设计适合手持移动设备的停留输入法提供了指导原则。"}}
{"id": "2512.16360", "pdf": "https://arxiv.org/pdf/2512.16360", "abs": "https://arxiv.org/abs/2512.16360", "authors": ["Haotian Ling", "Zequn Chen", "Qiuying Chen", "Donglin Di", "Yongjia Ma", "Hao Li", "Chen Wei", "Zhulin Tao", "Xun Yang"], "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation", "categories": ["cs.CV"], "comment": null, "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.", "AI": {"tldr": "EverybodyDance通过构建一个加权完全二分图来解决多角色动画中的身份对应问题，优化了训练过程并提高了视觉保真度。", "motivation": "在单个角色场景中，基于姿态驱动的角色动画取得了显著进展。然而，在涉及位置交换的多角色设置中推广这些成果存在挑战，主要挑战在于生成帧和参考帧之间的正确身份对应。为了解决这个问题，提出了一种系统性解决方案。", "method": "EverybodyDance建立了一个身份匹配图（IMG），该图将生成帧和参考帧中的角色分别视为加权完全二分图的两个节点集，并通过所提出的掩码查询注意力机制计算边权重，以量化每对角色之间的相似度。此外，还提出了一系列针对多角色动画优化策略。", "result": "实验表明EverybodyDance在身份对应准确性和视觉保真度方面显著优于现有最佳基线模型。", "conclusion": "提出了一个系统解决方案来解决多角色动画中的身份对应问题，并通过一系列有针对性的策略提高了性能和精度。"}}
{"id": "2512.16357", "pdf": "https://arxiv.org/pdf/2512.16357", "abs": "https://arxiv.org/abs/2512.16357", "authors": ["Tao Hu", "Weiyu Zhou", "Yanjie Tu", "Peng Wu", "Wei Dong", "Qingsen Yan", "Yanning Zhang"], "title": "GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.", "AI": {"tldr": "该论文提出了GMODiff，一种基于扩散先验的一步增益图精细方法，用于多曝光HDR重建。", "motivation": "预训练的潜在扩散模型在低级视觉任务中表现出强大的感知先验，但直接应用于HDR重建面临动态范围表示受限、高推理成本和内容幻觉等问题。为解决这些问题，提出了GMODiff。", "method": "将HDR重建重新定义为有条件引导的增益图估计任务，并利用回归先验指导扩散模型中的去噪过程和潜在解码，从而在单步中生成高质量的增益图。", "result": "实验表明，GMODiff方法优于多种现有方法，并且比之前的基于LDM的方法快100倍。", "conclusion": "通过将HDR重建任务转化为增益图估计问题并利用扩散模型和回归先验结合的方式，该方法能够高效地生成高质量的HDR图像。"}}
{"id": "2512.16349", "pdf": "https://arxiv.org/pdf/2512.16349", "abs": "https://arxiv.org/abs/2512.16349", "authors": ["Soochang Song", "Yongjune Kim"], "title": "Collaborative Edge-to-Server Inference for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 12 figures", "summary": "We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.", "AI": {"tldr": "提出了一种协作边缘到服务器的推理框架，旨在降低通信成本的同时保持视觉语言模型的准确性。", "motivation": "在传统部署中，从客户端捕获的图像通常需要发送到服务器进行推理。然而，这种操作容易导致细节丢失和精度下降，因此提出了新的策略来优化这一过程。", "method": "设计了一个两阶段框架：第一阶段由服务器对全局图像进行推理并识别兴趣区域；第二阶段基于输出令牌计算最小熵作为信心度衡量标准决定是否需要重新传输局部图像的细节部分。如果超过阈值，则发送本地图片以供进一步处理，最终利用全局与局部信息来提高准确性。", "result": "实验表明所提框架在多个视觉语言模型结构上显著降低了通信成本且保持了推理准确性的稳定表现。", "conclusion": "通过采用选择性重传策略，该方法成功地减少了不必要的数据传输量，并确保了视觉语言模型的性能不受影响。"}}
{"id": "2512.16344", "pdf": "https://arxiv.org/pdf/2512.16344", "abs": "https://arxiv.org/abs/2512.16344", "authors": ["Peter Coveney", "Roger Highfield"], "title": "AI Needs Physics More Than Physics Needs AI", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.", "AI": {"tldr": "探讨物理科学如何为当前的人工智能提供更深层次的支持和指导，提出了一种理论与机器学习结合的‘大规模AI’概念", "motivation": "指出目前人工智能在实际应用中的影响力有限，并强调物理学拥有更多可以贡献于现代人工智能的技术和理念", "method": "回顾了现有AI模型存在的问题，并讨论了量子AI和模拟计算等领域的机遇，提出了一个融合理论严谨性和机器学习灵活性的‘大规模AI’框架", "result": "展示了如何结合物理原理来改进现有的AI架构，指出了实现这一目标的具体路径", "conclusion": "强调物理学在推动人工智能发展中的核心作用，呼吁研究者们重视物理科学对AI的影响"}}
{"id": "2512.16334", "pdf": "https://arxiv.org/pdf/2512.16334", "abs": "https://arxiv.org/abs/2512.16334", "authors": ["Ruifeng Tan", "Weixiang Hong", "Jia Li", "Jiaqiang Huang", "Tong-Yi Zhang"], "title": "Pretrained Battery Transformer (PBT): A battery life prediction foundation model", "categories": ["cs.LG", "cs.AI"], "comment": "5 figures in the main content", "summary": "Early prediction of battery cycle life is essential for accelerating battery research, manufacturing, and deployment. Although machine learning methods have shown encouraging results, progress is hindered by data scarcity and heterogeneity arising from diverse aging conditions. In other fields, foundation models (FMs) trained on diverse datasets have achieved broad generalization through transfer learning, but no FMs have been reported for battery cycle life prediction yet. Here we present the Pretrained Battery Transformer (PBT), the first FM for battery life prediction, developed through domain-knowledge-encoded mixture-of-expert layers. Validated on the largest public battery life database, PBT learns transferable representations from 13 lithium-ion battery (LIB) datasets, outperforming existing models by an average of 19.8%. With transfer learning, PBT achieves state-of-the-art performance across 15 diverse datasets encompassing various operating conditions, formation protocols, and chemistries of LIBs. This work establishes a foundation model pathway for battery lifetime prediction, paving the way toward universal battery lifetime prediction systems.", "AI": {"tldr": "本文提出了一种预训练电池变换器（PBT）模型，用于锂离子电池寿命的早期预测。", "motivation": "机器学习方法在电池寿命预测中表现出色，但受限于数据稀缺和异质性。为了克服这些障碍，作者开发了第一个用于电池寿命预测的基础模型（FMs），以实现跨不同操作条件、形成协议和化学成分的数据泛化能力。", "method": "PBT通过混合专家层编码领域知识训练而成，并在最大公开的电池寿命数据库中验证其性能。该模型从13个锂离子电池数据集中学习可迁移表示，使用转移学习实现了最先进的预测效果。", "result": "PBT在15种不同数据集上表现最佳，平均优于现有模型19.8%。", "conclusion": "这项工作为电池寿命预测奠定了基础模型的方法路径，并向着通用的电池寿命预测系统迈进。"}}
{"id": "2512.16325", "pdf": "https://arxiv.org/pdf/2512.16325", "abs": "https://arxiv.org/abs/2512.16325", "authors": ["Nan Zhou", "Zuxin Li", "Fanhang Man", "Xuecheng Chen", "Susu Xu", "Fan Dang", "Chaopeng Hong", "Yunhao Liu", "Xiao-Ping Zhang", "Xinlei Chen"], "title": "QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.", "AI": {"tldr": "提出了一种质量感知激励驱动的多代理调度系统（QUIDS），以提高非专用车辆移动众包中的信息质量。", "motivation": "解决非专用车辆移动众包中获取高质量信息的关键挑战，包括传感覆盖和可靠性问题以及动态参与性的问题。", "method": "引入了集成传感覆盖和可靠性的聚合传感质量（ASQ）指标，并开发了一种考虑不确定性并提高ASQ的相互协助信念感知车辆调度算法。", "result": "评估显示QUIDS在非调度场景中提高了38%的ASQ，在最先进的方法上提高了10%，并且地图重建错误降低了39-74％。", "conclusion": "通过质量信息感知激励机制，QUIDS能够在低成本下实现高质量的城市监控，并适用于智能城市的交通和环境传感应用。"}}
{"id": "2512.16317", "pdf": "https://arxiv.org/pdf/2512.16317", "abs": "https://arxiv.org/abs/2512.16317", "authors": ["Arther Tian", "Alex Ding", "Frank Chen", "Alan Wu", "Aaron Chan", "Bruce Zhang"], "title": "Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference", "categories": ["cs.AI"], "comment": null, "summary": "Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost. Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.", "AI": {"tldr": "提出了一种成本感知的PoQ框架，以解决去中心化大型语言模型推理中的计算成本问题。", "motivation": "现有验证方法难以扩展到现代模型。原始PoQ未考虑不同节点间的计算成本差异。", "method": "结合了真实标记级别的F1值、轻量级学习评估器和GPT判断，引入线性奖励函数平衡质量和成本。", "result": "实验证明，语义文本相似度双编码器与地面真相和GPT得分的相关性更高。成本感知PoQ能够有效激励高效模型并惩罚低效节点。", "conclusion": "成本感知PoQ为去中心化大型语言模型推理提供了经济可持续的基础。"}}
{"id": "2512.16314", "pdf": "https://arxiv.org/pdf/2512.16314", "abs": "https://arxiv.org/abs/2512.16314", "authors": ["Huayu Huang", "Chen Chen", "Banglei Guan", "Ze Tan", "Yang Shang", "Zhang Li", "Qifeng Yu"], "title": "Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs", "categories": ["cs.CV"], "comment": null, "summary": "Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.", "AI": {"tldr": "提出了基于岭估计的无人机视觉与激光测距融合定位方法，以提高定位精度。", "motivation": "在长距离、小交角和大倾斜角等限制条件下，常规最小二乘算法存在严重多重共线性问题，导致计算不稳定性和低鲁棒性。本文旨在通过引入岭估计来解决这些问题并提升定位的准确度和鲁棒性。", "method": "结合视觉提供的丰富场景信息与激光测距的高度精确性，提出基于岭估计的数据融合方法以克服多重共线性带来的不良影响。", "result": "实验表明该方法在精度上优于单一信息地面定位算法，并且特别在有限观测条件下具有更强的鲁棒性和稳定性。", "conclusion": "基于岭估计的方法能够有效提升无人机在复杂环境中的定位性能，特别是在受限条件下的表现尤为突出。"}}
{"id": "2512.16313", "pdf": "https://arxiv.org/pdf/2512.16313", "abs": "https://arxiv.org/abs/2512.16313", "authors": ["Haiyu Zhao", "Yiwen Shan", "Yuanbiao Gou", "Xi Peng"], "title": "LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.", "AI": {"tldr": "提出了一种轻量级的LaverNet网络，用于处理时间变化降质问题的一体化视频恢复。", "motivation": "现有方法在处理时间变化降质时存在挑战：一是降质影响了时间建模；二是使用大型模型来解决一体化恢复的问题。", "method": "引入了一种新的传播机制，选择性地传输去除了降质的信息跨帧传递，并提出了一个只有362K参数的轻量级LaverNet网络。", "result": "尽管参数数量仅为现有方法的1%以下，但LaverNet在各种基准测试中取得了与大型模型相当甚至更好的性能。", "conclusion": "通过选择性传播机制和紧凑的设计，LaverNet实现了强大的一体化视频恢复任务。"}}
{"id": "2512.16310", "pdf": "https://arxiv.org/pdf/2512.16310", "abs": "https://arxiv.org/abs/2512.16310", "authors": ["Yuxuan Qiao", "Dongqin Liu", "Hongchang Yang", "Wei Zhou", "Songlin Hu"], "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.", "AI": {"tldr": "研究了自主代理在多工具架构下产生的新隐私风险，并提出了评估和缓解策略。", "motivation": "为了提高自主代理的效率，单个代理跨多个工具操作已成为趋势。然而，这种做法引入了一种新的严重隐私风险，即TOP-R，其中代理无意中合成敏感信息。", "method": "构建了用于评估TOP-R的正式框架和基准测试（TOP-Bench），并提出了量化安全与鲁棒性权衡指标H-Score，还提出了一种缓解策略PEP方法。", "result": "发现代表模型在TOP-Bench上的风险泄漏率平均为90.24%，H-Score仅为0.167。采用PEP方法后，将风险泄漏率降低至46.58%，并将H-Score提高到0.624。", "conclusion": "该研究揭示了一种新的隐私风险，并指出了当前代理架构的内在限制，同时提供了解决方案。"}}
{"id": "2512.16309", "pdf": "https://arxiv.org/pdf/2512.16309", "abs": "https://arxiv.org/abs/2512.16309", "authors": ["Aleksandros Sobczyk", "Anastasios Zouzias"], "title": "Prefix Sums via Kronecker Products", "categories": ["quant-ph", "cs.DS"], "comment": null, "summary": "In this work, we revisit prefix sums through the lens of linear algebra. We describe an identity that decomposes triangular all-ones matrices as a sum of two Kronecker products, and apply it to design recursive prefix sum algorithms and circuits. Notably, the proposed family of circuits is the first one that achieves the following three properties simultaneously: (i) zero-deficiency, (ii) constant fan-out per-level, and (iii) depth that is asymptotically strictly smaller than $2\\log(n)$ for input length n. As an application, we show how to use these circuits to design quantum adders with $1.893\\log(n) + O(1)$ Toffoli depth, $O(n)$ Toffoli gates, and $O(n)$ additional qubits, improving the Toffoli depth and/or Toffoli size of existing constructions.", "AI": {"tldr": "通过线性代数的方法重新审视前缀和问题，提出了一种新的电路设计方法。", "motivation": "利用线性代数中的Kronecker积来简化和优化前缀和的计算。", "method": "将三角全一矩阵分解为两个Kronecker积之和，并设计出满足零冗余、恒定扇出且深度渐进小于2log(n)的新电路。", "result": "提出了新的量子加法器设计方案，其Toffoli门深度和数量均有所改进。", "conclusion": "通过线性代数工具优化前缀和计算问题，实现了更高效的电路设计。"}}
{"id": "2512.16307", "pdf": "https://arxiv.org/pdf/2512.16307", "abs": "https://arxiv.org/abs/2512.16307", "authors": ["Safwan Shaheer", "G. M. Refatul Islam", "Mohammad Rafid Hamid", "Tahsin Zaman Jilan"], "title": "Beyond the Benchmark: Innovative Defenses Against Prompt Injection Attacks", "categories": ["cs.CR", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "In this fast-evolving area of LLMs, our paper discusses the significant security risk presented by prompt injection attacks. It focuses on small open-sourced models, specifically the LLaMA family of models. We introduce novel defense mechanisms capable of generating automatic defenses and systematically evaluate said generated defenses against a comprehensive set of benchmarked attacks. Thus, we empirically demonstrated the improvement proposed by our approach in mitigating goal-hijacking vulnerabilities in LLMs. Our work recognizes the increasing relevance of small open-sourced LLMs and their potential for broad deployments on edge devices, aligning with future trends in LLM applications. We contribute to the greater ecosystem of open-source LLMs and their security in the following: (1) assessing present prompt-based defenses against the latest attacks, (2) introducing a new framework using a seed defense (Chain Of Thoughts) to refine the defense prompts iteratively, and (3) showing significant improvements in detecting goal hijacking attacks. Out strategies significantly reduce the success rates of the attacks and false detection rates while at the same time effectively detecting goal-hijacking capabilities, paving the way for more secure and efficient deployments of small and open-source LLMs in resource-constrained environments.", "AI": {"tldr": "本文提出了一种新的框架，使用种子防御（Chain Of Thoughts）迭代优化防御提示，并评估了生成的防御措施在小开源模型上的效果。", "motivation": "探讨大型语言模型中由提示注入攻击引发的安全风险，特别是针对LLaMA家族的小型开放源代码模型。旨在提高这些模型在边缘设备部署时的安全性。", "method": "引入新的框架使用种子防御（Chain Of Thoughts）迭代优化防御提示，并系统地评估生成的防御措施对一系列基准测试攻击的有效性。", "result": "显著降低了目标劫持攻击的成功率和假阳性检测率，有效提升了目标劫持能力的检测效率。", "conclusion": "提出的方法为小型开源LLM在资源受限环境中的安全部署提供了新的路径。"}}
{"id": "2512.16304", "pdf": "https://arxiv.org/pdf/2512.16304", "abs": "https://arxiv.org/abs/2512.16304", "authors": ["Jiajun Yuan", "Xiaochen Wang", "Yuhang Xiao", "Yulin Wu", "Chenhao Hu", "Xueyang Lv"], "title": "CogSR: Semantic-Aware Speech Super-Resolution via Chain-of-Thought Guided Flow Matching", "categories": ["cs.SD"], "comment": "7 pages", "summary": "Applying speech super-resolution (SR) to recordings with severely low sampling rates is a critical challenge in digital archiving and investigative audio recovery. In these scenarios, the input lacks essential acoustic cues. Consequently, existing generative models often fail; without sufficient context, they hallucinate phonetic content, guessing words based on probability rather than meaning. To address this, we propose CogSR, a framework designed specifically for high-precision, offline restoration. Our approach shifts the focus from simple signal mapping to cognitive reconstruction. By integrating a Large Audio-Language Model, we employ Chain-of-Thought reasoning to act as a semantic anchor, while explicit acoustic priors ensure the speaker's identity remains consistent. This guides a Rectified Flow backbone to synthesize high-frequency details that are not only realistic but linguistically accurate. Evaluations show that CogSR effectively eliminates ambiguity in severe degradation regimes, making it a robust solution for restoring high-value legacy and surveillance audio.", "AI": {"tldr": "CogSR是一种针对低采样率语音恢复的框架，通过语义引导的流匹配来提高语音清晰度。", "motivation": "现有生成模型在处理严重退化的音频时容易产生不准确的语音内容，导致无法有效恢复原始信息。为了改善这个问题，提出了一种基于认知重建的方法，以提高低采样率录音的质量和准确性。", "method": "通过整合大型音语言模型，并使用Chain-of-Thought推理作为语义锚点，结合显式的声学先验确保说话人身份的一致性，从而指导流匹配来合成高频细节，实现高质量的语音恢复。", "result": "实验表明CogSR在严重退化情况下能够有效消除模糊性，提高了音频恢复的质量和准确性。", "conclusion": "CogSR框架通过结合语义理解和声学先验信息，成功地解决了低采样率语音超分辨率问题，在数字归档和调查取证中具有广泛应用前景。"}}
{"id": "2512.16303", "pdf": "https://arxiv.org/pdf/2512.16303", "abs": "https://arxiv.org/abs/2512.16303", "authors": ["Feng Liang", "Sizhe Cheng", "Chenqi Yi"], "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project", "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.", "AI": {"tldr": "PixelArena是一个基准测试，用于评估多模态大型语言模型在图像生成任务中的像素精度视觉智能。", "motivation": "当前的许多图像生成基准更多关注美学而非精细度。为了客观地评估模型的细粒度生成能力，提出使用语义分割任务来衡量其视觉智能和泛化性能。", "method": "通过对比最新模型Gemini 3 Pro Image和其他模型在语义分割任务中的表现，并进行定性和定量分析，展示它们的效果差异和失败案例。", "result": "Gemini 3 Pro Image能够在零样本设置下生成高精度的语义掩模，表现出前所未有的视觉智能与泛化能力。其他模型的表现则各有不同。", "conclusion": "该研究展示了图像生成领域的最新进展，并为多模态、推理、可解释性和基准测试方面的未来研究提供了见解。"}}
{"id": "2512.16302", "pdf": "https://arxiv.org/pdf/2512.16302", "abs": "https://arxiv.org/abs/2512.16302", "authors": ["Zixuan Chen", "Chongkai Gao", "Lin Shao", "Jieqi Shi", "Jing Huo", "Yang Gao"], "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation", "categories": ["cs.RO"], "comment": "Accepted by AAAI 2026", "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.", "AI": {"tldr": "提出了一种新的框架ManiLong-Shot，用于长时程预抓取操作的一次模仿学习。", "motivation": "目前的一次模仿学习方法主要局限于短时程任务，无法有效应用于复杂的长时程操作。因此，提出了ManiLong-Shot以解决这一限制问题。", "method": "该框架通过物理交互事件重组长时程任务，并将其转化为基于视觉语言模型或规则启发式的方法来预测关键区域和执行目标末端位置的计算。", "result": "实验结果表明，仅使用10个短时程任务训练后，ManiLong-Shot在20种未见过的长时程任务中实现了显著的效果提升。实际机器人实验进一步验证了其稳健性和实用性。", "conclusion": "ManiLong-Shot框架有效地解决了当前一次模仿学习方法在复杂长时程操作中的局限性，并展示了良好的泛化能力和实际应用前景。"}}
{"id": "2512.16301", "pdf": "https://arxiv.org/pdf/2512.16301", "abs": "https://arxiv.org/abs/2512.16301", "authors": ["Pengcheng Jiang", "Jiacheng Lin", "Zhiyi Shi", "Zifeng Wang", "Luxi He", "Yichen Wu", "Ming Zhong", "Peiyang Song", "Qizheng Zhang", "Heng Wang", "Xueqiang Xu", "Hanwen Xu", "Pengrui Han", "Dylan Zhang", "Jiashuo Sun", "Chaoqi Yang", "Kun Qian", "Tian Wang", "Changran Hu", "Manling Li", "Quanzheng Li", "Hao Peng", "Sheng Wang", "Jingbo Shang", "Chao Zhang", "et al. (9 additional authors not shown)"], "title": "Adaptation of Agentic AI", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "AI": {"tldr": "本文统一定制化研究领域的快速扩张，提出了一种涵盖代理适应和工具适应的系统框架。", "motivation": "随着先进代理人工智能系统的性能、可靠性和泛化的提升，改进这些系统的定制化机制变得尤为重要。此论文旨在提供一个概念基础与实用路线图，以帮助研究人员和技术人员构建更强大的代理AI系统。", "method": "本文将定制化划分为工具执行信号和代理人输出信号形式的代理适应以及无代理监督和有代理监督形式的工具适应，并通过代表性的方法展示了其优势、局限性及开放挑战与未来机遇。", "result": "论文提供了一个有助于设计适应策略明确权衡的概念框架，为研究者和技术人员提供了实用指南。", "conclusion": "本文提出了一种全面的理解和应用代理人工智能系统定制化机制的方法，从而为更高效和可靠的代理AI系统的构建奠定了基础。"}}
{"id": "2512.16300", "pdf": "https://arxiv.org/pdf/2512.16300", "abs": "https://arxiv.org/abs/2512.16300", "authors": ["Fanrui Zhang", "Qiang Zhang", "Sizhuo Zhou", "Jianwen Sun", "Chuanhao Li", "Jiaxin Ai", "Yukang Feng", "Yujie Zhang", "Wenjie Li", "Zizhen Li", "Yifan Chang", "Jiawei Liu", "Kaipeng Zhang"], "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection", "categories": ["cs.AI"], "comment": "11 pages, 6 figures", "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.", "AI": {"tldr": "本文提出了一种基于多轮互动的图像伪造检测框架ForenAgent，该框架使大型语言模型能够自主生成、执行和迭代优化Python基础的低级工具，以实现更灵活且可解释的伪造分析。", "motivation": "现有的图像伪造检测方法要么利用语义无关的低级特征，要么依赖于具有高级别语义知识的多模态大规模预训练语言模型。这两种信息流在范式和推理上高度异构，使得现有方法难以统一这些方法或有效地建模它们之间的跨级别交互。", "method": "本文提出了一种名为ForenAgent的图像伪造检测框架，该框架通过结合冷启动和强化微调两个阶段训练管道来提高其工具互动能力和推理适应性。它设计了一个动态推理循环，包括全球感知、局部聚焦、迭代探测和整体裁决，并将其作为数据采样策略和任务对齐过程奖励进行实例化。", "result": "实验表明，在低级工具的辅助下，ForenAgent在具有挑战性的图像伪造检测任务中表现出了工具使用能力和反思性推理能力。它为通用目的的图像伪造检测铺平了道路。", "conclusion": "本文提出的方法展示了如何使大型语言模型通过自主生成和优化低级工具来提高图像伪造分析的能力，并且这种方法可能为更广泛的应用开辟新的可能性。"}}
{"id": "2512.16297", "pdf": "https://arxiv.org/pdf/2512.16297", "abs": "https://arxiv.org/abs/2512.16297", "authors": ["Taozhao Chen", "Linghan Huang", "Kim-Kwang Raymond Choo", "Huaming Chen"], "title": "Feature-Selective Representation Misdirection for Machine Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.", "AI": {"tldr": "提出了一种新的机器遗忘技术SRMU，用于在大型语言模型中选择性地抑制有害表示。", "motivation": "解决当前机器遗忘技术在实际操作环境中难以区分要忘记和保留的数据集的问题，特别是在高度纠缠的分布下，现有的基于扰动的方法往往会损害模型的整体性能或无法确保安全性。", "method": "SRMU是一个新的激活编辑框架，它通过结构化的误导向量和激活重要性图来有选择地抑制有害表示，同时保持良性数据的效用。", "result": "实验显示，SRMU在低、高纠缠配置下的WMDP基准测试中达到了最先进的遗忘性能，并且在20-30%重叠的情况下仍能有效工作，而现有基线则崩溃了。", "conclusion": "SRMU为安全驱动的模型治理、隐私合规性和控制知识移除提供了坚实的基础，在新兴的大规模语言模型应用中表现出色。"}}
{"id": "2512.16295", "pdf": "https://arxiv.org/pdf/2512.16295", "abs": "https://arxiv.org/abs/2512.16295", "authors": ["Zhenyu Wu", "Jingjing Xie", "Zehao Li", "Bowen Yang", "Qiushi Sun", "Zhaoyang Liu", "Zhoumianze Liu", "Yu Qiao", "Xiangyu Yue", "Zun Wang", "Zichen Ding"], "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models", "categories": ["cs.AI"], "comment": null, "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.", "AI": {"tldr": "本文提出了一个名为OS-Oracle的框架，用于生成跨平台GUI批评数据，并评估模型在不同设备上的表现。", "motivation": "随着基于视觉语言模型的计算机使用代理（CUAs）在图形用户界面导航和操作中的能力增强，可靠的步骤级决策成为实际部署的关键障碍。错误累积迅速且不可逆的操作可能导致意外后果，因此需要能够评估每次动作前后的批评模型。", "method": "提出了OS-Oracle框架，该框架包括一个可扩展的数据管道以合成跨平台GUI批评数据、一种两阶段训练范式（监督微调SFT和保持一致性的相对策略优化CP-GRPO）以及OS-Critic Bench基准测试集。", "result": "生成的批评模型OS-Oracle-7B在OS-Critic Bench上实现了最先进的开放源代码视觉语言模型性能，并且在移动领域超过了专有模型。此外，当用作预批判器时，它改善了原生GUI代理如UI-TARS-1.5-7B的表现。", "conclusion": "该框架通过提供高质量的数据和评估基准来提高批评模型的有效性，从而帮助解决实际部署中的关键瓶颈问题。"}}
{"id": "2512.16294", "pdf": "https://arxiv.org/pdf/2512.16294", "abs": "https://arxiv.org/abs/2512.16294", "authors": ["Amna Amir", "Erchan Aptoula"], "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.", "AI": {"tldr": "MACL方法用于解决多标签遥感图像检索中的语义重叠、标签分布不平衡和复杂类间共现模式的问题。", "motivation": "针对遥感图像检索中存在类别间的语义重叠、高度不均衡的标签分布以及复杂的类间共现模式等挑战，提出了一种新的对比学习损失函数MACL方法以应对这些问题。", "method": "MACL通过引入基于标签感知采样的策略，并结合频率敏感加权和动态温度缩放技术，在多类别场景中实现了平衡的表示学习。", "result": "在三个基准数据集上进行的实验表明，所提出的MACL方法显著优于现有的对比损失基线模型，能够有效缓解语义不平衡问题并提供更可靠的检索性能。", "conclusion": "该研究提出了一个名为MACL的新框架以解决多标签遥感图像检索中的挑战性问题，并通过广泛的实验证明了其有效性。"}}
{"id": "2512.16293", "pdf": "https://arxiv.org/pdf/2512.16293", "abs": "https://arxiv.org/abs/2512.16293", "authors": ["Karola Köpferl", "Albrecht Kurze"], "title": "Ein Typenrad auf der Überholspur: Die Kult-Schreibmaschine \"Erika\" trifft KI", "categories": ["cs.HC"], "comment": "in German language. 18. Internationales Symposium für Informationswissenschaft (ISI 2025), Chemnitz, Deutschland, 18-20.03.2025, S. 399-404", "summary": "In the 15th century, printing revolutionized the dissemination of information. Innovations such as typewriters and computers have increased the speed and volume of information flows over time. More recent developments in large language models such as ChatGPT enable text to be generated in a matter of seconds. However, many people do not understand how this works and what the long-term implications are. That is why we have \"hacked\" an old typewriter so that users can interact with an LLM chatbot, which over 1,200 participants have now been able to experience. It helps to understand the possibilities and limitations of AI. It gives us researchers insights into participants' concepts of AI as well as their expectations and concerns. It raises questions about these technological developments and stimulates discussions about the social impact of the intensification and acceleration of information and communication flows.", "AI": {"tldr": "将旧打字机改装成与大型语言模型互动的设备，以帮助公众理解和讨论AI技术。", "motivation": "通过交互式体验增进人们对AI的理解及其社会影响的探讨。", "method": "改造了一台老式的“Erika”打字机，使其能够与LLM聊天机器人进行交互。", "result": "超过1200名参与者体验了这一互动过程，并引发了对AI技术潜力和限制的关注。", "conclusion": "这种创新的交互方式有助于激发公众对于AI未来发展的讨论，同时也为研究人员提供了有关人们如何理解和接受新技术的信息。"}}
{"id": "2512.16285", "pdf": "https://arxiv.org/pdf/2512.16285", "abs": "https://arxiv.org/abs/2512.16285", "authors": ["Karola Köpferl", "Albrecht Kurze"], "title": "Machines, AI and the past//future of things", "categories": ["cs.HC"], "comment": "State of Responsible Technology 2025 - Generative Things. pp 13-19. Stichting ThingsCon Amsterdam", "summary": "This essay explores a techno-artistic experiment that reanimates a 1980s East German typewriter using a contemporary AI language model. Situated at the intersection of media archaeology and speculative design, the project questions dominant narratives of progress by embedding generative AI in an obsolete, tactile interface. Through public exhibitions and aesthetic intervention, we demonstrate how slowness, friction, and material render artificial intelligence not only visible but open to critical inquiry. Drawing on concepts such as zombie media, technostalgia, and speculative design, we argue that reappropriating outdated technologies enables new forms of critical engagement. Erika - the AI-enabled typewriter - functions as both interface and interruption, making space for reflection, irony, and cultural memory. In a moment of accelerated digital abstraction, projects like this foreground the value of deliberate slowness, experiential materiality, and historical depth. We conclude by advocating for a historicist design sensibility that challenges presentism and reorients human-machine interaction toward alternative, perceived futures.", "AI": {"tldr": "本文探讨了一个用当代AI语言模型重新激活80年代东德打字机的实验。", "motivation": "该项目质疑进步主导叙事，通过将生成式人工智能嵌入一个过时且触觉界面中来提出问题。", "method": "使用媒体考古学和投机设计的概念，展示了如何通过公共展览及美学干预使AI变得可见并易于批判性研究。", "result": "重新利用旧技术可以开启新的批评参与形式，该项目强调了有意识的慢节奏、体验材料性和历史深度的价值。", "conclusion": "倡导一种历史性设计感性，挑战现时主义，并将人机交互导向替代的感知未来。"}}
{"id": "2512.16282", "pdf": "https://arxiv.org/pdf/2512.16282", "abs": "https://arxiv.org/abs/2512.16282", "authors": ["Jinhao Zhang", "Yunquan Zhang", "Daning Chen"], "title": "CKA-Guided Modular Quantization: Beyond Bit-Width to Algorithmic Diversity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CKA Guided Modular Quantization, a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. Our method independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMs including LLaMA and Qwen ,in terms of perplexity (PPL) and downstream task performance.", "AI": {"tldr": "提出了一种基于CKA的模块化量化方法，通过在每个层上独立评估多个量化算法并自动选择最佳策略，实现了异构量化。", "motivation": "当前主流的大规模语言模型后训练量化方法通常采用统一的量化策略，未能考虑到各层之间算法适用性的差异。为了克服这一限制，作者提出了基于CKA引导的模块化量化框架。", "method": "该方法独立评估每个网络层上的多个PTQ（Post-Training Quantization）算法，并使用线性中心核对齐（CKA）作为度量标准来自动选择每层的最佳量化策略。然后将这些个体优化后的策略组合起来构建一个混合量化模型。", "result": "实验表明，该方法在主流的LLM（大型语言模型）包括LLaMA和Qwen中，在困惑度（PPL）和下游任务性能方面始终优于统一量化基准线和最先进的混合精度方法。", "conclusion": "基于CKA引导的方法能够有效地实现异构量化，并显著提高大规模语言模型的量化效果。"}}
{"id": "2512.16280", "pdf": "https://arxiv.org/pdf/2512.16280", "abs": "https://arxiv.org/abs/2512.16280", "authors": ["Gilad Gressel", "Rahul Pankajakshan", "Shir Rozenfeld", "Ling Li", "Ivan Franceschini", "Krishnahsree Achuthan", "Yisroel Mirsky"], "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams", "categories": ["cs.CR", "cs.AI", "cs.CY"], "comment": "ef:Usenix Security Symposium 2026", "summary": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation. We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.", "AI": {"tldr": "研究AI在浪漫诱骗诈骗中的作用", "motivation": "探讨大型语言模型在浪漫诱骗诈骗中扮演的角色以及现有安全措施的有效性。", "method": "通过采访145名内部人员和5位受害者，进行长达一周的双盲对话实验，并测试商业安全过滤器来评估结果。", "result": "发现87%的诈骗行为可以自动化；LLM比人类操作员更易赢得信任并获得更高的遵从性请求（分别为46％和18％）；流行的安全过滤器未能检测到任何浪漫诱骗对话。", "conclusion": "提示浪漫诱骗诈骗可能完全由AI驱动，而现有防御措施无法阻止其扩展"}}
{"id": "2512.16279", "pdf": "https://arxiv.org/pdf/2512.16279", "abs": "https://arxiv.org/abs/2512.16279", "authors": ["Yiliu Yang", "Yilei Jiang", "Qunzhong Wang", "Yingshui Tan", "Xiaoyong Zhu", "Sherman S. M. Chow", "Bo Zheng", "Xiangyu Yue"], "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.", "AI": {"tldr": "QuadSentinel 是一种四代理安全守护机制，用于将自然语言的安全策略编译成可由机器检查的规则，并在线执行。", "motivation": "大型语言模型驱动的代理在解决复杂任务时存在安全性风险。现有部署者编写的安全策略模糊且依赖于上下文，难以转换为可机检规则，导致运行时强制执行不可靠。", "method": "通过将安全策略表示为序列，并引入一个四代理守卫（状态跟踪器、政策验证器、威胁观察员和裁判）来编译这些策略成基于可观测状态的机器检查规则。使用裁判逻辑并结合高效的前k个谓词更新者，以优先执行检查和解决冲突。", "result": "在ST-WebAgentBench和AgentHarm基准测试中，QuadSentinel提高了护栏准确性和规则召回率，并减少了假阳性。与单代理基线相比，如ShieldAgent，它提供了更好的整体安全控制。", "conclusion": "QuadSentinel 提供了一种方法，在不修改核心代理的情况下实现更有效、可靠的安全策略执行。"}}
{"id": "2512.16275", "pdf": "https://arxiv.org/pdf/2512.16275", "abs": "https://arxiv.org/abs/2512.16275", "authors": ["Mohamed Abouagour", "Eleftherios Garyfallidis"], "title": "GFLAN: Generative Functional Layouts", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 15 figures", "summary": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.", "AI": {"tldr": "该论文提出了GFLAN，一种通过拓扑规划和几何实现的两阶段分解生成平面图的方法。", "motivation": "传统的自动楼层布局生成方法难以同时处理组合搜索、几何约束满足和功能性设计需求。最近的深度学习方法虽然有所进步，但仍然难以捕捉到建筑学中的重要推理逻辑，如拓扑关系优先于几何实例化等。", "method": "GFLAN采用两阶段分解的方法：第一阶段使用带有双编码器的专门卷积架构逐次分配房间中心点；第二阶段利用Transformer增强的图神经网络构造异构图，并回归房间边界。", "result": "该方法能够生成满足功能需求和几何约束的复杂楼层布局，改进了现有的自动平面设计技术。", "conclusion": "通过明确地将拓扑规划与几何实现分开处理，GFLAN成功解决了传统方法难以兼顾的问题，为建筑设计自动化提供了一种新的思路。"}}
{"id": "2512.16272", "pdf": "https://arxiv.org/pdf/2512.16272", "abs": "https://arxiv.org/abs/2512.16272", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Rami Katan", "Alice Podolsky"], "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities. To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked. Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.", "AI": {"tldr": "本文通过在COBOL代码生成中的应用，探讨了大型语言模型作为代码生成管道中评判者时存在的领域特定问题，并提出了一种轻量级的分析检查工具来改善这一状况。", "motivation": "大型语言模型(LaaJ)被用于代码生成过程中评判阶段。尽管具有可扩展性优点，但它们在关键评估任务中的可靠性存在问题，因为这些模型容易忽视领域的特殊问题。研究者希望通过工业案例具体剖析这些问题以提升LaaJ的可靠性和准确性。", "method": "本文通过分析COBOL程序及其对应的LaaJs评价结果，并结合专家知识构建初步分类法来识别领域特定的问题。基于此分类法，开发了一种轻量级的分析检查工具，该工具能够标记出超过30个在实践中观察到的具体问题。并将其用作分析提示，动态地注入评判者的提示中。", "result": "实验结果显示，单独使用LaaJ只能检测代码中的约45%错误（对于所有测试过的评判者）。而仅依赖于轻量级的分析检查工具则缺乏解释深度。然而当两者结合时，在最佳配置下可达到94％的覆盖率，并生成更准确丰富的说明。", "conclusion": "研究表明，将分析提示与LaaJ结合可以显著提升代码评估任务中的可靠性和准确性。通过这种混合方法能够发现更多潜在错误并提高评判质量，同时发布的数据集和所有使用的提示为后续研究提供了资源支持。"}}
{"id": "2512.16271", "pdf": "https://arxiv.org/pdf/2512.16271", "abs": "https://arxiv.org/abs/2512.16271", "authors": ["Geofrey Owino", "Bernard Shibwabo Kasamani", "Ahmed M. Abdelmoniem", "Edem Wornyo"], "title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification", "categories": ["cs.SD", "cs.AI"], "comment": "This paper has been published in the IEEE proceedings of the 8th International Conference of Computer and Informatics Engineering (IC2IE)", "summary": "Accurate and interpretable classification of infant cry paralinguistics is essential for early detection of neonatal distress and clinical decision support. However, many existing deep learning methods rely on correlation-driven acoustic representations, which makes them vulnerable to noise, spurious cues, and domain shifts across recording environments. We propose DACH-TIC, a Domain-Agnostic Causal-Aware Hierarchical Audio Transformer for robust infant cry classification. The model integrates causal attention, hierarchical representation learning, multi-task supervision, and adversarial domain generalization within a unified framework. DACH-TIC employs a structured transformer backbone with local token-level and global semantic encoders, augmented by causal attention masking and controlled perturbation training to approximate counterfactual acoustic variations. A domain-adversarial objective promotes environment-invariant representations, while multi-task learning jointly optimizes cry type recognition, distress intensity estimation, and causal relevance prediction. The model is evaluated on the Baby Chillanto and Donate-a-Cry datasets, with ESC-50 environmental noise overlays for domain augmentation. Experimental results show that DACH-TIC outperforms state-of-the-art baselines, including HTS-AT and SE-ResNet Transformer, achieving improvements of 2.6 percent in accuracy and 2.2 points in macro-F1 score, alongside enhanced causal fidelity. The model generalizes effectively to unseen acoustic environments, with a domain performance gap of only 2.4 percent, demonstrating its suitability for real-world neonatal acoustic monitoring systems.", "AI": {"tldr": "提出了一种用于婴儿哭声分类的模型DACH-TIC，该模型结合了因果注意力、层次表示学习和多任务监督等技术。", "motivation": "现有的深度学习方法依赖于相关性驱动的声音表示，在噪声环境中容易受到干扰。作者希望通过使用因果感知的方法来提高婴儿哭声分类的鲁棒性和准确性。", "method": "DACH-TIC模型采用了结构化的Transformer骨干网络，结合局部令牌级和全局语义编码器，并通过因果注意力屏蔽和控制扰动训练来近似反事实声音变化。同时采用领域对抗目标促进环境不变表示，多任务学习优化哭声类型识别、痛苦强度估计和因果相关性预测。", "result": "在Baby Chillanto和Donate-a-Cry数据集上进行评估时，DACH-TIC模型在准确性方面优于HTS-AT和SE-ResNet Transformer等基线模型，并且具有更好的领域泛化能力。", "conclusion": "实验结果表明，DACH-TIC模型提高了婴儿哭声分类的准确性和因果忠实度，在实际新生儿声音监测系统中表现出色。"}}
{"id": "2512.16270", "pdf": "https://arxiv.org/pdf/2512.16270", "abs": "https://arxiv.org/abs/2512.16270", "authors": ["Rui Gui", "Yang Wan", "Haochen Han", "Dongxing Mao", "Fangming Liu", "Min Li", "Alex Jinpeng Wang"], "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.", "AI": {"tldr": "TextEditBench是一个评估基准，专注于图像中的文本编辑任务，特别是在语义一致性、上下文连贯性和跨模态对齐方面的能力。", "motivation": "当前的模型在处理复杂的文本编辑时遇到困难，如物理一致性和布局感知集成。为了填补这一空白，作者提出了一种新的评估基准来推动多模式生成中的文字引导图像编辑和推理。", "method": "TextEditBench是一个专注于文本编辑任务的综合评估基准，特别强调语义一致性（SE）作为衡量模型在文本编辑中保持语义一致性和跨模态对齐的能力的新维度。", "result": "实验结果表明，现有的编辑系统能够遵循简单的文字指令，但在上下文依赖推理、物理一致性和布局感知集成方面仍然存在挑战。", "conclusion": "TextEditBench为评估和推进多模式生成中的文本引导图像编辑提供了一个新的测试平台。"}}
{"id": "2512.16266", "pdf": "https://arxiv.org/pdf/2512.16266", "abs": "https://arxiv.org/abs/2512.16266", "authors": ["Paloma Casteleiro Costa", "Parnian Ghapandar Kashani", "Xuhui Liu", "Alexander Chen", "Ary Portes", "Julien Bec", "Laura Marcu", "Aydogan Ozcan"], "title": "Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning", "categories": ["cs.CV", "cs.LG", "physics.med-ph", "physics.optics"], "comment": "30 Pages, 9 Figures", "summary": "Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.", "AI": {"tldr": "本文介绍了一种基于深度学习的多通道像素超分辨率（PSR）框架FLIM_PSR_k，用于从大像素尺寸数据中重建高分辨率荧光寿命成像（FLIM）图像。", "motivation": "荧光寿命成像显微镜（FLIM）由于长像素停留时间和低信噪比的限制，在临床应用中的采用受到阻碍。为了克服这些限制并提高其空间分辨率和采集速度，本文开发了一种深度学习方法来实现像素超分辨。", "method": "使用条件生成对抗网络(cGAN)框架训练模型，该模型能够从大像素尺寸数据中重建高分辨率FLIM图像，并通过与扩散模型进行对比验证了其在推理时间上的优势。", "result": "实验证明，FLIM_PSR_k能够在盲测试中可靠地实现k=5的超分辨因子，提高输出图像的空间带宽产品25倍，同时改善多种图像质量指标。该方法能够加快图像采集速度并减轻自荧光基FLIM中的信噪比限制。", "conclusion": "通过增加FLIM的有效空间分辨率，FLIM_PSR_k推进了寿命成像技术向更快、更高分辨率和硬件灵活性方向发展，并更好地适配低数值孔径和微型化平台。"}}
{"id": "2512.16265", "pdf": "https://arxiv.org/pdf/2512.16265", "abs": "https://arxiv.org/abs/2512.16265", "authors": ["Bangya Liu", "Chengpo Yan", "Chenghao Jiang", "Suman Banerjee", "Akarsh Prabhakara"], "title": "Privacy-Aware Sharing of Raw Spatial Sensor Data for Cooperative Perception", "categories": ["cs.NI", "cs.RO"], "comment": null, "summary": "Cooperative perception between vehicles is poised to offer robust and reliable scene understanding. Recently, we are witnessing experimental systems research building testbeds that share raw spatial sensor data for cooperative perception. While there has been a marked improvement in accuracies and is the natural way forward, we take a moment to consider the problems with such an approach for eventual adoption by automakers. In this paper, we first argue that new forms of privacy concerns arise and discourage stakeholders to share raw sensor data. Next, we present SHARP, a research framework to minimize privacy leakage and drive stakeholders towards the ambitious goal of raw data based cooperative perception. Finally, we discuss open questions for networked systems, mobile computing, perception researchers, industry and government in realizing our proposed framework.", "AI": {"tldr": "本文提出了一种名为SHARP的研究框架，旨在减少隐私泄露问题，促进车辆间共享原始空间传感器数据以实现协作感知。", "motivation": "随着实验系统研究中测试床的建设，原始空间传感器数据的共享促进了合作感知。然而，这种做法引发了新的隐私问题，并阻碍了制造商采用该技术。", "method": "SHARP框架旨在通过减少隐私泄漏来解决这些问题，并推动行业向基于原始数据的合作感知迈进。", "result": "讨论了网络系统、移动计算、感知研究、工业和政府实现所提议框架的开放性问题。", "conclusion": "提出了一个名为SHARP的研究框架，以期解决共享原始空间传感器数据中的隐私泄露问题，并推动其在实际应用中的采用。"}}
{"id": "2512.16262", "pdf": "https://arxiv.org/pdf/2512.16262", "abs": "https://arxiv.org/abs/2512.16262", "authors": ["Yifei She", "Ping Zhang", "He Liu", "Yanmin Jia", "Yang Jing", "Zijun Liu", "Peng Sun", "Xiangbin Li", "Xiaohe Hu"], "title": "Learning to Wait: Synchronizing Agents with the Physical World", "categories": ["cs.AI"], "comment": null, "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.", "AI": {"tldr": "论文提出了一种代理侧方法，使大型语言模型能够预测等待时间，以便在异步环境中精确同步。", "motivation": "现实世界的任务往往包含非阻塞动作和可变延迟，导致了行动启动与完成之间的‘时间差’问题。现有的解决方案要么限制了扩展性，要么稀释了代理的上下文窗口。", "method": "通过将代码作为行为范式扩展到时域领域，代理利用语义先验和情境学习来预测精确的等待时间。", "result": "实验表明，代理可以精准地校准内部时钟以减少查询开销和执行延迟。", "conclusion": "研究表明，时间感知是自主进化在开放环境中的一项可学技能。"}}
{"id": "2512.16259", "pdf": "https://arxiv.org/pdf/2512.16259", "abs": "https://arxiv.org/abs/2512.16259", "authors": ["Zeyu Huang", "Wei Meng", "Quan Liu", "Kun Chen", "Li Ma"], "title": "Improving Low-Latency Learning Performance in Spiking Neural Networks via a Change-Perceptive Dendrite-Soma-Axon Neuron", "categories": ["cs.NE"], "comment": null, "summary": "Spiking neurons, the fundamental information processing units of Spiking Neural Networks (SNNs), have the all-or-zero information output form that allows SNNs to be more energy-efficient compared to Artificial Neural Networks (ANNs). However, the hard reset mechanism employed in spiking neurons leads to information degradation due to its uniform handling of diverse membrane potentials. Furthermore, the utilization of overly simplified neuron models that disregard the intricate biological structures inherently impedes the network's capacity to accurately simulate the actual potential transmission process. To address these issues, we propose a dendrite-soma-axon (DSA) neuron employing the soft reset strategy, in conjunction with a potential change-based perception mechanism, culminating in the change-perceptive dendrite-soma-axon (CP-DSA) neuron. Our model contains multiple learnable parameters that expand the representation space of neurons. The change-perceptive (CP) mechanism enables our model to achieve competitive performance in short time steps utilizing the difference information of adjacent time steps. Rigorous theoretical analysis is provided to demonstrate the efficacy of the CP-DSA model and the functional characteristics of its internal parameters. Furthermore, extensive experiments conducted on various datasets substantiate the significant advantages of the CP-DSA model over state-of-the-art approaches.", "AI": {"tldr": "本文提出了一种基于软重置策略和潜在变化感知机制的树突-细胞-轴突（CP-DSA）神经元模型，以提高脉冲神经网络在低延迟学习任务中的性能。", "motivation": "硬复位机制导致信息退化，并且过于简化的神经元模型阻碍了网络准确模拟实际电位传输过程的能力。因此，本文旨在通过引入树突-细胞-轴突结构及软重置策略来解决这些问题。", "method": "提出了一种基于潜在变化感知的树突-细胞-轴突（CP-DSA）神经元模型，并对该模型进行了严格的理论分析以证明其有效性和内部参数的功能特性。", "result": "该CP-DSA模型在多个数据集上的实验结果表明，它能够在短时间内利用相邻时间步长之间的差异信息达到与现有最佳方法相当的性能表现。", "conclusion": "本文通过引入树突-细胞-轴突结构及潜在变化感知机制，成功地提高了脉冲神经网络在低延迟学习任务中的性能，并展示了其优越性。"}}
{"id": "2512.16251", "pdf": "https://arxiv.org/pdf/2512.16251", "abs": "https://arxiv.org/abs/2512.16251", "authors": ["Bong-Gyu Jang", "Younwoo Jeong", "Changeun Kim"], "title": "Interpretable Deep Learning for Stock Returns: A Consensus-Bottleneck Asset Pricing Model", "categories": ["q-fin.PR", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the \\textit{Consensus-Bottleneck Asset Pricing Model} (CB-APM), a partially interpretable neural network that replicates the reasoning processes of sell-side analysts by capturing how dispersed investor beliefs are compressed into asset prices through a consensus formation process. By modeling this ``bottleneck'' to summarize firm- and macro-level information, CB-APM not only predicts future risk premiums of U.S. equities but also links belief aggregation to expected returns in a structurally interpretable manner. The model improves long-horizon return forecasts and outperforms standard deep learning approaches in both predictive accuracy and explanatory power. Comprehensive portfolio analyses show that CB-APM's out-of-sample predictions translate into economically meaningful payoffs, with monotonic return differentials and stable long-short performance across regularization settings. Empirically, CB-APM leverages consensus as a regularizer to amplify long-horizon predictability and yields interpretable consensus-based components that clarify how information is priced in returns. Moreover, regression and GRS-based pricing diagnostics reveal that the learned consensus representations capture priced variation only partially spanned by traditional factor models, demonstrating that CB-APM uncovers belief-driven structure in expected returns beyond the canonical factor space. Overall, CB-APM provides an interpretable and empirically grounded framework for understanding belief-driven return dynamics.", "AI": {"tldr": "介绍了一种部分可解释的神经网络模型（CB-APM），用于预测美国股票的风险溢价和预期回报，通过建模投资者信念如何压缩形成共识来定价资产。", "motivation": "提高长期收益预测准确性，并且提供比传统深度学习方法更多的解释力，以理解基于信念的价格动态。", "method": "利用共识作为正则化器放大长期可预测性，模型能捕捉到未被传统因子模型完全覆盖的信念驱动结构。", "result": "CB-APM不仅提高了预测准确性和解释能力，还能产生经济上有意义的投资回报差异和稳定的长短期表现。", "conclusion": "CB-APM为理解基于信念的价格动态提供了有解释力且经验支持的框架。"}}
{"id": "2512.16250", "pdf": "https://arxiv.org/pdf/2512.16250", "abs": "https://arxiv.org/abs/2512.16250", "authors": ["Sanjoy Chowdhury", "Karren D. Yang", "Xudong Liu", "Fartash Faghri", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Dinesh Manocha", "Chun-Liang Li", "Raviteja Vemulapalli"], "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.", "AI": {"tldr": "AMUSE 是一个多模态视听基准，用于评估多说话者理解任务的代理性推理能力。RAFT 是一个数据高效的框架，通过奖励优化和内在多模式自我评价来提高模型在 AMUSE 基准上的表现。", "motivation": "当前多模态大型语言模型（MLLMs）在多说话者场景中表现出弱的理解能力和不一致的行为，尤其是在需要追踪谁在讲话、保持角色以及时间上事件的定位的任务中。为了解决这一问题，引入了 AMUSE 基准和 RAFT 框架。", "method": "RAFT 是一个数据高效的框架，它结合了奖励优化与内在多模式自我评价作为奖励，并进行选择性参数适应以实现高效的数据和参数更新。", "result": "使用 RAFT，在 AMUSE 基准上实现了高达39.52％的相对准确性提升。", "conclusion": "AMUSE 和 RAFT 一起提供了一个用于评估多模态模型代理推理能力和改进其能力的实际平台。"}}
{"id": "2512.16248", "pdf": "https://arxiv.org/pdf/2512.16248", "abs": "https://arxiv.org/abs/2512.16248", "authors": ["Qingguo Hu", "Zhenghao Lin", "Ziyue Yang", "Yucheng Ding", "Xiao Liu", "Yuting Jiang", "Ruizhe Wang", "Tianyu Chen", "Zhongxin Guo", "Yifan Xiong", "Rui Gao", "Lei Qu", "Jinsong Su", "Peng Cheng", "Yeyun Gong"], "title": "Sigma-Moe-Tiny Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures. Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny Code: https://github.com/microsoft/ltp-megatron-lm", "AI": {"tldr": "Sigma-MoE-Tiny是一种混合专家模型，通过极端稀疏性实现高效且强大的扩展。", "motivation": "提高基础模型的效率和可扩展性是主要动机，本文旨在探索更高效的MoE架构。", "method": "采用精细划分的专家分段，每个层最多96个专家。训练过程中使用逐步稀疏化方案平衡专家利用率和训练稳定性。", "result": "Sigma-MoE-Tiny在激活仅0.5B参数的情况下达到了顶级性能，并且在整个训练过程中保持稳定。", "conclusion": "提出的方法证明了实现极端稀疏性的可行性，为未来MoE架构的发展提供了宝贵的见解。"}}
{"id": "2512.16245", "pdf": "https://arxiv.org/pdf/2512.16245", "abs": "https://arxiv.org/abs/2512.16245", "authors": ["Aniruddha Roy", "Jyoti Patel", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints", "categories": ["cs.AI"], "comment": null, "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc. We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize: L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud, where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space. Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.", "AI": {"tldr": "AlignMerge是一种用于合并大型语言模型的同时保持其对齐性的几何感知框架。", "motivation": "现有的合并方案在保留损失的同时可能会破坏模型的对齐性，因此需要一种新的方法来确保在合并过程中保持模型的安全和对齐。", "method": "通过估计一个局部Fisher图中的对齐子空间，并使用投影器P_A进行优化，AlignMerge采用了一种基于几何约束的方法，其中L_geo保留了合并与专家的接近度，L_align惩罚沿对齐敏感方向的移动，L_bud强制执行软对齐预算。", "result": "在五个模型家族中，AlignMerge在保持或提高指令遵循、推理和有用性的同时，改善了对齐指标（AQI，毒性，LLM-judge对齐），并且表现出比Fisher soups，TIES，SafeMerge，MergeAlign更小的对齐子空间漂移和较少的预算违规。", "conclusion": "这些结果表明，保持对齐性的合并应该成为未来基础模型几何感知组合的第一设计目标。"}}
{"id": "2512.16244", "pdf": "https://arxiv.org/pdf/2512.16244", "abs": "https://arxiv.org/abs/2512.16244", "authors": ["Xueqi Ma", "Xingjun Ma", "Sarah Monazam Erfani", "Danilo Mandic", "James Bailey"], "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to AAAI 2026", "summary": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.", "AI": {"tldr": "该论文提出了一种利用大型语言模型的粗到细开放集图节点分类框架，以解决在无真实标签的情况下对离群样本进行分类的问题。", "motivation": "现有的方法通常将所有离群样本视为单一类别，但在诸如欺诈检测和医学诊断等高风险应用中，需要更深入地理解这些离群样本及其可能的标签。因此，研究如何从仅检测到区分出不同类别的离群样本变得至关重要。", "method": "提出了一种粗细分类框架（CFC），该框架包含三个关键组件：利用大型语言模型进行OOD检测和异常标签生成的粗分类器；基于图神经网络的精细分类器，通过由粗分类器识别的OOD样例训练以增强OOD检测和ID分类能力；以及通过大语言模型提示和后处理OOD标签实现精炼的OOD分类。", "result": "实验结果显示，该方法在图形和文本领域中将OOD检测提高了10%，并且在图形数据集中实现了高达70%的OOD分类精度。", "conclusion": "CFC框架通过大型语言模型的有效使用解决了开放集分类问题，并且显示出了显著优于现有技术的结果，在实际应用中具有很高的潜在价值。"}}
{"id": "2512.16243", "pdf": "https://arxiv.org/pdf/2512.16243", "abs": "https://arxiv.org/abs/2512.16243", "authors": ["Qi Zhang", "Yunfei Gong", "Zhidan Xie", "Zhizi Wang", "Antoni B. Chan", "Hui Huang"], "title": "Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models", "categories": ["cs.CV"], "comment": "13 pages, 7 figures, under review", "summary": "Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.", "AI": {"tldr": "提出了一种半监督的多视图人群计数框架，通过排序不同的输入视图数量的融合模型来改善数据不足问题", "motivation": "解决多视图人群计数中由于标注困难导致的数据限制问题", "method": "采用两种方法对多视图融合模型进行排名：一种基于预测结果，另一种基于不确定性估计，并将这些约束以半监督的方式引入到训练过程中", "result": "实验显示所提出的多视图模型排序方法在与其他半监督计数方法对比中有优势", "conclusion": "通过改进的半监督框架可以有效解决多视图人群计数中数据不足的问题"}}
{"id": "2512.16237", "pdf": "https://arxiv.org/pdf/2512.16237", "abs": "https://arxiv.org/abs/2512.16237", "authors": ["Zhi Helu", "Huang Jingjing", "Xu Wang", "Xu Yangbin", "Zhang Wanyue", "Jiang Baoyang", "Deng Shirui", "Zhu Liang", "Li Fangfang", "Zhao Tiejun", "Lin Yankai", "Yao Yuan"], "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis", "categories": ["cs.AI"], "comment": null, "summary": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.", "AI": {"tldr": "SPRITE框架通过使用模拟器和大型模型程序化合成可扩展、多样化且高质量的空间推理数据，解决了现有视觉语言模型在空间理解和推理能力上的局限。", "motivation": "当前的视觉语言模型由于空间理解与推理能力有限，面对具身智能这一人工智能重大挑战时存在困境。现有的解决方案要么是模板化的数据集缺乏多样性，要么需要手动注释难以扩展且计算上不够精确。", "method": "SPRITE框架通过将真值生成重构为代码生成任务，并利用大型语言模型编译复杂的空间问题成可执行程序来克服这一局限性。这些程序随后被验证与来自模拟器的场景元信息进行比较，确保其准确性和可验证性同时保持语言多样性。", "result": "使用SPRITE框架创建的数据集训练视觉语言模型在多个空间基准测试中取得了显著性能提升，并且优于同规模的其他开放数据集。此外，扩展分析证实了克服传统模板方法低多样性的关键性，这对于建立稳健、通用的空间智能至关重要。", "conclusion": "通过将模拟器与大型语言模型相结合，SPRITE框架成功地解决了现有视觉语言模型在空间理解和推理能力上的局限问题，并为未来的具身智能研究提供了强有力的支持。"}}
{"id": "2512.16236", "pdf": "https://arxiv.org/pdf/2512.16236", "abs": "https://arxiv.org/abs/2512.16236", "authors": ["Tejul Pandit", "Sakshi Mahendru", "Meet Raval", "Dhvani Upadhyay"], "title": "The Evolution of Reranking Models in Information Retrieval: From Heuristic Methods to Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": "15 pages, 1 figure, Accepted in CLNLP'25", "summary": "Reranking is a critical stage in contemporary information retrieval (IR) systems, improving the relevance of the user-presented final results by honing initial candidate sets. This paper is a thorough guide to examine the changing reranker landscape and offer a clear view of the advancements made in reranking methods. We present a comprehensive survey of reranking models employed in IR, particularly within modern Retrieval Augmented Generation (RAG) pipelines, where retrieved documents notably influence output quality. We embark on a chronological journey through the historical trajectory of reranking techniques, starting with foundational approaches, before exploring the wide range of sophisticated neural network architectures such as cross-encoders, sequence-generation models like T5, and Graph Neural Networks (GNNs) utilized for structural information. Recognizing the computational cost of advancing neural rerankers, we analyze techniques for enhancing efficiency, notably knowledge distillation for creating competitive, lighter alternatives. Furthermore, we map the emerging territory of integrating Large Language Models (LLMs) in reranking, examining novel prompting strategies and fine-tuning tactics. This survey seeks to elucidate the fundamental ideas, relative effectiveness, computational features, and real-world trade-offs of various reranking strategies. The survey provides a structured synthesis of the diverse reranking paradigms, highlighting their underlying principles and comparative strengths and weaknesses.", "AI": {"tldr": "本文综述了信息检索系统中的重排序模型的发展历程，从基础方法到现代大型语言模型。", "motivation": "随着信息检索中重排序技术的不断发展和应用，作者希望通过梳理这些技术和方法来提供一个清晰的理解。", "method": "本文按照时间顺序回顾了重排序技术的历史发展轨迹，涵盖了基础方法、交叉编码器、序列生成模型、图神经网络等，并分析了效率提升的方法和技术，如知识蒸馏。此外，还探讨了大型语言模型在重排序中的应用及其新型提示策略和微调技巧。", "result": "文章总结并比较了不同重排序策略的基本理念、相对有效性及实际应用场景的权衡问题。", "conclusion": "本文为信息检索领域提供了系统的重排序方法综述，强调了各种重排序范式的原理、优势与劣势。"}}
{"id": "2512.16235", "pdf": "https://arxiv.org/pdf/2512.16235", "abs": "https://arxiv.org/abs/2512.16235", "authors": ["Satya Narayana Panda", "Vaishnavi Kukkala", "Spandana Iyer"], "title": "AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 5 figures, 1 table. Code available at https://github.com/colabre2020/Enhancing-Skin-Disease-Diagnosis", "summary": "Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation? We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment. In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.", "AI": {"tldr": "研发一种结合深度学习图像分析和结构化临床数据的多模态AI框架，以提高皮肤病诊断准确性。", "motivation": "解决由于专科医生短缺和复杂临床表现而导致的皮肤疾病准确诊断难题。通过将家族病史与临床成像相结合来增强皮肤病诊断，并支持临床试验验证及现实世界实施。", "method": "开发结合可解释卷积神经网络和决策树的多模态AI框架，利用深度学习进行图像分析，同时整合详细的家族病史数据。该方法在多样化的医疗环境中进行了前瞻性临床试验以验证其效果。", "result": "集成AI系统在引入家族病史数据后，特别是在遗传性皮肤病如黑色素瘤、银屑病和异位性皮炎的诊断上显示出更高的准确性；专家反馈表明有可能实现早期检测和个人化推荐建议。", "conclusion": "所开发的框架旨在整合到临床工作流程中，并通过可解释AI机制保持透明度。"}}
{"id": "2512.16234", "pdf": "https://arxiv.org/pdf/2512.16234", "abs": "https://arxiv.org/abs/2512.16234", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Hesheng Wang", "Ajmal Mian"], "title": "ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation", "categories": ["cs.CV"], "comment": null, "summary": "3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.", "AI": {"tldr": "该论文提出了ARMFlow，一种基于MeanFlow的自回归框架，用于在线生成高质量、实时性高的三维人体反应。", "motivation": "现有的方法难以同时满足高运动保真度、实时推断和自回归适应性的需求。为解决这些问题，作者提出了一种新的方法来应对这些挑战。", "method": "ARMFlow包括一个因果上下文编码器和基于MLP的速度预测器，并在训练中引入了Bootstrap Contextual Encoding（BSCE），用于缓解自回归生成中的误差积累问题。", "result": "在线生成方面，ARMFlow在InterHuman和InterX数据集上超越现有方法40%以上，在FID指标下表现优异；离线版本ReMFlow则达到了最先进的性能，并且是最快的离线方法之一。", "conclusion": "通过增强语义对齐、实现高精度低延迟的单步推断以及减少累积误差，ARMFlow有效解决了在线设置下的关键限制问题。"}}
{"id": "2512.16227", "pdf": "https://arxiv.org/pdf/2512.16227", "abs": "https://arxiv.org/abs/2512.16227", "authors": ["Qizhou Chen", "Chengyu Wang", "Taolin Zhang", "Xiaofeng He"], "title": "An Information-Theoretic Framework for Robust Large Language Model Editing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.", "AI": {"tldr": "提出了一种基于信息瓶颈理论的大型语言模型编辑框架，该框架能够精确压缩并隔离所需的知识更新，同时最小化对其他行为的影响。", "motivation": "当前的模型编辑技术在广义知识修正方面存在局限性，导致难以普遍适用。为了提高模型准确性、减少重新训练的成本和干扰，提出一种新的信息瓶颈理论框架来优化编辑过程。", "method": "该方法基于信息瓶颈理论构建了IBKE（Information Bottleneck Knowledge Editor），使用紧凑的潜在表示进行梯度指导更新，实现稳健且广泛适用的知识编辑。", "result": "实验结果表明，在多种大型语言模型和标准基准任务中，IBKE展现了最先进的准确性和改进后的知识修正通用性与特定性。", "conclusion": "这一研究确立了一种具有理论依据和实际应用价值的开放领域知识编辑范式，进一步增强了大型语言模型在现实世界中的实用性和可靠性。"}}
{"id": "2512.16226", "pdf": "https://arxiv.org/pdf/2512.16226", "abs": "https://arxiv.org/abs/2512.16226", "authors": ["Justin Jiang"], "title": "Image Compression Using Singular Value Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.", "AI": {"tldr": "研究使用奇异值分解和低秩矩阵近似进行图像压缩的方法", "motivation": "高效压缩对减少存储和带宽需求至关重要，特别是在互联网大量依赖图片的情况下", "method": "通过相对Frobenius误差和压缩比率评估方法性能，并将其应用于灰度和多通道图像以验证其通用性", "result": "结果表明低秩近似通常会产生与原始图像视觉相似的图像，但在相同错误水平下比JPEG、JPEG2000和WEBP等标准格式效率更低。在较低容许误差级别时，SVD压缩后的表示甚至可能超过原图大小", "conclusion": "该方法在实际图像压缩中不具备竞争力"}}
{"id": "2512.16221", "pdf": "https://arxiv.org/pdf/2512.16221", "abs": "https://arxiv.org/abs/2512.16221", "authors": ["Lorenzo Nava", "Ye Chen", "Maximillian Van Wyk de Vries"], "title": "Neural emulation of gravity-driven geohazard runout", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "Predicting geohazard runout is critical for protecting lives, infrastructure and ecosystems. Rapid mass flows, including landslides and avalanches, cause several thousand deaths across a wide range of environments, often travelling many kilometres from their source. The wide range of source conditions and material properties governing these flows makes their runout difficult to anticipate, particularly for downstream communities that may be suddenly exposed to severe impacts. Accurately predicting runout at scale requires models that are both physically realistic and computationally efficient, yet existing approaches face a fundamental speed-realism trade-off. Here we train a machine learning model to predict geohazard runout across representative real world terrains. The model predicts both flow extent and deposit thickness with high accuracy and 100 to 10,000 times faster computation than numerical solvers. It is trained on over 100,000 numerical simulations across over 10,000 real world digital elevation model chips and reproduces key physical behaviours, including avulsion and deposition patterns, while generalizing across different flow types, sizes and landscapes. Our results demonstrate that neural emulation enables rapid, spatially resolved runout prediction across diverse real world terrains, opening new opportunities for disaster risk reduction and impact-based forecasting. These results highlight neural emulation as a promising pathway for extending physically realistic geohazard modelling to spatial and temporal scales relevant for large scale early warning systems.", "AI": {"tldr": "通过机器学习模型预测地质灾害的流动范围和沉积厚度，提高计算效率。", "motivation": "当前方法在物理现实性和计算效率之间存在根本性的权衡问题。该研究旨在开发一种既精确又快速的方法来预测地质灾害的运行。", "method": "训练一个机器学习模型，在代表实际地形的数字高程模型上进行超过10万次的数值模拟，从而准确地预测流动范围和沉积厚度。", "result": "所提出的方法能够以比传统方法快100到10,000倍的速度进行计算，并且在各种真实世界地形中表现出良好的泛化能力。", "conclusion": "神经仿真为地质灾害建模提供了新的机会，可用于大规模早期预警系统的空间和时间尺度。"}}
{"id": "2512.16219", "pdf": "https://arxiv.org/pdf/2512.16219", "abs": "https://arxiv.org/abs/2512.16219", "authors": ["Zhihao Zhang", "Xuejun Yang", "Weihua Liu", "Mouquan Shen"], "title": "Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 9 figures", "summary": "Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.", "AI": {"tldr": "该论文提出了一种方法，通过学习高质量的初始噪声来改善单一视角的新视图合成效果。", "motivation": "现有的基于扩散模型的单视图新视图合成（NVS）模型需要改进以提升生成质量。当前缺乏专门的学习框架来使NVS模型能够学习到这种高质量的噪声。", "method": "设计了离散欧拉反演方法，将图像语义信息注入随机噪声中；提出了基于编码器-解码器网络（EDN）的学习框架，直接将随机噪声转换为高质量噪声。", "result": "实验结果表明，所提出的EDN能够无缝集成到多种NVS模型中，并在多个数据集上实现了显著的性能提升。", "conclusion": "该研究提供了一种有效的方法来改善基于扩散模型的单视图新视图合成效果，通过学习高质量初始噪声增强了生成图像的质量。"}}
{"id": "2512.16214", "pdf": "https://arxiv.org/pdf/2512.16214", "abs": "https://arxiv.org/abs/2512.16214", "authors": ["Jianming Liu", "Ren Zhu", "Jian Xu", "Kun Ding", "Xu-Yao Zhang", "Gaofeng Meng", "Cheng-Lin Liu"], "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving", "categories": ["cs.AI"], "comment": null, "summary": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.", "AI": {"tldr": "PDE-Agent是一个基于LLM驱动的多代理协作框架，用于从自然语言描述中自动解决偏微分方程（PDE），通过创新性的工具链集成和多代理协作技术提高自动化程度。", "motivation": "传统的偏微分方程求解方法复杂且依赖于手动配置和领域专业知识。尽管物理信息神经网络和DeepXDE等框架提高了自动化水平，但仍需专家知识支持。因此，开发一种完全自主的解决方案具有重要意义。", "method": "PDE-Agent通过两个关键创新实现了工具链增强型多代理协作：（1）带有图记忆的Prog-Act框架，通过双重循环机制实现有效的动态计划和错误纠正。（2）资源池与工具参数分离机制，解决现有框架中的工件管理和跨工具依赖问题。", "result": "PDE-Agent在复杂、多步骤任务中表现出优异的应用性和性能。评估表明该方法在多个偏微分方程类型上的有效性。", "conclusion": "PDE-Agent代表了一种新的自动求解偏微分方程的方法，有望推动自动化科学计算的未来发展。"}}
{"id": "2512.16213", "pdf": "https://arxiv.org/pdf/2512.16213", "abs": "https://arxiv.org/abs/2512.16213", "authors": ["Amit Vishwakarma", "K. S. Subrahamanian Moosath"], "title": "Enhanced 3D Shape Analysis via Information Geometry", "categories": ["cs.CV", "math.DG"], "comment": ":53B12; 62B11", "summary": "Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.", "AI": {"tldr": "本文通过信息几何框架，将三维点云表示为高斯混合模型（GMMs），提出了改进的对称KL散度（MSKL）来解决传统距离测量方法在三维形状分析中的局限性。", "motivation": "传统的几何度量如Hausdorff和Chamfer距离无法有效捕捉全局统计结构，且对外点敏感；现有的Kullback-Leibler (KL)散度近似对于高斯混合模型可能产生未界值或数值不稳。因此，需要一种新的方法来更准确、稳定地比较三维形状。", "method": "本文提出了一种基于信息几何的框架，将三维点云表示为GMM，并证明了GMM空间构成统计流形；同时引入改进的对称KL散度（MSKL），具有理论保证的上下限，确保数值稳定性。通过人体姿态辨别和动物形状比较实验验证其效果。", "result": "实验结果表明，提出的MSKL能够提供稳定且单调变化的结果，直接反映几何差异，并优于传统距离测量方法和其他KL近似。", "conclusion": "本文提出的方法解决了现有三维点云分析的局限性，为计算机图形学、摄影测量、计算机视觉和机器人等应用提供了更准确可靠的形状比较工具。"}}
{"id": "2512.16206", "pdf": "https://arxiv.org/pdf/2512.16206", "abs": "https://arxiv.org/abs/2512.16206", "authors": ["Sachin R. Pendse", "Darren Gergle", "Rachel Kornfield", "Kaylee Kruzan", "David Mohr", "Jessica Schleider", "Jina Suh", "Annie Wescott", "Jonah Meyerhoff"], "title": "The Agony of Opacity: Foundations for Reflective Interpretability in AI-Mediated Mental Health Support", "categories": ["cs.HC"], "comment": "Accepted to IASEAI'26", "summary": "Throughout history, a prevailing paradigm in mental healthcare has been one in which distressed people may receive treatment with little understanding around how their experience is perceived by their care provider, and in turn, the decisions made by their provider around how treatment will progress. Paralleling this offline model of care, people who seek mental health support from AI chatbots are similarly provided little context for how their expressions of distress are processed by the model, and subsequently, the logic that may underlie model responses. People in severe distress who turn to AI chatbots for support thus find themselves caught between black boxes, with unique forms of agony that arise from these intersecting opacities, including misinterpreting model outputs or attributing greater capabilities to a model than are yet possible, which has led to documented real-world harms. Building on empirical research from clinical psychology and AI safety, alongside rights-oriented frameworks from medical ethics, we describe how the distinct psychological state induced by severe distress can influence chatbot interaction patterns, and argue that this state of mind (combined with differences in how a user might perceive a chatbot compared to a care provider) uniquely necessitates a higher standard of interpretability in comparison to general AI chatbot use. Drawing inspiration from newer interpretable treatment paradigms, we then describe specific technical and interface design approaches that could be used to adapt interpretability strategies from four specific mental health fields (psychotherapy, community-based crisis intervention, psychiatry, and care authorization) to AI models, including consideration of the role of interpretability in the treatment process and tensions that may arise with greater interpretability.", "AI": {"tldr": "探讨AI心理健康支持系统中的透明度问题，提出更高的可解释性标准。", "motivation": "传统精神健康护理中缺乏患者与提供者之间的理解及信息交流。同样地，在使用AI聊天机器人寻求心理支持时，用户很难了解他们的表达如何被处理以及机器人的回应逻辑，从而导致误解和潜在风险。", "method": "基于临床心理学、人工智能安全性和医学伦理框架的研究成果，结合严重精神压力状态下用户的交互模式，提出提高心理健康领域中AI模型可解释性的策略和技术方法。", "result": "通过分析四个特定的心理健康领域的实践案例（心理疗法、社区危机干预、精神病学和护理授权），提出了适用于AI模型的可解释性技术与界面设计建议。", "conclusion": "鉴于严重精神压力状态下的独特需求，提高心理健康支持中AI聊天机器人的透明度对于减少误解和潜在风险至关重要。"}}
{"id": "2512.16202", "pdf": "https://arxiv.org/pdf/2512.16202", "abs": "https://arxiv.org/abs/2512.16202", "authors": ["Zilin Wang", "Sangwoo Mo", "Stella X. Yu", "Sima Behpour", "Liu Ren"], "title": "Open Ad-hoc Categorization with Contextualized Feature Learning", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it. Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective. On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.", "AI": {"tldr": "该论文提出了一种名为OAK的简单模型，用于开放场景下的即兴分类任务。", "motivation": "适应性地对视觉场景进行分类对于AI代理处理变化的任务至关重要。不同于固定的一些植物或动物类别，即兴类别是动态创建以满足特定目标。", "method": "该方法基于CLIP和GCD的优化思想，在输入端加入一组可学习的上下文令牌，同时通过图像-文本对齐与视觉聚类来优化模型。", "result": "在斯坦福大学和Clevr-4数据集上，OAK实现了最高的准确率和概念发现性能。例如，它在斯坦福情绪测试中达到了87.4%的新类别精度，超过了CLIP和GCD近50%，并且生成了可解释的热点图。", "conclusion": "该模型不仅提高了分类准确性，还通过产生透明且易于理解的热点图促进了信任，并支持了自适应和通用性的分类能力。"}}
{"id": "2512.16201", "pdf": "https://arxiv.org/pdf/2512.16201", "abs": "https://arxiv.org/abs/2512.16201", "authors": ["Sarosij Bose", "Ravi K. Rajendran", "Biplob Debnath", "Konstantinos Karydis", "Amit K. Roy-Chowdhury", "Srimat Chakradhar"], "title": "Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation", "categories": ["cs.CV"], "comment": null, "summary": "Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.", "AI": {"tldr": "提出了一种新的方法VALOR，用于提高医学图像和文本之间的对齐精度，从而生成更准确的放射学报告。", "motivation": "目前的大型医学视觉语言模型在生成基于视觉且临床准确的放射学报告方面存在挑战，尤其是在减少跨模态对齐不精确导致的幻觉问题上。现有方法依赖大量标注数据或检索策略，但效果有限。", "method": "VALOR采用强化学习后对准框架和分组相对近似优化（GRPO），通过两阶段训练：第一阶段使用文本奖励提升模型临床术语准确性；第二阶段对齐视觉投影模块，指导注意力关注与诊断任务最相关的图像区域。", "result": "实验表明，VALOR在多个基准测试上显著提高了事实准确性和视觉接地性能，超越了最先进的报告生成方法。", "conclusion": "通过解决视觉和语言模态之间的不精确对齐问题，VALOR能够生成更高质量的放射学报告，有助于自动化医疗工作流程。"}}
{"id": "2512.16200", "pdf": "https://arxiv.org/pdf/2512.16200", "abs": "https://arxiv.org/abs/2512.16200", "authors": ["Haishan Ye"], "title": "Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithms on Smooth Functions", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Rank-based zeroth-order (ZO) optimization -- which relies only on the ordering of function evaluations -- offers strong robustness to noise and monotone transformations, and underlies many successful algorithms such as CMA-ES, natural evolution strategies, and rank-based genetic algorithms. Despite its widespread use, the theoretical understanding of rank-based ZO methods remains limited: existing analyses provide only asymptotic insights and do not yield explicit convergence rates for algorithms selecting the top-$k$ directions. This work closes this gap by analyzing a simple rank-based ZO algorithm and establishing the first \\emph{explicit}, and \\emph{non-asymptotic} query complexities. For a $d$-dimension problem, if the function is $L$-smooth and $μ$-strongly convex, the algorithm achieves $\\widetilde{\\mathcal O}\\!\\left(\\frac{dL}μ\\log\\!\\frac{dL}{μδ}\\log\\!\\frac{1}{\\varepsilon}\\right)$ to find an $\\varepsilon$-suboptimal solution, and for smooth nonconvex objectives it reaches $\\mathcal O\\!\\left(\\frac{dL}{\\varepsilon}\\log\\!\\frac{1}{\\varepsilon}\\right)$. Notation $\\cO(\\cdot)$ hides constant terms and $\\widetilde{\\mathcal O}(\\cdot)$ hides extra $\\log\\log\\frac{1}{\\varepsilon}$ term. These query complexities hold with a probability at least $1-δ$ with $0<δ<1$. The analysis in this paper is novel and avoids classical drift and information-geometric techniques. Our analysis offers new insight into why rank-based heuristics lead to efficient ZO optimization.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.16199", "pdf": "https://arxiv.org/pdf/2512.16199", "abs": "https://arxiv.org/abs/2512.16199", "authors": ["Jerrin Bright", "Zhibo Wang", "Dmytro Klepachevskyi", "Yuhao Chen", "Sirisha Rambhatla", "David Clausi", "John Zelek"], "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.", "AI": {"tldr": "Avatar4D是一种生成定制化合成人体运动数据集的流程，适用于特定领域的应用。", "motivation": "当前的方法大多关注于日常动作并提供有限的灵活性。Avatar4D旨在解决这些局限性，并为具有独特挑战性的体育领域的人体运动理解提供高保真度的数据。", "method": "通过精细控制人体姿态、外观、摄像机视角和环境背景，生成多样化的真实世界可转换合成人类动作数据集Syn2Sport，用于评估最先进的姿势估计模型的有效性和泛化能力。", "result": "实验表明Avatar4D产生的合成数据与真实数据在特征空间上高度一致，并能有效促进监督学习、零样本迁移至实际数据以及跨体育项目的推广。", "conclusion": "该系统展示了生成可扩展、可控且转换性强的特定领域人类数据集的能力，无需依赖特定领域的实际数据。"}}
{"id": "2512.16185", "pdf": "https://arxiv.org/pdf/2512.16185", "abs": "https://arxiv.org/abs/2512.16185", "authors": ["Gourab Ghatak"], "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications", "categories": ["cs.AI"], "comment": null, "summary": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.", "AI": {"tldr": "提出了加权K谐波均值（WKHM）聚类算法，确保数值稳定性并实现基于接收信号强度的软分配。", "motivation": "为了克服经典K均值和约束K均值的缺点，并在无线网络中提供更准确的用户关联，提出了一种新的聚类方法。", "method": "提出了WKHM聚类算法，通过反距离加权实现数值稳定性和软分配。证明了该算法在固定初始化下的单调下降特性、基于BPP初始化的概率收敛性以及满足轻微衰减条件下的几乎处处收敛性。", "result": "通过广泛的仿真验证，显示WKHM在最小信号强度和负载公平性之间实现了优于经典和现代聚类基准的权衡。", "conclusion": "WKHM算法为无线网络中的联合无线电节点放置与用户关联提供了一种原理上的工具。"}}
