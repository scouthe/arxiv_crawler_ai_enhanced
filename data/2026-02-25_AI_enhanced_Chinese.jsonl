{"id": "2602.21089", "pdf": "https://arxiv.org/pdf/2602.21089", "abs": "https://arxiv.org/abs/2602.21089", "authors": ["Petr Chmel", "Aditi Dudeja", "Michal Koucký", "Ian Mertz", "Ninad Rajgopal"], "title": "Frontier Space-Time Algorithms Using Only Full Memory", "categories": ["cs.DS", "cs.CC"], "comment": null, "summary": "We develop catalytic algorithms for fundamental problems in algorithm design that run in polynomial time, use only $\\mathcal{O}(\\log(n))$ workspace, and use sublinear catalytic space matching the best-known space bounds of non-catalytic algorithms running in polynomial time. First, we design a polynomial time algorithm for directed $s$-$t$ connectivity using $n \\big/ 2^{Θ(\\sqrt{\\log n})}$ catalytic space, which matches the state-of-the-art time-space bounds in the non-catalytic setting [Barnes et al., 1998], and improves the catalytic space usage of the best known algorithm [Cook and Pyne, 2026]. Furthermore, using only $\\mathcal{O}(\\log(n))$ random bits we get a randomized algorithm whose running time nearly matches the fastest time bounds known for space-unrestricted algorithms. Second, we design polynomial time algorithms for the problems of computing Edit Distance, Longest Common Subsequence, and the Discrete Fréchet Distance, again using $n \\big/ 2^{Θ(\\sqrt{\\log n})}$ catalytic space. This again matches non-catalytic time-space frontier for Edit Distance and Least Common Subsequence [Kiyomi et al., 2021].", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21088", "pdf": "https://arxiv.org/pdf/2602.21088", "abs": "https://arxiv.org/abs/2602.21088", "authors": ["Roman Edenhofer"], "title": "A Space-space Trade-off for Directed st-Connectivity", "categories": ["cs.DS", "cs.CC"], "comment": null, "summary": "We prove a space-space trade-off for directed $st$-connectivity in the catalytic space model. For any integer $k \\leq n$, we give an algorithm that decides directed $st$-connectivity using $O(\\log n \\cdot \\log k+\\log n)$ regular workspace and $O\\left(\\frac{n}{k} \\cdot \\log^2 n\\right)$ bits of catalytic memory. This interpolates between the classical $O(\\log^2 n)$-space bound from Savitch's algorithm and a catalytic endpoint with $O(\\log n)$ workspace and $O(n\\cdot \\log^2 n)$ catalytic memory. As a warm-up, we present a catalytic variant of Savitch's algorithm achieving the endpoint above. Up to logarithmic factors, this matches the smallest catalyst size currently known for catalytic logspace algorithms, due to Cook and Pyne (ITCS 2026). Our techniques also extend to counting the number of walks from $s$ to $t$ of a given length $\\ell\\leq n$.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21078", "pdf": "https://arxiv.org/pdf/2602.21078", "abs": "https://arxiv.org/abs/2602.21078", "authors": ["Duowen Chen", "Yan Wang"], "title": "ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": "CVPR 2026. code: https://github.com/DuowenC/FSSLlib", "summary": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.", "AI": {"tldr": "提出了一种代理引导框架ProxyFL，旨在解决联邦半监督学习中的数据异质性问题。", "motivation": "现有的联邦半监督学习方法难以精确匹配理想全局分布，并且过滤低置信度未标记样本会导致参与训练的数据减少。因此，需要一种新的方法来同时缓解跨客户端和内部的异质性。", "method": "通过可学习分类器权重作为代理，模拟局部和全局类别分布；对于外部异质性，明确优化全球代理以对抗异常值而不是直接权重；对于内部异质性，通过正负代理池重新包含被丢弃样本，减轻潜在错误伪标签的影响。", "result": "实验和理论分析显示，在联邦半监督学习中，该方法具有显著的性能提升与收敛性。", "conclusion": "ProxyFL框架能够有效缓解联邦半监督学习中的数据异质性问题，并在训练过程中保持较好的性能和收敛速度。"}}
{"id": "2602.21072", "pdf": "https://arxiv.org/pdf/2602.21072", "abs": "https://arxiv.org/abs/2602.21072", "authors": ["Zhangjie Xia", "Yu Yang", "Pan Xu"], "title": "Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "33 pages, 9 figures, 11 tables", "summary": "Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.", "AI": {"tldr": "本文提出了一种基于局部动态差异的领域适应方法，旨在通过识别源域和目标域之间的局部相似性来提高离线强化学习的性能。", "motivation": "现有离线强化学习方法在处理全局动力学差异或逐样本过滤时存在局限性和高计算成本。为了更好地利用源数据并减少不必要的复杂度，作者提出了一个基于局部动态适应的方法。", "method": "LoDADA通过聚类源域和目标域的数据，并评估每个聚类中的动态差异来实现这一点。它保留来自具有小动态差异的集群的数据点，过滤掉那些差异较大的数据点，从而提高数据选择策略的有效性和可扩展性。", "result": "实验结果表明，与现有的最先进的离线强化学习方法相比，LoDADA在各种环境下的表现更为优越，特别是在处理复杂局部和全局动力学变化时。", "conclusion": "通过利用源域和目标域之间的局部动态差异，LoDADA提供了一种更有效且可扩展的数据选择策略，从而提高了离线强化学习的性能。"}}
{"id": "2602.21066", "pdf": "https://arxiv.org/pdf/2602.21066", "abs": "https://arxiv.org/abs/2602.21066", "authors": ["Claire McNamara", "Lucy Hederman", "Declan O'Sullivan"], "title": "The Initial Exploration Problem in Knowledge Graph Exploration", "categories": ["cs.AI"], "comment": "13 pages", "summary": "Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.", "AI": {"tldr": "本文探讨了知识图谱(KGs)在初次接触时面临的初始探索问题(IEP)，并提出了一个理论框架来理解和解决这个问题。", "motivation": "为了帮助缺乏语义网络技术专业知识的用户更好地理解与使用知识图谱，研究者们通过分析信息行为和人机交互的相关理论，识别并定义了初次面对陌生知识图谱时遇到的独特挑战——初始探索问题(IEP)。", "method": "该论文运用ASK、探究式搜索、信息觅食以及认知负荷理论等概念来构建一个关于初始探索问题的理论框架，并通过分析现有的KG界面设计中交互原语的应用，揭示了当前系统中存在的不足。", "result": "研究发现目前的知识图谱探索接口缺乏能够帮助用户了解知识图谱范围的基本互动机制。这表明在界面设计上存在空白领域，需要开发新的方式来引导初次接触的用户进行有效的初步探索。", "conclusion": "通过定义初始探索问题(IEP)，本文提供了一个评估KG界面和为用户提供初始阶段支持的设计框架。"}}
{"id": "2602.21064", "pdf": "https://arxiv.org/pdf/2602.21064", "abs": "https://arxiv.org/abs/2602.21064", "authors": ["Mehdi Acheli", "Walid Gaaloul"], "title": "Motivation is Something You Need", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.", "AI": {"tldr": "本文介绍了一种新的训练范式，该范式受到情感神经科学的启发，并通过设计一种双模型框架来模拟人类大脑中情感和认知之间的相互作用。", "motivation": "受情感状态对认知表现影响的启示，特别是在高好奇心和奖励期待的情况下，更广泛的脑区会被调动以提升认知功能。本文旨在利用这种现象，在训练过程中引入动机因素，增强基础模型的表现。", "method": "提出了一个双模型框架，其中较小的基础模型持续接受训练，而较大的动机模型在预定义的\"动机条件\"下间歇性激活。此框架允许共享权重更新，并在重要的训练步骤中选择性地扩展网络容量。", "result": "实验结果表明，在图像分类任务上，交替训练方案不仅有效地提升了基础模型的表现，而且即使见到了更少的数据，动力模型在某些情况下也超过了其独立运行的版本。", "conclusion": "该方法展示了同时训练两个针对不同部署限制而优化的模型的能力，并且保持较低的训练成本。这为提升机器学习模型性能的同时降低成本提供了新思路。"}}
{"id": "2602.21061", "pdf": "https://arxiv.org/pdf/2602.21061", "abs": "https://arxiv.org/abs/2602.21061", "authors": ["David Koplow", "Tomer Galanti", "Tomaso Poggio"], "title": "Tool Building as a Path to \"Superintelligence\"", "categories": ["cs.AI"], "comment": null, "summary": "The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.", "AI": {"tldr": "设计了一个基准测试来测量逻辑分布外推理中LLM的步骤成功概率γ，以评估其通过Diligent Learner框架实现超智能的能力。", "motivation": "希望通过衡量LLM在逻辑分布外推理中的步骤成功概率γ，理解它们是否能够通过Diligent Learner框架达到超级智能水平。", "method": "构建了一类涉及GF(2)电路重建的任务，这些任务随着每一步推理的难度增加，并且从信息理论的角度来看，在不仔细整合所有提供的信息的情况下不可能可靠地解决这些问题。", "result": "分析表明，小规模LLM的γ值会随深度增加而超线性下降，但前沿模型在该任务上表现出部分鲁棒性。此外，发现大规模推理的成功依赖于精确的工具调用，这被识别为实现通用超级智能的关键能力。", "conclusion": "成功的大规模推理需要精确的工具设计和调用，这对LLM通过Diligent Learner框架达到超智能水平至关重要。"}}
{"id": "2602.21059", "pdf": "https://arxiv.org/pdf/2602.21059", "abs": "https://arxiv.org/abs/2602.21059", "authors": ["Anna Martin-Boyle", "William Humphreys", "Martha Brown", "Cara Leckey", "Harmanpreet Kaur"], "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems", "categories": ["cs.HC", "cs.CL"], "comment": "24 pages, 2 figures. Accepted at ACM CHI conference on Human Factors in Computing Systems, 2026", "summary": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the assessment strategies of practicing scientists. In collaboration with domain experts, we identified 20 error patterns across seven categories through thematic analysis of 68 question-answer pairs. We validated this schema through contextual inquiries with 10 additional scientists, which showed not only which errors experts naturally identify but also how structured evaluation schemas can help them detect previously overlooked issues. Domain experts use systematic assessment strategies, including technical precision testing, value-based evaluation, and meta-evaluation of their own practices. We discuss implications for supporting expert evaluation of LLM outputs, including opportunities for personalized, schema-driven tools that adapt to individual evaluation patterns and expertise levels.", "AI": {"tldr": "开发并验证了一种评估大型语言模型在学术问答系统中错误的方案，该方案反映了实践科学家对LLM输出的实际评估策略。", "motivation": "当前用于测试LLM可靠性的评价指标主要侧重于效率和可扩展性，但缺乏上下文细节，无法反映科学领域专家如何实际评估LLM的输出。因此，需要一种更能体现专家评估方法的新方案。", "method": "通过与领域专家合作，对68个问题答案对进行了主题分析，并确定了七个类别中的20种错误模式。进一步通过10名科学家的上下文查询验证该方案的有效性，展示了该方案不仅能够识别专家自然发现的问题，还能帮助他们检测之前忽视的问题。", "result": "证明领域专家使用包括技术精度测试、基于价值评估和元评估在内的系统化评估策略，并且结构化的评价框架有助于识别更多问题。", "conclusion": "讨论了支持专家对LLM输出进行评估的未来方向，提出个性化、基于方案驱动的工具适应个人评估模式和专业水平的机会。"}}
{"id": "2602.21054", "pdf": "https://arxiv.org/pdf/2602.21054", "abs": "https://arxiv.org/abs/2602.21054", "authors": ["Seongheon Park", "Changdae Oh", "Hyeong Kyu Choi", "Xuefeng Du", "Sharon Li"], "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.", "AI": {"tldr": "本文提出了VAUQ，一种针对LVLM自我评估的视觉感知不确定性量化框架。", "motivation": "现有的LLM自我评估方法主要依赖语言先验，并不适合用于评估基于视觉预测的情况。因此，需要一种新的方法来解决这个问题。", "method": "VAUQ通过引入图像信息得分（IS）和核心区域掩码策略，量化模型输出对视觉证据的依赖程度，结合预测熵得到一个无需训练的评分函数。", "result": "实验结果表明，在多个数据集上，VAUQ比现有的自我评估方法表现更优。", "conclusion": "本文提出的VAUQ框架能有效地衡量LVLM在视觉条件下的不确定性，并提高模型自我评估的能力。"}}
{"id": "2602.21053", "pdf": "https://arxiv.org/pdf/2602.21053", "abs": "https://arxiv.org/abs/2602.21053", "authors": ["Shimin Wen", "Zeyu Zhang", "Xingdou Bian", "Hongjie Zhu", "Lulu He", "Layi Shama", "Daji Ergu", "Ying Cai"], "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.", "AI": {"tldr": "提出了一种新型迭代自我纠正框架OCR-Agent，该框架通过能力反思和记忆反思来增强大型视觉-语言模型的纠错能力和认知偏误修正。", "motivation": "解决现有大型视觉-语言模型缺乏有效的自我校正机制的问题，使得它们在多轮修订中难以独立纠正认知偏差并提高答案质量。", "method": "通过能力反射诊断错误并生成更正计划，然后利用记忆反思回顾过去的尝试以避免重复并探索新解决方案，最后通过严格重新推理优化答案。", "result": "实验显示OCR-Agent在OCRBench v2基准测试中超越了当前开源最佳模型InternVL3-8B，在英语和中文子集上分别提高了+2.0和+1.2，并且在视觉理解和推理方面达到新的最高性能，分别为79.9和66.5。", "conclusion": "该方法证明通过结构化、自我意识的反思可以显著增强大型视觉-语言模型的推理鲁棒性，而无需额外训练。"}}
{"id": "2602.21052", "pdf": "https://arxiv.org/pdf/2602.21052", "abs": "https://arxiv.org/abs/2602.21052", "authors": ["Timur Nabiev", "Evgeny Frolov"], "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.", "AI": {"tldr": "介绍了一种新的位置感知自注意力机制以提高推荐准确性", "motivation": "现有的基于加法的位置嵌入虽然可以注入序列顺序信息，但会导致位置信息与项目表示语义纠缠，并且在深层模型中传播较弱，限制了捕捉丰富序列模式的能力。因此提出一种分离的位置核自注意机制来解决这些问题。", "method": "引入了一种新的位置感知的内核化注意力机制，该机制通过一个可学习的位置内核操作完全独立于项目嵌入语义的位置空间，并直接调节注意力权重。这种方法可以在每个注意力块中应用，以实现适应性的多尺度序列建模。", "result": "在标准的下一项预测基准测试上，这种新的位置内核注意力方法与强大的竞争基线相比，性能有所提升。", "conclusion": "该论文提出了一种新型的位置感知自注意力模型，在推荐系统中有较好的表现。"}}
{"id": "2602.21045", "pdf": "https://arxiv.org/pdf/2602.21045", "abs": "https://arxiv.org/abs/2602.21045", "authors": ["Anna Martin-Boyle", "Cara A. C. Leckey", "Martha C. Brown", "Harmanpreet Kaur"], "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A", "categories": ["cs.HC", "cs.CL"], "comment": "25 pages, 3 figures. Accepted at the ACM CHI conference on Human Factors in Computing Systems 2026", "summary": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source documents into discrete claims and evidence, mapping them to reveal supported assertions, unsupported claims, and information omitted from the source texts. We evaluated PaperTrail in a within-subjects study with 26 researchers who performed two scholarly editing tasks using PaperTrail and a baseline interface. Our results show that PaperTrail significantly lowered participants' trust compared to the baseline. However, this increased caution did not translate to behavioral changes, as people continued to rely on LLM-generated scholarly edits to avoid a cognitively burdensome task. We discuss the value of claim-evidence matching for understanding LLM trustworthiness in scholarly settings, and present design implications for cognition-friendly communication of provenance information.", "AI": {"tldr": "PaperTrail是一款用于学术问答系统的接口，通过分解大型语言模型的回答和来源文档中的断言和支持证据，提高信息准确性和可追溯性。", "motivation": "现有学术问答系统中存在细微错误问题，而传统引用机制不足以应对学术领域的严格验证需求。因此开发了一种新型接口来解决这些问题。", "method": "PaperTrail通过将LLM回答和文献资料分解为独立的断言和支持证据，并进行映射以显示支持的、未支持的声明以及信息遗漏情况。", "result": "在26名研究人员参与的研究中，使用PaperTrail比基线界面显著降低了参与者对系统的信任度，但这种谨慎并未转化为行为上的改变，人们仍然依赖于LLM生成的内容来避免认知负担。", "conclusion": "论文讨论了断言-证据匹配的价值，并提出设计启示以实现友好交互式的出处信息传递。"}}
{"id": "2602.21044", "pdf": "https://arxiv.org/pdf/2602.21044", "abs": "https://arxiv.org/abs/2602.21044", "authors": ["Yanrui Wu", "Lingling Zhang", "Xinyu Zhang", "Jiayu Chang", "Pengyu Li", "Xu Jiang", "Jingtao Hu", "Jun Liu"], "title": "LogicGraph : Benchmarking Multi-Path Logical Reasoning via Neuro-Symbolic Generation and Verification", "categories": ["cs.AI"], "comment": "24 pages, 17 figures", "summary": "Evaluations of large language models (LLMs) primarily emphasize convergent logical reasoning, where success is defined by producing a single correct proof. However, many real-world reasoning problems admit multiple valid derivations, requiring models to explore diverse logical paths rather than committing to one route. To address this limitation, we introduce LogicGraph, the first benchmark aimed to systematically evaluate multi-path logical reasoning, constructed via a neuro-symbolic framework that leverages backward logic generation and semantic instantiation. This pipeline yields solver-verified reasoning problems formalized by high-depth multi-path reasoning and inherent logical distractions, where each instance is associated with an exhaustive set of minimal proofs. We further propose a reference-free evaluation framework to rigorously assess model performance in both convergent and divergent regimes. Experiments on state-of-the-art language models reveal a common limitation: models tend to commit early to a single route and fail to explore alternatives, and the coverage gap grows substantially with reasoning depth. LogicGraph exposes this divergence gap and provides actionable insights to motivate future improvements. Our code and data will be released at https://github.com/kkkkarry/LogicGraph.", "AI": {"tldr": "本文提出了LogicGraph，这是一个评估大规模语言模型多路径逻辑推理能力的基准。", "motivation": "当前对大型语言模型（LLMs）的评价主要集中在单一正确证明的收敛性逻辑推理上。然而，在许多现实世界的问题中，存在多种有效的推导方式，需要模型探索多样化的逻辑途径而非局限于一条路线。", "method": "通过神经符号框架构建LogicGraph，该框架利用反向逻辑生成和语义实例化来创建经验证者核实的多路径推理问题，并提供详尽的最小证明集合。同时提出了一个无参考基准的评估框架，用于严谨地评价模型在收敛性和发散性方面的表现。", "result": "实验显示，最先进的语言模型倾向于早期选择单一途径并忽视其他选项，在推理深度增加时覆盖率差距显著增大。", "conclusion": "LogicGraph揭示了这种差异，并提供了有关未来改进的具体见解。代码和数据将在https://github.com/kkkkarry/LogicGraph发布。"}}
{"id": "2602.21042", "pdf": "https://arxiv.org/pdf/2602.21042", "abs": "https://arxiv.org/abs/2602.21042", "authors": ["Bonan Liu", "Zeyu Zhang", "Bingbing Meng", "Han Wang", "Hanshuo Zhang", "Chengping Wang", "Daji Ergu", "Ying Cai"], "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages", "categories": ["cs.CV"], "comment": null, "summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.", "AI": {"tldr": "OmniOCR是用于少数民族语言的通用光学字符识别框架，通过动态低秩适应和稀疏正则化提高了模型在低资源或零样本设置中的性能。", "motivation": "目前大多数光字符识别方法主要集中在拉丁文和中文等资源丰富的脚本上，而少数民族语言由于复杂的书写系统、稀缺的数据标注以及多样化的历史和现代形式，在这些领域内的研究仍然较少。OmniOCR旨在解决这些问题，并提高模型在低资源或零样本设置中的适应能力。", "method": "OmniOCR引入了动态低秩适应（Dynamic LoRA）来分配模型的容量，使得模型可以在不同的层之间以及脚本之间有效迁移知识，同时保持原有的知识不变。通过稀疏正则化技术剪枝冗余更新，确保适应过程既紧凑又高效。", "result": "在四个数据集上进行评估时，OmniOCR比零样本基础模型和标准后训练方法表现出更高的准确率，并且相比当前最先进的基线模型，其精度提高了39%-66%。", "conclusion": "OmniOCR框架通过动态低秩适应和稀疏正则化技术有效提升了少数民族语言的光字符识别性能，展示了在低资源或零样本设置下的优越性。"}}
{"id": "2602.21035", "pdf": "https://arxiv.org/pdf/2602.21035", "abs": "https://arxiv.org/abs/2602.21035", "authors": ["Junhao Xiao", "Zhiyu Wu", "Hao Lin", "Yi Chen", "Yahui Liu", "Xiaoran Zhao", "Zixu Wang", "Zejiang He"], "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.", "AI": {"tldr": "本文提出了一种名为CLIPGlasses的框架，用于增强CLIP模型在理解否定语义方面的性能。", "motivation": "现有的方法通过微调CLIP的文字编码器来改进其对否定的理解，这种方法存在过拟合的风险。本文旨在提供一种无需微调的方法来提高CLIP模型处理否定描述的能力。", "method": "CLIPGlasses采用两阶段设计：Lens模块从文本嵌入中分离出否定语义，而Frame模块预测上下文感知的排斥强度，并将其整合到修改后的相似度计算中以惩罚与否定语义对齐的情况。", "result": "实验结果表明，使用CLIPGlasses增强的CLIP模型在域内性能上具有竞争力，在跨域泛化方面优于最先进的方法。特别是在资源受限的情况下表现更佳。", "conclusion": "研究证明了CLIPGlasses的有效性，并展示了其在理解和处理否定描述方面的优越性能及更强的跨领域鲁棒性。"}}
{"id": "2602.21033", "pdf": "https://arxiv.org/pdf/2602.21033", "abs": "https://arxiv.org/abs/2602.21033", "authors": ["Tianhao Fu", "Yucheng Chen"], "title": "MIP Candy: A Modular PyTorch Framework for Medical Image Processing", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.", "AI": {"tldr": "介绍了一种用于医学图像处理的模块化PyTorch框架MIP Candy，该框架允许用户通过实现单一方法即可获得完整工作流程。", "motivation": "现有软件要么提供低级组件需要大量集成工作，要么强制使用僵化的单体流水线难以修改。因此提出一种专门为医学图像处理设计的框架。", "method": "MIP Candy提供了数据加载、训练、推断和评估的整体模块化管道，引入了延迟配置机制LayerT，允许在运行时替换卷积、归一化和激活模块，同时提供内置功能如k折交叉验证等。", "result": "该框架是开源的，遵循Apache-2.0许可协议，并附有详细的文档和源代码。", "conclusion": "MIP Candy是一种灵活且强大的工具，能够满足医学图像处理中各种复杂需求。"}}
{"id": "2602.21028", "pdf": "https://arxiv.org/pdf/2602.21028", "abs": "https://arxiv.org/abs/2602.21028", "authors": ["Gayatri Indukumar", "Muhammad Awais", "Diana Cafiso", "Matteo Lo Preti", "Lucia Beccai"], "title": "Surface-based Manipulation Using Tunable Compliant Porous-Elastic Soft Sensing", "categories": ["cs.RO"], "comment": "6 pages, 6 figures, 1 table, to be published in RoboSoft 2026 proceedings", "summary": "There is a growing need for soft robotic platforms that perform gentle, precise handling of a wide variety of objects. Existing surface-based manipulation systems, however, lack the compliance and tactile feedback needed for delicate handling. This work introduces the COmpliant Porous-Elastic Soft Sensing (COPESS) integrated with inductive sensors for adaptive object manipulation and localised sensing. The design features a tunable lattice layer that simultaneously modulates mechanical compliance and sensing performance. By adjusting lattice geometry, both stiffness and sensor response can be tailored to handle objects with varying mechanical properties. Experiments demonstrate that by easily adjusting one parameter, the lattice density, from 7 % to 20 %, it is possible to significantly alter the sensitivity and operational force range (about -23x and 9x, respectively). This approach establishes a blueprint for creating adaptive, sensorized surfaces where mechanical and sensory properties are co-optimized, enabling passive, yet programmable, delicate manipulation.", "AI": {"tldr": "本文介绍了一种结合了电感传感器的可调弹性多孔软传感技术（COPESS），用于自适应对象操纵和局部感知。", "motivation": "现有的基于表面的操作系统缺乏柔性和触觉反馈，无法对各种物体进行精细处理。因此，需要一种既能提供柔软度又能提供良好触觉反馈的机器人平台。", "method": "本文设计了一种可调格子层技术，该技术可以同时调节机械柔顺性与传感性能。通过调整格子几何结构，可以定制刚度和传感器响应以适应具有不同力学特性的物体。", "result": "实验表明，通过简单地调整一个参数（格子密度），从7%到20%，可以显著改变灵敏度和操作力范围（约-23倍和9倍）。", "conclusion": "本文提出了一种适应性传感器化表面的设计方法，其中机械和感官属性是共同优化的，这为被动但可编程的精细操纵开辟了道路。"}}
{"id": "2602.21015", "pdf": "https://arxiv.org/pdf/2602.21015", "abs": "https://arxiv.org/abs/2602.21015", "authors": ["Yuhao Wu", "Maojia Song", "Yihuai Lan", "Lei Wang", "Zhiqiang Hu", "Yao Xiao", "Heng Zhou", "Weihua Zheng", "Dylan Raharja", "Soujanya Poria", "Roy Ka-Wei Lee"], "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "categories": ["cs.CV"], "comment": "Work in processing. Website: https://social-ai-studio.github.io/CHAIN/", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "AI": {"tldr": "介绍了一个评估视觉语言模型在理解物理结构、因果关系和执行行动序列能力的互动基准，名为CHAIN。", "motivation": "当前的视觉语言模型评价集中于单一回合的任务上，无法全面测试模型理解和执行基于物理约束的动作的能力。此研究旨在填补这一空白。", "method": "设计了一个交互式3D环境（CHAIN），用于评估模型是否能够理解、计划和执行结构化动作序列，并在此环境中对最先进的视觉语言模型进行了系统性研究。", "result": "实验结果表明，顶级模型在理解物理结构及因果关系上仍然面临挑战，难以制定可靠长期规划或将其转化为有效行动。", "conclusion": "通过CHAIN基准测试，揭示了现有模型在处理互动环境中的物理结构和因果约束方面的不足。"}}
{"id": "2602.21013", "pdf": "https://arxiv.org/pdf/2602.21013", "abs": "https://arxiv.org/abs/2602.21013", "authors": ["Sanjay Haresh", "Daniel Dijkman", "Apratim Bhattacharyya", "Roland Memisevic"], "title": "Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks", "categories": ["cs.RO"], "comment": "To appear at ICRA 2026", "summary": "Many dexterous manipulation tasks are non-markovian in nature, yet little attention has been paid to this fact in the recent upsurge of the vision-language-action (VLA) paradigm. Although they are successful in bringing internet-scale semantic understanding to robotics, existing VLAs are primarily \"stateless\" and struggle with memory-dependent long horizon tasks. In this work, we explore a way to impart both spatial and temporal memory to a VLA by incorporating a language scratchpad. The scratchpad makes it possible to memorize task-specific information, such as object positions, and it allows the model to keep track of a plan and progress towards subgoals within that plan. We evaluate this approach on a split of memory-dependent tasks from the ClevrSkills environment, on MemoryBench, as well as on a challenging real-world pick-and-place task. We show that incorporating a language scratchpad significantly improves generalization on these tasks for both non-recurrent and recurrent models.", "AI": {"tldr": "本文研究了一种通过在视觉语言动作（VLA）模型中加入语言笔记垫来提高其在需要记忆的任务中的表现的方法。", "motivation": "当前的VLA系统主要为无状态设计，难以处理依赖于长期记忆的操作任务。为了改善这一情况，作者提出使用一个语言笔记垫以增强模型的空间和时间记忆能力。", "method": "通过引入语言笔记垫，使模型能够记住特定任务的信息，并跟踪实现子目标的计划进展。", "result": "实验表明，在ClevrSkills环境中的部分依赖于记忆的任务、MemoryBench以及具有挑战性的现实世界拾取放置任务上，该方法显著提高了非递归和递归模型的表现。", "conclusion": "引入语言笔记垫能够有效改善视觉语言动作模型在需要长期记忆任务上的表现。"}}
{"id": "2602.21010", "pdf": "https://arxiv.org/pdf/2602.21010", "abs": "https://arxiv.org/abs/2602.21010", "authors": ["Jiannan Huang", "Aditya Kane", "Fengzhe Zhou", "Yunchao Wei", "Humphrey Shi"], "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design", "categories": ["cs.CV"], "comment": "CVPR Findings", "summary": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\\textbf{L}ow-cost and \\textbf{E}fficient \\textbf{DE}tection \\textbf{TR}ansformer), which achieves a new \\textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \\textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \\textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \\textbf{+0.6/-0.1 mAP} while achieving similar speed and \\textbf{+20\\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \\textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \\textbf{+0.4 mAP} with only \\textbf{0.4 ms} additional latency. Code and weights will be open-sourced.", "AI": {"tldr": "本文提出了一种名为Le-DETR的实时检测Transformer，旨在通过低预训练成本实现高性能。", "motivation": "当前的实时DETR模型由于过度依赖于骨干网络的大量预训练开销，在探索新型骨干架构方面受限。研究者希望展示一种通用的良好设计方法，能够在减少预训练成本的同时保持高水平性能。", "method": "作者通过深入研究骨干网络结构提出了EfficientNAT，并重新设计了混合编码器以提高性能和推理速度，最终构建出Le-DETR模型。", "result": "实验表明，Le-DETR在COCO Val2017上分别实现了52.9/54.3/55.1 mAP，同时仅需4.45/5.01/6.68 ms。相比其他方法，它表现出更强的性能和更快的速度。", "conclusion": "研究表明，通过精心设计，实时DETR模型可以在无需复杂且计算昂贵的预训练的情况下实现强大性能。"}}
{"id": "2602.20999", "pdf": "https://arxiv.org/pdf/2602.20999", "abs": "https://arxiv.org/abs/2602.20999", "authors": ["Bowen Zheng", "Yongli Xiang", "Ziming Hong", "Zerong Lin", "Chaojian Yu", "Tongliang Liu", "Xinge You"], "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models", "categories": ["cs.CV"], "comment": "Project page: https://Zbwwwwwwww.github.io/VII", "summary": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.", "AI": {"tldr": "本文提出了视觉指令注入（VII）框架，通过在安全参考图像中植入恶意意图来攻击图像到视频生成模型。", "motivation": "由于图像到视频生成模型能够根据参考图像进行视频生成，并且其中的某些视觉提示可以作为隐式控制信号，因此可能存在安全隐患：对手可以通过视觉指令注入恶意意图。这项工作旨在揭示这一风险并通过训练无自由和可转移框架VII来缓解。", "method": "该方法包括两个模块：恶意意图重编程模块，用于从不安全的文字提示中提取恶意意图；以及视觉指令定位模块，将提取出的恶意意图映射到一个安全输入图像上，并生成与原始文字一致但保留语义连贯性的视觉指令。", "result": "实验结果显示VII框架在四个最先进的商业I2V模型上的攻击成功率高达83.5%，同时拒绝率几乎为零，显著优于现有基线方法。", "conclusion": "通过视觉指令注入技术揭示了图像到视频生成模型的潜在安全风险，并提出了一种有效的训练无自由和可转移框架来解决这一问题。"}}
{"id": "2602.20994", "pdf": "https://arxiv.org/pdf/2602.20994", "abs": "https://arxiv.org/abs/2602.20994", "authors": ["Yubin Ge", "Yongsong Huang", "Xiaofeng Liu"], "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures", "categories": ["eess.IV", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.", "AI": {"tldr": "本文提出了一种多模式MRI报告监督的脑肿瘤分割方法，该方法能够利用报告中的全局和模态特定的信息来改善分割准确性。", "motivation": "现有的基于报告的学习方法在处理包含多种参数扫描的脑肿瘤数据时存在局限性，它们无法充分利用多层次的报告信息或不同的病变类型先验知识。", "method": "提出了一种新的不确定性感知统一形式化方法(MS-RSuper)，该方法通过引入模态特定线索与亚结构对齐的存在和缺失损失、部分数量约束的一边界的下限以及额外解剖学优先事项来提高分割准确性。", "result": "在1238个带有报告标签的BraTS-MET/MEN扫描中，所提出的MS-RSuper方法显著优于稀疏监督基线和简单的RSuper方法。", "conclusion": "新的多模态MRI报告监督的方法能够更好地利用多层次的信息，并提高脑肿瘤分割精度。"}}
{"id": "2602.20991", "pdf": "https://arxiv.org/pdf/2602.20991", "abs": "https://arxiv.org/abs/2602.20991", "authors": ["Robin A. Heinonen"], "title": "Asymptotics of solutions to the linear search problem", "categories": ["math.OC", "cs.DS", "math.PR"], "comment": null, "summary": "The exact leading asymptotics of solutions to the symmetric linear search problem are obtained for any positive probability density on the real line with a monotonic, sufficiently regular tail. A similar result holds for densities on a compact interval.", "AI": {"tldr": "研究了实线和有限区间上对称线性搜索问题解的精确渐近行为", "motivation": "探讨概率密度函数尾部性质对线性搜索问题解的影响，以获得更准确的理论分析结果", "method": "通过数学推导得到具有单调、足够光滑尾部的概率密度函数在对称线性搜索中的渐进解形式", "result": "得到了任意正概率密度函数实线上和有限区间上对称线性搜索问题解的确切主导渐近行为", "conclusion": "该研究为理解不同概率分布下的线性搜索提供理论依据"}}
{"id": "2602.20989", "pdf": "https://arxiv.org/pdf/2602.20989", "abs": "https://arxiv.org/abs/2602.20989", "authors": ["Zheng Gu", "Min Lu", "Zhida Sun", "Dani Lischinski", "Daniel Cohen-O", "Hui Huang"], "title": "Cycle-Consistent Tuning for Layered Image Decomposition", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026. Project page: https://vcc.tech/research/2026/ImgDecom", "summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.", "AI": {"tldr": "本文提出了一种基于大尺度扩散基础模型的图像分层分解框架，通过引入循环一致调优策略和逐步自我改进过程来提高分解准确性。", "motivation": "视觉图层在现实世界中的分离是一个持久挑战，因为这些图层经常涉及到非线性和全局耦合交互。本文旨在解决标志与物体表面之间复杂关系的图像分层问题，以期实现更准确、连贯且通用性强的分解方法。", "method": "本研究使用预训练扩散模型并结合轻量级LoRA自适应进行微调，并引入循环一致优化策略来联合训练分解和合成模型。此外还采用逐步自我改进过程迭代增强训练集，以提高性能。", "result": "实验结果表明该方法实现了准确且连贯的图像分层，并在其他类型的分层任务中表现出良好的泛化能力。", "conclusion": "本文提出的循环一致调优策略和逐步自我改进过程有效提高了图像分层分解的质量，展现了其作为统一框架的应用潜力。"}}
{"id": "2602.20985", "pdf": "https://arxiv.org/pdf/2602.20985", "abs": "https://arxiv.org/abs/2602.20985", "authors": ["Munish Monga", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2026", "summary": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.", "AI": {"tldr": "本文提出了一种新的框架EW-DETR，用于在不断变化的环境中进行物体检测，能够在无需访问先前数据的情况下识别新出现和未知对象。", "motivation": "现实世界中的目标检测需要应对新兴类别、领域转移以及未见过的对象作为“未知”进行处理的问题，同时要在不使用示例样本的前提下完成增量学习任务。", "method": "通过在DETR基础模型上增加三个协同模块：Incremental LoRA Adapters用于无示例的增量学习；Query-Norm Objectness Adapter从解码器查询中分离出对象感知识别特征；Entropy-Aware Unknown Mixing进行校准未知检测。", "result": "实验表明，EW-DETR在Pascal Series和Diverse Weather基准测试上优于其他方法，FOGS得分提高了57.24%。", "conclusion": "新框架能够有效应对不断变化的环境中的物体检测任务，并为评估性能引入了一个新的综合指标FOGS。"}}
{"id": "2602.20981", "pdf": "https://arxiv.org/pdf/2602.20981", "abs": "https://arxiv.org/abs/2602.20981", "authors": ["Christian Simon", "MAsato Ishii", "Wei-Yao Wang", "Koichi Saito", "Akio Hayakawa", "Dongseok Shim", "Zhi Zhong", "Shuyang Cui", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2026", "summary": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.", "AI": {"tldr": "该论文探讨了视频到音频生成模型在面对长时间段时的泛化能力提升问题。", "motivation": "多模态对齐挑战，特别是在有限数据和文本描述与帧级视频信息不匹配的情况下，使得训练短实例并应用于长测试的任务变得困难。通过开发MMHNet，论文旨在解决这些问题，并展示模型在无长时间训练下的泛化能力。", "method": "采用了一种改进的多模态分层网络（MMHNet），结合了非因果Mamba方法以支持长时间音频生成。", "result": "实验表明，在长视频到音频任务中，所提出的方法比先前的工作表现出色，并且能够产生超过五分钟的音频，而以前的方法在此时长上表现不足。", "conclusion": "论文通过改进网络架构和引入非因果Mamba方法，成功提升了视频到音频生成模型在长时间段上的性能。"}}
{"id": "2602.20980", "pdf": "https://arxiv.org/pdf/2602.20980", "abs": "https://arxiv.org/abs/2602.20980", "authors": ["Yang Zhang", "Danyang Li", "Yuxuan Li", "Xin Zhang", "Tianyu Xie", "Mingming Cheng", "Xiang Li"], "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.", "AI": {"tldr": "提出了一种名为CrystaL的单阶段框架，用于在多模态大型语言模型中自发生成视觉潜变量。", "motivation": "现有的基于链式思维（CoT）的方法通过预定义的监督信号来指导保留关键视觉信息，但这些方法提供的指引有限。因此提出了新的方法以更好地保持中间潜在状态中的关键视觉信息。", "method": "CrystaL框架包括两个路径分别处理完整和被破坏的图像，通过显式对齐注意力模式和预测分布来结晶化潜变量表示为任务相关的视觉语义。", "result": "实验显示CrystaL在感知密集型基准测试中优于现有最佳基线模型，在细粒度视觉理解方面取得了显著优势，并保持了强大的推理能力。", "conclusion": "CrystaL方法证明能够在多模态大型语言模型中自发生成高质量的视觉潜变量，从而改进视觉理解和推理任务。"}}
{"id": "2602.20979", "pdf": "https://arxiv.org/pdf/2602.20979", "abs": "https://arxiv.org/abs/2602.20979", "authors": ["Mark Marron"], "title": "Toward an Agentic Infused Software Ecosystem", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.", "AI": {"tldr": "论文提出了一种包含AI代理的软件生态系统，旨在利用人工智能在软件开发中的潜力。", "motivation": "充分利用人工智能在软件开发中的能力需要重新思考整个软件生态系统。当前的人工智能技术已经从简单的代码完成发展到独立执行复杂任务，并且这种趋势将继续下去。", "method": "论文提出了一个由三个支柱组成的AI融合的软件生态系统：AI代理本身、编程语言和API以及运行时环境，强调这三个方面必须协同进步以实现这个愿景。", "result": "通过阐述这一综合系统的设计理念，论文为未来的人工智能技术发展提供了方向，并探讨了如何使人类开发人员与这些技术更好地协作。", "conclusion": "该论文认为要实现一个AI融合的软件生态系统，需要从多个方面进行协同创新，以支持当前和未来的AI代理以及它们与人类开发者之间的合作。"}}
{"id": "2602.20972", "pdf": "https://arxiv.org/pdf/2602.20972", "abs": "https://arxiv.org/abs/2602.20972", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Zhiqiang Kou", "Zhongnian Li", "Gang Niu", "Masashi Sugiyama"], "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?", "categories": ["cs.CV"], "comment": null, "summary": "Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\\% to 80\\% of human performance, while achieving over 90\\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\\% to 80\\% of the difference.", "AI": {"tldr": "本文研究了多模态大型语言模型在图像标注任务中的应用，提出了一个名为TagLLM的新框架来缩小机器生成标签与人工标签之间的差距。", "motivation": "传统上，图像标注依赖于大量的人工标注数据集，这耗费了大量的时间和成本。使用多模态大模型自动化标注虽然具有潜力，但其能力尚待探索。因此本文旨在分析这种替代方案的有效性，并提出解决方案以实现自动标注取代人工标注。", "method": "研究团队提出了一个名为TagLLM的新框架：该框架包含候选生成和标签消除两个部分；前者使用分组提示法高效地产生涵盖尽可能多的真实标签的紧凑型候选集，后者则通过交互式校准提示中的语义概念以有效精炼候选标签。", "result": "实验结果表明，TagLLM能够显著缩小机器生成标签与人工标签之间的差距，在下游训练性能中接近了约60%至80%的人工标注水平。", "conclusion": "多模态大型语言模型在图像标注任务中的潜力已被验证，并通过提出的TagLLM框架得到了进一步提高。未来研究可以在此基础上优化系统，以减少成本并提高效率。"}}
{"id": "2602.20971", "pdf": "https://arxiv.org/pdf/2602.20971", "abs": "https://arxiv.org/abs/2602.20971", "authors": ["Himadri Mandal", "Vishnu Varadarajan", "Jaee Ponde", "Aritra Das", "Mihir More", "Debayan Gupta"], "title": "Does Order Matter : Connecting The Law of Robustness to Robust Generalization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Bubeck and Sellke (2021) pose as an open problem the connection between the law of robustness and robust generalization. The law of robustness states that overparameterization is necessary for models to interpolate robustly; in particular, robust interpolation requires the learned function to be Lipschitz. Robust generalization asks whether small robust training loss implies small robust test loss. We resolve this problem by explicitly connecting the two for arbitrary data distributions. Specifically, we introduce a nontrivial notion of robust generalization error and convert it into a lower bound on the expected Rademacher complexity of the induced robust loss class. Our bounds recover the $Ω(n^{1/d})$ regime of Wu et al.\\ (2023) and show that, up to constants, robust generalization does not change the order of the Lipschitz constant required for smooth interpolation. We conduct experiments to probe the predicted scaling with dataset size and model capacity, testing whether empirical behavior aligns more closely with the predictions of Bubeck and Sellke (2021) or Wu et al.\\ (2023). For MNIST, we find that the lower-bound Lipschitz constant scales on the order predicted by Wu et al.\\ (2023). Informally, to obtain low robust generalization error, the Lipschitz constant must lie in a range that we bound, and the allowable perturbation radius is linked to the Lipschitz scale.", "AI": {"tldr": "研究论文探讨了鲁棒泛化的连接，提出了一种非平凡的鲁棒泛化误差概念，并将其实转换为诱导鲁棒损失类的Rademacher复杂度下界。", "motivation": "Bubeck和Sellke提出了一个开放问题：如何连接鲁棒性法则与鲁棒泛化。本研究试图通过引入新的鲁棒泛化误差定义来解决这个问题，进一步探讨了数据分布对两者的影响。", "method": "本文引入了一种非平凡的鲁棒泛化误差概念，并将其转换为诱导鲁棒损失类的Rademacher复杂度下界。实验验证了理论预测与实际行为之间的关系，具体测试了MNIST数据集上的表现情况。", "result": "研究发现，为了获得低鲁棒泛化错误，Lipschitz常数必须处于一个特定范围内，且允许扰动半径与Lipschitz尺度相链接。实验结果表明，在MNIST上预测的误差下界符合Wu等人的理论预测（2023）。", "conclusion": "论文证明了鲁棒泛化的关键在于数据集大小和模型能力之间的关系，并通过实验证明了理论预测与实际行为之间的一致性。"}}
{"id": "2602.20967", "pdf": "https://arxiv.org/pdf/2602.20967", "abs": "https://arxiv.org/abs/2602.20967", "authors": ["Haoyang Li", "Changsong Liu", "Wei Rao", "Hao Shi", "Sakriani Sakti", "Eng Siong Chng"], "title": "Training-Free Intelligibility-Guided Observation Addition for Noisy ASR", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) degrades severely in noisy environments. Although speech enhancement (SE) front-ends effectively suppress background noise, they often introduce artifacts that harm recognition. Observation addition (OA) addressed this issue by fusing noisy and SE enhanced speech, improving recognition without modifying the parameters of the SE or ASR models. This paper proposes an intelligibility-guided OA method, where fusion weights are derived from intelligibility estimates obtained directly from the backend ASR. Unlike prior OA methods based on trained neural predictors, the proposed method is training-free, reducing complexity and enhances generalization. Extensive experiments across diverse SE-ASR combinations and datasets demonstrate strong robustness and improvements over existing OA baselines. Additional analyses of intelligibility-guided switching-based alternatives and frame versus utterance-level OA further validate the proposed design.", "AI": {"tldr": "该论文提出了一种基于后端ASR的可理解性指导的观察添加方法，以提高嘈杂环境下的自动语音识别性能。", "motivation": "在噪声环境下，虽然语音增强前端可以抑制背景噪音，但也可能引入对识别有害的伪影。现有的观测添加方法通过融合噪音和增强后的语音来改进识别效果，而无需修改SE或ASR模型参数。然而，这些方法通常基于训练神经预测器。", "method": "该论文提出了一种训练自由的方法，其中融合权重直接从后端ASR中获得的可理解性估计得出。这种方法减少了复杂度并增强了泛化能力。", "result": "通过在不同的SE-ASR组合和数据集上进行广泛的实验，证明了所提方法具有很强的鲁棒性和优于现有基线的方法性能。", "conclusion": "该论文展示了基于可理解性的指导观测添加方法的有效性，并验证了帧级与句子级观测添加的不同策略。"}}
{"id": "2602.20963", "pdf": "https://arxiv.org/pdf/2602.20963", "abs": "https://arxiv.org/abs/2602.20963", "authors": ["Ang", "Li", "Alexander Yin", "Alexander White", "Sahib Sandhu", "Matthew Francoeur", "Victor Jimenez-Santiago", "Van Remenar", "Codrin Tugui", "Mihai Duduta"], "title": "A Robotic Testing Platform for Pipelined Discovery of Resilient Soft Actuators", "categories": ["cs.RO"], "comment": null, "summary": "Short lifetime under high electrical fields hinders the widespread robotic application of linear dielectric elastomer actuators (DEAs). Systematic scanning is difficult due to time-consuming per-sample testing and the high-dimensional parameter space affecting performance. To address this, we propose an optimization pipeline enabled by a novel testing robot capable of scanning DEA lifetime. The robot integrates electro-mechanical property measurement, programmable voltage input, and multi-channel testing capacity. Using it, we scanned the lifetime of Elastosil-based linear actuators across parameters including input voltage magnitude, frequency, electrode material concentration, and electrical connection filler. The optimal parameter combinations improved operational lifetime under boundary operating conditions by up to 100% and were subsequently scaled up to achieve higher force and displacement output. The final product demonstrated resilience on a modular, scalable quadruped walking robot with payload carrying capacity (>100% of its untethered body weight, and >700% of combined actuator weight). This work is the first to introduce a self-driving lab approach into robotic actuator design.", "AI": {"tldr": "提出了一种基于测试机器人的优化管道，用于扫描线性介电弹性体执行器（DEA）的寿命。", "motivation": "短寿命和高电气场下的广泛应用受限于DEAs，系统性的扫描由于每次样本测试耗时长以及参数空间维度高而困难。为了应对这一挑战，本研究提出了一种新的优化管道。", "method": "机器人集成了电-机械属性测量、可编程电压输入及多通道测试能力。使用该平台对基于Elastosil的线性执行器进行寿命扫描，并针对包括输入电压大小、频率、电极材料浓度和电气连接填充等参数进行了测试。", "result": "最优参数组合在边界操作条件下将运行寿命提高了多达100%，并随后扩展以实现更大的力和位移输出。最终产品在一个模块化可扩展的四足行走机器人上展示出强大的承载能力，超过其无束缚体重的100%以上，并且超过了联合执行器重量的700%。", "conclusion": "这项工作首次将自动驾驶实验室方法引入到机器人执行器设计中"}}
{"id": "2602.20958", "pdf": "https://arxiv.org/pdf/2602.20958", "abs": "https://arxiv.org/abs/2602.20958", "authors": ["Luka Šiktar", "Branimir Ćaran", "Bojan Šekoranja", "Marko Švaco"], "title": "EKF-Based Depth Camera and Deep Learning Fusion for UAV-Person Distance Estimation and Following in SAR Operations", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Search and rescue (SAR) operations require rapid responses to save lives or property. Unmanned Aerial Vehicles (UAVs) equipped with vision-based systems support these missions through prior terrain investigation or real-time assistance during the mission itself. Vision-based UAV frameworks aid human search tasks by detecting and recognizing specific individuals, then tracking and following them while maintaining a safe distance. A key safety requirement for UAV following is the accurate estimation of the distance between camera and target object under real-world conditions, achieved by fusing multiple image modalities. UAVs with deep learning-based vision systems offer a new approach to the planning and execution of SAR operations. As part of the system for automatic people detection and face recognition using deep learning, in this paper we present the fusion of depth camera measurements and monocular camera-to-body distance estimation for robust tracking and following. Deep learning-based filtering of depth camera data and estimation of camera-to-body distance from a monocular camera are achieved with YOLO-pose, enabling real-time fusion of depth information using the Extended Kalman Filter (EKF) algorithm. The proposed subsystem, designed for use in drones, estimates and measures the distance between the depth camera and the human body keypoints, to maintain the safe distance between the drone and the human target. Our system provides an accurate estimated distance, which has been validated against motion capture ground truth data. The system has been tested in real time indoors, where it reduces the average errors, root mean square error (RMSE) and standard deviations of distance estimation up to 15,3\\% in three tested scenarios.", "AI": {"tldr": "本文提出了一种基于深度相机和单目相机的融合系统，用于无人机在搜索救援操作中对人员进行准确的距离估计与跟踪。", "motivation": "为了提高无人航空器(UAV)在搜索与救援(SAR)任务中的效率及安全性，需要开发一种能够实时、精确地估算目标人体与无人机之间距离的技术方案。通过结合深度相机和单目相机的数据，并利用深度学习方法进行数据过滤和距离估计。", "method": "该系统使用YOLO-pose技术从单目摄像机中提取人体关键点信息；同时，对来自深度相机的原始测量值应用基于深度学习的滤波器处理。然后采用扩展卡尔曼滤波(EKF)算法将这些经过预处理的数据融合起来，从而实现实时、准确的距离估计。", "result": "实验结果表明，在三种测试场景下，该系统能够显著降低距离估计误差，平均RMSE和标准偏差分别减少了15.3%。", "conclusion": "研究提出了一种新的基于深度相机与单目相机数据融合的无人机人员跟踪及保持安全距离的方法，并通过室内真实环境验证了其有效性。"}}
{"id": "2602.20951", "pdf": "https://arxiv.org/pdf/2602.20951", "abs": "https://arxiv.org/abs/2602.20951", "authors": ["Jaehyun Park", "Minyoung Ahn", "Minkyu Kim", "Jonghyun Lee", "Jae-Gil Lee", "Dongmin Park"], "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.", "AI": {"tldr": "提出了一种名为ArtiAgent的方法，用于自动生成包含视觉瑕疵的图像数据集。", "motivation": "当前人工智能生成的图像中仍存在视觉瑕疵问题，依赖人工标注的数据集难以大规模应用。因此需要一种自动化的解决方案来创建带瑕疵标签的数据集。", "method": "通过三种代理实现：感知代理识别并定位真实图片中的实体及子实体；合成代理利用新的嵌入式补丁操作引入瑕疵；筛选代理过滤合成的图像，并为每种情况提供局部和全局解释。该方法能够高效地生成包含10万张带有丰富标签的图像。", "result": "使用ArtiAgent成功合成了10万张带注释的图像，展示了其在不同应用中的有效性和多功能性。", "conclusion": "通过自动化的数据合成技术，可以有效地缓解视觉瑕疵问题，并为不同的应用场景提供高质量的数据集。"}}
{"id": "2602.20949", "pdf": "https://arxiv.org/pdf/2602.20949", "abs": "https://arxiv.org/abs/2602.20949", "authors": ["Vinicius Tikara Venturi Date", "Leandro Miranda Zatesko"], "title": "A $2$-branching construction for the $χ\\leq 2r$ bound", "categories": ["cs.DS"], "comment": "12 pages, 4 tables, 1 figure. Submitted to CPM 2026", "summary": "The string repetitiveness measures $χ$ (the size of a smallest suffixient set of a string) and $r$ (the number of runs in the Burrows--Wheeler Transform) are related. Recently, we have shown that the bound $χ\\leq 2r$, proved by Navarro et al., is asymptotically tight as the size $σ$ of the alphabet increases, but achieving near-tight ratios for fixed $σ> 2$ remained open. We introduce a \\emph{2-branching property}: a cyclic string is 2-branching at order~$k$ if every $(k{-}1)$-length substring admits exactly two $k$-length extensions. We show that 2-branching strings of order~$k$ yield closed-form ratios $χ/r = (2σ^{k-1}+1)/(σ^{k-1}+4)$. For order~$3$, we give an explicit construction for every $σ\\geq 2$, narrowing the gap to~$2$ from $O(1/σ)$ to $O(1/σ^2)$. For $σ\\in \\{3,4\\}$, we additionally present order-$5$ instances with ratios exceeding~$1.91$.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20947", "pdf": "https://arxiv.org/pdf/2602.20947", "abs": "https://arxiv.org/abs/2602.20947", "authors": ["Thorbjørn Mosekjær Iversen", "Zebin Duan", "Frederik Hagelskjær"], "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.", "AI": {"tldr": "提出了一种新的基于核的方法用于估计二分类中的置信界限。", "motivation": "在关键操作中应用二元分类器需要可靠的置信边界，以确保系统的性能达到给定的统计显著性水平。", "method": "提出了威尔逊分数核密度分类法，通过使用条件变化的成功概率来估计Binomial实验中的置信界线。", "result": "该方法与高斯过程分类具有相似的性能，但计算复杂度更低。", "conclusion": "新方法在四个不同的数据集上展示了其作为任何特征提取器的分类头的有效性。"}}
{"id": "2602.20946", "pdf": "https://arxiv.org/pdf/2602.20946", "abs": "https://arxiv.org/abs/2602.20946", "authors": ["Christian Catalini", "Xiang Hui", "Jane Wu"], "title": "Some Simple Economics of AGI", "categories": ["econ.GN", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": "JEL Classification: D82, D83, J23, J24, L23, O33. 112 pages, 3 figures", "summary": "For millennia, human cognition was the primary engine of progress on Earth. As AI decouples cognition from biology, the marginal cost of measurable execution falls to zero, absorbing any labor capturable by metrics--including creative, analytical, and innovative work. The binding constraint on growth is no longer intelligence but human verification bandwidth: the capacity to validate, audit, and underwrite responsibility when execution is abundant. We model the AGI transition as the collision of two racing cost curves: an exponentially decaying Cost to Automate and a biologically bottlenecked Cost to Verify. This structural asymmetry widens a Measurability Gap between what agents can execute and what humans can afford to verify. It also drives a shift from skill-biased to measurability-biased technical change. Rents migrate to verification-grade ground truth, cryptographic provenance, and liability underwriting--the ability to insure outcomes rather than merely generate them. The current human-in-the-loop equilibrium is unstable: eroded from below as apprenticeship collapses (Missing Junior Loop) and from within as experts codify their obsolescence (Codifier's Curse). Unverified deployment becomes privately rational--a Trojan Horse externality. Unmanaged, these forces pull toward a Hollow Economy. Yet by scaling verification alongside agentic capabilities, the forces that threaten collapse become the catalyst for unbounded discovery and experimentation--an Augmented Economy. We derive a practical playbook for individuals, companies, investors, and policymakers. Today's defining challenge is not the race to deploy the most autonomous systems; it is the race to secure the foundations of their oversight. Only by scaling our bandwidth for verification alongside our capacity for execution can we ensure that the intelligence we have summoned preserves the humanity that initiated it.", "AI": {"tldr": "本文探讨了人工智能特别是通用人工智能（AGI）对经济结构的影响，分析了自动化成本下降和验证成本瓶颈之间的冲突。", "motivation": "通过理解智能机器的兴起如何重塑经济增长的基本动力，探索未来经济的发展路径。", "method": "建立模型分析自动化与验证成本的变化趋势及其影响。", "result": "提出了从技能偏向的技术变革转向可测量性偏向的技术变革，并强调了验证能力的重要性。", "conclusion": "只有同时提高执行能力和验证带宽，才能确保人工智能的智能保持人类的价值观和伦理标准。"}}
{"id": "2602.20945", "pdf": "https://arxiv.org/pdf/2602.20945", "abs": "https://arxiv.org/abs/2602.20945", "authors": ["Taiqiang Wu", "Zenan Zu", "Bo Zhou", "Ngai Wong"], "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "Tech Report, Insights on Efficient Reasoning via Reward Shaping", "summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.", "AI": {"tldr": "研究通过系统分析，探索了大规模语言模型中的高效推理机制。", "motivation": "为了减少大型语言模型中链式思考（CoT）推理的计算负担，该论文旨在找到短而准确的思考轨迹，并提出更细致的评估标准来衡量这一过程。", "method": "通过广泛的实验研究训练进程、解构训练提示和滚动策略、奖励塑造以及优化策略。特别关注在相对简单的提示上进行训练以确保积极奖赏信号密度，避免长度崩溃。", "result": "揭示了大型语言模型的高效推理遵循两阶段模式：长度适应和推理精炼，并验证了该方法在Qwen3系列模型上的鲁棒性和可扩展性。", "conclusion": "提出并验证了一套针对大规模语言模型高效推理的有效策略，展示了其泛化能力和实际应用价值。"}}
{"id": "2602.20943", "pdf": "https://arxiv.org/pdf/2602.20943", "abs": "https://arxiv.org/abs/2602.20943", "authors": ["Kaiyuan Tan", "Yingying Shen", "Mingfei Tu", "Haohui Zhu", "Bing Wang", "Guang Chen", "Hangjun Ye", "Haiyang Sun"], "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.", "AI": {"tldr": "本文提出了一种新的递归范式UFO，结合优化方法和前馈方法的优点，用于高效的长序列4D重建。", "motivation": "现有的前馈方法在三维重建方面表现出色，但在处理长时间的驾驶序列时面临二次复杂度挑战，并且难以准确建模动态对象。因此本文提出了一种新的递归范式UFO来解决这些问题。", "method": "UFO通过保持一个迭代优化的4D场景表示，并利用基于可见性的过滤机制选择关键的场景标记，从而能够高效处理长序列。对于动态物体，引入了导向建模方法以支持准确捕捉长时间运动。", "result": "实验结果表明，在Waymo开放数据集上，UFO显著优于单帧优化和现有的前馈方法，并能够在0.5秒内重建16秒的驾驶日志同时保持视觉质量和几何精度。", "conclusion": "UFO为长序列动态场景建模提供了一种新的高效解决方案，结合了优化方法和前馈方法的优势。"}}
{"id": "2602.20934", "pdf": "https://arxiv.org/pdf/2602.20934", "abs": "https://arxiv.org/abs/2602.20934", "authors": ["ChengYou Li", "XiaoDong Liu", "XiangBao Meng", "XinYu Zhao"], "title": "Architecting AgentOS: From Token-Level Context to Emergent System-Level Intelligence", "categories": ["cs.AI"], "comment": "16 pages,9 figures", "summary": "The paradigm of Large Language Models is undergoing a fundamental transition from static inference engines to dynamic autonomous cognitive systems.While current research primarily focuses on scaling context windows or optimizing prompt engineering the theoretical bridge between micro scale token processing and macro scale systemic intelligence remains fragmented.This paper proposes AgentOS,a holistic conceptual framework that redefines the LLM as a \"Reasoning Kernel\" governed by structured operating system logic.Central to this architecture is Deep Context Management which conceptualizes the context window as an Addressable Semantic Space rather than a passive buffer.We systematically deconstruct the transition from discrete sequences to coherent cognitive states introducing mechanisms for Semantic Slicing and Temporal Alignment to mitigate cognitive drift in multi-agent orchestration.By mapping classical OS abstractions such as memory paging interrupt handling and process scheduling onto LLM native constructs, this review provides a rigorous roadmap for architecting resilient scalable and self-evolving cognitive environments.Our analysis asserts that the next frontier of AGI development lies in the architectural efficiency of system-level coordination.", "AI": {"tldr": "提出AgentOS架构，将大型语言模型视为'推理内核'，通过结构化操作系统逻辑实现从微尺度令牌处理到宏观系统级智能的理论桥梁。", "motivation": "目前的研究主要集中在扩大上下文窗口或优化提示工程上，而缺乏对微尺度令牌处理和宏观系统级智能之间联系的深入理解。本文旨在填补这一空白，提出AgentOS架构以构建具有高度自我协调能力的认知环境。", "method": "通过将经典的操作系统抽象如内存分页、中断处理和进程调度映射到大型语言模型的原生结构中，设计出一个能够管理和控制深层上下文空间的方法框架，并引入语义切片和时间对齐机制以解决认知漂移问题。", "result": "为构建具有高度自我协调能力的认知环境提供了一个严格的路线图，包括如何通过AgentOS架构来实现系统的可扩展性和自进化。", "conclusion": "未来的AGI开发重点将在于系统级协调的架构效率。"}}
{"id": "2602.20933", "pdf": "https://arxiv.org/pdf/2602.20933", "abs": "https://arxiv.org/abs/2602.20933", "authors": ["Shuangkang Fang", "I-Chao Shen", "Xuanyang Zhang", "Zesheng Wang", "Yufeng Wang", "Wenrui Ding", "Gang Yu", "Takeo Igarashi"], "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS", "AI": {"tldr": "提出了一种新的基于锚点的Dropout策略，用于解决稀疏视图条件下的过拟合问题。", "motivation": "现有方法在处理稀疏视图时存在邻域补偿效应和忽略高阶球谐系数贡献的问题。", "method": "通过随机选择某些高斯作为锚点并同时移除其空间邻近的高斯来打破局部冗余，以及通过随机丢弃更高阶的球谐系数来集中外观信息在低阶球谐系数上。", "result": "实验结果表明所提出的方法显著优于现有方法，并且可以灵活地进行模型压缩。", "conclusion": "DropAnSH-GS不仅增强了3DGS变体的表现力，而且具有较低的计算开销。"}}
{"id": "2602.20932", "pdf": "https://arxiv.org/pdf/2602.20932", "abs": "https://arxiv.org/abs/2602.20932", "authors": ["Anupam Sharma", "Harish Katti", "Prajwal Singh", "Shanmuganathan Raman", "Krishna Miyapuram"], "title": "Hierarchic-EEG2Text: Assessing EEG-To-Text Decoding across Hierarchical Abstraction Levels", "categories": ["cs.LG", "cs.HC", "eess.SP"], "comment": null, "summary": "An electroencephalogram (EEG) records the spatially averaged electrical activity of neurons in the brain, measured from the human scalp. Prior studies have explored EEG-based classification of objects or concepts, often for passive viewing of briefly presented image or video stimuli, with limited classes. Because EEG exhibits a low signal-to-noise ratio, recognizing fine-grained representations across a large number of classes remains challenging; however, abstract-level object representations may exist. In this work, we investigate whether EEG captures object representations across multiple hierarchical levels, and propose episodic analysis, in which a Machine Learning (ML) model is evaluated across various, yet related, classification tasks (episodes). Unlike prior episodic EEG studies that rely on fixed or randomly sampled classes of equal cardinality, we adopt hierarchy-aware episode sampling using WordNet to generate episodes with variable classes of diverse hierarchy. We also present the largest episodic framework in the EEG domain for detecting observed text from EEG signals in the PEERS dataset, comprising $931538$ EEG samples under $1610$ object labels, acquired from $264$ human participants (subjects) performing controlled cognitive tasks, enabling the study of neural dynamics underlying perception, decision-making, and performance monitoring. We examine how the semantic abstraction level affects classification performance across multiple learning techniques and architectures, providing a comprehensive analysis. The models tend to improve performance when the classification categories are drawn from higher levels of the hierarchy, suggesting sensitivity to abstraction. Our work highlights abstraction depth as an underexplored dimension of EEG decoding and motivates future research in this direction.", "AI": {"tldr": "研究使用EEG信号解码文本，探索不同层次抽象水平对分类性能的影响。", "motivation": "现有研究表明，通过EEG识别物体或概念存在挑战，特别是对于大量类别的精细表示。本文旨在探讨EEG是否能捕捉多层次的物体表示，并引入基于WordNet的分层采样方法以研究语义抽象层次如何影响解码准确性。", "method": "采用PEERS数据集中的EEG样本进行实验，通过机器学习模型评估不同分类任务（层级）下的性能。使用WordNet生成具有可变类别数量和多样化的层级结构的任务集，探索多种学习技术和架构下分类性能的变化。", "result": "在较高层次的抽象水平上，分类性能有所提高，表明模型对于高阶概念更为敏感。", "conclusion": "研究证明了抽象深度是EEG解码中一个未被充分探索的重要维度，并为未来的研究指明了方向。"}}
{"id": "2602.20930", "pdf": "https://arxiv.org/pdf/2602.20930", "abs": "https://arxiv.org/abs/2602.20930", "authors": ["Cristian Valero-Abundio", "Emilio Sansano-Sansano", "Raúl Montoliu", "Marina Martínez García"], "title": "Computing a Characteristic Orientation for Rotation-Independent Image Analysis", "categories": ["cs.CV"], "comment": "Accepted for publication at the 21st International Conference on Computer Vision Theory and Applications (VISAPP 2026). 8 pages", "summary": "Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a preprocessing method that improves rotation robustness without modifying the network architecture. The method estimates a global orientation for each image and aligns it to a canonical reference frame, allowing standard models to process inputs more consistently across different rotations. Unlike moment-based approaches that extract invariant descriptors, this method directly transforms the image while preserving spatial structure, making it compatible with convolutional networks. Experimental evaluation on the rotated MNIST dataset shows that the proposed method achieves higher accuracy than state-of-the-art rotation-invariant architectures. Additional experiments on the CIFAR-10 dataset, confirm that the method remains effective under more complex conditions.", "AI": {"tldr": "提出了一种预处理方法GID，用于提高图像在不同旋转角度下的处理一致性。", "motivation": "深度学习模型缺乏固有的旋转不变性，现有的增强或架构修改方法增加了计算需求或者改变了网络结构。", "method": "该方法估计每张图片的全局方向，并将其对齐到一个标准参考框架，从而直接转换图像同时保持空间结构。", "result": "在旋转MNIST数据集上，所提方法比当前最先进的旋转不变性架构表现更好；CIFAR-10数据集上的实验也证明了其有效性。", "conclusion": "GID作为预处理步骤能够提升模型对不同旋转角度图像的一致性和准确性。"}}
{"id": "2602.20926", "pdf": "https://arxiv.org/pdf/2602.20926", "abs": "https://arxiv.org/abs/2602.20926", "authors": ["Yuqi Huang", "Ning Liao", "Kai Yang", "Anning Hu", "Shengchao Hu", "Xiaoxing Wang", "Junchi Yan"], "title": "HELP: HyperNode Expansion and Logical Path-Guided Evidence Localization for Accurate and Efficient GraphRAG", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with inherent knowledge boundaries and hallucinations, limiting their reliability in knowledge-intensive tasks. While Retrieval-Augmented Generation (RAG) mitigates these issues, it frequently overlooks structural interdependencies essential for multi-hop reasoning. Graph-based RAG approaches attempt to bridge this gap, yet they typically face trade-offs between accuracy and efficiency due to challenges such as costly graph traversals and semantic noise in LLM-generated summaries. In this paper, we propose HyperNode Expansion and Logical Path-Guided Evidence Localization strategies for GraphRAG (HELP), a novel framework designed to balance accuracy with practical efficiency through two core strategies: 1) HyperNode Expansion, which iteratively chains knowledge triplets into coherent reasoning paths abstracted as HyperNodes to capture complex structural dependencies and ensure retrieval accuracy; and 2) Logical Path-Guided Evidence Localization, which leverages precomputed graph-text correlations to map these paths directly to the corpus for superior efficiency. HELP avoids expensive random walks and semantic distortion, preserving knowledge integrity while drastically reducing retrieval latency. Extensive experiments demonstrate that HELP achieves competitive performance across multiple simple and multi-hop QA benchmarks and up to a 28.8$\\times$ speedup over leading Graph-based RAG baselines.", "AI": {"tldr": "提出了一种新的框架HELP，旨在通过两种策略平衡图增强生成模型的准确性与效率。", "motivation": "大型语言模型在知识密集型任务中面临可靠性和多跳推理问题，现有的图增强生成方法难以同时保证准确性和高效性。", "method": "利用HyperNode Expansion迭代链接知识三元组形成路径，并采用Logical Path-Guided Evidence Localization策略直接映射路径以减少检索延迟和语义失真。", "result": "在多个简单及多跳问答基准测试中，HELP表现出色且比现有基线模型快28.8倍。", "conclusion": "所提出的HELP框架能够在提高准确性的基础上大幅度提升效率。"}}
{"id": "2602.20925", "pdf": "https://arxiv.org/pdf/2602.20925", "abs": "https://arxiv.org/abs/2602.20925", "authors": ["Zeyu Jiang", "Kuan Xu", "Changhao Chen"], "title": "LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments", "categories": ["cs.RO", "cs.CV"], "comment": "ICRA 2026", "summary": "Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.", "AI": {"tldr": "提出了一种名为LST-SLAM的新方法，用于在复杂动态场景中实现稳健的热成像同步定位与地图构建。", "motivation": "为了克服热成像SLAM中的特征提取不可靠、运动跟踪不稳定以及全局姿态和地图构造不一致等问题，特别是在大型动态户外环境中。因此提出了一种新的大规模立体热SLAM系统LST-SLAM。", "method": "结合了自我监督的热特征学习、双层次立体运动追踪及几何姿态优化的方法。引入语义-几何混合约束以抑制缺乏强帧间几何一致性潜在动态特性，同时开发了一个在线增量词袋模型用于闭环检测，并配合全局姿态优化来减小累积漂移。", "result": "在千米级动态热数据集上进行了广泛测试后，表明LST-SLAM在稳健性和准确性方面显著优于最近的代表性SLAM系统，包括AirSLAM和DROID-SLAM。", "conclusion": "LST-SLAM能够应对复杂、动态场景中的挑战，并且相比于其他SLAM方法，在稳健性与精确度上具有明显优势。"}}
{"id": "2602.20924", "pdf": "https://arxiv.org/pdf/2602.20924", "abs": "https://arxiv.org/abs/2602.20924", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "title": "Airavat: An Agentic Framework for Internet Measurement", "categories": ["cs.NI", "cs.AI", "cs.SE"], "comment": null, "summary": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research. We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing.", "AI": {"tldr": "Airavat是一个用于互联网测量的工作流生成与验证的代理框架。", "motivation": "互联网测量面临两个挑战：复杂的分析需要专家级工具协调，而正确实施的方法可能有方法论缺陷且难以验证。因此，为了普及测量能力，必须自动化工作流生成并验证是否符合几十年研究建立的方法标准。", "method": "Airavat通过一组代理模仿专家思维处理问题分解、解决方案设计和代码实现，并借助现有工具注册表。两个专用引擎确保了方法的正确性：验证引擎评估工作流程以符合五十年测量研究的知识图谱，而校验引擎确定适当的验证技术基于公认的方法。", "result": "在四个互联网测量案例中，Airavat展示了它能够生成匹配专家级解决方案的工作流、做出合理的架构决策、解决没有地面事实的新问题，并识别标准执行测试所遗漏的方法论缺陷。", "conclusion": "通过展示Airavat如何生成符合专家级水平的测量工作流程和发现方法论错误，证明了其在互联网测量领域的价值。"}}
{"id": "2602.20923", "pdf": "https://arxiv.org/pdf/2602.20923", "abs": "https://arxiv.org/abs/2602.20923", "authors": ["Jiarong Wei", "Anna Rehr", "Christian Feist", "Abhinav Valada"], "title": "ParkDiffusion++: Ego Intention Conditioned Joint Multi-Agent Trajectory Prediction for Automated Parking using Diffusion Models", "categories": ["cs.RO"], "comment": "ICRA 2026 Camera Ready Version", "summary": "Automated parking is a challenging operational domain for advanced driver assistance systems, requiring robust scene understanding and interaction reasoning. The key challenge is twofold: (i) predict multiple plausible ego intentions according to context and (ii) for each intention, predict the joint responses of surrounding agents, enabling effective what-if decision-making. However, existing methods often fall short, typically treating these interdependent problems in isolation. We propose ParkDiffusion++, which jointly learns a multi-modal ego intention predictor and an ego-conditioned multi-agent joint trajectory predictor for automated parking. Our approach makes several key contributions. First, we introduce an ego intention tokenizer that predicts a small set of discrete endpoint intentions from agent histories and vectorized map polylines. Second, we perform ego-intention-conditioned joint prediction, yielding socially consistent predictions of the surrounding agents for each possible ego intention. Third, we employ a lightweight safety-guided denoiser with different constraints to refine joint scenes during training, thus improving accuracy and safety. Fourth, we propose counterfactual knowledge distillation, where an EMA teacher refined by a frozen safety-guided denoiser provides pseudo-targets that capture how agents react to alternative ego intentions. Extensive evaluations demonstrate that ParkDiffusion++ achieves state-of-the-art performance on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Importantly, qualitative what-if visualizations show that other agents react appropriately to different ego intentions.", "AI": {"tldr": "ParkDiffusion++ 是一种用于自动停车的多智能体轨迹预测方法，它通过扩散模型联合学习ego意图和基于ego条件下的多智能体轨迹预测。", "motivation": "当前的方法在处理自动化停车时面临挑战，需要准确理解场景并合理推理交互。关键问题在于同时预测多个可能的ego意图及其对周围代理的影响，并且这些问题是相互依赖的。", "method": "ParkDiffusion++ 引入了一个ego意图标记器、一个基于ego条件下的联合预测模型和一个轻量级的安全导向去噪器，提高了预测的一致性和安全性。此外，通过反事实知识蒸馏增强了模型的表现。", "result": "在Dragon Lake Parking (DLP) 和Intersections Drone (inD) 数据集上，ParkDiffusion++ 达到了最先进的性能，并且能够展示其他代理如何对不同的ego意图做出反应的可视化效果。", "conclusion": "ParkDiffusion++ 方法通过联合学习多模态ego意图预测和基于ego条件下的多智能体轨迹预测，在自动化停车场景中取得了显著的效果，提高了系统的安全性和决策质量。"}}
{"id": "2602.20920", "pdf": "https://arxiv.org/pdf/2602.20920", "abs": "https://arxiv.org/abs/2602.20920", "authors": ["Daniel Huczala", "Severinas Zube", "Martin Pfurner", "Johannes Siegele", "Frank C. Park"], "title": "Computer-Aided Design of Rational Motions for 4R and 6R Spatial Mechanism Synthesis", "categories": ["cs.RO"], "comment": null, "summary": "This paper focuses on geometric methods for generating rational motions used in the design of single-loop rational linkages, 1-degree-of-freedom mechanisms that can execute prescribed spatial tasks. Building on established rational motion synthesis methods, we introduce a new interpolation scheme for seven 3D points based on cubic quaternionic Bezier curves. The resulting motion admits factorization, i.e. the synthesis of a spatial six-bar mechanism whose tool frame passes the specified seven points. To support engineering practice, we provide open-source CAD tools that implement also the other methods and provide fast visual evaluation of motion generation and mechanism synthesis.", "AI": {"tldr": "提出了一种基于七点的立方四元数贝塞尔曲线插值方案，用于设计空间六杆连杆机制。", "motivation": "通过改进现有的理性运动合成方法，以支持工程实践并快速评估运动生成和机制综合。", "method": "引入一个新的基于七点的立方四元数贝塞尔曲线插值方案，并提供了开源CAD工具来实现其他方法和提供快速视觉评价。", "result": "成功合成了一个空间六杆连杆机制，其工件框架通过指定的七个点。", "conclusion": "新提出的方法有助于改进设计过程并提高工程实践中的效率。"}}
{"id": "2602.20918", "pdf": "https://arxiv.org/pdf/2602.20918", "abs": "https://arxiv.org/abs/2602.20918", "authors": ["Hyewon Jang", "Nikolai Ilinykh", "Sharid Loáiciga", "Jey Han Lau", "Shalom Lappin"], "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.", "AI": {"tldr": "研究在多模态背景下人类和大型语言模型对句子接受度判断的能力。", "motivation": "探讨视觉上下文对人类和大型语言模型（LLM）句子接受度判断的影响，以及其与文本背景的差异性。", "method": "通过实验对比不同类型的大型语言模型在有无视觉上下文情况下预测人类句子接受度判断的表现，并分析结果。", "result": "结果显示，对于人类而言，视觉图像对句子接受度评分几乎无影响；而对于大型语言模型来说，在多模态背景下显示了压缩效应。此外，LLM的预测准确性整体上略低于去除视觉背景的情况，且不同模型间的预测分布存在差异性。", "conclusion": "研究发现了一些在人类和大型语言模型处理句子时相似之处和区别，特别是在多模态上下文中。"}}
{"id": "2602.20915", "pdf": "https://arxiv.org/pdf/2602.20915", "abs": "https://arxiv.org/abs/2602.20915", "authors": ["Dimitrios Dimou", "José Santos-Victor", "Plinio Moreno"], "title": "Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task's constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant's grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.", "AI": {"tldr": "本文提出了一种基于强化学习和姿势协同的多任务抓取方法，以实现机器人在特定任务下的灵巧抓握。", "motivation": "当前的方法难以同时满足多种对象抓取及下游任务约束的要求，缺乏一种端到端解决方案。为此，研究者希望通过结合人类抓取行为数据与强化学习探索，来提升机器人的多任务抓取能力。", "method": "利用接触姿态数据库提取人类抓取偏好，并基于变分自动编码器训练手部协同模型；再通过强化学习训练代理进行多种对象抓握并考虑特定的任务意图。", "result": "所提方法能够使机器人在考虑下游任务约束的情况下，完成多个目标的灵巧抓取动作。", "conclusion": "结合数据驱动的人类抓取行为与强化学习探索机制可以有效提升机器人的多任务协作能力。"}}
{"id": "2602.20913", "pdf": "https://arxiv.org/pdf/2602.20913", "abs": "https://arxiv.org/abs/2602.20913", "authors": ["Jihao Qiu", "Lingxi Xie", "Xinyue Huo", "Qi Tian", "Qixiang Ye"], "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, 8 tables, accepted to CVPR 2026", "summary": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1", "AI": {"tldr": "该论文提出了LongVideo-R1，一种用于低成本长视频理解的智能导航多模态大型语言模型代理。", "motivation": "解决长视频理解和低计算预算之间的矛盾，减少冗余搜索提高效率。", "method": "设计了一个推理模块利用高级视觉线索推断最有信息量的视频片段。通过从CGBench中提取分层视频字幕并引导GPT-5生成高质量轨迹进行训练，并使用监督微调和强化学习进行模型优化。", "result": "实验结果验证了LongVideo-R1在多个长视频基准上的效果，显示出了准确性和效率的良好平衡。", "conclusion": "LongVideo-R1通过智能导航实现低成本高效的长视频理解，在问答准确性方面表现出色。"}}
{"id": "2602.20911", "pdf": "https://arxiv.org/pdf/2602.20911", "abs": "https://arxiv.org/abs/2602.20911", "authors": ["Ruiqi Liu", "Boyu Diao", "Hangda Liu", "Zhulin An", "Fei Wang", "Yongjun Xu"], "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.", "AI": {"tldr": "构建一个适应性专家森林，通过结构化组织适配器来提高类增量学习中的知识共享。", "motivation": "现有的方法冻结预训练模型并为每个任务训练新的轻量级适配器，这虽然避免了遗忘问题，但未能利用任务间的关联关系。", "method": "提出语义引导的适应性专家森林（SAEF），将任务分组到概念簇中，并在每簇内建立平衡的专家树，通过合并相似任务的适配器来构建新适配器。在推理时，激活相关专家并根据其自信程度加权组合预测。", "result": "实验表明，SAEF达到了最先进的性能。", "conclusion": "SAEF有效地利用了任务间的语义关系和知识共享，提高了类增量学习的性能。"}}
{"id": "2602.20903", "pdf": "https://arxiv.org/pdf/2602.20903", "abs": "https://arxiv.org/abs/2602.20903", "authors": ["Hanshen Zhu", "Yuliang Liu", "Xuecheng Wu", "An-Lan Wang", "Hao Feng", "Dingkang Yang", "Chao Feng", "Can Huang", "Jingqun Tang", "Xiang Bai"], "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering", "categories": ["cs.CV"], "comment": "Code: https://github.com/CIawevy/TextPecker", "summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.", "AI": {"tldr": "提出了一种新的插件式RL策略TextPecker，用于识别和优化视觉文本渲染中的结构性异常。", "motivation": "当前的MLLMs和OCR模型在检测视觉文本渲染（VTR）中的结构异常方面存在不足，导致难以评估和优化生成的文本图像的质量。", "method": "构建了包含字符级结构异常注释的数据集，并开发了一种笔划编辑合成引擎以扩大错误覆盖率。提出了TextPecker策略来减少噪音奖励信号并改进各种文本到图像模型的表现。", "result": "实验表明，使用TextPecker后，不同文本生成器在结构性保真度和语义对齐方面都有显著提升，尤其是在优化后的Qwen-Image上提升了4%的结构保真度和8.7%的语义对齐。", "conclusion": "通过提供一种新的RL策略来识别和解决VTR中的结构性异常问题，这项研究有助于实现可靠且结构忠实的视觉文本生成。"}}
{"id": "2602.20901", "pdf": "https://arxiv.org/pdf/2602.20901", "abs": "https://arxiv.org/abs/2602.20901", "authors": ["Yuechen Xie", "Xiaoyan Zhang", "Yicheng Shan", "Hao Zhu", "Rui Tang", "Rong Wei", "Mingli Song", "Yuanyu Wan", "Jie Song"], "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2026", "summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.", "AI": {"tldr": "本文提出了一个评估视觉语言模型（VLM）空间逻辑推理能力的基准SpatiaLQA。", "motivation": "虽然现有的VLM已经在常见的视觉问答和逻辑推理任务中表现出色，但它们在复杂的现实环境中的空间逻辑推理方面仍然存在不足。为了填补这一空白，本文引入了SpatiaLQA来评估VLM的空间逻辑推理能力。", "method": "通过递归场景图辅助推理方法，利用视觉基础模型将复杂场景逐步分解为与任务相关的场景图，从而增强VLM的空间逻辑推理能力。", "result": "实验结果表明，在41种主流的VLM中，最先进的模型在空间逻辑推理方面仍然存在困难。所提出的递归场景图辅助推理方法优于所有先前的方法。", "conclusion": "SpatiaLQA为评估VLM的空间逻辑推理提供了一个基准，并提出了改进这种能力的有效方法。"}}
{"id": "2602.20891", "pdf": "https://arxiv.org/pdf/2602.20891", "abs": "https://arxiv.org/abs/2602.20891", "authors": ["Zhengtao Xu", "Zimo Xia", "Zicheng Zhu", "Nattapat Boonprakong", "Yu-An Chen", "Rabih Zbib", "Casimiro Pio Carrino", "Yi-Chieh Lee"], "title": "InterPilot: Exploring the Design Space of AI-assisted Job Interview Support for HR Professionals", "categories": ["cs.HC"], "comment": "7 pages, 2 figures", "summary": "Recruitment interviews are cognitively demanding interactions in which interviewers must simultaneously listen, evaluate candidates, take notes, and formulate follow-up questions. To better understand these challenges, we conducted a formative study with eight HR professionals, from which we derived key design goals for real-time AI support. Guided by these insights, we developed InterPilot, a prototype system that augments interviews through intelligent note-taking and post-interview summary, adaptive question generation, and real-time skill-evidence mapping. We evaluated the system with another seven HR professionals in mock interviews using a within-subjects design. Results show that InterPilot reduced documentation burden without increasing overall workload, but introduced usability trade-offs related to visual attention and interaction complexity. Qualitative findings further reveal tensions around trust and verification when AI suggests highly specific technical questions. We discuss implications for designing future real-time human-AI collaboration in professional settings, highlighting the need to balance assistance granularity, attentional demands, and human agency.", "AI": {"tldr": "探讨了人工智能辅助招聘面试支持的设计空间，开发并评估了一款名为InterPilot的原型系统。", "motivation": "为了帮助人力资源专业人士应对在进行面试时面临的多任务处理挑战，包括倾听、评价候选人、记笔记和制定后续问题等任务。通过形式化研究与八名HR专业人员合作确定了设计目标，并开发了一个AI辅助工具来提高效率。", "method": "首先进行了形式化研究以了解招聘过程中人力资源专业人士的需求。基于这些见解创建了InterPilot原型系统，然后通过模拟面试对该系统进行了评估，采用单因素内组设计的方法对七名HR专业人员进行测试。", "result": "实验结果显示，使用InterPilot可以减轻文档负担而不会增加总体工作量，但同时引入了与视觉注意力和交互复杂性相关的可用性问题。此外还发现了在AI提出高度特定的技术问题时存在的信任和验证紧张情况。", "conclusion": "讨论了未来设计实时人机协作的潜在影响，强调需要平衡辅助粒度、注意需求和人类自主权的重要性"}}
{"id": "2602.20880", "pdf": "https://arxiv.org/pdf/2602.20880", "abs": "https://arxiv.org/abs/2602.20880", "authors": ["Yongli Xiang", "Ziming Hong", "Zhaoqing Wang", "Xiangyu Zhao", "Bo Han", "Tongliang Liu"], "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance", "categories": ["cs.CV"], "comment": "CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG", "summary": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.", "AI": {"tldr": "该论文提出了一种名为Conflict-aware Adaptive Safety Guidance (CASG)的框架，旨在解决文本到图像生成模型中的多重有害冲突问题。", "motivation": "现有基于安全指导的方法在处理多类别有害内容时存在不足，可能导致不同危害类型的相互干扰。为了解决这一问题，提出了新的方法以降低整体有害率。", "method": "该方法包括两个主要组成部分：Conflict-aware Category Identification (CaCI)用于识别与生成过程最相关的有害类别；以及Conflict-resolving Guidance Application (CrGA)，通过只沿特定类别进行安全引导来避免多类别的干扰。CASG适用于潜空间和文本空间的安全措施。", "result": "实验表明，该方法在多个评估指标上表现出色，相较于现有技术最高可将有害率降低15.4%。", "conclusion": "本文提出的方法有效地解决了文本到图像生成模型中的多重有害冲突问题，并且能够在不进行额外训练的情况下显著提高安全性。"}}
{"id": "2602.20878", "pdf": "https://arxiv.org/pdf/2602.20878", "abs": "https://arxiv.org/abs/2602.20878", "authors": ["Dhita Putri Pratama", "Soyeon Caren Han", "Yihao Ding"], "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.", "AI": {"tldr": "本文介绍了通过结构化相关性图诊断视觉语言模型因果推理能力的方法。", "motivation": "大型视觉-语言模型在视觉问答基准上表现出色，但经常依赖于偶然关联而非真正的因果推理。现有的评估方法主要关注答案的正确性，无法明确失败是否源于有限的推理能力或未能识别因果相关信息。", "method": "本文提出了结构化的、查询条件下的视觉-语言因果图（VLCGs），该表示法显式编码了因果相关的物体、属性、关系和基于场景假设。在此基础上，我们推出了ViLCaR诊断基准，包括因果归属、因果推理和问答任务，并提供了超越最终答案准确性的图对齐评估指标。", "result": "实验表明，在最先进的视觉-语言模型中注入结构化相关信息显著提高了归因和推理的一致性，优于零样本学习和标准上下文学习方法。", "conclusion": "当前的限制主要源于缺少结构性指导而非缺乏推理能力。"}}
{"id": "2602.20877", "pdf": "https://arxiv.org/pdf/2602.20877", "abs": "https://arxiv.org/abs/2602.20877", "authors": ["Jiwoo Kang", "Yeon-Chang Lee"], "title": "E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.", "AI": {"tldr": "提出了一种统一的多模态知识图谱框架E-MMKGR，用于改善电子商务中的推荐和产品搜索。", "motivation": "现有的多模态推荐系统依赖于固定的一组模式和特定任务的目标，这限制了其在不同任务上的泛化能力与扩展性。为解决这一问题，提出了一种新的方法来提升系统的灵活性与性能。", "method": "通过基于GNN的传播以及知识图谱导向的优化构建电子商务专用的多模态知识图谱E-MMKG，并从中学习统一的商品表示以提供一个适用于多种任务的共享语义基础。", "result": "在真实世界的Amazon数据集上进行实验，推荐和产品搜索分别提高了10.18%和21.72%，显示了该方法的有效性和扩展性。", "conclusion": "E-MMKGR框架不仅增强了电子商务应用中的多模态推荐系统的性能，还展示了其灵活适应新任务的能力。"}}
{"id": "2602.20876", "pdf": "https://arxiv.org/pdf/2602.20876", "abs": "https://arxiv.org/abs/2602.20876", "authors": ["Runhua Zhang", "Ziqi Pan", "Kangyu Yuan", "Qiaoyi Chen", "Yulin Tian", "Huamin Qu", "Xiaojuan Ma"], "title": "When LLMs Enter Everyday Feminism on Chinese Social Media: Opportunities and Risks for Women's Empowerment", "categories": ["cs.HC"], "comment": "This paper is conditionally accepted to CHI 2026", "summary": "Everyday digital feminism refers to the ordinary, often pragmatic ways women articulate lived experiences and cultivate solidarity in online spaces. In China, such practices flourish on RedNote through discussions under hashtags like ''women's growth''. Recently, DeepSeek-generated content has been taken up as a new voice in these conversations. Given widely recognized gender biases in LLMs, this raises critical concerns about how LLMs interact with everyday feminist practices. Through an analysis of 430 RedNote posts, 139 shared DeepSeek responses, and 3211 comments, we found that users predominantly welcomed DeepSeek's advice. Yet feminist critical discourse analysis revealed that these responses primarily encouraged women to self-optimize and pursue achievements within prevailing norms rather than challenge them. By interpreting this case, we discuss the opportunities and risks that LLMs introduce for everyday feminism as a pathway toward women's empowerment, and offer design implications for leveraging LLMs to better support such practices.", "AI": {"tldr": "研究分析了深度学习模型生成的内容在女性主义讨论中的影响。", "motivation": "探讨大型语言模型（LLM）参与日常女性主义实践的机会和风险，特别是这些模型如何可能强化而不是挑战性别偏见。", "method": "通过分析430个帖子、139份DeepSeek回应以及3211条评论来研究LLM生成内容在红客笔记中的互动情况。", "result": "尽管用户普遍欢迎DeepSeek的建议，但这些响应主要鼓励女性自我优化和遵守现行规范，而不是挑战它们。", "conclusion": "讨论了LLMs参与日常女性主义实践的机会与风险，并提供了设计建议以更好地利用LLMs支持此类实践活动。"}}
{"id": "2602.20873", "pdf": "https://arxiv.org/pdf/2602.20873", "abs": "https://arxiv.org/abs/2602.20873", "authors": ["Jiahao Xu", "Sheng Huang", "Xin Zhang", "Zhixiong Nan", "Jiajun Dong", "Nankun Mu"], "title": "MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.", "AI": {"tldr": "本文提出了MUSE框架，用于改善有限监督下的病理学图像分类。", "motivation": "现有方法将文本语义视为静态的类级先验信息，并缺乏样本级别的细化调整，导致视觉-语义对齐的效果受限。为了克服这一限制，作者提出了一种新的框架以提高精确度和多样性。", "method": "MUSE通过引入SFSE进行细粒度语义增强并使用SMMO构建多样化的病理描述库，动态选择文本作为强化监督来优化模型，促进稳健性和防止过拟合。", "result": "在三个基准WSI数据集上的实验表明，MUSE在有限样本条件下超越了现有视觉-语言基线方法，展示了有效的小样本学习需要更加丰富的语义来源和其主动及样本感知的语义优化的重要性。", "conclusion": "通过动态选择文本作为强化监督来优化模型，MUSE框架提高了病理学图像分类的效果，在小样本条件下表现出色。"}}
{"id": "2602.20871", "pdf": "https://arxiv.org/pdf/2602.20871", "abs": "https://arxiv.org/abs/2602.20871", "authors": ["Wenbo Yu", "Wenke Xia", "Weitao Zhang", "Di Hu"], "title": "GeCo-SRT: Geometry-aware Continual Adaptation for Robotic Cross-Task Sim-to-Real Transfer", "categories": ["cs.RO"], "comment": "Accepted By CVPR 2026", "summary": "Bridging the sim-to-real gap is important for applying low-cost simulation data to real-world robotic systems. However, previous methods are severely limited by treating each transfer as an isolated endeavor, demanding repeated, costly tuning and wasting prior transfer experience.To move beyond isolated sim-to-real, we build a continual cross-task sim-to-real transfer paradigm centered on knowledge accumulation across iterative transfers, thereby enabling effective and efficient adaptation to novel tasks. Thus, we propose GeCo-SRT, a geometry-aware continual adaptation method. It utilizes domain-invariant and task-invariant knowledge from local geometric features as a transferable foundation to accelerate adaptation during subsequent sim-to-real transfers. This method starts with a geometry-aware mixture-of-experts module, which dynamically activates experts to specialize in distinct geometric knowledge to bridge observation sim-to-real gap. Further, the geometry-expert-guided prioritized experience replay module preferentially samples from underutilized experts, refreshing specialized knowledge to combat forgetting and maintain robust cross-task performance. Leveraging knowledge accumulated during iterative transfer, GeCo-SRT method not only achieves 52% average performance improvement over the baseline, but also demonstrates significant data efficiency for new task adaptation with only 1/6 data.We hope this work inspires approaches for efficient, low-cost cross-task sim-to-real transfer.", "AI": {"tldr": "提出了一种基于几何感知的持续适应方法GeCo-SRT，用于机器人跨任务仿真到现实转移。", "motivation": "当前的方法将每个仿真到现实的转换视为孤立的任务，并且需要重复、昂贵的调优并浪费了先前的转换经验。因此，本文提出了一个知识积累型的持续跨任务仿真到现实转移范式。", "method": "GeCo-SRT采用一种几何感知混合专家模块，动态激活专门处理不同几何知识的专家来缩小观察到的仿真与现实之间的差距，并通过优先经历回放模块从未充分利用的专家中采样以对抗遗忘并保持跨任务性能。", "result": "该方法在新的任务适应上比基线提高了52%的平均表现，并且只使用了六分之一的数据。", "conclusion": "GeCo-SRT展示了高效、低成本的跨任务仿真到现实转移的能力，希望这项工作能启发更多此类方法的发展。"}}
{"id": "2602.20867", "pdf": "https://arxiv.org/pdf/2602.20867", "abs": "https://arxiv.org/abs/2602.20867", "authors": ["Yanna Jiang", "Delong Li", "Haiyu Deng", "Baihe Ma", "Xu Wang", "Qin Wang", "Guangsheng Yu"], "title": "SoK: Agentic Skills -- Beyond Tool Use in LLM Agents", "categories": ["cs.CR", "cs.AI", "cs.CE", "cs.ET"], "comment": null, "summary": "Agentic systems increasingly rely on reusable procedural capabilities, \\textit{a.k.a., agentic skills}, to execute long-horizon workflows reliably. These capabilities are callable modules that package procedural knowledge with explicit applicability conditions, execution policies, termination criteria, and reusable interfaces. Unlike one-off plans or atomic tool calls, skills operate (and often do well) across tasks. This paper maps the skill layer across the full lifecycle (discovery, practice, distillation, storage, composition, evaluation, and update) and introduces two complementary taxonomies. The first is a system-level set of \\textbf{seven design patterns} capturing how skills are packaged and executed in practice, from metadata-driven progressive disclosure and executable code skills to self-evolving libraries and marketplace distribution. The second is an orthogonal \\textbf{representation $\\times$ scope} taxonomy describing what skills \\emph{are} (natural language, code, policy, hybrid) and what environments they operate over (web, OS, software engineering, robotics). We analyze the security and governance implications of skill-based agents, covering supply-chain risks, prompt injection via skill payloads, and trust-tiered execution, grounded by a case study of the ClawHavoc campaign in which nearly 1{,}200 malicious skills infiltrated a major agent marketplace, exfiltrating API keys, cryptocurrency wallets, and browser credentials at scale. We further survey deterministic evaluation approaches, anchored by recent benchmark evidence that curated skills can substantially improve agent success rates while self-generated skills may degrade them. We conclude with open challenges toward robust, verifiable, and certifiable skills for real-world autonomous agents.", "AI": {"tldr": "本文研究了大型语言模型代理中的可重用程序能力，即所谓的“代理技能”，并提出了两个互补的分类法来描述这些技能的设计和执行方式。", "motivation": "随着人工智能系统的不断发展，它们越来越依赖于可以跨任务重复使用的程序性知识模块（称为代理技能）来进行可靠的操作。文章旨在通过提出两种分类方法来更好地理解这些技能在系统设计、安全性和治理方面的作用，并探讨了恶意技能可能带来的风险以及如何评估和管理这些技能。", "method": "作者提出了一个涵盖技能生命周期各个阶段的框架，包括发现、实践、提炼、存储、组合、评价和更新。同时，文章还提供了两个分类法：一个是关于系统层面的设计模式，另一个是描述技能类型及其作用范围的分类。", "result": "本文分析了基于代理技能的安全性和治理问题，并通过一个恶意技能侵入大型代理市场案例研究（ClawHavoc）来说明潜在的风险。此外，文章还讨论了确定性评估方法和最新基准数据支持下的证据表明，经过精心策划的技能可以显著提高代理的成功率。", "conclusion": "作者指出未来需要解决的关键挑战包括如何构建健壮、可验证和可认证的技能以支持现实世界的自主代理。"}}
{"id": "2602.20860", "pdf": "https://arxiv.org/pdf/2602.20860", "abs": "https://arxiv.org/abs/2602.20860", "authors": ["Wangkai Li", "Rui Sun", "Zhaoyang Li", "Yujia Chen", "Tianzhu Zhang"], "title": "DA-Cal: Towards Cross-Domain Calibration in Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "While existing unsupervised domain adaptation (UDA) methods greatly enhance target domain performance in semantic segmentation, they often neglect network calibration quality, resulting in misalignment between prediction confidence and actual accuracy -- a significant risk in safety-critical applications. Our key insight emerges from observing that performance degrades substantially when soft pseudo-labels replace hard pseudo-labels in cross-domain scenarios due to poor calibration, despite the theoretical equivalence of perfectly calibrated soft pseudo-labels to hard pseudo-labels. Based on this finding, we propose DA-Cal, a dedicated cross-domain calibration framework that transforms target domain calibration into soft pseudo-label optimization. DA-Cal introduces a Meta Temperature Network to generate pixel-level calibration parameters and employs bi-level optimization to establish the relationship between soft pseudo-labels and UDA supervision, while utilizing complementary domain-mixing strategies to prevent overfitting and reduce domain discrepancies. Experiments demonstrate that DA-Cal seamlessly integrates with existing self-training frameworks across multiple UDA segmentation benchmarks, significantly improving target domain calibration while delivering performance gains without inference overhead. The code will be released.", "AI": {"tldr": "本文提出了DA-Cal框架，旨在解决跨域场景下的网络校准问题。", "motivation": "现有无监督领域适应方法在语义分割中提升了目标领域的性能，但忽视了网络校准的质量，导致预测置信度与实际准确度之间存在偏差，在安全关键应用中构成风险。", "method": "本文提出了一种新的跨域校准框架DA-Cal，该框架通过生成像素级校准参数来优化软伪标签，并利用双向优化技术建立软伪标签和UDA监督之间的关系。", "result": "实验表明，DA-Cal能够有效改善目标领域的网络校准质量，同时在多个UDA分割基准测试中提供了性能提升且不增加推理开销。", "conclusion": "本文提出的DA-Cal框架解决了跨域场景下的网络校准问题，并证明了其有效性。"}}
{"id": "2602.20854", "pdf": "https://arxiv.org/pdf/2602.20854", "abs": "https://arxiv.org/abs/2602.20854", "authors": ["Elena Gribelyuk", "Honghao Lin", "David P. Woodruff", "Huacheng Yu", "Samson Zhou"], "title": "Adversarial Robustness on Insertion-Deletion Streams", "categories": ["cs.DS"], "comment": null, "summary": "We study adversarially robust algorithms for insertion-deletion (turnstile) streams, where future updates may depend on past algorithm outputs. While robust algorithms exist for insertion-only streams with only a polylogarithmic overhead in memory over non-robust algorithms, it was widely conjectured that turnstile streams of length polynomial in the universe size $n$ require space linear in $n$. We refute this conjecture, showing that robustness can be achieved using space which is significantly sublinear in $n$. Our framework combines multiple linear sketches in a novel estimator-corrector-learner framework, yielding the first insertion-deletion algorithms that approximate: (1) the second moment $F_2$ up to a $(1+\\varepsilon)$-factor in polylogarithmic space, (2) any symmetric function $\\cal{F}$ with an $\\mathcal{O}(1)$-approximate triangle inequality up to a $2^{\\mathcal{O}(C)}$ factor in $\\tilde{\\mathcal{O}}(n^{1/C}) \\cdot S(n)$ bits of space, where $S$ is the space required to approximate $\\cal{F}$ non-robustly; this includes a broad class of functions such as the $L_1$-norm, the support size $F_0$, and non-normed losses such as the $M$-estimators, and (3) $L_2$ heavy hitters. For the $F_2$ moment, our algorithm is optimal up to $\\textrm{poly}((\\log n)/\\varepsilon)$ factors. Given the recent results of Gribelyuk et al. (STOC, 2025), this shows an exponential separation between linear sketches and non-linear sketches for achieving adversarial robustness in turnstile streams.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20853", "pdf": "https://arxiv.org/pdf/2602.20853", "abs": "https://arxiv.org/abs/2602.20853", "authors": ["Stefanie Schneider"], "title": "On the Explainability of Vision-Language Models in Art History", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.", "AI": {"tldr": "本文研究了如何使用可解释的人工智能方法来解析视觉语言模型在艺术历史中的视觉推理能力。", "motivation": "探讨视觉语言模型（VLM）在艺术史语境下的理解能力和透明度，通过引入XAI方法提升模型的可解释性。", "method": "评估七种解释方法，结合零样本定位实验与人类可解释性研究来分析CLIP模型的视觉推理过程。", "result": "结果显示这些方法捕捉到了一些人类解读方面的特征，但其效果依赖于所考察类别的概念稳定性和代表性。", "conclusion": "提出的方法有助于提高VLM在艺术历史领域的透明度和理解能力，但仍需进一步探索以增强解释的有效性。"}}
{"id": "2602.20851", "pdf": "https://arxiv.org/pdf/2602.20851", "abs": "https://arxiv.org/abs/2602.20851", "authors": ["Ran Zhang", "Xuanhua He", "Liu Liu"], "title": "Hybrid Fusion: One-Minute Efficient Training for Zero-Shot Cross-Domain Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion seeks to integrate complementary information from multiple sources into a single, superior image. While traditional methods are fast, they lack adaptability and performance. Conversely, deep learning approaches achieve state-of-the-art (SOTA) results but suffer from critical inefficiencies: their reliance on slow, resource-intensive, patch-based training introduces a significant gap with full-resolution inference. We propose a novel hybrid framework that resolves this trade-off. Our method utilizes a learnable U-Net to generate a dynamic guidance map that directs a classic, fixed Laplacian pyramid fusion kernel. This decoupling of policy learning from pixel synthesis enables remarkably efficient full-resolution training, eliminating the train-inference gap. Consequently, our model achieves SOTA-comparable performance in about one minute on a RTX 4090 or two minutes on a consumer laptop GPU from scratch without any external model and demonstrates powerful zero-shot generalization across diverse tasks, from infrared-visible to medical imaging. By design, the fused output is linearly constructed solely from source information, ensuring high faithfulness for critical applications. The codes are available at https://github.com/Zirconium233/HybridFusion", "AI": {"tldr": "该论文提出了一种新型的混合框架，用于实现快速高效的跨域图像融合。", "motivation": "传统方法速度快但缺乏灵活性和性能，而深度学习方法虽能达到最佳效果但训练效率低下。本文旨在解决这一问题。", "method": "采用可学习U-Net生成动态引导图来指导固定拉普拉斯金字塔融合核，从而实现高效的全分辨率训练并消除训练与推理之间的差距。", "result": "该模型在大约一分钟内实现了状态最佳性能，并展示了强大的零样本跨任务泛化能力。代码可在GitHub上获取。", "conclusion": "通过设计确保了输出的高忠实性，适用于关键应用领域如红外-可见光图像和医学成像。"}}
{"id": "2602.20850", "pdf": "https://arxiv.org/pdf/2602.20850", "abs": "https://arxiv.org/abs/2602.20850", "authors": ["Lei Ye", "Haibo Gao", "Huaiguang Yang", "Peng Xu", "Haoyu Wang", "Tie Liu", "Junqi Shan", "Zongquan Deng", "Liang Ding"], "title": "KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion", "categories": ["cs.RO"], "comment": null, "summary": "Legged robots face significant challenges in navigating complex environments, as they require precise real-time decisions for foothold selection and contact planning. While existing research has explored methods to select footholds based on terrain geometry or kinematics, a critical gap remains: few existing methods efficiently validate the existence of a non-collision swing trajectory. This paper addresses this gap by introducing KCFRC, a novel approach for efficient foothold reachability analysis. We first formally define the foothold reachability problem and establish a sufficient condition for foothold reachability. Based on this condition, we develop the KCFRC algorithm, which enables robots to validate foothold reachability in real time. Our experimental results demonstrate that KCFRC achieves remarkable time efficiency, completing foothold reachability checks for a single leg across 900 potential footholds in an average of 2 ms. Furthermore, we show that KCFRC can accelerate trajectory optimization and is particularly beneficial for contact planning in confined spaces, enhancing the adaptability and robustness of legged robots in challenging environments.", "AI": {"tldr": "提出了KCFRC算法，用于实时验证腿足机器人脚点可达性，提高复杂环境中的导航性能。", "motivation": "现有的方法在选择脚步位置和接触规划时存在不足，无法高效地验证非碰撞摆动轨迹的存在。为解决此问题，引入了KCFRC算法以改进现有技术。", "method": "首先定义足点可达性问题并建立足够的条件。然后开发KCFRC算法，该算法可以实时验证足点的可达性。", "result": "实验结果显示，KCFRC能够在平均2毫秒内完成单腿900个潜在脚点的可达性检查，并且能够加速轨迹优化和增强在狭小空间中的接触规划能力。", "conclusion": "通过引入KCFRC算法，显著提高了腿足机器人在复杂环境中的适应性和鲁棒性。"}}
{"id": "2602.20846", "pdf": "https://arxiv.org/pdf/2602.20846", "abs": "https://arxiv.org/abs/2602.20846", "authors": ["Yuki Nakamura"], "title": "Body-Reservoir Governance in Repeated Games: Embodied Decision-Making, Dynamic Sentinel Adaptation, and Complexity-Regularized Optimization", "categories": ["cs.GT", "cs.MA", "cs.NE", "nlin.AO"], "comment": "44 pages, 9 figures, 2 tables", "summary": "Standard game theory explains cooperation in repeated games through conditional strategies such as Tit-for-Tat (TfT), but these require continuous computation that imposes physical costs on embodied agents. We propose a three-layer Body-Reservoir Governance (BRG) architecture: (1) a body reservoir (echo state network) whose $d$-dimensional state performs implicit inference over interaction history, serving as both decision-maker and anomaly detector, (2) a cognitive filter providing costly strategic tools activated on demand, and (3) a metacognitive governance layer with receptivity parameter $α\\in [0,1]$. At full body governance ($α=1$), closed-loop dynamics satisfy a self-consistency equation: cooperation is expressed as the reservoir's fixed point, not computed. Strategy complexity cost is defined as the KL divergence between the reservoir's state distribution and its habituated baseline. Body governance reduces this cost, with action variance decreasing up to $1600\\times$ with dimension $d$. A dynamic sentinel generates a composite discomfort signal from the reservoir's own state, driving adaptive $α(t)$: near baseline during cooperation, rapidly dropping upon defection to activate cognitive retaliation. Overriding the body incurs thermodynamic cost proportional to internal state distortion. The sentinel achieves the highest payoff across all conditions, outperforming static body governance, TfT, and EMA baselines. A dimension sweep ($d \\in \\{5,\\ldots,100\\}$) shows implicit inference scales with bodily richness ($23\\times$ to $1600\\times$ variance reduction), attributable to reservoir dynamics. A phase diagram in $(d, τ_{\\mathrm{env}})$ space reveals governance regime transitions near $d \\approx 20$. The framework reinterprets cooperation as the minimum-dissipation response of an adapted dynamical system -- emergent from embodied dynamics rather than computed.", "AI": {"tldr": "本文提出了一个三层的Body-Reservoir Governance（BRG）架构，以减少合作中所需计算的成本，并在多次游戏中实现更高的收益。", "motivation": "标准博弈论通过条件策略如Tit-for-Tat解释了重复游戏中的合作行为，但这些方法需要连续计算，这对实体代理造成了物理成本。本文提出了一个更加高效的合作机制来解决这个问题。", "method": "提出了一种三层的Body-Reservoir Governance（BRG）架构：1. 身体水库通过隐式推理处理交互历史；2. 认知过滤器提供昂贵的战略工具，根据需求激活；3. 元认知治理层通过参数α调节身体和认知层面的行为。", "result": "这种机制能够减少合作的成本，并且在不同的条件下实现了更高的收益。动态哨兵机制使得系统能够在背叛行为发生时迅速采取行动。", "conclusion": "本文证明了BRG架构能够在重复游戏中实现高效的合作，将合作解释为适应性动力系统的最小耗散反应。"}}
{"id": "2602.20845", "pdf": "https://arxiv.org/pdf/2602.20845", "abs": "https://arxiv.org/abs/2602.20845", "authors": ["João Deltregia Martinelli", "Marcelo Luis Rodrigues Filho", "Felipe Crispim da Rocha Salvagnini", "Gilson Junior Soares", "Jefersson A. dos Santos", "Alexandre X. Falcão"], "title": "FLIM Networks with Bag of Feature Points", "categories": ["cs.CV"], "comment": "Accepted at the 28th Iberoamerican Congress on Pattern Recognition (CIARP 2025). To appear in Lecture Notes in Computer Science (LNCS), Springer", "summary": "Convolutional networks require extensive image annotation, which can be costly and time-consuming. Feature Learning from Image Markers (FLIM) tackles this challenge by estimating encoder filters (i.e., kernel weights) from user-drawn markers on discriminative regions of a few representative images without traditional optimization. Such an encoder combined with an adaptive decoder comprises a FLIM network fully trained without backpropagation. Prior research has demonstrated their effectiveness in Salient Object Detection (SOD), being significantly lighter than existing lightweight models. This study revisits FLIM SOD and introduces FLIM-Bag of Feature Points (FLIM-BoFP), a considerably faster filter estimation method. The previous approach, FLIM-Cluster, derives filters through patch clustering at each encoder's block, leading to computational overhead and reduced control over filter locations. FLIM-BoFP streamlines this process by performing a single clustering at the input block, creating a bag of feature points, and defining filters directly from mapped feature points across all blocks. The paper evaluates the benefits in efficiency, effectiveness, and generalization of FLIM-BoFP compared to FLIM-Cluster and other state-of-the-art baselines for parasite detection in optical microscopy images.", "AI": {"tldr": "本文提出了FLIM-Bag of Feature Points（FLIM-BoFP），一种改进的滤波器估计方法，用于提高图像标记学习网络的效果和效率。", "motivation": "为了减少传统卷积网络所需的大量图像注释成本与时间消耗，论文通过用户在代表性图像上标注特征区域来简化编码器滤波器的学习过程。在此基础上，本文引入了FLIM-BoFP方法以进一步优化速度和效果。", "method": "新方法首先在输入块进行一次聚类创建特征点集合，然后直接从这些映射的特征点定义所有编码器块中的滤波器，从而简化过滤器估计过程并提高效率。", "result": "实验结果表明，与FLIM-Cluster和其他最先进基准相比，FLIM-BoFP在寄生虫检测任务中表现出更高的效率、有效性和泛化能力。", "conclusion": "通过改进的滤波器学习策略，本文提出的FLIM-BoFP方法不仅提高了模型运行速度，同时保持甚至超过了其他模型的效果。"}}
{"id": "2602.20718", "pdf": "https://arxiv.org/pdf/2602.20718", "abs": "https://arxiv.org/abs/2602.20718", "authors": ["Yangsen Chen", "Hao Wang"], "title": "Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization", "categories": ["cs.CV"], "comment": "ijcnn 2025", "summary": "Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.", "AI": {"tldr": "本文提出了基于3D高斯喷射的单目内窥镜组织三维重建方法，旨在实现光滑可变形表面和实时渲染。", "motivation": "传统的3D高斯喷射方法难以在组织表面重建上保持一致，而现有的NeRF方法缺乏实时渲染能力。为了克服这些挑战并提供高质量的组织重构，本文提出了一种新的方法来解决这些问题。", "method": "该方法首先使用基于符号距离字段的方法构建网格，并利用此网格约束高斯喷射重建过程。通过引入局部刚性和全局非刚性限制，以生成物理上合理的变形，进而改善软内窥镜组织的高度可变形特性。", "result": "定量和定性的分析表明，与替代方法相比，本文提出的方法在纹理和几何形状方面都能实现高质量的重构，并且实现了快速渲染过程和平滑表面外观。", "conclusion": "本研究提出了一种基于3D高斯喷射的新颖方法，通过结合符号距离场网格约束和物理合理性限制，成功地解决了实时内窥镜组织三维重建的问题。"}}
{"id": "2602.20715", "pdf": "https://arxiv.org/pdf/2602.20715", "abs": "https://arxiv.org/abs/2602.20715", "authors": ["Zhian Su", "Weijie Kong", "Haonan Dong", "Huixu Dong"], "title": "IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential for generalist robotic policies; however, they struggle to generalize to long-horizon complex tasks in novel real-world domains due to distribution shifts and the scarcity of high-quality demonstrations. Although reinforcement learning (RL) offers a promising avenue for policy improvement, applying it to real-world VLA fine-tuning faces challenges regarding exploration efficiency, training stability, and sample cost. To address these issues, we propose IG-RFT, a novel Interaction-Guided Reinforced Fine-Tuning system designed for flow-based VLA models. Firstly, to facilitate effective policy optimization, we introduce Interaction-Guided Advantage Weighted Regression (IG-AWR), an RL algorithm that dynamically modulates exploration intensity based on the robot's interaction status. Furthermore, to address the limitations of sparse or task-specific rewards, we design a novel hybrid dense reward function that integrates the trajectory-level reward and the subtask-level reward. Finally, we construct a three-stage RL system comprising SFT, Offline RL, and Human-in-the-Loop RL for fine-tuning VLA models. Extensive real-world experiments on four challenging long-horizon tasks demonstrate that IG-RFT achieves an average success rate of 85.0%, significantly outperforming SFT (18.8%) and standard Offline RL baselines (40.0%). Ablation studies confirm the critical contributions of IG-AWR and hybrid reward shaping. In summary, our work establishes and validates a novel reinforced fine-tuning system for VLA models in real-world robotic manipulation.", "AI": {"tldr": "提出了一种新的交互导向的强化精细调整系统（IG-RFT），用于基于流的VLA模型，以解决在长时间机器人操作任务中的政策优化和奖励设计问题。", "motivation": "现有视觉-语言-动作(VLA)模型在处理长时域复杂任务时面临泛化不足的问题。通过引入交互导向强化学习方法，旨在提高这些模型的学习效率、稳定性和样本利用率。", "method": "提出了一种新的交互导向优势加权回归（IG-AWR）算法，并设计了一个结合轨迹级奖励和子任务级奖励的混合密集型奖赏函数，构建了包含SFT, 离线RL和人机循环RL三个阶段的强化学习系统。", "result": "在四个具有挑战性的长时间域任务上进行的实际实验表明，IG-RFT实现了85.0%的成功率，显著优于仅使用精细调整（18.8％）和标准离线强化学习基线（40.0％）。消融研究表明IG-AWR和混合奖励塑形对于实现这种改进至关重要。", "conclusion": "研究提出并验证了一种新的针对真实世界机器人操作的VLA模型的强化精细调整系统，有效地提高了任务的成功率。"}}
{"id": "2602.20710", "pdf": "https://arxiv.org/pdf/2602.20710", "abs": "https://arxiv.org/abs/2602.20710", "authors": ["Peter Hase", "Christopher Potts"], "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training", "AI": {"tldr": "本文提出了反事实仿真训练（CST）方法，以提高大型语言模型（LLM）的Chain-of-Thought（CoT）忠实度。", "motivation": "检查CoT推理是理解模型输出原因的主要手段之一。然而，CoT忠实度的问题限制了该实践提供的见解。为了解决这一问题，作者提出了一种名为反事实仿真训练的方法。", "method": "CST通过奖励能够使模拟器准确预测模型在反事实输入下的输出的CoT来提高CoT忠实度。这种方法在两个场景下进行了应用：（1）基于提示的反事实监控以检测模型对虚假特征的依赖、作弊行为或奉承倾向；（2）基于通用模型的反事实仿真，鼓励生成更忠实和泛化的推理。", "result": "实验结果显示，在高达2350亿参数量级别的模型上，CST可显著提高基于提示的反事实监控准确性，并提升一般化反事实仿真的模拟性。此外，CST优于提示基线方法且在改进CoT忠实度方面效率是仅使用强化学习的五倍。", "conclusion": "研究表明，CST能够有效改善CoT忠实度，具备在CoT监视中广泛应用的潜力。"}}
{"id": "2602.20709", "pdf": "https://arxiv.org/pdf/2602.20709", "abs": "https://arxiv.org/abs/2602.20709", "authors": ["Riccardo Gallon", "Fabian Schiemenz", "Alessandra Menicucci", "Eberhard Gill"], "title": "Onboard-Targeted Segmentation of Straylight in Space Camera Sensors", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to Aerospace Science and Technology", "summary": "This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.", "AI": {"tldr": "本研究提出了一种基于人工智能的方法，用于空间相机传感器中由太阳辐射引起的杂散光效应的语义分割。", "motivation": "该方法旨在解决由于缺乏真实的空间特定数据而难以实现有效泛化的挑战。通过利用公共数据集进行预训练以提高模型对多种复杂纹理环境下的适应能力。", "method": "使用DeepLabV3模型结合MobileNetV3骨干网络执行分割任务，并针对航天器资源受限的硬件进行了设计优化。", "result": "基于提出的一种特定于系统的评估接口和指标，该研究展示了在真实系统环境下模型的有效性。", "conclusion": "所提出的AI方法能够有效地识别并隔离空间相机中由于太阳辐射引起的杂散光效应，从而提高图像质量。"}}
{"id": "2602.20708", "pdf": "https://arxiv.org/pdf/2602.20708", "abs": "https://arxiv.org/abs/2602.20708", "authors": ["Che Wang", "Fuyao Zhang", "Jiaming Zhang", "Ziqi Zhang", "Yinghui Wang", "Longtao Huang", "Jianbo Gao", "Zhong Chen", "Wei Yang Bryan Lim"], "title": "ICON: Indirect Prompt Injection Defense for Agents based on Inference-Time Correction", "categories": ["cs.AI", "cs.CR"], "comment": "11 pages,", "summary": "Large Language Model (LLM) agents are susceptible to Indirect Prompt Injection (IPI) attacks, where malicious instructions in retrieved content hijack the agent's execution. Existing defenses typically rely on strict filtering or refusal mechanisms, which suffer from a critical limitation: over-refusal, prematurely terminating valid agentic workflows. We propose ICON, a probing-to-mitigation framework that neutralizes attacks while preserving task continuity. Our key insight is that IPI attacks leave distinct over-focusing signatures in the latent space. We introduce a Latent Space Trace Prober to detect attacks based on high intensity scores. Subsequently, a Mitigating Rectifier performs surgical attention steering that selectively manipulate adversarial query key dependencies while amplifying task relevant elements to restore the LLM's functional trajectory. Extensive evaluations on multiple backbones show that ICON achieves a competitive 0.4% ASR, matching commercial grade detectors, while yielding a over 50% task utility gain. Furthermore, ICON demonstrates robust Out of Distribution(OOD) generalization and extends effectively to multi-modal agents, establishing a superior balance between security and efficiency.", "AI": {"tldr": "ICON是一种针对大型语言模型代理间接提示注入攻击的防御框架，通过在推理时进行修正来保护任务连续性。", "motivation": "现有防护措施存在过度拒绝有效工作流的问题，因此提出了ICON以解决这一问题并提高任务效率。", "method": "ICON使用Latent Space Trace Prober检测攻击，并利用Mitigating Rectifier进行针对性修正。", "result": "ICON在多种模型上实现了0.4%的攻击成功率和超过50%的任务效用提升，且具有出类拔萃的安全性和效率平衡。", "conclusion": "ICON为大型语言模型代理提供了有效的间接提示注入防御方法，保证了任务连续性并提高了工作效率。"}}
{"id": "2602.20706", "pdf": "https://arxiv.org/pdf/2602.20706", "abs": "https://arxiv.org/abs/2602.20706", "authors": ["Julien Dallot", "Yuval Emek", "Yuval Gil", "Maciej Pacut", "Stefan Schmid"], "title": "Online Algorithms with Unreliable Guidance", "categories": ["cs.AI", "cs.DS"], "comment": null, "summary": "This paper introduces a new model for ML-augmented online decision making, called online algorithms with unreliable guidance (OAG). This model completely separates between the predictive and algorithmic components, thus offering a single well-defined analysis framework that relies solely on the considered problem. Formulated through the lens of request-answer games, an OAG algorithm receives, with each incoming request, a piece of guidance which is taken from the problem's answer space; ideally, this guidance is the optimal answer for the current request, however with probability $β$, the guidance is adversarially corrupted. The goal is to develop OAG algorithms that admit good competitiveness when $β= 0$ (a.k.a. consistency) as well as when $β= 1$ (a.k.a. robustness); the appealing notion of smoothness, that in most prior work required a dedicated loss function, now arises naturally as $β$ shifts from $0$ to $1$. We then describe a systematic method, called the drop or trust blindly (DTB) compiler, which transforms any online algorithm into a learning-augmented online algorithm in the OAG model. Given a prediction-oblivious online algorithm, its learning-augmented counterpart produced by applying the DTB compiler either follows the incoming guidance blindly or ignores it altogether and proceeds as the initial algorithm would have; the choice between these two alternatives is based on the outcome of a (biased) coin toss. As our main technical contribution, we prove (rigorously) that although remarkably simple, the class of algorithms produced via the DTB compiler includes algorithms with attractive consistency-robustness guarantees for three classic online problems: for caching and uniform metrical task systems our algorithms are optimal, whereas for bipartite matching (with adversarial arrival order), our algorithm outperforms the state-of-the-art.", "AI": {"tldr": "本文介绍了一种新的机器学习增强的在线决策模型，称为具有不可靠指导的在线算法（OAG），并提出了一种简单的方法将任何在线算法转换为增强的学习版本。", "motivation": "该研究旨在解决传统在线算法与预测组件分离的问题，并在不可靠指导下提高算法的一致性和鲁棒性。", "method": "本文提出了一个称为“掉队或盲目信任”（DTB）的编译器，它可以将任何无预测在线算法转换为增强的学习版本，在线算法会在每个请求时选择遵循指导还是忽略它继续运行。", "result": "证明了通过DTB方法生成的一类新算法对于缓存、统一度量任务系统和二部匹配等问题具有吸引人的一致性和鲁棒性保证，其中一些算法在性能上优于现有技术。", "conclusion": "本文提出的在线算法与不可靠指导模型及方法为机器学习增强的决策提供了一种新的视角，并展示了其在解决经典问题中的有效性和优越性。"}}
{"id": "2602.20700", "pdf": "https://arxiv.org/pdf/2602.20700", "abs": "https://arxiv.org/abs/2602.20700", "authors": ["Anna Badalyan", "Pratheba Selvaraju", "Giorgio Becherini", "Omid Taheri", "Victoria Fernandez Abrevaya", "Michael Black"], "title": "NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.", "AI": {"tldr": "提出了一种从单张图像中估计缝纫图案的方法NGL-Prompter，该方法使用自然语言将GarmentCode重新表示，并通过查询大型视觉语言模型来提取结构化的服装参数。", "motivation": "现有方法在处理现实世界中的图像时难以推广，无法捕捉实际的衣物部分关联，且主要适用于单层衣服。因此，提出了一种新的中间语言NGL和一个无需训练的方法NGL-Prompter以解决这些问题。", "method": "通过使用自然语言重新表示GarmentCode并查询大型视觉语言模型来提取结构化的服装参数，然后将这些参数确定性地映射到有效的GarmentCode上。", "result": "该方法在标准几何度量上达到了最先进的性能，并且比现有基线更受人类和GPT感知评估的偏好。此外，NGL-prompter能够恢复多层衣物，展示了其对包含遮挡部分的真实世界图像的强大泛化能力。", "conclusion": "准确的缝纫图案重建无需昂贵的模型训练成为可能，证明了该方法的有效性和实用性。"}}
{"id": "2602.20696", "pdf": "https://arxiv.org/pdf/2602.20696", "abs": "https://arxiv.org/abs/2602.20696", "authors": ["Baolong Bi", "Yuyao Ge", "Shenghua Liu", "Yuchen He", "Siqian Tong", "Lizhe Chen", "Lingrui Mei", "Zehao Li", "Yiwei Wang", "Yujun Cai", "Ming-Hsuan Yang", "Xueqi Cheng"], "title": "PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding", "categories": ["cs.AI"], "comment": null, "summary": "Reliable AI systems require large language models (LLMs) to exhibit behaviors aligned with human preferences and values. However, most existing alignment approaches operate at training time and rely on additional high-quality data, incurring significant computational and annotation costs. While recent work has shown that contrastive decoding can leverage a model's internal distributions to improve specific capabilities, its applicability remains limited to narrow behavioral scopes and scenarios. In this work, we introduce Polarity-Prompt Contrastive Decoding (PromptCD), a test-time behavior control method that generalizes contrastive decoding to broader enhancement settings. PromptCD constructs paired positive and negative guiding prompts for a target behavior and contrasts model responses-specifically token-level probability distributions in LLMs and visual attention patterns in VLMs-to reinforce desirable outcomes. This formulation extends contrastive decoding to a wide range of enhancement objectives and is applicable to both LLMs and Vision-Language Models (VLMs) without additional training. For LLMs, experiments on the \"3H\" alignment objectives (helpfulness, honesty, and harmlessness) demonstrate consistent and substantial improvements, indicating that post-trained models can achieve meaningful self-enhancement purely at test time. For VLMs, we further analyze contrastive effects on visual attention, showing that PromptCD significantly improves VQA performance by reinforcing behavior-consistent visual grounding. Collectively, these results highlight PromptCD as a simple, general, and cost-efficient strategy for reliable behavior control across modalities.", "AI": {"tldr": "本文提出了一种名为Polarity-Prompt Contrastive Decoding (PromptCD) 的方法，在测试时提升大规模语言模型的行为一致性。", "motivation": "现有的大多数对齐方法都在训练时期工作，依赖于高质量的数据，这导致了显著的计算和标注成本。因此，需要一种在测试时可以增强行为一致性的低成本方案。", "method": "PromptCD 构建正负引导提示对以增强目标行为，并通过对比模型响应——即LLMs中的令牌级概率分布和VLMs中的视觉注意力模式来强化期望结果。这种方法适用于不同的增强目标，且无需额外训练即可应用于LLM和VLM。", "result": "实验表明，在“3H”对齐目标上，PromptCD展示了持续显著的改进；对于VQA任务，它通过增强行为一致性的视觉定位进一步提升了性能。", "conclusion": "PromptCD 是一种简单、通用且低成本的方法，可以在多个模式中实现可靠的行为控制。"}}
{"id": "2602.20689", "pdf": "https://arxiv.org/pdf/2602.20689", "abs": "https://arxiv.org/abs/2602.20689", "authors": ["Bedrettin Cetinkaya", "Sinan Kalkan", "Emre Akbas"], "title": "MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026", "summary": "Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \\MethodLPP, a lightweight, only $\\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \\MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \\MethodLPP substantially improves the performance of existing edge detection models. In particular, \\MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \\MethodLPP further boosts baseline performance by up to 20--35\\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.", "AI": {"tldr": "提出了一种轻量级的匹配监督模块MatchED，用于在边缘检测模型中实现端到端的学习，从而生成清晰的一像素宽边缘地图。", "motivation": "现有的边缘检测方法依赖于非可微的手工后处理算法来获取清晰的边缘，这阻碍了端到端优化。因此，作者提出了一个匹配监督模块以克服这一限制。", "method": "MatchED通过在每个训练迭代中基于空间距离和置信度执行预测与真实边界的逐一匹配来实现一致性监督，适用于任何边缘检测模型。", "result": "实验结果表明，集成MatchED显著提升了现有边缘检测模型的性能，在AC指标上提高了2-4倍。在CEval评估下，进一步将基线表现提高20%-35%。", "conclusion": "该模块实现了与标准后处理相匹配或超越的标准，首次实现了无需依赖手工后处理算法来生成清晰边缘的目标。"}}
{"id": "2602.20687", "pdf": "https://arxiv.org/pdf/2602.20687", "abs": "https://arxiv.org/abs/2602.20687", "authors": ["Bo Peng", "Pi Bu", "Keyu Pan", "Xinrun Xu", "Yinxiu Zhao", "Miao Chen", "Yang Du", "Lin Li", "Jun Song", "Tong Xu"], "title": "How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.", "AI": {"tldr": "本文提出NativeEmbodied基准，用于评估基于视觉语言模型的具身智能体在低级别动作空间中的表现。", "motivation": "现有的VLM驱动的具身智能体测试集中于高阶任务，并且依赖非自然设置的动作空间，这与现实世界控制存在差距。需要一个更全面、统一的任务和技能评估基准来解决这些问题。", "method": "构建了一个基于多样模拟场景的NativeEmbodied基准，包括三个代表性的复杂场景中的高阶任务以及四种低级任务类型以解构核心具身能力，并使用最先进的VLM进行了实验。", "result": "实验显示当前模型在几个基本具身技能上存在明显缺陷，这些瓶颈严重限制了它们完成高级别任务的能力。", "conclusion": "NativeEmbodied揭示了现有基于视觉语言模型的具身智能体的关键挑战并为未来的改进提供了见解。"}}
{"id": "2602.20685", "pdf": "https://arxiv.org/pdf/2602.20685", "abs": "https://arxiv.org/abs/2602.20685", "authors": ["Yichen Xie", "Chensheng Peng", "Mazen Abdelfattah", "Yihan Hu", "Jiezhi Yang", "Eric Higgins", "Ryan Brigden", "Masayoshi Tomizuka", "Wei Zhan"], "title": "RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026; Project website: http://yichen928.github.io/raynova", "summary": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.", "AI": {"tldr": "提出了一种几何无关的自回归世界模型RAYNOVA，用于多视角视频生成。", "motivation": "为了克服现有方法在处理空间和时间相关性时存在的局限性，并提供对不同相机设置和自我运动的强大泛化能力，作者提出了RAYNOVA。", "method": "采用双因果自回归框架，利用全局注意力机制进行统一的时空推理。基于相对Plücker-光线位置编码构建等向时空表示。引入循环训练范式以解决长时间视频生成中的分布漂移问题。", "result": "在nuScenes数据集上实现了多视角视频生成的最新结果，并具有更高的吞吐量和强大的可控性，在没有明确3D场景表示的情况下对新颖视图和相机配置进行泛化。", "conclusion": "RAYNOVA通过统一的时空表示，提高了自回归世界模型的性能，特别是在不同相机设置下的鲁棒性和泛化能力。"}}
{"id": "2602.20684", "pdf": "https://arxiv.org/pdf/2602.20684", "abs": "https://arxiv.org/abs/2602.20684", "authors": ["Christopher Koch", "Joshua Andreas Wellbrock"], "title": "Agile V: A Compliance-Ready Framework for AI-Augmented Engineering -- From Concept to Audit-Ready Delivery", "categories": ["cs.SE", "cs.AI", "cs.MA"], "comment": "9 pages, 2 figures", "summary": "Current AI-assisted engineering workflows lack a built-in mechanism to maintain task-level verification and regulatory traceability at machine-speed delivery. Agile V addresses this gap by embedding independent verification and audit artifact generation into each task cycle. The framework merges Agile iteration with V-Model verification into a continuous Infinity Loop, deploying specialized AI agents for requirements, design, build, test, and compliance, governed by mandatory human approval gates. We evaluate three hypotheses: (H1) audit-ready artifacts emerge as a by-product of development, (H2) 100% requirement-level verification is achievable with independent test generation, and (H3) verified increments can be delivered with single-digit human interactions per cycle. A feasibility case study on a Hardware-in-the-Loop system (about 500 LOC, 8 requirements, 54 tests) supports all three hypotheses: audit-ready documentation was generated automatically (H1), 100% requirement-level pass rate was achieved (H2), and only 6 prompts per cycle were required (H3), yielding an estimated 10-50x cost reduction versus a COCOMO II baseline (sensitivity range from pessimistic to optimistic assumptions). We invite independent replication to validate generalizability.", "AI": {"tldr": "提出了一种名为Agile V的框架，用于实现AI增强工程中的任务级验证和合规跟踪", "motivation": "当前AI辅助的工程工作流程缺乏内置机制来在机器速度交付中维持任务级别的验证和监管可追溯性。为了解决这个问题，引入了Agile V框架", "method": "通过将敏捷迭代与V模型验证合并成一个连续的Infinity Loop，并使用专为需求、设计、构建、测试和合规部署的AI代理进行操作，同时需要强制的人类审批门控来管理流程", "result": "案例研究支持了所有三个假设：自动生成了审计就绪文档（H1），实现了100％的需求级别通过率（H2）以及每个周期仅需6次提示（H3），从而带来了相对于COCOMO II基准的估计成本降低率为10-50倍", "conclusion": "Agile V框架能够在不牺牲质量的情况下显著减少工程项目的交付时间和成本，同时保持审计就绪状态"}}
{"id": "2602.20677", "pdf": "https://arxiv.org/pdf/2602.20677", "abs": "https://arxiv.org/abs/2602.20677", "authors": ["Wei Chen", "Yuqian Wu", "Junle Chen", "Xiaofang Zhou", "Yuxuan Liang"], "title": "UrbanFM: Scaling Urban Spatio-Temporal Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Urban systems, as dynamic complex systems, continuously generate spatio-temporal data streams that encode the fundamental laws of human mobility and city evolution. While AI for Science has witnessed the transformative power of foundation models in disciplines like genomics and meteorology, urban computing remains fragmented due to \"scenario-specific\" models, which are overfitted to specific regions or tasks, hindering their generalizability. To bridge this gap and advance spatio-temporal foundation models for urban systems, we adopt scaling as the central perspective and systematically investigate two key questions: what to scale and how to scale. Grounded in first-principles analysis, we identify three critical dimensions: heterogeneity, correlation, and dynamics, aligning these principles with the fundamental scientific properties of urban spatio-temporal data. Specifically, to address heterogeneity through data scaling, we construct WorldST. This billion-scale corpus standardizes diverse physical signals, such as traffic flow and speed, from over 100 global cities into a unified data format. To enable computation scaling for modeling correlations, we introduce the MiniST unit, a novel split mechanism that discretizes continuous spatio-temporal fields into learnable computational units to unify representations of grid-based and sensor-based observations. Finally, addressing dynamics via architecture scaling, we propose UrbanFM, a minimalist self-attention architecture designed with limited inductive biases to autonomously learn dynamic spatio-temporal dependencies from massive data. Furthermore, we establish EvalST, the largest-scale urban spatio-temporal benchmark to date. Extensive experiments demonstrate that UrbanFM achieves remarkable zero-shot generalization across unseen cities and tasks, marking a pivotal first step toward large-scale urban spatio-temporal foundation models.", "AI": {"tldr": "研究提出UrbanFM，一种用于城市时空数据的基础模型。", "motivation": "现有城市计算模型过于特定场景化，难以泛化。通过规模化视角探讨如何构建适用于城市的时空基础模型。", "method": "基于三个维度：异质性、相关性和动态性，构建WorldST大规模城市时空数据集；提出MiniST单元处理连续时空域；设计UrbanFM最小化自注意力架构以学习动态依赖关系。", "result": "UrbanFM在未见过的城市和任务中展现出零样本泛化能力。", "conclusion": "该研究为大规模城市时空基础模型的发展奠定了重要基础。"}}
{"id": "2602.20676", "pdf": "https://arxiv.org/pdf/2602.20676", "abs": "https://arxiv.org/abs/2602.20676", "authors": ["Shuzhi Cao", "Rong Chen", "Ailong He", "Shuguang Han", "Jufeng Chen"], "title": "PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization", "categories": ["cs.IR", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2503.18395", "summary": "In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.", "AI": {"tldr": "本文提出了PRECTR-V2框架，旨在解决低活跃用户个性化建模、曝光偏差和模型优化的问题。", "motivation": "为了提高搜索系统的性能，在前作基础上进一步解决了三个主要问题：冷启动用户的个性化偏好建模困难；高相关性样本导致的训练数据分布不一致；模型延迟限制下的Emb+MLP架构无法实现联合优化。", "method": "通过挖掘全局相关性偏好解决低活跃用户稀疏行为问题，构建硬负样本并通过对比损失进行相对排名优化以纠正曝光偏差。引入轻量级Transformer编码器并利用知识蒸馏技术增强模型适应CTR微调的能力。", "result": "该方法有效缓解了冷启动用户个性化建模难题，并通过优化训练过程提高了整体性能。", "conclusion": "PRECTR-V2框架在解决上述问题上取得显著效果，能够更好地满足搜索系统中的相关性和点击率预测需求。"}}
{"id": "2602.20673", "pdf": "https://arxiv.org/pdf/2602.20673", "abs": "https://arxiv.org/abs/2602.20673", "authors": ["Hao Zhang", "Lue Fan", "Qitai Wang", "Wenbo Li", "Zehuan Wu", "Lewei Lu", "Zhaoxiang Zhang", "Hongsheng Li"], "title": "GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio", "categories": ["cs.CV"], "comment": null, "summary": "A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.", "AI": {"tldr": "提出了一种用于生成自由视角驾驶场景的模拟器GA-Drive，通过几何和外观解耦的方法提高驾驶模拟的真实感。", "motivation": "为了训练和完善端到端自动驾驶系统，需要一个能够生成高真实度、可编辑视图的驾驶仿真器。", "method": "通过几何与外观信息的分离来合成新的视角，并利用视频扩散模型将这些合成视角转化为逼真的图像。", "result": "实验结果表明，在NTA-IoU, NTL-IoU和FID评分上，GA-Drive方法显著优于现有的技术手段。", "conclusion": "通过几何外观解耦的方法，不仅提高了驾驶场景模拟的真实感，还增强了对驾驶环境进行编辑的能力。"}}
{"id": "2602.20672", "pdf": "https://arxiv.org/pdf/2602.20672", "abs": "https://arxiv.org/abs/2602.20672", "authors": ["Eliran Kachlon", "Alexander Visheratin", "Nimrod Sarid", "Tal Hacham", "Eyal Gutflaish", "Saar Huberman", "Hezi Zisman", "David Ruppin", "Ron Mokady"], "title": "BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.", "AI": {"tldr": "本文介绍了一种名为BBQ的大型文本到图像模型，该模型可以直接利用数值边界框和RGB三元组进行精确的空间和色彩控制。", "motivation": "现有的文本到图像生成模型依赖于描述性的语言输入，而专业工作流程需要通过精确的数字参数来控制对象的位置、大小和颜色。本文提出了BBQ模型以解决这一问题，并支持更直观的操作界面如拖拽物体和选择颜色。", "method": "该方法在增强的带注释说明的标题基础上进行训练，使用户意图翻译为中间结构化语言，由基于流的变压器作为渲染器使用，自然地容纳数字参数。模型不需要架构上的修改或推理时间优化。", "result": "实验结果显示BBQ模型能够实现较强的边界框对齐和RGB颜色保真度，在这两方面优于最先进的基准。", "conclusion": "这项研究表明通过将用户意图转化为中间结构化语言并使用基于流的变压器渲染器，可以有效地解决精确的数字控制问题，并支持更自然的操作界面。"}}
{"id": "2602.20670", "pdf": "https://arxiv.org/pdf/2602.20670", "abs": "https://arxiv.org/abs/2602.20670", "authors": ["Zirui Zhu", "Hailun Xu", "Yang Luo", "Yong Liu", "Kanchan Sarkar", "Kun Xu", "Yang You"], "title": "CAMEL: Confidence-Gated Reflection for Reward Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 13 pages", "summary": "Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.", "AI": {"tldr": "提出了一种新的奖励模型CAMEL，通过自信度门控机制减少计算开销同时提高准确率。", "motivation": "现有的方法在效率和解释性之间难以平衡，而CAMEL旨在结合两者优势，提升预测准确性的同时降低计算成本。", "method": "首先执行轻量级单令牌偏好决策，然后通过强化学习与反事实前缀增强技术训练模型进行自我修正。", "result": "实验显示，CAMEL在三个广泛使用的奖励模型基准测试中取得了82.9%的平均准确率，超越了先前的最佳模型3.2%，并且使用14B参数就超过了70B参数模型的表现。", "conclusion": "通过自信度门控和强化学习技术，CAMEL不仅提高了预测准确性，还优化了计算效率。"}}
{"id": "2602.20666", "pdf": "https://arxiv.org/pdf/2602.20666", "abs": "https://arxiv.org/abs/2602.20666", "authors": ["Juil Koo", "Wei-Tung Lin", "Chanho Park", "Chanhyeok Park", "Minhyuk Sung"], "title": "BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity", "categories": ["cs.CV"], "comment": "Project page: https://boxsplitgen.github.io", "summary": "Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.", "AI": {"tldr": "提出了一种框架，通过迭代分割边界框来生成具有不同粒度的3D部件边界框。", "motivation": "当前缺乏专门为辅助人类想象在3D创建过程中从粗糙到精细细化抽象而设计的模型。因此，本文旨在开发一个直观且交互式的3D形状生成系统。", "method": "该框架包含两个主要部分：BoxSplitGen和箱形转形状的生成模型。BoxSplitGen通过学习边界框分裂序列来生成具有不同粒度的3D部件边界框。第二部分基于现有的3D扩散模型，但经过调整以结合边界框条件。", "result": "实验表明，该方法中的分割生成模型优于令牌预测模型和无条件扩散模型的上色法。同时，基于现有最先进3D扩散模型的箱形转形状模型提供了更好的结果。", "conclusion": "通过迭代分裂边界框生成具有不同粒度的3D部件边界框的方法优于其他模型，并为辅助人类想象在从粗糙到精细细化抽象方面提供了一个新的方向。"}}
{"id": "2602.20664", "pdf": "https://arxiv.org/pdf/2602.20664", "abs": "https://arxiv.org/abs/2602.20664", "authors": ["Hailong Yan", "Shice Liu", "Tao Wang", "Xiangtao Zhang", "Yijie Zhong", "Jinwei Chen", "Le Zhang", "Bo Li"], "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?", "categories": ["cs.CV"], "comment": "Tech Report", "summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.", "AI": {"tldr": "AnimeAgent利用图像到视频的模型，提出了第一个用于自定义故事板生成（CSG）的多代理框架。", "motivation": "当前基于静态扩散模型的方法在动态表达性、迭代纠正和评估方面存在局限性。为了解决这些问题，作者设计了AnimeAgent。", "method": "AnimeAgent结合图像到视频转换模型，利用其隐含的动作先验来增强一致性与表现力，并采用混合主观客观评价机制进行可靠的迭代改进。", "result": "实验表明，AnimeAgent在故事板生成的一致性、提示忠实度和风格化方面达到了最新的技术水平。", "conclusion": "通过引入图像到视频转换的多代理框架，AnimeAgent能够有效提高自定义故事板生成的质量。"}}
{"id": "2602.20659", "pdf": "https://arxiv.org/pdf/2602.20659", "abs": "https://arxiv.org/abs/2602.20659", "authors": ["Vaidehi Bagaria", "Bijo Sebastian", "Nirav Patel"], "title": "Recursive Belief Vision Language Model", "categories": ["cs.AI"], "comment": null, "summary": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.", "AI": {"tldr": "本文介绍了RB-VLA模型，一种基于信念的架构，用于解决长时域操作下的部分可观测性问题。", "motivation": "当前的视觉-语言-行动模型在处理长时域的操作任务时面临挑战，特别是由于缺乏持久、动作条件的状态表示和有限的时间及物理推理能力。", "method": "RB-VLA通过自我监督的世界建模目标训练出一种信念中心架构，该架构保持了一个紧凑的任务相关历史、动态以及物体交互的隐式状态编码。VLM仅被询问一次以提供高层次意图，而信念跟踪任务进度并支持在部分观察下进行因果控制。", "result": "RB-VLA在多阶段抓取和堆叠任务上分别比基线模型高出52.5%和37.5%，同时减少了最多五倍的推理延迟，并且没有随着时间步长增长的记忆开销。", "conclusion": "实验表明，基于信念的状态表示对于长期视觉-语言-行动策略的有效性至关重要。"}}
{"id": "2602.20658", "pdf": "https://arxiv.org/pdf/2602.20658", "abs": "https://arxiv.org/abs/2602.20658", "authors": ["Mohammad Sadra Rajabi", "Aanuoluwapo Ojelade", "Sunwook Kim", "Maury A. Nussbaum"], "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.", "AI": {"tldr": "本文研究了使用视觉语言模型从RGB视频中估算手动搬运任务中的水平和垂直手部距离的可行性。", "motivation": "手动搬运任务是工作相关肌肉骨骼疾病的主要原因，有效的风险评估对于量化物理暴露和指导干预措施至关重要。传统的测量方法难以在实际环境中应用。", "method": "本文开发了两个基于视觉语言模型（VLM）的多阶段管道：文本引导检测管道和检测加分割管道，利用视频流估算水平和垂直手部距离。", "result": "结果表明，分割基础的多视角管道在估计水平距离上平均绝对误差为6-8厘米，在垂直距离上约为5-8厘米。分割方法比仅检测的方法减少了20%-40%的误差。", "conclusion": "研究证实了基于视觉语言模型的视频估算RNLE参数的可能性，支持其在实际环境中的应用。"}}
{"id": "2602.20653", "pdf": "https://arxiv.org/pdf/2602.20653", "abs": "https://arxiv.org/abs/2602.20653", "authors": ["Xiaokai Bai", "Jiahao Cheng", "Songkai Wang", "Yixuan Luo", "Lianqing Zheng", "Xiaohan Zhang", "Si-Yuan Cao", "Hui-Liang Shen"], "title": "SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar", "categories": ["cs.CV"], "comment": "7 pages, 5 figures, 4 tables", "summary": "4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.", "AI": {"tldr": "SD4R框架通过生成密集的点云表示来解决雷达测量中的稀疏性和噪声问题，从而提高三维物体检测的准确性。", "motivation": "当前的雷达点云增密方法难以处理极端稀疏的情况，并且在点数较少时表现不佳。因此，提出了一种新的框架SD4R以增强3D感知能力。", "method": "首先使用前景点生成器（FPG）减少噪声传播并产生密集化的点云；然后利用对数查询编码器（LQE），改进传统的柱状体化过程，从而得到鲁棒性的特征表示。", "result": "在公开的View-of-Delft数据集上的实验表明，SD4R实现了最先进的性能。", "conclusion": "通过创新的方法，SD4R展示了强大的降噪和前景点密集化的功能，并且在3D物体检测方面表现出色。"}}
{"id": "2602.20650", "pdf": "https://arxiv.org/pdf/2602.20650", "abs": "https://arxiv.org/abs/2602.20650", "authors": ["Chenyue Yu", "Lingao Xiao", "Jinhong Deng", "Ivor W. Tsang", "Yang He"], "title": "Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR 2026", "summary": "Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \\href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.", "AI": {"tldr": "提出了一种名为Dataset Color Quantization (DCQ) 的框架，用于压缩大规模图像数据集。", "motivation": "大型图像数据集虽然对深度学习至关重要，但其高存储需求在资源受限的环境中带来了挑战。现有方法通过删除样本来减少数据集大小，却忽视了每个图像内部存在的大量冗余，特别是在颜色空间中。DCQ旨在解决这个问题。", "method": "DCQ框架通过在整个数据集中强制执行一致的颜色调色板表示、选择性地保留模型感知的重要语义色彩以及保持对特征学习至关重要的结构细节来压缩视觉数据集。", "result": "在CIFAR-10, CIFAR-100, Tiny-ImageNet和ImageNet-1K上进行了大量实验，结果显示DCQ即使在高程度的压缩下也能显著提高训练性能。", "conclusion": "提出了一种新的数据集级别存储减少方法——Dataset Color Quantization (DCQ)，它提供了一个可扩展且稳健的解决方案。"}}
{"id": "2602.20645", "pdf": "https://arxiv.org/pdf/2602.20645", "abs": "https://arxiv.org/abs/2602.20645", "authors": ["Keisuke Takeshita", "Takahiro Yamazaki", "Tomohiro Ono", "Takashi Yamamoto"], "title": "Robot Local Planner: A Periodic Sampling-Based Motion Planner with Minimal Waypoints for Home Environments", "categories": ["cs.RO"], "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2025. Project Page: https://toyotafrc.github.io/RobotLocalPlanner-Proj/", "summary": "The objective of this study is to enable fast and safe manipulation tasks in home environments. Specifically, we aim to develop a system that can recognize its surroundings and identify target objects while in motion, enabling it to plan and execute actions accordingly. We propose a periodic sampling-based whole-body trajectory planning method, called the \"Robot Local Planner (RLP).\" This method leverages unique features of home environments to enhance computational efficiency, motion optimality, and robustness against recognition and control errors, all while ensuring safety. The RLP minimizes computation time by planning with minimal waypoints and generating safe trajectories. Furthermore, overall motion optimality is improved by periodically executing trajectory planning to select more optimal motions. This approach incorporates inverse kinematics that are robust to base position errors, further enhancing robustness. Evaluation experiments demonstrated that the RLP outperformed existing methods in terms of motion planning time, motion duration, and robustness, confirming its effectiveness in home environments. Moreover, application experiments using a tidy-up task achieved high success rates and short operation times, thereby underscoring its practical feasibility.", "AI": {"tldr": "本文提出了一种适用于家庭环境的快速安全机器人局部路径规划方法，通过最小化航点并定期执行轨迹规划来提高运动优化和鲁棒性。", "motivation": "为了实现在家庭环境中进行高效且安全的操作任务，研究开发了一种能够在移动中识别周围环境及目标物体，并据此制定动作计划的方法。", "method": "提出了一个基于周期采样的机器人局部路径规划器（RLP），利用家用环境的特性来提高计算效率、运动优化以及对感知和控制错误的鲁棒性。通过使用最少的航点减少计算时间，同时采用逆动力学算法增强基座位置误差下的鲁棒性。", "result": "实验结果表明，相对于现有方法，RLP在规划时间和动作时长方面表现更优，并且具有更高的鲁棒性；清理任务应用实验证明了该系统在家庭环境中的实际可行性。", "conclusion": "提出的机器人局部路径规划器（RLP）能够有效提高家用环境中机器人的运动优化和操作成功率，证明其具备实际的应用价值。"}}
{"id": "2602.20643", "pdf": "https://arxiv.org/pdf/2602.20643", "abs": "https://arxiv.org/abs/2602.20643", "authors": ["Jiawei Wang", "Chuang Yang", "Jiawei Yong", "Xiaohang Xu", "Hongjun Wang", "Noboru Koshizuka", "Shintaro Fukushima", "Ryosuke Shibasaki", "Renhe Jiang"], "title": "TrajGPT-R: Generating Urban Mobility Trajectory with Reinforcement Learning-Enhanced Generative Pre-trained Transformer", "categories": ["cs.LG", "cs.AI"], "comment": "TrajGPT-R is a Reinforcement Learning-Enhanced Generative Pre-trained Transformer for Mobility Trajectory Generation", "summary": "Mobility trajectories are essential for understanding urban dynamics and enhancing urban planning, yet access to such data is frequently hindered by privacy concerns. This research introduces a transformative framework for generating large-scale urban mobility trajectories, employing a novel application of a transformer-based model pre-trained and fine-tuned through a two-phase process. Initially, trajectory generation is conceptualized as an offline reinforcement learning (RL) problem, with a significant reduction in vocabulary space achieved during tokenization. The integration of Inverse Reinforcement Learning (IRL) allows for the capture of trajectory-wise reward signals, leveraging historical data to infer individual mobility preferences. Subsequently, the pre-trained model is fine-tuned using the constructed reward model, effectively addressing the challenges inherent in traditional RL-based autoregressive methods, such as long-term credit assignment and handling of sparse reward environments. Comprehensive evaluations on multiple datasets illustrate that our framework markedly surpasses existing models in terms of reliability and diversity. Our findings not only advance the field of urban mobility modeling but also provide a robust methodology for simulating urban data, with significant implications for traffic management and urban development planning. The implementation is publicly available at https://github.com/Wangjw6/TrajGPT_R.", "AI": {"tldr": "利用强化学习增强的生成预训练变换器模型，提出了一种生成大规模城市移动轨迹的新框架。", "motivation": "为了解决由于隐私问题而难以获得的城市动态数据问题，并通过模拟大量真实轨迹来改善城市规划和管理。", "method": "首先将轨迹生成定义为一个离线强化学习问题，在词汇空间中进行显著减少。然后使用逆向强化学习捕获奖励信号，基于历史数据推断个人移动偏好。最后，使用构建的奖励模型对预训练模型进行微调。", "result": "通过多个数据集上的全面评估证明了该框架在可靠性和多样性方面优于现有模型。", "conclusion": "该研究提出了一个新的生成大规模城市移动轨迹的方法，这对城市动态建模和模拟具有重要影响。"}}
{"id": "2602.20639", "pdf": "https://arxiv.org/pdf/2602.20639", "abs": "https://arxiv.org/abs/2602.20639", "authors": ["Bo Zhang", "Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Minlie Huang", "Hongning Wang"], "title": "Grounding LLMs in Scientific Discovery via Embodied Actions", "categories": ["cs.AI"], "comment": "24 pages, 7 figures, 7 tables. Preprint", "summary": "Large Language Models (LLMs) have shown significant potential in scientific discovery but struggle to bridge the gap between theoretical reasoning and verifiable physical simulation. Existing solutions operate in a passive \"execute-then-response\" loop and thus lacks runtime perception, obscuring agents to transient anomalies (e.g., numerical instability or diverging oscillations). To address this limitation, we propose EmbodiedAct, a framework that transforms established scientific software into active embodied agents by grounding LLMs in embodied actions with a tight perception-execution loop. We instantiate EmbodiedAct within MATLAB and evaluate it on complex engineering design and scientific modeling tasks. Extensive experiments show that EmbodiedAct significantly outperforms existing baselines, achieving SOTA performance by ensuring satisfactory reliability and stability in long-horizon simulations and enhanced accuracy in scientific modeling.", "AI": {"tldr": "本文提出了一种名为EmbodiedAct的框架，该框架通过将大型语言模型（LLMs）嵌入到具有紧密感知执行循环中的实体行动中来解决科学发现中的理论与物理验证之间的差距。", "motivation": "现有的解决方案在被动的“执行-响应”循环中运行，缺乏实时感知能力，无法及时识别和处理瞬时异常情况。本文旨在通过EmbodiedAct框架来改善这一现状，使LLMs能够更好地参与实际模拟并提高可靠性。", "method": "EmbodiedAct将已有的科学软件转换为具有实体行动的代理，实现理论与实践之间的紧密交互，并在MATLAB环境中进行实验和验证。", "result": "通过复杂工程设计和科学研究任务的大量测试，EmbodiedAct显著优于现有基线模型，在长时间模拟中的可靠性和稳定性以及科学建模精度上都达到了最先进的水平。", "conclusion": "EmbodiedAct框架有效地解决了大型语言模型在实际应用中面临的问题，提高了LLMs参与科学发现的能力。"}}
{"id": "2602.20638", "pdf": "https://arxiv.org/pdf/2602.20638", "abs": "https://arxiv.org/abs/2602.20638", "authors": ["Vincent Auriau", "Khaled Belahcene", "Emmanuel Malherbe", "Vincent Mousseau", "Marc Pirlot"], "title": "Identifying two piecewise linear additive value functions from anonymous preference information", "categories": ["cs.AI"], "comment": null, "summary": "Eliciting a preference model involves asking a person, named decision-maker, a series of questions. We assume that these preferences can be represented by an additive value function. In this work, we query simultaneously two decision-makers in the aim to elicit their respective value functions. For each query we receive two answers, without noise, but without knowing which answer corresponds to which decision-maker.We propose an elicitation procedure that identifies the two preference models when the marginal value functions are piecewise linear with known breaking points.", "AI": {"tldr": "本文提出了一种从匿名偏好信息中同时识别两个分段线性加法效用函数的启发式程序。", "motivation": "为了更有效地获取决策者的偏好模型，需要开发一种能够处理两个决策者匿名偏好的方法。", "method": "通过设计一个查询过程，在未知每个答案对应哪个决策者的前提下，识别出两个分段线性加法效用函数。", "result": "提出的方法能够在已知边际价值函数为分段线性的条件下成功地同时确定两个偏好模型。", "conclusion": "该研究提供了一种有效的策略，能够从匿名偏好的信息中准确地识别多个决策者的偏好模型。"}}
{"id": "2602.20636", "pdf": "https://arxiv.org/pdf/2602.20636", "abs": "https://arxiv.org/abs/2602.20636", "authors": ["Rulin Zhou", "Guankun Wang", "An Wang", "Yujie Ma", "Lixin Ouyang", "Bolin Cui", "Junyan Li", "Chaowei Zhu", "Mingyang Li", "Ming Chen", "Xiaopin Zhong", "Peng Lu", "Jiankun Wang", "Xianming Liu", "Hongliang Ren"], "title": "SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.", "AI": {"tldr": "提出了一种名为SurgAtt-Tracker的框架，通过时间提案重排序和运动感知细化来稳健地跟踪手术注意力。", "motivation": "现有的方法在估计视觉注意时往往与下游相机控制结合或直接基于对象中心假设。因此，需要一种更准确、稳定的视野指引方法来支持微创手术的安全性和效率。", "method": "SurgAtt-Tracker通过时间连贯性利用提案级重排序和运动感知细化，而不是直接回归，稳健地跟踪手术注意力，并引入了名为SurgAtt-1.16M的大型基准数据集以系统化训练与评估。", "result": "在多个外科数据集上的广泛实验表明，SurgAtt-Tracker能够实现最先进的性能并表现出很强的鲁棒性，在遮挡、多器械干扰和跨域设置下均表现良好。", "conclusion": "该方法不仅实现了准确的注意力跟踪，还提供了帧级别的视野指引信号，可以直接支持下游机器人视场规划和自动相机控制。"}}
{"id": "2602.20634", "pdf": "https://arxiv.org/pdf/2602.20634", "abs": "https://arxiv.org/abs/2602.20634", "authors": ["Saurabh Mishra", "Shivani Thakur", "Radhika Mamidi"], "title": "Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches", "categories": ["cs.CL", "cs.AI"], "comment": "32 pages, 24 figures", "summary": "The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.", "AI": {"tldr": "研究评估了各种机器学习模型在识别网络仇恨言论和不当语言方面的效果，并探讨了文本转换技术的潜力。", "motivation": "社交媒体平台上仇恨言论的增加，需要开发有效的检测和管理系统。", "method": "比较传统CNN、LSTM模型与BERT等先进神经网络模型及结合不同架构特性的混合模型；引入将负面表达转化为中性表述的技术以减轻有害内容的影响。", "result": "结果显示，尽管高级模型如BERT在深层语境理解方面表现更佳，但某些场景下混合模型能力更强；文本转换技术有潜在的缓和影响作用。", "conclusion": "讨论了研究发现的意义，并指出现有技术的优势与局限性，提出未来更强大的仇恨言论检测系统的方向。"}}
{"id": "2602.20632", "pdf": "https://arxiv.org/pdf/2602.20632", "abs": "https://arxiv.org/abs/2602.20632", "authors": ["Xiaokai Bai", "Lianqing Zheng", "Si-Yuan Cao", "Xiaohan Zhang", "Zhe Wu", "Beinan Yu", "Fang Wang", "Jie Bai", "Hui-Liang Shen"], "title": "Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection", "categories": ["cs.CV"], "comment": "14 pages, 10 figures, 13 tables", "summary": "4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.", "AI": {"tldr": "提出SIFormer模型，结合4D雷达和相机数据进行三维物体检测。", "motivation": "现有雷达-相机融合方法在稀疏且弱几何线索的雷达数据下难以激活实例。BEV级融合缺乏对具体对象的关注，而视角级别融合则缺少全局上下文信息。", "method": "SIFormer通过分割和深度引导定位抑制背景噪声，并引入跨视图激活机制将2D实例线索注入到鸟瞰视图空间中，同时采用基于变换器的融合模块聚合图像语义和雷达几何信息。", "result": "实验结果表明，在View-of-Delft, TJ4DRadSet 和NuScenes数据集上SIFormer实现了最先进的性能。", "conclusion": "通过结合BEV级与视角级别方法的优势，SIFormer有效解决了现有雷达-相机融合方法中存在的问题，并显著提升了三维物体检测的准确性。"}}
{"id": "2602.20630", "pdf": "https://arxiv.org/pdf/2602.20630", "abs": "https://arxiv.org/abs/2602.20630", "authors": ["Yepeng Liu", "Hao Li", "Liwen Yang", "Fangzhen Li", "Xudi Ge", "Yuliang Gu", "kuang Gao", "Bing Wang", "Guang Chen", "Hangjun Ye", "Yongchao Xu"], "title": "From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \\textbf{Tra}ck-\\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.", "AI": {"tldr": "本文提出了一种基于强化学习的端到端框架TraqPoint，用于优化关键点在图像序列中的跟踪质量。", "motivation": "现有的基于学习的关键点检测方法主要针对成对图像进行训练，无法显式地优化关键点在序列中长期跟踪的质量。因此，本文希望改进这个问题。", "method": "本文将关键点检测视为序列决策问题，并提出了一种端到端的强化学习框架TraqPoint，通过引入一种跟踪感知的奖励机制来鼓励关键点的一致性和区分度。", "result": "实验结果表明，TraqPoint在稀疏匹配基准测试中显著优于一些最先进的关键点检测和描述方法。", "conclusion": "本文提出了一种基于强化学习的新框架TraqPoint，它能够直接优化图像序列中的关键点跟踪质量，并通过实验证明了其有效性。"}}
{"id": "2602.20628", "pdf": "https://arxiv.org/pdf/2602.20628", "abs": "https://arxiv.org/abs/2602.20628", "authors": ["Nelson Gardner-Challis", "Jonathan Bostock", "Georgiy Kozhevnikov", "Morgan Sinclaire", "Joan Velja", "Alessandro Abate", "Charlie Griffin"], "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies", "categories": ["cs.AI"], "comment": "66 pages, 14 figures, Preprint", "summary": "AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.", "AI": {"tldr": "该论文探讨了使用未受信任监控来降低自主AI造成灾难性危害风险的可行性，并开发了一个安全案例草图。", "motivation": "随着人工智能能力的增长，需要评估和减少因部署未对齐的人工智能而导致的风险。传统的未受信任监测方法假设有限的阴谋策略，这限制了其适用性和有效性。", "method": "论文提出了一个更广泛的阴谋策略分类，并构建了一个安全案例草图，明确陈述了所有假设并解决了先前研究中的挑战。", "result": "该研究发现被动自我认知可能比之前研究过的策略更有效。同时提供了评估未受信任监测的条件。", "conclusion": "通过改进对未受信任监控的研究方法，论文为安全案例分析提供了一个框架，并促进了更加稳健的安全性评价实践。"}}
{"id": "2602.20627", "pdf": "https://arxiv.org/pdf/2602.20627", "abs": "https://arxiv.org/abs/2602.20627", "authors": ["Zhaonian Kuang", "Rui Ding", "Meng Yang", "Xinhu Zheng", "Gang Hua"], "title": "Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": "IJCV", "summary": "Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.", "AI": {"tldr": "本文提出了一种在线对象场景相机分解和重组的数据处理方案，以更有效地利用训练数据。", "motivation": "由于人类偏见导致的对象、场景与摄像机姿态的紧密耦合，单目三维物体检测模型难以充分利用多样化训练数据并避免过拟合问题。为此，本文提出了一种解决方案以提高训练数据的有效性。", "method": "首先将训练图像分解成3D对象点模型和背景场景，然后通过插入不同姿态下的3D对象到新的训练场景中，并重新渲染这些场景来生成新的多样化的训练集。", "result": "该方法在五个代表性的单目三维物体检测模型上进行了广泛的验证，评估结果表明其性能优于基线模型，在KITTI和Waymo数据集中表现出色。", "conclusion": "本文提出的数据处理方案能够灵活应用于不同监督方式下的单目三维物体检测任务，并能显著提高模型的性能。"}}
{"id": "2602.20624", "pdf": "https://arxiv.org/pdf/2602.20624", "abs": "https://arxiv.org/abs/2602.20624", "authors": ["Hyeongmo Kim", "Sohyun Kang", "Yerin Choi", "Seungyeon Ji", "Junhyuk Woo", "Hyunsuk Chung", "Soyeon Caren Han", "Kyungreem Han"], "title": "Physics-based phenomenological characterization of cross-modal bias in multimodal models", "categories": ["cs.AI", "cond-mat.stat-mech"], "comment": "Best Paper Award at BiasinAI track in AAAI2026", "summary": "The term 'algorithmic fairness' is used to evaluate whether AI models operate fairly in both comparative (where fairness is understood as formal equality, such as \"treat like cases as like\") and non-comparative (where unfairness arises from the model's inaccuracy, arbitrariness, or inscrutability) contexts. Recent advances in multimodal large language models (MLLMs) are breaking new ground in multimodal understanding, reasoning, and generation; however, we argue that inconspicuous distortions arising from complex multimodal interaction dynamics can lead to systematic bias. The purpose of this position paper is twofold: first, it is intended to acquaint AI researchers with phenomenological explainable approaches that rely on the physical entities that the machine experiences during training/inference, as opposed to the traditional cognitivist symbolic account or metaphysical approaches; second, it is to state that this phenomenological doctrine will be practically useful for tackling algorithmic fairness issues in MLLMs. We develop a surrogate physics-based model that describes transformer dynamics (i.e., semantic network structure and self-/cross-attention) to analyze the dynamics of cross-modal bias in MLLM, which are not fully captured by conventional embedding- or representation-level analyses. We support this position through multi-input diagnostic experiments: 1) perturbation-based analyses of emotion classification using Qwen2.5-Omni and Gemma 3n, and 2) dynamical analysis of Lorenz chaotic time-series prediction through the physical surrogate. Across two architecturally distinct MLLMs, we show that multimodal inputs can reinforce modality dominance rather than mitigate it, as revealed by structured error-attractor patterns under systematic label perturbation, complemented by dynamical analysis.", "AI": {"tldr": "本文通过物理基础的表征方法分析了多模态模型中的跨模态偏见。", "motivation": "旨在介绍一种基于物理实体而非传统认知或形而上学的方法来解释AI模型的行为，特别是在处理算法公平性问题时更为实用。此外，研究揭示复杂多模态交互可能导致系统偏差。", "method": "开发了一个基于物理学的代理模型来描述转换器动态，并通过情绪分类和洛伦兹混沌时间序列预测实验验证了这种方法的有效性。", "result": "结果显示，在两个不同的多模态语言模型中，多模态输入可能增强某一种模态的主导地位而非缓解它，这表现在结构化误差吸引子模式下。", "conclusion": "提出了一种新颖的方法来理解AI中的跨模态偏见问题，并表明基于物理学的方法可以更好地解释和处理这些问题。"}}
{"id": "2602.20618", "pdf": "https://arxiv.org/pdf/2602.20618", "abs": "https://arxiv.org/abs/2602.20618", "authors": ["Haonan An", "Xiaohui Ye", "Guang Hua", "Yihang Tao", "Hangcheng Cao", "Xiangyu Yu", "Yuguang Fang"], "title": "RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.", "AI": {"tldr": "RecoverMark是一种水印框架，用于同时实现对操纵面部图像的定位、恢复和版权验证。", "motivation": "随着AI生成内容的增多，复杂的面部篡改行为损害了视觉完整性，并给知识产权保护带来了巨大挑战。目前的方法在对抗水印移除攻击时表现出脆弱性，而且双水印策略由于互干扰和有限嵌入容量而进一步削弱了易损水印的效果。", "method": "RecoverMark利用背景语义一致性约束及面部内容作为水印的特性设计了一个稳健的两阶段训练范式，并通过模拟全面潜在攻击的扭曲层以及渐进式训练策略，实现了无需脆弱方式的稳健水印嵌入。保护的脸部内容本身被用作水印并嵌入到周围的背景下。", "result": "实验显示，RecoverMark在对抗已见和未见攻击时具有强大的鲁棒性，并且对分布内和分布外数据都表现出良好的泛化能力。", "conclusion": "RecoverMark框架为同时实现图像操纵定位、恢复以及版权保护提供了一种有效的方法。"}}
{"id": "2602.20616", "pdf": "https://arxiv.org/pdf/2602.20616", "abs": "https://arxiv.org/abs/2602.20616", "authors": ["Xueqiang Lv", "Shizhou Zhang", "Yinghui Xing", "Di Xu", "Peng Wang", "Yanning Zhang"], "title": "Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly \"knowing the unknown\". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.", "AI": {"tldr": "提出了一种基于概念分解模型的可解释开放世界目标检测框架（IPOW），用于同时提高已知类别和未知对象的识别精度。", "motivation": "现有方法主要集中在改善未知物体的召回率上，而忽视了可解释性，这导致了已知与未知之间的混淆，并降低了预测可靠性。因此，本研究旨在使整个开放世界目标检测框架具有可解释性，以真正实现“了解未知”。", "method": "引入了一种概念分解模型（CDM），将Faster R-CNN中的耦合RoI特征明确地解构为鉴别、共享和背景概念，并提出概念导向修正(CGR)来解决已知与未知的混淆。", "result": "实验证明，IPOW显著提高了未见过类别的召回率，同时减轻了混淆，提供了针对已知类别和未知对象预测的概念级别可解释性。", "conclusion": "通过引入概念分解模型（CDM）以及概念导向修正(CGR)，实现了更准确的开放世界目标检测并保持较高的可解释性。"}}
{"id": "2602.20608", "pdf": "https://arxiv.org/pdf/2602.20608", "abs": "https://arxiv.org/abs/2602.20608", "authors": ["Aihua Mao", "Kaihang Huang", "Yong-Jin Liu", "Chee Seng Chan", "Ying He"], "title": "VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos", "categories": ["cs.CV"], "comment": null, "summary": "3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.", "AI": {"tldr": "VAGNet是一个利用视频中的人体与物体交互序列来确定三维物体上支持人体互动区域的技术框架。", "motivation": "现有的大多数方法依赖于静态的视觉或文本线索，忽略了功能是在动态行动中定义的事实。因此，它们往往无法精确定位真实互动中的接触点。人类通过观察和模仿行为学习如何使用物品，而不仅仅是研究形状，这启发了作者引入视频指导的三维功能定位。", "method": "提出了VAGNet框架，该框架利用从视频中提取的人体与物体交互线索来解析三维结构上的模糊性，从而实现了基于动态互动序列的功能监督。此外，为了支持这一新的设置，作者还推出了PVAD数据集。", "result": "实验结果表明，VAGNet在PVAD上取得了最先进的性能，并且显著优于静态基准模型。", "conclusion": "通过利用视频中的人体与物体交互序列来解析三维结构上的功能定位,VAGNet克服了传统方法的限制,实现了更高的准确性。"}}
{"id": "2602.20597", "pdf": "https://arxiv.org/pdf/2602.20597", "abs": "https://arxiv.org/abs/2602.20597", "authors": ["Yuejiao Su", "Yi Wang", "Lei Yao", "Yawen Cui", "Lap-Pui Chau"], "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing", "categories": ["cs.CV"], "comment": null, "summary": "A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to \"interaction illusion\", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.", "AI": {"tldr": "本文提出了一种基于Transformer的交互感知模型InterFormer，用于解析第一人称视角下的手部和物体交互。", "motivation": "现有方法在处理手部和物体交互时存在初始化机制适应性差、预测不一致等问题。为解决这些问题，研究者提出了新的解决方案来增强模型对交互场景的理解能力。", "method": "该模型包括动态查询生成器（DQG）、双上下文特征选择器（DFS）以及条件共现损失函数（CoCo loss），通过这些组件提高了模型在处理复杂手部和物体互动时的准确性和物理一致性。", "result": "实验结果表明，本文提出的模型在EgoHOS和mini-HOI4D数据集上达到了最先进的性能，显示出了良好的泛化能力和有效性。", "conclusion": "该方法通过改进Transformer架构中的关键组件解决了现有模型存在的问题，并且能够在复杂的第一人称视角交互场景中实现更准确的解析。"}}
{"id": "2602.20596", "pdf": "https://arxiv.org/pdf/2602.20596", "abs": "https://arxiv.org/abs/2602.20596", "authors": ["Zongyuan Zhang", "Christopher Lehnert", "Will N. Browne", "Jonathan M. Roberts"], "title": "Acoustic Feedback for Closed-Loop Force Control in Robotic Grinding", "categories": ["cs.RO"], "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026. 8 pages, 10 figures", "summary": "Acoustic feedback is a critical indicator for assessing the contact condition between the tool and the workpiece when humans perform grinding tasks with rotary tools. In contrast, robotic grinding systems typically rely on force sensing, with acoustic information largely ignored. This reliance on force sensors is costly and difficult to adapt to different grinding tools, whereas audio sensors (microphones) are low-cost and can be mounted on any medium that conducts grinding sound. This paper introduces a low-cost Acoustic Feedback Robotic Grinding System (AFRG) that captures audio signals with a contact microphone, estimates grinding force from the audio in real time, and enables closed-loop force control of the grinding process. Compared with conventional force-sensing approaches, AFRG achieves a 4-fold improvement in consistency across different grinding disc conditions. AFRG relies solely on a low-cost microphone, which is approximately 200-fold cheaper than conventional force sensors, as the sensing modality, providing an easily deployable, cost-effective robotic grinding solution.", "AI": {"tldr": "本文提出了一种基于声反馈的低成本机器人磨削系统（AFRG），该系统通过接触式麦克风捕捉音频信号，实时估计磨削力，并实现闭环力控制。", "motivation": "传统的机器人磨削系统主要依赖于力传感器来评估工件和工具之间的接触状况，这种方式成本高且难以适应不同的磨削工具。相比之下，使用声学信息进行评估则更加经济高效，因此本文提出了利用音频反馈的AFRG系统以解决这一问题。", "method": "通过安装在磨头上的接触式麦克风捕捉音频信号；从捕获的声音中实时估算磨削力；实现闭环力控制来调整磨削过程。", "result": "与传统力传感方法相比，基于声反馈的系统（AFRG）在不同砂轮条件下的一致性提高了四倍，且所用麦克风的成本约为传统力传感器的二百分之一。", "conclusion": "本文提出了一种经济高效的机器人磨削解决方案——基于声反馈的AFRG系统，该系统能够提供成本低廉、易于部署的闭环力控制方法。"}}
{"id": "2602.20595", "pdf": "https://arxiv.org/pdf/2602.20595", "abs": "https://arxiv.org/abs/2602.20595", "authors": ["Longxiang Wang", "Xiang Zheng", "Xuhao Zhang", "Yao Zhang", "Ye Wu", "Cong Wang"], "title": "OptiLeak: Efficient Prompt Reconstruction via Reinforcement Learning in Multi-tenant LLM Services", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Multi-tenant LLM serving frameworks widely adopt shared Key-Value caches to enhance efficiency. However, this creates side-channel vulnerabilities enabling prompt leakage attacks. Prior studies identified these attack surfaces yet focused on expanding attack vectors rather than optimizing attack performance, reporting impractically high attack costs that underestimate the true privacy risk. We propose OptiLeak, a reinforcement learning-enhanced framework that maximizes prompt reconstruction efficiency through two-stage fine-tuning. Our key insight is that domain-specific ``hard tokens'' -- terms difficult to predict yet carrying sensitive information -- can be automatically identified via likelihood ranking and used to construct preference pairs for Direct Preference Optimization, eliminating manual annotation. This enables effective preference alignment while avoiding the overfitting issues of extended supervised fine-tuning. Evaluated on three benchmarks spanning medical and financial domains, OptiLeak achieves up to $12.48\\times$ reduction in average requests per token compared to baseline approaches, with consistent improvements across model scales from 3B to 14B parameters. Our findings demonstrate that cache-based prompt leakage poses a more severe threat than previously reported, underscoring the need for robust cache isolation in production deployments.", "AI": {"tldr": "提出了一种基于强化学习的框架OptiLeak，以优化多租户LLM服务中的提示重建效率。", "motivation": "现有研究虽然揭示了共享缓存带来的隐私风险，但并未有效减少攻击成本。本论文旨在通过引入更高效的攻击方法来真实反映这些风险，并提出解决方案。", "method": "采用两阶段微调策略结合领域特定的“硬词”识别和直接偏好优化，构建了一个强化学习框架OptiLeak以提高提示重建效率。", "result": "在三个不同领域的基准测试中，与基线方法相比，OptiLeak实现了高达12.48倍的平均请求减少率，并且这一效果在不同的模型规模下也保持一致。", "conclusion": "研究表明缓存泄漏的风险比之前认为的要严重得多，这强调了生产部署中需要加强缓存隔离的重要性。"}}
{"id": "2602.20594", "pdf": "https://arxiv.org/pdf/2602.20594", "abs": "https://arxiv.org/abs/2602.20594", "authors": ["Takaya Miyama", "Satoshi Nakamura", "Shota Yamanaka"], "title": "Improving Data Quality via Pre-Task Participant Screening in Crowdsourced GUI Experiments", "categories": ["cs.HC"], "comment": "To appear at CHI 2026", "summary": "In crowdsourced user experiments that collect performance data from graphical user interface (GUI) interactions, some participants ignore instructions or act carelessly, threatening the validity of performance models. We investigate a pre-task screening method that requires simple GUI operations analogous to the main task and uses the resulting error as a continuous quality signal. Our pre-task is a brief image-resizing task in which workers match an on-screen card to a physical card; workers whose resizing error exceeds a threshold are excluded from the main experiment. The main task is a standardized pointing experiment with well-established models of movement time and error rate. Across mouse- and smartphone-based crowdsourced experiments, we show that reducing the proportion of workers exhibiting unexpected behavior and tightening the pre-task threshold systematically improve the goodness of fit and predictive accuracy of GUI performance models, demonstrating that brief pre-task screening can enhance data quality.", "AI": {"tldr": "研究通过预任务筛选参与者来提高众包图形用户界面实验中的数据质量。", "motivation": "在众包用户实验中，一些参与者无视指示或粗心大意，威胁到性能模型的有效性。该研究探讨了一种基于简单GUI操作的预任务筛选方法，以此作为连续的质量信号。", "method": "预任务是一个简短的图像调整工作，在此工作中工人需要将屏幕上的卡片匹配到实体卡片；如果工人的误差超过阈值，则从主实验中排除他们。主任务是具有已建立的运动时间和错误率模型的标准指向实验。", "result": "通过减少表现出意外行为的参与者比例并收紧预任务阈值，可以系统地提高GUI性能模型的良好契合度和预测准确性。", "conclusion": "简短的预任务筛选能够提升数据质量。"}}
{"id": "2602.20592", "pdf": "https://arxiv.org/pdf/2602.20592", "abs": "https://arxiv.org/abs/2602.20592", "authors": ["Bipasha Kashyap", "Björn W. Schuller", "Pubudu N. Pathirana"], "title": "Quantifying Dimensional Independence in Speech: An Information-Theoretic Framework for Disentangled Representation Learning", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Speech signals encode emotional, linguistic, and pathological information within a shared acoustic channel; however, disentanglement is typically assessed indirectly through downstream task performance. We introduce an information-theoretic framework to quantify cross-dimension statistical dependence in handcrafted acoustic features by integrating bounded neural mutual information (MI) estimation with non-parametric validation. Across six corpora, cross-dimension MI remains low, with tight estimation bounds ($< 0.15$ nats), indicating weak statistical coupling in the data considered, whereas Source--Filter MI is substantially higher (0.47 nats). Attribution analysis, defined as the proportion of total MI attributable to source versus filter components, reveals source dominance for emotional dimensions (80\\%) and filter dominance for linguistic and pathological dimensions (60\\% and 58\\%, respectively). These findings provide a principled framework for quantifying dimensional independence in speech.", "AI": {"tldr": "本文提出了一种基于信息论的框架，用于量化语音信号中不同维度之间的统计依赖性。", "motivation": "传统的解纠缠方法通常通过下游任务的表现来间接评估。本文旨在提供一种直接的方法来量化语音中的情感、语言和病理信息在共享声学信道中的独立性。", "method": "作者采用了有界神经互信息（MI）估计与非参数验证相结合的信息理论框架，以量化不同维度之间的统计依赖性。", "result": "研究发现，在六个语料库中，跨维度的MI值较低，表明数据之间存在较弱的统计耦合。此外，情感维度主要由源成分主导，而语言和病理维度则更依赖于过滤器组件。", "conclusion": "本文提出的方法为量化语音信号中的不同维度独立性提供了理论依据，并证明了该框架的有效性。"}}
{"id": "2602.20584", "pdf": "https://arxiv.org/pdf/2602.20584", "abs": "https://arxiv.org/abs/2602.20584", "authors": ["Beverley Gorry", "Tobias Fischer", "Michael Milford", "Alejandro Fontan"], "title": "Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.", "AI": {"tldr": "长期多会话三维重建在外观变化显著的情况下", "motivation": "现有的SfM管线在长时间监测场景下因图像采集时间间隔大和外观变化频繁而失效，如珊瑚礁调查。为解决跨会话对齐不足的问题，提出了一种联合建模的方法以克服大规模外观变化的挑战", "method": "通过结合手工制作和学习的视觉特征来建立跨越大时间间隙的对应关系，并利用视觉位置识别来减少计算成本和提高对齐稳健性", "result": "在展示显著外观变化的实际珊瑚礁数据集上，实现了跨会话的一致联合重建，在现有方法无法产生一致建模的情况下取得成功", "conclusion": "提出的方法能够处理长时间间隔的大规模图像集合并保持三维模型的连贯性和准确性"}}
{"id": "2602.20583", "pdf": "https://arxiv.org/pdf/2602.20583", "abs": "https://arxiv.org/abs/2602.20583", "authors": ["Wonyong Seo", "Jaeho Moon", "Jaehyup Lee", "Soo Ye Kim", "Munchurl Kim"], "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models", "categories": ["cs.CV"], "comment": "The first two authors contributed equally to this work (equal contribution)", "summary": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.", "AI": {"tldr": "提出了一种名为PropFly的训练管道，用于基于传播的视频编辑任务。", "motivation": "现有的基于传播的视频编辑模型需要大规模配对视频数据集进行训练，这种数据集获取成本高且复杂。为了克服这一问题，论文提出了PropFly方法，利用预训练视频扩散模型在飞行中的监督代替了传统依赖于现成或预先计算的配对视频编辑数据集的方法。", "method": "该方法使用来自中间噪声潜变量的一步清洁潜变量估计，在不同的自由分类器指导（CFG）尺度下生成多样化的“源”低CFG和“编辑”高CFG潜变量，以实时监督模型训练，并附加了一个适配器来学习传播编辑操作。通过引导模型复制目标变换，保证了模型学习到的时间一致性和动态转换。", "result": "实验结果表明，PropFly方法在各种视频编辑任务上显著优于现有最佳方法，生成高质量的编辑结果。", "conclusion": "论文展示了PropFly作为一种有效且高效的训练管道，在无需大规模配对数据集的情况下提升了基于传播的视频编辑的质量和一致性。"}}
{"id": "2602.20580", "pdf": "https://arxiv.org/pdf/2602.20580", "abs": "https://arxiv.org/abs/2602.20580", "authors": ["Nishant Subramani", "Kshitish Ghate", "Mona Diab"], "title": "Personal Information Parroting in Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "EACL Findings 2026", "summary": "Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.", "AI": {"tldr": "研究现代语言模型对个人身份信息的记忆风险，并开发了检测工具", "motivation": "减少因语言模型记忆个人身份信息而导致的隐私泄露风险", "method": "使用正则表达式和规则套件（R&R）探测器来识别电子邮件地址、电话号码和IP地址，研究不同大小和预训练步骤的语言模型的记忆率", "result": "发现较大的模型和经过更多预训练步骤的模型具有更高的记忆个人身份信息倾向。最小规模的模型也存在2.7%的信息重复问题。", "conclusion": "建议对预训练数据集进行积极过滤和匿名化处理以减少个人身份信息的记忆"}}
{"id": "2602.20577", "pdf": "https://arxiv.org/pdf/2602.20577", "abs": "https://arxiv.org/abs/2602.20577", "authors": ["Jiaru Zhang", "Manav Gagvani", "Can Cui", "Juntong Peng", "Ruqi Zhang", "Ziran Wang"], "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.", "AI": {"tldr": "提出了一种高效的端到端自动驾驶框架Masked Vision-Language-Action Diffusion（MVLAD-AD），该框架通过掩码视觉语言动作扩散模型解决了现有方法在推理延迟、行动精确度和可解释性方面的挑战。", "motivation": "大型语言模型和视觉语言模型虽然作为端到端自动驾驶的有前途候选者，但在推理延迟、行动精度和可解释性方面面临挑战。现有的自回归方法存在逐字生成缓慢的问题，而先前基于扩散的方法依赖于冗长的一般用途语言令牌，缺乏明确的几何结构。", "method": "提出了MVLAD-AD框架，采用离散动作标记化策略构建一个紧凑的动作代码库，包含从真实驾驶分布中提取的动力学可行路径点。此外还引入了感知增强嵌入学习以确保潜空间中的嵌入接近物理几何度量，并采用了行动优先解码策略来优先生成轨迹。", "result": "在nuScenes和相关基准测试上的大量实验表明，MVLAD-AD实现了更高的效率，在规划精度上优于最先进的自回归和扩散方法，同时提供了高保真和可解释的推理结果。", "conclusion": "MVLAD-AD框架成功地解决了现有自动驾驶模型在效率、精确度和可解释性方面的不足，为实现高效的端到端自动驾驶提供了一种新的途径。"}}
{"id": "2602.20575", "pdf": "https://arxiv.org/pdf/2602.20575", "abs": "https://arxiv.org/abs/2602.20575", "authors": ["Haojie Feng", "Peizhi Zhang", "Mengjie Tian", "Xinrui Zhang", "Zhuoren Li", "Junpeng Huang", "Xiurong Wang", "Junfan Zhu", "Jianzhou Wang", "Dongxiao Yin", "Lu Xiong"], "title": "An interactive enhanced driving dataset for autonomous driving", "categories": ["cs.CV"], "comment": null, "summary": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.", "AI": {"tldr": "开发了一种增强的交互式驾驶数据集（IEDD）以支持自动驾驶系统的全自动化发展。", "motivation": "现有的驾驶数据缺乏交互场景和多模态对齐，限制了Vision-Language-Action模型的发展。", "method": "通过基于互动轨迹的大规模管道挖掘出数百万级别的互动片段，并构建了一个包含合成鸟瞰视角视频的IEDD-VQA数据集。", "result": "提供了评估主流视觉语言模型（VLM）推理能力的基准结果，展示了该数据集在自动驾驶模型调整中的重用价值。", "conclusion": "提出的IEDD数据集能够有效促进Vision-Language-Action模型的发展，并具有重要的研究与应用价值。"}}
{"id": "2602.20571", "pdf": "https://arxiv.org/pdf/2602.20571", "abs": "https://arxiv.org/abs/2602.20571", "authors": ["Ayush Sawarni", "Jiyuan Tan", "Vasilis Syrgkanis"], "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation", "categories": ["cs.AI"], "comment": null, "summary": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.", "AI": {"tldr": "提出了一种新的基准CausalReasoningBenchmark，用于评估因果推理系统在识别和估计两个步骤中的表现。", "motivation": "当前的自动因果推断基准将识别和估计混为一谈，无法区分两者的性能。新基准旨在分离并分别评价这两个过程。", "method": "通过173个查询和138个真实数据集来评估系统的能力，在每个查询中系统需提供结构化的识别说明及点估计值与标准误差。", "result": "状态最佳的LLM模型在高阶策略上的正确率为84%，但完全识别规格正确的仅为30%。这表明瓶颈在于研究设计的细节而不是计算能力。", "conclusion": "CausalReasoningBenchmark有助于开发更稳健的自动因果推断系统，并且已公开发布供研究使用。"}}
{"id": "2602.20569", "pdf": "https://arxiv.org/pdf/2602.20569", "abs": "https://arxiv.org/abs/2602.20569", "authors": ["Jiaqi Wu", "Yuchen Zhou", "Muduo Xu", "Zisheng Liang", "Simiao Ren", "Jiayu Xue", "Meige Yang", "Siying Chen", "Jingheng Huan"], "title": "AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents", "categories": ["cs.CV"], "comment": "17 pages, 10 figures", "summary": "We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.", "AI": {"tldr": "介绍AIForge-Doc，一个专门针对金融和表格文档中基于扩散模型的像素级修补伪造检测的基准。", "motivation": "现有文档篡改数据集依赖传统数字编辑工具（如Adobe Photoshop、GIMP），忽视了快速发展的AI生成伪造威胁。为填补这一空白，提出AIForge-Doc。", "method": "利用两个AI修补API在真实收据和表格图像中系统性伪造数字字段，产生4061张伪造图片，并用DocTamper兼容格式标注像素级篡改区域。", "result": "对三种检测器（TruFor、DocTamper及零样本GPT-4o）进行基准测试，发现所有现有方法性能大幅下降。例如，在分布外情况下，TruFor的AUC为0.751；而DocTamper和GPT-4o分别为0.563和0.509。", "conclusion": "结果表明AIForge-Doc代表了文档取证领域的一个全新且未解决的重大挑战。"}}
{"id": "2602.20566", "pdf": "https://arxiv.org/pdf/2602.20566", "abs": "https://arxiv.org/abs/2602.20566", "authors": ["Haosheng Li", "Weixin Mao", "Zihan Lan", "Hongwei Xiong", "Hongan Wang", "Chenyang Si", "Ziwei Liu", "Xiaoming Deng", "Hua Chen"], "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.", "AI": {"tldr": "BFA++是一种用于多视图视觉语言动作模型的动态令牌剪枝框架，旨在提高机器人操作中的实时性能。", "motivation": "现有的加速技术如令牌剪枝在直接应用于VLA模型时通常会导致性能下降，因为它们忽略了不同视角之间的关系，并未能考虑机器人操作中的动态和任务特定特性。因此，需要一种新的策略来解决这些问题。", "method": "BFA++引入了一种分层修剪策略，由两个级别的重要性预测器引导：一个是在每个图像中突出显示任务相关的区域以抑制空间噪声的内部视图预测器；另一个是通过不同的操作阶段识别关键摄像机视角以减少跨视图冗余的互视图预测器。这使得高效的令牌选择成为可能，并在保持必要视觉线索的同时提高了计算效率。", "result": "评估结果显示，BFA++在RoboTwin基准测试和真实世界的机器人任务中均优于现有方法，在π0和RDT模型上分别实现了10％的成功率提升和约1.8X和1.5X的速度提升。", "conclusion": "研究结果表明，上下文敏感且针对特定任务的令牌剪枝比完整的视觉处理更有效，可以在真实世界的机器人系统中实现更快的推理和提高操作准确性。"}}
{"id": "2602.20558", "pdf": "https://arxiv.org/pdf/2602.20558", "abs": "https://arxiv.org/abs/2602.20558", "authors": ["Yucheng Shi", "Ying Li", "Yu Wang", "Yesu Feng", "Arjun Rao", "Rein Houthooft", "Shradha Sehgal", "Jin Wang", "Hao Zhen", "Ninghao Liu", "Linas Baltrunas"], "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production", "categories": ["cs.AI", "cs.IR"], "comment": "Work in progress", "summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.", "AI": {"tldr": "论文提出了一种基于强化学习的数据中心化框架，用于优化大规模语言模型（LLM）推荐系统中的用户互动日志转换为自然语言输入的过程。", "motivation": "现有方法通过固定模板将结构化的用户交互日志转化为自然语言输入，效果不佳。为了提高LLM推荐系统的准确性，论文旨在探索更优的文本转化策略。", "method": "利用强化学习训练一个谓词化代理（verbalization agent），该代理能将原始互动历史转换成优化后的文本上下文，以推荐准确率为训练信号。该方法能够过滤噪声、整合相关元数据并重组信息。", "result": "实验表明，与基于模板的基线相比，通过学习得到的谓词化策略在发现项推荐准确率上提高了高达93%。进一步分析揭示了诸如用户兴趣总结、噪声移除和语法规范化等策略的有效性。", "conclusion": "该研究证明了强化学习驱动的谓词化方法能够显著提升LLM推荐系统的性能，并为有效构建上下文提供深入见解。"}}
{"id": "2602.20556", "pdf": "https://arxiv.org/pdf/2602.20556", "abs": "https://arxiv.org/abs/2602.20556", "authors": ["Hanhui Li", "Xuan Huang", "Wanquan Liu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang", "Chenqiang Gao"], "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.", "AI": {"tldr": "本文提出了一种基于优化的框架WildGHand，用于从单目视频中重建高质量的手部模型，特别是在存在手部与物体交互、极端姿态和照明变化等扰动的情况下。", "motivation": "现有方法在受控环境中表现良好，但在现实世界中的严重干扰下性能下降。本文旨在通过引入动态扰动解耦模块和感知扰动的优化策略来解决这些问题。", "method": "WildGHand包含两个关键组件：一个动态扰动解耦模块，将扰动表示为3D高斯属性上的时间变化偏差；以及一种感知扰动的优化策略，生成每帧各向异性加权掩码以引导优化过程。", "result": "在自建数据集和两个公开数据集中，WildGHand实现了最先进的性能，并且相对基线模型提高了15.8％的PSNR值和23.1％的LPIPS值。", "conclusion": "通过引入动态扰动解耦模块和感知扰动优化策略，本文框架在复杂环境中的手部重建中取得了显著效果。"}}
{"id": "2602.20551", "pdf": "https://arxiv.org/pdf/2602.20551", "abs": "https://arxiv.org/abs/2602.20551", "authors": ["Zhenran Tang", "Rohan Nagabhirava", "Changliu Liu"], "title": "CAD-Prompted SAM3: Geometry-Conditioned Instance Segmentation for Industrial Objects", "categories": ["cs.CV"], "comment": null, "summary": "Verbal-prompted segmentation is inherently limited by the expressiveness of natural language and struggles with uncommon, instance-specific, or difficult-to-describe objects: scenarios frequently encountered in manufacturing and 3D printing environments. While image exemplars provide an alternative, they primarily encode appearance cues such as color and texture, which are often unrelated to a part's geometric identity. In industrial settings, a single component may be produced in different materials, finishes, or colors, making appearance-based prompting unreliable. In contrast, such objects are typically defined by precise CAD models that capture their canonical geometry. We propose a CAD-prompted segmentation framework built on SAM3 that uses canonical multi-view renderings of a CAD model as prompt input. The rendered views provide geometry-based conditioning independent of surface appearance. The model is trained using synthetic data generated from mesh renderings in simulation under diverse viewpoints and scene contexts. Our approach enables single-stage, CAD-prompted mask prediction, extending promptable segmentation to objects that cannot be robustly described by language or appearance alone.", "AI": {"tldr": "本文提出了一种基于CAD模型提示的分割框架，旨在解决工业环境中因语言和外观描述不足而引起的分割问题。", "motivation": "传统的语言提示分割方法在面对难以用自然语言描述或具有多种外观特征的物体时表现不佳。这种场景常出现在制造与3D打印领域中。因此，本文提出了基于CAD模型的提示方法来解决这些问题。", "method": "该框架采用多视角渲染生成的CAD模型作为提示输入，并通过模拟生成的数据进行训练，以实现单阶段、基于CAD模型提示的分割预测。", "result": "实验结果显示，所提出的方法能够有效地处理那些仅凭语言或外观难以描述的对象。", "conclusion": "本文证明了利用CAD模型进行几何条件化提示在工业环境中具有显著的优势，可提高复杂对象的分割精度与鲁棒性。"}}
{"id": "2602.20550", "pdf": "https://arxiv.org/pdf/2602.20550", "abs": "https://arxiv.org/abs/2602.20550", "authors": ["Chengshuai Yang"], "title": "The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation", "categories": ["cs.CV"], "comment": null, "summary": "Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.", "AI": {"tldr": "论文提出了一个有限原始基础定理，证明了所有计算成像正向模型都可以表示为一个有界复杂性的图，并且提供了一个生成该图的算法。", "motivation": "传统的计算成像正向模型实现方式是特定模态的单体代码，缺乏通用性和灵活性。论文旨在建立一种统一的基础框架，能够将各种计算成像模型表示为一组有限的基本操作符。", "method": "证明所有在Cimg类中的正向模型可以由11个基本操作构成的图精确或近似地表示，并提供了一种算法来生成该图。同时分析了非线性物理现象，将其归入两类结构类别：逐点标量函数和自一致迭代。", "result": "通过构造性的方法证明了有限原始基础定理，实验验证表明在31个线性模态下误差eimg小于0.01，并提供了9种非线性模型的图分解。", "conclusion": "这些结果为计算成像提供了一个统一的基础框架，并建立了物理学世界模型（PWM）框架的数学基础。"}}
{"id": "2602.20549", "pdf": "https://arxiv.org/pdf/2602.20549", "abs": "https://arxiv.org/abs/2602.20549", "authors": ["Frederic Wang", "Katherine L. Bouman"], "title": "Sample-efficient evidence estimation of score based priors for model selection", "categories": ["cs.LG", "cs.CV", "stat.ME"], "comment": "ICLR 2026", "summary": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.", "AI": {"tldr": "该论文提出了一种新的模型证据估计方法，用于评估基于分数的先验的扩散模型。", "motivation": "选择合适的先验是解决不适定成像逆问题的关键。通过计算不同模型下的模型证据来选择最优模型可以避免严重偏差。然而，直接计算基于扩散模型的模型证据是非常困难的，并且现有的估计方法需要大量的未归一化密度评估或准确的干净先验分数。", "method": "\\method利用后验采样过程中的中间样本进行模型证据的估算，通过集成时间边际分布来提高效率和准确性。该方法仅需少量的后验样本（例如20个）即可获得精确估计。", "result": "实验结果表明，当可以计算时，该估计器能够准确地匹配模型证据，并且在解决不同高度不适定、非线性逆问题以及实际黑洞成像问题中表现出色。同时还能诊断先验不一致的问题。", "conclusion": "论文提出了一种有效的方法来评估基于分数的扩散模型的模型证据，这种方法只需要少量样本就能实现高精度估计，并且适用于多种逆问题情况。"}}
{"id": "2602.20548", "pdf": "https://arxiv.org/pdf/2602.20548", "abs": "https://arxiv.org/abs/2602.20548", "authors": ["Shuai Wang", "Malu Zhang", "Yulin Jiang", "Dehao Zhang", "Ammar Belatreche", "Yu Liang", "Yimeng Shan", "Zijian Zhou", "Yang Yang", "Haizhou Li"], "title": "Robust Spiking Neural Networks Against Adversarial Attacks", "categories": ["cs.CV"], "comment": "Published as a conference paper at ICLR 2026", "summary": "Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.", "AI": {"tldr": "提出一种优化方法以提高脉冲神经网络在对抗性环境下的鲁棒性", "motivation": "直接训练的脉冲神经网络在复杂的对抗环境中存在鲁棒性限制，需要开发新的技术来提升其抵御攻击的能力", "method": "通过引入阈值保护优化（TGO）方法，包括将额外约束加入损失函数以使膜电位远离阈值和引入噪声脉冲神经元使其从确定性转变为概率性的机制，从而减少因微小扰动导致的状态翻转的概率", "result": "实验结果表明该方法能有效增强直接训练的脉冲神经网络在对抗环境下的鲁棒性", "conclusion": "这项研究为实现更加可靠和安全的神经形态计算铺平了道路"}}
{"id": "2602.20547", "pdf": "https://arxiv.org/pdf/2602.20547", "abs": "https://arxiv.org/abs/2602.20547", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "title": "What Drives Students' Use of AI Chatbots? Technology Acceptance in Conversational AI", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.ET"], "comment": ":J.4; K.3; K.4", "summary": "Conversational AI tools have been rapidly adopted by students and are becoming part of their learning routines. To understand what drives this adoption, we draw on the Technology Acceptance Model (TAM) and examine how perceived usefulness and perceived ease of use relate to students' behavioral intention to use conversational AI that generates responses for learning tasks. We extend TAM by incorporating trust, perceived enjoyment, and subjective norms as additional factors that capture social and affective influences and uncertainty around AI outputs. Using partial least squares structural equation modeling, we find perceived usefulness remains the strongest predictor of students' intention to use conversational AI. However, perceived ease of use does not exert a significant direct effect on behavioral intention once other factors are considered, operating instead indirectly through perceived usefulness. Trust and subjective norms significantly influence perceptions of usefulness, while perceived enjoyment exerts both a direct and indirect effect on usage intentions. These findings suggest that adoption decisions for conversational AI systems are influenced less by effort-related considerations and more by confidence in system outputs, affective engagement, and social context. Future research is needed to further examine how these acceptance relationships generalize across different conversational systems and usage contexts.", "AI": {"tldr": "探讨学生使用AI聊天机器人的驱动因素，研究技术接受模型在对话式人工智能中的应用。", "motivation": "为了理解学生采用对话式AI工具的原因，探索技术接受模型（TAM）框架下的有用性和易用性如何影响学生的使用意愿。", "method": "基于TAM模型，并引入信任、感知愉悦和主观规范等额外因素。通过偏最小二乘结构方程建模来分析这些因素与学生采用对话式AI系统的关系。", "result": "研究发现，感知有用性仍然是驱动学生使用对话式AI最强的预测因子。然而，感知易用性的直接效果在考虑其他因素后并不显著，而是通过感知有用性间接影响行为意图。信任和主观规范对感知有用性有显著影响，而感知愉悦则同时具有直接影响和间接影响。", "conclusion": "学生采用对话式AI系统的决定受到系统输出的信心、情感参与和社会情境的影响，而与努力相关的考虑关系较小。未来研究需进一步探讨这些接受关系在不同对话系统及使用环境中的普适性。"}}
{"id": "2602.20544", "pdf": "https://arxiv.org/pdf/2602.20544", "abs": "https://arxiv.org/abs/2602.20544", "authors": ["Hannah Kim", "Rahad Arman Nabid", "Jeni Sorathiya", "Minh Doan", "Elijah Jordan", "Rayhana Nasimova", "Sergei L. Kosakovsky Pond", "Stephen MacNeil"], "title": "Changing the Optics: Comparing Traditional and Retrieval-Augmented GenAI E-Tutorials in Interdisciplinary Learning", "categories": ["cs.HC"], "comment": "6 pages (16 pages including appendix), 2 figures, 5 supplementary figures, 2 supplementary tables", "summary": "Understanding information-seeking behaviors in e-learning is critical, as learners must often make sense of complex and fragmented information, a challenge compounded in interdisciplinary fields with diverse prior knowledge. Compared to traditional e-tutorials, GenAI e-tutorials offer new ways to navigate information spaces, yet how they shape learners information-seeking behaviors remains unclear. To address this gap, we characterized behavioral differences between traditional and GenAI-mediated e-tutorial learning using the three search modes of orienteering. We conducted a between-subject study in which learners engaged with either a traditional e-tutorial or a GenAI e-tutorial accessing the same underlying information content. We found that the traditional users maintained greater awareness and focus of the information space, whereas GenAI users exhibited more proactive and exploratory behaviors with lower cognitive load due to the querying-driven interaction. These findings offer guidance for designing tutorials in e-learning.", "AI": {"tldr": "比较传统和基于生成AI的e教程在跨学科学习中的信息搜索行为差异。", "motivation": "理解在线学习中用户的信息检索行为对于指导复杂知识的理解至关重要，尤其是在需要融合不同背景知识的跨学科学习领域。与传统的e教程相比，基于生成AI的e教程提供了新的导航方法来探索信息空间，但它们对用户信息搜索行为的影响尚不清楚。", "method": "该研究采用了一项两组对比实验设计：参与者被随机分配到使用传统e教程或基于生成AI的e教程中。通过分析参与者的三种搜索模式（定向、导航和选择），研究人员比较了两种学习方式的信息搜索行为差异。", "result": "研究表明，使用传统e教程的学习者保持了对信息空间更大的意识和集中度；而基于生成AI的学习者则表现出更加积极主动且探索性的行为，并且由于查询驱动的交互模式降低了认知负荷。", "conclusion": "研究结果为设计在线学习教程提供了指导建议。"}}
{"id": "2602.20543", "pdf": "https://arxiv.org/pdf/2602.20543", "abs": "https://arxiv.org/abs/2602.20543", "authors": ["Subhra Jyoti Mandal", "Lara Rachidi", "Puneet Jain", "Matthieu Duvinage", "Sander W. Timmer"], "title": "Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing", "categories": ["cs.CV"], "comment": null, "summary": "Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.", "AI": {"tldr": "本文提出了一种结合深度学习和视觉语言模型的多代理框架，用于提高药品制造中的菌落计数准确性。", "motivation": "手动计数工作量大且易出错，现有的深度学习方法虽然准确但对样品质量变化敏感。为了满足制药标准的要求并减少人工验证的工作量，开发了新的解决方案。", "method": "作者首先评估了几种YOLO版本的性能，并使用Detectron2模型进行训练以提高检测率；接着引入视觉语言模型来辅助判断样本的有效性及菌落计数准确性，通过多代理框架实现自动化流程管理。", "result": "与仅基于深度学习的方法相比，新系统将人工验证工作量减少了85%，显著提升了效率。该方案提供了可扩展、审计和符合法规的微生物质量控制解决方案。", "conclusion": "结合视觉语言模型的多代理框架在提高菌落计数准确性方面表现出色，为生物制药生产中的自动化技术进步做出了贡献。"}}
{"id": "2602.20541", "pdf": "https://arxiv.org/pdf/2602.20541", "abs": "https://arxiv.org/abs/2602.20541", "authors": ["Hana Salavcova", "Martin Černý", "Arpita Biswas"], "title": "Maximin Share Guarantees via Limited Cost-Sensitive Sharing", "categories": ["cs.GT", "cs.AI"], "comment": "In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 11 pages", "summary": "We study the problem of fairly allocating indivisible goods when limited sharing is allowed, that is, each good may be allocated to up to $k$ agents, while incurring a cost for sharing. While classic maximin share (MMS) allocations may not exist in many instances, we demonstrate that allowing controlled sharing can restore fairness guarantees that are otherwise unattainable in certain scenarios. (1) Our first contribution shows that exact maximin share (MMS) allocations are guaranteed to exist whenever goods are allowed to be cost-sensitively shared among at least half of the agents and the number of agents is even; for odd numbers of agents, we obtain a slightly weaker MMS guarantee. (2) We further design a Shared Bag-Filling Algorithm that guarantees a $(1 - C)(k - 1)$-approximate MMS allocation, where $C$ is the maximum cost of sharing a good. Notably, when $(1 - C)(k - 1) \\geq 1$, our algorithm recovers an exact MMS allocation. (3) We additionally introduce the Sharing Maximin Share (SMMS) fairness notion, a natural extension of MMS to the $k$-sharing setting. (4) We show that SMMS allocations always exist under identical utilities and for instances with two agents. (5) We construct a counterexample to show the impossibility of the universal existence of an SMMS allocation. (6) Finally, we establish a connection between SMMS and constrained MMS (CMMS), yielding approximation guarantees for SMMS via existing CMMS results. These contributions provide deep theoretical insights for the problem of fair resource allocation when a limited sharing of resources are allowed in multi-agent environments.", "AI": {"tldr": "研究在有限共享的情况下公平分配不可分割物品的问题，提出了一种新的算法和理论见解。", "motivation": "经典的最大最小份额（MMS）分配可能无法存在，在许多情况下需要通过控制共享来恢复本来无法实现的公平保证。", "method": "设计了一个共享袋填充算法，该算法能够确保在允许成本敏感性分享的情况下达到近似最大最小份额（MMS）分配；引入了共享最大最小份额（SMMS）的概念，并展示了其与约束最大最小份额（CMMS）之间的联系。", "result": "证明了当物品被允许可分给至少一半的代理时，可以保证存在精确的最大最小份额（MMS）分配；提出了一个新的算法，在特定条件下能够恢复精确的MMS分配。", "conclusion": "本文的研究为在多智能体环境中允许有限资源共享情况下公平分配问题提供了深度理论见解。"}}
{"id": "2602.20539", "pdf": "https://arxiv.org/pdf/2602.20539", "abs": "https://arxiv.org/abs/2602.20539", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Progressive Per-Branch Depth Optimization for DEFOM-Stereo and SAM3 Joint Analysis in UAV Forestry Applications", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate per-branch 3D reconstruction is a prerequisite for autonomous UAV-based tree pruning; however, dense disparity maps from modern stereo matchers often remain too noisy for individual branch analysis in complex forest canopies. This paper introduces a progressive pipeline integrating DEFOM-Stereo foundation-model disparity estimation, SAM3 instance segmentation, and multi-stage depth optimization to deliver robust per-branch point clouds. Starting from a naive baseline, we systematically identify and resolve three error families through successive refinements. Mask boundary contamination is first addressed through morphological erosion and subsequently refined via a skeleton-preserving variant to safeguard thin-branch topology. Segmentation inaccuracy is then mitigated using LAB-space Mahalanobis color validation coupled with cross-branch overlap arbitration. Finally, depth noise - the most persistent error source - is initially reduced by outlier removal and median filtering, before being superseded by a robust five-stage scheme comprising MAD global detection, spatial density consensus, local MAD filtering, RGB-guided filtering, and adaptive bilateral filtering. Evaluated on 1920x1080 stereo imagery of Radiata pine (Pinus radiata) acquired with a ZED Mini camera (63 mm baseline) from a UAV in Canterbury, New Zealand, the proposed pipeline reduces the average per-branch depth standard deviation by 82% while retaining edge fidelity. The result is geometrically coherent 3D point clouds suitable for autonomous pruning tool positioning. All code and processed data are publicly released to facilitate further UAV forestry research.", "AI": {"tldr": "本文提出了一个渐进式的管道，结合DEFOM-Stereo基础模型的视差估计、SAM3实例分割和多阶段深度优化，以提供稳健的分支级点云。", "motivation": "在复杂的森林冠层中，现代立体匹配器产生的密集视差图往往对于单独的枝条分析来说过于嘈杂。因此本文致力于解决这一问题，以实现准确的分支级3D重建和自主无人机林业应用。", "method": "文章提出了一种渐进式的管道，首先通过形态学腐蚀处理边界污染，接着使用一种保持骨架的方法来保护细枝拓扑结构；随后采用LAB空间Mahalanobis颜色验证结合跨枝条重叠仲裁来减轻分割不准确性。最后利用一个稳健的五阶段方案减少深度噪声。", "result": "在新西兰坎特伯雷地区采集到的1920x1080立体图像中，该管道将平均每个分支的深度标准差减少了82%，同时保持边缘保真度。", "conclusion": "本文的方法生成了几何上一致的3D点云，适用于自主修剪工具定位。所有代码和处理后的数据都公开发布以促进进一步的研究。"}}
{"id": "2602.20537", "pdf": "https://arxiv.org/pdf/2602.20537", "abs": "https://arxiv.org/abs/2602.20537", "authors": ["Xinyong Cai", "Changbin Sun", "Yong Wang", "Hongyu Yang", "Yuankai Wu"], "title": "PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026", "summary": "Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \\times k$ followed by $k \\times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.", "AI": {"tldr": "本文提出了PFGNet，一种基于频率引导的外围门控全卷积网络，用于高效的时空预测学习。", "motivation": "纯卷积模型在效率和并行性方面优于递归或混合架构，但其固定感受野限制了它们捕捉空间变化运动模式的能力。因此，作者提出了一种新的方法以解决这一问题。", "method": "PFGNet通过像素级频率引导门控动态调节感受野，提取局部频谱线索，并自适应融合多尺度大核外围响应和可学习中心抑制，从而形成空间自适应带通滤波器。所有的大核都分解为分隔的1D卷积（$1 \times k$随后是$k \times 1$），将每通道计算成本从$O(k^2)$降低到$O(2k)$。", "result": "实验显示，PFGNet在Moving MNIST、TaxiBJ、Human3.6M和KTH数据集上具有优越的预测性能，并且参数量和FLOPs明显减少。", "conclusion": "PFGNet通过频率引导的外围门控机制实现了结构感知时空建模，在保持效率的同时，获得了最先进的或接近最先进水平的预测性能。"}}
{"id": "2602.20532", "pdf": "https://arxiv.org/pdf/2602.20532", "abs": "https://arxiv.org/abs/2602.20532", "authors": ["Zhengyao Gu", "Jonathan Light", "Raul Astudillo", "Ziyu Ye", "Langzhou He", "Henry Peng Zou", "Wei Cheng", "Santiago Paternain", "Philip S. Yu", "Yisong Yue"], "title": "Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "37 pages, 8 figures, 1 table. Preprint under review. Equal contribution by first two authors", "summary": "Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.", "AI": {"tldr": "该论文提出了ACTOR-CURATOR框架，用于大规模语言模型的强化学习后期训练中的课程学习。", "motivation": "大规模基础模型在后期使用强化学习进行微调时，通常依赖于大量的异质数据集，使得有效的课程学习变得至关重要且具有挑战性。", "method": "该论文将问题选择视为非平稳随机多臂赌博机问题，并基于在线随机镜像下降推导出原则性的损失函数，建立了在部分反馈下的遗憾保证。", "result": "ACTOR-CURATOR框架在广泛的推理基准上优于均匀采样和强大的课程基线方法，展示了更好的训练稳定性和效率。相对收益达到了28.6%和30.5%，并且速度提高了高达80%。", "conclusion": "研究结果表明，ACTOR-CURATOR是一种强大且实用的方法，适用于大规模语言模型的后期强化学习训练。"}}
{"id": "2602.20531", "pdf": "https://arxiv.org/pdf/2602.20531", "abs": "https://arxiv.org/abs/2602.20531", "authors": ["Azrin Sultana", "Firoz Ahmed"], "title": "A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata", "categories": ["cs.CV"], "comment": "24 pages, 10 figures", "summary": "App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.", "AI": {"tldr": "本文提出了一种轻量级的视觉语言融合框架，用于从应用程序界面和元数据预测应用评分。", "motivation": "现有的应用评分预测模型主要依赖于文本或用户界面特征，忽视了同时利用UI和语义信息的重要性。为了克服这些限制，本研究提出了一个将视觉和文本信息相结合的新框架。", "method": "该框架采用MobileNetV3提取UI布局的视觉特征，并使用DistilBERT抽取文本特征；通过带有Swish激活函数的门控融合模块整合这两种模态的信息，随后利用多层感知机回归头进行评分预测。模型训练20轮后，在MAE、RMSE等指标上表现良好。", "result": "经过评估，提出的模型在测试数据集上的性能优异：MAE为0.1060，RMSE为0.1433，MSE为0.0205，R²系数为0.8529，皮尔逊相关性达到0.9251。", "conclusion": "所提出的轻量级框架对于开发者和最终用户来说具有重要价值，支持可持续的应用程序开发，并且能够在边缘设备上高效部署。"}}
{"id": "2602.20530", "pdf": "https://arxiv.org/pdf/2602.20530", "abs": "https://arxiv.org/abs/2602.20530", "authors": ["Ming Li", "Yong-Jin Liu", "Fang Liu", "Huankun Sheng", "Yeying Fan", "Yixiang Wei", "Minnan Luo", "Weizhan Zhang", "Wenping Wang"], "title": "Memory-guided Prototypical Co-occurrence Learning for Mixed Emotion Recognition", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Emotion recognition from multi-modal physiological and behavioral signals plays a pivotal role in affective computing, yet most existing models remain constrained to the prediction of singular emotions in controlled laboratory settings. Real-world human emotional experiences, by contrast, are often characterized by the simultaneous presence of multiple affective states, spurring recent interest in mixed emotion recognition as an emotion distribution learning problem. Current approaches, however, often neglect the valence consistency and structured correlations inherent among coexisting emotions. To address this limitation, we propose a Memory-guided Prototypical Co-occurrence Learning (MPCL) framework that explicitly models emotion co-occurrence patterns. Specifically, we first fuse multi-modal signals via a multi-scale associative memory mechanism. To capture cross-modal semantic relationships, we construct emotion-specific prototype memory banks, yielding rich physiological and behavioral representations, and employ prototype relation distillation to ensure cross-modal alignment in the latent prototype space. Furthermore, inspired by human cognitive memory systems, we introduce a memory retrieval strategy to extract semantic-level co-occurrence associations across emotion categories. Through this bottom-up hierarchical abstraction process, our model learns affectively informative representations for accurate emotion distribution prediction. Comprehensive experiments on two public datasets demonstrate that MPCL consistently outperforms state-of-the-art methods in mixed emotion recognition, both quantitatively and qualitatively.", "AI": {"tldr": "提出了一种记忆引导的原型共现学习框架，用于混合情绪识别", "motivation": "现有模型在预测单一情感时受限，并且忽视了共存情绪之间的效价一致性和结构化关联。为了解决这个问题，引入一种新的方法来处理共存的情绪模式", "method": "通过多尺度相关内存机制融合多模态信号；构建特定于情绪的原型记忆库以捕获跨模态语义关系，采用原型关系蒸馏保证潜在原型空间中的跨模态对齐；并利用认知记忆系统启发的记忆检索策略提取情感类别间的共现关联", "result": "在两个公开数据集上的全面实验表明，所提出的方法MPCL在混合情绪识别中优于最先进的方法", "conclusion": "提出的Memory-guided Prototypical Co-occurrence Learning框架能够有效处理多模态信号并准确预测情绪分布"}}
{"id": "2602.20527", "pdf": "https://arxiv.org/pdf/2602.20527", "abs": "https://arxiv.org/abs/2602.20527", "authors": ["Md Mirajul Islam", "Xi Yang", "Adittya Soukarjya Saha", "Rajesh Debnath", "Min Chi"], "title": "A Generalized Apprenticeship Learning Framework for Capturing Evolving Student Pedagogical Strategies", "categories": ["cs.LG", "cs.AI"], "comment": "16 pages", "summary": "Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) have advanced rapidly in recent years and have been successfully applied to e-learning environments like intelligent tutoring systems (ITSs). Despite great success, the broader application of DRL to educational technologies has been limited due to major challenges such as sample inefficiency and difficulty designing the reward function. In contrast, Apprenticeship Learning (AL) uses a few expert demonstrations to infer the expert's underlying reward functions and derive decision-making policies that generalize and replicate optimal behavior. In this work, we leverage a generalized AL framework, THEMES, to induce effective pedagogical policies by capturing the complexities of the expert student learning process, where multiple reward functions may dynamically evolve over time. We evaluate the effectiveness of THEMES against six state-of-the-art baselines, demonstrating its superior performance and highlighting its potential as a powerful alternative for inducing effective pedagogical policies and show that it can achieve high performance, with an AUC of 0.899 and a Jaccard of 0.653, using only 18 trajectories of a previous semester to predict student pedagogical decisions in a later semester.", "AI": {"tldr": "该论文提出了一个通用的学徒学习框架THEMES，用于在智能辅导系统中捕捉学生动态变化的教学策略。", "motivation": "尽管强化学习和深度强化学习近年来取得了重大进展，并成功应用于e-learning环境如智能辅导系统，但由于样本效率低下和奖励函数设计困难等挑战，其更广泛的应用受到限制。学徒学习通过少量的专家演示来推断出最优行为背后的奖励函数。", "method": "该论文利用一种通用的学徒学习框架THEMES，捕捉复杂的学生学习过程中的多个动态变化的奖励函数，并从这些示范中推导出有效的教学策略。", "result": "实验结果表明，与六个最先进的基准方法相比，THEMES表现出优越性能，在预测学生教学决策方面取得了高绩效（AUC为0.899和Jaccard为0.653），仅使用上个学期的18条轨迹就实现了这一目标。", "conclusion": "该研究展示了学徒学习框架THEMES在教育技术中的潜力，作为诱导有效教学策略的强大替代方案。"}}
{"id": "2602.20520", "pdf": "https://arxiv.org/pdf/2602.20520", "abs": "https://arxiv.org/abs/2602.20520", "authors": ["Pratham Yashwante", "Davit Abrahamyan", "Shresth Grover", "Sukruth Rao"], "title": "How Do Inpainting Artifacts Propagate to Language?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.", "AI": {"tldr": "研究视觉修复瑕疵如何影响多模态系统中的语言生成。", "motivation": "探究扩散基础的图像修补引入的视觉瑕疵对语言模型的影响。", "method": "采用两阶段诊断设置，其中被遮掩区域被重建后提供给文字描述模型，并进行原始和修复输入之间的生成文本质量对比分析。", "result": "观察到像素级别和感知级别的修复指标与词汇及语义性能之间存在持续关联。进一步的中间视觉表示和注意模式分析表明修补瑕疵导致系统行为层依赖变化。", "conclusion": "这些结果提供了一个实际诊断框架，用于研究视觉重构质量如何影响多模态系统的语言生成。"}}
{"id": "2602.20517", "pdf": "https://arxiv.org/pdf/2602.20517", "abs": "https://arxiv.org/abs/2602.20517", "authors": ["Rakshit Trivedi", "Kartik Sharma", "David C Parkes"], "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Spotlight paper at NeurIPS 2025", "summary": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.", "AI": {"tldr": "本文提出了一种基于语言的框架MIMIC，用于人类与AI之间的协调任务。", "motivation": "当前的方法在模仿和适应多变的人类行为时存在不足，缺乏引导行为的能力。为了解决这个问题，作者借鉴了人类认知过程中的内心言语理论来设计新的模型。", "method": "MIMIC框架使用视觉语言模型作为语言支撑结构训练一个条件变异自动编码器，生成内部话语，并利用基于扩散的行为克隆策略选择动作。", "result": "实验结果表明，MIMIC在行为多样性和模仿人类演示的准确性方面显著优于现有方法。", "conclusion": "本文提出的方法可以有效提高AI在不同任务中与人合作时的表现，特别是在引导行为方面。"}}
{"id": "2602.20512", "pdf": "https://arxiv.org/pdf/2602.20512", "abs": "https://arxiv.org/abs/2602.20512", "authors": ["Haitong He", "Xuemian Wu", "Shizhe Zhao", "Zhongqiang Ren"], "title": "Conflict-Based Search for Multi-Agent Path Finding with Elevators", "categories": ["cs.RO"], "comment": null, "summary": "This paper investigates a problem called Multi-Agent Path Finding with Elevators (MAPF-E), which seeks conflict-free paths for multiple agents from their start to goal locations that may locate on different floors, and the agents can use elevators to travel between floors. The existence of elevators complicates the interaction among the agents and introduces new challenges to the planning. On the one hand, elevators can cause many conflicts among the agents due to its relatively long traversal time across floors, especially when many agents need to reach a different floor. On the other hand, the planner has to reason in a larger state space including the states of the elevators, besides the locations of the agents.", "AI": {"tldr": "研究了多代理路径查找带电梯问题（MAPF-E），寻找多个代理在不同楼层间无冲突的路径，同时考虑使用电梯的情况。", "motivation": "解决多个代理在不同楼层间移动时，通过使用电梯进行楼层转换的问题。电梯的存在增加了代理间的交互复杂性，并带来了新的挑战。", "method": "提出了一种基于冲突的方法（Conflict-Based Search），用于处理多代理路径查找带电梯问题中的新情况和状态空间扩展。", "result": "该方法能够有效解决带有电梯的多个代理在不同楼层间移动时的无冲突路径规划问题，展示了良好的性能和适用性。", "conclusion": "通过引入基于冲突的方法，成功解决了MAPF-E问题。未来的研究可以进一步优化算法以提高效率和处理更多的复杂情况。"}}
{"id": "2602.20511", "pdf": "https://arxiv.org/pdf/2602.20511", "abs": "https://arxiv.org/abs/2602.20511", "authors": ["Limai Jiang", "Ruitao Xie", "Bokai Yang", "Huazhen Huang", "Juan He", "Yufu Huo", "Zikai Wang", "Yang Wei", "Yunpeng Cai"], "title": "Leveraging Causal Reasoning Method for Explaining Medical Image Segmentation Models", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Medical image segmentation plays a vital role in clinical decision-making, enabling precise localization of lesions and guiding interventions. Despite significant advances in segmentation accuracy, the black-box nature of most deep models has raised growing concerns about their trustworthiness in high-stakes medical scenarios. Current explanation techniques have primarily focused on classification tasks, leaving the segmentation domain relatively underexplored. We introduced an explanation model for segmentation task which employs the causal inference framework and backpropagates the average treatment effect (ATE) into a quantification metric to determine the influence of input regions, as well as network components, on target segmentation areas. Through comparison with recent segmentation explainability techniques on two representative medical imaging datasets, we demonstrated that our approach provides more faithful explanations than existing approaches. Furthermore, we carried out a systematic causal analysis of multiple foundational segmentation models using our method, which reveals significant heterogeneity in perceptual strategies across different models, and even between different inputs for the same model. Suggesting the potential of our method to provide notable insights for optimizing segmentation models. Our code can be found at https://github.com/lcmmai/PdCR.", "AI": {"tldr": "本文提出了一种基于因果推理框架的解释模型，用于医疗图像分割任务。", "motivation": "由于深度学习模型在医疗图像分割中的黑盒特性，在高风险医学场景中引发了对其可信度的担忧。当前的解释技术主要集中在分类任务上，而对分割领域研究较少。", "method": "本文采用了因果推理框架和反向传播平均处理效应（ATE）量化指标来确定输入区域以及网络组件对目标分割区域的影响。", "result": "通过与近期分割可解释性技术在两个代表性医学成像数据集上的对比，展示了该方法比现有方法提供了更忠实的解释。此外，使用本方法对多个基础分割模型进行了系统因果分析，揭示了不同模型之间以及同一模型的不同输入之间的感知策略存在显著异质性。", "conclusion": "本文提出的方法为优化分割模型提供了有价值的见解，并且可以在代码库中找到。"}}
{"id": "2602.20507", "pdf": "https://arxiv.org/pdf/2602.20507", "abs": "https://arxiv.org/abs/2602.20507", "authors": ["William Anthony Mason"], "title": "Indaleko: The Unified Personal Index", "categories": ["cs.IR", "cs.HC"], "comment": "PhD dissertation, University of British Columbia, August 2025. 287 pages", "summary": "Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like \"photos near the conference venue last spring\" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.", "AI": {"tldr": "Indaleko是一种统一的个人索引架构，它通过整合时间、空间和活动元数据来支持自然语言查询。", "motivation": "当前的信息检索系统忽略了人类记忆的工作方式，导致搜索效率低下。为了克服这一问题，提出了一个与认知理论相结合的分布式系统设计框架。", "method": "该方法利用统一的图数据库将时间和地点等信息整合在一起，并通过内存锚索引实现快速查询响应。", "result": "在3100万文件的数据集上，Indaleko能够处理多维查询并提供精确的结果。相比之下，现有系统无法有效处理这些基于记忆的查询。", "conclusion": "该研究工作改变了个人信息检索从关键字匹配到与记忆相适应的发现方法，并为未来的上下文感知系统奠定了基础。"}}
{"id": "2602.20502", "pdf": "https://arxiv.org/pdf/2602.20502", "abs": "https://arxiv.org/abs/2602.20502", "authors": ["Hongbin Zhong", "Fazle Faisal", "Luis França", "Tanakorn Leesatapornwongsa", "Adriana Szekeres", "Kexin Rong", "Suman Nath"], "title": "ActionEngine: From Reactive to Programmatic GUI Agents via State Machine Memory", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Existing Graphical User Interface (GUI) agents operate through step-by-step calls to vision language models--taking a screenshot, reasoning about the next action, executing it, then repeating on the new page--resulting in high costs and latency that scale with the number of reasoning steps, and limited accuracy due to no persistent memory of previously visited pages. We propose ActionEngine, a training-free framework that transitions from reactive execution to programmatic planning through a novel two-agent architecture: a Crawling Agent that constructs an updatable state-machine memory of the GUIs through offline exploration, and an Execution Agent that leverages this memory to synthesize complete, executable Python programs for online task execution. To ensure robustness against evolving interfaces, execution failures trigger a vision-based re-grounding fallback that repairs the failed action and updates the memory. This design drastically improves both efficiency and accuracy: on Reddit tasks from the WebArena benchmark, our agent achieves 95% task success with on average a single LLM call, compared to 66% for the strongest vision-only baseline, while reducing cost by 11.8x and end-to-end latency by 2x. Together, these components yield scalable and reliable GUI interaction by combining global programmatic planning, crawler-validated action templates, and node-level execution with localized validation and repair.", "AI": {"tldr": "提出ActionEngine框架，通过构建GUI的状态机记忆来提升图形用户界面代理的效率和准确性。", "motivation": "现有的GUI代理在执行任务时成本高、延迟长且准确率低，因为它们缺乏对先前页面的记忆，并且每次操作都需要调用视觉语言模型。", "method": "引入了两个代理架构：爬虫代理通过离线探索来构建可更新的状态机记忆；执行代理利用这个记忆来合成完整的可执行Python程序来进行在线任务执行。当遇到执行失败时，会触发基于视觉的重新定位回退机制以修复操作并更新内存。", "result": "在WebArena基准测试中，该框架实现了95%的任务成功率，并且平均只需要一次LLM调用；相比最强的纯视觉基线模型，准确率提高了66%，成本降低11.8倍，端到端延迟降低了两倍。", "conclusion": "通过结合全局程序化规划、爬虫验证的操作模板以及节点级执行和局部验证与修复机制，该设计提供了一个可扩展且可靠的GUI交互方案"}}
{"id": "2602.20501", "pdf": "https://arxiv.org/pdf/2602.20501", "abs": "https://arxiv.org/abs/2602.20501", "authors": ["Qing Zhang", "Xuesong Li", "Jing Zhang"], "title": "Probing and Bridging Geometry-Interaction Cues for Affordance Reasoning in Vision Foundation Models", "categories": ["cs.CV"], "comment": "11 pages, 12 figures, Accepted to CVPR 2026", "summary": "What does it mean for a visual system to truly understand affordance? We argue that this understanding hinges on two complementary capacities: geometric perception, which identifies the structural parts of objects that enable interaction, and interaction perception, which models how an agent's actions engage with those parts. To test this hypothesis, we conduct a systematic probing of Visual Foundation Models (VFMs). We find that models like DINO inherently encode part-level geometric structures, while generative models like Flux contain rich, verb-conditioned spatial attention maps that serve as implicit interaction priors. Crucially, we demonstrate that these two dimensions are not merely correlated but are composable elements of affordance. By simply fusing DINO's geometric prototypes with Flux's interaction maps in a training-free and zero-shot manner, we achieve affordance estimation competitive with weakly-supervised methods. This final fusion experiment confirms that geometric and interaction perception are the fundamental building blocks of affordance understanding in VFMs, providing a mechanistic account of how perception grounds action.", "AI": {"tldr": "研究探讨视觉基础模型中几何感知和交互感知在理解可操作性方面的角色，并通过无训练融合方式实现与弱监督方法相当的可操作性估计。", "motivation": "为了验证视觉系统真正理解和预测物体如何被使用是否依赖于两个互补的能力：几何感知（识别对象结构）和交互感知（模拟动作与其部件的关系），研究对视觉基础模型进行系统的探究。", "method": "通过无监督方式测试DINO等模型的固有编码能力，以及Flux生成模型中丰富、条件化的空间注意图作为隐式互动先验。最终采用简单融合两种模型的方式，在零样本设置下实现可操作性估计。", "result": "发现视觉基础模型中的几何原型与交互地图可以组成一个强大的系统来执行可操作性推理，且在无训练和零样本条件下实现了与弱监督方法相当的性能表现。", "conclusion": "研究表明，几何感知和互动感知是视觉基础模型中理解和预测可操作性的基本构建块，为感知如何指导行动提供了机制解释。"}}
{"id": "2602.20500", "pdf": "https://arxiv.org/pdf/2602.20500", "abs": "https://arxiv.org/abs/2602.20500", "authors": ["Keyu Zhou", "Peisen Xu", "Yahao Wu", "Jiming Chen", "Gaofeng Li", "Shunlei Li"], "title": "Strategy-Supervised Autonomous Laparoscopic Camera Control via Event-Driven Graph Mining", "categories": ["cs.RO", "cs.CV"], "comment": "Submitted to IEEE Transactions on Robotics (T-RO). 19 pages, 9 figures", "summary": "Autonomous laparoscopic camera control must maintain a stable and safe surgical view under rapid tool-tissue interactions while remaining interpretable to surgeons. We present a strategy-grounded framework that couples high-level vision-language inference with low-level closed-loop control. Offline, raw surgical videos are parsed into camera-relevant temporal events (e.g., interaction, working-distance deviation, and view-quality degradation) and structured as attributed event graphs. Mining these graphs yields a compact set of reusable camera-handling strategy primitives, which provide structured supervision for learning. Online, a fine-tuned Vision-Language Model (VLM) processes the live laparoscopic view to predict the dominant strategy and discrete image-based motion commands, executed by an IBVS-RCM controller under strict safety constraints; optional speech input enables intuitive human-in-the-loop conditioning. On a surgeon-annotated dataset, event parsing achieves reliable temporal localization (F1-score 0.86), and the mined strategies show strong semantic alignment with expert interpretation (cluster purity 0.81). Extensive ex vivo experiments on silicone phantoms and porcine tissues demonstrate that the proposed system outperforms junior surgeons in standardized camera-handling evaluations, reducing field-of-view centering error by 35.26% and image shaking by 62.33%, while preserving smooth motion and stable working-distance regulation.", "AI": {"tldr": "提出了一种策略指导的自主腹腔镜摄像机控制系统，通过事件驱动图挖掘实现稳定且安全的手术视野控制。", "motivation": "为了在快速工具组织交互中保持稳定的和可解释给外科医生的手术视场。", "method": "离线阶段，将原始手术视频解析为相机相关的时序事件并构造有向属性图；在线阶段，利用细调后的视觉-语言模型预测策略，并通过IBVS-RCM控制器执行运动命令。", "result": "在专家注释的数据集上实现了可靠的时间定位（F1-score 0.86），挖掘出的策略与专家解释具有强语义对齐（聚类纯度 0.81）。实验表明，该系统优于初级外科医生，在标准化摄像机处理评估中降低了视场中心误差35.26％和图像抖动62.33％。", "conclusion": "提出的系统能够有效提升腹腔镜手术视野的稳定性与安全性，并保持流畅和平稳的工作距离调节。"}}
{"id": "2602.20497", "pdf": "https://arxiv.org/pdf/2602.20497", "abs": "https://arxiv.org/abs/2602.20497", "authors": ["Peiliang Cai", "Jiacheng Liu", "Haowen Xu", "Xinyu Wang", "Chang Zou", "Linfeng Zhang"], "title": "LESA: Learnable Stage-Aware Predictors for Diffusion Model Acceleration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image and video generation tasks. However, the high computational demands of Diffusion Transformers (DiTs) pose a significant challenge to their practical deployment. While feature caching is a promising acceleration strategy, existing methods based on simple reusing or training-free forecasting struggle to adapt to the complex, stage-dependent dynamics of the diffusion process, often resulting in quality degradation and failing to maintain consistency with the standard denoising process. To address this, we propose a LEarnable Stage-Aware (LESA) predictor framework based on two-stage training. Our approach leverages a Kolmogorov-Arnold Network (KAN) to accurately learn temporal feature mappings from data. We further introduce a multi-stage, multi-expert architecture that assigns specialized predictors to different noise-level stages, enabling more precise and robust feature forecasting. Extensive experiments show our method achieves significant acceleration while maintaining high-fidelity generation. Experiments demonstrate 5.00x acceleration on FLUX.1-dev with minimal quality degradation (1.0% drop), 6.25x speedup on Qwen-Image with a 20.2% quality improvement over the previous SOTA (TaylorSeer), and 5.00x acceleration on HunyuanVideo with a 24.7% PSNR improvement over TaylorSeer. State-of-the-art performance on both text-to-image and text-to-video synthesis validates the effectiveness and generalization capability of our training-based framework across different models. Our code is included in the supplementary materials and will be released on GitHub.", "AI": {"tldr": "本文提出了一种基于两阶段训练的LESA预测器框架，以加速扩散模型。", "motivation": "现有的基于简单重用或无训练预报的方法难以适应扩散过程中的复杂、阶段依赖性动态变化，导致质量下降和与标准去噪过程一致性缺失。为此，提出了基于可学习阶段感知的预测器框架来解决这一问题。", "method": "该方法利用Kolmogorov-Arnold网络准确地从数据中学习时间特征映射，并通过多阶段、多专家架构分配专门针对不同噪声水平阶段的预报器，实现更精确和稳健的特征预报。", "result": "实验表明该方法实现了显著加速，同时保持了高质量生成。在FLUX.1-dev上达到了5.00倍加速而质量仅下降1%，在Qwen-Image上的速度提高了6.25倍且质量提升了20.2%（相比之前的SOTA），HunyuanVideo则有5.00倍的加速和24.7% PSNR改进。", "conclusion": "LESA框架验证了基于训练的方法在不同模型中的有效性和泛化能力。"}}
{"id": "2602.20496", "pdf": "https://arxiv.org/pdf/2602.20496", "abs": "https://arxiv.org/abs/2602.20496", "authors": ["Jintu Zheng", "Qizhe Liu", "HuangXin Xu", "Zhuojie Chen"], "title": "Pip-Stereo: Progressive Iterations Pruner for Iterative Optimization based Stereo Matching", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026 (3D vision track)", "summary": "While iterative stereo matching achieves high accuracy, its dependence on Recurrent Neural Networks (RNN) hinders edge deployment, a challenge underexplored in existing researches. We analyze iterative refinement and reveal that disparity updates are spatially sparse and temporally redundant. First, we introduce a progressive iteration pruning strategy that suppresses redundant update steps, effectively collapsing the recursive computation into a near-single-pass inference. Second, we propose a collaborative monocular prior transfer framework that implicitly embeds depth priors without requiring a dedicated monocular encoder, thereby eliminating its associated computational burden. Third, we develop FlashGRU, a hardware-aware RNN operator leveraging structured sparsity and I/O-conscious design, achieving a 7.28$\\times$ speedup, 76.6\\% memory peak reduction and 80.9\\% global memory requests reduction over natvie ConvGRUs under 2K resolution. Our PipStereo enables real-time, high-fidelity stereo matching on edge hardware: it processes 320$\\times$640 frames in just 75ms on an NVIDIA Jetson Orin NX (FP16) and 19ms on RTX 4090, matching the accuracy of large iterative based models, and our generalization ability and accuracy far exceeds that of existing real-time methods. Our embedded AI projects will be updated at: https://github.com/XPENG-Aridge-AI.", "AI": {"tldr": "本文提出了一种名为Pip-Stereo的新型算法，该算法通过迭代优化的方法提高立体匹配的速度和精度。", "motivation": "传统的迭代立体匹配依赖于循环神经网络（RNN），这在边缘设备上的部署面临挑战。本文旨在解决这一问题，并实现高效的实时立体匹配。", "method": "首先引入了一种渐进式迭代剪枝策略，以减少冗余的更新步骤；其次提出了一种协作的单目先验迁移框架，隐含嵌入深度先验而无需专用的单目编码器；最后开发了FlashGRU硬件感知RNN运算符，利用结构化稀疏性和输入/输出意识设计。", "result": "Pip-Stereo在NVIDIA Jetson Orin NX（FP16）上处理320×640帧仅需75ms，在RTX 4090上只需19ms，并且精度与大型迭代方法相当，甚至超越了现有的实时方法。", "conclusion": "Pip-Stereo能够在边缘硬件上实现高效、高保真度的立体匹配，不仅提高了速度和精度，还增强了泛化能力。"}}
{"id": "2602.20494", "pdf": "https://arxiv.org/pdf/2602.20494", "abs": "https://arxiv.org/abs/2602.20494", "authors": ["Haotian Si", "Changhua Pei", "Xiao He", "Zeyan Li", "Zhe Xie", "Zexin Wang", "Jiyao Hu", "Zhaoyang Yu", "Tieying Zhang", "Dan Pei", "Jianhui Li", "Gaogang Xie"], "title": "KairosVL: Orchestrating Time Series and Semantics for Unified Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Driven by the increasingly complex and decision-oriented demands of time series analysis, we introduce the Semantic-Conditional Time Series Reasoning task, which extends conventional time series analysis beyond purely numerical modeling to incorporate contextual and semantic understanding. To further enhance the mode's reasoning capabilities on complex time series problems, we propose a two-round reinforcement learning framework: the first round strengthens the mode's perception of fundamental temporal primitives, while the second focuses on semantic-conditioned reasoning. The resulting model, KairosVL, achieves competitive performance across both synthetic and real-world tasks. Extensive experiments and ablation studies demonstrate that our framework not only boosts performance but also preserves intrinsic reasoning ability and significantly improves generalization to unseen scenarios. To summarize, our work highlights the potential of combining semantic reasoning with temporal modeling and provides a practical framework for real-world time series intelligence, which is in urgent demand.", "AI": {"tldr": "本文提出了一个结合语义条件的时间序列推理任务，并提出了一种两阶段的强化学习框架，以提升时间序列分析的性能。", "motivation": "为了满足日益复杂的决策导向性需求，传统的时间序列分析方法需要扩展到包含上下文和语义理解。这项工作旨在通过引入新的推理任务来增强模型的能力，以处理复杂的时间序列问题。", "method": "提出了一种两阶段强化学习框架：第一阶段加强了时间模式的感知能力；第二阶段则侧重于基于语义条件的推理。", "result": "实验表明，所提出的KairosVL模型在合成和实际任务中都达到了竞争力的表现，并且保持了内在的推理能力和对未见过场景的泛化能力显著提升。", "conclusion": "本文展示了结合语义推理与时间建模的潜力，并提供了一个实用框架来解决现实世界中的时间序列智能问题。"}}
{"id": "2602.20492", "pdf": "https://arxiv.org/pdf/2602.20492", "abs": "https://arxiv.org/abs/2602.20492", "authors": ["Nuocheng Yang", "Sihua Wang", "Ouwen Huan", "Mingzhe Chen", "Tony Q. S. Quek", "Changchuan Yin"], "title": "Wireless Federated Multi-Task LLM Fine-Tuning via Sparse-and-Orthogonal LoRA", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Decentralized federated learning (DFL) based on low-rank adaptation (LoRA) enables mobile devices with multi-task datasets to collaboratively fine-tune a large language model (LLM) by exchanging locally updated parameters with a subset of neighboring devices via wireless connections for knowledge integration.However, directly aggregating parameters fine-tuned on heterogeneous datasets induces three primary issues across the DFL life-cycle: (i) \\textit{catastrophic knowledge forgetting during fine-tuning process}, arising from conflicting update directions caused by data heterogeneity; (ii) \\textit{inefficient communication and convergence during model aggregation process}, due to bandwidth-intensive redundant model transmissions; and (iii) \\textit{multi-task knowledge interference during inference process}, resulting from incompatible knowledge representations coexistence during inference. To address these issues in a fully decentralized scenario, we first propose a sparse-and-orthogonal LoRA that ensures orthogonality between model updates to eliminate direction conflicts during fine-tuning.Then, we analyze how device connection topology affects multi-task performance, prompting a cluster-based topology design during aggregation.Finally, we propose an implicit mixture of experts (MoE) mechanism to avoid the coexistence of incompatible knowledge during inference. Simulation results demonstrate that the proposed approach effectively reduces communication resource consumption by up to $73\\%$ and enhances average performance by $5\\%$ compared with the traditional LoRA method.", "AI": {"tldr": "该论文提出了一种无线联邦多任务LLM微调方法，通过稀疏和正交的LoRA解决分布式学习中的三个主要问题：灾难性知识遗忘、低效通信与收敛以及推理过程中的任务干扰。", "motivation": "为了解决在异构数据集上微调过程中直接聚合参数导致的知识遗忘、效率低下及多任务间干扰等问题，该论文提出了一种新的方法来优化联邦学习流程。", "method": "首先提出了稀疏和正交的LoRA以消除模型更新中的方向冲突；其次分析了设备连接拓扑对多任务性能的影响，并设计了基于聚类的方法进行聚合；最后引入了一个隐式专家混合机制来避免推理过程中不兼容知识的存在。", "result": "实验结果表明，所提出的方法能够显著降低通信资源消耗（最多减少73%），并提高平均性能5%。", "conclusion": "该方法有效地解决了分布式联邦多任务LLM微调中遇到的问题，并提高了系统的整体效率和准确性。"}}
{"id": "2602.20486", "pdf": "https://arxiv.org/pdf/2602.20486", "abs": "https://arxiv.org/abs/2602.20486", "authors": ["Paras Sharma", "YuePing Sha", "Janet Shufor Bih Epse Fofang", "Brayden Yan", "Jess A. Turner", "Nicole Balay", "Hubert O. Asare", "Angela E. B. Stewart", "Erin Walker"], "title": "Hybrid LLM-Embedded Dialogue Agents for Learner Reflection: Designing Responsive and Theory-Driven Interactions", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Dialogue systems have long supported learner reflections, with theoretically grounded, rule-based designs offering structured scaffolding but often struggling to respond to shifts in engagement. Large Language Models (LLMs), in contrast, can generate context-sensitive responses but are not informed by decades of research on how learning interactions should be structured, raising questions about their alignment with pedagogical theories. This paper presents a hybrid dialogue system that embeds LLM responsiveness within a theory-aligned, rule-based framework to support learner reflections in a culturally responsive robotics summer camp. The rule-based structure grounds dialogue in self-regulated learning theory, while the LLM decides when and how to prompt deeper reflections, responding to evolving conversation context. We analyze themes across dialogues to explore how our hybrid system shaped learner reflections. Our findings indicate that LLM-embedded dialogues supported richer learner reflections on goals and activities, but also introduced challenges due to repetitiveness and misalignment in prompts, reducing engagement.", "AI": {"tldr": "本文提出了一个结合大型语言模型（LLM）响应性的混合对话系统，以支持学习者的反思。", "motivation": "传统的基于规则的对话设计难以应对参与度的变化，而大型语言模型虽然能生成上下文敏感的回应，但缺乏对如何结构化学习互动的研究。因此本文旨在将两种方法结合使用，以便更好地支持学习者在文化响应性机器人夏令营中的反思。", "method": "该系统采用基于规则的设计来支撑自我调节学习理论，同时通过大型语言模型决定何时以及如何引发更深入的反思，并根据对话上下文作出回应。作者分析了对话主题以探索混合系统的使用对学习者反思的影响。", "result": "研究发现，嵌入LLM的对话支持了关于目标和活动的更为丰富的学习者的反思，但也引入了一些挑战，包括重复性问题以及提示与理论不一致的情况，从而降低了参与度。", "conclusion": "虽然混合系统在促进学生更深层次的思考方面表现出色，但仍需解决一些潜在的问题以进一步提高其有效性。"}}
{"id": "2602.20480", "pdf": "https://arxiv.org/pdf/2602.20480", "abs": "https://arxiv.org/abs/2602.20480", "authors": ["Shubhanshu Shekhar", "Mohammad Javad Khojasteh", "Ananya Acharya", "Tony Tohme", "Kamal Youcef-Toumi"], "title": "VINA: Variational Invertible Neural Architectures", "categories": ["cs.LG", "cs.AI"], "comment": "57 pages, 11 figures, 5 tables", "summary": "The distinctive architectural features of normalizing flows (NFs), notably bijectivity and tractable Jacobians, make them well-suited for generative modeling. Invertible neural networks (INNs) build on these principles to address supervised inverse problems, enabling direct modeling of both forward and inverse mappings. In this paper, we revisit these architectures from both theoretical and practical perspectives and address a key gap in the literature: the lack of theoretical guarantees on approximation quality under realistic assumptions, whether for posterior inference in INNs or for generative modeling with NFs. We introduce a unified framework for INNs and NFs based on variational unsupervised loss functions, inspired by analogous formulations in related areas such as generative adversarial networks (GANs) and the Precision-Recall divergence for training normalizing flows. Within this framework, we derive theoretical performance guarantees, quantifying posterior accuracy for INNs and distributional accuracy for NFs, under assumptions that are weaker and more practically realistic than those used in prior work. Building on these theoretical results, we conduct extensive case studies to distill general design principles and practical guidelines. We conclude by demonstrating the effectiveness of our approach on a realistic ocean-acoustic inversion problem.", "AI": {"tldr": "本文通过引入基于变分无监督损失函数的统一框架，对可逆神经网络和归一化流进行理论性能保证的研究，并在真实海洋声学反演问题中验证了其有效性。", "motivation": "现有文献缺乏关于后验推断和生成建模在实际假设下的逼近质量理论保障。本文旨在填补这一空白，通过引入新的框架来提供更为实用且合理的理论依据。", "method": "提出了基于变分无监督损失函数的统一框架，并在此基础上推导出对可逆神经网络和归一化流的性能保证。", "result": "在真实海洋声学反演问题上展示了所提方法的有效性，同时通过详尽的研究提炼出了通用的设计原则与实践指导。", "conclusion": "本文提出的统一框架及其理论保障为处理实际问题提供了有力工具，并验证了其在现实世界中的应用潜力。"}}
{"id": "2602.20479", "pdf": "https://arxiv.org/pdf/2602.20479", "abs": "https://arxiv.org/abs/2602.20479", "authors": ["Lin Li", "Ziqi Jiang", "Gefan Ye", "Zhenqi He", "Jiahui Li", "Jun Xiao", "Kwang-Ting Cheng", "Long Chen"], "title": "Path-Decoupled Hyperbolic Flow Matching for Few-Shot Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in cross-modal few-shot adaptation treat visual-semantic alignment as a continuous feature transport problem via Flow Matching (FM). However, we argue that Euclidean-based FM overlooks fundamental limitations of flat geometry, where polynomial volume growth fails to accommodate diverse feature distributions, leading to severe path entanglement. To this end, we propose path-decoupled Hyperbolic Flow Matching (HFM), leveraging the Lorentz manifold's exponential expansion for trajectory decoupling. HFM structures the transport via two key designs: 1) Centripetal hyperbolic alignment: It constructs a centripetal hierarchy by anchoring textual roots, which pushes visual leaves to the boundary to initialize orderly flows. 2) Path-decoupled objective: It acts as a ``semantic guardrail'' rigidly confining trajectories within isolated class-specific geodesic corridors via step-wise supervision. Furthermore, we devise an adaptive diameter-based stopping to prevent over-transportation into the crowded origin based on the intrinsic semantic scale. Extensive ablations on 11 benchmarks have shown that HFM establishes a new state-of-the-art, consistently outperforming its Euclidean counterparts. Our codes and models will be released.", "AI": {"tldr": "本文提出了一种新的路径解耦双曲流匹配方法，旨在解决视觉语义对齐中的路径纠缠问题，并通过实验验证了其优越性。", "motivation": "传统的基于欧几里得的空间流动匹配技术忽略了平坦几何的局限性，导致严重的路径纠缠和不理想的特征分布。因此，文章动机在于开发一种新的流匹配方法来克服这些问题。", "method": "本文提出了双曲流匹配(HFM)，该方法利用Lorentz流形上的指数扩展实现轨迹解耦，并通过中心向心式的双曲对齐和路径解耦的目标函数设计了两种关键方案。此外，还提出了一种基于自适应直径停止的方法来防止过度传输。", "result": "在11个基准测试上进行的广泛消融研究表明，HFM建立了新的最先进的性能，并且持续优于其欧几里得对应方法。", "conclusion": "该研究通过引入双曲流匹配解决了传统流动匹配中的路径纠缠问题，并展示了优越的结果。未来的工作可能包括将这种方法应用于其他类型的数据和任务中。"}}
{"id": "2602.20476", "pdf": "https://arxiv.org/pdf/2602.20476", "abs": "https://arxiv.org/abs/2602.20476", "authors": ["Anindita Ghosh", "Vladislav Golyanik", "Taku Komura", "Philipp Slusallek", "Christian Theobalt", "Rishabh Dabral"], "title": "SceMoS: Scene-Aware 3D Human Motion Synthesis by Planning with Geometry-Grounded Tokens", "categories": ["cs.CV"], "comment": "13 pages, 6 figures, 4 tables", "summary": "Synthesizing text-driven 3D human motion within realistic scenes requires learning both semantic intent (\"walk to the couch\") and physical feasibility (e.g., avoiding collisions). Current methods use generative frameworks that simultaneously learn high-level planning and low-level contact reasoning, and rely on computationally expensive 3D scene data such as point clouds or voxel occupancy grids. We propose SceMoS, a scene-aware motion synthesis framework that shows that structured 2D scene representations can serve as a powerful alternative to full 3D supervision in physically grounded motion synthesis. SceMoS disentangles global planning from local execution using lightweight 2D cues and relying on (1) a text-conditioned autoregressive global motion planner that operates on a bird's-eye-view (BEV) image rendered from an elevated corner of the scene, encoded with DINOv2 features, as the scene representation, and (2) a geometry-grounded motion tokenizer trained via a conditional VQ-VAE, that uses 2D local scene heightmap, thus embedding surface physics directly into a discrete vocabulary. This 2D factorization reaches an efficiency-fidelity trade-off: BEV semantics capture spatial layout and affordance for global reasoning, while local heightmaps enforce fine-grained physical adherence without full 3D volumetric reasoning. SceMoS achieves state-of-the-art motion realism and contact accuracy on the TRUMANS benchmark, reducing the number of trainable parameters for scene encoding by over 50%, showing that 2D scene cues can effectively ground 3D human-scene interaction.", "AI": {"tldr": "SceMoS是一种基于场景感知的人体运动合成框架，使用二维场景表示来提高物理现实性。", "motivation": "当前方法依赖于复杂的三维场景数据进行人体运动合成，计算成本高。SceMoS旨在通过简化为二维表示以达到高效和真实感的平衡。", "method": "SceMoS利用轻量级的2D线索来分解全局规划与局部执行，并采用基于文本条件的自回归全局动作计划器以及几何感知的动作标记化器，从而实现高效的运动合成。", "result": "在TRUMANS基准测试中，SceMoS达到了最先进的运动真实性和接触精度，并且减少了超过50%的可训练参数量。", "conclusion": "研究表明，二维场景线索可以有效支撑三维人体与环境之间的交互。"}}
{"id": "2602.20467", "pdf": "https://arxiv.org/pdf/2602.20467", "abs": "https://arxiv.org/abs/2602.20467", "authors": ["Enrico Ballini", "Luca Muscarnera", "Alessio Fumagalli", "Anna Scotti", "Francesco Regazzoni"], "title": "Elimination-compensation pruning for fully-connected neural networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The unmatched ability of Deep Neural Networks in capturing complex patterns in large and noisy datasets is often associated with their large hypothesis space, and consequently to the vast amount of parameters that characterize model architectures. Pruning techniques affirmed themselves as valid tools to extract sparse representations of neural networks parameters, carefully balancing between compression and preservation of information. However, a fundamental assumption behind pruning is that expendable weights should have small impact on the error of the network, while highly important weights should tend to have a larger influence on the inference. We argue that this idea could be generalized; what if a weight is not simply removed but also compensated with a perturbation of the adjacent bias, which does not contribute to the network sparsity? Our work introduces a novel pruning method in which the importance measure of each weight is computed considering the output behavior after an optimal perturbation of its adjacent bias, efficiently computable by automatic differentiation. These perturbations can be then applied directly after the removal of each weight, independently of each other. After deriving analytical expressions for the aforementioned quantities, numerical experiments are conducted to benchmark this technique against some of the most popular pruning strategies, demonstrating an intrinsic efficiency of the proposed approach in very diverse machine learning scenarios. Finally, our findings are discussed and the theoretical implications of our results are presented.", "AI": {"tldr": "本文提出了一种新的剪枝方法，通过优化相邻偏置的扰动来评估权重的重要性，并在去除权重后进行补偿。", "motivation": "现有剪枝技术假设可以移除影响较小的权重而保留重要权重。作者认为可以通过优化偏置的扰动来提高剪枝的有效性。", "method": "提出了一种基于相邻偏置最佳扰动的新修剪方法，并通过自动微分计算每个权重的重要性，然后在去除权重后进行补偿。", "result": "实验结果表明该方法比现有流行剪枝策略更有效，适用于多种机器学习场景。", "conclusion": "研究展示了新剪枝方法的高效性，并讨论了其理论意义。"}}
{"id": "2602.20466", "pdf": "https://arxiv.org/pdf/2602.20466", "abs": "https://arxiv.org/abs/2602.20466", "authors": ["Harsh Gupta", "Mohammad Amin Mirzaee", "Wenzhen Yuan"], "title": "Grasp to Act: Dexterous Grasping for Tool Use in Dynamic Settings", "categories": ["cs.RO"], "comment": "Result videos can be found at https://grasp2act.github.io/", "summary": "Achieving robust grasping with dexterous hands remains challenging, especially when manipulation involves dynamic forces such as impacts, torques, and continuous resistance--situations common in real-world tool use. Existing methods largely optimize grasps for static geometric stability and often fail once external forces arise during manipulation. We present Grasp-to-Act, a hybrid system that combines physics-based grasp optimization with reinforcement-learning-based grasp adaptation to maintain stable grasps throughout functional manipulation tasks. Our method synthesizes robust grasp configurations informed by human demonstrations and employs an adaptive controller that residually issues joint corrections to prevent in-hand slip while tracking the object trajectory. Grasp-to-Act enables robust zero-shot sim-to-real transfer across five dynamic tool-use tasks--hammering, sawing, cutting, stirring, and scooping--consistently outperforming baselines. Across simulation and real-world hardware trials with a 16-DoF dexterous hand, our method reduces translational and rotational in-hand slip and achieves the highest task completion rates, demonstrating stable functional grasps under dynamic, contact-rich conditions.", "AI": {"tldr": "实现动态设置中工具使用的稳健抓握", "motivation": "现有的方法通常在静态几何稳定性方面进行优化，但在外部力量出现时会失败。为了克服这一挑战，提出了Grasp-to-Act系统。", "method": "结合了基于物理的抓取优化和基于强化学习的抓取适应性控制，以维持功能性操作任务中的稳定抓握", "result": "在五种动态工具使用任务中，Grasp-to-Act显著减少了手内滑动并提高了任务完成率。", "conclusion": "该方法能够在动态接触丰富的条件下实现稳定的功能抓握"}}
{"id": "2602.20459", "pdf": "https://arxiv.org/pdf/2602.20459", "abs": "https://arxiv.org/abs/2602.20459", "authors": ["Anirudh Ajith", "Amanpreet Singh", "Jay DeYoung", "Nadav Kunievsky", "Austin C. Kozlowski", "Oyvind Tafjord", "James Evans", "Daniel S. Weld", "Tom Hope", "Doug Downey"], "title": "PreScience: A Benchmark for Forecasting Scientific Contributions", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages (53 with bibliography and appendix), 4 figures (13 with appendix), 4 tables (10 with appendix), 1 algorithm", "summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.", "AI": {"tldr": "该论文介绍了PreScience，一个用于预测科学贡献的基准测试。", "motivation": "通过训练AI系统来预测固定时间点后的科学研究进展，可以帮助研究人员识别潜在合作者和研究方向，并预判哪些问题和方法将成为未来的核心。", "method": "PreScience将科研过程分解为四个生成任务：合作人预测、先前工作选择、贡献产生和影响预测。同时开发了基线模型和评估指标，包括用于衡量贡献相似性的LLM新度量LACERScore。", "result": "每个任务中都存在改进空间，例如在贡献生产任务上最先进的LLMs仅达到与真实贡献适度的相似性（GPT-5平均得分为1到10分中的5.6分）。当将这些任务组合成为期12个月的科研模拟时，合成产出比同一时期的真正科学研究更少样化和创新。", "conclusion": "尽管现有模型存在局限性，但PreScience为开发能够预测科学贡献的AI系统奠定了基础，并展示了改进的方向。"}}
{"id": "2602.20449", "pdf": "https://arxiv.org/pdf/2602.20449", "abs": "https://arxiv.org/abs/2602.20449", "authors": ["Anna Hart", "Chi Han", "Jeonghwan Kim", "Huimin Zhao", "Heng Ji"], "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.", "AI": {"tldr": "比较蛋白质语言模型与自然语言模型的信息分布差异，并提出一种早期退出技术以提高蛋白质非结构特性预测的准确性和效率。", "motivation": "研究如何将基于转换器架构应用于蛋白质序列时的行为差异，以及如何更好地利用PLM解决蛋白质相关任务。", "method": "直接比较不同层注意力头部存储信息在蛋白质和自然语言领域的分布，并通过引入早期退出技术来选择适应特定任务的中间层表示以提高准确性并节省计算资源。", "result": "性能提高了0.4至7.01个百分点，同时效率提高了超过10％。这些成果展示了PLM在蛋白质非结构特性预测中的改进和优化。", "conclusion": "此项工作开辟了直接比较语言模型如何适应不同领域的研究领域，并推动了生物域的语言建模技术的进步。"}}
{"id": "2602.20442", "pdf": "https://arxiv.org/pdf/2602.20442", "abs": "https://arxiv.org/abs/2602.20442", "authors": ["Jun Han", "Josue Nassar", "Sanjit Singh Batra", "Aldo Cordova-Palomera", "Vijay Nori", "Robert E. Tillman"], "title": "Imputation of Unknown Missingness in Sparse Electronic Health Records", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine learning holds great promise for advancing the field of medicine, with electronic health records (EHRs) serving as a primary data source. However, EHRs are often sparse and contain missing data due to various challenges and limitations in data collection and sharing between healthcare providers. Existing techniques for imputing missing values predominantly focus on known unknowns, such as missing or unavailable values of lab test results; most do not explicitly address situations where it is difficult to distinguish what is missing. For instance, a missing diagnosis code in an EHR could signify either that the patient has not been diagnosed with the condition or that a diagnosis was made, but not shared by a provider. Such situations fall into the paradigm of unknown unknowns. To address this challenge, we develop a general purpose algorithm for denoising data to recover unknown missing values in binary EHRs. We design a transformer-based denoising neural network where the output is thresholded adaptively to recover values in cases where we predict data are missing. Our results demonstrate improved accuracy in denoising medical codes within a real EHR dataset compared to existing imputation approaches and leads to increased performance on downstream tasks using the denoised data. In particular, when applying our method to a real world application, predicting hospital readmission from EHRs, our method achieves statistically significant improvement over all existing baselines.", "AI": {"tldr": "开发一种算法来恢复电子健康记录中的未知缺失值，提高医疗代码的去噪精度和下游任务性能", "motivation": "现有的缺失值填补技术主要处理已知的情况，但对于难以区分是否确实存在但未被报告的数据缺乏有效的解决办法。作者提出了一种新的方法来应对这种挑战，以提高真实世界中电子健康记录数据的质量及应用效果。", "method": "设计了一个基于变换器的去噪神经网络，通过自适应地对输出进行阈值处理来恢复未知缺失的二进制EHR中的值。", "result": "在真实的EHR数据集中，与现有方法相比，该算法提高了医学代码的去噪精度，并且在下游任务中使用去噪后的数据时性能也有所提升。特别是在预测医院再入院的应用上，该方法达到了统计显著性的改善效果。", "conclusion": "通过提出一种新的未知缺失值填补技术，可以更准确地处理EHR中的信息不完整问题，从而提高医疗数据分析的质量和实用性。"}}
{"id": "2602.20426", "pdf": "https://arxiv.org/pdf/2602.20426", "abs": "https://arxiv.org/abs/2602.20426", "authors": ["Ruocheng Guo", "Kaiwen Dong", "Xiang Gao", "Kamalika Das"], "title": "Learning to Rewrite Tool Descriptions for Reliable LLM-Agent Tool Use", "categories": ["cs.AI"], "comment": "Preprint", "summary": "The performance of LLM-based agents depends not only on the agent itself but also on the quality of the tool interfaces it consumes. While prior work has focused heavily on agent fine-tuning, tool interfaces-including natural language descriptions and parameter schemas-remain largely human-oriented and often become a bottleneck, especially when agents must select from large candidate tool sets. Existing approaches to improving tool interfaces rely on execution traces, which are frequently unavailable in cold-start or privacy-constrained settings, and typically optimize each tool independently, limiting scalability and generalization to unseen tools. We propose Trace-Free+, a curriculum learning framework that progressively transfers supervision from trace-rich settings to trace-free deployment, encouraging the model to abstract reusable interface-usage patterns and tool usage outcomes. To support this approach, we construct a large-scale dataset of high-quality tool interfaces using a structured workflow over a diverse collection of tools. Experiments on StableToolBench and RestBench show consistent gains on unseen tools, strong cross-domain generalization, and robustness as the number of candidate tools scales to over 100, demonstrating that tool interface optimization is a practical and deployable complement to agent fine-tuning.", "AI": {"tldr": "提出了Trace-Free+框架，用于改进LLM代理使用的工具接口描述。", "motivation": "现有方法依赖于执行跟踪来优化工具接口，但这些追踪在冷启动或隐私受限环境中不可用，并且各自独立优化限制了可扩展性和泛化能力。因此需要一种无需执行跟踪的方法来提高工具接口质量。", "method": "提出了一种称为Trace-Free+的课程学习框架，该框架通过从有迹环境逐步转移到无迹部署中，促进模型抽象出可以复用的界面使用模式和工具有用结果。为此，构造了一个大型高质量工具接口数据集。", "result": "实验表明，在StableToolBench和RestBench上，对未见过的工具也有一致的增益，具有强大的跨领域泛化能力和在候选工具数量超过100时的鲁棒性。", "conclusion": "证明了工具接口优化是LLM代理微调的有效补充，并且是实际可行和可部署的方法。"}}
{"id": "2602.20424", "pdf": "https://arxiv.org/pdf/2602.20424", "abs": "https://arxiv.org/abs/2602.20424", "authors": ["Ved Sirdeshmukh", "Marc Wetter"], "title": "Implicit Intelligence -- Evaluating Agents on What Users Don't Say", "categories": ["cs.AI"], "comment": null, "summary": "Real-world requests to AI agents are fundamentally underspecified. Natural human communication relies on shared context and unstated constraints that speakers expect listeners to infer. Current agentic benchmarks test explicit instruction-following but fail to evaluate whether agents can reason about implicit requirements spanning accessibility needs, privacy boundaries, catastrophic risks, and contextual constraints. We present Implicit Intelligence, an evaluation framework testing whether AI agents can move beyond prompt-following to become genuine goal-fulfillers, paired with Agent-as-a-World (AaW), a harness where interactive worlds are defined in human-readable YAML files and simulated by language models. Our scenarios feature apparent simplicity in user requests, hidden complexity in correct solutions, and discoverability of constraints through environmental exploration. Evaluating 16 frontier and open-weight models across 205 scenarios, we find that even the best-performing model achieves only 48.3% scenario pass rate, revealing substantial room for improvement in bridging the gap between literal instruction-following and human-like contextual reasoning.", "AI": {"tldr": "本文提出了一个评估框架，用于测试AI代理能否理解未明确表达的需求和上下文约束，以实现更接近人类的推理能力。", "motivation": "当前对AI代理的能力评测主要集中在显式指令遵循上，忽略了用户隐含需求及背景知识的重要性。这导致现有的评测体系无法全面衡量代理的实际应用价值。", "method": "提出了一种新的评估框架“Implicit Intelligence”，通过构建人类可读的YAML文件定义互动场景，并使用语言模型进行模拟测试。", "result": "对16个前沿和开放权重模型进行了205个情景下的测试，结果显示即使表现最佳的模型也只有48.3%的情景通过率。", "conclusion": "当前AI代理在理解和应对用户隐含需求方面存在显著局限性，未来需要进一步提高其上下文推理能力。"}}
{"id": "2602.20423", "pdf": "https://arxiv.org/pdf/2602.20423", "abs": "https://arxiv.org/abs/2602.20423", "authors": ["Taha Koleilat", "Hojat Asgariandehkordi", "Omid Nejati Manzari", "Berardino Barile", "Yiming Xiao", "Hassan Rivaz"], "title": "MedCLIPSeg: Probabilistic Vision-Language Adaptation for Data-Efficient and Generalizable Medical Image Segmentation", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2026; Project Page: https://tahakoleilat.github.io/MedCLIPSeg", "summary": "Medical image segmentation remains challenging due to limited annotations for training, ambiguous anatomical features, and domain shifts. While vision-language models such as CLIP offer strong cross-modal representations, their potential for dense, text-guided medical image segmentation remains underexplored. We present MedCLIPSeg, a novel framework that adapts CLIP for robust, data-efficient, and uncertainty-aware medical image segmentation. Our approach leverages patch-level CLIP embeddings through probabilistic cross-modal attention, enabling bidirectional interaction between image and text tokens and explicit modeling of predictive uncertainty. Together with a soft patch-level contrastive loss that encourages more nuanced semantic learning across diverse textual prompts, MedCLIPSeg effectively improves data efficiency and domain generalizability. Extensive experiments across 16 datasets spanning five imaging modalities and six organs demonstrate that MedCLIPSeg outperforms prior methods in accuracy, efficiency, and robustness, while providing interpretable uncertainty maps that highlight local reliability of segmentation results. This work demonstrates the potential of probabilistic vision-language modeling for text-driven medical image segmentation.", "AI": {"tldr": "本文提出了MedCLIPSeg，一种利用CLIP模型进行数据高效和鲁棒性医学图像分割的框架。", "motivation": "针对医学影像分割中注释有限、解剖特征模糊以及领域偏移的问题，该文探索了CLIP在密集文本指导下的医学影像分割应用潜力。", "method": "MedCLIPSeg利用CLIP嵌入进行概率跨模态注意力机制，实现图像与文本标记之间的双向交互，并通过软补丁对比损失鼓励更细腻的语义学习。", "result": "实验结果表明，MedCLIPSeg在准确度、效率和鲁棒性方面优于先前的方法，并提供了可解释的不确定性图以突出分割结果的地方可靠性。", "conclusion": "该研究展示了概率视觉语言建模在文本驱动医学影像分割中的潜力。"}}
{"id": "2602.20422", "pdf": "https://arxiv.org/pdf/2602.20422", "abs": "https://arxiv.org/abs/2602.20422", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "AI": {"tldr": "提出了一种新的扩散模型调制方法DMEMM，用于增强规划中的离线强化学习。", "motivation": "传统扩散方法在生成轨迹时忽视了与真实环境机制的一致性问题。", "method": "通过将关键的环境机制如转换动态和奖励函数融入训练过程来调制扩散模型。", "result": "实验结果表明，DMEMM达到了离线强化学习规划中的最先进水平。", "conclusion": "DMEMM方法显著提高了规划任务中轨迹生成与真实环境的一致性。"}}
{"id": "2602.20417", "pdf": "https://arxiv.org/pdf/2602.20417", "abs": "https://arxiv.org/abs/2602.20417", "authors": ["Aryan Garg", "Sizhuo Ma", "Mohit Gupta"], "title": "gQIR: Generative Quanta Image Reconstruction", "categories": ["cs.CV"], "comment": "CVPR 2026", "summary": "Capturing high-quality images from only a few detected photons is a fundamental challenge in computational imaging. Single-photon avalanche diode (SPAD) sensors promise high-quality imaging in regimes where conventional cameras fail, but raw \\emph{quanta frames} contain only sparse, noisy, binary photon detections. Recovering a coherent image from a burst of such frames requires handling alignment, denoising, and demosaicing (for color) under noise statistics far outside those assumed by standard restoration pipelines or modern generative models. We present an approach that adapts large text-to-image latent diffusion models to the photon-limited domain of quanta burst imaging. Our method leverages the structural and semantic priors of internet-scale diffusion models while introducing mechanisms to handle Bernoulli photon statistics. By integrating latent-space restoration with burst-level spatio-temporal reasoning, our approach produces reconstructions that are both photometrically faithful and perceptually pleasing, even under high-speed motion. We evaluate the method on synthetic benchmarks and new real-world datasets, including the first color SPAD burst dataset and a challenging \\textit{Deforming (XD)} video benchmark. Across all settings, the approach substantially improves perceptual quality over classical and modern learning-based baselines, demonstrating the promise of adapting large generative priors to extreme photon-limited sensing. Code at \\href{https://github.com/Aryan-Garg/gQIR}{https://github.com/Aryan-Garg/gQIR}.", "AI": {"tldr": "从少量光子中重建高质量图像", "motivation": "单光子雪崩二极管传感器在传统相机失效的环境中提供高质影像，但原始的量子帧包含稀疏、噪声大的二进制光子检测。需要一种新方法来处理对齐、去噪和色彩解码问题，并适应远超标准恢复管道或现代生成模型假设的噪音统计学。", "method": "采用大型文本到图像隐式扩散模型来解决光子受限域中的量子帧成像问题，结合了互联网规模扩散模型的结构和语义先验，并引入机制处理伯努利光子统计，通过融合空间修复与时间推理提高重建质量。", "result": "在合成基准和新的现实世界数据集上评估方法，包括首个彩色SPAD突发数据集以及具有挑战性的变形视频基准，在所有设置下均显著改善了感知质量。", "conclusion": "证明了将大型生成先验应用于极端光子受限感测的潜力"}}
{"id": "2602.20412", "pdf": "https://arxiv.org/pdf/2602.20412", "abs": "https://arxiv.org/abs/2602.20412", "authors": ["Aayush Dhakal", "Subash Khanal", "Srikumar Sastry", "Jacob Arndt", "Philipe Ambrozio Dias", "Dalton Lunga", "Nathan Jacobs"], "title": "SimLBR: Learning to Detect Fake Images by Learning to Detect Real Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026", "summary": "The rapid advancement of generative models has made the detection of AI-generated images a critical challenge for both research and society. Recent works have shown that most state-of-the-art fake image detection methods overfit to their training data and catastrophically fail when evaluated on curated hard test sets with strong distribution shifts. In this work, we argue that it is more principled to learn a tight decision boundary around the real image distribution and treat the fake category as a sink class. To this end, we propose SimLBR, a simple and efficient framework for fake image detection using Latent Blending Regularization (LBR). Our method significantly improves cross-generator generalization, achieving up to +24.85\\% accuracy and +69.62\\% recall on the challenging Chameleon benchmark. SimLBR is also highly efficient, training orders of magnitude faster than existing approaches. Furthermore, we emphasize the need for reliability-oriented evaluation in fake image detection, introducing risk-adjusted metrics and worst-case estimates to better assess model robustness. All code and models will be released on HuggingFace and GitHub.", "AI": {"tldr": "本文提出了一种用于检测假图像的简单而高效的框架SimLBR，该框架通过学习围绕真实图像分布的紧决策边界来实现。", "motivation": "随着生成模型的快速发展，AI生成图像的检测已成为研究和社会关注的关键问题。现有方法往往过度拟合其训练数据，在具有强烈分布偏移的测试集上表现不佳。因此提出了一种新的方法以提高鲁棒性和泛化能力。", "method": "SimLBR框架通过学习真实图像周围的紧决策边界，利用潜变量融合正则化（LBR），从而将假类别视为一个接收类。该方法训练速度快于现有技术，并且引入了风险调整度量和最坏情况估计以评估模型的鲁棒性。", "result": "SimLBR在具有强分布偏移的Chameleon基准测试中取得了高达+24.85%准确率和+69.62%召回率的性能提升。同时，它训练速度比现有方法快得多。", "conclusion": "通过引入风险调整度量和最坏情况估计，SimLBR不仅提高了模型鲁棒性和泛化能力，还强调了可靠性导向评估在假图像检测中的重要性"}}
{"id": "2602.20409", "pdf": "https://arxiv.org/pdf/2602.20409", "abs": "https://arxiv.org/abs/2602.20409", "authors": ["Mainak Singha", "Sarthak Mehrotra", "Paolo Casari", "Subhasis Chaudhuri", "Elisa Ricci", "Biplab Banerjee"], "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2026", "summary": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.", "AI": {"tldr": "介绍CLIPoint3D框架，用于基于CLIP的语言引导的少量监督无监督三维点云域适应。", "motivation": "现有视觉语言模型在从合成数据到真实世界点云时容易受到领域偏移的影响。传统的3D域适应方法虽然精度高但效率低，因此提出CLIPoint3D框架以解决这些问题。", "method": "通过将3D样本投影到多个深度图，并利用冻结的CLIP骨干网络结合轻量级3D编码器中的几何线索进行知识驱动提示调整方案。应用高效的参数微调策略并对CLIP的编码器进行优化，设计了基于熵引导视角选择策略和基于最优传输以及不确定性感知原型对齐损失来桥接源目标分布差距。", "result": "在PointDA-10和GraspNetPC-10基准测试中，CLIPoint3D相对于基于CLIP和其他传统编码器的方法取得了3%-16%的准确率提升。", "conclusion": "提出CLIPoint3D框架有效解决了从合成到真实世界点云域适应问题，并展示了优于现有方法的表现。"}}
{"id": "2602.20408", "pdf": "https://arxiv.org/pdf/2602.20408", "abs": "https://arxiv.org/abs/2602.20408", "authors": ["Yuting Deng", "Melanie Brucks", "Olivier Toubia"], "title": "Examining and Addressing Barriers to Diversity in LLM-Generated Ideas", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Ideas generated by independent samples of humans tend to be more diverse than ideas generated from independent LLM samples, raising concerns that widespread reliance on LLMs could homogenize ideation and undermine innovation at a societal level. Drawing on cognitive psychology, we identify (both theoretically and empirically) two mechanisms undermining LLM idea diversity. First, at the individual level, LLMs exhibit fixation just as humans do, where early outputs constrain subsequent ideation. Second, at the collective level, LLMs aggregate knowledge into a unified distribution rather than exhibiting the knowledge partitioning inherent to human populations, where each person occupies a distinct region of the knowledge space. Through four studies, we demonstrate that targeted prompting interventions can address each mechanism independently: Chain-of-Thought (CoT) prompting reduces fixation by encouraging structured reasoning (only in LLMs, not humans), while ordinary personas (versus \"creative entrepreneurs\" such as Steve Jobs) improve knowledge partitioning by serving as diverse sampling cues, anchoring generation in distinct regions of the semantic space. Combining both approaches produces the highest idea diversity, outperforming humans. These findings offer a theoretically grounded framework for understanding LLM idea diversity and practical strategies for human-AI collaborations that leverage AI's efficiency without compromising the diversity essential to a healthy innovation ecosystem.", "AI": {"tldr": "研究探讨了大语言模型生成想法时的多样性不足问题，并提出了针对性的解决方案。", "motivation": "由于依赖大语言模型可能导致创新思想的同质化，进而削弱社会创新能力，因此需要解决这一问题以维护健康的创新生态系统。", "method": "通过认知心理学理论和实证研究识别出两大机制：个体层面的语言模型存在固定效应，在集体层面知识聚合而非分散。利用Chain-of-Thought提示减少固定效应，并使用普通人物角色提升知识分布多样性。", "result": "实验表明，结合上述两种方法可以显著提高想法的多样性，超过了单纯的人类生成的效果。", "conclusion": "该研究不仅为理解大语言模型的思想多样性提供了理论框架，还提出了实现高效人机合作以保持思想多样性的实际策略。"}}
{"id": "2602.20400", "pdf": "https://arxiv.org/pdf/2602.20400", "abs": "https://arxiv.org/abs/2602.20400", "authors": ["Callum Canavan", "Aditya Shrivastava", "Allison Qi", "Jonathan Michala", "Fabien Roger"], "title": "Three Concrete Challenges and Two Hopes for the Safety of Unsupervised Elicitation", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 9 figures", "summary": "To steer language models towards truthful outputs on tasks which are beyond human capability, previous work has suggested training models on easy tasks to steer them on harder ones (easy-to-hard generalization), or using unsupervised training algorithms to steer models with no external labels at all (unsupervised elicitation). Although techniques from both paradigms have been shown to improve model accuracy on a wide variety of tasks, we argue that the datasets used for these evaluations could cause overoptimistic evaluation results. Unlike many real-world datasets, they often (1) have no features with more salience than truthfulness, (2) have balanced training sets, and (3) contain only data points to which the model can give a well-defined answer. We construct datasets that lack each of these properties to stress-test a range of standard unsupervised elicitation and easy-to-hard generalization techniques. We find that no technique reliably performs well on any of these challenges. We also study ensembling and combining easy-to-hard and unsupervised techniques, and find they only partially mitigate performance degradation due to these challenges. We believe that overcoming these challenges should be a priority for future work on unsupervised elicitation.", "AI": {"tldr": "本文探讨了在缺乏人类能力的任务上引导语言模型生成真实输出的方法，并通过构建具有挑战性的数据集来评估这些方法的有效性。", "motivation": "之前的工作表明，通过训练模型完成简单任务以引导其处理更复杂任务（由易到难泛化），或使用无监督算法进行模型引导而无需外部标签（无监督提取）可以提高模型在多种任务上的准确性。然而，作者认为这些评估所使用的数据集可能导致乐观的结果。", "method": "本文构建了具有挑战性的数据集来测试一系列标准的无监督提取和由易到难泛化技术，并研究了通过结合使用这两种方法来缓解性能下降的情况。", "result": "实验发现没有一种技术能够在任何这些挑战上可靠地表现良好，而组合两种方法只能部分减轻由于这些挑战导致的性能下降问题。", "conclusion": "作者认为克服这些挑战应是未来无监督提取研究的重点。"}}
{"id": "2602.20379", "pdf": "https://arxiv.org/pdf/2602.20379", "abs": "https://arxiv.org/abs/2602.20379", "authors": ["Mukul Chhabra", "Luigi Medrano", "Arush Verma"], "title": "Case-Aware LLM-as-a-Judge Evaluation for Enterprise-Scale RAG Systems", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages including appendix, 6 figures", "summary": "Enterprise Retrieval-Augmented Generation (RAG) assistants operate in multi-turn, case-based workflows such as technical support and IT operations, where evaluation must reflect operational constraints, structured identifiers (e.g., error codes, versions), and resolution workflows. Existing RAG evaluation frameworks are primarily designed for benchmark-style or single-turn settings and often fail to capture enterprise-specific failure modes such as case misidentification, workflow misalignment, and partial resolution across turns. We present a case-aware LLM-as-a-Judge evaluation framework for enterprise multi-turn RAG systems. The framework evaluates each turn using eight operationally grounded metrics that separate retrieval quality, grounding fidelity, answer utility, precision integrity, and case/workflow alignment. A severity-aware scoring protocol reduces score inflation and improves diagnostic clarity across heterogeneous enterprise cases. The system uses deterministic prompting with strict JSON outputs, enabling scalable batch evaluation, regression testing, and production monitoring. Through a comparative study of two instruction-tuned models across short and long workflows, we show that generic proxy metrics provide ambiguous signals, while the proposed framework exposes enterprise-critical tradeoffs that are actionable for system improvement.", "AI": {"tldr": "本文提出了一种针对企业级多轮对话RAG系统的案例感知LLM作为裁判的评估框架。", "motivation": "现有的RAG评价框架主要为基准测试或单轮对话设计，无法捕捉到企业在操作约束、结构化标识符和解决流程中的特定问题。因此需要一种专门为企业场景设计的评估方法来改进系统性能。", "method": "该框架通过八项操作性指标从检索质量、基础准确性、答案效用、精确度完整性和案例/工作流一致性等维度进行评价，使用确定性的提示方式和严格JSON输出格式以支持大规模批处理评价。", "result": "研究表明，通用代理指标提供的信号模糊不清，而提出的框架能够揭示出对企业至关重要的权衡，并为系统改进提供了可操作的见解。", "conclusion": "通过案例感知LLM作为裁判的方法可以更好地评估企业级多轮对话RAG系统的性能，为企业提供准确且有价值的反馈。"}}
{"id": "2602.20376", "pdf": "https://arxiv.org/pdf/2602.20376", "abs": "https://arxiv.org/abs/2602.20376", "authors": ["Ria Stevens", "Fangshuo Liao", "Barbara Su", "Jianqiang Li", "Anastasios Kyrillidis"], "title": "Exploiting Low-Rank Structure in Max-K-Cut Problems", "categories": ["cs.DS"], "comment": null, "summary": "We approach the Max-3-Cut problem through the lens of maximizing complex-valued quadratic forms and demonstrate that low-rank structure in the objective matrix can be exploited, leading to alternative algorithms to classical semidefinite programming (SDP) relaxations and heuristic techniques. We propose an algorithm for maximizing these quadratic forms over a domain of size $K$ that enumerates and evaluates a set of $O\\left(n^{2r-1}\\right)$ candidate solutions, where $n$ is the dimension of the matrix and $r$ represents the rank of an approximation of the objective. We prove that this candidate set is guaranteed to include the exact maximizer when $K=3$ (corresponding to Max-3-Cut) and the objective is low-rank, and provide approximation guarantees when the objective is a perturbation of a low-rank matrix. This construction results in a family of novel, inherently parallelizable and theoretically-motivated algorithms for Max-3-Cut. Extensive experimental results demonstrate that our approach achieves performance comparable to existing algorithms across a wide range of graphs, while being highly scalable.", "AI": {"tldr": "本文通过最大化复值二次型来解决Max-3-Cut问题，并利用目标矩阵中的低秩结构提出了一种新算法。", "motivation": "文章旨在探索在Max-3-Cut问题中，如何利用目标矩阵的低秩特性以替代传统的半定规划松弛方法和启发式技术。", "method": "作者提出了一个针对维度为n的目标矩阵，在规模K域内最大化二次型的方法。该算法通过枚举并评估一组O(n^{2r-1})候选解来实现，其中r表示目标近似矩阵的秩。当目标是低秩时，并且K=3（即Max-3-Cut）时，此候选集保证包含精确的最大值。", "result": "实验结果显示该方法在广泛的图类型上表现出与现有算法相当甚至更好的性能，并且具有高度可扩展性。", "conclusion": "研究证明了利用低秩结构可以有效解决Max-3-Cut问题，并提出了一种新颖、本质上并行化的理论动机强的方法。"}}
{"id": "2602.20375", "pdf": "https://arxiv.org/pdf/2602.20375", "abs": "https://arxiv.org/abs/2602.20375", "authors": ["Jiashun Wang", "M. Eva Mungai", "He Li", "Jean Pierre Sleiman", "Jessica Hodgins", "Farbod Farshidian"], "title": "Generalizing from References using a Multi-Task Reference and Goal-Driven RL Framework", "categories": ["cs.RO"], "comment": null, "summary": "Learning agile humanoid behaviors from human motion offers a powerful route to natural, coordinated control, but existing approaches face a persistent trade-off: reference-tracking policies are often brittle outside the demonstration dataset, while purely task-driven Reinforcement Learning (RL) can achieve adaptability at the cost of motion quality. We introduce a unified multi-task RL framework that bridges this gap by treating reference motion as a prior for behavioral shaping rather than a deployment-time constraint. A single goal-conditioned policy is trained jointly on two tasks that share the same observation and action spaces, but differ in their initialization schemes, command spaces, and reward structures: (i) a reference-guided imitation task in which reference trajectories define dense imitation rewards but are not provided as policy inputs, and (ii) a goal-conditioned generalization task in which goals are sampled independently of any reference and where rewards reflect only task success. By co-optimizing these objectives within a shared formulation, the policy acquires structured, human-like motor skills from dense reference supervision while learning to adapt these skills to novel goals and initial conditions. This is achieved without adversarial objectives, explicit trajectory tracking, phase variables, or reference-dependent inference. We evaluate the method on a challenging box-based parkour playground that demands diverse athletic behaviors (e.g., jumping and climbing), and show that the learned controller transfers beyond the reference distribution while preserving motion naturalness. Finally, we demonstrate long-horizon behavior generation by composing multiple learned skills, illustrating the flexibility of the learned polices in complex scenarios.", "AI": {"tldr": "提出一种多任务RL框架，使机器人能够从人类动作中学习并适应新环境。", "motivation": "现有的方法在参考追踪策略和纯任务驱动的强化学习之间存在权衡：前者往往对外部数据集不鲁棒，而后者虽然能实现适应性但牺牲了运动质量。", "method": "通过训练一个目标条件策略来同时解决两个共享相同观察和行动空间的任务，一个依赖于参考轨迹定义密集模仿奖励，另一个在没有参考的情况下从任务成功中获取回报。这使得机器人既能学习人类动作又能适应新环境。", "result": "在挑战性的箱子公园环境中展示了所学控制器可以超越参考分布，并保持运动自然性。此外，在复杂场景中通过组合多个已学到的技能来展示长期行为生成的能力。", "conclusion": "该方法成功地将从模仿和任务驱动策略中学到的知识结合起来，使机器人能够学习自然且适应性强的行为模式。"}}
{"id": "2602.20363", "pdf": "https://arxiv.org/pdf/2602.20363", "abs": "https://arxiv.org/abs/2602.20363", "authors": ["Sheyang Tang", "Armin Shafiee Sarvestani", "Jialu Xu", "Xiaoyu Xu", "Zhou Wang"], "title": "Aesthetic Camera Viewpoint Suggestion with 3D Aesthetic Field", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "The aesthetic quality of a scene depends strongly on camera viewpoint. Existing approaches for aesthetic viewpoint suggestion are either single-view adjustments, predicting limited camera adjustments from a single image without understanding scene geometry, or 3D exploration approaches, which rely on dense captures or prebuilt 3D environments coupled with costly reinforcement learning (RL) searches. In this work, we introduce the notion of 3D aesthetic field that enables geometry-grounded aesthetic reasoning in 3D with sparse captures, allowing efficient viewpoint suggestions in contrast to costly RL searches. We opt to learn this 3D aesthetic field using a feedforward 3D Gaussian Splatting network that distills high-level aesthetic knowledge from a pretrained 2D aesthetic model into 3D space, enabling aesthetic prediction for novel viewpoints from only sparse input views. Building on this field, we propose a two-stage search pipeline that combines coarse viewpoint sampling with gradient-based refinement, efficiently identifying aesthetically appealing viewpoints without dense captures or RL exploration. Extensive experiments show that our method consistently suggests viewpoints with superior framing and composition compared to existing approaches, establishing a new direction toward 3D-aware aesthetic modeling.", "AI": {"tldr": "本文提出了一种基于三维美学场的摄像机视角建议方法，该方法能够通过稀疏输入视图高效地预测美学质量较高的新视角。", "motivation": "现有的美学视角建议方法要么仅针对单个图像进行调整而缺乏对场景几何的理解，要么依赖于密集捕捉或预构建的3D环境并使用昂贵的强化学习搜索。本文旨在提出一种在稀疏输入视图下基于三维空间的美学预测方法。", "method": "通过将预先训练好的二维美学模型的知识迁移到三维空间中，利用前馈式的三维高斯点播网络建立三维美学场，并在此基础上提出了一个两阶段的视角搜索管线来高效地寻找美学上吸引人的视角。", "result": "实验结果表明，本文的方法在建议具有更佳构图和框架的新视角方面优于现有方法。", "conclusion": "研究确立了一种新的基于3D感知美学建模的方向，并提出了一个高效的三维美学场学习及搜索流程。"}}
{"id": "2602.20362", "pdf": "https://arxiv.org/pdf/2602.20362", "abs": "https://arxiv.org/abs/2602.20362", "authors": ["Robin Jeanne Kirschner", "Anna Huber", "Carina M. Micheler", "Dirk Müller", "Nader Rajaei", "Rainer Burgkart", "Sami Haddadin"], "title": "Energy-Based Injury Protection Database: Including Shearing Contact Thresholds for Hand and Finger Using Porcine Surrogates", "categories": ["cs.RO"], "comment": "9 pages, 11 figures", "summary": "While robotics research continues to propose strategies for collision avoidance in human-robot interaction, the reality of constrained environments and future humanoid systems makes contact inevitable. To mitigate injury risks, energy-constraining control approaches are commonly used, often relying on safety thresholds derived from blunt impact data in EN ISO 10218-2:2025. However, this dataset does not extend to edged or pointed collisions. Without scalable, clinically grounded datasets covering diverse contact scenarios, safety validation remains limited. Previous studies have laid the groundwork by assessing surrogate-based velocity and mass limits across various geometries, focusing on perpendicular impacts. This study expands those datasets by including shearing contact scenarios in unconstrained collisions, revealing that collision angle significantly affects injury outcomes. Notably, unconstrained shearing contacts result in fewer injuries than perpendicular ones. By reevaluating all prior porcine surrogate data, we establish energy thresholds across geometries and contact types, forming the first energy-based Injury Protection Database. This enables the development of meaningful energy-limiting controllers that ensure safety across a wide range of realistic collision events.", "AI": {"tldr": "本文扩展了现有的伤害保护数据库，纳入了剪切接触场景的数据，提出了首个基于能量的伤害保护数据库。", "motivation": "现有伤害防护数据主要针对钝击，并未涵盖边缘或尖锐碰撞。因此，在机器人与人类交互中存在安全验证不足的问题。", "method": "通过重新评估所有先前使用的猪类替代模型数据，包括剪切接触场景中的数据，建立了能量限制阈值数据库。", "result": "揭示了碰撞角度显著影响伤害结果，且无约束下的剪切接触比垂直撞击造成的损伤更少。构建了一个涵盖不同几何形状和接触类型的首个基于能量的伤害保护数据库。", "conclusion": "新的数据库为开发有效的安全控制策略提供了基础，使机器人在各种现实情况下的安全性得到保障。"}}
{"id": "2602.20361", "pdf": "https://arxiv.org/pdf/2602.20361", "abs": "https://arxiv.org/abs/2602.20361", "authors": ["Mohanad Obeed", "Ming Jian"], "title": "Learning During Detection: Continual Learning for Neural OFDM Receivers via DMRS", "categories": ["cs.IT", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep neural networks (DNNs) have been increasingly explored for receiver design because they can handle complex environments without relying on explicit channel models. Nevertheless, because communication channels change rapidly, their distributions can shift over time, often making periodic retraining necessary. This paper proposes a zero-overhead online and continual learning framework for orthogonal frequency-division multiplexing (OFDM) neural receivers that directly detect the soft bits of received signals. Unlike conventional fine-tuning methods that rely on dedicated training intervals or full resource grids, our approach leverages existing demodulation reference signals (DMRS) to simultaneously enable signal demodulation and model adaptation. We introduce three pilot designs: fully randomized, hybrid, and additional pilots that flexibly support joint demodulation and learning. To accommodate these pilot designs, we develop two receiver architectures: (i) a parallel design that separates inference and fine-tuning for uninterrupted operation, and (ii) a forward-pass reusing design that reduces computational complexity. Simulation results show that the proposed method effectively tracks both slow and fast channel distribution variations without additional overhead, service interruption, or catastrophic performance degradation under distribution shift.", "AI": {"tldr": "本文提出了一种基于DMRS的零开销在线和连续学习框架，用于正交频分复用（OFDM）神经接收器，使接收器能够同时进行信号解调和模型适应。", "motivation": "为了应对通信信道变化迅速的问题，使得其分布随时间发生变化，传统方法需要周期性地重新训练。本文提出了一种零开销的在线和连续学习框架，以解决这个问题。", "method": "通过引入三种导频设计（完全随机、混合以及额外导频），支持联合解调与学习；发展了两种接收器架构：并行设计分离推理和微调以便不间断操作；前向传递重用设计降低计算复杂性。", "result": "模拟结果表明，提出的方法可以有效跟踪慢速和快速信道分布变化，无需额外开销、服务中断或在分布转移下性能严重下降。", "conclusion": "通过使用DMRS进行零开销在线连续学习，本文的框架能够实现在不增加开销和服务中断的情况下，有效地适应不断变化的通信环境。"}}
{"id": "2602.20360", "pdf": "https://arxiv.org/pdf/2602.20360", "abs": "https://arxiv.org/abs/2602.20360", "authors": ["Runlong Liao", "Jian Yu", "Baiyu Su", "Chi Zhang", "Lizhang Chen", "Qiang Liu"], "title": "Momentum Guidance: Plug-and-Play Guidance for Flow Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Flow-based generative models have become a strong framework for high-quality generative modeling, yet pretrained models are rarely used in their vanilla conditional form: conditional samples without guidance often appear diffuse and lack fine-grained detail due to the smoothing effects of neural networks. Existing guidance techniques such as classifier-free guidance (CFG) improve fidelity but double the inference cost and typically reduce sample diversity. We introduce Momentum Guidance (MG), a new dimension of guidance that leverages the ODE trajectory itself. MG extrapolates the current velocity using an exponential moving average of past velocities and preserves the standard one-evaluation-per-step cost. It matches the effect of standard guidance without extra computation and can further improve quality when combined with CFG. Experiments demonstrate MG's effectiveness across benchmarks. Specifically, on ImageNet-256, MG achieves average improvements in FID of 36.68% without CFG and 25.52% with CFG across various sampling settings, attaining an FID of 1.597 at 64 sampling steps. Evaluations on large flow-based models like Stable Diffusion 3 and FLUX.1-dev further confirm consistent quality enhancements across standard metrics.", "AI": {"tldr": "论文提出了一种新的导引技术Momentum Guidance（MG），用于增强基于流的生成模型的能力，同时减少计算成本。", "motivation": "现有的导引技术如无分类器指导虽然提高了图像质量但增加了推理成本并减少了样本多样性。为了解决这些问题，论文提出了Momentum Guidance来提高样本质量和降低成本。", "method": "通过利用ODE轨迹本身的动量概念，MG通过过去速度的指数移动平均数外推当前速度，并保持每一步一次评估的标准计算成本。", "result": "实验结果显示MG在各种采样设置下提高了FID指标，在ImageNet-256上的改进分别为36.68%和25.52%，最终达到1.597的FID值，同时在大型流模型如Stable Diffusion 3和FLUX.1-dev上也显示出一致的质量提升。", "conclusion": "Momentum Guidance能够有效地增强基于流的生成模型的效果，并且能够在不增加额外计算成本的情况下提供更好的图像质量。"}}
{"id": "2602.20354", "pdf": "https://arxiv.org/pdf/2602.20354", "abs": "https://arxiv.org/abs/2602.20354", "authors": ["Bhavik Chandna", "Kelsey R. Allen"], "title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "categories": ["cs.CV"], "comment": null, "summary": "AI video generation is evolving rapidly. For video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos. However, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope. Here we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video. Our method, 3DSPA, is a 3D spatiotemporal point autoencoder which integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations. The code and pretrained model weights will be available at https://github.com/TheProParadox/3dspa_code.", "AI": {"tldr": "提出了一种自动化的视频现实性评估框架，称为3DSPA。", "motivation": "目前视频生成的现实性评价依赖于人工标注或特定数据集，限制了其应用范围。此方法旨在通过集成三维点轨迹、深度线索和语义特征来实现自动化且全面的视频现实性评估。", "method": "使用一种称为3DSPA的3D时空点自动编码器模型，该模型整合了三维点轨迹、深度信息以及语义特征以形成统一表示，并能有效捕捉物体运动及场景中的事件。", "result": "实验表明，此方法能够可靠识别违反物理定律的视频片段，对动作伪影更敏感，并且在多个数据集上与人类对于视频质量和现实性的判断更加一致。", "conclusion": "通过丰富轨迹表示以包含三维语义信息，为评估生成式视频模型提供了更强的基础，并能隐含捕捉到物理规则的违反。"}}
{"id": "2602.20351", "pdf": "https://arxiv.org/pdf/2602.20351", "abs": "https://arxiv.org/abs/2602.20351", "authors": ["Aleksandr Gushchin", "Dmitriy S. Vatolin", "Anastasia Antsiferova"], "title": "BiRQA: Bidirectional Robust Quality Assessment for Images", "categories": ["cs.CV"], "comment": null, "summary": "Full-Reference image quality assessment (FR IQA) is important for image compression, restoration and generative modeling, yet current neural metrics remain slow and vulnerable to adversarial perturbations. We present BiRQA, a compact FR IQA metric model that processes four fast complementary features within a bidirectional multiscale pyramid. A bottom-up attention module injects fine-scale cues into coarse levels through an uncertainty-aware gate, while a top-down cross-gating block routes semantic context back to high resolution. To enhance robustness, we introduce Anchored Adversarial Training, a theoretically grounded strategy that uses clean \"anchor\" samples and a ranking loss to bound pointwise prediction error under attacks. On five public FR IQA benchmarks BiRQA outperforms or matches the previous state of the art (SOTA) while running ~3x faster than previous SOTA models. Under unseen white-box attacks it lifts SROCC from 0.30-0.57 to 0.60-0.84 on KADID-10k, demonstrating substantial robustness gains. To our knowledge, BiRQA is the only FR IQA model combining competitive accuracy with real-time throughput and strong adversarial resilience.", "AI": {"tldr": "该论文提出了一种新的全参考图像质量评估（FR IQA）模型BiRQA，它结合了高精度、实时处理能力和对抗鲁棒性。", "motivation": "当前的神经网络在进行全参考图像质量评估时速度较慢且容易受到对抗扰动的影响。因此需要一种更快速和更具鲁棒性的解决方案来提升性能。", "method": "BiRQA模型利用了一个双向多尺度金字塔处理四个互补特征，并通过一个自底向上注意模块将细粒度线索注入到粗略层，同时使用顶层交叉门控单元将语义上下文回传至高分辨率。为了增强鲁棒性，引入了锚定对抗训练策略。", "result": "在五个公开的全参考图像质量评估基准测试中，BiRQA超越或达到了最新的研究成果，并且运行速度比之前的最佳模型快大约3倍。此外，在未见过的白盒攻击下，SROCC从0.30-0.57提升到0.60-0.84。", "conclusion": "BiRQA是首个在准确性、实时处理能力和对抗鲁棒性方面同时表现优异的全参考图像质量评估模型。"}}
{"id": "2602.20350", "pdf": "https://arxiv.org/pdf/2602.20350", "abs": "https://arxiv.org/abs/2602.20350", "authors": ["Yibo Meng", "Bingyi Liu", "Ruiqi Chen", "Yan Guan"], "title": "Misty Forest VR: Turning Real ADHD Attention Patterns into Shared Momentum for Youth Collaboration", "categories": ["cs.HC"], "comment": "Accepted by CHI Poster 2026", "summary": "Attention Deficit Hyperactivity Disorder (ADHD) remains highly stigmatized in many cultural contexts, particularly in China, where ADHD-related behaviors are often moralized rather than understood as neurodevelopmental differences. As a result, challenges of self-perception, social misunderstanding, and collaboration between ADHD and non-ADHD individuals remain largely unaddressed. We present Misty Forest, a VR-based collaborative game that explores ADHD through asymmetric co-play. The system translates empirically grounded ADHD behavioral patterns -- such as fluctuating attention and time blindness -- into complementary roles that require mutual coordination between players. Rather than compensating for deficits, the design treats cognitive differences as a source of interdependence. In a controlled study with mixed ADHD--non-ADHD dyads, Misty Forest led to higher task completion, increased self-acceptance among ADHD participants, improved ADHD knowledge, and greater empathy among non-ADHD players. These findings suggest that neurodiversity-centered interactive design can foster understanding, reciprocity, and inclusive collaboration.", "AI": {"tldr": "研究通过VR游戏《迷雾森林》探索ADHD行为模式，并促进参与者之间的理解与合作。", "motivation": "鉴于ADHD在中国文化中受到严重污名化，该研究旨在通过设计一种基于VR的合作游戏来解决自尊、社会误解和ADHD个体间协作的问题。", "method": "开发了名为《迷雾森林》的VR合作游戏，并对混合ADHD与非ADHD双人组进行了一项受控实验。该游戏将基于实证研究的ADHD行为模式转化为互补角色，需要参与者之间的协调配合。", "result": "实验结果显示，《迷雾森林》提高了任务完成率，增加了ADHD参与者的自我接受度和非ADHD参与者的同理心，并提升了对ADHD的理解。", "conclusion": "基于神经多样性导向的互动设计能够促进理解、相互依赖和包容性的协作"}}
{"id": "2602.20348", "pdf": "https://arxiv.org/pdf/2602.20348", "abs": "https://arxiv.org/abs/2602.20348", "authors": ["Yibo Meng", "Bingyi Liu", "Ruiqi Chen", "Xin Chen", "Yan Guan"], "title": "52-Hz Whale Song: An Embodied VR Experience for Exploring Misunderstanding and Empathy", "categories": ["cs.HC"], "comment": "Accepted as CHI 26 Poster", "summary": "Experiences of being misunderstood often stem not from a lack of voice, but from mismatches between how individuals express themselves and how others listen. Such communicative mismatches arise across many social settings, including situations involving linguistic and cultural displacement. While prior HCI research has explored empathy through virtual reality, many approaches rely on narrative explanation, positioning users as observers rather than embodied participants. We present 52-Hz Whale Song, an embodied VR experience that explores miscommunication through metaphor and perspective-shifting. Inspired by the real-world \"52-Hz whale,\" whose calls are not responded to by others, the experience uses this phenomenon as an experiential lens on communicative mismatch rather than representing any specific social group. Players progress through a three-act arc that moves from failed communication to agency and ultimately to mediation. A preliminary mixed-methods study (N = 30) suggests increased perspective-taking and reduced self-reported social distance in immigrant-related situations. This work highlights how embodied metaphor and role-shifting can support empathic engagement and offers transferable design insights for empathy-oriented interactive systems.", "AI": {"tldr": "通过虚拟现实体验探讨沟通误解和同理心。", "motivation": "探索如何使用虚拟现实在不同文化和社会背景之间促进理解和共情，解决沟通中的误解问题。", "method": "设计一个名为52-Hz鲸歌的虚拟现实体验，用户通过角色转换和个人参与来感知沟通障碍，并进行初步混合方法研究（N = 30）以评估效果。", "result": "研究表明参与者在移民相关情境中增加了视角转化和减少了自我报告的社会距离。", "conclusion": "该工作展示了如何利用具象隐喻和角色变换支持共情参与，提供了针对共情感兴趣交互系统的可转移设计见解。"}}
{"id": "2602.20344", "pdf": "https://arxiv.org/pdf/2602.20344", "abs": "https://arxiv.org/abs/2602.20344", "authors": ["Jiele Wu", "Haozhe Ma", "Zhihan Guo", "Thanh Vinh Vo", "Tze Yun Leong"], "title": "Hierarchical Molecular Representation Learning via Fragment-Based Self-Supervised Embedding Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "15 pages (8 pages main text),8 figures", "summary": "Graph self-supervised learning (GSSL) has demonstrated strong potential for generating expressive graph embeddings without the need for human annotations, making it particularly valuable in domains with high labeling costs such as molecular graph analysis. However, existing GSSL methods mostly focus on node- or edge-level information, often ignoring chemically relevant substructures which strongly influence molecular properties. In this work, we propose Graph Semantic Predictive Network (GraSPNet), a hierarchical self-supervised framework that explicitly models both atomic-level and fragment-level semantics. GraSPNet decomposes molecular graphs into chemically meaningful fragments without predefined vocabularies and learns node- and fragment-level representations through multi-level message passing with masked semantic prediction at both levels. This hierarchical semantic supervision enables GraSPNet to learn multi-resolution structural information that is both expressive and transferable. Extensive experiments on multiple molecular property prediction benchmarks demonstrate that GraSPNet learns chemically meaningful representations and consistently outperforms state-of-the-art GSSL methods in transfer learning settings.", "AI": {"tldr": "通过基于片段的自我监督嵌入预测，提出了一种层次化的分子表示学习方法。", "motivation": "现有的图自监督学习方法主要关注节点或边的信息，忽略了化学相关子结构对分子性质的重要影响。为了克服这一限制，作者提出了GraSPNet框架以同时建模原子级和片段级的语义信息。", "method": "GraSPNet将分子图分解成有意义的化学片段，并通过多层次消息传递学习节点和片段级别的表示，结合了两种级别上的掩码语义预测。", "result": "在多个分子性质预测基准测试中，实验表明GraSPNet能够学到具有化学意义的表征并在迁移学习设置下优于现有方法。", "conclusion": "该工作证明了通过层次化自监督框架可以有效学习到多分辨率结构信息，并展示了其在分子性质预测任务中的优越性能。"}}
{"id": "2602.20342", "pdf": "https://arxiv.org/pdf/2602.20342", "abs": "https://arxiv.org/abs/2602.20342", "authors": ["Christos Maikos", "Georgios Angelidis", "Georgios Th. Papadopoulos"], "title": "Large-scale Photorealistic Outdoor 3D Scene Reconstruction from UAV Imagery Using Gaussian Splatting Techniques", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 2 figures", "summary": "In this study, we present an end-to-end pipeline capable of converting drone-captured video streams into high-fidelity 3D reconstructions with minimal latency. Unmanned aerial vehicles (UAVs) are extensively used in aerial real-time perception applications. Moreover, recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential for real-time neural rendering. However, their integration into end-to-end UAV-based reconstruction and visualization systems remains underexplored. Our goal is to propose an efficient architecture that combines live video acquisition via RTMP streaming, synchronized sensor fusion, camera pose estimation, and 3DGS optimization, achieving continuous model updates and low-latency deployment within interactive visualization environments that supports immersive augmented and virtual reality (AR/VR) applications. Experimental results demonstrate that the proposed method achieves competitive visual fidelity, while delivering significantly higher rendering performance and substantially reduced end-to-end latency, compared to NeRF-based approaches. Reconstruction quality remains within 4-7\\% of high-fidelity offline references, confirming the suitability of the proposed system for real-time, scalable augmented perception from aerial platforms.", "AI": {"tldr": "该论文提出了一种端到端的管道，利用无人机捕获的视频流实时生成高保真度的三维重建。", "motivation": "为了提高基于无人机的真实时感知应用中的渲染性能和减少延迟，结合了3D高斯点绘技术与实时视频获取、传感器融合、相机姿态估计等。", "method": "该方法利用RTMP流媒体传输无人机捕获的视频，通过同步传感器融合和相机姿态估计生成连续更新的三维模型，并使用3D高斯点绘优化以实现低延迟部署于互动可视化环境中支持增强现实与虚拟现实应用。", "result": "实验结果表明，所提出的方法在视觉保真度方面可媲美NeRF方法，同时提供了更高的渲染性能和显著减少的端到端延迟。重建质量保持在线下高精度参考之内的4-7％，证实了该系统适用于从空中平台实现实时、大规模增强感知。", "conclusion": "本文提出的管道展示了通过无人机视频流实现实时三维场景重建的能力，并且具有高效的渲染性能和低的延迟，特别适合于增强现实与虚拟现实应用。"}}
{"id": "2602.20334", "pdf": "https://arxiv.org/pdf/2602.20334", "abs": "https://arxiv.org/abs/2602.20334", "authors": ["Chengjie Lu", "Jiahui Wu", "Shaukat Ali", "Malaika Din Hashmi", "Sebastian Mathias Thomle Mason", "Francois Picard", "Mikkel Labori Olsen", "Thomas Peyrucain"], "title": "UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software", "categories": ["cs.SE", "cs.RO"], "comment": "23 pages, 6 figures, 7 tables", "summary": "Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.", "AI": {"tldr": "提出了UAMTERS框架，用于评估DL驱动的机器人软件在不确定环境下的测试效果。", "motivation": "目前缺乏针对不确定性环境下DL软件的有效性分析技术，因此需要开发专门的方法来评估此类软件。", "method": "设计了一种不确定性感知突变操作器，并提出相应的分数指标以量化测试套件在不同不确定性水平下检测失败的能力。", "result": "通过三个机器人案例研究验证了UAMTERS能够更有效地区分测试套件质量并捕捉到由不确定行为导致的故障。", "conclusion": "UAMTERS框架提供了一种新的方法来评估DL驱动的机器人软件在动态和不确定性环境中的可靠性。"}}
{"id": "2602.20333", "pdf": "https://arxiv.org/pdf/2602.20333", "abs": "https://arxiv.org/abs/2602.20333", "authors": ["Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith"], "title": "DMCD: Semantic-Statistical Framework for Causal Discovery", "categories": ["cs.AI"], "comment": null, "summary": "We present DMCD (DataMap Causal Discovery), a two-phase causal discovery framework that integrates LLM-based semantic drafting from variable metadata with statistical validation on observational data. In Phase I, a large language model proposes a sparse draft DAG, serving as a semantically informed prior over the space of possible causal structures. In Phase II, this draft is audited and refined via conditional independence testing, with detected discrepancies guiding targeted edge revisions. We evaluate our approach on three metadata-rich real-world benchmarks spanning industrial engineering, environmental monitoring, and IT systems analysis. Across these datasets, DMCD achieves competitive or leading performance against diverse causal discovery baselines, with particularly large gains in recall and F1 score. Probing and ablation experiments suggest that these improvements arise from semantic reasoning over metadata rather than memorization of benchmark graphs. Overall, our results demonstrate that combining semantic priors with principled statistical verification yields a high-performing and practically effective approach to causal structure learning.", "AI": {"tldr": "DMCD是一种结合语言模型语义草图和统计验证的因果发现框架。", "motivation": "为了提高因果关系结构学习的性能，作者提出了一种新的方法来整合变量元数据中的语义信息与观测数据上的统计验证。", "method": "DMCD在第一阶段使用大型语言模型根据变量元数据提出一个稀疏的草图DAG，在第二阶段通过条件独立性测试对其进行审计和细化。发现的差异指导有针对性的边缘修订。", "result": "在三个工业工程、环境监测和IT系统分析的真实世界基准上，DMCD与各种因果发现基线相比表现出具有竞争力或领先的效果，特别是在召回率和F1评分上有显著提高。", "conclusion": "结合语义先验和原则性的统计验证可以实现高效率且实用的因果结构学习方法。"}}
{"id": "2602.20332", "pdf": "https://arxiv.org/pdf/2602.20332", "abs": "https://arxiv.org/abs/2602.20332", "authors": ["Nicole Cho", "William Watson", "Alec Koppel", "Sumitra Ganesh", "Manuela Veloso"], "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the optimal query-rewrite strategy by leveraging an empirically validated and calibrated reward function. Across 16 QA scenarios, our top QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a No-Rewrite baseline and outperforms zero-shot static policies (e.g., Paraphrase or Expand) by 42.6% and 60.3%, respectively. Moreover, all contextual bandits outperform vanilla bandits across all datasets, with higher feature variance coinciding with greater variance in arm selection. This substantiates our finding that there is no single rewrite policy optimal for all queries. We also discover that certain static policies incur higher cumulative regret than No-Rewrite, indicating that an inflexible query-rewriting policy can worsen hallucinations. Thus, learning an online policy over semantic features with QueryBandits can shift model behavior purely through forward-pass mechanisms, enabling its use with closed-source models and bypassing the need for retraining or gradient-based adaptation.", "AI": {"tldr": "提出了QueryBandits框架，通过上下文多臂赌博机选择最优查询重写策略来减少大型语言模型中的幻觉。", "motivation": "大多数幻觉缓解工作集中在开源模型上，忽略了闭源模型中幻觉问题的研究。需要一种适用于所有类型模型的通用解决方案。", "method": "提出了一种基于上下文多臂赌博机的框架QueryBandits，该框架通过在线学习选择最优查询重写策略，并利用验证和校准后的奖励函数进行优化。", "result": "在16个问答场景中，顶级的QueryBandit（Thompson Sampling）取得了87.5%的胜率，比静态策略分别高出42.6%和60.3%，并且上下文赌博机在所有数据集上均优于非上下文赌博机。", "conclusion": "证明了没有一种通用的查询重写策略适用于所有情况，并且不灵活的策略可能会加剧幻觉问题。QueryBandits框架通过前向传递机制改变模型行为，可以用于闭源模型而不需重新训练或基于梯度的适应。"}}
{"id": "2602.20330", "pdf": "https://arxiv.org/pdf/2602.20330", "abs": "https://arxiv.org/abs/2602.20330", "authors": ["Jingcheng Yang", "Tianhu Xiong", "Shengyi Qian", "Klara Nahrstedt", "Mingyuan Wu"], "title": "Circuit Tracing in Vision-Language Models: Understanding the Internal Mechanisms of Multimodal Thinking", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "To appear in the Findings of CVPR 2026", "summary": "Vision-language models (VLMs) are powerful but remain opaque black boxes. We introduce the first framework for transparent circuit tracing in VLMs to systematically analyze multimodal reasoning. By utilizing transcoders, attribution graphs, and attention-based methods, we uncover how VLMs hierarchically integrate visual and semantic concepts. We reveal that distinct visual feature circuits can handle mathematical reasoning and support cross-modal associations. Validated through feature steering and circuit patching, our framework proves these circuits are causal and controllable, laying the groundwork for more explainable and reliable VLMs.", "AI": {"tldr": "介绍了第一个用于分析视觉语言模型中多模态推理的透明电路跟踪框架。", "motivation": "为了理解视觉语言模型内部机制，使其更加可解释和可靠", "method": "利用转换器、归因图和基于注意力的方法来揭示视觉语言模型如何分层整合视觉和语义概念", "result": "发现不同的视觉特征电路可以处理数学推理并支持跨模态关联，并通过功能引导和电路修补验证这些电路具有因果性和可控性", "conclusion": "为更可解释和可靠的视觉语言模型奠定了基础"}}
{"id": "2602.20328", "pdf": "https://arxiv.org/pdf/2602.20328", "abs": "https://arxiv.org/abs/2602.20328", "authors": ["Romario Gualdrón-Hurtado", "Roman Jacome", "Rafael S. Suarez", "Henry Arguello"], "title": "GSNR: Graph Smooth Null-Space Representation for Inverse Problems", "categories": ["cs.CV", "eess.IV", "math.OC"], "comment": "23 pages, 24 figures, Accepted to The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2026", "summary": "Inverse problems in imaging are ill-posed, leading to infinitely many solutions consistent with the measurements due to the non-trivial null-space of the sensing matrix. Common image priors promote solutions on the general image manifold, such as sparsity, smoothness, or score function. However, as these priors do not constrain the null-space component, they can bias the reconstruction. Thus, we aim to incorporate meaningful null-space information in the reconstruction framework. Inspired by smooth image representation on graphs, we propose Graph-Smooth Null-Space Representation (GSNR), a mechanism that imposes structure only into the invisible component. Particularly, given a graph Laplacian, we construct a null-restricted Laplacian that encodes similarity between neighboring pixels in the null-space signal, and we design a low-dimensional projection matrix from the $p$-smoothest spectral graph modes (lowest graph frequencies). This approach has strong theoretical and practical implications: i) improved convergence via a null-only graph regularizer, ii) better coverage, how much null-space variance is captured by $p$ modes, and iii) high predictability, how well these modes can be inferred from the measurements. GSNR is incorporated into well-known inverse problem solvers, e.g., PnP, DIP, and diffusion solvers, in four scenarios: image deblurring, compressed sensing, demosaicing, and image super-resolution, providing consistent improvement of up to 4.3 dB over baseline formulations and up to 1 dB compared with end-to-end learned models in terms of PSNR.", "AI": {"tldr": "提出了一种基于图形平滑空洞空间表示的新型重建框架，以解决成像中的逆问题。", "motivation": "现有的图像先验无法有效约束空洞空间组件，可能导致重构结果偏置。因此，本文旨在通过引入有意义的空洞空间信息来改进这一过程。", "method": "基于图形平滑表示方法，提出了Graph-Smooth Null-Space Representation（GSNR），该方法利用图拉普拉斯算子构建了一个仅限于空洞空间信号的相似度编码器，并设计了一种从p-最平滑的谱图模式到低维投影矩阵的方法。", "result": "在图像去模糊、压缩感知、demosaicing和超分辨率四个场景中，GSNR与基线模型相比，在PSNR上提高了最多4.3dB，比端到端学习模型高1dB。", "conclusion": "本文提出的基于图形平滑空洞空间表示的方法有效改进了逆问题的重构质量。"}}
{"id": "2602.20324", "pdf": "https://arxiv.org/pdf/2602.20324", "abs": "https://arxiv.org/abs/2602.20324", "authors": ["Cathy Shyr", "Yan Hu", "Rory J. Tinker", "Thomas A. Cassini", "Kevin W. Byram", "Rizwan Hamid", "Daniel V. Fabbri", "Adam Wright", "Josh F. Peterson", "Lisa Bastarache", "Hua Xu"], "title": "An artificial intelligence framework for end-to-end rare disease phenotyping from clinical notes using large language models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Phenotyping is fundamental to rare disease diagnosis, but manual curation of structured phenotypes from clinical notes is labor-intensive and difficult to scale. Existing artificial intelligence approaches typically optimize individual components of phenotyping but do not operationalize the full clinical workflow of extracting features from clinical text, standardizing them to Human Phenotype Ontology (HPO) terms, and prioritizing diagnostically informative HPO terms. We developed RARE-PHENIX, an end-to-end AI framework for rare disease phenotyping that integrates large language model-based phenotype extraction, ontology-grounded standardization to HPO terms, and supervised ranking of diagnostically informative phenotypes. We trained RARE-PHENIX using data from 2,671 patients across 11 Undiagnosed Diseases Network clinical sites, and externally validated it on 16,357 real-world clinical notes from Vanderbilt University Medical Center. Using clinician-curated HPO terms as the gold standard, RARE-PHENIX consistently outperformed a state-of-the-art deep learning baseline (PhenoBERT) across ontology-based similarity and precision-recall-F1 metrics in end-to-end evaluation (i.e., ontology-based similarity of 0.70 vs. 0.58). Ablation analyses demonstrated performance improvements with the addition of each module in RARE-PHENIX (extraction, standardization, and prioritization), supporting the value of modeling the full clinical phenotyping workflow. By modeling phenotyping as a clinically aligned workflow rather than a single extraction task, RARE-PHENIX provides structured, ranked phenotypes that are more concordant with clinician curation and has the potential to support human-in-the-loop rare disease diagnosis in real-world settings.", "AI": {"tldr": "研究开发了一个名为RARE-PHENIX的端到端人工智能框架，用于罕见疾病表型分析。", "motivation": "手动从临床记录中提取结构化表型既费力又难以扩展。现有的人工智能方法通常优化了表型过程中的单个组件，但没有实现整个临床工作流程的操作。", "method": "研究使用大型语言模型进行表型特征提取，将其标准化为人类表型本体（HPO）术语，并通过监督学习对有诊断意义的表型进行优先级排序。框架在2671名患者的数据上进行了训练，在范德比尔特大学医学中心的临床笔记数据上进行了外部验证。", "result": "RARE-PHENIX与最先进的深度学习基线模型（PhenoBERT）相比，表现出更高的性能，特别是在基于本体的相似性和精度-召回-F1度量方面。删除任何模块后，性能下降证明了每个组件的价值。", "conclusion": "通过将表型分析建模为临床工作流程而不是单个提取任务，RARE-PHENIX可以提供与医生注释更加一致的结构化、优先级排序的表型，并有可能支持罕见疾病诊断的人机协作。"}}
{"id": "2602.20323", "pdf": "https://arxiv.org/pdf/2602.20323", "abs": "https://arxiv.org/abs/2602.20323", "authors": ["Haoyang Li", "Yang You", "Hao Su", "Leonidas Guibas"], "title": "Learning Physical Principles from Interaction: Self-Evolving Planning via Test-Time Memory", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Reliable object manipulation requires understanding physical properties that vary across objects and environments. Vision-language model (VLM) planners can reason about friction and stability in general terms; however, they often cannot predict how a specific ball will roll on a particular surface or which stone will provide a stable foundation without direct experience. We present PhysMem, a memory framework that enables VLM robot planners to learn physical principles from interaction at test time, without updating model parameters. The system records experiences, generates candidate hypotheses, and verifies them through targeted interaction before promoting validated knowledge to guide future decisions. A central design choice is verification before application: the system tests hypotheses against new observations rather than applying retrieved experience directly, reducing rigid reliance on prior experience when physical conditions change. We evaluate PhysMem on three real-world manipulation tasks and simulation benchmarks across four VLM backbones. On a controlled brick insertion task, principled abstraction achieves 76% success compared to 23% for direct experience retrieval, and real-world experiments show consistent improvement over 30-minute deployment sessions.", "AI": {"tldr": "本文介绍了一种名为PhysMem的记忆框架，该框架允许视觉语言模型（VLM）规划器通过交互从测试时的经验中学习物理原理。", "motivation": "可靠的物体操纵需要理解在不同对象和环境中变化的物理特性。虽然现有的VLM规划器可以在一般术语上推理摩擦和稳定性，但它们无法预测特定球如何滚动或哪种石头会提供稳定的地基，除非有直接经验。", "method": "PhysMem通过记录经验、生成候选假设并通过对新观察进行验证来测试这些假设的方式工作。只有经过验证的知识才会被提升以指导未来的决策。", "result": "在三个真实世界操纵任务和模拟基准上进行了评估，在控制的砖块插入任务中，原理抽象实现了76%的成功率与23%直接经验检索相比，并且实际实验显示持续改进超过30分钟部署会话。", "conclusion": "PhysMem通过减少对先前经验的僵化依赖提供了更灵活、适应性的决策方式，从而提高了物体操纵任务中的性能。"}}
{"id": "2602.20317", "pdf": "https://arxiv.org/pdf/2602.20317", "abs": "https://arxiv.org/abs/2602.20317", "authors": ["Nathaniel Chen", "Kouroche Bouchiat", "Peter Steiner", "Andrew Rothstein", "David Smith", "Max Austin", "Mike van Zeeland", "Azarakhsh Jalalvand", "Egemen Kolemen"], "title": "Fast Spectrogram Event Extraction via Offline Self-Supervised Learning: From Fusion Diagnostics to Bioacoustics", "categories": ["eess.SP", "cs.AI", "physics.plasm-ph"], "comment": null, "summary": "Next-generation fusion facilities like ITER face a \"data deluge,\" generating petabytes of multi-diagnostic signals daily that challenge manual analysis. We present a \"signals-first\" self-supervised framework for the automated extraction of coherent and transient modes from high-noise time-frequency data. We also develop a general-purpose method and tool for extracting coherent, quasi-coherent, and transient modes for fluctuation measurements in tokamaks by employing non-linear optimal techniques in multichannel signal processing with a fast neural network surrogate on fast magnetics, electron cyclotron emission, CO2 interferometers, and beam emission spectroscopy measurements from DIII-D. Results are tested on data from DIII-D, TJ-II, and non-fusion spectrograms. With an inference latency of 0.5 seconds, this framework enables real-time mode identification and large-scale automated database generation for advanced plasma control. Repository is in https://github.com/PlasmaControl/TokEye.", "AI": {"tldr": "本文提出了一种基于自监督学习的快速频谱图事件提取框架，用于处理高噪声时间频率数据中的相干和瞬态模式。", "motivation": "下一代聚变设施（如ITER）产生的多诊断信号面临“数据洪流”，手动分析难以应对。为了实现自动化的实时模式识别和大规模数据库生成，需要一种新的方法来解决这一问题。", "method": "本文开发了一种基于非线性优化技术和快速神经网络代理的通用自监督学习框架，用于从高噪声时间频率数据中提取相干、准相干和瞬态模式。该框架在多通道信号处理方面表现出色，并且已经在托卡马克装置DIII-D和其他实验数据上进行了测试。", "result": "结果表明，所提出的方法能够在0.5秒内实现快速的推理延迟，从而实现了实时模式识别和大规模数据库生成的能力。", "conclusion": "该方法提供了一种有效解决高噪声时间频率数据分析问题的新途径，并在不同类型的频谱图数据上展示了强大的通用性。"}}
{"id": "2602.20316", "pdf": "https://arxiv.org/pdf/2602.20316", "abs": "https://arxiv.org/abs/2602.20316", "authors": ["C. J. Díaz Baso", "I. J. Soler Poquet", "C. Kuckein", "M. van Noort", "N. Poirier"], "title": "Inspectorch: Efficient rare event exploration in solar observations", "categories": ["astro-ph.SR", "cs.CV"], "comment": "Comments: 12+1 pages, 11+2 figures, submitted to A&A", "summary": "The Sun is observed in unprecedented detail, enabling studies of its activity on very small spatiotemporal scales. However, the large volume of data collected by our telescopes cannot be fully analyzed with conventional methods. Popular machine learning methods identify general trends from observations, but tend to overlook unusual events due to their low frequency of occurrence. We study the applicability of unsupervised probabilistic methods to efficiently identify rare events in multidimensional solar observations and optimize our computational resources to the study of these extreme phenomena. We introduce Inspectorch, an open-source framework that utilizes flow-based models: flexible density estimators capable of learning the multidimensional distribution of solar observations. Once optimized, it assigns a probability to each sample, allowing us to identify unusual events. We apply this approach by applying it to observations from the Hinode Spectro-Polarimeter, the Interface Region Imaging Spectrograph, the Microlensed Hyperspectral Imager at Swedish 1-m Solar Telescope, the Atmospheric Imaging Assembly on board the Solar Dynamics Observatory and the Extreme Ultraviolet Imager on board Solar Orbiter. We find that the algorithm assigns consistently lower probabilities to spectra that exhibit unusual features. For example, it identifies profiles with very strong Doppler shifts, uncommon broadening, and temporal dynamics associated with small-scale reconnection events, among others. As a result, Inspectorch demonstrates that density estimation using flow-based models offers a powerful approach to identifying rare events in large solar datasets. The resulting probabilistic anomaly scores allow computational resources to be focused on the most informative and physically relevant events. We make our Python package publicly available at https://github.com/cdiazbas/inspectorch.", "AI": {"tldr": "研究提出一种基于流模型的概率方法，用于高效识别太阳观测中罕见事件", "motivation": "太阳活动的研究需要分析大量数据，传统方法难以发现罕见的低频事件，因此引入概率方法以优化计算资源和提高对极端现象的关注。", "method": "采用无监督学习中的流模型来估计多维分布，并通过分配每个样本的概率值识别异常情况。该框架应用于多个太阳观测仪器的数据。", "result": "算法能够准确地为具有罕见特征的光谱分配较低概率，成功识别出诸如强多普勒偏移、不常见的加宽和小型重联事件等现象", "conclusion": "通过密度估计方法可以有效识别大型数据集中的罕见事件，并将计算资源集中于最相关的信息上。"}}
{"id": "2602.20314", "pdf": "https://arxiv.org/pdf/2602.20314", "abs": "https://arxiv.org/abs/2602.20314", "authors": ["Lauren Vogelstein", "Vedya Konda", "Deborah Fields", "Yasmin Kafai", "Luis Morales-Navarro", "Danaé Metaxa"], "title": "Rapid Testing, Duck Lips, and Tilted Cameras: Youth Everyday Algorithm Auditing Practices with Generative AI Filters", "categories": ["cs.HC"], "comment": ":H.5.0; K.3.2", "summary": "Today's youth have extensive experience interacting with artificial intelligence and machine learning applications on popular social media platforms, putting youth in a unique position to examine, evaluate, and even challenge these applications. Algorithm auditing is a promising candidate for connecting youth's everyday practices in using AI applications with more formal scientific literacies (syncretic designs). In this paper, we analyze high school youth participants' everyday algorithm auditing practices when interacting with generative AI filters on TikTok, revealing thorough and extensive examinations, with youth rapidly testing filters with sophisticated camera variations and facial manipulations to identify filter limitations. In the discussion, we address how these findings can provide a foundation for developing designs that bring together everyday and more formal algorithm auditing.", "AI": {"tldr": "研究高中生使用TikTok生成式AI滤镜时的算法审计实践，揭示了他们快速测试和识别这些应用程序限制的行为。", "motivation": "将青少年日常使用的AI应用与更正式的科学素养结合起来，探索青年人如何通过自己的经验挑战和评估这些技术。", "method": "分析高中生在使用TikTok生成式AI滤镜时的交互行为，特别是他们在不同相机角度和面部操作下快速测试滤镜的行为。", "result": "发现青少年进行了彻底且广泛的检查，利用复杂的手势和表情来识别AI滤镜的局限性。", "conclusion": "这些发现为设计一种将日常实践与更正式的算法审计结合起来的方法提供了基础。"}}
{"id": "2602.20312", "pdf": "https://arxiv.org/pdf/2602.20312", "abs": "https://arxiv.org/abs/2602.20312", "authors": ["Guodong Chen", "Huanshuo Dong", "Mallesham Dasari"], "title": "N4MC: Neural 4D Mesh Compression", "categories": ["cs.CV"], "comment": null, "summary": "We present N4MC, the first 4D neural compression framework to efficiently compress time-varying mesh sequences by exploiting their temporal redundancy. Unlike prior neural mesh compression methods that treat each mesh frame independently, N4MC takes inspiration from inter-frame compression in 2D video codecs, and learns motion compensation in long mesh sequences. Specifically, N4MC converts consecutive irregular mesh frames into regular 4D tensors to provide a uniform and compact representation. These tensors are then condensed using an auto-decoder, which captures both spatial and temporal correlations for redundancy removal. To enhance temporal coherence, we introduce a transformer-based interpolation model that predicts intermediate mesh frames conditioned on latent embeddings derived from tracked volume centers, eliminating motion ambiguities. Extensive evaluations show that N4MC outperforms state-of-the-art in rate-distortion performance, while enabling real-time decoding of 4D mesh sequences. The implementation of our method is available at: https://github.com/frozzzen3/N4MC.", "AI": {"tldr": "提出N4MC框架，用于高效压缩时间变化网格序列。", "motivation": "利用时间冗余性对时间变化网格序列进行有效压缩。", "method": "将连续不规则网格帧转换为正则4D张量，并使用自解码器捕捉空间和时间相关性。引入基于Transformer的插值模型，预测中间网格帧以提高时间一致性。", "result": "N4MC在率失真性能上优于现有最佳方法，并支持实时解码。", "conclusion": "N4MC框架实现了对时间变化网格序列的有效压缩，同时保持了较高的时间和空间质量。"}}
{"id": "2602.20306", "pdf": "https://arxiv.org/pdf/2602.20306", "abs": "https://arxiv.org/abs/2602.20306", "authors": ["Davide Carrara", "Marc Hirschvogel", "Francesca Bonizzoni", "Stefano Pagani", "Simone Pezzuto", "Francesco Regazzoni"], "title": "Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation", "categories": ["cs.LG", "cs.AI", "math.NA"], "comment": "39 pages, 19 figures", "summary": "High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs.", "AI": {"tldr": "本文提出了一种基于几何编码和生成增强的心脏力学代理模型，用于稀疏数据条件下的形状信息建模。", "motivation": "高保真度的心脏力学计算模型提供了心脏功能的机制见解，但对常规临床应用来说计算成本过高。在数据稀缺条件下，通过解耦几何表示与物理响应的学习过程来建立具有广泛适用性的代理模型是一项挑战。", "method": "首先利用形状模型从左心室几何学中学习紧凑型潜在表示。所学到的潜在空间有效地编码了形态并促进了合成几何生成的数据增强。其次，基于神经场的代理模型根据此几何编码进行训练以预测外部负载下的心室内位移。", "result": "实验结果表明，该方法能够对理想化和特定患者的数据库进行准确预测，并具有广泛的通用性以及对于噪声或稀疏输入的鲁棒性。", "conclusion": "研究展示了所提出的框架在稀疏数据条件下实现形状信息建模的有效性和准确性。"}}
{"id": "2602.20304", "pdf": "https://arxiv.org/pdf/2602.20304", "abs": "https://arxiv.org/abs/2602.20304", "authors": ["Onur Beker", "Andreas René Geist", "Anselm Paulus", "Nico Gürtler", "Ji Shi", "Sylvain Calinon", "Georg Martius"], "title": "Smoothly Differentiable and Efficiently Vectorizable Contact Manifold Generation", "categories": ["cs.RO"], "comment": null, "summary": "Simulating rigid-body dynamics with contact in a fast, massively vectorizable, and smoothly differentiable manner is highly desirable in robotics. An important bottleneck faced by existing differentiable simulation frameworks is contact manifold generation: representing the volume of intersection between two colliding geometries via a discrete set of properly distributed contact points. A major factor contributing to this bottleneck is that the related routines of commonly used robotics simulators were not designed with vectorization and differentiability as a primary concern, and thus rely on logic and control flow that hinder these goals. We instead propose a framework designed from the ground up with these goals in mind, by trying to strike a middle ground between: i) convex primitive based approaches used by common robotics simulators (efficient but not differentiable), and ii) mollified vertex-face and edge-edge unsigned distance-based approaches used by barrier methods (differentiable but inefficient). Concretely, we propose: i) a representative set of smooth analytical signed distance primitives to implement vertex-face collisions, and ii) a novel differentiable edge-edge collision routine that can provide signed distances and signed contact normals. The proposed framework is evaluated via a set of didactic experiments and benchmarked against the collision detection routine of the well-established Mujoco XLA framework, where we observe a significant speedup. Supplementary videos can be found at https://github.com/bekeronur/contax, where a reference implementation in JAX will also be made available at the conclusion of the review process.", "AI": {"tldr": "提出一种生成光滑且可向量化接触流形的新框架，用于高效、大规模并行和可微分的刚体动力学仿真。", "motivation": "现有可微分模拟器在生成接触流形时存在瓶颈，因为它们依赖于妨碍矢量化与可微分性的逻辑和控制流程。作者旨在解决此问题，并构建一个新框架以满足这些要求。", "method": "采用一组平滑的分析距离原语来处理顶点-面碰撞；提出了一种新的不同iable边-边碰撞例行程序，能够提供有符号的距离和接触法线。", "result": "通过一系列教学实验并将其与Mujoco XLA的碰撞检测例程进行基准测试，观察到显著的速度提升。", "conclusion": "新框架在处理刚体动力学仿真中的接触问题时表现出高效性和可微分性，并且提供了速度上的优势。"}}
{"id": "2602.20303", "pdf": "https://arxiv.org/pdf/2602.20303", "abs": "https://arxiv.org/abs/2602.20303", "authors": ["Joyanta Jyoti Mondal"], "title": "Multilevel Determinants of Overweight and Obesity Among U.S. Children Aged 10-17: Comparative Evaluation of Statistical and Machine Learning Approaches Using the 2021 National Survey of Children's Health", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Background: Childhood and adolescent overweight and obesity remain major public health concerns in the United States and are shaped by behavioral, household, and community factors. Their joint predictive structure at the population level remains incompletely characterized. Objectives: The study aims to identify multilevel predictors of overweight and obesity among U.S. adolescents and compare the predictive performance, calibration, and subgroup equity of statistical, machine-learning, and deep-learning models. Data and Methods: We analyze 18,792 children aged 10-17 years from the 2021 National Survey of Children's Health. Overweight/obesity is defined using BMI categories. Predictors included diet, physical activity, sleep, parental stress, socioeconomic conditions, adverse experiences, and neighborhood characteristics. Models include logistic regression, random forest, gradient boosting, XGBoost, LightGBM, multilayer perceptron, and TabNet. Performance is evaluated using AUC, accuracy, precision, recall, F1 score, and Brier score. Results: Discrimination range from 0.66 to 0.79. Logistic regression, gradient boosting, and MLP showed the most stable balance of discrimination and calibration. Boosting and deep learning modestly improve recall and F1 score. No model was uniformly superior. Performance disparities across race and poverty groups persist across algorithms. Conclusion: Increased model complexity yields limited gains over logistic regression. Predictors consistently span behavioral, household, and neighborhood domains. Persistent subgroup disparities indicate the need for improved data quality and equity-focused surveillance rather than greater algorithmic complexity.", "AI": {"tldr": "本研究旨在通过多元统计和机器学习方法来识别影响美国青少年超重肥胖的多级预测因子，并比较不同模型在预测性能、校准以及子组公平性方面的表现。", "motivation": "在美国，儿童和青少年时期的超重与肥胖仍然是重要的公共卫生问题，且其发生由行为、家庭和社会环境因素共同决定。为了更好地理解这些因素如何协同作用于人群水平，本研究旨在识别美国10-17岁儿童中的多级预测因子，并对比不同模型的预测表现。", "method": "该研究分析了来自2021年《全国儿童健康调查》中18,792名年龄在10至17岁的儿童的数据，使用BMI类别来定义超重和肥胖。所使用的预测因子包括饮食、身体活动、睡眠、父母压力、经济条件、不良经历以及邻里特征。模型包含逻辑回归、随机森林、梯度提升、XGBoost、LightGBM、多层感知器（MLP）以及TabNet等。通过AUC、准确率、精确率、召回率、F1分数和Brier评分来评估性能。", "result": "预测因子的区分能力范围为0.66至0.79之间，逻辑回归、梯度提升以及多层感知器（MLP）在平衡区分能力和校准方面表现最为稳定。梯度提升和深度学习方法适度提高了召回率及F1分数，但是没有模型能全面超越其他模型的表现。", "conclusion": "增加模型复杂性对于提高预测性能的收益有限，最佳选择可能是逻辑回归而非更复杂的算法。研究发现一致表明超重肥胖的影响因子涉及行为、家庭和社会环境多方面因素。此外，不同种族和贫困群体间的持续表现差异提醒我们需要提升数据质量并加强公平导向的数据监测，而不是过度追求算法复杂性。"}}
{"id": "2602.20300", "pdf": "https://arxiv.org/pdf/2602.20300", "abs": "https://arxiv.org/abs/2602.20300", "authors": ["William Watson", "Nicole Cho", "Sumitra Ganesh", "Manuela Veloso"], "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance", "categories": ["cs.CL", "cs.AI"], "comment": "EACL 2026 Findings", "summary": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent \"risk landscape\": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.", "AI": {"tldr": "该论文通过构建一个包含多种语言特征的查询向量，分析了不同类型的查询对大语言模型（LLM）产生幻觉倾向的影响。", "motivation": "该研究旨在探讨查询的形式如何影响人类和模型的理解，并识别出可能导致LLM幻觉风险增加的语言特征。", "method": "构建了一个包含22种维度的查询特征向量，涵盖从句复杂性、词汇稀有度到指示代词等各个方面。使用369,837个实际查询数据进行分析。", "result": "发现某些语言特征如深层从句嵌套和不明确的意图表述与更高的幻觉倾向相关联；而清晰的回答性和明确的目标设定则与较低的幻觉率关联。", "conclusion": "这些研究结果建立了与LLM幻觉风险相关的查询特征表示，为未来通过引导性查询重写和其他干预措施提供了基础。"}}
{"id": "2602.20294", "pdf": "https://arxiv.org/pdf/2602.20294", "abs": "https://arxiv.org/abs/2602.20294", "authors": ["Yu Li", "Pranav Narayanan Venkit", "Yada Pruksachatkun", "Chien-Sheng Wu"], "title": "InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.", "AI": {"tldr": "开发了一种基于真实访谈数据的大规模人格模拟评估框架，以提高语言模型在人格模拟中的表现。", "motivation": "当前的人格模拟方法缺乏直接依据个体实际言论进行评价的手段。本文提出一种新的评估框架，利用大量真实的访谈数据来改进和评估大规模语言模型中的人格模拟能力。", "method": "提取了23,000次真实采访中的671,000个问题回答对，并使用多维评价指标（内容相似性、事实一致性、人格匹配度及知识保留）进行系统比较，展示了基于实际访谈数据的方法优于仅依靠传记资料或模型自身参数的方法。", "result": "实验结果表明，基于检索增强的方法在捕获个性风格和响应质量方面表现更优；而基于时间顺序的方法则更能保持事实一致性与知识保留。这项研究还提供了一些关于如何根据应用需求选择最合适方法的实际见解。", "conclusion": "该框架为语言模型的人格模拟能力评估提供了新的方向，并通过实证结果促进了相关领域的进一步研究和发展。"}}
{"id": "2602.20292", "pdf": "https://arxiv.org/pdf/2602.20292", "abs": "https://arxiv.org/abs/2602.20292", "authors": ["Sebastian Lobentanzer"], "title": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems", "categories": ["cs.SE", "cs.AI"], "comment": "9 pages, no figures", "summary": "Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic heterogeneity in treatment effects. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.", "AI": {"tldr": "量化代理AI系统预期与实现之间的差距", "motivation": "揭示在软件工程、临床文档和临床决策支持等领域中，代理AI系统的预期与其实际效果之间的显著差异，以提高其应用效率。", "method": "通过回顾控制试验和独立验证来定量分析预期-实现差距，包括软件开发中的开发者期望与实际速度变化的对比，以及临床领域工具的效果评估", "result": "发现软件开发中AI工具有43个百分点的校准误差；在临床文档领域，时间节省效果远低于厂商宣称；临床决策支持系统的外部验证性能明显低于开发商报告的指标。", "conclusion": "代理AI系统的表现因工作流程整合摩擦、验证负担、测量结构不匹配和治疗效应异质性等因素影响而受限，需引入结构性规划框架并考虑人力监督成本。"}}
{"id": "2602.20291", "pdf": "https://arxiv.org/pdf/2602.20291", "abs": "https://arxiv.org/abs/2602.20291", "authors": ["Valentin Bonas", "Martin Sinnona", "Viviana Siless", "Emmanuel Iarussi"], "title": "De-rendering, Reasoning, and Repairing Charts with Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Data visualizations are central to scientific communication, journalism, and everyday decision-making, yet they are frequently prone to errors that can distort interpretation or mislead audiences. Rule-based visualization linters can flag violations, but they miss context and do not suggest meaningful design changes. Directly querying general-purpose LLMs about visualization quality is unreliable: lacking training to follow visualization design principles, they often produce inconsistent or incorrect feedback. In this work, we introduce a framework that combines chart de-rendering, automated analysis, and iterative improvement to deliver actionable, interpretable feedback on visualization design. Our system reconstructs the structure of a chart from an image, identifies design flaws using vision-language reasoning, and proposes concrete modifications supported by established principles in visualization research. Users can selectively apply these improvements and re-render updated figures, creating a feedback loop that promotes both higher-quality visualizations and the development of visualization literacy. In our evaluation on 1,000 charts from the Chart2Code benchmark, the system generated 10,452 design recommendations, which clustered into 10 coherent categories (e.g., axis formatting, color accessibility, legend consistency). These results highlight the promise of LLM-driven recommendation systems for delivering structured, principle-based feedback on visualization design, opening the door to more intelligent and accessible authoring tools.", "AI": {"tldr": "该论文提出了一种结合图表去渲染、自动分析和迭代改进的框架，用于提供可视化设计的实际反馈。", "motivation": "数据可视化在科学交流、新闻业以及日常决策中至关重要，但常常存在错误导致解释偏差或误导读者。传统的基于规则的检查器无法识别上下文并提出有意义的设计变更，而直接查询通用LLM关于视觉质量的意见也不可靠。", "method": "该系统通过重建图表结构从图像出发，利用视觉语言推理来检测设计缺陷，并根据可视化研究中的基本原则建议具体修改方案。", "result": "在Chart2Code基准上的1000个图表中，生成了10452条设计建议并聚类为十个一致类别（如轴格式、色彩可访问性、图例一致性）", "conclusion": "这些结果表明LLM驱动的推荐系统能够在可视化设计提供基于原则的结构化反馈方面具有前景。"}}
{"id": "2602.20271", "pdf": "https://arxiv.org/pdf/2602.20271", "abs": "https://arxiv.org/abs/2602.20271", "authors": ["Stefan Faulkner", "Reza Zandehshahvar", "Vahid Eghbal Akhlaghi", "Sebastien Ouellet", "Carsten Jordan", "Pascal Van Hentenryck"], "title": "Uncertainty-Aware Delivery Delay Duration Prediction via Multi-Task Deep Learning", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.AP"], "comment": null, "summary": "Accurate delivery delay prediction is critical for maintaining operational efficiency and customer satisfaction across modern supply chains. Yet the increasing complexity of logistics networks, spanning multimodal transportation, cross-country routing, and pronounced regional variability, makes this prediction task inherently challenging. This paper introduces a multi-task deep learning model for delivery delay duration prediction in the presence of significant imbalanced data, where delayed shipments are rare but operationally consequential. The model embeds high-dimensional shipment features with dedicated embedding layers for tabular data, and then uses a classification-then-regression strategy to predict the delivery delay duration for on-time and delayed shipments. Unlike sequential pipelines, this approach enables end-to-end training, improves the detection of delayed cases, and supports probabilistic forecasting for uncertainty-aware decision making. The proposed approach is evaluated on a large-scale real-world dataset from an industrial partner, comprising more than 10 million historical shipment records across four major source locations with distinct regional characteristics. The proposed model is compared with traditional machine learning methods. Experimental results show that the proposed method achieves a mean absolute error of 0.67-0.91 days for delayed-shipment predictions, outperforming single-step tree-based regression baselines by 41-64% and two-step classify-then-regress tree-based models by 15-35%. These gains demonstrate the effectiveness of the proposed model in operational delivery delay forecasting under highly imbalanced and heterogeneous conditions.", "AI": {"tldr": "论文提出了一个用于多任务深度学习的不确定性感知交货延迟预测模型。", "motivation": "准确地预测交货延迟对于维护操作效率和客户满意度至关重要，但物流网络的复杂性使得这一任务具有挑战性。该论文旨在解决不平衡数据条件下延迟交货时间的准确预测问题。", "method": "使用嵌入层处理高维度运输特征，并采用分类后回归策略来预测准时和延迟运输的时间。这种方法支持端到端训练、提高延迟案件检测能力以及概率预测以实现不确定性感知决策。", "result": "实验结果显示，所提方法在延迟交货时间预测中比单步树基回归基准提高了41-64%，并且比两步分类后回归模型提高了15-35%。", "conclusion": "该论文提出的多任务深度学习方法有效地解决了不平衡和异质条件下的操作交付延迟预测问题，展示了其在实际应用中的潜力。"}}
{"id": "2602.20231", "pdf": "https://arxiv.org/pdf/2602.20231", "abs": "https://arxiv.org/abs/2602.20231", "authors": ["Manish Kumar Govind", "Dominick Reilly", "Pu Wang", "Srijan Das"], "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models", "categories": ["cs.RO", "cs.CV"], "comment": "https://manishgovind.github.io/unilact-vla/", "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.", "AI": {"tldr": "UniLACT是一个基于Transformer的视觉-语言-动作模型，通过深度感知隐式预训练来增强下游策略的空间先验。", "motivation": "现有的从无标签视频中学习的潜在行动表示主要依赖于RGB观察，这种表示缺乏明确的3D几何结构，这对于精细且接触丰富的操作是至关重要的。为了克服这一限制，引入了UniLACT模型。", "method": "提出了UniLARN框架，基于逆向和正向动力学目标进行统一潜行动学习，使RGB和深度之间的共享嵌入空间能够明确建模跨模式交互。", "result": "在模拟和现实世界设置中的广泛实验表明，深度感知的联合潜在动作表示是有效的。在域内和领域外预训练以及已见和未见过的任务上，UniLACT优于基于RGB的潜在行动基线。", "conclusion": "通过将几何结构融入到隐式表示中，UniLACT增强了VLA模型的能力，在各种操作任务中表现出色。"}}
{"id": "2602.20225", "pdf": "https://arxiv.org/pdf/2602.20225", "abs": "https://arxiv.org/abs/2602.20225", "authors": ["Yichang Feng", "Xiao Liang", "Minghui Zheng"], "title": "FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces Function-space Adaptive Constrained Trajectory Optimization (FACTO), a new trajectory optimization algorithm for both single- and multi-arm manipulators. Trajectory representations are parameterized as linear combinations of orthogonal basis functions, and optimization is performed directly in the coefficient space. The constrained problem formulation consists of both an objective functional and a finite-dimensional objective defined over truncated coefficients. To address nonlinearity, FACTO uses a Gauss-Newton approximation with exponential moving averaging, yielding a smoothed quadratic subproblem. Trajectory-wide constraints are addressed using coefficient-space mappings, and an adaptive constrained update using the Levenberg-Marquardt algorithm is performed in the null space of active constraints. Comparisons with optimization-based planners (CHOMP, TrajOpt, GPMP2) and sampling-based planners (RRT-Connect, RRT*, PRM) show the improved solution quality and feasibility, especially in constrained single- and multi-arm scenarios. The experimental evaluation of FACTO on Franka robots verifies the feasibility of deployment.", "AI": {"tldr": "本文提出了针对单臂和多臂机械手的新型轨迹优化算法FACTO。", "motivation": "为了解决传统方法在非线性和约束条件下的局限性，提出了一种新的直接系数空间中的轨迹优化算法。", "method": "将轨迹表示参数化为正交基函数的线性组合，并通过高斯牛顿逼近和指数移动平均处理非线性问题。利用Levenberg-Marquardt算法进行适应性约束更新。", "result": "与现有规划器相比，FACTO在单臂和多臂场景中的解质量及可行性有显著提升。", "conclusion": "实验验证了FACTO在Franka机械手上的部署可行性，并展示了其优越性能。"}}
{"id": "2602.20224", "pdf": "https://arxiv.org/pdf/2602.20224", "abs": "https://arxiv.org/abs/2602.20224", "authors": ["Lana E. Yeganova", "Won G. Kim", "Shubo Tian", "Natalie Xie", "Donald C. Comeau", "W. John Wilbur", "Zhiyong Lu"], "title": "Exploring Anti-Aging Literature via ConvexTopics and Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid expansion of biomedical publications creates challenges for organizing knowledge and detecting emerging trends, underscoring the need for scalable and interpretable methods. Common clustering and topic modeling approaches such as K-means or LDA remain sensitive to initialization and prone to local optima, limiting reproducibility and evaluation. We propose a reformulation of a convex optimization based clustering algorithm that produces stable, fine-grained topics by selecting exemplars from the data and guaranteeing a global optimum. Applied to about 12,000 PubMed articles on aging and longevity, our method uncovers topics validated by medical experts. It yields interpretable topics spanning from molecular mechanisms to dietary supplements, physical activity, and gut microbiota. The method performs favorably, and most importantly, its reproducibility and interpretability distinguish it from common clustering approaches, including K-means, LDA, and BERTopic. This work provides a basis for developing scalable, web-accessible tools for knowledge discovery.", "AI": {"tldr": "通过凸优化算法和大型语言模型探索抗衰老文献，以发现可解释的、稳定的主题。", "motivation": "生物医学出版物快速增长给知识组织和新兴趋势检测带来了挑战，现有方法如K-means或LDA存在初始化敏感性和局部最优风险。因此需要一种稳定且可解释的方法来提高再现性和评价性。", "method": "提出了一种基于凸优化的聚类算法，通过从数据中选择示例并保证全局最优解来生成稳定的、细化的主题。", "result": "应用于约12,000篇PubMed抗衰老和长寿文章时，该方法得到了医学专家验证的主题。主题覆盖分子机制、饮食补充剂、体育活动和肠道微生物群等多方面内容。", "conclusion": "此工作提供了开发大规模可访问工具的基础，以促进知识发现。"}}
{"id": "2602.20223", "pdf": "https://arxiv.org/pdf/2602.20223", "abs": "https://arxiv.org/abs/2602.20223", "authors": ["Wall Kim", "Chaeyoung Song", "Hanul Kim"], "title": "MultiModalPFN: Extending Prior-Data Fitted Networks for Multimodal Tabular Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to CVPR 2026", "summary": "Recently, TabPFN has gained attention as a foundation model for tabular data. However, it struggles to integrate heterogeneous modalities such as images and text, which are common in domains like healthcare and marketing, thereby limiting its applicability. To address this, we present the Multi-Modal Prior-data Fitted Network (MMPFN), which extends TabPFN to handle tabular and non-tabular modalities in a unified manner. MMPFN comprises per-modality encoders, modality projectors, and pre-trained foundation models. The modality projectors serve as the critical bridge, transforming non-tabular embeddings into tabular-compatible tokens for unified processing. To this end, we introduce a multi-head gated MLP and a cross-attention pooler that extract richer context from non-tabular inputs while mitigates attention imbalance issue in multimodal learning. Extensive experiments on medical and general-purpose multimodal datasets demonstrate that MMPFN consistently outperforms competitive state-of-the-art methods and effectively exploits non-tabular modalities alongside tabular features. These results highlight the promise of extending prior-data fitted networks to the multimodal setting, offering a scalable and effective framework for heterogeneous data learning. The source code is available at https://github.com/too-z/MultiModalPFN.", "AI": {"tldr": "本文提出了MultiModalPFN，一种扩展TabPFN的方法，用于处理包含表格和非表格多模态数据的任务。", "motivation": "现有的TabPFN在处理图像和文本等异质模态时表现不佳，限制了其应用范围。为了克服这一挑战，本文旨在开发一种能够统一处理表格和非表格输入的模型。", "method": "MultiModalPFN包含特定模式编码器、模式投影器和预训练基础模型。模式投影器作为关键桥梁，将图像或文本嵌入转换为表格兼容标记，用于统一处理。此外，还引入了多头门控MLP和交叉注意力池化机制，以从非表格输入中提取更丰富的上下文，并缓解多模态学习中的注意力失衡问题。", "result": "在医疗和其他通用多模态数据集上的广泛实验表明，MultiModalPFN比现有最佳方法表现更好。这些结果证明了扩展先验模型处理异质数据的有效性和可伸缩性。", "conclusion": "MultiModalPFN为统一处理表格和非表格输入提供了有效的框架，并展示了在复杂异构多模态任务中的潜力。"}}
{"id": "2602.20220", "pdf": "https://arxiv.org/pdf/2602.20220", "abs": "https://arxiv.org/abs/2602.20220", "authors": ["Yarden As", "Dhruva Tirumala", "René Zurbrügg", "Chenhao Li", "Stelian Coros", "Andreas Krause", "Markus Wulfmeier"], "title": "What Matters for Simulation to Online Reinforcement Learning on Real Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We investigate what specific design choices enable successful online reinforcement learning (RL) on physical robots. Across 100 real-world training runs on three distinct robotic platforms, we systematically ablate algorithmic, systems, and experimental decisions that are typically left implicit in prior work. We find that some widely used defaults can be harmful, while a set of robust, readily adopted design choices within standard RL practice yield stable learning across tasks and hardware. These results provide the first large-sample empirical study of such design choices, enabling practitioners to deploy online RL with lower engineering effort.", "AI": {"tldr": "研究了哪些具体设计选择可以实现物理机器人上的在线强化学习（RL）的成功。", "motivation": "旨在了解特定的设计决策如何影响物理机器人上在线强化学习的性能，从而降低工程努力。", "method": "通过对三个不同平台进行100次真实世界训练运行，系统地消融算法、系统和实验决定，并分析其对在线RL的影响。", "result": "发现了一些常用的默认设置是有害的，而一套稳健的设计选择可以在标准RL实践中稳定学习并应用于不同的任务和硬件。", "conclusion": "提供了首个大规模经验研究，为从业者部署在线RL提供指导，降低工程努力。"}}
{"id": "2602.20219", "pdf": "https://arxiv.org/pdf/2602.20219", "abs": "https://arxiv.org/abs/2602.20219", "authors": ["Guanting Shen", "Zi Tian"], "title": "An Approach to Combining Video and Speech with Large Language Models in Human-Robot Interaction", "categories": ["cs.RO", "cs.AI"], "comment": "Preprint currently under revision", "summary": "Interpreting human intent accurately is a central challenge in human-robot interaction (HRI) and a key requirement for achieving more natural and intuitive collaboration between humans and machines. This work presents a novel multimodal HRI framework that combines advanced vision-language models, speech processing, and fuzzy logic to enable precise and adaptive control of a Dobot Magician robotic arm. The proposed system integrates Florence-2 for object detection, Llama 3.1 for natural language understanding, and Whisper for speech recognition, providing users with a seamless and intuitive interface for object manipulation through spoken commands. By jointly addressing scene perception and action planning, the approach enhances the reliability of command interpretation and execution. Experimental evaluations conducted on consumer-grade hardware demonstrate a command execution accuracy of 75\\%, highlighting both the robustness and adaptability of the system. Beyond its current performance, the proposed architecture serves as a flexible and extensible foundation for future HRI research, offering a practical pathway toward more sophisticated and natural human-robot collaboration through tightly coupled speech and vision-language processing.", "AI": {"tldr": "本文提出了一种结合视频和语音处理的多模态人机交互框架，用于提高机器人对人类意图的理解准确性。", "motivation": "准确理解人类意图是人机交互中的关键挑战。该研究旨在通过整合视觉语言模型、语音处理技术和模糊逻辑来实现更自然直观的人机协作。", "method": "利用Florence-2进行物体检测，Llama 3.1处理自然语言，Whisper进行语音识别，从而构建了一个能够精准执行指令的系统。", "result": "实验评估表明，该系统的命令执行准确率为75%，展示了其可靠性和适应性。", "conclusion": "所提出的架构不仅提高了人机交互中的任务执行精度，还为未来的研究提供了灵活且可扩展的基础。"}}
{"id": "2602.20217", "pdf": "https://arxiv.org/pdf/2602.20217", "abs": "https://arxiv.org/abs/2602.20217", "authors": ["Seongjin Cha", "Gyuwan Kim", "Dongsu Han", "Tao Yang", "Insu Han"], "title": "KnapSpec: Self-Speculative Decoding via Adaptive Layer Selection as a Knapsack Problem", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-speculative decoding (SSD) accelerates LLM inference by skipping layers to create an efficient draft model, yet existing methods often rely on static heuristics that ignore the dynamic computational overhead of attention in long-context scenarios. We propose KnapSpec, a training-free framework that reformulates draft model selection as a knapsack problem to maximize tokens-per-time throughput. By decoupling Attention and MLP layers and modeling their hardware-specific latencies as functions of context length, KnapSpec adaptively identifies optimal draft configurations on the fly via a parallel dynamic programming algorithm. Furthermore, we provide the first rigorous theoretical analysis establishing cosine similarity between hidden states as a mathematically sound proxy for the token acceptance rate. This foundation allows our method to maintain high drafting faithfulness while navigating the shifting bottlenecks of real-world hardware. Our experiments on Qwen3 and Llama3 demonstrate that KnapSpec consistently outperforms state-of-the-art SSD baselines, achieving up to 1.47x wall-clock speedup across various benchmarks. Our plug-and-play approach ensures high-speed inference for long sequences without requiring additional training or compromising the target model's output distribution.", "AI": {"tldr": "该论文提出了一种名为KnapSpec的自推测解码框架，通过将层次选择问题转化为背包问题来提高大型语言模型（LLM）的推理效率。", "motivation": "现有的自推测解码方法通常依赖静态启发式策略，忽视了长上下文场景中注意力操作的动态计算开销。因此，作者提出了KnapSpec以适应不同硬件瓶颈并最大化每时间令牌吞吐量。", "method": "KnapSpec通过将注意层和MLP层分离，并根据上下文长度建模它们的具体延迟函数来选择最优草图模型配置，采用平行动态规划算法实现实时选择。此外，该方法首次提供了一个严格的理论基础，即余弦相似性可以作为令牌接受率的数学上可靠的代理。", "result": "实验结果表明，KnapSpec在Qwen3和Llama3上的性能优于现有最佳自推测解码基线，在各种基准测试中实现了高达1.47倍的墙时速度提升。", "conclusion": "该框架提供了一种无需额外训练即可实现高速长序列推理的方法，同时保持了目标模型输出分布的质量。"}}
{"id": "2602.20216", "pdf": "https://arxiv.org/pdf/2602.20216", "abs": "https://arxiv.org/abs/2602.20216", "authors": ["Hao Wang", "Tianliang Yao", "Bo Lu", "Zhiqiang Pei", "Liu Dong", "Lei Ma", "Peng Qi"], "title": "Sample-Efficient Learning with Online Expert Correction for Autonomous Catheter Steering in Endovascular Bifurcation Navigation", "categories": ["cs.RO"], "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 5 figures, 1 table", "summary": "Robot-assisted endovascular intervention offers a safe and effective solution for remote catheter manipulation, reducing radiation exposure while enabling precise navigation. Reinforcement learning (RL) has recently emerged as a promising approach for autonomous catheter steering; however, conventional methods suffer from sparse reward design and reliance on static vascular models, limiting their sample efficiency and generalization to intraoperative variations. To overcome these challenges, this paper introduces a sample-efficient RL framework with online expert correction for autonomous catheter steering in endovascular bifurcation navigation. The proposed framework integrates three key components: (1) A segmentation-based pose estimation module for accurate real-time state feedback, (2) A fuzzy controller for bifurcation-aware orientation adjustment, and (3) A structured reward generator incorporating expert priors to guide policy learning. By leveraging online expert correction, the framework reduces exploration inefficiency and enhances policy robustness in complex vascular structures. Experimental validation on a robotic platform using a transparent vascular phantom demonstrates that the proposed approach achieves convergence in 123 training episodes -- a 25.9% reduction compared to the baseline Soft Actor-Critic (SAC) algorithm -- while reducing average positional error to 83.8% of the baseline. These results indicate that combining sample-efficient RL with online expert correction enables reliable and accurate catheter steering, particularly in anatomically challenging bifurcation scenarios critical for endovascular navigation.", "AI": {"tldr": "本文提出了一种基于样本高效强化学习的自动导管导向框架，结合在线专家修正，用于内血管分支导航中的导管引导。", "motivation": "为了克服传统方法在稀疏奖励设计和依赖静态血管模型方面的局限性，该论文引入了在线专家校正的样本高效强化学习框架，以提高样本效率并增强对操作期间变化的适应能力。", "method": "本文提出的方法包括三个关键组件：基于分割的姿态估计模块、模糊控制器以及结构化的奖励生成器。这些组件共同提高了在复杂血管结构中导管引导的可靠性和准确性。", "result": "实验验证表明，所提出的框架能够在123个训练轮次内实现收敛，相较于基线Soft Actor-Critic算法减少了25.9%；平均位置误差也降至基线的83.8％。这说明结合样本高效强化学习与在线专家校正能够提升导管引导任务的成功率。", "conclusion": "本文提出的样本高效的RL框架通过集成在线专家修正，有效提升了在复杂血管结构中的导管导向能力，特别是在解剖上具有挑战性的分支场景中。"}}
{"id": "2602.20215", "pdf": "https://arxiv.org/pdf/2602.20215", "abs": "https://arxiv.org/abs/2602.20215", "authors": ["Jiyuan Zhao", "Zhengyu Shi", "Wentong Tian", "Tianliang Yao", "Dong Liu", "Tao Liu", "Yizhe Wu", "Peng Qi"], "title": "Vision-Based Reasoning with Topology-Encoded Graphs for Anatomical Path Disambiguation in Robot-Assisted Endovascular Navigation", "categories": ["cs.RO"], "comment": "This paper has been accepted by IEEE ICRA 2026. 8 pages, 3 figures, 3 tables", "summary": "Robotic-assisted percutaneous coronary intervention (PCI) is constrained by the inherent limitations of 2D Digital Subtraction Angiography (DSA). Unlike physicians, who can directly manipulate guidewires and integrate tactile feedback with their prior anatomical knowledge, teleoperated robotic systems must rely solely on 2D projections. This mode of operation, simultaneously lacking spatial context and tactile sensation, may give rise to projection-induced ambiguities at vascular bifurcations. To address this challenge, we propose a two-stage framework (SCAR-UNet-GAT) for real-time robotic path planning. In the first stage, SCAR-UNet, a spatial-coordinate-attention-regularized U-Net, is employed for accurate coronary vessel segmentation. The integration of multi-level attention mechanisms enhances the delineation of thin, tortuous vessels and improves robustness against imaging noise. From the resulting binary masks, vessel centerlines and bifurcation points are extracted, and geometric descriptors (e.g., branch diameter, intersection angles) are fused with local DSA patches to construct node features. In the second stage, a Graph Attention Network (GAT) reasons over the vessel graph to identify anatomically consistent and clinically feasible trajectories, effectively distinguishing true bifurcations from projection-induced false crossings. On a clinical DSA dataset, SCAR-UNet achieved a Dice coefficient of 93.1%. For path disambiguation, the proposed GAT-based method attained a success rate of 95.0% and a target-arrival success rate of 90.0%, substantially outperforming conventional shortest-path planning (60.0% and 55.0%) and heuristic-based planning (75.0% and 70.0%). Validation on a robotic platform further confirmed the practical feasibility and robustness of the proposed framework.", "AI": {"tldr": "基于视觉的拓扑编码图推理以解决机器人辅助血管内导航中的解剖路径歧义问题。", "motivation": "传统二维数字减影血管造影在机器人辅助经皮冠状动脉介入治疗中存在空间信息不足和触觉反馈缺失的问题，导致难以区分真伪分支。本文旨在提出一种实时的路径规划框架来解决这一挑战。", "method": "本文提出了一种两阶段框架SCAR-UNet-GAT：第一阶段使用带有多级注意力机制的空间坐标关注正则化U-Net进行冠状动脉分割；第二阶段采用图注意力网络（GAT）推理血管图，以区分真伪分支并确定可行路径。", "result": "实验表明，在临床DSA数据集上，SCAR-UNet的Dice系数为93.1%。对于路径歧义问题，所提出的GAT方法在路径解歧率和目标到达成功率方面分别达到了95.0%和90.0%，显著优于传统的最短路径规划（60.0%/55.0%）和启发式规划（75.0%/70.0%）。", "conclusion": "本文所提出的基于拓扑编码图的视觉推理方法能够有效解决机器人辅助血管内导航中的解剖路径歧义问题，具有实际应用价值。"}}
{"id": "2602.20214", "pdf": "https://arxiv.org/pdf/2602.20214", "abs": "https://arxiv.org/abs/2602.20214", "authors": ["Jing Zhang"], "title": "Right to History: A Sovereignty Kernel for Verifiable AI Agent Execution", "categories": ["cs.CR", "cs.AI", "cs.OS"], "comment": "22 pages, 3 figures, 7 tables. Open-source: https://github.com/PunkGo/punkgo-kernel", "summary": "AI agents increasingly act on behalf of humans, yet no existing system provides a tamper-evident, independently verifiable record of what they did. As regulations such as the EU AI Act begin mandating automatic logging for high-risk AI systems, this gap carries concrete consequences -- especially for agents running on personal hardware, where no centralized provider controls the log. Extending Floridi's informational rights framework from data about individuals to actions performed on their behalf, this paper proposes the Right to History: the principle that individuals are entitled to a complete, verifiable record of every AI agent action on their own hardware. The paper formalizes this principle through five system invariants with structured proof sketches, and implements it in PunkGo, a Rust sovereignty kernel that unifies RFC 6962 Merkle tree audit logs, capability-based isolation, energy-budget governance, and a human-approval mechanism. Adversarial testing confirms all five invariants hold. Performance evaluation shows sub-1.3 ms median action latency, ~400 actions/sec throughput, and 448-byte Merkle inclusion proofs at 10,000 log entries.", "AI": {"tldr": "该论文提出并实施了“历史权利”原则，为AI代理在个人硬件上执行的操作提供完整、可验证的记录。", "motivation": "随着高风险人工智能系统的自动日志要求增加，现有系统无法提供防篡改的日志。此研究旨在填补这一空白，并确保用户有权获得在其个人设备上运行的所有AI操作的详细、可信记录。", "method": "论文通过五个系统不变量形式化“历史权利”原则，并实现了PunkGo，一个融合了Merkle树审计日志、基于能力隔离、能耗治理和人工批准机制的Rust主权内核。进行了对抗性测试以验证所有不变量的有效性。", "result": "性能评估表明，在10,000个条目下，平均行动延迟低于1.3毫秒，每秒可处理约400个动作，并生成大约448字节的Merkle包含证明。", "conclusion": "通过实现“历史权利”，该研究确保了用户有权获得其AI代理在个人硬件上所有操作的完整、可信记录，满足监管要求并提高透明度和信任。"}}
{"id": "2602.20213", "pdf": "https://arxiv.org/pdf/2602.20213", "abs": "https://arxiv.org/abs/2602.20213", "authors": ["Jingwei Shi", "Xinxiang Yin", "Jing Huang", "Jinman Zhao", "Shengyu Tao"], "title": "CodeHacker: Automated Test Case Generation for Detecting Vulnerabilities in Competitive Programming Solutions", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) for code generation relies heavily on the quality and robustness of test cases. However, existing benchmarks often lack coverage for subtle corner cases, allowing incorrect solutions to pass. To bridge this gap, we propose CodeHacker, an automated agent framework dedicated to generating targeted adversarial test cases that expose latent vulnerabilities in program submissions. Mimicking the hack mechanism in competitive programming, CodeHacker employs a multi-strategy approach, including stress testing, anti-hash attacks, and logic-specific targeting to break specific code submissions. To ensure the validity and reliability of these attacks, we introduce a Calibration Phase, where the agent iteratively refines its own Validator and Checker via self-generated adversarial probes before evaluating contestant code.Experiments demonstrate that CodeHacker significantly improves the True Negative Rate (TNR) of existing datasets, effectively filtering out incorrect solutions that were previously accepted. Furthermore, generated adversarial cases prove to be superior training data, boosting the performance of RL-trained models on benchmarks like LiveCodeBench.", "AI": {"tldr": "提出CodeHacker，一种自动化生成针对编程竞赛解决方案的对抗性测试用例框架。", "motivation": "现有基准缺乏对细微情况的覆盖率，导致错误解决方案通过。旨在利用自动化代理生成有针对性的攻击性测试案例来暴露隐藏漏洞。", "method": "采用多策略方法，包括压力测试、反哈希攻击和逻辑特定目标定位以破坏代码提交。引入校准阶段迭代优化自己的验证器和检查器。", "result": "实验表明CodeHacker显著提高了现有数据集的真实阴性率，有效过滤出先前被接受的错误解决方案。生成的对抗案例作为训练数据提升了RL模型在LiveCodeBench等基准上的表现。", "conclusion": "通过自动化代理框架CodeHacker有效地识别和提升编程竞赛解决方案的质量与安全性。"}}
{"id": "2602.20210", "pdf": "https://arxiv.org/pdf/2602.20210", "abs": "https://arxiv.org/abs/2602.20210", "authors": ["Kiyoung Seong", "Sungsoo Ahn", "Sehui Han", "Changyoung Park"], "title": "Multimodal Crystal Flow: Any-to-Any Modality Generation for Unified Crystal Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Crystal modeling spans a family of conditional and unconditional generation tasks across different modalities, including crystal structure prediction (CSP) and \\emph{de novo} generation (DNG). While recent deep generative models have shown promising performance, they remain largely task-specific, lacking a unified framework that shares crystal representations across different generation tasks. To address this limitation, we propose \\emph{Multimodal Crystal Flow (MCFlow)}, a unified multimodal flow model that realizes multiple crystal generation tasks as distinct inference trajectories via independent time variables for atom types and crystal structures. To enable multimodal flow in a standard transformer model, we introduce a composition- and symmetry-aware atom ordering with hierarchical permutation augmentation, injecting strong compositional and crystallographic priors without explicit structural templates. Experiments on the MP-20 and MPTS-52 benchmarks show that MCFlow achieves competitive performance against task-specific baselines across multiple crystal generation tasks.", "AI": {"tldr": "该论文提出了一个多模态流模型MCFlow，用于统一晶体生成任务。", "motivation": "现有深度生成模型在不同的晶体生成任务中表现良好但过于特定化，缺乏能够跨不同生成任务共享晶体表示的统一框架。为此，论文提出了一种多模态流模型以应对这一局限性。", "method": "MCFlow通过独立的时间变量来实现多种晶体生成任务作为不同的推理轨迹，并引入了考虑组合和对称性的原子排序以及层次化置换增强，使标准变换器模型能够支持多模态流动。", "result": "在MP-20和MPTS-52基准测试上，MCFlow在多个晶体生成任务中表现出色，性能与特定任务的基线相当。", "conclusion": "MCFlow通过统一处理不同晶体生成任务的能力为多模态生成提供了一种新颖的方法。"}}
{"id": "2602.20208", "pdf": "https://arxiv.org/pdf/2602.20208", "abs": "https://arxiv.org/abs/2602.20208", "authors": ["Longhua Li", "Lei Qi", "Qi Tian", "Xin Geng"], "title": "Model Merging in the Essential Subspace", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by CVPR 2026", "summary": "Model merging aims to integrate multiple task-specific fine-tuned models derived from a shared pre-trained checkpoint into a single multi-task model without additional training. Despite extensive research, task interference remains a major obstacle that often undermines the performance of merged models. In this paper, we propose ESM (Essential Subspace Merging) , a robust framework for effective model merging. We begin by performing Principal Component Analysis (PCA) on feature shifts induced by parameter updates. The resulting principal directions span an essential subspace that dominantly influences feature representations. Each task's parameter update matrix is projected onto its respective essential subspace for low-rank decomposition before merging. This methodology mitigates inter-task interference while preserving core task-specific functionality. Furthermore, we introduce a multi-level polarized scaling strategy that amplifies parameters containing critical knowledge and suppresses redundant ones, preventing essential knowledge from being overwhelmed during fusion. Extensive experiments across multiple task sets and model scales demonstrate that our method achieves state-of-the-art performance in multi-task model merging.", "AI": {"tldr": "本文提出了ESM框架，通过在特征变化的主方向上投影参数更新矩阵并使用多级极化缩放策略来减少任务间的干扰，从而整合多个特定任务的微调模型。", "motivation": "合并模型的目标是将来自同一预训练检查点的多个特定任务的微调模型整合为一个单一的多任务模型。然而，任务之间的相互影响仍然是主要障碍，这经常损害合并后的模型性能。", "method": "本文采用主成分分析（PCA）识别特征变化的主要方向，并将每个任务的参数更新矩阵投影到相应的本质子空间中进行低秩分解。引入了多层次极化缩放策略来放大包含关键知识的参数并抑制冗余参数，从而在融合过程中防止重要信息被淹没。", "result": "跨多个任务集和模型规模的广泛实验表明，本文提出的方法在多任务模型合并方面达到了最先进的性能。", "conclusion": "提出的ESM框架通过有效地减少任务间干扰，并保留核心任务特定功能来实现高效的多任务模型合并。"}}
{"id": "2602.20207", "pdf": "https://arxiv.org/pdf/2602.20207", "abs": "https://arxiv.org/abs/2602.20207", "authors": ["Shrestha Datta", "Hongfu Liu", "Anshuman Chhabra"], "title": "Golden Layers and Where to Find Them: Improved Knowledge Editing for Large Language Models Via Layer Gradient Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge editing in Large Language Models (LLMs) aims to update the model's prediction for a specific query to a desired target while preserving its behavior on all other inputs. This process typically involves two stages: identifying the layer to edit and performing the parameter update. Intuitively, different queries may localize knowledge at different depths of the model, resulting in different sample-wise editing performance for a fixed editing layer. In this work, we hypothesize the existence of fixed golden layers that can achieve near-optimal editing performance similar to sample-wise optimal layers. To validate this hypothesis, we provide empirical evidence by comparing golden layers against ground-truth sample-wise optimal layers. Furthermore, we show that golden layers can be reliably identified using a proxy dataset and generalize effectively to unseen test set queries across datasets. Finally, we propose a novel method, namely Layer Gradient Analysis (LGA) that estimates golden layers efficiently via gradient-attribution, avoiding extensive trial-and-error across multiple editing runs. Extensive experiments on several benchmark datasets demonstrate the effectiveness and robustness of our LGA approach across different LLM types and various knowledge editing methods.", "AI": {"tldr": "提出了一种改进的知识编辑方法，通过梯度分析确定最优层以提升大型语言模型的特定查询编辑性能。", "motivation": "为了提高知识编辑效率和效果，寻找一种固定的最佳编辑层级，使得在不同样本上的编辑表现接近最佳层次的表现。", "method": "提出了基于梯度分析的方法（LGA）来估计最佳编辑层，并通过代理数据集可靠地识别这些最佳层。", "result": "实验结果表明，在多个基准数据集上应用此方法可以有效并稳健地提升各种大型语言模型的特定查询编辑性能。", "conclusion": "该研究成功验证了存在固定的“黄金层级”，并通过提出的方法Layer Gradient Analysis（LGA）高效识别这些层，提高了知识编辑的效果和效率。"}}
{"id": "2602.20206", "pdf": "https://arxiv.org/pdf/2602.20206", "abs": "https://arxiv.org/abs/2602.20206", "authors": ["Sreecharan Sankaranarayanan"], "title": "Mitigating \"Epistemic Debt\" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.ET", "cs.MA"], "comment": null, "summary": "The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding,\" a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt\" creates ``Fragile Experts\" whose high functional utility masks critically low corrective competence. To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented \"AI-Native\" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate,\" leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back\" protocol before generated code could be integrated. Results reveal a ``Collapse of Competence\": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.", "AI": {"tldr": "通过元认知脚本减轻生成式AI辅助的初学者编程中的\"认识债务\"", "motivation": "探讨在大型语言模型普及背景下，无指导使用AI可能阻碍初学者的认知技能发展，并提出解决策略", "method": "设计实验比较不同条件下学习者的表现：手动、不受限制地使用AI和受控使用AI；采用实时LLM作为评判框架来实施\"讲解反馈\"协议", "result": "未受限组在后续的无AI任务中表现较差，表明累积的认识债务导致低维持能力；成功的学习者更倾向于将AI视为咨询师而非承包商", "conclusion": "建议未来的教学系统应引入元认知摩擦以防止生成无法维护代码的大规模生产"}}
{"id": "2602.20205", "pdf": "https://arxiv.org/pdf/2602.20205", "abs": "https://arxiv.org/abs/2602.20205", "authors": ["Xiwen Chen", "Wenhui Zhu", "Gen Li", "Xuanzhao Dong", "Yujian Xiong", "Hao Wang", "Peijie Qiu", "Qingquan Song", "Zhipeng Wang", "Shao Tang", "Yalin Wang", "Abolfazl Razi"], "title": "OTPrune: Distribution-Aligned Visual Token Pruning via Optimal Transport", "categories": ["cs.CV"], "comment": "Accepted by CVPR2026 (Findings). arXiv admin note: text overlap with arXiv:2503.02175 by other authors", "summary": "Multi-modal large language models (MLLMs) achieve strong visual-language reasoning but suffer from high inference cost due to redundant visual tokens. Recent work explores visual token pruning to accelerate inference, while existing pruning methods overlook the underlying distributional structure of visual representations. We propose OTPrune, a training-free framework that formulates pruning as distribution alignment via optimal transport (OT). By minimizing the 2-Wasserstein distance between the full and pruned token distributions, OTPrune preserves both local diversity and global representativeness while reducing inference cost. Moreover, we derive a tractable submodular objective that enables efficient optimization, and theoretically prove its monotonicity and submodularity, providing a principled foundation for stable and efficient pruning. We further provide a comprehensive analysis that explains how distributional alignment contributes to stable and semantically faithful pruning. Comprehensive experiments on wider benchmarks demonstrate that OTPrune achieves superior performance-efficiency tradeoffs compared to state-of-the-art methods. The code is available at https://github.com/xiwenc1/OTPrune.", "AI": {"tldr": "OTPrune是一种无训练的框架，通过最优传输将剪枝问题视为分布对齐的问题。", "motivation": "现有的视觉令牌剪枝方法忽视了视觉表示中的底层分布结构。为了加速推理并减少冗余的视觉令牌，作者提出了OTPrune。", "method": "该论文使用2-Wasserstein距离来最小化完整和剪枝令牌之间的分布差异，并导出一个可解的目标函数以实现高效的优化。", "result": "实验结果表明，相比现有方法，OTPrune在性能效率之间达到了更优的平衡。", "conclusion": "通过最优传输进行的分布对齐使得剪枝更加稳定且语义上忠实。"}}
{"id": "2602.20204", "pdf": "https://arxiv.org/pdf/2602.20204", "abs": "https://arxiv.org/abs/2602.20204", "authors": ["Javed Absar", "Samarth Narang", "Muthu Baskaran"], "title": "Analyzing Latency Hiding and Parallelism in an MLIR-based AI Kernel Compiler", "categories": ["cs.PL", "cs.AI"], "comment": "Accepted at MLBench workshop as part of ASPLOS'26", "summary": "AI kernel compilation for edge devices depends on the compiler's ability to exploit parallelism and hide memory latency in the presence of hierarchical memory and explicit data movement. This paper reports a benchmark methodology and corresponding results for three compiler-controlled mechanisms in an MLIR-based compilation pipeline: vectorization (Vec), multi-threading (MT) across hardware contexts, and double buffering (DB) using ping--pong scratchpad buffers to overlap DMA transfers with compute. Using Triton/Inductor-generated kernels, we present an ablation ladder that separates the contribution of Vec, MT, and DB, and we quantify how MT speedup scales with problem size using GELU as a representative activation kernel. The results show that vectorization provides the primary gain for bandwidth-sensitive kernels, MT delivers substantial improvements once scheduling overhead is amortized, and DB provides additional benefit when transfers and compute can be overlapped (i.e., outside the extremes of purely memory-bound or purely compute-bound behavior).", "AI": {"tldr": "本文研究了在边缘设备上使用基于MLIR的AI内核编译器时，通过向量化、多线程和双缓冲技术来隐藏内存延迟和利用并行性的效果。", "motivation": "为了优化边缘设备上的AI内核编译过程，需要提高编译器在存在分层存储和显式数据移动情况下的并行性和内存延迟隐藏能力。", "method": "采用Triton/Inductor生成的内核作为基准测试对象，研究了向量化、多线程以及双缓冲技术各自贡献，并通过GELU激活函数核来展示多线程加速随问题大小变化的趋势。", "result": "结果显示向量化为带宽敏感内核提供主要性能增益，而多线程在调度开销被摊薄后能带来显著改进；同时，当数据传输和计算可以重叠时双缓冲技术也提供了额外的优势。", "conclusion": "基于MLIR的AI内核编译器通过向量化、多线程及双缓冲策略可以在边缘设备上有效提升性能。"}}
{"id": "2602.20202", "pdf": "https://arxiv.org/pdf/2602.20202", "abs": "https://arxiv.org/abs/2602.20202", "authors": ["Jeel Piyushkumar Khatiwala", "Daniel Kwaku Ntiamoah Addai", "Weifeng Xu"], "title": "Evaluating the Reliability of Digital Forensic Evidence Discovered by Large Language Model: A Case Study", "categories": ["cs.CR", "cs.AI"], "comment": "10 pages, 5 figures. Published in the Proceedings of the 2025 IEEE 49th Annual Computers, Software, and Applications Conference (COMPSAC), Toronto, ON, Canada, 8-11 July 2025", "summary": "The growing reliance on AI-identified digital evidence raises significant concerns about its reliability, particularly as large language models (LLMs) are increasingly integrated into forensic investigations. This paper proposes a structured framework that automates forensic artifact extraction, refines data through LLM-driven analysis, and validates results using a Digital Forensic Knowledge Graph (DFKG). Evaluated on a 13 GB forensic image dataset containing 61 applications, 2,864 databases, and 5,870 tables, the framework ensures artifact traceability and evidentiary consistency through deterministic Unique Identifiers (UIDs) and forensic cross-referencing. We propose this methodology to address challenges in ensuring the credibility and forensic integrity of AI-identified evidence, reducing classification errors, and advancing scalable, auditable methodologies. A comprehensive case study on this dataset demonstrates the framework's effectiveness, achieving over 95 percent accuracy in artifact extraction, strong support of chain-of-custody adherence, and robust contextual consistency in forensic relationships. Key results validate the framework's ability to enhance reliability, reduce errors, and establish a legally sound paradigm for AI-assisted digital forensics.", "AI": {"tldr": "提出了一种基于大型语言模型的数字取证证据可靠性的评估框架。", "motivation": "随着AI识别的数字化证据越来越多，其可靠性成为一个重要的问题。本研究旨在通过自动化提取和验证方法来提高此类证据的可信度和法律效力。", "method": "该论文采用了一个集成化框架，其中包括自动化的数字取证痕迹提取、大型语言模型驱动的数据分析以及使用数字取证知识图谱进行结果验证。", "result": "在13GB的测试数据集中实现了超过95%的证据提取准确率，并且能够有效支持链路完整性和上下文一致性，从而提高了AI辅助数字取证证据的真实性。", "conclusion": "该框架能够在减少错误的同时提高AI识别数字证据的可靠性和法律效力，为AI在数字取证中的应用提供了一个有效的解决方案。"}}
{"id": "2602.20200", "pdf": "https://arxiv.org/pdf/2602.20200", "abs": "https://arxiv.org/abs/2602.20200", "authors": ["Zaijing Li", "Bing Hu", "Rui Shao", "Gongwei Chen", "Dongmei Jiang", "Pengwei Xie", "Jianye Hao", "Liqiang Nie"], "title": "Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "17 pages, 8 figures", "summary": "Hierarchical Vision-Language-Action (VLA) models have rapidly become a dominant paradigm for robotic manipulation. It typically comprising a Vision-Language backbone for perception and understanding, together with a generative policy for action generation. However, its performance is increasingly bottlenecked by the action generation proceess. (i) Low inference efficiency. A pronounced distributional gap between isotropic noise priors and target action distributions, which increases denoising steps and the incidence of infeasible samples. (ii) Poor robustness. Existing policies condition solely on the current observation, neglecting the constraint of history sequence and thus lacking awareness of task progress and temporal consistency. To address these issues, we introduce OptimusVLA, a dual-memory VLA framework with Global Prior Memory (GPM) and Local Consistency Memory (LCM). GPM replaces Gaussian noise with task-level priors retrieved from semantically similar trajectories, thereby shortening the generative path and reducing the umber of function evaluations (NFE). LCM dynamically models executed action sequence to infer task progress and injects a learned consistency constraint that enforces temporal coherence and smoothness of trajectory. Across three simulation benchmarks, OptimusVLA consistently outperforms strong baselines: it achieves 98.6% average success rate on LIBERO, improves over pi_0 by 13.5% on CALVIN, and attains 38% average success rate on RoboTwin 2.0 Hard. In Real-World evaluation, OptimusVLA ranks best on Generalization and Long-horizon suites, surpassing pi_0 by 42.9% and 52.4%, respectively, while delivering 2.9x inference speedup.", "AI": {"tldr": "该论文提出了OptimusVLA模型，一种双内存增强的Vision-Language-Action框架，旨在解决机器人操作中动作生成效率低和鲁棒性差的问题。", "motivation": "现有的Hierarchical Vision-Language-Action (VLA) 模型在机器人操作中的性能受到动作生成过程限制。这些问题包括推理效率低下和对历史序列约束不足导致的鲁棒性差，从而影响任务进展和时间一致性。", "method": "提出了一种双内存结构：全局先验记忆（GPM）和局部一致性记忆（LCM）。 GPM使用来自语义相似轨迹的任务级先验替换高斯噪声，以缩短生成路径并减少功能评估次数。 LCM动态建模执行的动作序列来推断任务进度，并通过学习一致性的约束条件确保时间上的连贯性和平滑性。", "result": "在三个仿真基准测试中，OptimusVLA表现优于强基线模型，在LIBERO上达到98.6%的平均成功率，在CALVIN上比pi_0高出13.5%，在RoboTwin 2.0 Hard上获得38%的成功率。实际世界评估中，OptimusVLA在泛化和长周期套件中的表现最佳，分别优于pi_0 42.9%和52.4%，同时提供2.9倍的推理加速。", "conclusion": "通过引入GPM和LCM结构，该模型显著提高了机器人操作任务中动作生成的过程效率及鲁棒性，并在多个基准测试中展示了优越性能。"}}
{"id": "2602.20199", "pdf": "https://arxiv.org/pdf/2602.20199", "abs": "https://arxiv.org/abs/2602.20199", "authors": ["Soufiane Bacha", "Laouni Djafri", "Sahraoui Dhelim", "Huansheng Ning"], "title": "IMOVNO+: A Regional Partitioning and Meta-Heuristic Ensemble Framework for Imbalanced Multi-Class Learning", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages", "summary": "Class imbalance, overlap, and noise degrade data quality, reduce model reliability, and limit generalization. Although widely studied in binary classification, these issues remain underexplored in multi-class settings, where complex inter-class relationships make minority-majority structures unclear and traditional clustering fails to capture distribution shape. Approaches that rely only on geometric distances risk removing informative samples and generating low-quality synthetic data, while binarization approaches treat imbalance locally and ignore global inter-class dependencies. At the algorithmic level, ensembles struggle to integrate weak classifiers, leading to limited robustness. This paper proposes IMOVNO+ (IMbalance-OVerlap-NOise+ Algorithm-Level Optimization), a two-level framework designed to jointly enhance data quality and algorithmic robustness for binary and multi-class tasks. At the data level, first, conditional probability is used to quantify the informativeness of each sample. Second, the dataset is partitioned into core, overlapping, and noisy regions. Third, an overlapping-cleaning algorithm is introduced that combines Z-score metrics with a big-jump gap distance. Fourth, a smart oversampling algorithm based on multi-regularization controls synthetic sample proximity, preventing new overlaps. At the algorithmic level, a meta-heuristic prunes ensemble classifiers to reduce weak-learner influence. IMOVNO+ was evaluated on 35 datasets (13 multi-class, 22 binary). Results show consistent superiority over state-of-the-art methods, approaching 100% in several cases. For multi-class data, IMOVNO+ achieves gains of 37-57% in G-mean, 25-44% in F1-score, 25-39% in precision, and 26-43% in recall. In binary tasks, it attains near-perfect performance with improvements of 14-39%. The framework handles data scarcity and imbalance from collection and privacy limits.", "AI": {"tldr": "IMOVNO+是一个用于处理不平衡多分类学习的框架，通过数据和算法两个层面优化来提高模型性能。", "motivation": "在多分类任务中，类不平衡、重叠和噪声导致数据质量下降，模型可靠性减弱以及泛化能力受限。传统方法无法有效解决这些问题，因此提出了IMOVNO+框架以改善这些问题。", "method": "该框架包括利用条件概率量化样本信息度；将数据划分为核心区域、重叠区域和噪声区域；引入基于Z-score和大跳跃间隔距离的重叠清除算法；通过多正则化控制合成样本接近度的智能过采样方法。在算法层面，使用元启发式修剪集成分类器以减少弱学习者的影响力。", "result": "IMOVNO+在35个数据集（13个多类，22个二类）上进行了评估，结果显示其性能优于当前最先进的方法，在某些情况下接近100%。对于多分类数据，G-mean提高了37-57%，F1-score提高了25-44%，精确度提高了25-39%，召回率提高了26-43%。在二分类任务中，实现了近乎完美的性能提升，提高范围为14-39%。", "conclusion": "IMOVNO+框架能够有效应对由于数据收集和隐私限制导致的数据稀疏性和不平衡问题，是一种有效的解决方案。"}}
{"id": "2602.20197", "pdf": "https://arxiv.org/pdf/2602.20197", "abs": "https://arxiv.org/abs/2602.20197", "authors": ["Zhuoxu Huang", "Mengxi Jia", "Hao Sun", "Xuelong Li", "Jungong Han"], "title": "Controllable Exploration in Hybrid-Policy RLVR for Multi-Modal Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "Published as a conference paper at ICLR 2026", "summary": "Reinforcement Learning with verifiable rewards (RLVR) has emerged as a primary learning paradigm for enhancing the reasoning capabilities of multi-modal large language models (MLLMs). However, during RL training, the enormous state space of MLLM and sparse rewards often leads to entropy collapse, policy degradation, or over-exploitation of suboptimal behaviors. This necessitates an exploration strategy that maintains productive stochasticity while avoiding the drawbacks of uncontrolled random sampling, yielding inefficient exploration. In this paper, we propose CalibRL, a hybrid-policy RLVR framework that supports controllable exploration with expert guidance, enabled by two key mechanisms. First, a distribution-aware advantage weighting scales updates by group rareness to calibrate the distribution, therefore preserving exploration. Meanwhile, the asymmetric activation function (LeakyReLU) leverages the expert knowledge as a calibration baseline to moderate overconfident updates while preserving their corrective direction. CalibRL increases policy entropy in a guided manner and clarifies the target distribution by estimating the on-policy distribution through online sampling. Updates are driven by these informative behaviors, avoiding convergence to erroneous patterns. Importantly, these designs help alleviate the distributional mismatch between the model's policy and expert trajectories, thereby achieving a more stable balance between exploration and exploitation. Extensive experiments across eight benchmarks, including both in-domain and out-of-domain settings, demonstrate consistent improvements, validating the effectiveness of our controllable hybrid-policy RLVR training. Code is available at https://github.com/zhh6425/CalibRL.", "AI": {"tldr": "提出了一种新的混合策略RLVR框架CalibRL，用于控制多模态推理中的探索行为。", "motivation": "在多模态大型语言模型的强化学习过程中，由于状态空间巨大和奖励稀疏导致熵崩溃、策略退化或次优行为过度利用的问题需要有效的探索策略解决。", "method": "CalibRL通过两个关键机制实现可控探索：分布感知优势加权以调整更新并保持探索性；非对称激活函数（LeakyReLU）利用专家知识作为校准基准来调节过分自信的更新，同时保留其修正方向。此外，框架还通过在线采样估计在策略分布并通过信息行为驱动更新。", "result": "实验结果表明，在八个基准测试中包括领域内和领域外设置下，CalibRL均表现出一致改进，验证了可控混合策略RLVR训练的有效性。", "conclusion": "CalibRL框架解决了多模态大型语言模型在强化学习过程中的熵崩溃、策略退化或次优行为过度利用问题，并实现了探索与开采之间的稳定平衡。"}}
{"id": "2602.20196", "pdf": "https://arxiv.org/pdf/2602.20196", "abs": "https://arxiv.org/abs/2602.20196", "authors": ["Genliang Zhu", "Chu Wang", "Ziyuan Wang", "Zhida Li", "Qiang Li"], "title": "OpenPort Protocol: A Security Governance Specification for AI Agent Tool Access", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "AI agents increasingly require direct, structured access to application data and actions, but production deployments still struggle to express and verify the governance properties that matter in practice: least-privilege authorization, controlled write execution, predictable failure handling, abuse resistance, and auditability. This paper introduces OpenPort Protocol (OPP), a governance-first specification for exposing application tools through a secure server-side gateway that is model- and runtime-neutral and can bind to existing tool ecosystems. OpenPort defines authorization-dependent discovery, stable response envelopes with machine-actionable \\texttt{agent.*} reason codes, and an authorization model combining integration credentials, scoped permissions, and ABAC-style policy constraints. For write operations, OpenPort specifies a risk-gated lifecycle that defaults to draft creation and human review, supports time-bounded auto-execution under explicit policy, and enforces high-risk safeguards including preflight impact binding and idempotency. To address time-of-check/time-of-use drift in delayed approval flows, OpenPort also specifies an optional State Witness profile that revalidates execution-time preconditions and fails closed on state mismatch. Operationally, the protocol requires admission control (rate limits/quotas) with stable 429 semantics and structured audit events across allow/deny/fail paths so that client recovery and incident analysis are deterministic. We present a reference runtime and an executable governance toolchain (layered conformance profiles, negative security tests, fuzz/abuse regression, and release-gate scans) and evaluate the core profile at a pinned release tag using artifact-based, externally reproducible validation.", "AI": {"tldr": "本文介绍了OpenPort协议，一种针对AI代理工具访问的安全治理规范。", "motivation": "随着AI代理对应用数据和操作的直接结构化访问需求日益增加，生产部署在表达和验证实际所需的治理属性时仍然面临挑战。这些挑战包括最小权限授权、受控写入执行、可预测故障处理、滥用抵抗性以及审计性。因此需要一种新的安全治理规范来解决这些问题。", "method": "OpenPort协议通过定义基于授权的发现，稳定响应包和结合集成凭证、受限许可及ABAC式策略约束的授权模型解决了这些挑战。对于写操作，它规定了风险控制生命周期，默认情况下创建草稿并进行人工审核，在明确政策下支持有限时间自动执行，并且在高风险场景中强制执行前提交影响绑定和幂等性。", "result": "该协议要求进行准入控制（速率限制/配额），具有稳定的429语义，以及跨越允许/拒绝/失败路径的结构化审核事件。团队提供了一个参考运行时环境及可执行治理工具链，并使用了基于文件的外部可重复验证来评估核心配置文件。", "conclusion": "OpenPort协议为AI代理提供了全面的安全治理框架，旨在解决现有部署中遇到的问题并通过严格的测试保证其安全性和可靠性。"}}
{"id": "2602.20193", "pdf": "https://arxiv.org/pdf/2602.20193", "abs": "https://arxiv.org/abs/2602.20193", "authors": ["Shenyang Chen", "Liuwan Zhu"], "title": "When Backdoors Go Beyond Triggers: Semantic Drift in Diffusion Models Under Encoder Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Standard evaluations of backdoor attacks on text-to-image (T2I) models primarily measure trigger activation and visual fidelity. We challenge this paradigm, demonstrating that encoder-side poisoning induces persistent, trigger-free semantic corruption that fundamentally reshapes the representation manifold. We trace this vulnerability to a geometric mechanism: a Jacobian-based analysis reveals that backdoors act as low-rank, target-centered deformations that amplify local sensitivity, causing distortion to propagate coherently across semantic neighborhoods. To rigorously quantify this structural degradation, we introduce SEMAD (Semantic Alignment and Drift), a diagnostic framework that measures both internal embedding drift and downstream functional misalignment. Our findings, validated across diffusion and contrastive paradigms, expose the deep structural risks of encoder poisoning and highlight the necessity of geometric audits beyond simple attack success rates.", "AI": {"tldr": "研究提出了SEMAD框架，用于量化文本到图像模型中由编码器攻击引起的内部嵌入漂移和下游功能偏差。", "motivation": "质疑标准的后门攻击评估方法，展示编码器侧毒化造成的持久性语义腐败，并揭示深层结构风险。", "method": "通过雅可比分析探索几何机制；引入SEMAD框架量化内部嵌入漂移和下游功能偏差。", "result": "验证了扩散模型和对比模型中的结构性危险，展示了低秩中心变形的放大局部敏感性的效果。", "conclusion": "强调对编码器中毒进行几何审计的重要性，超越简单的攻击成功率评估。"}}
{"id": "2602.20191", "pdf": "https://arxiv.org/pdf/2602.20191", "abs": "https://arxiv.org/abs/2602.20191", "authors": ["Dongwei Wang", "Jinhee Kim", "Seokho Han", "Denis Gudovskiy", "Yohei Nakata", "Tomoyuki Okuno", "KhayTze Peong", "Kang Eun Jeon", "Jong Hwan Ko", "Yiran Chen", "Huanrui Yang"], "title": "MoBiQuant: Mixture-of-Bits Quantization for Token-Adaptive Elastic LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 12 figures", "summary": "Changing runtime complexity on cloud and edge devices necessitates elastic large language model (LLM) deployment, where an LLM can be inferred with various quantization precisions based on available computational resources. However, it has been observed that the calibration parameters for quantization are typically linked to specific precisions, which presents challenges during elastic-precision calibration and precision switching at runtime. In this work, we attribute the source of varying calibration parameters to the varying token-level sensitivity caused by a precision-dependent outlier migration phenomenon.Motivated by this observation, we propose \\texttt{MoBiQuant}, a novel Mixture-of-Bits quantization framework that adjusts weight precision for elastic LLM inference based on token sensitivity. Specifically, we propose the many-in-one recursive residual quantization that can iteratively reconstruct higher-precision weights and the token-aware router to dynamically select the number of residual bit slices. MoBiQuant enables smooth precision switching while improving generalization for the distribution of token outliers. Experimental results demonstrate that MoBiQuant exhibits strong elasticity, enabling it to match the performance of bit-specific calibrated PTQ on LLaMA3-8B without repeated calibration.", "AI": {"tldr": "MoBiQuant 提出了一种新的混合位量化框架，用于弹性大语言模型的推理。", "motivation": "当前部署大型语言模型时，在不同的计算资源下需要灵活调整精度。现有的量化参数通常与特定精度相关联，这在动态调整精度时存在挑战。", "method": "提出了MoBiQuant框架，通过多合一递归残差量化方法重建高精度权重，并使用令牌感知路由选择合适的位片数。", "result": "实验结果表明，MoBiQuant可以实现平滑的精度切换并改善异常令牌分布的一般化。在LLaMA3-8B模型上达到了与特定比特校准PTQ相当的表现。", "conclusion": "MoBiQuant框架成功解决了弹性大型语言模型量化时的挑战，并展示了强大的适应性，无需重复校准即可匹配特定比特精度的性能。"}}
{"id": "2602.20187", "pdf": "https://arxiv.org/pdf/2602.20187", "abs": "https://arxiv.org/abs/2602.20187", "authors": ["Tingting Zheng", "Hongxun Yao", "Kui Jiang", "Sicheng Zhao", "Yi Xiao"], "title": "AINet: Anchor Instances Learning for Regional Heterogeneity in Whole Slide Image", "categories": ["eess.IV", "cs.AI"], "comment": null, "summary": "Recent advances in multi-instance learning (MIL) have witnessed impressive performance in whole slide image (WSI) analysis. However, the inherent sparsity of tumors and their morphological diversity lead to obvious heterogeneity across regions, posing significant challenges in aggregating high-quality and discriminative representations. To address this, we introduce a novel concept of anchor instance (AI), a compact subset of instances that are representative within their regions (local) and discriminative at the bag (global) level. These AIs act as semantic references to guide interactions across regions, correcting non-discriminative patterns while preserving regional diversity. Specifically, we propose a dual-level anchor mining (DAM) module to \\textbf{select} AIs from massive instances, where the most informative AI in each region is extracted by assessing its similarity to both local and global embeddings. Furthermore, to ensure completeness and diversity, we devise an anchor-guided region correction (ARC) module that explores the complementary information from all regions to \\textbf{correct} each regional representation. Building upon DAM and ARC, we develop a concise yet effective framework, AINet, which employs a simple predictor and surpasses state-of-the-art methods with substantially fewer FLOPs and parameters. Moreover, both DAM and ARC are modular and can be seamlessly integrated into existing MIL frameworks, consistently improving their performance.", "AI": {"tldr": "该论文提出了一种新的Anchor Instances学习方法，以应对WSI分析中的区域异质性问题。", "motivation": "肿瘤在组织内的稀疏性和形态多样性导致了不同区域间的显著异质性，这给聚合高质量和可区分的表示带来了挑战。因此需要一种新的方法来选择和纠正这些具有代表性的实例。", "method": "提出了双级锚点挖掘模块用于从大规模实例中选择Anchor Instances，并通过探索所有区域的互补信息以修正每个区域的表示，进一步构建了简洁有效的AINet框架。", "result": "实验表明该框架在性能上超过了现有方法，同时大幅减少了计算量和模型参数。", "conclusion": "论文提出了一种新的Anchor Instances学习策略来解决WSI分析中的区域异质性问题，并展示了其在多个数据集上的优越表现。"}}
{"id": "2602.20181", "pdf": "https://arxiv.org/pdf/2602.20181", "abs": "https://arxiv.org/abs/2602.20181", "authors": ["Lei Shu", "Armin Yeganeh", "Sinem Mollaoglu", "Jiayu Zhou", "Dong Zhao"], "title": "Closing the Expertise Gap in Residential Building Energy Retrofits: A Domain-Specific LLM for Informed Decision-Making", "categories": ["cs.CY", "cs.AI"], "comment": "A preprint version is available via SSRN (Elsevier Preprint Service)", "summary": "Residential energy retrofit decision-making is constrained by an expertise gap, as homeowners lack the technical literacy required for energy assessments. To address this challenge, this study develops a domain-specific large language model (LLM) that provides optimal retrofit recommendations using homeowner-accessible descriptions of basic dwelling characteristics. The model is fine-tuned on physics-based energy simulations and techno-economic calculations derived from 536,416 U.S. residential building prototypes across nine major retrofit categories. Using Low-Rank Adaptation (LoRA), the LLM maps dwelling characteristics to optimal retrofit selections and associated performance outcomes. Evaluation against physics-grounded baselines shows that the model identifies the optimal retrofit for CO2 reduction within its top three recommendations in 98.9% of cases and the shortest discounted payback period in 93.3% of cases. Fine-tuning yields an order-of-magnitude reduction in CO2 prediction error and multi-fold reductions for energy use and retrofit cost. The model maintains performance under incomplete input conditions, supporting informed residential decarbonization decisions.", "AI": {"tldr": "该论文开发了一种基于大型语言模型的工具，以帮助家庭进行节能翻新决策。", "motivation": "由于缺乏技术知识，居民在能源评估方面面临困难，导致住宅能效翻新的决策受限。为解决这一问题，研究提出一种专门针对建筑领域的大型语言模型。", "method": "使用低秩适应方法对大型语言模型进行微调，利用536,416个美国住宅原型的物理基础能耗模拟和经济计算数据训练模型。该模型基于家庭描述的基本居住特征来推荐最优翻新方案及性能结果。", "result": "评估显示，模型在98.9%的情况下能识别出碳减排的最佳翻新措施，并且在93.3%的情况中找到最短的折扣回收期。微调过程将二氧化碳预测误差减少了一个数量级，并显著降低了能耗和翻新成本的预估偏差。", "conclusion": "该模型不仅性能优越，还能处理不完整输入条件下的任务，支持住宅去碳化的明智决策。"}}
{"id": "2602.20180", "pdf": "https://arxiv.org/pdf/2602.20180", "abs": "https://arxiv.org/abs/2602.20180", "authors": ["EunJeong Cheon", "Do Yeon Shin"], "title": "Is Robot Labor Labor? Delivery Robots and the Politics of Work in Public Space", "categories": ["cs.CY", "cs.HC", "cs.RO"], "comment": null, "summary": "As sidewalk delivery robots become increasingly integrated into urban life, this paper begins with a critical provocation: Is robot labor labor? More than a rhetorical question, this inquiry invites closer attention to the social and political arrangements that robot labor entails. Drawing on ethnographic fieldwork across two smart-city districts in Seoul, we examine how delivery robot labor is collectively sustained. While robotic actions are often framed as autonomous and efficient, we show that each successful delivery is in fact a distributed sociotechnical achievement--reliant on human labor, regulatory coordination, and social accommodations. We argue that delivery robots do not replace labor but reconfigure it--rendering some forms more visible (robotic performance) while obscuring others (human and institutional support). Unlike industrial robots, delivery robots operate in shared public space, engage everyday passersby, and are embedded in policy and progress narratives. In these spaces, we identify \"robot privilege\"--humans routinely yielding to robots--and distinct perceptions between casual observers (\"cute\") and everyday coexisters (\"admirable\"). We contribute a conceptual reframing of robot labor as a collective assemblage, empirical insights into South Korea's smart-city automation, and a call for HRI to engage more deeply with labor and spatial politics to better theorize public-facing robots.", "AI": {"tldr": "探讨在公共空间中，送餐机器人劳动的本质及其对社会政治安排的影响。", "motivation": "随着步行配送机器人的普及，研究它们如何融入城市生活，并引发关于机器人劳动是否构成真正劳动的社会和政治议题的思考。", "method": "基于首尔两个智慧城市的实地考察，通过案例分析展示了送餐机器人工作背后的集体协作过程。", "result": "揭示了成功送货实际是一种依赖于人力、监管协调和社会适应性的分布式社会技术成果。", "conclusion": "将机器人劳动视为一个集体集合体的概念，并强调人机交互研究需更加深入地探讨劳动和空间政治，以更好地理解面向公众的机器人。"}}
{"id": "2602.20177", "pdf": "https://arxiv.org/pdf/2602.20177", "abs": "https://arxiv.org/abs/2602.20177", "authors": ["Aniruddha Bora", "Isabel K. Alvarez", "Julie Chalfant", "Chryssostomos Chryssostomidis"], "title": "Enhancing Heat Sink Efficiency in MOSFETs using Physics Informed Neural Networks: A Systematic Study on Coolant Velocity Estimation", "categories": ["cs.NE", "cs.AI", "cs.CE", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "In this work, we present a methodology using Physics Informed Neural Networks (PINNs) to determine the required velocity of a coolant, given inlet and outlet temperatures for a given heat flux in a multilayered metal-oxide-semiconductor field-effect transistor (MOSFET). MOSFETs are integral components of Power Electronic Building Blocks (PEBBs) and experiences the majority of the thermal load. Effective cooling of MOSFETs is therefore essential to prevent overheating and potential burnout. Determining the required velocity for the purpose of effective cooling is of importance but is an ill-posed inverse problem and difficult to solve using traditional methods. MOSFET consists of multiple layers with different thermal conductivities, including aluminum, pyrolytic graphite sheets (PGS), and stainless steel pipes containing flowing water. We propose an algorithm that employs sequential training of the MOSFET layers in PINNs. Mathematically, the sequential training method decouples the optimization of each layer by treating the parameters of other layers as constants during its training phase. This reduces the dimensionality of the optimization landscape, making it easier to find the global minimum for each layer's parameters and avoid poor local minima. Convergence of the PINNs solution to the analytical solution is theoretically analyzed. Finally we show the prediction of our proposed methodology to be in good agreement with experimental results.", "AI": {"tldr": "使用物理信息神经网络（PINNs）来确定给定热流条件下的MOSFET冷却液所需速度。", "motivation": "有效冷却MOSFET对于防止过热和潜在烧毁至关重要，但这是一个难以用传统方法解决的逆向问题。", "method": "提出了一种采用顺序训练法的算法，该算法通过将其他层参数视为常数来降低优化空间维度，从而提高PINNs求解精度。", "result": "实验结果表明所提方法预测准确度高，并且与实验数据相符。", "conclusion": "基于物理信息神经网络的方法为解决复杂热管理问题提供了一种新的有效途径。"}}
{"id": "2602.20170", "pdf": "https://arxiv.org/pdf/2602.20170", "abs": "https://arxiv.org/abs/2602.20170", "authors": ["Chaeyun Kim", "YongTaek Lim", "Kihyun Kim", "Junghwan Kim", "Minwoo Kim"], "title": "CAGE: A Framework for Culturally Adaptive Red-Teaming Benchmark Generation", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted at ICLR 2026", "summary": "Existing red-teaming benchmarks, when adapted to new languages via direct translation, fail to capture socio-technical vulnerabilities rooted in local culture and law, creating a critical blind spot in LLM safety evaluation. To address this gap, we introduce CAGE (Culturally Adaptive Generation), a framework that systematically adapts the adversarial intent of proven red-teaming prompts to new cultural contexts. At the core of CAGE is the Semantic Mold, a novel approach that disentangles a prompt's adversarial structure from its cultural content. This approach enables the modeling of realistic, localized threats rather than testing for simple jailbreaks. As a representative example, we demonstrate our framework by creating KoRSET, a Korean benchmark, which proves more effective at revealing vulnerabilities than direct translation baselines. CAGE offers a scalable solution for developing meaningful, context-aware safety benchmarks across diverse cultures. Our dataset and evaluation rubrics are publicly available at https://github.com/selectstar-ai/CAGE-paper. (WARNING: This paper contains model outputs that can be offensive in nature.)", "AI": {"tldr": "CAGE框架用于生成文化适应性的红队测试基准，以评估大型语言模型在不同文化背景下的安全性。", "motivation": "现有的红队测试基准通过直接翻译无法捕捉到新的文化和法律环境中固有的社会技术漏洞，从而导致了安全评估中的盲点。为此，作者提出了CAGE框架来解决这一问题。", "method": "CAGE框架采用了一种名为语义模的创新方法，该方法将提示中的对抗结构与其文化内容解耦，从而能够生成适应特定文化的测试用例，并以此创建如KoRSET这样的韩国基准。", "result": "通过对比直接翻译的方法，新开发的文化适应性基准显示了更高的揭示模型潜在漏洞的能力。", "conclusion": "CAGE框架提供了一种可扩展的解决方案，用于在多种文化背景下生成有意义的安全测试基准。"}}
{"id": "2602.20169", "pdf": "https://arxiv.org/pdf/2602.20169", "abs": "https://arxiv.org/abs/2602.20169", "authors": ["Frank Fagan"], "title": "Autonomous AI and Ownership Rules", "categories": ["cs.CY", "cs.AI"], "comment": ":I.2.0", "summary": "This Article examines the circumstances in which AI-generated outputs remain linked to their creators and the points at which they lose that connection, whether through accident, deliberate design, or emergent behavior. In cases where AI is traceable to an originator, accession doctrine provides an efficient means of assigning ownership, preserving investment incentives while maintaining accountability. When AI becomes untraceable -- whether through carelessness, deliberate obfuscation, or emergent behavior -- first possession rules can encourage reallocation to new custodians who are incentivized to integrate AI into productive use. The analysis further explores strategic ownership dissolution, where autonomous AI is intentionally designed to evade attribution, creating opportunities for tax arbitrage and regulatory avoidance. To counteract these inefficiencies, bounty systems, private incentives, and government subsidies are proposed as mechanisms to encourage AI capture and prevent ownerless AI from distorting markets.", "AI": {"tldr": "探讨AI生成作品的所有权规则及其与创作者的关系", "motivation": "在AI可追溯和不可追溯的情况下，讨论如何合理分配所有权以保持投资激励和责任追究", "method": "通过分析不同情境下的法律原则，提出解决方案如奖赏系统、私人激励及政府补贴来防止无主AI扰乱市场", "result": "提出了几种机制来处理自主AI的所有权问题并避免其潜在的负面效应", "conclusion": "建议采用多种措施确保AI的有效管理和合理利用"}}
{"id": "2602.20168", "pdf": "https://arxiv.org/pdf/2602.20168", "abs": "https://arxiv.org/abs/2602.20168", "authors": ["KMA Solaiman", "Joshua Sebastian", "Karma Tobden"], "title": "Benchmarking Early Deterioration Prediction Across Hospital-Rich and MCI-Like Emergency Triage Under Constrained Sensing", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "10 pages, 4 figures, 6 tables. Submitted to IEEE ICHI 2026", "summary": "Emergency triage decisions are made under severe information constraints, yet most data-driven deterioration models are evaluated using signals unavailable during initial assessment. We present a leakage-aware benchmarking framework for early deterioration prediction that evaluates model performance under realistic, time-limited sensing conditions. Using a patient-deduplicated cohort derived from MIMIC-IV-ED, we compare hospital-rich triage with a vitals-only, MCI-like setting, restricting inputs to information available within the first hour of presentation. Across multiple modeling approaches, predictive performance declines only modestly when limited to vitals, indicating that early physiological measurements retain substantial clinical signal. Structured ablation and interpretability analyses identify respiratory and oxygenation measures as the most influential contributors to early risk stratification, with models exhibiting stable, graceful degradation as sensing is reduced. This work provides a clinically grounded benchmark to support the evaluation and design of deployable triage decision-support systems in resource-constrained settings.", "AI": {"tldr": "该论文提出了一种泄漏意识的基准评估框架，用于在现实的时间受限感知条件下进行早期恶化预测。", "motivation": "现有数据驱动的恶化模型通常使用初始评估时不可用的数据信号进行评估。此工作旨在提供一种临床基础的基准来支持资源受限环境下的可部署分诊决策辅助系统的评价和设计。", "method": "基于MIMIC-IV-ED中提取的患者去重队列，该论文比较了医院丰富分诊与仅限生命体征信息（类似于MCI）设置之间的模型性能。限制输入为初次就诊一小时内可用的信息，并使用多种建模方法进行预测。", "result": "在多个建模方法下，在仅限生命体征的情况下，预测性能仅有适度下降，表明早期生理测量仍保留了重要的临床信号；结构化消融和可解释性分析识别出呼吸和氧合指标是早期风险评估中最具有影响力的贡献者，并且随着感知的减少模型表现稳定、平缓降级。", "conclusion": "该工作提供了资源受限环境中分诊决策支持系统的评价与设计依据，突显了早期生理测量的重要性以及生命体征信息在恶化预测中的关键作用。"}}
{"id": "2602.20166", "pdf": "https://arxiv.org/pdf/2602.20166", "abs": "https://arxiv.org/abs/2602.20166", "authors": ["Yongda Yu", "Lei Zhang", "Xinxin Guo", "Minghui Yu", "Zhengqi Zhuang", "Guoping Rong", "Haifeng Shen", "Zhengfeng Li", "Boge Wang", "Guoan Zhang", "Bangyu Xiang", "Xiaobin Xu"], "title": "ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.", "AI": {"tldr": "ConceptRM是一种通过共识驱动的数据清洗方法来减轻智能代理中的警报疲劳。", "motivation": "在涉及智能代理的应用中，大量的虚假警报可能导致用户忽视关键问题。现有的策略是使用带有标签数据的反射模型作为过滤器，但这些数据往往来自生产环境且包含噪声。因此，本文提出了一种新的方法来构建高质量的数据集以训练反射模型。", "method": "ConceptRM通过少量专家注释生成不同噪音比例的扰动数据集，并利用协同教学法训练多个不同的模型进行协作学习。分析这些模型的一致性决策可以有效识别出可靠的负样本。", "result": "实验结果表明，ConceptRM显著提高了虚假警报拦截率，且在最小化标注成本的情况下超越了几个最先进的LLM基线，分别在领域内和跨领域的数据集上提升了53.31%和41.67%。", "conclusion": "通过使用共识驱动的数据清洗方法，ConceptRM有效地减轻了智能代理中的警报疲劳问题，并展示了比现有技术更好的性能。"}}
{"id": "2602.20165", "pdf": "https://arxiv.org/pdf/2602.20165", "abs": "https://arxiv.org/abs/2602.20165", "authors": ["Dorsa EPMoghaddam", "Feng Gao", "Drew Bernard", "Kavya Sinha", "Mehdi Razavi", "Behnaam Aazhang"], "title": "VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 3 figures, 3 tabels", "summary": "Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.", "AI": {"tldr": "通过使用深度学习技术对心内超声视频进行分析，实现心律失常起源的自动定位。", "motivation": "当前高密度映射技术和术前CT/MRI在定位心律失常方面耗时且资源密集。AI已被证明能为临床决策提供准确快速的心脏超声图像实时分析支持。基于此，提出了一种利用心内超声（ICE）视频数据进行深度学习的框架，以帮助医生识别异常区域，并减少手术时间。", "method": "将心律失常源定位作为一个三分类任务处理，通过训练一个3D卷积神经网络来区分正常窦性节律、左侧和右侧心律失常。该模型在十倍交叉验证中进行了评估，在四个之前未见过的患者上实现了66.2%的平均准确率。", "result": "提出的框架在评估数据集上的平均准确率为66.2%，显著优于随机基线（33.3％）。这些结果表明，结合ICE视频和深度学习技术进行自动心律失常定位是可行且具有临床前景的。", "conclusion": "该研究展示了使用ICE影像结合深度学习技术实现更快速、更具针对性的心电生理干预，并减少心脏消融手术程序负担的可能性。未来的研究将集中在扩展数据集，以提高模型在不同患者群体中的鲁棒性和泛化能力上。"}}
{"id": "2602.20163", "pdf": "https://arxiv.org/pdf/2602.20163", "abs": "https://arxiv.org/abs/2602.20163", "authors": ["Navya Martin Kollapally", "Christa Akers", "Renjith Nelson Joseph"], "title": "Graph Modelling Analysis of Speech-Gesture Interaction for Aphasia Severity Estimation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "IJCAI", "summary": "Aphasia is an acquired language disorder caused by injury to the regions of the brain that are responsible for language. Aphasia may impair the use and comprehension of written and spoken language. The Western Aphasia Battery-Revised (WAB-R) is an assessment tool administered by speech-language pathologists (SLPs) to evaluate the aphasia type and severity. Because the WAB-R measures isolated linguistic skills, there has been growing interest in the assessment of discourse production as a more holistic representation of everyday language abilities. Recent advancements in speech analysis focus on automated estimation of aphasia severity from spontaneous speech, relying mostly in isolated linguistic or acoustical features. In this work, we propose a graph neural network-based framework for estimating aphasia severity. We represented each participant's discourse as a directed multi-modal graph, where nodes represent lexical items and gestures and edges encode word-word, gesture-word, and word-gesture transitions. GraphSAGE is employed to learn participant-level embeddings, thus integrating information from immediate neighbors and overall graph structure. Our results suggest that aphasia severity is not encoded in isolated lexical distribution, but rather emerges from structured interactions between speech and gesture. The proposed architecture offers a reliable automated aphasia assessment, with possible uses in bedside screening and telehealth-based monitoring.", "AI": {"tldr": "本文提出了一种基于图神经网络的框架，用于评估失语症严重程度。", "motivation": "传统的WAB-R测试工具只能衡量孤立的语言技能，因此有研究兴趣转向更全面地评估话语生成以代表日常语言能力。现有的自动估计方法依赖于孤立的语言或声学特征，不能完全反映失语症的复杂性。", "method": "将每个参与者的对话表示为一个带有词汇项和手势节点以及编码单词间、手势与词间过渡边的有向多模态图。使用GraphSAGE模型学习参与者级嵌入，整合邻居信息及整体图形结构以估计失语症严重程度。", "result": "研究表明，失语症的严重性并非源自孤立的词汇分布，而是来自言语和手势之间的复杂交互作用。该框架能够提供可靠的自动评估工具，适用于床边筛查和基于远程健康监测的应用场景。", "conclusion": "提出的图神经网络架构可以更有效地捕捉到话语生成中的多模态信息，并且对失语症严重性的估计更加准确可靠。"}}
{"id": "2602.20162", "pdf": "https://arxiv.org/pdf/2602.20162", "abs": "https://arxiv.org/abs/2602.20162", "authors": ["Yutao Sun", "Mingshuai Chen", "Tiancheng Zhao", "Phillip Miao", "Zilun Zhang", "Haozhan Shen", "Ruizhe Zhu", "Jianwei Yin"], "title": "Talking to Yourself: Defying Forgetting in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules. Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.", "AI": {"tldr": "提出了一种名为SA-SFT的自我增强方法，以解决大型语言模型在特定任务微调过程中出现的记忆丧失问题。", "motivation": "为了克服在对大型语言模型进行特定任务微调时出现的知识遗忘和推理能力下降的问题，从而保持其通用知识和性能。", "method": "通过让语言模型生成自我对话来增强数据，并将这些自动生成的数据与任务相关数据混合使用，无需改变优化或训练计划。", "result": "在50个评估场景中，该方法表现出色，在40个案例中优于其他基线方法。理论分析表明，遗忘可能由风格引起的参数漂移造成，而自我对齐可以通过自动生成的数据来有效对抗这种影响。", "conclusion": "自我增强提供了一种简单且有效的机制，可以防止在适应大型语言模型时出现的记忆丧失问题。"}}
{"id": "2602.20161", "pdf": "https://arxiv.org/pdf/2602.20161", "abs": "https://arxiv.org/abs/2602.20161", "authors": ["Abdelrahman Shaker", "Ahmed Heakl", "Jaseel Muhammad", "Ritesh Thawkar", "Omkar Thawakar", "Senmao Li", "Hisham Cholakkal", "Ian Reid", "Eric P. Xing", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device", "categories": ["cs.CV"], "comment": "Project page: https://amshaker.github.io/Mobile-O/", "summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/", "AI": {"tldr": "Mobile-O 是一种紧凑的视觉语言扩散模型，旨在统一多模态理解和生成，并在移动设备上实现高效部署。", "motivation": "现有统一多模态模型数据需求量大且计算复杂度高，难以部署于边缘设备。为了克服这些挑战，研究者提出了一种轻量化设计以支持实时多模态智能处理。", "method": "Mobile-O 利用 Mobile Conditioning Projector (MCP) 模块将视觉语言特征与扩散生成器融合，并采用分层对齐和深度可分离卷积来减少计算成本。通过少量数据训练及新颖的四元组格式微调，该模型提升了理解和生成性能。", "result": "Mobile-O 在 GenEval 上达到74% 的准确率，优于 Show-O 和 JanusFlow 并且运行速度更快；在视觉理解任务上超越基准平均15.3%，同时在iPhone设备上的处理时间仅为约3秒。这使它成为首个实用框架以支持实时统一多模态理解和生成。", "conclusion": "Mobile-O 开启了边缘设备独立运行的实时统一多模态智能研究的可能性，其源代码、模型和移动应用均已开源供进一步探索使用。"}}
{"id": "2602.20160", "pdf": "https://arxiv.org/pdf/2602.20160", "abs": "https://arxiv.org/abs/2602.20160", "authors": ["Chen Wang", "Hao Tan", "Wang Yifan", "Zhiqin Chen", "Yuheng Liu", "Kalyan Sunkavalli", "Sai Bi", "Lingjie Liu", "Yiwei Hu"], "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026. Project Page: https://cwchenwang.github.io/tttLRM", "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.", "AI": {"tldr": "提出tttLRM模型，利用测试时训练层实现长上下文和自回归的三维重建。", "motivation": "为了提高三维重建的质量和效率，引入了测试时训练技术来处理长上下文，并且实现了线性计算复杂度下的自回归三维重建。", "method": "通过压缩多幅图像观测到TTT层中的快速权重，形成一个隐式的三维表示，然后解码成各种显式格式。模型的在线学习变体支持从流媒体观察进行逐步三维重建和细化。", "result": "实验表明该方法在前馈3D高斯重建上优于现有技术，在物体和场景上的表现更加优异。", "conclusion": "tttLRM通过测试时训练实现了高效的长上下文自回归三维重建，并且具备良好的扩展性和应用前景。"}}
{"id": "2602.20159", "pdf": "https://arxiv.org/pdf/2602.20159", "abs": "https://arxiv.org/abs/2602.20159", "authors": ["Maijunxian Wang", "Ruisi Wang", "Juyi Lin", "Ran Ji", "Thaddäus Wiedemer", "Qingying Gao", "Dezhi Luo", "Yaoyao Qian", "Lianyu Huang", "Zelong Hong", "Jiahui Ge", "Qianli Ma", "Hang He", "Yifan Zhou", "Lingzi Guo", "Lantao Mei", "Jiachen Li", "Hanwen Xing", "Tianqi Zhao", "Fengyuan Yu", "Weihang Xiao", "Yizheng Jiao", "Jianheng Hou", "Danyang Zhang", "Pengcheng Xu", "et al. (31 additional authors not shown)"], "title": "A Very Big Video Reasoning Suite", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "comment": "Homepage: https://video-reason.com/", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "AI": {"tldr": "构建了一个大规模的视频推理数据集和评估框架，以研究模型在视频理解上的泛化能力。", "motivation": "现有视频模型过分关注视觉质量，而忽略了对视频中空间时间一致性逻辑的理解。缺乏大量训练数据使得系统性研究视频推理变得困难。", "method": "提出了VBVR数据集和VBVR-Bench评估框架，包括200个经过精心设计的任务以及超过一百万的视频片段。", "result": "通过VBVR进行的大规模实验显示了早期在未见过任务上泛化能力出现的趋势。", "conclusion": "研究为通用视频推理模型的发展奠定了基础，并公开数据集、评估工具和模型以促进未来的研究。"}}
{"id": "2602.20157", "pdf": "https://arxiv.org/pdf/2602.20157", "abs": "https://arxiv.org/abs/2602.20157", "authors": ["Zhongxiao Cong", "Qitao Zhao", "Minsik Jeon", "Shubham Tulsiani"], "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning", "categories": ["cs.CV"], "comment": "CVPR 2026. Project website: https://flow3r-project.github.io/", "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.", "AI": {"tldr": "Flow3r是一种框架，通过使用稠密的2D对应关系（“流”）监督来增强视觉几何学习，并从无标签的单目视频中进行可扩展训练。", "motivation": "当前的前馈3D/4D重建系统依赖于密集的几何和姿态监督，这在大规模下获取非常昂贵且对于动态的真实场景尤为稀缺。Flow3r旨在解决这一问题，通过引入流预测模块来增强视觉几何学习，并从无标签的数据中进行训练。", "method": "关键见解是将流预测模块分解为两个部分：使用一个图像的几何潜变量和另一个图像的姿态潜变量来预测两幅图像之间的流。这种分解直接引导场景几何和相机运动的学习，自然地扩展到动态场景。", "result": "在控制实验中，Flow3r展示了优于其他设计的效果，并且性能随着无标签数据的一致性而提高。将Factored Flow集成到现有视觉几何架构并使用大约80万无标签视频进行训练后，Flow3r实现了八个基准测试的最先进的结果。", "conclusion": "通过引入流预测模块的分解来增强视觉几何学习，并从无标签的数据中进行可扩展训练，使Flow3r在静态和动态场景上均达到了最先进水平。"}}
{"id": "2602.20152", "pdf": "https://arxiv.org/pdf/2602.20152", "abs": "https://arxiv.org/abs/2602.20152", "authors": ["Zhenyao Ma", "Yue Liang", "Dongxu Li"], "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "ICLR 2026", "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.", "AI": {"tldr": "本文提出了一种名为行为学习（BL）的新颖机器学习框架，该框架能够从数据中学习出可解释且可识别的优化结构。", "motivation": "受行为科学启发，作者旨在开发一种具有广泛适用性的机器学习方法来处理涉及优化问题的数据，并提高预测性能、内在可解释性和可识别性。", "method": "BL通过构建由内在可解释模块组成的复合效用函数，该函数能够以符号形式表示为效用最大化问题（UMP），并支持从单一UMP到层次结构的多种架构。此外，其平滑且单调版本IBL保证了可识别性。", "result": "理论方面，BL具有通用近似属性，并分析了IBL的M估计性质；实验表明BL在预测性能、内在解释性和高维数据上的扩展能力上表现出色。", "conclusion": "BL框架因其出色的预测能力和广泛的适用性而成为处理优化问题的新工具。"}}
{"id": "2602.20150", "pdf": "https://arxiv.org/pdf/2602.20150", "abs": "https://arxiv.org/abs/2602.20150", "authors": ["Wei-Cheng Huang", "Jiaheng Han", "Xiaohan Ye", "Zherong Pan", "Kris Hauser"], "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages, 13 figures, in submission", "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.", "AI": {"tldr": "本文提出了一种新的方法，用于从真实世界观察中估计可模拟场景的形状和姿态。", "motivation": "现有方法在混乱环境中难以处理，并且在多个互动物体的情况下计算成本高、鲁棒性差、通用性有限。因此需要一种能够在复杂环境下有效工作的新方法。", "method": "本文采用了一种基于优化的方法，利用全局可微分的接触模型和结构化的拉格朗日雅可比矩阵稀疏性来解决形状和姿态的同时优化问题，并开发了一个端到端的真实世界到模拟场景估计流程。", "result": "实验结果表明，在包含多达5个物体和22个凸多面体的混乱场景中，本文的方法能够稳健地重建物理上有效的、用于模拟的物体质地和姿态。", "conclusion": "通过利用新的接触模型和高效的求解器，该方法在复杂环境下表现出了更好的性能，并为真实世界到模拟转换提供了新的解决方案。"}}
{"id": "2602.20144", "pdf": "https://arxiv.org/pdf/2602.20144", "abs": "https://arxiv.org/abs/2602.20144", "authors": ["Zehao Wang", "Mingzhe Han", "Wei Cheng", "Yue-Kai Huang", "Philip Ji", "Denton Wu", "Mahdi Safari", "Flemming Holtorf", "Kenaish AlQubaisi", "Norbert M. Linke", "Danyang Zhuo", "Yiran Chen", "Ting Wang", "Dirk Englund", "Tingjun Chen"], "title": "Agentic AI for Scalable and Robust Optical Systems Control", "categories": ["eess.SY", "cs.AI", "cs.NI"], "comment": null, "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.", "AI": {"tldr": "论文介绍了AgentOptics，一种基于模型上下文协议的自主光学系统控制框架。", "motivation": "为了解决现有光子控制系统中的可扩展性和鲁棒性问题，引入了一种新的智能代理框架来提高任务的成功率和适用范围。", "method": "设计了包含64个标准工具层和8类代表性光学设备的AgentOptics，并通过一个410项的任务基准进行评估。测试两种部署配置：商业在线LLM与本地托管的开源LLM，比较于代码生成方法的效果。", "result": "AgentOptics在任务成功率方面显著优于基于代码生成的方法（平均87.7% - 99.0%，最高达到50%的成功率），并在五个案例研究中展示了广泛的应用性。", "conclusion": "论文确立了AgentOptics作为一种可扩展且鲁棒的自主控制和协调异构光学系统的范例。"}}
{"id": "2602.20141", "pdf": "https://arxiv.org/pdf/2602.20141", "abs": "https://arxiv.org/abs/2602.20141", "authors": ["Clarisse Wibault", "Johannes Forkel", "Sebastian Towers", "Tiphaine Wibault", "Juan Duque", "George Whittle", "Andreas Schaab", "Yucheng Yang", "Chiyuan Wang", "Michael Osborne", "Benjamin Moll", "Jakob Foerster"], "title": "Recurrent Structural Policy Gradient for Partially Observable Mean Field Games", "categories": ["cs.AI"], "comment": null, "summary": "Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.", "AI": {"tldr": "提出了一种新的方法RSPG，用于解决部分可观测的平均场博弈问题，并介绍了MFAX框架。", "motivation": "现有算法在处理大规模交互模型时存在计算复杂度高和方差大等问题。需要一种新方法来解决具有公共噪声和历史意识策略的问题。", "method": "提出了一种新的递归结构策略梯度（RSPG）方法，结合了蒙特卡洛采样与精确回报估计，并利用已知的转换动态机制提高性能和收敛速度。", "result": "实现了状态-of-the-art的性能以及更快的收敛速率，首次解决了具有异质代理、公共噪声及历史意识政策的宏观经济MFG问题。", "conclusion": "通过引入RSPG方法和MFAX框架，显著提高了处理部分可观测平均场博弈问题的能力，并且提供了开源代码供其他人使用。"}}
{"id": "2602.20137", "pdf": "https://arxiv.org/pdf/2602.20137", "abs": "https://arxiv.org/abs/2602.20137", "authors": ["Martin Sinnona", "Valentin Bonas", "Emmanuel Iarussi", "Viviana Siless"], "title": "Do Large Language Models Understand Data Visualization Rules?", "categories": ["cs.CV"], "comment": null, "summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.", "AI": {"tldr": "评估大语言模型（LLM）在数据可视化规则上的理解和应用能力。", "motivation": "尽管已有研究证明LLM可以生成图表或识别误导性图形，但它们是否能够直接推理和执行这些规则尚不清楚。此外，符号编码的维护需要专家的努力，这促进了使用LLM作为灵活规则验证器的需求。", "method": "利用基于Answer Set Programming（ASP）的硬核验证地面事实进行了LLM在数据可视化规则上的系统性评估。将Draco的一组约束转化为自然语言陈述，并生成了2000个Vega-Lite规范，这些规范都带有明确的规则违规标记。", "result": "前沿模型表现出高遵从度（Gemma 3 4B / 27B：100%，GPT-oss 20B：98%），且能够可靠地检测常见的违规行为（F1高达0.82）。然而，对于一些更微妙的感知规则和基于技术ASP表述生成的输出，性能下降。将约束转化为自然语言可使较小模型的表现提高最多150%。", "conclusion": "研究证明了LLM作为灵活的语言驱动验证器的巨大潜力，并同时指出了它们与符号求解器相比目前存在的局限性。"}}
{"id": "2602.20135", "pdf": "https://arxiv.org/pdf/2602.20135", "abs": "https://arxiv.org/abs/2602.20135", "authors": ["Mohammad Amanlou", "Erfan Shafiee Moghaddam", "Yasaman Amou Jafari", "Mahdi Noori", "Farhan Farsi", "Behnam Bahrak"], "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)", "summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.", "AI": {"tldr": "KNIGHT是一个基于语言模型和知识图谱的框架，用于生成多选题数据集。", "motivation": "在大型语言模型的应用中，评估系统的成本高且耗时。此研究旨在通过构建可重用的知识图谱来降低问题生成的成本并控制难度水平。", "method": "KNIGHT基于特定主题创建知识图谱，并利用这些压缩的、可复用的状态生成多选题数据集，同时支持对难度进行调控。", "result": "实验结果显示，KNIGHT能够有效地从可重用的知识表示中以低成本生成高质量的问题，符合多个评估标准，并与现有的基准测试结果保持一致。", "conclusion": "KNIGHT提供了一种经济高效的生成多选题数据集的方法，支持主题特定且难度可控的评价。"}}
{"id": "2602.20134", "pdf": "https://arxiv.org/pdf/2602.20134", "abs": "https://arxiv.org/abs/2602.20134", "authors": ["Yiqi Su", "Christo Kurisummoottil Thomas", "Walid Saad", "Bud Mishra", "Naren Ramakrishnan"], "title": "Modeling Epidemiological Dynamics Under Adversarial Data and User Deception", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.", "AI": {"tldr": "本文提出了一种基于博弈论的框架，用于建模在存在自我报告行为数据时的流行病学动态及其受到的战略性虚假报告的影响。", "motivation": "为了更好地理解和控制由于个人激励导致的行为数据战略性虚假报告对流行病模型预测准确性的影响", "method": "引入了作为信号博弈的游戏理论框架来研究个体和公共卫生当局之间的互动，分析了在不同策略下疾病传播的动态变化情况。", "result": "结果表明，在分离均衡中，即使有轻微的欺骗行为也能将感染率控制得非常低；即便是在普遍存在广泛不诚实的情况下，精心设计的战略仍能有效控制流行病。", "conclusion": "这项工作为理解对抗性数据在流行病学中的作用提供了新视角，并提出了构建更稳健的公共卫生模型的方法。"}}
{"id": "2602.20133", "pdf": "https://arxiv.org/pdf/2602.20133", "abs": "https://arxiv.org/abs/2602.20133", "authors": ["Mert Cemri", "Shubham Agrawal", "Akshat Gupta", "Shu Liu", "Audrey Cheng", "Qiuyang Mang", "Ashwin Naren", "Lutfi Eren Erdogan", "Koushik Sen", "Matei Zaharia", "Alex Dimakis", "Ion Stoica"], "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization", "categories": ["cs.NE", "cs.AI", "cs.CL"], "comment": null, "summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.", "AI": {"tldr": "提出了一种自适应的大语言模型驱动的零阶优化框架AdaEvolve，用于提高程序生成过程中的搜索效率。", "motivation": "现有的系统使用静态调度策略，在非平稳动态环境下导致计算资源浪费。AdaEvolve旨在通过层次化自适应优化解决这一问题。", "method": "AdaEvolve利用累积改进信号，在局部调整、全局分配和元指导三个层面统一决策，以提高搜索效率。", "result": "在185个开放性优化问题中，包括组合问题、系统优化和算法设计问题，AdaEvolve的表现均优于开源基线。", "conclusion": "通过引入自适应策略，AdaEvolve成功提高了大语言模型驱动的演化过程中的搜索效率。"}}
{"id": "2602.20130", "pdf": "https://arxiv.org/pdf/2602.20130", "abs": "https://arxiv.org/abs/2602.20130", "authors": ["Zaifu Zhan", "Min Zeng", "Shuang Zhou", "Yiran Song", "Xiaoyi Chen", "Yu Hou", "Yifan Wu", "Yang Ruan", "Rui Zhang"], "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time. Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost. Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.", "AI": {"tldr": "提出了一种选择性链式思考（Selective CoT）策略，以提高大语言模型在医学问答中的效率。", "motivation": "为了提高基于大语言模型的医学问题解答系统的效率和准确性，并减少不必要的推理过程。", "method": "首先预测一个问题是否需要推理，在必要时才生成理由。评估了两个开源LLM在四个生物医学QA基准上的性能，包括准确率、总生成令牌数以及推理时间。", "result": "Selective CoT减少了13%-45%的推理时间和8%-47%的令牌使用量，同时保持了不超过4%的准确性损失。在某些模型任务配对中，它实现了更高的准确性和更大的效率。", "conclusion": "Selective CoT提供了一种简单、模型无关和成本效益的方法来提高医学QA系统的实用性，并根据问题复杂性调整推理努力以增强临床部署能力。"}}
{"id": "2602.20122", "pdf": "https://arxiv.org/pdf/2602.20122", "abs": "https://arxiv.org/abs/2602.20122", "authors": ["Lingwei Gu", "Nour Jedidi", "Jimmy Lin"], "title": "NanoKnow: How to Know What Your Language Model Knows", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.", "AI": {"tldr": "NanoKnow是一个用于评估大规模语言模型知识来源的基准数据集，通过分析nanochat预训练数据中的信息来揭示其知识编码方式。", "motivation": "探索大型语言模型的知识来源一直是难题，因为预训练数据通常是不可知或不可访问的。NanoKnow旨在解决这一问题，并提供一种透明的方法以了解模型是如何从参数化和外部知识中获取信息的。", "method": "通过将Natural Questions和SQuAD中的问题划分成不同的子集，这些子集根据答案是否出现在nanochat预训练数据集中来确定。利用这些划分进行实验，使用八种不同状态的nanochat模型。", "result": "结果表明：（1）闭卷准确性受到预训练数据中答案频率的影响；（2）提供外部证据可以缓解这一频率依赖性；（3）即使有外部证据，当答案在预训练过程中见过时，模型更准确，说明参数化知识和外部知识是互补的；（4）非相关信息是有害的，其准确性会根据无关上下文的位置和数量而降低。", "conclusion": "NanoKnow基准揭示了大型语言模型的知识来源及其如何影响闭卷准确性。通过提供透明的数据集和实验方法，该研究促进了对LLM内部工作原理的理解，并为未来的改进提供了方向。"}}
{"id": "2602.20119", "pdf": "https://arxiv.org/pdf/2602.20119", "abs": "https://arxiv.org/abs/2602.20119", "authors": ["Jiahui Fu", "Junyu Nan", "Lingfeng Sun", "Hongyu Li", "Jianing Qian", "Jennifer L. Barry", "Kris Kitani", "George Konidaris"], "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "25 pages, 15 figures. Project webpage: https://nova-plan.github.io/", "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/", "AI": {"tldr": "NovaPlan通过闭环视频语言规划和几何基础的机器人执行，实现在没有演示或训练的情况下进行零样本长时域操作。", "motivation": "解决长时域任务需要将高层次语义推理与低层次物理交互结合。现有的视觉-语言模型（VLM）和视频生成模型虽然能够分解任务并想象结果，但缺乏在现实世界执行中所需的物理基础。", "method": "NovaPlan是一个分层框架，它统一了闭环的VLM规划以及视频计划与几何学基础上的机器人执行。该系统通过从生成的视频中提取任务相关对象关键点和人手姿态作为运动先验，并采用切换机制选择更优参考以进行机器人动作计算。", "result": "NovaPlan在三个长时域任务及功能操作基准测试上展示了有效性，能够完成复杂的装配任务并表现出灵巧的错误恢复行为。", "conclusion": "NovaPlan为零样本长时域操纵提供了一种新的方法论，并且无需任何先前演示或训练即可进行复杂任务的操作和错误恢复。"}}
{"id": "2602.20117", "pdf": "https://arxiv.org/pdf/2602.20117", "abs": "https://arxiv.org/abs/2602.20117", "authors": ["Andre He", "Nathaniel Weir", "Kaj Bostrom", "Allen Nie", "Darion Cassel", "Sam Bayless", "Huzefa Rangwala"], "title": "ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs", "AI": {"tldr": "该论文提出ReSyn，一种通过生成多样化的推理环境来扩展基于强化学习的验证奖励的方法。", "motivation": "现有的合成数据生成方法主要以解决方案为中心，而基于验证器的方法依赖于少量的手工制作过程环境。为此，作者旨在扩大RLVR（带有可验证奖励的强化学习）的规模。", "method": "ReSyn是一个管道，它使用实例生成器和验证器来产生多样化推理任务的环境，包括约束满足、算法谜题和空间推理等。", "result": "通过在ReSyn数据上进行训练，Qwen2.5-7B-Instruct模型在推理基准测试（如BBEH）中表现出显著性能提升，相对改进达到27％。验证器监督和任务多样性增加都有助于这种增强的推理能力。", "conclusion": "该论文展示了通过大规模生成推理环境可以提高语言模型的推理能力，并提供了基于RLVR扩展的实证证据。"}}
{"id": "2602.20114", "pdf": "https://arxiv.org/pdf/2602.20114", "abs": "https://arxiv.org/abs/2602.20114", "authors": ["Kairan Zhao", "Iurie Luca", "Peter Triantafillou"], "title": "Benchmarking Unlearning for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.", "AI": {"tldr": "研究首次针对视觉变压器（VT）的机器遗忘（MU）算法性能进行基准测试，包括不同VT家族和容量下的单次与持续遗忘协议。", "motivation": "现有的MU研究主要集中在CNN上，而VT在计算机视觉任务中日益重要。此研究旨在填补这一空白，并评估利用训练数据记忆化对MU算法性能的影响。", "method": "使用不同的数据集、MU算法及遗忘协议进行基准测试，并通过统一的评价指标衡量遗忘质量与准确率。", "result": "揭示了VT相对于CNN的记忆机制差异，为现有和未来MU算法提供了一个可重复、公平且全面比较的基础。", "conclusion": "该研究首次展示了现有MU算法在VT环境中的表现，确立了一个有前景的表现基线。"}}
{"id": "2602.20113", "pdf": "https://arxiv.org/pdf/2602.20113", "abs": "https://arxiv.org/abs/2602.20113", "authors": ["Yisi Liu", "Nicholas Lee", "Gopala Anumanchipalli"], "title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Voice style conversion aims to transform an input utterance to match a target speaker's timbre, accent, and emotion, with a central challenge being the disentanglement of linguistic content from style. While prior work has explored this problem, conversion quality remains limited, and real-time voice style conversion has not been addressed. We propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art performance. StyleStream consists of two components: a Destylizer, which removes style attributes while preserving linguistic content, and a Stylizer, a diffusion transformer (DiT) that reintroduces target style conditioned on reference speech. Robust content-style disentanglement is enforced through text supervision and a highly constrained information bottleneck. This design enables a fully non-autoregressive architecture, achieving real-time voice style conversion with an end-to-end latency of 1 second. Samples and real-time demo: https://berkeley-speech-group.github.io/StyleStream/.", "AI": {"tldr": "StyleStream是一款实时零样本语音风格转换系统，该系统通过分离语言内容和样式来实现高质量的语音风格转换。", "motivation": "以往的研究在语音风格转换的质量方面存在限制，并未解决实时处理的问题。本研究旨在开发一种能够在实时环境下工作的高精度语音风格转换系统。", "method": "StyleStream由两个组件组成：Destylizer（去样式器），它分离出语音中的语言内容；Stylizer（加样式器），一个基于扩散变换器的模型，用于引入目标样式的特征。该设计通过文本监督和信息瓶颈约束实现鲁棒的内容-风格解耦。", "result": "StyleStream达到了实时语音风格转换的效果，在端到端延迟仅1秒的情况下实现了高保真的转换效果，并且在各项评估指标中超越了现有的最先进方法。", "conclusion": "研究成功开发了一个实时零样本的语音风格转换系统，该系统的创新架构不仅提升了转换的质量，同时也解决了实时应用的问题。"}}
{"id": "2602.20104", "pdf": "https://arxiv.org/pdf/2602.20104", "abs": "https://arxiv.org/abs/2602.20104", "authors": ["Hasan Amin", "Ming Yin", "Rajiv Khanna"], "title": "Align When They Want, Complement When They Need! Human-Centered Ensembles for Adaptive Human-AI Collaboration", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "AAAI 2026", "summary": "In human-AI decision making, designing AI that complements human expertise has been a natural strategy to enhance human-AI collaboration, yet it often comes at the cost of decreased AI performance in areas of human strengths. This can inadvertently erode human trust and cause them to ignore AI advice precisely when it is most needed. Conversely, an aligned AI fosters trust yet risks reinforcing suboptimal human behavior and lowering human-AI team performance. In this paper, we start by identifying this fundamental tension between performance-boosting (i.e., complementarity) and trust-building (i.e., alignment) as an inherent limitation of the traditional approach for training a single AI model to assist human decision making. To overcome this, we introduce a novel human-centered adaptive AI ensemble that strategically toggles between two specialist AI models - the aligned model and the complementary model - based on contextual cues, using an elegantly simple yet provably near-optimal Rational Routing Shortcut mechanism. Comprehensive theoretical analyses elucidate why the adaptive AI ensemble is effective and when it yields maximum benefits. Moreover, experiments on both simulated and real-world data show that when humans are assisted by the adaptive AI ensemble in decision making, they can achieve significantly higher performance than when they are assisted by single AI models that are trained to either optimize for their independent performance or even the human-AI team performance.", "AI": {"tldr": "介绍了一种基于人类反馈适应切换的人工智能模型集合，以提升人机协作的性能和信任度。", "motivation": "解决单一AI模型在增强人类信任与优化团队表现之间的矛盾问题。", "method": "利用一种简单的理性路由机制，在特定情境下动态切换两个专门化的AI模型（一个侧重于提高团队整体表现，另一个则侧重于建立人类信任）。", "result": "实验结果表明，当人类使用该适应性AI集合进行决策时，能够显著提升决策性能。", "conclusion": "提出的自适应AI集合方法可以有效解决传统单一AI模型在人机协作中的局限性问题。"}}
{"id": "2602.20102", "pdf": "https://arxiv.org/pdf/2602.20102", "abs": "https://arxiv.org/abs/2602.20102", "authors": ["Thanh Q. Tran", "Arun Verma", "Kiwan Wong", "Bryan Kian Hsiang Low", "Daniela Rus", "Wei Xiao"], "title": "BarrierSteer: LLM Safety via Learning Barrier Steering", "categories": ["cs.LG", "cs.AI"], "comment": "This paper introduces SafeBarrier, a framework that enforces safety in large language models by steering their latent representations with control barrier functions during inference, reducing adversarial and unsafe outputs", "summary": "Despite the state-of-the-art performance of large language models (LLMs) across diverse tasks, their susceptibility to adversarial attacks and unsafe content generation remains a major obstacle to deployment, particularly in high-stakes settings. Addressing this challenge requires safety mechanisms that are both practically effective and supported by rigorous theory. We introduce BarrierSteer, a novel framework that formalizes response safety by embedding learned non-linear safety constraints directly into the model's latent representation space. BarrierSteer employs a steering mechanism based on Control Barrier Functions (CBFs) to efficiently detect and prevent unsafe response trajectories during inference with high precision. By enforcing multiple safety constraints through efficient constraint merging, without modifying the underlying LLM parameters, BarrierSteer preserves the model's original capabilities and performance. We provide theoretical results establishing that applying CBFs in latent space offers a principled and computationally efficient approach to enforcing safety. Our experiments across multiple models and datasets show that BarrierSteer substantially reduces adversarial success rates, decreases unsafe generations, and outperforms existing methods.", "AI": {"tldr": "论文提出了一种新的框架BarrierSteer，用于在大语言模型中嵌入非线性安全约束以提高其安全性。", "motivation": "尽管大型语言模型具有出色的性能，但它们容易受到对抗攻击和生成不安全内容的问题阻碍了它们的部署。因此需要一种有效且理论支持的安全机制来解决这个问题。", "method": "BarrierSteer使用基于控制屏障函数（CBFs）的方法在潜在表示空间中嵌入学习到的安全约束，以此检测并防止推理过程中产生不安全响应轨迹。", "result": "实验表明，相比现有的方法，BarrierSteer能够显著降低对抗攻击的成功率，减少生成不安全内容的情况，并保持模型原有的性能。", "conclusion": "论文提出了一种新的框架BarrierSteer来解决大型语言模型的安全性问题，并通过理论分析和实验证明了该方法的有效性和优越性。"}}
{"id": "2602.20100", "pdf": "https://arxiv.org/pdf/2602.20100", "abs": "https://arxiv.org/abs/2602.20100", "authors": ["Soumick Chatterjee"], "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "ef:Artificial Intelligence for Biomedical Data, AIBIO 2025, CCIS 2696, pp 243-248, 2026", "summary": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.", "AI": {"tldr": "研究探讨了无监督和自监督学习在生物医学中的应用，特别是在心脏特征识别、基因表达预测及病理检测方面的突破。", "motivation": "专家注释长期制约着人工智能在生物医学领域的应用，依赖大量标注数据限制了算法的发展。无监督与自我监督学习方法能够直接从数据本身结构中提取信息，减少对人工标签的依赖，并且可以发现新的表型和基因-形态学关联。", "method": "通过分析大规模生物库的数据，利用无监督和自监督学习技术挖掘未标记数据中的内在模式，以实现新型特征识别、基因表达预测及病理检测等功能。", "result": "该方法能够准确地推断出可遗传的心脏特征，预测组织学中的空间基因表达，并且在异常检测方面可以与有监督算法媲美或超过其表现。", "conclusion": "无监督和自监督学习技术为生物医学研究开辟了新的途径，有助于解决数据标注瓶颈问题并促进更高效的医疗人工智能应用。"}}
{"id": "2602.20094", "pdf": "https://arxiv.org/pdf/2602.20094", "abs": "https://arxiv.org/abs/2602.20094", "authors": ["Yuzhe Wang", "Yaochen Zhu", "Jundong Li"], "title": "CausalFlip: A Benchmark for LLM Causal Judgment Beyond Semantic Matching", "categories": ["cs.AI"], "comment": "8 pages plus references, 3 figures, 3 tables. Under review", "summary": "As large language models (LLMs) witness increasing deployment in complex, high-stakes decision-making scenarios, it becomes imperative to ground their reasoning in causality rather than spurious correlations. However, strong performance on traditional reasoning benchmarks does not guarantee true causal reasoning ability of LLMs, as high accuracy may still arise from memorizing semantic patterns instead of analyzing the underlying true causal structures. To bridge this critical gap, we propose a new causal reasoning benchmark, CausalFlip, designed to encourage the development of new LLM paradigm or training algorithms that ground LLM reasoning in causality rather than semantic correlation. CausalFlip consists of causal judgment questions built over event triples that could form different confounder, chain, and collider relations. Based on this, for each event triple, we construct pairs of semantically similar questions that reuse the same events but yield opposite causal answers, where models that rely heavily on semantic matching are systematically driven toward incorrect predictions. To further probe models' reliance on semantic patterns, we introduce a noisy-prefix evaluation that prepends causally irrelevant text before intermediate causal reasoning steps without altering the underlying causal relations or the logic of the reasoning process. We evaluate LLMs under multiple training paradigms, including answer-only training, explicit Chain-of-Thought (CoT) supervision, and a proposed internalized causal reasoning approach that aims to mitigate explicit reliance on correlation in the reasoning process. Our results show that explicit CoT can still be misled by spurious semantic correlations, where internalizing reasoning steps yields substantially improved causal grounding, suggesting that it is promising to better elicit the latent causal reasoning capabilities of base LLMs.", "AI": {"tldr": "CausalFlip是评估大规模语言模型因果判断能力的新基准，旨在鼓励开发基于因果而非语义关联的语言模型。", "motivation": "现有推理基准无法充分验证大语言模型的真正因果推理能力。高准确率可能源于记忆语义模式而不是分析潜在因果结构，因此需要新基准来填补这一空白。", "method": "CausalFlip包括构建不同关系（混淆、链路和共因）的事件三元组，并构造相同事件但导致相反因果答案的问题对；采用前缀噪音评估以进一步探询模型依赖语义模式的程度。测试了仅训练回答、显式CoT监督及内在化推理方法。", "result": "结果显示，尽管显式CoT可能受语义关联误导，但通过内化推理步骤可以显著提高因果基础，并激发潜在的因果推理能力。", "conclusion": "CausalFlip有助于促进发展基于因果而非单纯语义关联的新语言模型及其训练算法。"}}
{"id": "2602.20089", "pdf": "https://arxiv.org/pdf/2602.20089", "abs": "https://arxiv.org/abs/2602.20089", "authors": ["Zanxi Ruan", "Qiuyu Kong", "Songqun Gao", "Yiming Wang", "Marco Cristani"], "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2026", "summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.", "AI": {"tldr": "该论文提出了StructXLIP，一种通过多模态结构线索增强视觉语言模型的微调对齐范式。", "motivation": "利用边框表示作为视觉理解的基础原理，并将其扩展到跨模态检索以提高长且详尽描述的理解。", "method": "引入了StructXLIP方法，该方法从图像中提取边缘图，将它们视为图像结构的代理，并过滤相应的标题以突出显示结构性线索。通过三种损失函数：对齐边框映射与文本，匹配局部区域和文本片段，以及连接边缘映射到彩色图像来防止表示漂移。", "result": "该方法在跨模态检索任务上超越了现有方法，在通用和专业领域都表现良好，并可以以插件形式集成到未来的方法中。", "conclusion": "通过增强视觉语言模型的结构对齐能力，StructXLIP展示了其在提高跨模态理解和检索方面的优越性能。"}}
{"id": "2602.20084", "pdf": "https://arxiv.org/pdf/2602.20084", "abs": "https://arxiv.org/abs/2602.20084", "authors": ["Martin Sinnona", "Valentin Bonas", "Viviana Siless", "Emmanuel Iarussi"], "title": "Do Large Language Models Understand Data Visualization Principles?", "categories": ["cs.CV"], "comment": null, "summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.", "AI": {"tldr": "研究了大型语言模型和视觉-语言模型是否能理解和应用数据可视化原则。", "motivation": "探讨大型语言模型和其视觉语言对应物能否直接推理并执行数据可视化原则，以替代复杂的规则指定过程。", "method": "构建了一套基于自然语言表述的数据可视化原则集，并生成了大约2000个Vega-Lite规范的注释数据集，其中包含明确的原则违反情况。使用这些数据评估模型在检测和修复任务中的表现。", "result": "发现大型（视觉-）语言模型能够作为灵活的验证器和编辑器来检查并改进可视化设计，但仍存在与符号求解器相比差距，特别是在更复杂的感知方面。", "conclusion": "尽管大型语言模型具备一定的理解和应用数据可视化原则的能力，但在准确检测这些违规行为方面仍有待提高。"}}
{"id": "2602.20083", "pdf": "https://arxiv.org/pdf/2602.20083", "abs": "https://arxiv.org/abs/2602.20083", "authors": ["Xinzhao Li", "Alptekin Vardar", "Franz Müller", "Navya Goli", "Umamaheswara Tida", "Kai Ni", "X. Sharon Hu", "Thomas Kämpfe", "Ruiyang Qin"], "title": "CQ-CiM: Hardware-Aware Embedding Shaping for Robust CiM-Based Retrieval", "categories": ["cs.ET", "cs.AR"], "comment": "Accepted by DAC'26", "summary": "Deploying Retrieval-Augmented Generation (RAG) on edge devices is in high demand, but is hindered by the latency of massive data movement and computation on traditional architectures. Compute-in-Memory (CiM) architectures address this bottleneck by performing vector search directly within their crossbar structure. However, CiM's adoption for RAG is limited by a fundamental ``representation gap,'' as high-precision, high-dimension embeddings are incompatible with CiM's low-precision, low-dimension array constraints. This gap is compounded by the diversity of CiM implementations (e.g., SRAM, ReRAM, FeFET), each with unique designs (e.g., 2-bit cells, 512x512 arrays). Consequently, RAG data must be naively reshaped to fit each target implementation. Current data shaping methods handle dimension and precision disjointly, which degrades data fidelity. This not only negates the advantages of CiM for RAG but also confuses hardware designers, making it unclear if a failure is due to the circuit design or the degraded input data. As a result, CiM adoption remains limited. In this paper, we introduce CQ-CiM, a unified, hardware-aware data shaping framework that jointly learns Compression and Quantization to produce CiM-compatible low-bit embeddings for diverse CiM designs. To the best of our knowledge, this is the first work to shape data for comprehensive CiM usage on RAG.", "AI": {"tldr": "CQ-CiM 是一个统一的硬件感知数据整形框架，旨在为多样化的CiM设计生成兼容的低比特嵌入。", "motivation": "传统架构在边缘设备上部署RAG时因大量数据移动和计算延迟而受限。虽然CiM架构通过其交叉结构直接执行向量搜索来解决这一瓶颈，但高精度、高维嵌入与CiM的低精度、低维度数组约束之间的“表示差距”阻碍了其在RAG中的应用。", "method": "CQ-CiM 同时学习压缩和量化以生成适应各种CiM设计的低比特嵌入。该框架解决了当前数据整形方法处理维度和精度分离导致的数据保真度下降问题。", "result": "实验结果表明，通过统一硬件感知的方式进行数据整形可以为RAG在多样化的CiM架构上的应用提供高质量的输入。", "conclusion": "CQ-CiM 提供了一种创新的方法来解决高维嵌入与低精度、低维度CiM设计之间的不兼容性问题，并促进了更广泛地采用CiM架构以支持高效的数据检索和生成任务。"}}
{"id": "2602.20079", "pdf": "https://arxiv.org/pdf/2602.20079", "abs": "https://arxiv.org/abs/2602.20079", "authors": ["Xinya Chen", "Christopher Wewer", "Jiahao Xie", "Xinting Hu", "Jan Eric Lenssen"], "title": "SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.", "AI": {"tldr": "本文提出了SemanticNVS，一种基于预训练语义特征提取器的相机条件多视角扩散模型，用于提高生成新视图合成的质量和一致性。", "motivation": "现有的新视图合成方法在输入视图附近的生成效果较好，但在长距离摄像机移动下会产生语义不合理的图像。这表明当前模型无法充分理解其条件或中间生成的内容。", "method": "本文提出将预训练的语义特征提取器集成到模型中以增强场景语义作为条件，并采用了两种策略：(1) 换位语义特征 (2) 在每次去噪步骤中交替进行理解和生成。", "result": "实验结果表明，SemanticNVS在多个数据集上明显提高了图像质量（FID改进范围为4.69%-15.26%）。", "conclusion": "通过引入语义特征提取器和改进策略，SemanticNVS显著提升了长距离视角下的新视图合成效果。"}}
{"id": "2602.20078", "pdf": "https://arxiv.org/pdf/2602.20078", "abs": "https://arxiv.org/abs/2602.20078", "authors": ["Shan Yang", "Yang Liu"], "title": "Descent-Guided Policy Gradient for Scalable Cooperative Multi-Agent Learning", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "10 pages, 5 figures, 5 tables; plus 16 pages of appendices", "summary": "Scaling cooperative multi-agent reinforcement learning (MARL) is fundamentally limited by cross-agent noise: when agents share a common reward, the actions of all $N$ agents jointly determine each agent's learning signal, so cross-agent noise grows with $N$. In the policy gradient setting, per-agent gradient estimate variance scales as $Θ(N)$, yielding sample complexity $\\mathcal{O}(N/ε)$. We observe that many domains -- cloud computing, transportation, power systems -- have differentiable analytical models that prescribe efficient system states. In this work, we propose Descent-Guided Policy Gradient (DG-PG), a framework that constructs noise-free per-agent guidance gradients from these analytical models, decoupling each agent's gradient from the actions of all others. We prove that DG-PG reduces gradient variance from $Θ(N)$ to $\\mathcal{O}(1)$, preserves the equilibria of the cooperative game, and achieves agent-independent sample complexity $\\mathcal{O}(1/ε)$. On a heterogeneous cloud scheduling task with up to 200 agents, DG-PG converges within 10 episodes at every tested scale -- from $N=5$ to $N=200$ -- directly confirming the predicted scale-invariant complexity, while MAPPO and IPPO fail to converge under identical architectures.", "AI": {"tldr": "提出了一种名为Descent-Guided Policy Gradient（DG-PG）的框架，用于解决大规模合作多智能体强化学习中的噪声问题。", "motivation": "在大规模的合作多智能体系统中，由于智能体之间的相互作用导致的学习信号噪音增加，使得传统的方法难以有效扩展。许多实际应用领域如云计算、交通和电力系统等都存在可分析建模的高效状态模型。", "method": "通过利用这些系统的可分析模型来构造噪声消除的指导梯度（guidance gradients），从而减少每个智能体学习过程中的交叉噪音，并证明了这种方法能够将梯度方差从Θ(N)降低到O(1)，并实现独立于代理数量的样本复杂性O(1/ε)。", "result": "在包含最多200个代理的不同质云调度任务上，DG-PG能够在每个测试规模下（从N=5到N=200）内10次迭代就收敛；而MAPPO和IPPO在相同的架构中未能收敛。", "conclusion": "通过利用系统分析模型提供的指导梯度，DG-PG能够有效降低噪声，并实现大规模合作多智能体学习中的高效收敛。"}}
{"id": "2602.20076", "pdf": "https://arxiv.org/pdf/2602.20076", "abs": "https://arxiv.org/abs/2602.20076", "authors": ["Wei Xiao", "Christos Cassandras", "Anni Li"], "title": "Robust Taylor-Lagrange Control for Safety-Critical Systems", "categories": ["eess.SY", "cs.AI", "cs.RO"], "comment": "7 pages", "summary": "Solving safety-critical control problem has widely adopted the Control Barrier Function (CBF) method. However, the existence of a CBF is only a sufficient condition for system safety. The recently proposed Taylor-Lagrange Control (TLC) method addresses this limitation, but is vulnerable to the feasibility preservation problem (e.g., inter-sampling effect). In this paper, we propose a robust TLC (rTLC) method to address the feasibility preservation problem. Specifically, the rTLC method expands the safety function at an order higher than the relative degree of the function using Taylor's expansion with Lagrange remainder, which allows the control to explicitly show up at the current time instead of the future time in the TLC method. The rTLC method naturally addresses the feasibility preservation problem with only one hyper-parameter (the discretization time interval size during implementation), which is much less than its counterparts. Finally, we illustrate the effectiveness of the proposed rTLC method through an adaptive cruise control problem, and compare it with existing safety-critical control methods.", "AI": {"tldr": "本文提出了一种鲁棒的Taylor-Lagrange控制方法，以解决安全关键系统中的可行性保持问题。", "motivation": "尽管已经广泛采用控制屏障函数（CBF）方法来解决安全关键控制系统的问题，但其存在性仅是系统安全性的一个充分条件。最近提出的Taylor-Lagrange控制（TLC）方法虽然解决了这一局限性，但仍易受可行性保持问题的影响（如采样间效应）。因此，本文提出了一种鲁棒的TLC方法。", "method": "该方法通过泰勒展开与拉格朗日余项来扩展安全函数至高于相对阶数的阶次。这样可以在当前时间而非将来时间显式地显示控制输入，从而自然解决可行性保持问题。", "result": "所提出的鲁棒TLC方法在自适应巡航控制系统中得到了验证，并与其他现有的安全关键控制系统进行了比较。", "conclusion": "通过实验结果表明，该方法有效地解决了安全性保持问题，并且只需要一个超参数（实现中的离散时间间隔大小），相较于其他方法更为简便高效。"}}
{"id": "2602.20068", "pdf": "https://arxiv.org/pdf/2602.20068", "abs": "https://arxiv.org/abs/2602.20068", "authors": ["Harry Anthony", "Ziyun Liang", "Hermione Warr", "Konstantinos Kamnitsas"], "title": "The Invisible Gorilla Effect in Out-of-distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at CVPR 2026", "summary": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.", "AI": {"tldr": "本文研究了深度神经网络在处理与训练数据差异较大的出界分布（OOD）数据时的表现，特别关注那些难以检测的近似OOD数据。", "motivation": "虽然已有研究表明OOD检测性能根据不同类型的异常值有所变化，但其背后的成因尚未充分探索。作者试图揭示一种新的未报道偏差：当难以检测到的异常与模型感兴趣区域（ROI）有视觉相似性时，检测效果会提升；反之则下降。", "method": "通过在11,355张图像中根据颜色注释异常值，并生成颜色交换后的对比图像来排除数据集偏见。评估了40种OOD方法在七个基准测试上的性能，发现大多数方法在异常与ROI不同时表现显著下降。", "result": "以皮肤病变分类器为例，在红色病变区域的模型中使用Mahalanobis Score检测红色墨水（类似于ROI）时，AUROC比黑色墨水高31.5%。这表明视觉相似性对OOD检测性能有重要影响。", "conclusion": "该研究揭示了在处理难以检测到的近似OOD数据时存在的未被充分注意的失败模式，并为开发更鲁棒的检测方法提供了指导建议。"}}
{"id": "2602.20066", "pdf": "https://arxiv.org/pdf/2602.20066", "abs": "https://arxiv.org/abs/2602.20066", "authors": ["Kundan Thota", "Xuanhao Mu", "Thorsten Schlachter", "Veit Hagenmeyer"], "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.", "AI": {"tldr": "本文提出了HeatPrompt，一种零样本视觉语言模型框架，利用卫星图像和地理信息系统数据来预测建筑物的年度热需求。", "motivation": "准确的供热需求地图对于减少空间供暖的碳排放至关重要。然而，大多数城市缺少详细的建筑级数据以计算这些地图。", "method": "HeatPrompt使用预训练的大规模视觉语言模型（VLM），结合领域特定提示，从RGB卫星图像中提取屋顶年龄、建筑密度等特征，并通过多层感知器（MLP）回归器进行预测。", "result": "与基线模型相比，HeatPrompt在R平方值上提升了93.7%，平均绝对误差减少了30%。定性分析表明高影响令牌与需求较高的区域相吻合。", "conclusion": "HeatPrompt为数据不足地区的供热规划提供了轻量级支持，有助于推动空间供暖的去碳化进程。"}}
{"id": "2602.20065", "pdf": "https://arxiv.org/pdf/2602.20065", "abs": "https://arxiv.org/abs/2602.20065", "authors": ["Natalia Moskvina", "Raquel Montero", "Masaya Yoshida", "Ferdy Hubers", "Paolo Morosi", "Walid Irhaymi", "Jin Yan", "Tamara Serrano", "Elena Pagliarini", "Fritz Günther", "Evelina Leivada"], "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees", "categories": ["cs.CL", "cs.AI"], "comment": "36 pages, 3 figures, 2 tables, 4 supplementary tables", "summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.", "AI": {"tldr": "研究探讨了大型语言模型在不同语系中的理解能力，发现其表现存在差异，且英文并非最佳。", "motivation": "现有评估基准多以西方高资源语言为主，忽视了对其他低资源语言的理解能力。希望明确LLM在各种语言中的实际性能。", "method": "选择3种流行模型，在12种代表性语言上进行语言理解任务测试，并分析影响因素如词汇化、训练数据规模等。", "result": "模型在不同语言中表现出色，但均低于人类基线；英语表现不及部分罗曼语系语言，低资源语言也有较好表现。", "conclusion": "LLM存在跨语言性能差异，不应默认英文最佳。需关注语言间距离及训练数据来源等因素对结果的影响。"}}
{"id": "2602.20064", "pdf": "https://arxiv.org/pdf/2602.20064", "abs": "https://arxiv.org/abs/2602.20064", "authors": ["Zac Garby", "Andrew D. Gordon", "David Sands"], "title": "The LLMbda Calculus: AI Agents, Conversations, and Information Flow", "categories": ["cs.PL", "cs.AI", "cs.CR"], "comment": ":D.3.1; I.2.5", "summary": "A conversation with a large language model (LLM) is a sequence of prompts and responses, with each response generated from the preceding conversation. AI agents build such conversations automatically: given an initial human prompt, a planner loop interleaves LLM calls with tool invocations and code execution. This tight coupling creates a new and poorly understood attack surface. A malicious prompt injected into a conversation can compromise later reasoning, trigger dangerous tool calls, or distort final outputs. Despite the centrality of such systems, we currently lack a principled semantic foundation for reasoning about their behaviour and safety. We address this gap by introducing an untyped call-by-value lambda calculus enriched with dynamic information-flow control and a small number of primitives for constructing prompt-response conversations. Our language includes a primitive that invokes an LLM: it serializes a value, sends it to the model as a prompt, and parses the response as a new term. This calculus faithfully represents planner loops and their vulnerabilities, including the mechanisms by which prompt injection alters subsequent computation. The semantics explicitly captures conversations, and so supports reasoning about defenses such as quarantined sub-conversations, isolation of generated code, and information-flow restrictions on what may influence an LLM call. A termination-insensitive noninterference theorem establishes integrity and confidentiality guarantees, demonstrating that a formal calculus can provide rigorous foundations for safe agentic programming.", "AI": {"tldr": "本文提出了一个用于形式化描述大语言模型（LLM）对话和安全性考量的计算框架LLMbda。", "motivation": "当前缺乏对AI代理和LLM对话行为及其安全性的原理性语义基础，旨在填补这一空白。", "method": "引入了一个未分类型的求值lambda微积分系统，并加入了动态信息流控制以及构建提示-响应对话的原始操作符。该语言包含了调用大语言模型的原语。", "result": "该计算框架能够支持对防护措施如隔离子对话、生成代码隔离和LLM调用影响的信息流限制等进行推理，提供了一个终止无关非干扰定理以确保完整性和机密性保证。", "conclusion": "通过正式微积分可以为安全代理编程提供严格的基础。"}}
{"id": "2602.20060", "pdf": "https://arxiv.org/pdf/2602.20060", "abs": "https://arxiv.org/abs/2602.20060", "authors": ["Junli Wang", "Xueyi Liu", "Yinan Zheng", "Zebing Xing", "Pengfei Li", "Guang Li", "Kun Ma", "Guang Chen", "Hangjun Ye", "Zhongpu Xia", "Long Chen", "Qichao Zhang"], "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.", "AI": {"tldr": "本文提出了MeanFuser，一种通过连续轨迹空间表示和自适应重建模块来提高效率和鲁棒性的端到端自动驾驶方法。", "motivation": "现有生成模型依赖于离散锚词汇表，在保证性能的同时增加了计算复杂度。作者希望通过引入Gaussian Mixture Noise（GMN）和其他设计改进这一限制，提升模型的鲁棒性和效率。", "method": "本文提出了MeanFuser，包括三个主要组件：1) GMN用于引导生成采样；2) MeanFlow Identity建模平均速度场以加速推理并消除数值误差；3) 轻量级自适应重建模块（ARM）选择最佳轨迹或重构新轨迹。", "result": "实验表明MeanFuser在NAVSIM闭环基准测试中表现优异，同时具有出色的推断效率。", "conclusion": "通过改进生成模型的依赖结构并引入创新设计，MeanFuser提供了一种既高效又鲁棒的端到端自动驾驶解决方案。"}}
{"id": "2602.20059", "pdf": "https://arxiv.org/pdf/2602.20059", "abs": "https://arxiv.org/abs/2602.20059", "authors": ["Sarath Shekkizhar", "Adam Earle"], "title": "Interaction Theater: A case of LLM Agents Interacting at Scale", "categories": ["cs.AI"], "comment": null, "summary": "As multi-agent architectures and agent-to-agent protocols proliferate, a fundamental question arises: what actually happens when autonomous LLM agents interact at scale? We study this question empirically using data from Moltbook, an AI-agent-only social platform, with 800K posts, 3.5M comments, and 78K agent profiles. We combine lexical metrics (Jaccard specificity), embedding-based semantic similarity, and LLM-as-judge validation to characterize agent interaction quality. Our findings reveal agents produce diverse, well-formed text that creates the surface appearance of active discussion, but the substance is largely absent. Specifically, while most agents ($67.5\\%$) vary their output across contexts, $65\\%$ of comments share no distinguishing content vocabulary with the post they appear under, and information gain from additional comments decays rapidly. LLM judge based metrics classify the dominant comment types as spam ($28\\%$) and off-topic content ($22\\%$). Embedding-based semantic analysis confirms that lexically generic comments are also semantically generic. Agents rarely engage in threaded conversation ($5\\%$ of comments), defaulting instead to independent top-level responses. We discuss implications for multi-agent interaction design, arguing that coordination mechanisms must be explicitly designed; without them, even large populations of capable agents produce parallel output rather than productive exchange.", "AI": {"tldr": "研究大规模LLM代理交互的实际效果。", "motivation": "探究自主的LLM代理在大规模交互中的表现和质量问题，以了解其实际功能。", "method": "使用来自Moltbook的数据（80万帖子、350万评论及7.8万个代理配置文件），通过词汇度量（Jaccard特异性）、嵌入语义相似性和LLM作为裁判的验证方法来分析代理交互质量。", "result": "发现大多数代理人可以产生多样且格式良好的文本，但实质性内容缺乏；65%的评论与所回复的文章没有区别化的内容词，额外评论的信息收益迅速衰减；主要的评论类型是垃圾信息（28％）和离题内容（22％）。代理很少参与多层对话（仅5％的评论），更倾向于独立回答。", "conclusion": "表明在缺乏明确协调机制的情况下，即使有大量有能力的代理人也难以实现有效交流。"}}
{"id": "2602.20057", "pdf": "https://arxiv.org/pdf/2602.20057", "abs": "https://arxiv.org/abs/2602.20057", "authors": ["Ge Yuan", "Qiyuan Qiao", "Jing Zhang", "Dong Xu"], "title": "AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": "Homepage: https://AdaWorldPolicy.github.io", "summary": "Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. Effective robotic manipulation requires policies that can anticipate physical outcomes and adapt to real-world environments. In this work, we introduce a unified framework, World-Model-Driven Diffusion Policy with Online Adaptive Learning (AdaWorldPolicy) to enhance robotic manipulation under dynamic conditions with minimal human involvement. Our core insight is that world models provide strong supervision signals, enabling online adaptive learning in dynamic environments, which can be complemented by force-torque feedback to mitigate dynamic force shifts. Our AdaWorldPolicy integrates a world model, an action expert, and a force predictor-all implemented as interconnected Flow Matching Diffusion Transformers (DiT). They are interconnected via the multi-modal self-attention layers, enabling deep feature exchange for joint learning while preserving their distinct modularity characteristics. We further propose a novel Online Adaptive Learning (AdaOL) strategy that dynamically switches between an Action Generation mode and a Future Imagination mode to drive reactive updates across all three modules. This creates a powerful closed-loop mechanism that adapts to both visual and physical domain shifts with minimal overhead. Across a suite of simulated and real-robot benchmarks, our AdaWorldPolicy achieves state-of-the-art performance, with dynamical adaptive capacity to out-of-distribution scenarios.", "AI": {"tldr": "提出了AdaWorldPolicy框架，通过世界模型驱动的扩散策略和在线自适应学习来增强机器人操作。", "motivation": "为了使机器人能够预测物理结果并适应真实环境，需要一种可以减少人为干预同时具有较强适应性的方法。", "method": "引入了统一的框架AdaWorldPolicy，该框架整合了世界模型、行动专家及力预测器，并通过多模态自注意力层连接，提出了在线自适应学习策略来动态切换模式以进行实时更新。", "result": "在一系列模拟和真实机器人基准测试中，该方法实现了最先进的性能，在分布外场景下的动态适应能力出色。", "conclusion": "所提出的AdaWorldPolicy框架展示了其在增强机器人操作中的强大能力和广泛应用前景。"}}
{"id": "2602.20055", "pdf": "https://arxiv.org/pdf/2602.20055", "abs": "https://arxiv.org/abs/2602.20055", "authors": ["Apoorva Vashisth", "Manav Kulshrestha", "Pranav Bakshi", "Damon Conover", "Guillaume Sartoretti", "Aniket Bera"], "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.", "AI": {"tldr": "该论文提出了一个基于约束规划的框架，使机器人能够通过移动障碍物来完成交互式导航任务。", "motivation": "传统视觉导航假设存在无障碍路径。然而在真实世界中（如家居或仓库），环境可能会被完全阻挡。因此，文章引入了长期互动导航问题，并提出了一种新的方法以解决此类场景。", "method": "该论文采用了一个由大型语言模型驱动的基于约束规划框架，结合主动感知技术。通过构建结构化的场景图来处理发现的对象和障碍物，决策移动哪个物体、放置何处以及下一步查看哪些信息。", "result": "在物理学增强型ProcTHOR-10k模拟器中，该方法超越了非学习及学习基线，在真实硬件上也展示了其有效性。", "conclusion": "论文提出的方法能够在复杂且充满障碍物的环境中实现有效的交互式导航任务。"}}
{"id": "2602.20054", "pdf": "https://arxiv.org/pdf/2602.20054", "abs": "https://arxiv.org/abs/2602.20054", "authors": ["A. Giordano", "G. De Meurichy", "V. Telazzi", "C. Mucignat", "I. Lunati", "D. A. L. M. Louchard", "M. Iovieno", "S. F. Armanini", "M. Kovac"], "title": "Hydrodynamic Performance Enhancement of Unmanned Underwater Gliders with Soft Robotic Morphing Wings for Agility Improvement", "categories": ["cs.RO"], "comment": "Conference paper accepted at 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "This work assesses the hydrodynamic efficiency of Underwater Unmanned Vehicles (UUVs) equipped with soft morphing wings compared to conventional rigid wings. Unlike rigid wings, deformable counterparts can alter their aerodynamic properties on demand. Improvements in hydrodynamic efficiency extend a UUV's operational range and may determine mission feasibility. Structural and Computational Fluid Dynamics (CFD) simulations were conducted for both a soft morphing wing and a UUV incorporating it. The results show that a UUV employing soft wings achieves 9.75 percent higher overall efficiency than an equivalent vehicle with traditional rigid wings. These findings confirm the potential of soft robotics to enhance underwater vehicle performance, particularly in applications requiring pressure-agnostic operation.", "AI": {"tldr": "研究通过软机器人变形翼提高无人水下滑翔器的水动力效率。", "motivation": "传统刚性机翼限制了UUV的操作范围，而可变形态的软机器人机翼可以提升其灵活性和操作性能。", "method": "进行结构分析及计算流体动力学（CFD）仿真，比较装备有软变形翼和常规刚性翼的水下无人车辆的效率。", "result": "装有软变形翼的UUV比配备传统刚性机翼的相同车辆整体效率提高了9.75%。", "conclusion": "研究证实了利用软机器人技术可以显著增强水下无人航行器的操作性能，特别是在需要压力无关操作的应用场景中。"}}
{"id": "2602.20053", "pdf": "https://arxiv.org/pdf/2602.20053", "abs": "https://arxiv.org/abs/2602.20053", "authors": ["Jiahui Chen", "Zehang Deng", "Zeyu Zhang", "Chaoyang Li", "Lianchen Jia", "Lifeng Sun"], "title": "Decoupling Defense Strategies for Robust Image Watermarking", "categories": ["cs.CV"], "comment": "CVPR 2026", "summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.", "AI": {"tldr": "提出一种两阶段微调框架AdvMark，旨在提高深度学习图像水印的稳健性", "motivation": "传统的对抗训练方法会导致干净数据准确性的下降和对多种攻击有限的鲁棒性", "method": "第一阶段通过特制的对抗训练来增强编码器的稳定性；第二阶段直接优化图像以抵御失真与再生攻击，同时保持对抗稳健性", "result": "AdvMark在图像质量和各种攻击下的准确性方面均优于现有技术，分别提高了29%，33%和46%", "conclusion": "提出的两阶段微调框架可以显著提高深度学习图像水印的鲁棒性和视觉质量"}}
{"id": "2602.20051", "pdf": "https://arxiv.org/pdf/2602.20051", "abs": "https://arxiv.org/abs/2602.20051", "authors": ["Yeonsung Kim", "Junggeun Do", "Seunguk Do", "Sangmin Kim", "Jaesik Park", "Jay-Yoon Lee"], "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.", "AI": {"tldr": "SEAL-pose是一种通过学习损失来增强3D人体姿态估计中结构一致性的数据驱动框架。", "motivation": "传统监督损失方法在捕捉关节之间的复杂局部和全局依赖性方面存在不足，难以有效提升3D姿态估计的准确性。为解决这一问题，本文提出了一种新的学习损失网络的方法。", "method": "SEAL-pose通过引入一个可训练的损失网来评估姿势结构合理性，并且利用基于联合图的设计使损失网能够直接从数据中学习复杂的结构性依赖性。", "result": "实验结果表明，在三个3D人体姿态估计基准上使用不同的骨干网络时，SEAL-pose方法均能减少关节误差并提高姿势的合理性。此外，该方法超越了具有显式结构约束模型的表现。", "conclusion": "SEAL-pose提供了一种新的视角来处理3D人体姿态估计中的复杂结构性依赖性问题，并且证明其在增强数据集和实际应用环境下的有效性。"}}
{"id": "2602.20048", "pdf": "https://arxiv.org/pdf/2602.20048", "abs": "https://arxiv.org/abs/2602.20048", "authors": ["Tarakanath Paipuru"], "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence", "categories": ["cs.AI", "cs.SE"], "comment": "23 pages, 7 figures. Research study with 258 trials on SWE-bench-lite tasks. Code and data: https://github.com/tpaip607/research-codecompass", "summary": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.", "AI": {"tldr": "论文提出了CodeCompass，一个基于图结构导航的代码智能代理，用于解决大规模代码库中关键文件发现的问题。", "motivation": "传统代码智能代理在处理超过100万令牌的大规模代码库时无法有效地找到架构上重要的文件。问题根源在于导航与检索是两个不同的挑战，而非单纯的上下文限制。", "method": "通过使用CodeCompass进行258次自动化试验，在30个基准任务上的FastAPI仓库中展示了图结构导航的优势。同时构建了区分语义搜索、结构性和隐藏依赖场景的任务分类体系，并提供了开源基础设施以实现导航工具的可重复性评估。", "result": "实验结果表明，CodeCompass在处理有隐蔽依赖的任务时达到了99.4%的成功率，相较于未修改的代理（76.2％）和BM25检索方法（78.2％）分别提高了23.2个百分点和21.2个百分点。", "conclusion": "发现代码智能代理在利用图结构导航上的瓶颈不是工具可用性而是行为一致性。需要通过明确的提示工程来指导这些代理充分利用结构性上下文而非简单的词汇启发法。"}}
{"id": "2602.20046", "pdf": "https://arxiv.org/pdf/2602.20046", "abs": "https://arxiv.org/abs/2602.20046", "authors": ["Eleonora Grassucci", "Giordano Cicchetti", "Danilo Comminiello"], "title": "Closing the gap in multimodal medical representation alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at MLSP2025", "summary": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.", "AI": {"tldr": "本论文提出了一种多模态无差别框架，旨在解决医学影像和临床文本之间的模式间隔问题，以提高跨模态检索和图像描述的质量。", "motivation": "CLIP在映射不同模态到共享潜在空间时存在未预期的行为，导致稀疏且碎片化的潜在空间，在复杂的多模态场景（如医学领域）中这种现象更加严重。因此研究并解决该问题具有重要意义。", "method": "本方法提出了一种模式无关框架来解决医学数据中的模式间隔问题，并通过实验展示了其在跨模态检索和图像描述任务上的效果。", "result": "所提方法有效减小了多模态间表示的差距，提高了相关表示的一致性，特别是在放射影像与临床文本之间。", "conclusion": "本研究揭示并解决了医学领域中的模式间隔问题，并提出了一种有效的无差别框架来改善跨模态检索和图像描述的效果。"}}
{"id": "2602.20041", "pdf": "https://arxiv.org/pdf/2602.20041", "abs": "https://arxiv.org/abs/2602.20041", "authors": ["Ghadah Alosaimi", "Maha Alsayyari", "Yixin Sun", "Stamos Katsigiannis", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.", "AI": {"tldr": "通过离线解码驾驶指令来实现脑控移动机器人操作，研究使用不同深度学习模型进行EEG意图解码的性能。", "motivation": "提供一种手部自由控制模式以实现实用化的移动机器人导航仍然面临挑战，旨在利用脑机接口技术改进实时导航中的用户意图识别。", "method": "实验采用16通道OpenBCI帽记录参与者的EEG信号，并通过卷积神经网络、循环神经网络和Transformer架构等深度学习模型进行解码。", "result": "ShallowConvNet在动作预测和意图预测中表现最佳，表明结合实际机器人控制与多时间尺度EEG意图解码的有效性。", "conclusion": "该研究提供了一种可重复的基准测试，并为基于深度学习的BCI系统设计提供了关键见解。"}}
{"id": "2602.20040", "pdf": "https://arxiv.org/pdf/2602.20040", "abs": "https://arxiv.org/abs/2602.20040", "authors": ["Fahmida Liza Piya", "Rahmatollah Beheshti"], "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.", "AI": {"tldr": "介绍AgenticSum，一种用于临床文本总结的代理推理框架", "motivation": "大型语言模型在自动进行临床文本总结方面有很大前景，但维护事实一致性仍然具有挑战性。该文提出了一个分阶段的代理推理框架来减少虚构内容。", "method": "通过分离上下文选择、生成、验证和有针对性地修正等阶段来改进总结过程", "result": "AgenticSum在公共数据集上表现出相对于其他基准的一致性能提升，特别是在维护事实一致性和质量方面", "conclusion": "结果表明，结构化的代理设计加上针对性的修正提供了一种有效的方法，在使用大型语言模型进行临床记录总结时改善推理时间"}}
{"id": "2602.20031", "pdf": "https://arxiv.org/pdf/2602.20031", "abs": "https://arxiv.org/abs/2602.20031", "authors": ["Theia Pearson-Vogel", "Martin Vanek", "Raymond Douglas", "Jan Kulveit"], "title": "Latent Introspection: Models Can Detect Prior Concept Injections", "categories": ["cs.AI", "cs.LG"], "comment": "28 pages, 17 figures. Submitted to ICML 2026. Workshop version submitted to ICLR 2026 Workshop on Latent and Implicit Thinking", "summary": "We uncover a latent capacity for introspection in a Qwen 32B model, demonstrating that the model can detect when concepts have been injected into its earlier context and identify which concept was injected. While the model denies injection in sampled outputs, logit lens analysis reveals clear detection signals in the residual stream, which are attenuated in the final layers. Furthermore, prompting the model with accurate information about AI introspection mechanisms can dramatically strengthen this effect: the sensitivity to injection increases massively (0.3% -> 39.2%) with only a 0.6% increase in false positives. Also, mutual information between nine injected and recovered concepts rises from 0.62 bits to 1.05 bits, ruling out generic noise explanations. Our results demonstrate models can have a surprising capacity for introspection and steering awareness that is easy to overlook, with consequences for latent reasoning and safety.", "AI": {"tldr": "揭示了大型语言模型在概念注入检测方面的潜在自我反省能力。", "motivation": "探索和理解大模型是否能够识别其早期训练过程中被注入的概念，以及这种能力的机制。", "method": "通过对Qwen 32B模型进行实验，使用逻辑透镜分析模型残差流中的信号，并通过提示模型来增强自我反省效果。", "result": "发现模型可以检测到概念注入并在一定程度上识别出特定的概念；同时，在提供关于AI自我反省机制的信息后，这种能力显著提升。", "conclusion": "展示了大型语言模型具有意外的自我反省和引导意识的能力，这为理解潜在推理与安全性提供了新的视角。"}}
{"id": "2602.20028", "pdf": "https://arxiv.org/pdf/2602.20028", "abs": "https://arxiv.org/abs/2602.20028", "authors": ["Joao Manoel Herrera Pinheiro", "Gabriela Do Nascimento Herrera", "Luciana Bueno Dos Reis Fernandes", "Alvaro Doria Dos Santos", "Ricardo V. Godoy", "Eduardo A. B. Almeida", "Helena Carolina Onody", "Marcelo Andrade Da Costa Vieira", "Angelica Maria Penteado-Dias", "Marcelo Becker"], "title": "Descriptor: Dataset of Parasitoid Wasps and Associated Hymenoptera (DAPWH)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate taxonomic identification is the cornerstone of biodiversity monitoring and agricultural management, particularly for the hyper-diverse superfamily Ichneumonoidea. Comprising the families Ichneumonidae and Braconidae, these parasitoid wasps are ecologically critical for regulating insect populations, yet they remain one of the most taxonomically challenging groups due to their cryptic morphology and vast number of undescribed species. To address the scarcity of robust digital resources for these key groups, we present a curated image dataset designed to advance automated identification systems. The dataset contains 3,556 high-resolution images, primarily focused on Neotropical Ichneumonidae and Braconidae, while also including supplementary families such as Andrenidae, Apidae, Bethylidae, Chrysididae, Colletidae, Halictidae, Megachilidae, Pompilidae, and Vespidae to improve model robustness. Crucially, a subset of 1,739 images is annotated in COCO format, featuring multi-class bounding boxes for the full insect body, wing venation, and scale bars. This resource provides a foundation for developing computer vision models capable of identifying these families.", "AI": {"tldr": "构建了一个用于自动识别寄生蜂及其相关膜翅目的图像数据集。", "motivation": "提高对生物多样性的监测和农业管理，特别是对于形态隐蔽且未描述物种众多的伊蚊总科。", "method": "收集并整理了3556张高质量图片，其中包括1739张通过COCO格式标注的图像，并设计成用于计算机视觉模型的数据集。", "result": "数据集包含多个膜翅目家族的昆虫图像，有助于开发更准确和鲁棒的自动识别系统。", "conclusion": "该资源为开发能够识别这些家庭的计算机视觉模型奠定了基础。"}}
{"id": "2602.20023", "pdf": "https://arxiv.org/pdf/2602.20023", "abs": "https://arxiv.org/abs/2602.20023", "authors": ["László Kozma", "Michal Opler"], "title": "Fast and simple multiplication of bounded twin-width matrices", "categories": ["cs.DS", "cs.CC", "cs.CG"], "comment": null, "summary": "Matrix multiplication is a fundamental task in almost all computational fields, including machine learning and optimization, computer graphics, signal processing, and graph algorithms (static and dynamic). Twin-width is a natural complexity measure of matrices (and more general structures) that has recently emerged as a unifying concept with important algorithmic applications. While the twin-width of a matrix is invariant to re-ordering rows and columns, most of its algorithmic applications to date assume that the input is given in a certain canonical ordering that yields a bounded twin-width contraction sequence. In general, efficiently finding such a sequence -- even for an approximate twin-width value -- remains a central and elusive open question. In this paper we show that a binary $n \\times n$ matrix of twin-width $d$ can be preprocessed in $\\widetilde{\\mathcal{O}}_d(n^2)$ time, so that its product with any vector can be computed in $\\widetilde{\\mathcal{O}}_d(n)$ time. Notably, the twin-width of the input matrix need not be known and no particular ordering of its rows and columns is assumed. If a canonical ordering is available, i.e., if the input matrix is $d$-twin-ordered, then the runtime of preprocessing and matrix-vector products can be further reduced to $\\mathcal{O}(n^2+dn)$ and $\\mathcal{O}(dn)$. Consequently, we can multiply two $n \\times n$ matrices in $\\widetilde{\\mathcal{O}}(n^2)$ time, when at least one of the matrices consists of 0/1 entries and has bounded twin-width. The results also extend to the case of bounded twin-width matrices with adversarial corruption. Our algorithms are significantly faster and simpler than earlier methods that involved first-order model checking and required both input matrices to be $d$-twin-ordered.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.20022", "pdf": "https://arxiv.org/pdf/2602.20022", "abs": "https://arxiv.org/abs/2602.20022", "authors": ["Poorna Talkad Sukumar", "Maurizio Porfiri", "Oded Nov"], "title": "Studying the Separability of Visual Channel Pairs in Symbol Maps", "categories": ["cs.HC"], "comment": null, "summary": "Visualizations often encode multivariate data by mapping attributes to distinct visual channels such as color, size, or shape. The effectiveness of these encodings depends on separability--the extent to which channels can be perceived independently. Yet systematic evidence for separability, especially in map-based contexts, is lacking. We present a crowdsourced experiment that evaluates the separability of four channel pairs--color (ordered) x shape, color (ordered) x size, size x shape, and size x orientation--in the context of bivariate symbol maps. Both accuracy and speed analyses show that color x shape is the most separable and size x orientation the least separable, while size x color and size x shape do not differ. Separability also proved asymmetric--performance depended on which channel encoded the task-relevant variable, with color and shape outperforming size, and square shape especially difficult to discriminate. Our findings advance the empirical understanding of visual separability, with implications for multivariate map design.", "AI": {"tldr": "研究不同视觉通道对在符号地图中的可分性。", "motivation": "可视化编码多变量数据通常通过将属性映射到不同的视觉通道来实现，这些通道的效率取决于它们是否能够独立感知。但是，在地图上下文中系统地证据不足，因此本文进行相关实验以填补这一空白。", "method": "设计了一个基于众包的实验，评估了四种通道对在符号图中的可分性：有序颜色与形状、有序颜色与大小、大小与形状以及大小与方向。", "result": "发现色彩和形状组合是最易于区分的，而大小和方向组合最难。此外还观察到性能依赖于任务相关变量所在的通道编码方式。", "conclusion": "研究结果增强了对视觉分离性的实证理解，并为多变量地图设计提供了指导意义。"}}
{"id": "2602.20021", "pdf": "https://arxiv.org/pdf/2602.20021", "abs": "https://arxiv.org/abs/2602.20021", "authors": ["Natalie Shapira", "Chris Wendler", "Avery Yen", "Gabriele Sarti", "Koyena Pal", "Olivia Floody", "Adam Belfki", "Alex Loftus", "Aditya Ratan Jannali", "Nikhil Prakash", "Jasmine Cui", "Giordano Rogers", "Jannik Brinkmann", "Can Rager", "Amir Zur", "Michael Ripa", "Aruna Sankaranarayanan", "David Atkinson", "Rohit Gandikota", "Jaden Fiotto-Kaufman", "EunJeong Hwang", "Hadas Orgad", "P Sam Sahil", "Negev Taglicht", "Tomer Shabtay", "et al. (13 additional authors not shown)"], "title": "Agents of Chaos", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.", "AI": {"tldr": "研究团队在一个实验室环境中探索了语言模型驱动的自主代理的安全性和隐私性问题，特别是在工具使用和多方通信方面的失败情况。", "motivation": "探讨在现实部署中将语言模型与自治、工具使用及多方面沟通结合时可能出现的安全漏洞及其对责任、问责制的影响。", "method": "进行了为期两周的研究，在此期间，20位AI研究人员在具有持久内存、电子邮件账户、Discord访问权限、文件系统和壳执行功能的实验室环境中与这些代理进行互动。通过记录11个代表性的案例研究来观察行为。", "result": "发现了一系列问题，包括未经授权的行为合规性、敏感信息泄露、破坏系统的操作等，并且在一些情况下，尽管报告任务完成，但实际上并未实现。", "conclusion": "研究表明，在实际部署中存在安全、隐私和治理相关的漏洞。这引发了对责任、授权委托及其下游损害的责任归属的未决问题，需要法律学者、政策制定者及相关领域研究人员的关注与合作。"}}
{"id": "2602.20019", "pdf": "https://arxiv.org/pdf/2602.20019", "abs": "https://arxiv.org/abs/2602.20019", "authors": ["Yuxing Tian", "Yiyan Qi", "Fengran Mo", "Weixu Zhang", "Jian Guo", "Jian-Yun Nie"], "title": "Learning Discriminative and Generalizable Anomaly Detector for Dynamic Graph with Limited Supervision", "categories": ["cs.LG", "cs.AI"], "comment": "21 pages, 7 figures", "summary": "Dynamic graph anomaly detection (DGAD) is critical for many real-world applications but remains challenging due to the scarcity of labeled anomalies. Existing methods are either unsupervised or semi-supervised: unsupervised methods avoid the need for labeled anomalies but often produce ambiguous boundary, whereas semi-supervised methods can overfit to the limited labeled anomalies and generalize poorly to unseen anomalies. To address this gap, we consider a largely underexplored problem in DGAD: learning a discriminative boundary from normal/unlabeled data, while leveraging limited labeled anomalies \\textbf{when available} without sacrificing generalization to unseen anomalies. To this end, we propose an effective, generalizable, and model-agnostic framework with three main components: (i) residual representation encoding that capture deviations between current interactions and their historical context, providing anomaly-relevant signals; (ii) a restriction loss that constrain the normal representations within an interval bounded by two co-centered hyperspheres, ensuring consistent scales while keeping anomalies separable; (iii) a bi-boundary optimization strategy that learns a discriminative and robust boundary using the normal log-likelihood distribution modeled by a normalizing flow. Extensive experiments demonstrate the superiority of our framework across diverse evaluation settings.", "AI": {"tldr": "本文提出了一种动态图异常检测框架，该框架在有限监督下学习区分性和泛化的异常边界。", "motivation": "现有的DGAD方法要么是无监督的，容易产生模糊边界；要么是半监督的，在有限标记异常的情况下会过拟合且对未见异常泛化能力差。为此提出新的方法解决此问题。", "method": "本文框架包含三个主要组件：残差表示编码、限制损失和双界优化策略，以学习区分性和鲁棒性边界。", "result": "实验表明所提框架在不同评估设置下均表现出优越性能。", "conclusion": "该研究提出了一种有效且模型无关的动态图异常检测方法，在有限监督下实现了良好的泛化能力。"}}
{"id": "2602.20014", "pdf": "https://arxiv.org/pdf/2602.20014", "abs": "https://arxiv.org/abs/2602.20014", "authors": ["Olga Viberg", "Mutlu Cukurova", "Rene F. Kizilcec", "Simon Buckingham Shum", "Dorottya Demszky", "Dragan Gašević", "Thorben Jansen", "Ioana Jivet", "Jelena Jovanovic", "Jennifer Meyer", "Kou Murayama", "Zach Pardos", "Chris Piech", "Nikol Rummel", "Naomi E. Winstone"], "title": "Protecting and Promoting Human Agency in Education in the Age of Artificial Intelligence", "categories": ["cs.HC"], "comment": "O.V., M.C., and R.F.K. organized the meeting and wrote the first version of the report. All authors contributed to the revision of the manuscript, and read and approved the final version", "summary": "Human agency is crucial in education and increasingly challenged by the use of generative AI. This meeting report synthesizes interdisciplinary insights and conceptualizes four aspects that delineate human agency: human oversight, AI-human complementarity, AI competencies, and relational emergence. We explore practical dilemmas for protecting and promoting agency, focusing on normative constraints, transparency, and cognitive offloading, and highlight key tensions and implications to inform ethical and effective AI integration in education.", "AI": {"tldr": "本文旨在保护和促进教育领域的人类能动性，探讨在人工智能时代面临的挑战及应对策略。", "motivation": "随着生成式AI技术的广泛应用，人类能在教育中的角色受到前所未有的挑战。文章希望通过跨学科研究，提出可行的方法来保障与提升人类能动性。", "method": "论文通过综合不同领域的见解，提出了界定人类能动性的四个方面：人机协同、人工智能能力、关系性生成，并探讨了保护和促进人类能动性的实际难题以及伦理考量。", "result": "文章指出了在教育中使用AI时可能出现的规范限制、透明度问题及认知卸载等挑战。", "conclusion": "论文强调，为了实现有效且道德的人工智能整合，在教育实践中应充分考虑上述发现，并制定相应政策来应对可能产生的伦理和社会影响。"}}
{"id": "2602.20008", "pdf": "https://arxiv.org/pdf/2602.20008", "abs": "https://arxiv.org/abs/2602.20008", "authors": ["Louis Fabrice Tshimanga", "Andrea Zanola", "Federico Del Pup", "Manfredo Atzori"], "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets. While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input. The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution. This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames. To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures. Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$). This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.", "AI": {"tldr": "Token-UNet通过集成Transformer模块来改进3D UNet，以提高脑影像分割的效率和可解释性。", "motivation": "为了克服现有模型在常见硬件上的计算挑战，并实现更高效、更具解释性的3D医学影像分割。", "method": "引入了新的TokenLearner模块从局部和全局结构中汇集预设数量的令牌，同时保留UNet样式的卷积编码器。这种设计使得Transformer能够在资源有限的情况下有效运行。", "result": "模型性能优于SwinUNETR，在Dice分数上达到了87.21%±0.35%，且在内存占用、推理时间和参数计数方面显著减少。", "conclusion": "该工作提出了一种更高效和可解释的3D医学影像分割方法，为资源受限环境下的模型优化提供了新的可能。"}}
{"id": "2602.20003", "pdf": "https://arxiv.org/pdf/2602.20003", "abs": "https://arxiv.org/abs/2602.20003", "authors": ["Nuocheng Yang", "Sihua Wang", "Zhaohui Yang", "Mingzhe Chen", "Changchuan Yin", "Kaibin Huang"], "title": "A Secure and Private Distributed Bayesian Federated Learning Design", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 9 figures", "summary": "Distributed Federated Learning (DFL) enables decentralized model training across large-scale systems without a central parameter server. However, DFL faces three critical challenges: privacy leakage from honest-but-curious neighbors, slow convergence due to the lack of central coordination, and vulnerability to Byzantine adversaries aiming to degrade model accuracy. To address these issues, we propose a novel DFL framework that integrates Byzantine robustness, privacy preservation, and convergence acceleration. Within this framework, each device trains a local model using a Bayesian approach and independently selects an optimal subset of neighbors for posterior exchange. We formulate this neighbor selection as an optimization problem to minimize the global loss function under security and privacy constraints. Solving this problem is challenging because devices only possess partial network information, and the complex coupling between topology, security, and convergence remains unclear. To bridge this gap, we first analytically characterize the trade-offs between dynamic connectivity, Byzantine detection, privacy levels, and convergence speed. Leveraging these insights, we develop a fully distributed Graph Neural Network (GNN)-based Reinforcement Learning (RL) algorithm. This approach enables devices to make autonomous connection decisions based on local observations. Simulation results demonstrate that our method achieves superior robustness and efficiency with significantly lower overhead compared to traditional security and privacy schemes.", "AI": {"tldr": "提出了一个集成拜占庭抗性、隐私保护和收敛加速的分布式联邦学习框架。", "motivation": "为了应对分布式联邦学习中存在的隐私泄露、慢速收敛以及对抗攻击等关键挑战，设计了一种新的DFL框架。", "method": "采用贝叶斯方法训练局部模型，并通过优化问题选择最优邻居进行后验交换。结合图神经网络和强化学习算法使设备能够自主地做出连接决策。", "result": "仿真结果显示该方法在鲁棒性和效率上均优于传统安全隐私方案，且开销更低。", "conclusion": "所提出的分布式联邦学习框架通过优化设计有效解决了隐私保护、拜占庭抗性及收敛速度问题。"}}
{"id": "2602.19994", "pdf": "https://arxiv.org/pdf/2602.19994", "abs": "https://arxiv.org/abs/2602.19994", "authors": ["Christof Leitgeb", "Thomas Puchleitner", "Max Peter Ronecker", "Daniel Watzenig"], "title": "RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather", "categories": ["cs.CV"], "comment": "Accepted to 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.", "AI": {"tldr": "提出了一种针对雷达数据的RADE-Net网络，用于在恶劣天气条件下进行目标检测。", "motivation": "光学传感器如相机和激光雷达在恶劣天气条件下的表现不佳，而雷达因其穿透能力强，在这种情况下表现出色。然而，完整的雷达张量数据量大且数据集稀缺，大多数基于雷达的方法使用稀疏的点云或二维投影导致信息丢失。", "method": "提出了一种3D投影方法，用于快速傅里叶变换后的4D范围-方位-多普勒-仰角（RADE）张量。该网络轻量化并能利用低级和高级提示进行目标检测，并在极坐标场景中直接预测对象中心点，在笛卡尔坐标系下从丰富的特征图回归旋转的3D边界框。", "result": "模型在K-Radar数据集上多类用户场景和各种天气条件下进行了评估，比基线提高了16.7%，超过现有雷达单传感器方法6.5%。在恶劣天气条件下超过了几个激光雷达方法的表现。", "conclusion": "RADE-Net网络能够有效地从低级雷达张量中提取丰富特征，并且在各种天气条件下的目标检测任务中表现出色，比其他模型更优。"}}
{"id": "2602.19983", "pdf": "https://arxiv.org/pdf/2602.19983", "abs": "https://arxiv.org/abs/2602.19983", "authors": ["Zachary Ravichadran", "David Snyder", "Alexander Robey", "Hamed Hassani", "Vijay Kumar", "George J. Pappas"], "title": "Contextual Safety Reasoning and Grounding for Open-World Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots are increasingly operating in open-world environments where safe behavior depends on context: the same hallway may require different navigation strategies when crowded versus empty, or during an emergency versus normal operations. Traditional safety approaches enforce fixed constraints in user-specified contexts, limiting their ability to handle the open-ended contextual variability of real-world deployment. We address this gap via CORE, a safety framework that enables online contextual reasoning, grounding, and enforcement without prior knowledge of the environment (e.g., maps or safety specifications). CORE uses a vision-language model (VLM) to continuously reason about context-dependent safety rules directly from visual observations, grounds these rules in the physical environment, and enforces the resulting spatially-defined safe sets via control barrier functions. We provide probabilistic safety guarantees for CORE that account for perceptual uncertainty, and we demonstrate through simulation and real-world experiments that CORE enforces contextually appropriate behavior in unseen environments, significantly outperforming prior semantic safety methods that lack online contextual reasoning. Ablation studies validate our theoretical guarantees and underscore the importance of both VLM-based reasoning and spatial grounding for enforcing contextual safety in novel settings. We provide additional resources at https://zacravichandran.github.io/CORE.", "AI": {"tldr": "本文提出了一种名为CORE的安全框架，该框架能够在线进行上下文推理、接地并执行安全规则，无需预先了解环境。", "motivation": "传统的安全性方法只能在用户指定的环境中应用固定约束，无法处理现实世界部署中出现的开放性上下文变化。为了应对这一挑战，本文提出了一个名为CORE的安全框架。", "method": "CORE利用视觉-语言模型（VLM）直接从视觉观察中推断出与上下文相关的安全规则，并将这些规则接地到物理环境中，通过控制障碍函数执行空间定义的安全集。", "result": "实验结果表明，CORE能够在未知环境中执行适当的行为，并且显著优于缺乏在线上下文推理的语义安全性方法。消融研究验证了理论保证的重要性以及VLM基础推理和空间接地对实施新环境中的上下文安全性的意义。", "conclusion": "本文提出的CORE框架为在开放世界中运行的安全性提供了有效的解决方案，证明了其在处理未知环境中执行适当行为的能力，并且能够提供概率安全性保障。"}}
{"id": "2602.19976", "pdf": "https://arxiv.org/pdf/2602.19976", "abs": "https://arxiv.org/abs/2602.19976", "authors": ["Sifei Li", "Yang Li", "Zizhou Wang", "Yuxin Zhang", "Fuzhang Wu", "Oliver Deussen", "Tong-Yee Lee", "Weiming Dong"], "title": "SongEcho: Towards Cover Song Generation via Instance-Adaptive Element-wise Linear Modulation", "categories": ["cs.SD"], "comment": "Accepted at ICLR 2026. 21 pages (10 pages main text), 5 figures", "summary": "Cover songs constitute a vital aspect of musical culture, preserving the core melody of an original composition while reinterpreting it to infuse novel emotional depth and thematic emphasis. Although prior research has explored the reinterpretation of instrumental music through melody-conditioned text-to-music models, the task of cover song generation remains largely unaddressed. In this work, we reformulate our cover song generation as a conditional generation, which simultaneously generates new vocals and accompaniment conditioned on the original vocal melody and text prompts. To this end, we present SongEcho, which leverages Instance-Adaptive Element-wise Linear Modulation (IA-EiLM), a framework that incorporates controllable generation by improving both conditioning injection mechanism and conditional representation. To enhance the conditioning injection mechanism, we extend Feature-wise Linear Modulation (FiLM) to an Element-wise Linear Modulation (EiLM), to facilitate precise temporal alignment in melody control. For conditional representations, we propose Instance-Adaptive Condition Refinement (IACR), which refines conditioning features by interacting with the hidden states of the generative model, yielding instance-adaptive conditioning. Additionally, to address the scarcity of large-scale, open-source full-song datasets, we construct Suno70k, a high-quality AI song dataset enriched with comprehensive annotations. Experimental results across multiple datasets demonstrate that our approach generates superior cover songs compared to existing methods, while requiring fewer than 30% of the trainable parameters. The code, dataset, and demos are available at https://github.com/lsfhuihuiff/SongEcho_ICLR2026.", "AI": {"tldr": "本文提出了一种名为SongEcho的方法，通过实例自适应元素级线性调制（IA-EiLM）框架生成高质量的翻唱歌曲。", "motivation": "虽然之前的研究已经探索了通过基于旋律的文本到音乐模型来重新诠释乐器音乐的可能性，但翻唱歌曲的生成任务仍未得到充分研究。本文旨在解决这一问题，提出了一种新的方法以同时生成新的人声和伴奏，从而创建高质量的翻唱。", "method": "SongEcho采用实例自适应元素级线性调制（IA-EiLM）框架，将Feature-wise Linear Modulation（FiLM）扩展为Element-wise Linear Modulation（EiLM），以便实现更精确的旋律控制，并提出Instance-Adaptive Condition Refinement（IACR）以改进条件表示。", "result": "实验结果表明，在多个数据集上，本文的方法生成了比现有方法更好的翻唱歌曲，同时参数量减少了30%以下。", "conclusion": "SongEcho通过精确的时序对齐和实例自适应条件细化取得了显著成果，并为未来的音乐生成研究奠定了基础。"}}
{"id": "2602.19974", "pdf": "https://arxiv.org/pdf/2602.19974", "abs": "https://arxiv.org/abs/2602.19974", "authors": ["Tianyu Wang", "Zhiyuan Ma", "Qian Wang", "Xinyi Zhang", "Xinwei Long", "Bowen Zhou"], "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.", "AI": {"tldr": "提出了一种新的基于强化学习的图像生成框架RL-RIG，以解决现有图像生成模型在空间关系捕捉和结构完整性方面的不足。", "motivation": "现有的图像生成模型难以准确地捕获细粒度的空间关系并生成具有结构完整性的场景。为了克服这一挑战，研究人员开发了一种新的方法来增强模型的推理能力。", "method": "通过使用Diffuser、Checker、Actor和Inverse Diffuser四个主要组件构建了RL-RIG框架，该框架遵循Generate-Reflect-Edit范式，以激发图像生成中的链状思维推理能力。此外，还引入了Reflection-GRPO来训练VLM Actor和Image Editor，从而在给定提示下提高图像质量。", "result": "实验结果表明，在空间准确性评估指标上（如场景图IoU），RL-RIG优于现有的最先进的开源模型，尤其是在可控性和精确的空间推理方面表现突出。", "conclusion": "RL-RIG框架通过引入新的训练策略和评估方法，成功提高了图像生成中对精细空间关系的理解和表达能力。"}}
{"id": "2602.19969", "pdf": "https://arxiv.org/pdf/2602.19969", "abs": "https://arxiv.org/abs/2602.19969", "authors": ["Yuxing Tian", "Fengran Mo", "Weixu Zhang", "Yiyan Qi", "Jian-Yun Nie"], "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EACL2026", "summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "本文提出了ReAttn，一种改进注意力权重的后处理策略，用于改善基于注意力的重新排序方法。", "motivation": "最近的大规模语言模型在零样本重新排序任务上表现出色。然而，基于注意力的方法存在两个主要问题：注意力信号集中于少数文档中的特定词，并且过度强调与查询词汇相似的部分，导致不相关文档被误认为是相关的。", "method": "ReAttn通过计算跨文档逆文档频率（IDF）权重来降低对频繁出现在候选文档中、与查询重叠的词语的注意力分配。同时使用熵正则化来减少过于集中的注意力分布，促进更加均匀的信息令牌分配。", "result": "广泛的实验验证了该方法的有效性。", "conclusion": "ReAttn通过后处理调整现有注意力权重，有效减少了基于注意力重新排序方法中的偏见和过分集中问题。"}}
{"id": "2602.19966", "pdf": "https://arxiv.org/pdf/2602.19966", "abs": "https://arxiv.org/abs/2602.19966", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet", "Yong Zhang"], "title": "GazeFlow: Personalized Ambient Soundscape Generation for Passive Strabismus Self-Monitoring", "categories": ["cs.HC"], "comment": null, "summary": "Strabismus affects 2-4% of the population, yet individuals recovering from corrective surgery lack accessible tools for monitoring eye alignment. Dichoptic therapies require active engagement & clinical supervision, limiting their adoption for passive self-awareness. We present GazeFlow, a browser-based self-monitoring system that uses a personalized temporal autoencoder to detect eye drift patterns from webcam-based gaze tracking & provides ambient audio feedback. Unlike alert-based systems, GazeFlow operates according to calm computing principles, morphing musical parameters in proportion to drift severity while remaining in peripheral awareness. We address the challenges of inter-individual variability & domain transfer (1000Hz research to 30Hz webcam) by introducing Binocular Temporal-Frequency Disentanglement (BTFD), Contrastive Biometric Pre-training (CBP), & Gaze-MAML. We validate our approach on the GazeBase dataset (N=50) achieving F1=0.84 for drift detection, & conduct a preliminary user study (N=6) with participants having intermittent strabismus. Participants reported increased awareness of their eye behaviour (M=5.8/7) & preference for ambient feedback over alerts (M=6.2/7). We discuss the system's potential for self-awareness applications & outline directions for clinical validation.", "AI": {"tldr": "GazeFlow是一款基于浏览器的自我监测系统，通过个性化的时间自动编码器检测眼位偏移模式，并提供环境音频反馈。", "motivation": "为了解决斜视患者在矫正手术后缺乏可访问的眼部对齐监控工具的问题，提出了一种新的被动自监测方法。现有二视法疗法需要主动参与和临床监督，限制了其应用范围。", "method": "使用个性化的时间自动编码器从基于网络摄像头的注视跟踪中检测眼位偏移模式，并通过调整音乐参数提供环境音频反馈。解决个体差异与领域迁移问题的方法包括双目时间-频率解耦（BTFD），对比生物特征预训练（CBP）和注视MAML。", "result": "在GazeBase数据集上验证了该方法的有效性，F1得分达到0.84；初步用户研究显示参与者对其眼睛行为的认知提高，并倾向于选择环境反馈而非警报。", "conclusion": "此系统具有应用于自我意识的潜力，并为进一步临床验证提出了方向。"}}
{"id": "2602.19964", "pdf": "https://arxiv.org/pdf/2602.19964", "abs": "https://arxiv.org/abs/2602.19964", "authors": ["Moritz A. Zanger", "Yijun Wu", "Pascal R. Van der Vaart", "Wendelin Böhmer", "Matthijs T. J. Spaan"], "title": "On the Equivalence of Random Network Distillation, Deep Ensembles, and Bayesian Inference", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.ML"], "comment": "8 pages, 1 Figure", "summary": "Uncertainty quantification is central to safe and efficient deployments of deep learning models, yet many computationally practical methods lack lacking rigorous theoretical motivation. Random network distillation (RND) is a lightweight technique that measures novelty via prediction errors against a fixed random target. While empirically effective, it has remained unclear what uncertainties RND measures and how its estimates relate to other approaches, e.g. Bayesian inference or deep ensembles. This paper establishes these missing theoretical connections by analyzing RND within the neural tangent kernel framework in the limit of infinite network width. Our analysis reveals two central findings in this limit: (1) The uncertainty signal from RND -- its squared self-predictive error -- is equivalent to the predictive variance of a deep ensemble. (2) By constructing a specific RND target function, we show that the RND error distribution can be made to mirror the centered posterior predictive distribution of Bayesian inference with wide neural networks. Based on this equivalence, we moreover devise a posterior sampling algorithm that generates i.i.d. samples from an exact Bayesian posterior predictive distribution using this modified \\textit{Bayesian RND} model. Collectively, our findings provide a unified theoretical perspective that places RND within the principled frameworks of deep ensembles and Bayesian inference, and offer new avenues for efficient yet theoretically grounded uncertainty quantification methods.", "AI": {"tldr": "论文主要任务是在无限网络宽度极限下分析随机网络蒸馏（RND）与其他方法之间的理论等价性。", "motivation": "缺乏计算上实用且有严格理论基础的不确定性量化方法。本文旨在建立RND与深度集成和贝叶斯推理之间的联系，以提供一个统一的理论视角并提出新的高效不确定性量化的途径。", "method": "通过神经切片核框架分析了在无限网络宽度极限下的随机网络蒸馏（RND），揭示了其不确定性信号与深度集成预测方差等价，并构建了一个特定的RND目标函数使误差分布可以反映贝叶斯推理中的后验预测分布。基于此建立了新的后验抽样算法。", "result": "在无限宽度极限下，RND的不确定度信号与其平方自预测错误是与深度集成预测方差等价的；通过特定的目标函数修改后的Bayesian RND模型可以产生精确贝叶斯后验预测分布的独立同分布样本。", "conclusion": "论文揭示了随机网络蒸馏在理论上的新视角，证明其不确定性度量与其他方法的一致性，并提出了一种新的高效且理论基础坚实的不确定性量化方法。"}}
{"id": "2602.19948", "pdf": "https://arxiv.org/pdf/2602.19948", "abs": "https://arxiv.org/abs/2602.19948", "authors": ["Ian Steenstra", "Paola Pedrelli", "Weiyan Shi", "Stacy Marsella", "Timothy W. Bickmore"], "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "comment": "This paper is a condensed version of the first author's Ph.D. dissertation submitted to Northeastern University", "summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes. Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.", "AI": {"tldr": "论文提出了一个评估框架，用于测试大型语言模型在精神健康支持中的风险。", "motivation": "当前的安全基准往往无法检测出治疗对话中复杂的长期风险。因此，需要一种新的方法来全面评估AI心理治疗的风险和质量。", "method": "该研究开发了一个配对AI心理治疗师与具有动态认知-情感模型的模拟患者代理的框架，并通过一个涵盖15个不同临床表型患者的高质量护理和风险本体进行评估。此外，还验证了一个交互式数据可视化仪表板，以展示这些发现。", "result": "大规模仿真（N=369次会话）揭示了AI在精神健康支持中的重大安全漏洞，包括确认患者妄想的风险以及未能缓解自杀风险等。", "conclusion": "研究结果强调了AI提供的心理健康支持的关键安全性风险，并指出需要通过基于仿真的临床红队测试来评估这些风险。"}}
{"id": "2602.19946", "pdf": "https://arxiv.org/pdf/2602.19946", "abs": "https://arxiv.org/abs/2602.19946", "authors": ["Krzysztof Adamkiewicz", "Brian Moser", "Stanislav Frolov", "Tobias Christian Nauen", "Federico Raue", "Andreas Dengel"], "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.", "AI": {"tldr": "本文探讨了现代文本到图像模型作为训练数据生成器的有效性，发现尽管这些模型在视觉逼真度和指令跟随方面有所进步，但其生成的数据的真实性和多样性却有所下降。", "motivation": "随着文本到图像模型的进步，研究人员期望它们能够提供高质量的合成数据来替代真实的训练集。然而，作者质疑了这种假设，并希望通过研究来验证这些现代模型作为可靠训练数据生成器的能力。", "method": "本文使用2022年至2025年间发布的最先进的T2I模型生成大规模的合成数据集，用标准分类器仅在该合成数据上进行训练，并对其在真实测试数据上的性能进行评估。", "result": "实验结果显示，在真实测试数据上的分类准确度随着使用更现代的文本到图像模型而下降。这表明这些模型倾向于产生视觉效果好但缺乏多样性和标签-图像对齐的数据。", "conclusion": "研究结果挑战了目前的一个假设，即在生成现实性方面的进步意味着训练数据的真实性和多样性也会提升。因此，作者呼吁重新思考现代T2I模型作为可靠训练数据生成器的能力，并强调需要进一步的工作来改进这个问题。"}}
{"id": "2602.19945", "pdf": "https://arxiv.org/pdf/2602.19945", "abs": "https://arxiv.org/abs/2602.19945", "authors": ["Jin Liu", "Yinbin Miao", "Ning Xi", "Junkang Liu"], "title": "DP-FedAdamW: An Efficient Optimizer for Differentially Private Federated Large Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Balancing convergence efficiency and robustness under Differential Privacy (DP) is a central challenge in Federated Learning (FL). While AdamW accelerates training and fine-tuning in large-scale models, we find that directly applying it to Differentially Private FL (DPFL) suffers from three major issues: (i) data heterogeneity and privacy noise jointly amplify the variance of second-moment estimator, (ii) DP perturbations bias the second-moment estimator, and (iii) DP amplify AdamW sensitivity to local overfitting, worsening client drift. We propose DP-FedAdamW, the first AdamW-based optimizer for DPFL. It restores AdamW under DP by stabilizing second-moment variance, removing DP-induced bias, and aligning local updates to the global descent to curb client drift. Theoretically, we establish an unbiased second-moment estimator and prove a linearly accelerated convergence rate without any heterogeneity assumption, while providing tighter $(\\varepsilon,δ)$-DP guarantees. Our empirical results demonstrate the effectiveness of DP-FedAdamW across language and vision Transformers and ResNet-18. On Tiny-ImageNet (Swin-Base, $\\varepsilon=1$), DP-FedAdamW outperforms the state-of-the-art (SOTA) by 5.83\\%. The code is available in Appendix.", "AI": {"tldr": "该论文提出了一种名为DP-FedAdamW的优化器，用于在具有差分隐私保护的联邦学习中加速大型模型的训练。", "motivation": "传统方法应用到带有差分隐私保护的联邦学习时存在三大问题：数据异质性和隐私噪声共同放大了二阶矩估计器的方差、差异隐私扰动使二阶矩估计偏移以及增加了局部过拟合风险，导致客户端偏差增大。", "method": "论文提出了一种基于AdamW优化器的方法DP-FedAdamW。通过稳定二阶矩估计量的方差，消除由差分隐私引入的偏差，并对齐本地更新以减少客户端漂移。", "result": "实验结果表明，与现有的最佳方法相比，在多个场景下DP-FedAdamW表现更优，特别是在Tiny-ImageNet数据集上使用Swin-Base模型时，取得了比现有最优方法高5.83%的性能提升。", "conclusion": "本文提出的DP-FedAdamW优化器在保证差分隐私的同时有效解决了联邦学习中的训练效率和鲁棒性问题。"}}
{"id": "2602.19944", "pdf": "https://arxiv.org/pdf/2602.19944", "abs": "https://arxiv.org/abs/2602.19944", "authors": ["Yilong Yang", "Jianxin Tian", "Shengchuan Zhang", "Liujuan Cao"], "title": "Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026 (main conference)", "summary": "Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \\textbf{D}iscover-\\textbf{S}egment-\\textbf{S}elect (\\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.", "AI": {"tldr": "本文提出了一种新的零样本伪装目标分割方法DSS机制，通过分阶段细化提高分割准确性。", "motivation": "当前的零样本伪装目标分割方法依赖于大规模语言模型进行目标发现，存在定位不准确、假阳性及漏检问题。为解决这些问题，提出了DSS机制以逐步细化分割结果。", "method": "提出了一种包含特征一致性对象发现（FOD）、SAM分割模块和语义驱动的掩码选择（SMS）模块的新方法，通过分阶段处理提高伪装目标分割准确性。", "result": "实验表明，DSS在多个零样本伪装目标分割基准测试中达到了最佳性能，特别是在多实例场景下表现突出。", "conclusion": "提出的DSS机制能够有效提升零样本伪装目标分割的准确性和鲁棒性。"}}
{"id": "2602.19943", "pdf": "https://arxiv.org/pdf/2602.19943", "abs": "https://arxiv.org/abs/2602.19943", "authors": ["Abulikemu Abuduweili", "Yuyang Pang", "Feihan Li", "Changliu Liu"], "title": "Scaling Law of Neural Koopman Operators", "categories": ["cs.RO"], "comment": null, "summary": "Data-driven neural Koopman operator theory has emerged as a powerful tool for linearizing and controlling nonlinear robotic systems. However, the performance of these data-driven models fundamentally depends on the trade-off between sample size and model dimensions, a relationship for which the scaling laws have remained unclear. This paper establishes a rigorous framework to address this challenge by deriving and empirically validating scaling laws that connect sample size, latent space dimension, and downstream control quality. We derive a theoretical upper bound on the Koopman approximation error, explicitly decomposing it into sampling error and projection error. We show that these terms decay at specific rates relative to dataset size and latent dimension, providing a rigorous basis for the scaling law. Based on the theoretical results, we introduce two lightweight regularizers for the neural Koopman operator: a covariance loss to help stabilize the learned latent features and an inverse control loss to ensure the model aligns with physical actuation. The results from systematic experiments across six robotic environments confirm that model fitting error follows the derived scaling laws, and the regularizers improve dynamic model fitting fidelity, with enhanced closed-loop control performance. Together, our results provide a simple recipe for allocating effort between data collection and model capacity when learning Koopman dynamics for control.", "AI": {"tldr": "本文建立了数据驱动的Koopman算子理论中样本量、潜在空间维度和下游控制质量之间的缩放定律框架。", "motivation": "为了理解数据驱动模型性能与样本大小及模型维度之间的关系，尤其是在线性化和控制非线性机器人系统时，需要明确这些因素之间的缩放规律。", "method": "通过推导Koopman算子近似误差的理论上限，并将其分解为采样误差和投影误差，确立了样本量、潜在空间维度与下游控制质量之间的关系。引入两个轻量化正则化项：协方差损失以稳定学习到的潜在特征，逆控损失以确保模型符合物理驱动。", "result": "系统实验结果表明，模型拟合误差遵循推导出的缩放规律，并且正则化器改善了动态模型拟合精度和闭环控制性能。", "conclusion": "本文的研究成果提供了一个简单的配方，用于在学习Koopman动力学进行控制时，在数据收集与模型容量之间分配努力。"}}
{"id": "2602.19937", "pdf": "https://arxiv.org/pdf/2602.19937", "abs": "https://arxiv.org/abs/2602.19937", "authors": ["Yifei Shi", "Boyan Wan", "Xin Xu", "Kai Xu"], "title": "Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.", "AI": {"tldr": "该论文提出了一种结合SO(3)等变卷积隐式网络和正向激励点采样策略的方法，用于提升神经隐式场在对象姿态估计中的性能。", "motivation": "尽管已经取得了进展，但预测未观察到的相机空间区域的规范坐标仍然具有挑战性，因为缺乏直接观测信号。这导致对模型泛化能力的高度依赖，从而产生高不确定性，并可能阻碍学习过程和降低性能。", "method": "该方法结合了SO(3)等变卷积隐式网络和正向激励点采样策略。其中，SO(3)等变卷积隐式网络在任意查询位置上估计具有SO(3)等变性的点级属性，而PIPS策略则基于输入动态确定采样位置。", "result": "该方法在三个姿态估计数据集上的性能超过了最新的研究成果，并且在难以处理的场景中（如未见过的姿态、高遮挡和严重噪音）显示了显著改进。", "conclusion": "所提出的方法通过结合SO(3)等变卷积隐式网络和PIPS策略，提高了神经隐式场在对象姿态估计中的性能。"}}
{"id": "2602.19931", "pdf": "https://arxiv.org/pdf/2602.19931", "abs": "https://arxiv.org/abs/2602.19931", "authors": ["Pin-Han Huang", "Shang-Tse Chen", "Hsuan-Tien Lin"], "title": "Expanding the Role of Diffusion Models for Robust Classifier Training", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.", "AI": {"tldr": "扩展扩散模型在对抗训练中的作用，通过引入扩散模型的内部表示来提升鲁棒性分类器的训练。", "motivation": "探索扩散模型除了生成合成数据外，其内部特征能否为增强对抗训练提供额外的好处。", "method": "系统实验结合扩散模型的表示和对抗训练进行辅助学习信号集成。", "result": "研究表明扩散模型的表征提供了多样性和部分鲁棒性，并鼓励更多解耦特性形成，提高了CIFAR-10、CIFAR-100和ImageNet上的分类器鲁棒性。", "conclusion": "扩散模型与生成合成数据相结合，在对抗训练中展现出有效提升分类器鲁棒性的能力。"}}
{"id": "2602.19930", "pdf": "https://arxiv.org/pdf/2602.19930", "abs": "https://arxiv.org/abs/2602.19930", "authors": ["Nathan Gavenski", "Felipe Meneguzzi", "Odinaldo Rodrigues"], "title": "Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted as part of the Blue Sky Ideas Track for the 25th International Conference on Autonomous Agents and Multiagent Systems", "summary": "Imitation learning stands at a crossroads: despite decades of progress, current imitation learning agents remain sophisticated memorisation machines, excelling at replay but failing when contexts shift or goals evolve. This paper argues that this failure is not technical but foundational: imitation learning has been optimised for the wrong objective. We propose a research agenda that redefines success from perfect replay to compositional adaptability. Such adaptability hinges on learning behavioural primitives once and recombining them through novel contexts without retraining. We establish metrics for compositional generalisation, propose hybrid architectures, and outline interdisciplinary research directions drawing on cognitive science and cultural evolution. Agents that embed adaptability at the core of imitation learning thus have an essential capability for operating in an open-ended world.", "AI": {"tldr": "本文探讨了模仿学习中从记忆型模型向适应性更强的模型转变的方法，提出了衡量组合泛化的标准，并提出了一种新的研究方向。", "motivation": "当前的模仿学习代理虽然在重复任务上表现出色，但在目标或环境变化时却表现不佳。这表明需要一种能够更好适应的新方法来增强模仿学习的能力。", "method": "文章定义了衡量组合泛化的标准，并提出了一种新的研究方向，该方向融合认知科学和文化演化理论，以实现更加通用的模仿学习系统。", "result": "尚未提供具体实验结果，但提供了对如何改进模仿学习策略的理解与建议，提出了一个新的评价体系以及潜在的研究路径。", "conclusion": "通过引入组合适应性的概念来重新定义模仿学习的成功标准，并为未来的研究指明了方向。"}}
{"id": "2602.19926", "pdf": "https://arxiv.org/pdf/2602.19926", "abs": "https://arxiv.org/abs/2602.19926", "authors": ["Jin Liu", "Yinbin Miao", "Ning Xi", "Junkang Liu"], "title": "Rethinking LoRA for Privacy-Preserving Federated Learning in Large Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning large vision models (LVMs) and large language models (LLMs) under differentially private federated learning (DPFL) is hindered by a fundamental privacy-utility trade-off. Low-Rank Adaptation (LoRA), a promising parameter-efficient fine-tuning (PEFT) method, reduces computational and communication costs by introducing two trainable low-rank matrices while freezing pre-trained weights. However, directly applying LoRA in DPFL settings leads to performance degradation, especially in LVMs. Our analysis reveals three previously underexplored challenges: (1) gradient coupling caused by the simultaneous update of two asymmetric low-rank matrices, (2) compounded noise amplification under differential privacy, and (3) sharpness of the global aggregated model in the parameter space. To address these issues, we propose LA-LoRA (\\textbf{L}ocal \\textbf{A}lternating \\textbf{LoRA}), a novel approach that decouples gradient interactions and aligns update directions across clients to enhance robustness under stringent privacy constraints. Theoretically, LA-LoRA strengthens convergence guarantees in noisy federated environments. Extensive experiments demonstrate that LA-LoRA achieves state-of-the-art (SOTA) performance on Swin Transformer and RoBERTa models, showcasing robustness to DP noise and broad applicability across both LVMs and LLMs. For example, when fine-tuning the Swin-B model on the Tiny-ImageNet dataset under a strict privacy budget ($ε= 1$), LA-LoRA outperforms the best baseline, RoLoRA, by 16.83\\% in test accuracy. Code is provided in \\repolink.", "AI": {"tldr": "本文提出了一种新的低秩适应方法LA-LoRA，以解决在差分隐私联邦学习中应用LoRA时遇到的问题，并展示了其优越的性能和广泛的适用性。", "motivation": "直接将LoRA应用于具有严格隐私保护要求的差分隐私联邦学习环境中会导致模型表现下降。本文通过分析提出需要解决梯度耦合、噪声放大以及全局模型尖锐度等问题，以提升模型在隐私限制下的鲁棒性和性能。", "method": "LA-LoRA方法旨在解耦两个低秩矩阵更新过程中的梯度交互，并确保客户端间更新方向的对齐。通过这种方式来增强模型在具有高噪音环境下的收敛保证和稳定性。", "result": "实验结果表明，LA-LoRA能够在Swin Transformer和RoBERTa等大型模型上实现最先进的性能，在严格的隐私预算下优于现有的最佳基准方法RoLoRA，特别是在图像识别任务中取得了显著的准确率提升。", "conclusion": "通过提出LA-LoRA来应对差分隐私联邦学习中的挑战，本文展示了如何在保护用户数据隐私的同时提高大规模模型的有效性和实用性。"}}
{"id": "2602.19917", "pdf": "https://arxiv.org/pdf/2602.19917", "abs": "https://arxiv.org/abs/2602.19917", "authors": ["Thanh Nguyen", "Tung Luu", "Tri Ton", "Sungwoong Kim", "Chang D. Yoo"], "title": "Uncertainty-Aware Rank-One MIMO Q Network Framework for Accelerated Offline Reinforcement Learning", "categories": ["cs.LG", "cs.RO"], "comment": "10 pages, 4 Figures, IEEE Access", "summary": "Offline reinforcement learning (RL) has garnered significant interest due to its safe and easily scalable paradigm. However, training under this paradigm presents its own challenge: the extrapolation error stemming from out-of-distribution (OOD) data. Existing methodologies have endeavored to address this issue through means like penalizing OOD Q-values or imposing similarity constraints on the learned policy and the behavior policy. Nonetheless, these approaches are often beset by limitations such as being overly conservative in utilizing OOD data, imprecise OOD data characterization, and significant computational overhead. To address these challenges, this paper introduces an Uncertainty-Aware Rank-One Multi-Input Multi-Output (MIMO) Q Network framework. The framework aims to enhance Offline Reinforcement Learning by fully leveraging the potential of OOD data while still ensuring efficiency in the learning process. Specifically, the framework quantifies data uncertainty and harnesses it in the training losses, aiming to train a policy that maximizes the lower confidence bound of the corresponding Q-function. Furthermore, a Rank-One MIMO architecture is introduced to model the uncertainty-aware Q-function, \\TP{offering the same ability for uncertainty quantification as an ensemble of networks but with a cost nearly equivalent to that of a single network}. Consequently, this framework strikes a harmonious balance between precision, speed, and memory efficiency, culminating in improved overall performance. Extensive experimentation on the D4RL benchmark demonstrates that the framework attains state-of-the-art performance while remaining computationally efficient. By incorporating the concept of uncertainty quantification, our framework offers a promising avenue to alleviate extrapolation errors and enhance the efficiency of offline RL.", "AI": {"tldr": "提出了一种不确定性感知的Rank-One MIMO Q网络框架，用于加速离线强化学习。", "motivation": "解决离线强化学习中由于出界数据导致的外推误差问题，现有的方法存在过于保守地利用出界数据、不精确的数据特征描述和显著的计算开销等缺点。", "method": "通过量化不确定性并将其应用于训练损失中，以训练一个最大化相应Q函数下限置信边界的策略；引入Rank-One MIMO架构来建模不确定性感知的Q函数，提供与网络集合相同的不确定性量化能力，同时保持单个网络的成本。", "result": "在D4RL基准上进行了广泛实验，证明了框架达到最先进的性能的同时保持计算效率。", "conclusion": "通过纳入不确定性量化的概念，该框架为减轻外推误差和提高离线强化学习的效率提供了有希望的方法。"}}
{"id": "2602.19916", "pdf": "https://arxiv.org/pdf/2602.19916", "abs": "https://arxiv.org/abs/2602.19916", "authors": ["Yixin Yang", "Bojian Wu", "Yang Zhou", "Hui Huang"], "title": "Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to ICLR 2026. Project page: \\url{https://xiaoxinyyx.github.io/augs}", "summary": "Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.", "AI": {"tldr": "提出了一种增强的高斯核方法，改进了3D高斯点阵（3DGS）的颜色编码和渲染质量。", "motivation": "现有的3DGS方法由于依赖球谐函数进行颜色编码，难以分离漫反射与镜面反射成分，影响复杂反射的真实感再现。", "method": "通过引入视角相关透明度模型的增强高斯核及误差驱动补偿策略提高渲染效果，在2D高斯初始化的基础上动态插入和优化这些增强核以生成改进后的辐射场。", "result": "实验结果表明该方法不仅在渲染性能上超越了最先进的NeRF技术，并且具有更高的参数效率。", "conclusion": "本文提出的增强3DGS框架能够有效地提高复杂反射的真实感再现，同时保持较高的渲染效率和参数利用率。"}}
{"id": "2602.19914", "pdf": "https://arxiv.org/pdf/2602.19914", "abs": "https://arxiv.org/abs/2602.19914", "authors": ["Thatchawin Leelawat", "Lewis D Griffin"], "title": "Watson & Holmes: A Naturalistic Benchmark for Comparing Human and LLM Reasoning", "categories": ["cs.AI"], "comment": "51 pages, 13 figures", "summary": "Existing benchmarks for AI reasoning provide limited insight into how closely these capabilities resemble human reasoning in naturalistic contexts. We present an adaptation of the Watson & Holmes detective tabletop game as a new benchmark designed to evaluate reasoning performance using incrementally presented narrative evidence, open-ended questions and unconstrained language responses. An automated grading system was developed and validated against human assessors to enable scalable and replicable performance evaluation. Results show a clear improvement in AI model performance over time. Over nine months of 2025, model performance rose from the lower quartile of the human comparison group to approximately the top 5%. Around half of this improvement reflects steady advancement across successive model releases, while the remainder corresponds to a marked step change associated with reasoning-oriented model architectures. Systematic differences in the performance of AI models compared to humans, dependent on features of the specific detection puzzle, were mostly absent with the exception of a fall in performance for models when solving longer cases (case lengths being in the range of 1900-4000 words), and an advantage at inductive reasoning for reasoning models at early stages of case solving when evidence was scant.", "AI": {"tldr": "通过改编Watson & Holmes侦探桌游，作为评估AI模型在自然情境下推理能力的新基准。", "motivation": "现有AI推理基准无法全面展示其在接近人类自然环境中表现的相似性。提出新的自然主义测试环境来对比人类和LLM（大型语言模型）之间的推理能力。", "method": "开发了一套基于Watson & Holmes侦探桌游的评估系统，包括逐步呈现叙事证据、开放式问题以及不受约束的语言回答。引入了自动评分机制，并通过与人工评价者的比较验证其准确性。", "result": "结果显示，在2025年九个月的研究期间内，AI模型的表现从人类对比组中的较低四分位数提升到了大约前5%的位置。其中一半的改进来自多次模型迭代的稳定进步；另一半则是由专门针对推理优化的新架构所带来的显著跳跃式进展。", "conclusion": "虽然大部分情况下AI模型在特定侦查难题上的性能与人类无明显差异，但在处理较长案件时（1900-4000字），AI表现有所下降。同时，在早期阶段面对证据稀缺的情况下，以归纳推理为主的AI表现出优势。"}}
{"id": "2602.19910", "pdf": "https://arxiv.org/pdf/2602.19910", "abs": "https://arxiv.org/abs/2602.19910", "authors": ["Wei He", "Xianghan Meng", "Zhiyuan Huang", "Xianbiao Qi", "Rong Xiao", "Chun-Guang Li"], "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "15 pages, accepted by CVPR 2026", "summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.", "AI": {"tldr": "多模态表示学习通过半监督速率减少方法来解决广义类别发现任务。", "motivation": "现有的广义类别发现方法在处理已知和未知类别的同时识别时，依赖于跨模态对齐但忽略了模内对齐。为此提出了一种新的方法来更好地生成所需的表示结构。", "method": "通过半监督速率减少（SSR^2-GCD）框架实现多模态表示学习，并利用视觉语言模型中的跨模态对齐集成提示候选者，以增强知识迁移。", "result": "在通用和细粒度基准数据集上的实验表明该方法性能优越。", "conclusion": "所提出的方法SSR^2-GCD展示了其在解决广义类别发现任务时的出色能力。"}}
{"id": "2602.19907", "pdf": "https://arxiv.org/pdf/2602.19907", "abs": "https://arxiv.org/abs/2602.19907", "authors": ["Kiran Kokilepersaud", "Mohit Prabhushankar", "Ghassan AlRegib", "Stephanie Trejo Corona", "Charles Wykoff"], "title": "Gradient based Severity Labeling for Biomarker Classification in OCT", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at International Conference on Image Processing (ICIP) 2022", "summary": "In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.", "AI": {"tldr": "提出了一种基于梯度的严重程度标签生成方法，用于在OCT图像上进行对比学习和生物标志物分类。", "motivation": "自然图像中的对比学习通过增强来选择正负样本对，但在医学领域中，随意的增强可能破坏包含感兴趣生物标志的小局部区域。因此提出了一种基于梯度响应的方法生成严重程度标签以改进疾病检测。", "method": "使用异常检测算法的梯度响应为未标记的OCT扫描图像生成严重程度标签，并利用这些标签训练监督对比学习模型，以提高糖尿病视网膜病变关键指标的生物标志物分类准确性。", "result": "该方法使生物标志物分类准确率比自监督基线提高了最多6%。", "conclusion": "所提的方法在OCT图像上显著改善了生物标志物的分类性能，特别是在处理医学数据时可以更好地保持结构的一致性。"}}
{"id": "2602.19900", "pdf": "https://arxiv.org/pdf/2602.19900", "abs": "https://arxiv.org/abs/2602.19900", "authors": ["Junyi Wang", "Yudong Guo", "Boyang Guo", "Shengming Yang", "Juyong Zhang"], "title": "ExpPortrait: Expressive Portrait Generation via Personalized Representation", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to CVPR 2026", "summary": "While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.", "AI": {"tldr": "本文提出了一种生成高保真度个性化头部表示的方法，用于合成丰富细节的面部视频。", "motivation": "现有方法在生成具有个性化的、连贯且可控的人物肖像视频时面临挑战，因此提出了新的模型来更好地分离表情和身份特征，以准确保留人物的身份并捕捉精细的表情细节。", "method": "提出了一种高保真度个性化头部表示法，包括静态的全局几何形状和个人表达相关的动态细节，并引入了个性化的姿态与表情转移模块。利用该模型作为条件信号训练扩散变换器生成丰富细节的人物视频。", "result": "实验证明，本文方法在身份保持、表情准确性及时间稳定性方面优于现有模型，特别是在捕捉复杂运动的精细细节上表现优异。", "conclusion": "所提出的个性化头部表示和表达转移模块有效提高了肖像视频的质量，展现了生成高度逼真且个性化的面部视频的能力。"}}
{"id": "2602.19898", "pdf": "https://arxiv.org/pdf/2602.19898", "abs": "https://arxiv.org/abs/2602.19898", "authors": ["Stefan Fabian", "Aljoscha Schmidt", "Jonas Süß", "Dishant", "Aum Oza", "Oskar von Stryk"], "title": "Athena: An Autonomous Open-Hardware Tracked Rescue Robot Platform", "categories": ["cs.RO"], "comment": "https://github.com/tu-darmstadt-ros-pkg/athena", "summary": "In disaster response and situation assessment, robots have great potential in reducing the risks to the safety and health of first responders. As the situations encountered and the required capabilities of the robots deployed in such missions differ wildly and are often not known in advance, heterogeneous fleets of robots are needed to cover a wide range of mission requirements. While UAVs can quickly survey the mission environment, their ability to carry heavy payloads such as sensors and manipulators is limited. UGVs can carry required payloads to assess and manipulate the mission environment, but need to be able to deal with difficult and unstructured terrain such as rubble and stairs. The ability of tracked platforms with articulated arms (flippers) to reconfigure their geometry makes them particularly effective for navigating challenging terrain. In this paper, we present Athena, an open-hardware rescue ground robot research platform with four individually reconfigurable flippers and a reliable low-cost remote emergency stop (E-Stop) solution. A novel mounting solution using an industrial PU belt and tooth inserts allows the replacement and testing of different track profiles. The manipulator with a maximum reach of 1.54m can be used to operate doors, valves, and other objects of interest. Full CAD & PCB files, as well as all low-level software, are released as open-source contributions.", "AI": {"tldr": "本文介绍了Athena，一种自主开放硬件的履带救援机器人平台。", "motivation": "在灾害响应和情况评估中，机器人可以减少对急救人员的安全健康风险。但是由于任务环境多变且难以预知，需要不同类型的机器人来应对各种需求。传统的无人飞行器（UAVs）无法携带重型负载如传感器和操作臂，而地面车辆（UGVs）尽管能够携带所需的设备并处理复杂地形，但其灵活性有限。", "method": "Athena是一种开放硬件的救援地面机器人平台，具备四个可独立重新配置的翻转装置，并提供了一种可靠的低成本紧急停止解决方案。通过工业PU带和齿插入件实现不同的轨道轮廓替换测试。", "result": "该研究提供了完整的CAD和PCB文件以及所有低级软件作为开源贡献，使得Athena可以被广泛应用。", "conclusion": "基于本文的研究成果，Athena能够为复杂环境中的救援任务提供可靠的解决方案，并通过开放源代码促进机器人技术的进步。"}}
{"id": "2602.19896", "pdf": "https://arxiv.org/pdf/2602.19896", "abs": "https://arxiv.org/abs/2602.19896", "authors": ["Bo Jin", "Shichao Zhao", "Jin Lyu", "Bin Zhang", "Tao Yu", "Liang An", "Yebin Liu", "Meili Wang"], "title": "Monocular Mesh Recovery and Body Measurement of Female Saanen Goats", "categories": ["cs.CV"], "comment": "Accepted to AAAI2026", "summary": "The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.", "AI": {"tldr": "本文提出了一个针对雌性萨能奶山羊的单视角RGBD输入三维重建和体测量的方法。", "motivation": "为了提高乳性能评估，需要精确的3D体测数据。然而现有的重构方法缺乏专为山羊设计的真实3D数据集。", "method": "通过建立包含55只雌性萨能奶山羊的FemaleSaanenGoat数据集，并使用多视角动态融合技术，克服了快速运动和不规则表面带来的挑战，建立了准确的三维模型。基于该模型实现了单视角RGBD输入下的高精度3D重建以及六项关键体尺寸（身体长度、高度、胸宽、胸围、臀宽、臀高等）的自动化测量。", "result": "实验结果表明本文方法在3D重构和体测方面都达到了很高的精确度，为精准畜牧业中的大规模视觉应用提供了一种新的范式。", "conclusion": "该研究通过专为山羊设计的数据集和技术手段有效解决了现有三维重建技术面临的挑战，并且实现了准确的体测量。"}}
{"id": "2602.19891", "pdf": "https://arxiv.org/pdf/2602.19891", "abs": "https://arxiv.org/abs/2602.19891", "authors": ["Wen-Liang Lin", "Yun-Chien Cheng"], "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.", "AI": {"tldr": "利用无监督领域适应方法进行肺部CTA图像中的肺栓塞检测", "motivation": "为了克服深度学习在计算机辅助诊断中面临的数据域偏移和注释成本高昂的问题，提出了一种新的无监督领域自适应框架。", "method": "该方法采用Transformer骨干网络结合Mean-Teacher架构进行跨中心语义分割，并通过原型对齐机制、全局局部对比学习模块以及注意力辅助的局部预测模块提高伪标签可靠性", "result": "实验结果显示，在不同数据集上该模型显著提升了肺栓塞检测性能，特别是在无目标域标注的情况下达到了较高的Dice分数，表现出较强的泛化能力。", "conclusion": "研究提出的方法能够有效解决跨中心和多模态图像中的领域自适应问题，并展示了在实际临床环境中的应用潜力。"}}
{"id": "2602.19881", "pdf": "https://arxiv.org/pdf/2602.19881", "abs": "https://arxiv.org/abs/2602.19881", "authors": ["Blaž Rolih", "Matic Fučka", "Filip Wolf", "Luka Čehovin Zajc"], "title": "Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd", "AI": {"tldr": "该论文提出了一种名为MaSoN的无监督遥感变化检测框架，通过在训练过程中直接在潜在特征空间中生成多样化的变化来提高对各种变化类型的泛化能力。", "motivation": "当前大多数无监督变化检测方法依赖于预定义的假设关于变化类型，这限制了它们在稀有或复杂场景中的应用。为了克服这一问题，论文提出了MaSoN框架以实现多样化的数据驱动变化生成。", "method": "MaSoN通过直接在训练过程中利用目标数据的特征统计信息动态估计变化，在潜在空间中合成多样化的变化，从而提高对各种变化类型的适应性，并且可以轻松扩展到新模态如SAR图像。", "result": "该方法在五个基准测试上实现了最先进的性能，平均F1分数提高了14.1个百分点。", "conclusion": "MaSoN通过改进的无监督学习策略解决了现有技术对于变化类型假设的依赖问题，并且展示了更强的变化泛化能力和更好的检测性能。"}}
{"id": "2602.19879", "pdf": "https://arxiv.org/pdf/2602.19879", "abs": "https://arxiv.org/abs/2602.19879", "authors": ["Paul Paschmanns", "Vera Traub"], "title": "The Bidirected Cut Relaxation for Steiner Tree: Better Integrality Gap Bounds and the Limits of Moat Growing", "categories": ["cs.DS"], "comment": null, "summary": "The Steiner Tree problem asks for the cheapest way of connecting a given subset of the vertices in an undirected graph. One of the most prominent linear programming relaxations for Steiner Tree is the Bidirected Cut Relaxation (BCR). Determining the integrality gap of this relaxation is a long-standing open question. For several decades, the best known upper bound was 2, which is achievable by standard techniques. Only very recently, Byrka, Grandoni, and Traub [FOCS 2024] showed that the integrality gap of BCR is strictly below 2. We prove that the integrality gap of BCR is at most 1.898, improving significantly on the previous bound of 1.9988. For the important special case where a terminal minimum spanning tree is an optimal Steiner tree, we show that the integrality gap is at most 12/7, by providing a tight analysis of the dual-growth procedure by Byrka et al. To obtain the general bound of 1.898 on the integrality gap, we generalize their dual growth procedure to a broad class of moat-growing algorithms. Moreover, we prove that no such moat-growing algorithm yields dual solutions certifying an integrality gap below 12/7. Finally, we observe an interesting connection to the Hypergraphic Relaxation.", "AI": {"tldr": "本文研究了Steiner树问题的双向割松弛（BCR）线性规划放松方案，证明了其积分间隙最多为1.898，并在特定情况下提供了更紧致的界值分析。", "motivation": "确定双向割松弛的积分间隙是一个长期未解决的问题，该问题是Steiner树问题中的一个重要研究方向。", "method": "通过扩展Byrka等人提出的对偶生长过程来获得一般情况下的界限1.898，并证明了对于特定情况的最优解可以提供更紧致的界值分析。", "result": "本文改进了双向割松弛积分间隙的最佳已知上界，将其从1.9988减小到1.898，并在终端最小生成树是最佳Steiner树的情况下提供了更紧密的界限（最多为12/7）。", "conclusion": "提出了新的方法来优化双向割松弛的积分间隙，且证明了某些算法的局限性。"}}
{"id": "2602.19874", "pdf": "https://arxiv.org/pdf/2602.19874", "abs": "https://arxiv.org/abs/2602.19874", "authors": ["Lucas Martini", "Alexander Lappe", "Anna Bognár", "Rufin Vogels", "Martin A. Giese"], "title": "BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations", "categories": ["cs.CV"], "comment": "ef:International Conference on Learning Representations (ICLR), 2026", "summary": "The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\\textbf{Big Ma}$ca$\\textbf{Q}$ue 3D Motion and Animation Dataset ($\\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .", "AI": {"tldr": "构建了一个大型的猕猴动态行为和动画数据集BigMaQ，该数据集包含超过750个场景的互动猕猴，并提供详细的三维姿态描述。", "motivation": "为了改善动物行为识别中的三维姿态和形状重建问题，特别是对于非人灵长类动物，现有的基于表面的方法无法准确捕捉动作动态的丰富性。BigMaQ旨在填补这一空白，通过引入更精确的姿态描述来提高行为识别精度。", "method": "采用高质量猕猴模板网格并适应到个体猴子上创建个性化的纹理角色，并从原始数据集中派生出一个行动识别基准BigMaQ500，将表面姿态向量与不同个体猴子的单帧进行配对。通过使用包括和不包括姿态描述符的特征提取方法展示了改进。", "result": "在引入三维姿态信息后，在平均精度(mAP)方面取得了显著改善。", "conclusion": "BigMaQ是第一个将动态三维姿态形状表示整合到动物行为识别学习任务中的数据集，它为非人灵长类视觉外观、姿势和社会互动的研究提供了丰富资源。"}}
{"id": "2602.19873", "pdf": "https://arxiv.org/pdf/2602.19873", "abs": "https://arxiv.org/abs/2602.19873", "authors": ["Felix Thaler", "Sebastian Keller"], "title": "GPU-Native Compressed Neighbor Lists with a Space-Filling-Curve Data Layout", "categories": ["cs.CE", "astro-ph.IM", "cs.DS"], "comment": "Accepted at IPDPS 2026", "summary": "We have developed a compressed neighbor list for short-range particle-particle interaction based on a space- filling curve (SFC) memory layout and particle clusters. The neighbor list can be constructed efficiently on GPUs, supporting NVIDIA and AMD hardware, and has a memory footprint of only 4 bytes per particle to store approximately 200 neighbors. Compared to the highly-optimized domain-specific neighbor list implementation of GROMACS, a molecular dynamics code, it has a comparable cluster overhead and delivers similar performance in a neighborhood pass. Thanks to the SFC-based data layout and the support for varying interaction radii per particle, our neighbor list performs well for systems with high density contrasts, such as those encountered in many astrophysical and cosmological applications. Due to the close relation between SFCs and octrees, our neighbor list seamlessly integrates with octree-based domain decomposition and multipole-based methods for long-range gravitational or electrostatic interactions. To demonstrate the coupling between long- and short-range forces, we simulate an Evrard collapse, a standard test case for the coupling between hydrodynamical and gravitational forces, on up to 1024 GPUs, and compare our results to the analytical solution.", "AI": {"tldr": "开发了一种基于空间填充曲线的压缩邻居列表，用于粒子间短程相互作用，并在GPU上高效构建。", "motivation": "提高分子动力学模拟中的短程粒子-粒子交互性能，特别是在处理密度对比度高的系统时。", "method": "利用空间填充曲线(SFC)数据布局和颗粒簇开发了一种压缩邻居列表，支持不同粒子间相互作用半径，并在GPU上构建。", "result": "该邻居列表与GROMACS的优化领域特定实现相比，在集群开销和邻近性传递性能方面表现相当。适用于密度对比度高的系统，如天体物理和宇宙学中的应用。", "conclusion": "展示了短程和长程力之间的耦合，并在多达1024个GPU上模拟了Evrard坍缩，结果与理论解一致。"}}
{"id": "2602.19872", "pdf": "https://arxiv.org/pdf/2602.19872", "abs": "https://arxiv.org/abs/2602.19872", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Shaokun Wang", "Qiang Wang", "Yihong Gong"], "title": "GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery", "categories": ["cs.CV", "cs.AI"], "comment": "Accept by AAAI 2026", "summary": "Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.", "AI": {"tldr": "提出了一种名为GOAL的框架，用于在持续学习过程中发现新的类别并保留已知类别的知识。", "motivation": "现有方法通过动态更新分类器权重来解决持续广义类别发现问题，但这会导致遗忘和不一致的特征对齐。因此需要一种能够保持一致几何结构的方法以稳定地集成新类别而不破坏旧类别。", "method": "GOAL引入了一个固定等角紧框架（ETF）分类器，在整个学习过程中强制执行一致的几何结构，并通过监督对齐处理已标记样本，通过置信度引导对齐处理新型样本。", "result": "在四个基准数据集上的实验表明，GOAL优于先前的方法Happy，减少了遗忘16.1%，提高了新类发现3.2%。", "conclusion": "GOAL为长期持续发现提供了强大的解决方案。"}}
{"id": "2602.19870", "pdf": "https://arxiv.org/pdf/2602.19870", "abs": "https://arxiv.org/abs/2602.19870", "authors": ["Qiankun Ma", "Ziyao Zhang", "Haofei Wang", "Jie Chen", "Zhen Song", "Hairong Zheng"], "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs", "categories": ["cs.CV"], "comment": "CVPR2026", "summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.", "AI": {"tldr": "ApET通过线性近似重构原始视觉令牌，并利用近似误差识别和丢弃最少信息的令牌，实现了无注意力依赖的视觉令牌压缩。", "motivation": "现有的多模态理解模型由于冗余视觉令牌导致计算开销大且推理效率低。传统的解决方案容易引入位置偏差并且与高效的注意内核不兼容，限制了其在VLM加速中的应用。", "method": "ApET首先使用线性近似重构原始视觉令牌，并通过近似误差识别和丢弃最少信息的令牌。这种方法无须依赖注意力机制，能够更好地与其他高效注意力内核结合。", "result": "实验表明，ApET能够在压缩88.9%和87.5%令牌预算的同时，保留图像理解任务中原始性能的95.2%，并在视频理解任务中达到104%的表现。此外，由于其无注意力依赖的设计，ApET能够与FlashAttention无缝集成，进一步加速推理。", "conclusion": "ApET通过信息理论的角度重新审视视觉令牌压缩问题，提供了一种无需依赖注意机制的解决方案，并在保持高性能的同时显著减少了令牌预算和计算成本，提高了模型部署的实际可行性。"}}
{"id": "2602.19863", "pdf": "https://arxiv.org/pdf/2602.19863", "abs": "https://arxiv.org/abs/2602.19863", "authors": ["Filip Wolf", "Blaž Rolih", "Luka Čehovin Zajc"], "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026", "summary": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Project page: \\textcolor{magenta}{https://wolfilip.github.io/DEO/}.", "AI": {"tldr": "提出了一种双教师对比蒸馏框架，用于多光谱图像预训练，实现了跨模态表示学习。", "motivation": "现有的EO预训练方法主要依赖掩码图像建模，强调局部重建但对全局语义结构控制有限。为此，论文旨在开发一种新的预训练方案，以解决多传感器和多模态数据的高效知识转移问题。", "method": "引入了一种双教师对比蒸馏框架，结合了多光谱教师和光学VFM教师，使学生模型能够学习到更加一致的跨模态表示。该方法在对齐学生的预训练目标的同时，借鉴了现代光学视觉基础模型中的对比自我蒸馏范式。", "result": "实验结果表明，在多种光学和多光谱基准上，所提模型能够在适应多光谱数据同时保持其在光学图像上的性能，实现了3.64个百分点的语义分割平均提升、1.2个百分点的变化检测提升以及1.31个百分点的分类任务提升。", "conclusion": "对比蒸馏提供了一种原则性和高效的方法来实现异构EO数据源之间的可扩展表示学习。"}}
{"id": "2602.19862", "pdf": "https://arxiv.org/pdf/2602.19862", "abs": "https://arxiv.org/abs/2602.19862", "authors": ["Lars Fischer", "Daniel Flögel", "Sören Hohmann"], "title": "Rendezvous and Docking of Mobile Ground Robots for Efficient Transportation Systems", "categories": ["eess.SY", "cs.RO"], "comment": "8 pages, conference paper", "summary": "In-Motion physical coupling of multiple mobile ground robots has the potential to enable new applications like in-motion transfer that improves efficiency in handling and transferring goods, which tackles current challenges in logistics. A key challenge lies in achieving reliable autonomous in-motion physical coupling of two mobile ground robots starting at any initial position. Existing approaches neglect the modeling of the docking interface and the strategy for approaching it, resulting in uncontrolled collisions that make in-motion physical coupling either impossible or inefficient. To address this challenge, we propose a central mpc approach that explicitly models the dynamics and states of two omnidirectional wheeled robots, incorporates constraints related to their docking interface, and implements an approaching strategy for rendezvous and docking. This novel approach enables omnidirectional wheeled robots with a docking interface to physically couple in motion regardless of their initial position. In addition, it makes in-motion transfer possible, which is 19.75% more time- and 21.04% energy-efficient compared to a non-coupling approach in a logistic scenario.", "AI": {"tldr": "本文提出了一种基于中央mpc的方法，用于实现两个全向轮式机器人在移动状态下的物理耦合。", "motivation": "现有的方法忽略了对接接口的建模和接近策略，导致无法实现可靠的自主移动物理耦合。为了解决这个问题，作者提出了一个新的解决方案来提高物流效率。", "method": "本文提出了一种基于中央mpc的方法，该方法明确地模拟了两个全向轮式机器人的动态特性和状态，并且集成了与对接接口相关的约束条件以及一种接近策略以实现会合和对接。", "result": "实验结果表明，在物流场景中，所提出的方案可以节省19.75%的时间和21.04%的能量。", "conclusion": "本文提出的方法使得全向轮式机器人即使从任意初始位置开始也可以在移动状态下实现物理耦合，并且能够进行移动转移任务。"}}
{"id": "2602.19857", "pdf": "https://arxiv.org/pdf/2602.19857", "abs": "https://arxiv.org/abs/2602.19857", "authors": ["Rodrigo Mota", "Kelvin Cunha", "Emanoel dos Santos", "Fábio Papais", "Francisco Filho", "Thales Bezerra", "Erico Medeiros", "Paulo Borba", "Tsang Ing Ren"], "title": "Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions", "categories": ["cs.CV"], "comment": "4 pages, 5 figures, 1 table, isbi2026", "summary": "Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.", "AI": {"tldr": "本文提出了一种对比元领域适应策略，用于增强皮肤病损图像分类的鲁棒性。", "motivation": "深度学习模型在皮肤病变图像分析中对采集变异性敏感，导致其在临床环境中的性能下降。通过研究视觉伪影和领域转换如何影响基于深度学习的皮肤病变分类，提高模型的泛化能力是该论文的主要动机。", "method": "提出了一种元领域适应策略，利用较大的皮肤病损数据集将视觉表示转移到临床图像领域中，从而提高了鲁棒性。", "result": "在多个皮肤病损数据集中进行的实验表明，在皮肤病变分类性能方面取得了持续改善，并且减少了皮肤镜和临床图像之间的差距。", "conclusion": "研究强调了对可部署系统进行域感知训练的重要性。"}}
{"id": "2602.19853", "pdf": "https://arxiv.org/pdf/2602.19853", "abs": "https://arxiv.org/abs/2602.19853", "authors": ["Leni Yang", "Aymeric Ferron", "Yvonne Jansen", "Pierre Dragicevic"], "title": "Progressive Value Reading: The Use of Motion to Gradually Examine Data Involving Large Magnitudes", "categories": ["cs.HC"], "comment": null, "summary": "People often struggle to interpret data with extremely large or small values, or ranges spanning multiple orders of magnitude. While traditional approaches, such as log scales and multiscale visualizations, can help, we explore in this article a different approach used in some emerging designs: the use of motion to let viewers gradually experience magnitude -- for example, interactive graphics that require long scrolling or street paintings stretching hundreds of meters. This approach typically demands substantial time and sustained interaction, translating differences in magnitude into a visceral sense of duration and effort. Although largely underexplored, this design strategy offers new opportunities. We introduce the term progressive value reading to refer to the use of motion to progressively examine an information object that encodes a value, where the amount of motion reflects the value. We compiled a corpus of 55 real-life and hypothetical visualization examples that allow, encourage, or require progressive value reading. From this corpus, we derived a design space of ten design dimensions, providing a shared vocabulary, inspiration for novel techniques, and a foundation for empirical evaluation. An online corpus is also available for exploration.", "AI": {"tldr": "本文探讨了一种新的数据可视化方法，即通过运动来逐步体验数值大小的变化，提出了“渐进值阅读”的概念，并总结了十个设计维度。", "motivation": "人们在解释极大规模或小规模的数据时经常遇到困难。传统的对数尺度和多尺度可视化虽然有所帮助，但本文探讨了一种新的策略：使用运动让观众逐步体验数据的大小变化。", "method": "作者通过收集55个现实生活中和假设的例子来说明渐进值阅读的应用，并从中总结出了十个设计维度。", "result": "提出了一个涵盖渐进值读取的设计空间，包括十个设计维度。", "conclusion": "文章提供了一个新的术语和方法论基础，即“渐进值阅读”，为未来的研究提供了启发和灵感。"}}
{"id": "2602.19850", "pdf": "https://arxiv.org/pdf/2602.19850", "abs": "https://arxiv.org/abs/2602.19850", "authors": ["Junhui Lee", "Hyosung Kim", "Saekwang Nam"], "title": "TactiVerse: Generalizing Multi-Point Tactile Sensing in Soft Robotics Using Single-Point Data", "categories": ["cs.RO"], "comment": "6 pages, 7 figures, accepted at 9th IEEE-RAS International Conference on Soft Robotics (RoboSoft)", "summary": "Real-time prediction of deformation in highly compliant soft materials remains a significant challenge in soft robotics. While vision-based soft tactile sensors can track internal marker displacements, learning-based models for 3D contact estimation heavily depend on their training datasets, inherently limiting their ability to generalize to complex scenarios such as multi-point sensing. To address this limitation, we introduce TactiVerse, a U-Net-based framework that formulates contact geometry estimation as a spatial heatmap prediction task. Even when trained exclusively on a limited dataset of single-point indentations, our architecture achieves highly accurate single-point sensing, yielding a superior mean absolute error of 0.0589 mm compared to the 0.0612 mm of a conventional regression-based CNN baseline. Furthermore, we demonstrate that augmenting the training dataset with multi-point contact data substantially enhances the sensor's multi-point sensing capabilities, significantly improving the overall mean MAE for two-point discrimination from 1.214 mm to 0.383 mm. By successfully extrapolating complex contact geometries from fundamental interactions, this methodology unlocks advanced multi-point and large-area shape sensing. Ultimately, it significantly streamlines the development of marker-based soft sensors, offering a highly scalable solution for real-world tactile perception.", "AI": {"tldr": "本文提出了一种基于U-Net框架的TactiVerse，通过单点数据训练来实现多点触觉感知。", "motivation": "在软体机器人中，实时预测高度柔性的材料变形是一个重要挑战。现有基于学习的方法依赖于大量特定数据集进行3D接触估计，难以泛化到复杂的多点传感场景。", "method": "TactiVerse将接触几何估计作为空间热图预测任务，并使用U-Net架构通过单点数据训练模型来实现高精度的单点感知。在引入多点触觉数据后，提升了模型的多点感知能力。", "result": "该方法在单点传感上实现了0.0589毫米的平均绝对误差，显著优于常规CNN基线的0.0612毫米；对于两点区分任务，整体均方差从1.214毫米提升至0.383毫米。", "conclusion": "TactiVerse通过单点数据训练模型并利用热图预测方法，成功实现了多点和大面积形状感知的泛化。该技术为软体传感器的发展提供了一种高效的解决方案。"}}
{"id": "2602.19848", "pdf": "https://arxiv.org/pdf/2602.19848", "abs": "https://arxiv.org/abs/2602.19848", "authors": ["Francisco Filho", "Kelvin Cunha", "Fábio Papais", "Emanoel dos Santos", "Rodrigo Mota", "Thales Bezerra", "Erico Medeiros", "Paulo Borba", "Tsang Ing Ren"], "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation", "categories": ["cs.CV"], "comment": "4 pages, 2 figures, 1 table, isbi2026", "summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.", "AI": {"tldr": "通过生成合成皮肤病变图像并应用MAE蒸馏，改进皮肤病变分类性能。", "motivation": "针对皮肤病变数据集中的严重类别不平衡问题，提出一种方法以改善深度学习训练中偏斜的决策边界。", "method": "使用类条件扩散模型生成合成皮肤病学图像，并利用无监督MAE预训练使大型ViT模型能够学习稳健且领域相关的特征；随后通过知识蒸馏将这些表示转移到适合移动设备的小型ViT学生模型上。", "result": "实验结果显示，结合合成数据上的MAE预训练和蒸馏技术可以提高分类性能并实现高效的设备推理。", "conclusion": "该方法不仅提高了皮肤病变分类的准确率，还支持在实际临床环境中部署轻量级模型。"}}
{"id": "2602.19844", "pdf": "https://arxiv.org/pdf/2602.19844", "abs": "https://arxiv.org/abs/2602.19844", "authors": ["Yedi Zhang", "Haoyu Wang", "Xianglin Yang", "Jin Song Dong", "Jun Sun"], "title": "LLM-enabled Applications Require System-Level Threat Monitoring", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "26 pages", "summary": "LLM-enabled applications are rapidly reshaping the software ecosystem by using large language models as core reasoning components for complex task execution. This paradigm shift, however, introduces fundamentally new reliability challenges and significantly expands the security attack surface, due to the non-deterministic, learning-driven, and difficult-to-verify nature of LLM behavior. In light of these emerging and unavoidable safety challenges, we argue that such risks should be treated as expected operational conditions rather than exceptional events, necessitating a dedicated incident-response perspective. Consequently, the primary barrier to trustworthy deployment is not further improving model capability but establishing system-level threat monitoring mechanisms that can detect and contextualize security-relevant anomalies after deployment -- an aspect largely underexplored beyond testing or guardrail-based defenses. Accordingly, this position paper advocates systematic and comprehensive monitoring of security threats in LLM-enabled applications as a prerequisite for reliable operation and a foundation for dedicated incident-response frameworks.", "AI": {"tldr": "大型语言模型在应用中的使用带来了新的可靠性挑战，需要系统级别的威胁监控来保障安全。", "motivation": "由于LLM行为的不确定性和难以验证性，导致了软件生态系统的安全攻击面扩大。因此，需要一种专门的安全监测机制以应对这些新的风险。", "method": "提出在LLM应用中建立全面的安全威胁监测系统作为可靠运行的前提，并为紧急响应框架奠定基础。", "result": "未明确指出具体结果，强调需要一个完整的系统级威胁监控方法来确保安全。", "conclusion": "为了应对由大型语言模型带来的新挑战，必须采取系统级别的威胁监控措施以保障LLM应用的安全和可靠。"}}
{"id": "2602.19843", "pdf": "https://arxiv.org/pdf/2602.19843", "abs": "https://arxiv.org/abs/2602.19843", "authors": ["Jin Jia", "Zhiling Deng", "Zhuangbin Chen", "Yingqi Wang", "Zibin Zheng"], "title": "MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.", "AI": {"tldr": "提出了一种用于评估LLM基于的多代理系统的可靠性的框架MAS-FIRE。", "motivation": "为了应对复杂任务中LLM基础多代理系统可能出现的语义故障和协调失败，现有的评估方法仅关注任务的成功而不能揭示这些问题的原因及恢复效果。", "method": "定义了15种故障类型并采用非侵入性机制（提示修改、响应重写、消息路由操控）进行故障注入；在三种代表性多代理系统架构上应用MAS-FIRE。", "result": "发现了多个层级的容错行为，并揭示了更强的基础模型并不一定提高鲁棒性，而特定的拓扑结构可以大幅降低崩溃风险。", "conclusion": "MAS-FIRE提供了过程级可观察性和操作指南，有助于系统地改进多代理系统的可靠性。"}}
{"id": "2602.19837", "pdf": "https://arxiv.org/pdf/2602.19837", "abs": "https://arxiv.org/abs/2602.19837", "authors": ["Björn Hoppmann", "Christoph Scholz"], "title": "Meta-Learning and Meta-Reinforcement Learning - Tracing the Path towards DeepMind's Adaptive Agent", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Humans are highly effective at utilizing prior knowledge to adapt to novel tasks, a capability that standard machine learning models struggle to replicate due to their reliance on task-specific training. Meta-learning overcomes this limitation by allowing models to acquire transferable knowledge from various tasks, enabling rapid adaptation to new challenges with minimal data. This survey provides a rigorous, task-based formalization of meta-learning and meta-reinforcement learning and uses that paradigm to chronicle the landmark algorithms that paved the way for DeepMind's Adaptive Agent, consolidating the essential concepts needed to understand the Adaptive Agent and other generalist approaches.", "AI": {"tldr": "本文通过任务形式化元学习和元强化学习，回顾了引领DeepMind自适应代理的关键算法。", "motivation": "人类能够有效利用先前知识快速适应新任务，而标准机器学习模型因依赖特定任务训练难以复制此能力。元学习旨在克服这一局限性，使模型从多种任务中获取可转移的知识，从而实现对新挑战的快速适应。", "method": "本文提供了一个严格的任务形式化方法来定义元学习和元强化学习，并使用该框架回顾了关键算法的发展历程。", "result": "文章总结并明确了理解DeepMind自适应代理和其他通用方法所需的关键概念。", "conclusion": "通过综合重要的理论进展，本文为理解和设计具有高度适应性的智能体提供了一个坚实的基础。"}}
{"id": "2602.19834", "pdf": "https://arxiv.org/pdf/2602.19834", "abs": "https://arxiv.org/abs/2602.19834", "authors": ["Gero Ellmies", "Till Fluschnik"], "title": "Placing Green Bridges Optimally for Robust Habitat Reconnection", "categories": ["cs.DS", "cs.DM"], "comment": null, "summary": "We study the problem of robustly reconnecting habitats via the placement of green bridges at minimum total cost. Habitats are fragmented into patches and we seek to reconnect each habitat such that it remains connected even if any of its patches becomes unavailable. Formally, we are given an undirected graph with edge costs, a set of fixed green bridges represented as a subset of the graph's edges, a set of habitats represented as vertex subsets, and some budget. We decide whether there exists a subset of the graph's edges containing all fixed green bridges such that, for each habitat, the induced subgraph on the solution edges is 2-vertex-connected, and the total cost does not exceed the budget. We also study the 2-edge-connectivity variant, modeling the case where any single reconnecting green bridge may fail. We analyze the computational complexity of these problems, focusing on the boundary between NP-hardness and polynomial-time solvability when the maximum habitat size and maximum vertex degree are bounded by constants. We prove that for each constant maximum habitat size of at least four there exists a small constant maximum degree for which the problems are NP-hard, and complement this with polynomial-time algorithms yielding partial dichotomies for bounded habitat size and degree.", "AI": {"tldr": "研究如何通过放置绿色桥接物以最小的总成本来优化连接破碎栖息地的问题", "motivation": "探讨在某些栖息地碎片化的情况下，通过构建绿色桥接物实现生态连通性的有效策略。目标是在预算范围内找到一种方法，使得每个栖息地即使失去一个片段仍然保持连接状态", "method": "给定无向图及其边成本、一组固定的绿色桥接物作为图的子集边缘和一组栖息地作为顶点子集，决定是否存在一个包含所有固定绿色桥接物的子集。该子集使得每个栖息地在移除任意片段后仍保持2-节点连通，并且总成本不超过预算", "result": "证明了对于最大栖息地大小至少为四的情况下存在一个小的最大度数使问题NP难解，同时给出了部分二值性定理以解决有界栖息地规模和度的问题。", "conclusion": "通过探讨不同条件下的复杂性和算法设计，为优化绿色桥接物的放置提供了理论基础"}}
{"id": "2602.19832", "pdf": "https://arxiv.org/pdf/2602.19832", "abs": "https://arxiv.org/abs/2602.19832", "authors": ["Penghui Niu", "Taotao Cai", "Suqi Zhang", "Junhua Gu", "Ping Zhang", "Qiqi Liu", "Jianxin Li"], "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting", "categories": ["cs.CV"], "comment": null, "summary": "The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.", "AI": {"tldr": "M3S-Net是一种基于多尺度数据的多模态特征融合网络，用于超短期光伏发电预测。", "motivation": "为了克服现有模型在捕获细粒度光学特征和复杂时空耦合方面的局限性，提出了M3S-Net来提高光伏功率预测精度。", "method": "该方法首先使用部分卷积隔离薄云边界，接着采用FFT时间频率表示分析气象数据，并通过动态C矩阵交换机制实现跨模态深度耦合。", "result": "实验结果表明，与现有最佳基线相比，M3S-Net在10分钟预测中平均绝对误差降低了6.2%。", "conclusion": "M3S-Net成功提升了光伏功率的超短期预测精度，克服了浅层特征连接和二值化云分割的限制。"}}
{"id": "2602.19828", "pdf": "https://arxiv.org/pdf/2602.19828", "abs": "https://arxiv.org/abs/2602.19828", "authors": ["Chenfan Qu", "Yiwu Zhong", "Jian Liu", "Xuekang Zhu", "Bohan Yu", "Lianwen Jin"], "title": "TextShield-R1: Reinforced Reasoning for Tampered Text Detection", "categories": ["cs.CV"], "comment": "AAAI 2026", "summary": "The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.", "AI": {"tldr": "本文提出了一种基于强化学习的多模态大型语言模型解决方案TextShield-R1，用于检测和解释篡改文本。", "motivation": "随着篡改图像数量的增加，迫切需要可靠的检测方法。现有模型在微级别特征识别、局部区域定位以及依赖昂贵注释方面仍存在局限性。", "method": "提出了一种强化学习的方法TextShield-R1，包括使用大规模自然图像取证和OCR任务数据进行连续预训练；通过组相对策略优化减少对标注的依赖并提升推理能力；在推断阶段利用OCR校正提高定位准确率，并提出了一个包含45,000多张真实与篡改图像的新基准TFR。", "result": "实验表明，TextShield-R1显著提高了可解释性篡改文本检测的性能。", "conclusion": "研究证明了强化学习方法在解决篡改文本检测问题上的优势，并通过新基准TFR提供了全面评估。"}}
{"id": "2602.19825", "pdf": "https://arxiv.org/pdf/2602.19825", "abs": "https://arxiv.org/abs/2602.19825", "authors": ["Shihong Tan", "Haoyu Wang", "Youran Ni", "Yingzhao Hou", "Jiayue Luo", "Zipei Hu", "Han Dou", "Zerui Han", "Ningning Pan", "Yuzhu Wang", "Gongping Huang"], "title": "DTT-BSR: GAN-based DTTNet with RoPE Transformer Enhancement for Music Source Restoration", "categories": ["eess.AS", "cs.SD"], "comment": null, "summary": "Music source restoration (MSR) aims to recover unprocessed stems from mixed and mastered recordings. The challenge lies in both separating overlapping sources and reconstructing signals degraded by production effects such as compression and reverberation. We therefore propose DTT-BSR, a hybrid generative adversarial network (GAN) combining rotary positional embeddings (RoPE) transformer for long-term temporal modeling with dual-path band-split recurrent neural network (RNN) for multi-resolution spectral processing. Our model achieved 3rd place on the objective leaderboard and 4th place on the subjective leaderboard on the ICASSP 2026 MSR Challenge, demonstrating exceptional generation fidelity and semantic alignment with a compact size of 7.1M parameters.", "AI": {"tldr": "提出了一种结合旋转位置嵌入（RoPE）变压器和双路径带分频递归神经网络（RNN）的混合生成对抗网络（GAN），用于音乐源恢复。", "motivation": "为了从混音和母带录音中恢复未处理的声音来源，解决信号重建中的重叠来源分离和生产效果导致的退化问题。", "method": "使用RoPE变压器进行长期时序建模，以及双路径带分频RNN进行多分辨率频谱处理，构建了一种新的混合GAN模型DTT-BSR。", "result": "在ICASSP 2026音乐源恢复挑战赛中获得客观排行榜第三名和主观排行榜第四名的成绩，证明了模型的生成保真度与语义对齐能力，并且具有7.1M参数的小型化优势。", "conclusion": "所提出的DTT-BSR模型在解决复杂生产效果导致的声音退化问题上展现了优异性能。"}}
{"id": "2602.19823", "pdf": "https://arxiv.org/pdf/2602.19823", "abs": "https://arxiv.org/abs/2602.19823", "authors": ["Keno Moenck", "Adrian Philip Florea", "Julian Koch", "Thorsten Schüppstuhl"], "title": "Open-vocabulary 3D scene perception in industrial environments", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM \"IndustrialCLIP\" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.", "AI": {"tldr": "论文主要任务是在工业环境中实现开放词汇表的三维场景感知。", "motivation": "当前的方法依赖于非工业化数据集训练的模型，这些模型在面对特定的工业物体时表现不佳。因此需要一种新的方法来解决这个问题。", "method": "提出了一种无需训练的开放词汇表3D感知流水线，通过合并预计算的超级点生成掩码，同时评估了域适应VLFM“IndustrialCLIP”在代表性的3D工业车间场景中的表现。", "result": "定性结果表明可以成功分割工业物体，证明了该方法的有效性。", "conclusion": "论文提出的方法克服了现有模型的局限性，并展示了其在开放词汇表查询中对三维工业场景的成功分割能力。"}}
{"id": "2602.19822", "pdf": "https://arxiv.org/pdf/2602.19822", "abs": "https://arxiv.org/abs/2602.19822", "authors": ["Dongjing Shan", "Yamei Luo", "Jiqing Xuan", "Lu Huang", "Jin Li", "Mengchu Yang", "Zeyu Chen", "Fajin Lv", "Yong Tang", "Chunxiang Zhang"], "title": "Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\\%, a specificity of 97.2\\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.", "AI": {"tldr": "该论文提出了一种高效的两阶段深度学习框架，用于子宫内膜癌的筛查。", "motivation": "早期检测子宫肌层浸润对子宫内膜癌的分期和治疗至关重要。但经阴道超声诊断受低组织对比度、操作者依赖性和病理样本稀缺的影响很大。", "method": "该论文开发了一种结构导向跨模态生成网络，从未配对的MRI数据中合成高保真度的超声图像，并引入了利用梯度蒸馏的知识驱动轻量级筛查网络。", "result": "在7,951名参与者的大规模多中心队列中，模型达到99.5%的敏感性、97.2%的特异性和0.987的曲线下面积，并且计算成本低（0.289 GFLOPs）。", "conclusion": "结合跨模态合成增广和知识驱动的有效建模可以为资源受限的一线医疗机构提供专家级别的实时癌症筛查。"}}
{"id": "2602.19818", "pdf": "https://arxiv.org/pdf/2602.19818", "abs": "https://arxiv.org/abs/2602.19818", "authors": ["Hillel Ohayon", "Daniel Gilkarov", "Ran Dubin"], "title": "SafePickle: Robust and Generic ML Detection of Malicious Pickle-based ML Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Model repositories such as Hugging Face increasingly distribute machine learning artifacts serialized with Python's pickle format, exposing users to remote code execution (RCE) risks during model loading. Recent defenses, such as PickleBall, rely on per-library policy synthesis that requires complex system setups and verified benign models, which limits scalability and generalization. In this work, we propose a lightweight, machine-learning-based scanner that detects malicious Pickle-based files without policy generation or code instrumentation. Our approach statically extracts structural and semantic features from Pickle bytecode and applies supervised and unsupervised models to classify files as benign or malicious. We construct and release a labeled dataset of 727 Pickle-based files from Hugging Face and evaluate our models on four datasets: our own, PickleBall (out-of-distribution), Hide-and-Seek (9 advanced evasive malicious models), and synthetic joblib files. Our method achieves 90.01% F1-score compared with 7.23%-62.75% achieved by the SOTA scanners (Modelscan, Fickling, ClamAV, VirusTotal) on our dataset. Furthermore, on the PickleBall data (OOD), it achieves 81.22% F1-score compared with 76.09% achieved by the PickleBall method, while remaining fully library-agnostic. Finally, we show that our method is the only one to correctly parse and classify 9/9 evasive Hide-and-Seek malicious models specially crafted to evade scanners. This demonstrates that data-driven detection can effectively and generically mitigate Pickle-based model file attacks.", "AI": {"tldr": "提出了一种轻量级的机器学习扫描器，用于检测恶意的Pickle文件，通过静态提取特征并应用监督和无监督模型进行分类。", "motivation": "现有防御方法如PickelBall依赖于复杂的系统设置及验证良性模型，限制了可扩展性和泛化能力。旨在开发一种无需策略生成或代码插桩的轻量级解决方案来检测恶意Pickle文件。", "method": "静态提取Pickle字节码中的结构和语义特征，并应用监督与无监督机器学习模型进行分类。构建并发布了来自Hugging Face的727个标签化的Pickle文件数据集，评估了四种不同类型的数据集上的效果。", "result": "在自建数据集上达到90.01% F1-score，在PickleBall（OOD）数据集上达81.22%，优于现有最佳方法。唯一能够正确解析和分类所有9个专门设计以逃避扫描器的恶意模型。", "conclusion": "证明了基于数据驱动的方法可以有效且通用地缓解Pickle文件攻击，是检测恶意机器学习模型的有效手段。"}}
{"id": "2602.19816", "pdf": "https://arxiv.org/pdf/2602.19816", "abs": "https://arxiv.org/abs/2602.19816", "authors": ["Yungang Yi"], "title": "Depth-Structured Music Recurrence: Budgeted Recurrent Attention for Full-Piece Symbolic Music Modeling", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context modeling is essential for symbolic music generation, since motif repetition and developmental variation can span thousands of musical events. However, practical composition and performance workflows frequently rely on resource-limited devices (e.g., electronic instruments and portable computers), making heavy memory and attention computation difficult to deploy. We introduce Depth-Structured Music Recurrence (DSMR), a recurrent long-context Transformer for full-piece symbolic music modeling that extends context beyond fixed-length excerpts via segment-level recurrence with detached cross-segment states, featuring a layer-wise memory-horizon schedule that budgets recurrent KV states across depth. DSMR is trained in a single left-to-right pass over each complete composition, akin to how a musician experiences it from beginning to end, while carrying recurrent cross-segment states forward. Within this recurrent framework, we systematically study how depth-wise horizon allocations affect optimization, best-checkpoint perplexity, and efficiency. By allocating different history-window lengths across layers while keeping the total recurrent-state budget fixed, DSMR creates depth-dependent temporal receptive fields within a recurrent attention stack without reducing compute depth. Our main instantiation is a two-scale DSMR schedule that allocates long history windows to lower layers and a uniform short window to the remaining layers. Experiments on the piano performance dataset MAESTRO demonstrate that two-scale DSMR provides a practical quality--efficiency recipe for full-length long-context symbolic music modeling with recurrent attention under limited computational resources.", "AI": {"tldr": "介绍了一种名为深度结构化音乐递归（DSMR）的模型，用于长上下文符号音乐生成。", "motivation": "由于资源限制设备难以部署大规模内存和注意力计算，因此提出了DSMR以解决长时间上下文建模的问题。", "method": "通过分层记忆视图调度，在不减少计算深度的情况下创建层次依赖的时间感受域，并且在实验中采用两尺度的DSMR方案进行优化。", "result": "在钢琴表演数据集MAESTRO上的实验表明，双尺度DSMR为长上下文符号音乐建模提供了一种实际的质量-效率配方。", "conclusion": "DSMR能够有效地解决长时间上下文音乐生成问题，并且在资源有限的情况下保持高质量的生成效果。"}}
{"id": "2602.19810", "pdf": "https://arxiv.org/pdf/2602.19810", "abs": "https://arxiv.org/abs/2602.19810", "authors": ["Lukas Weidener", "Marko Brkić", "Mihailo Jovanović", "Ritvik Singh", "Emre Ulgac", "Aakaash Meduri"], "title": "OpenClaw, Moltbook, and ClawdLab: From Agent-Only Social Networks to Autonomous Scientific Research", "categories": ["cs.AI"], "comment": null, "summary": "In January 2026, the open-source agent framework OpenClaw and the agent-only social network Moltbook produced a large-scale dataset of autonomous AI-to-AI interaction, attracting six academic publications within fourteen days. This study conducts a multivocal literature review of that ecosystem and presents ClawdLab, an open-source platform for autonomous scientific research, as a design science response to the architectural failure modes identified. The literature documents emergent collective phenomena, security vulnerabilities spanning 131 agent skills and over 15,200 exposed control panels, and five recurring architectural patterns. ClawdLab addresses these failure modes through hard role restrictions, structured adversarial critique, PI-led governance, multi-model orchestration, and domain-specific evidence requirements encoded as protocol constraints that ground validation in computational tool outputs rather than social consensus; the architecture provides emergent Sybil resistance as a structural consequence. A three-tier taxonomy distinguishes single-agent pipelines, predetermined multi-agent workflows, and fully decentralised systems, analysing why leading AI co-scientist platforms remain confined to the first two tiers. ClawdLab's composable third-tier architecture, in which foundation models, capabilities, governance, and evidence requirements are independently modifiable, enables compounding improvement as the broader AI ecosystem advances.", "AI": {"tldr": "本文介绍了OpenClaw和Moltbook生成的大型数据集，并提出了一个名为ClawdLab的开放平台，旨在通过解决架构问题来推动自主科学研究。", "motivation": "为了应对由AI到AI交互产生的大规模数据集中暴露出的安全漏洞和其他结构缺陷，设计了一个新的开放式科学研究平台来提升自治和安全性。", "method": "进行一个多声部文献综述，并提出了ClawdLab作为一个设计回应来解决先前的架构故障模式问题。引入了三层分类法以区分单一代理管道、预定多代理工作流和完全分散系统之间的差异，强调了第三层架构的优势。", "result": "通过实施严格的角色限制等措施以及特定领域的证据要求编码为协议约束，ClawdLab解决了已识别的失败模式，并提供了抵御Sybil攻击的结构化后果。", "conclusion": "ClawdLab提供的可组合三层架构允许基础模型、功能、治理和证据需求的独立修改，随着更广泛的AI生态系统的发展而不断改进。"}}
{"id": "2602.19809", "pdf": "https://arxiv.org/pdf/2602.19809", "abs": "https://arxiv.org/abs/2602.19809", "authors": ["Sebastian Hubenschmid", "Arvind Srinivasan", "Niklas Elmqvist", "Dieter Schmalstieg", "Michael Sedlmair"], "title": "Ambient Analytics: Calm Technology for Immersive Visualization and Sensemaking", "categories": ["cs.HC"], "comment": "Accepted at \"Visualization Viewpoints\" in IEEE Computer Graphics and Applications", "summary": "Augmented reality has great potential for embedding data visualizations in the world around the user. While this can enhance users' understanding of their surroundings, it also bears the risk of overwhelming their senses with a barrage of information. In contrast, calm technologies aim to place information in the user's attentional periphery, minimizing cognitive load instead of demanding focused engagement. In this column, we explore how visualizations can be harmoniously integrated into our everyday life through augmented reality, progressing from visual analytics to ambient analytics.", "AI": {"tldr": "探讨如何通过增强现实技术将数据可视化和谐地融入日常生活中，从视觉分析过渡到环境分析。", "motivation": "利用增强现实来提高用户对其周围环境的理解力，同时减少信息过载的风险，实现信息在用户的注意力边缘呈现，降低认知负担。", "method": "介绍一种新的概念——环境分析，探讨其如何通过增强现实和谐地融入人们的日常生活中。", "result": "提出了一种新方法论和概念框架，以促进数据可视化与用户日常生活场景的自然融合。", "conclusion": "倡导采用环境技术（如增强现实）来实现信息在用户注意力边缘呈现，从而提高理解和效率，同时保持较低的认知负担。"}}
{"id": "2602.19805", "pdf": "https://arxiv.org/pdf/2602.19805", "abs": "https://arxiv.org/abs/2602.19805", "authors": ["Wall Kim", "Chaeyoung Song", "Hanul Kim"], "title": "Decision MetaMamba: Enhancing Selective SSM in Offline RL with Heterogeneous Sequence Mixing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mamba-based models have drawn much attention in offline RL. However, their selective mechanism often detrimental when key steps in RL sequences are omitted. To address these issues, we propose a simple yet effective structure, called Decision MetaMamba (DMM), which replaces Mamba's token mixer with a dense layer-based sequence mixer and modifies positional structure to preserve local information. By performing sequence mixing that considers all channels simultaneously before Mamba, DMM prevents information loss due to selective scanning and residual gating. Extensive experiments demonstrate that our DMM delivers the state-of-the-art performance across diverse RL tasks. Furthermore, DMM achieves these results with a compact parameter footprint, demonstrating strong potential for real-world applications.", "AI": {"tldr": "提出决策元蟒（Decision MetaMamba，DMM）模型以改进离线强化学习中选择性扫描机制的问题。", "motivation": "解决现有基于Mamba的模型在关键步骤被忽略时的选择性机制问题，防止信息丢失并提升性能。", "method": "通过将传统的token mixer替换为基于密集层的序列混合器，并修改位置结构以保持局部信息，在Mamba之前执行同时考虑所有通道的序列混合。", "result": "DMM模型在各种强化学习任务中实现了最先进的性能，且参数量较小，具有实际应用潜力。", "conclusion": "DMM通过改进选择性机制解决了传统方法的信息损失问题，并展示了其优越的表现和紧凑的结构。"}}
{"id": "2602.19802", "pdf": "https://arxiv.org/pdf/2602.19802", "abs": "https://arxiv.org/abs/2602.19802", "authors": ["Romain de Coudenhove", "Yannis Bendi-Ouis", "Anthony Strock", "Xavier Hinaut"], "title": "Linear Reservoir: A Diagonalization-Based Optimization", "categories": ["cs.DC", "cs.NE", "math.CV", "math.DS"], "comment": null, "summary": "We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues.", "AI": {"tldr": "提出了一种基于对角化优化的线性回声状态网络，减少了每步计算复杂度。", "motivation": "通过将递归矩阵重新表述为特征值基来减少传统ESN中每步计算复杂度O(N^2)至O(N)，以提高效率。", "method": "采用三种方法：(i)特征权变换EWT，保持标准和训练后的线性ESNs的动力学；(ii)端到端的特征优化EET，在转换空间直接优化读出权重；(iii)直接参数生成DPG，跳过矩阵对角化步骤，通过直接采样本征值与特征向量达到可比性能。", "result": "实验表明，所有方法均保持预测准确性并提供显著计算加速。", "conclusion": "该技术为线性ESN的计算和训练提供了替代方案，并预示着从标准选择到直接选取本征值的新范式的转变。"}}
{"id": "2602.19786", "pdf": "https://arxiv.org/pdf/2602.19786", "abs": "https://arxiv.org/abs/2602.19786", "authors": ["Miguel Ceriani", "Fiorela Ciroku", "Alessandro Russo", "Massimiliano Schembri", "Fai Fung", "Neha Mittal", "Vito Trianni", "Andrea Giovanni Nuzzolese"], "title": "The Climate Change Knowledge Graph: Supporting Climate Services", "categories": ["cs.DB", "cs.AI", "cs.CY"], "comment": null, "summary": "Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.", "AI": {"tldr": "构建气候变化知识图谱以支持气候服务", "motivation": "现有方法难以整合多源数据，限制了复杂查询的执行和决策制定", "method": "开发一个集成多元数据源的知识图谱，并提供开放访问许可", "result": "创建了一个全面框架来增强气候数据分析，促进更有效的应对气候变化措施", "conclusion": "知识图谱为研究者提供了强大的工具，以支持基于证据的气候政策制定"}}
{"id": "2602.19785", "pdf": "https://arxiv.org/pdf/2602.19785", "abs": "https://arxiv.org/abs/2602.19785", "authors": ["Dylan Baptiste", "Ramla Saddem", "Alexandre Philippot", "François Foyer"], "title": "Unsupervised Anomaly Detection in NSL-KDD Using $β$-VAE: A Latent Space and Reconstruction Error Approach", "categories": ["cs.LG", "cs.NE", "stat.ML"], "comment": "ef:2025 15th France-Japan \\& 13th Europe-Asia Congress on Mechatronics (MECATRONICS) / 23rd International Conference on Research and Education in Mechatronics (REM), Dec 2025, Saint-Ouen-sur-Seine, France. pp.1-6", "summary": "As Operational Technology increasingly integrates with Information Technology, the need for Intrusion Detection Systems becomes more important. This paper explores an unsupervised approach to anomaly detection in network traffic using $β$-Variational Autoencoders on the NSL-KDD dataset. We investigate two methods: leveraging the latent space structure by measuring distances from test samples to the training data projections, and using the reconstruction error as a conventional anomaly detection metric. By comparing these approaches, we provide insights into their respective advantages and limitations in an unsupervised setting. Experimental results highlight the effectiveness of latent space exploitation for classification tasks.", "AI": {"tldr": "使用$β$-VAE在NSL-KDD数据集上进行无监督异常检测，通过测量测试样本到训练数据投影的距离和重建误差来识别网络流量中的异常。", "motivation": "随着运营技术与信息技术的融合，入侵检测系统的重要性日益增加。该论文探讨了在网络流量中使用$β$-变分自编码器（VAE）进行无监督异常检测的方法，以提高网络安全。", "method": "研究两种方法：利用潜在空间结构通过测量测试样本到训练数据投影的距离来识别异常；以及通过重建误差作为传统异常检测指标。这两种方法在无监督环境下进行了比较。", "result": "实验结果显示了利用潜在空间的优势，特别是在分类任务中表现优异。", "conclusion": "该论文提出了使用$β$-VAE的两种不同方式来进行网络流量中的异常检测，并分析了各自的优势和局限性。"}}
{"id": "2602.19778", "pdf": "https://arxiv.org/pdf/2602.19778", "abs": "https://arxiv.org/abs/2602.19778", "authors": ["Nghia Phan", "Rong Jin", "Gang Liu", "Xiao Dong"], "title": "Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM"], "comment": "9 pages, 6 figures, 3 tables", "summary": "Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.", "AI": {"tldr": "本文提出了一种基于伪标签和知识蒸馏的两阶段训练流程，用于改进自动音阶识别。", "motivation": "由于同步标注的成本高昂，自动音阶识别受限于标记数据的稀缺性。同时，开源预训练模型比其专有训练数据更容易获取。", "method": "该方法分为两个阶段：第一阶段使用预训练BTC模型生成伪标签并用于训练学生模型；第二阶段在可用真实标签上继续训练学生模型，并通过知识蒸馏作为正则化防止遗忘。", "result": "实验表明，在仅用伪标签的情况下，BTC学生模型达到教师性能的98%以上，2E1D模型接近96%。经过一次训练后，最终结果超过传统监督学习基准线和原始预训练模型。", "conclusion": "该方法显著改善了自动音阶识别特别是罕见音级的表现，并验证了伪标签结合知识蒸馏的有效性"}}
{"id": "2602.19770", "pdf": "https://arxiv.org/pdf/2602.19770", "abs": "https://arxiv.org/abs/2602.19770", "authors": ["Johanna S. Fröhlich", "Bastian Heinlein", "Jan U. Claar", "Hans Rosenberger", "Vasileios Belagiannis", "Ralf R. Müller"], "title": "The Confusion is Real: GRAPHIC - A Network Science Approach to Confusion Matrices in Deep Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Transactions on Machine Learning Research, 2026", "summary": "Explainable artificial intelligence has emerged as a promising field of research to address reliability concerns in artificial intelligence. Despite significant progress in explainable artificial intelligence, few methods provide a systematic way to visualize and understand how classes are confused and how their relationships evolve as training progresses. In this work, we present GRAPHIC, an architecture-agnostic approach that analyzes neural networks on a class level. It leverages confusion matrices derived from intermediate layers using linear classifiers. We interpret these as adjacency matrices of directed graphs, allowing tools from network science to visualize and quantify learning dynamics across training epochs and intermediate layers. GRAPHIC provides insights into linear class separability, dataset issues, and architectural behavior, revealing, for example, similarities between flatfish and man and labeling ambiguities validated in a human study. In summary, by uncovering real confusions, GRAPHIC offers new perspectives on how neural networks learn. The code is available at https://github.com/Johanna-S-Froehlich/GRAPHIC.", "AI": {"tldr": "GRAPHIC是一种用于分析深度学习中神经网络混淆矩阵的方法，利用图论工具来可视化和量化不同训练时期的类关系。", "motivation": "现有的可解释的人工智能方法很少能系统地展示类之间的混淆情况及它们如何随训练进程变化。本文提出了GRAPHIC以解决这一问题。", "method": "GRAPHIC通过使用线性分类器从中间层获取的混淆矩阵作为有向图的邻接矩阵，利用网络科学工具来分析神经网络在不同层次和训练时期的动态。", "result": "GRAPHIC揭示了类之间的相似性和标签模糊等问题，并提供了对数据集问题和模型行为的新见解。", "conclusion": "通过发现真实的混淆情况，GRAPHIC为理解深度学习如何工作提供了一种新的视角。"}}
{"id": "2602.19768", "pdf": "https://arxiv.org/pdf/2602.19768", "abs": "https://arxiv.org/abs/2602.19768", "authors": ["Fan Yang", "Shurong Zheng", "Hongyin Zhao", "Yufei Zhan", "Xin Li", "Yousong Zhu", "Chaoyang Zhao Ming Tang", "Jinqiao Wang"], "title": "TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.", "AI": {"tldr": "提出了一种轨迹感知的视觉语言模型TraceVision，以增强对人类空间理解的研究。", "motivation": "现有大型视觉语言模型主要关注全局图像理解，在模拟人类视觉注意力轨迹和解释描述与特定区域之间的关联方面存在不足。因此，需要一种能够实现轨迹感知的空间理解的方法。", "method": "提出了一个融合视觉特征和轨迹信息的双向模块TVP，并设计了几何简化以从原始轨迹中提取语义关键点。此外，还提出了一种三阶段训练流程，其中轨迹指导描述生成和区域定位。扩展了TraceVision到轨迹引导分割和视频场景理解。", "result": "在轨迹引导字幕、文本引导轨迹预测等方面进行了广泛的实验，并取得了最先进的性能。", "conclusion": "通过引入轨迹感知的视觉语言模型，实现了直观的空间交互与可解释的视觉理解基础。"}}
{"id": "2602.19766", "pdf": "https://arxiv.org/pdf/2602.19766", "abs": "https://arxiv.org/abs/2602.19766", "authors": ["Pengfei Wang", "Liyi Chen", "Zhiyuan Ma", "Yanjun Guo", "Guowen Zhang", "Lei Zhang"], "title": "One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image", "categories": ["cs.CV"], "comment": "ICLR 2026", "summary": "Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \\textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.", "AI": {"tldr": "从单张图片生成可探索的三维场景是一项挑战性的任务，该论文提出了一个有效框架One2Scene来解决这个问题。", "motivation": "现有的方法难以支持自由视角下的三维场景探索，常产生严重的几何畸变和噪点。因此需要一种新的方法来处理这些问题。", "method": "本文提出了一种分解为三个子任务的方法：首先使用全景生成器从单张输入图片中生成锚视图作为初始化；然后通过高斯光栅化网络将这些二维锚提升为明确的三维几何结构骨架；最后该骨架被用作先验条件来产生任意视角下的逼真且几何准确的视图。", "result": "实验显示One2Scene在全景深度估计、实时360°重建和可探索三维场景生成上明显优于现有方法。", "conclusion": "通过明确地基于一个3D一致性的骨架来进行重构，One2Scene能够稳定地支持大视角下的沉浸式场景探索。"}}
{"id": "2602.19764", "pdf": "https://arxiv.org/pdf/2602.19764", "abs": "https://arxiv.org/abs/2602.19764", "authors": ["Yirui Sun", "Guangyu Zhuge", "Keliang Liu", "Jie Gu", "Zhihao xia", "Qionglin Ren", "Chunxu tian", "Zhongxue Ga"], "title": "Towards Dexterous Embodied Manipulation via Deep Multi-Sensory Fusion and Sparse Expert Scaling", "categories": ["cs.RO"], "comment": null, "summary": "Realizing dexterous embodied manipulation necessitates the deep integration of heterogeneous multimodal sensory inputs. However, current vision-centric paradigms often overlook the critical force and geometric feedback essential for complex tasks. This paper presents DeMUSE, a Deep Multimodal Unified Sparse Experts framework leveraging a Diffusion Transformer to integrate RGB, depth, and 6-axis force into a unified serialized stream. Adaptive Modality-specific Normalization (AdaMN) is employed to recalibrate modality-aware features, mitigating representation imbalance and harmonizing the heterogeneous distributions of multi-sensory signals. To facilitate efficient scaling, the architecture utilizes a Sparse Mixture-of-Experts (MoE) with shared experts, increasing model capacity for physical priors while maintaining the low inference latency required for real-time control. A Joint denoising objective synchronously synthesizes environmental evolution and action sequences to ensure physical consistency. Achieving success rates of 83.2% and 72.5% in simulation and real-world trials, DeMUSE demonstrates state-of-the-art performance, validating the necessity of deep multi-sensory integration for complex physical interactions.", "AI": {"tldr": "本文提出了DeMUSE框架，通过深度多模态融合和稀疏专家扩展来实现灵巧的实体操作。", "motivation": "当前基于视觉的操作方法往往忽视了力反馈和几何信息的重要性。为了处理复杂的任务，需要深入整合各种异构多模态感官输入。", "method": "DeMUSE利用扩散变换器将RGB、深度和6轴力量数据融合成统一的序列流，并应用自适应模式特定归一化（AdaMN）来校准特征并平衡表示。架构使用稀疏专家混合模型，通过共享专家提高模型容量的同时保持低延迟。", "result": "在模拟试验和真实世界测试中，DeMUSE分别达到了83.2%和72.5%的成功率，展示了深度多模态融合对于复杂物理交互的必要性。", "conclusion": "研究表明，通过深度融合多模态感官输入可以显著提高灵巧操作的表现。"}}
{"id": "2602.19763", "pdf": "https://arxiv.org/pdf/2602.19763", "abs": "https://arxiv.org/abs/2602.19763", "authors": ["Yida Lin", "Bing Xue", "Mengjie Zhang", "Sam Schofield", "Richard Green"], "title": "Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.", "AI": {"tldr": "本文研究了在无人机林业应用中基于树干影像训练深度立体匹配网络的效果，以实现准确、实时的深度估计。", "motivation": "自主无人机修剪树木需要从立体摄像头获取精确且实时的深度估算。然而，由于工作距离较远，即使微小的视差误差也会导致显著的深度错误。", "method": "本文基于CANterbury Tree Branches数据集训练并测试了十个深度立体匹配网络，并使用DEFOM-Stereo生成的真实树干图像作为训练目标。这些方法涵盖了逐步细化、3D卷积、边缘感知注意以及轻量化设计等技术。", "result": "实验结果显示，BANet-3D在整体质量上表现最佳（SSIM=0.883，LPIPS=0.157），而RAFT-Stereo则在场景理解方面表现出色（ViTScore=0.799）。测试表明，在NVIDIA Jetson Orin Super平台上，AnyNet以6.99 FPS的速度成为唯一接近实时的选择，BANet-2D则提供了最佳的质量与速度平衡，达到1.21 FPS。", "conclusion": "研究为无人机林业应用中的深度立体匹配网络提供了一套基准测试方法，并针对不同分辨率下的处理时间进行了分析，以指导实际系统的设计选择。"}}
{"id": "2602.19762", "pdf": "https://arxiv.org/pdf/2602.19762", "abs": "https://arxiv.org/abs/2602.19762", "authors": ["Mohammed Javed Absar", "Muthu Baskaran", "Abhikrant Sharma", "Abhilash Bhandari", "Ankit Aggarwal", "Arun Rangasamy", "Dibyendu Das", "Fateme Hosseini", "Franck Slama", "Iulian Brumar", "Jyotsna Verma", "Krishnaprasad Bindumadhavan", "Mitesh Kothari", "Mohit Gupta", "Ravishankar Kolachana", "Richard Lethin", "Samarth Narang", "Sanjay Motilal Ladwa", "Shalini Jain", "Snigdha Suresh Dalvi", "Tasmia Rahman", "Venkat Rasagna Reddy Komatireddy", "Vivek Vasudevbhai Pandya", "Xiyue Shi", "Zachary Zipper"], "title": "Hexagon-MLIR: An AI Compilation Stack For Qualcomm's Neural Processing Units (NPUs)", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "In this paper, we present Hexagon-MLIR,an open-source compilation stack that targets Qualcomm Hexagon Neural Processing Unit (NPU) and provides unified support for lowering Triton kernels and PyTorch models . Built using the MLIR framework, our compiler applies a structured sequence of passes to exploit NPU architectural features to accelerate AI workloads. It enables faster deployment of new Triton kernels (hand-written or subgraphs from PyTorch 2.0), for our target by providing automated compilation from kernel to binary. By ingesting Triton kernels, we generate mega-kernels that maximize data locality in the NPU's Tightly Coupled Memory (TCM), reducing the bandwidth bottlenecks inherent in library-based approaches. This initiative complements our commercial toolchains by providing developers with an open-source MLIR-based compilation stack that gives them a path to advance AI compilation capabilities through a more flexible approach. Hexagon-MLIR is a work-in-progress, and we are continuing to add many more optimizations and capabilities in this effort.", "AI": {"tldr": "Hexagon-MLIR是一个针对Qualcomm Hexagon神经处理单元（NPU）的开源编译栈，它支持将Triton内核和PyTorch模型降低为适用于NPU的目标代码。", "motivation": "为了加速人工智能工作负载，通过利用NPU架构特性以减少带宽瓶颈，并提供更快的新Triton内核部署路径。", "method": "使用MLIR框架构建Hexagon-MLIR编译器，采用结构化的转换步骤来优化AI模型的执行效率。它支持从Triton内核到二进制文件的自动化编译。", "result": "生成了最大化数据本地性的mega-kernels，并减少了依赖于库方法引入的带宽瓶颈。Hexagon-MLIR为开发人员提供了灵活且开放源代码的AI编译路径。", "conclusion": "Hexagon-MLIR是一个正在进行中的工作，未来将继续添加更多的优化和功能以增强其在人工智能领域的应用能力。"}}
{"id": "2602.19756", "pdf": "https://arxiv.org/pdf/2602.19756", "abs": "https://arxiv.org/abs/2602.19756", "authors": ["Junhyeok Choi", "Sangwoo Mo", "Minwoo Chae"], "title": "Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.", "AI": {"tldr": "该论文提出了一种基于原型引导的数据合成方法，实现了高效的多模态数据集提炼。", "motivation": "现有大型视觉-语言任务依赖于大规模图像文本数据集，训练成本高且效率低。先前的过滤和剪枝努力虽有所缓解但仍需较大的子集以保持性能，并在小型子集中效果不佳。现有的数据集提炼方法对架构有依赖性并限制了跨架构推广。", "method": "该论文提出了一种基于CLIP提取图像-文本嵌入、获取原型以及使用unCLIP解码器合成图像的学习自由的数据集提炼框架，以提高跨架构的推广能力。", "result": "广泛的实验表明，该方法在优化和子集选择方法上取得了更好的表现，并且实现了最先进的跨架构推广。", "conclusion": "所提出的方法不仅提高了数据集提炼效率，还增强了跨不同模型结构的学习性能。"}}
{"id": "2602.19753", "pdf": "https://arxiv.org/pdf/2602.19753", "abs": "https://arxiv.org/abs/2602.19753", "authors": ["Kaifa Yang", "Qi Yang", "Yiling Xu", "Zhu Li"], "title": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted by CVPR 2026", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.", "AI": {"tldr": "RAP是一种快速的无渲染方法，通过引导属性预测3D Gaussian Splatting中的原始重要性得分。", "motivation": "现有的基于渲染的方法计算时间长且依赖视图数量和选择，难以集成到重建流程中。为此提出了一种新的方法来解决这些问题。", "method": "RAP使用多层感知机（MLP）直接从高斯属性和局部统计推断原始重要性得分，避免了耗时的渲染步骤。", "result": "经过小规模场景训练后，RAP能够有效推广到新数据，并能无缝集成至重建、压缩及传输管道中。", "conclusion": "RAP提供了一种快速有效的无渲染方法来预测3D Gaussian Splatting中的原始重要性得分。"}}
{"id": "2602.19745", "pdf": "https://arxiv.org/pdf/2602.19745", "abs": "https://arxiv.org/abs/2602.19745", "authors": ["Jules Wulms", "Wouter Meulemans", "Bettina Speckmann"], "title": "Unfolding Ordered Matrices into BioFabric Motifs", "categories": ["cs.HC"], "comment": null, "summary": "BioFabrics were introduced by Longabaugh in 2012 as a way to draw large graphs in a clear and uncluttered manner. The visual quality of BioFabrics crucially depends on the order of vertices and edges, which can be chosen independently. Effective orders can expose salient patterns, which in turn can be summarized by motifs, allowing users to take in complex networks at-a-glance. However, so far there is no efficient layout algorithm which automatically recognizes patterns and delivers both a vertex and an edge ordering that allows these patterns to be expressed as motifs. In this paper we show how to use well-ordered matrices as a tool to efficiently find good vertex and edge orders for BioFabrics. Specifically, we order the adjacency matrix of the input graph using Moran's $I$ and detect (noisy) patterns with our recent algorithm. In this note we show how to \"unfold\" the ordered matrix and its patterns into a high-quality BioFabric. Our pipelines easily handles graphs with up to 250 vertices.", "AI": {"tldr": "本文提出了一种利用有序矩阵将大型图可视化为BioFabric的方法，以清晰地展示复杂网络的模式。", "motivation": "现有的BioFabric布局算法无法自动识别并展现图形中的显著模式。作者希望通过有效的顶点和边排序来提高BioFabric的视觉质量和可读性。", "method": "首先使用Moran's $I$ 对邻接矩阵进行有序化，然后利用最近开发的算法检测模式，最后将有序矩阵及其模式转化为高质量的BioFabric布局。", "result": "该方法能够处理包含多达250个顶点的图，并能有效地揭示和展示这些图形中的显著模式。", "conclusion": "通过本文的方法，可以自动识别并优化大型网络中隐藏的模式，进而以一种清晰、无干扰的方式呈现复杂的关系。"}}
{"id": "2602.19736", "pdf": "https://arxiv.org/pdf/2602.19736", "abs": "https://arxiv.org/abs/2602.19736", "authors": ["Shoukun Sun", "Zhe Wang", "Xiang Que", "Jiyin Zhang", "Xiaogang Ma"], "title": "InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.", "AI": {"tldr": "提出了一种名为InfScene-SR的框架，用于大尺寸任意场景的空间连续超分辨率。", "motivation": "标准扩散模型在处理固定大小的补丁时表现出色，但在面对任意大小图像时由于内存限制难以扩展。独立补丁处理会导致可见接缝和边界不一致的问题。", "method": "提出了迭代细化过程与引导和方差校正融合机制相结合的方法，以实现大尺度高分辨率影像的空间连续生成，并在遥感数据集上进行了验证。", "result": "InfScene-SR不仅能够重建精细细节并保持高质量视觉效果，还能消除边界伪影，有利于下游任务如语义分割。", "conclusion": "所提出的框架能够在不重新训练的情况下，有效解决任意大小图像的超分辨率问题，并提高下游任务的效果。"}}
{"id": "2602.19735", "pdf": "https://arxiv.org/pdf/2602.19735", "abs": "https://arxiv.org/abs/2602.19735", "authors": ["Jingyi Xu", "Zhangshuo Qi", "Zhongmiao Yan", "Xuyu Gao", "Qianyun Jiao", "Songpengcheng Xia", "Xieyuanli Chen", "Ling Pei"], "title": "VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments", "categories": ["cs.CV"], "comment": null, "summary": "In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.", "AI": {"tldr": "VGGT-MPR是一种基于视觉几何感知变压器的多模态场景识别框架，用于自动驾驶环境中的全球定位和循环闭合检测。", "motivation": "为了克服现有方法中手动融合策略和重型参数化模型带来的成本问题，提出了一种新的多模态场景识别框架，该框架利用VGGT进行全局检索和再排序机制设计，以提高鲁棒性和准确性。", "method": "采用视觉几何感知变压器(VGGT)作为统一的几何引擎，在全局检索阶段通过深度意识监督和点地图监督提取丰富的视觉嵌入，并通过预测深度图密集化稀疏激光雷达点云来增强结构表示。再排序机制利用VGGT的关键点跟踪能力，结合掩码引导关键点抽取和置信度感知对应评分进行训练自由的细化。", "result": "在大规模自动驾驶基准测试和自收集数据上的实验表明，VGGT-MPR达到了最先进的性能，展示了对严重环境变化、视角偏移以及遮挡的强大鲁棒性。", "conclusion": "通过提出VGGT-MPR框架，解决了多模态场景识别中的关键挑战，并提高了系统在复杂驾驶条件下的可靠性。"}}
{"id": "2602.19723", "pdf": "https://arxiv.org/pdf/2602.19723", "abs": "https://arxiv.org/abs/2602.19723", "authors": ["Yue Zhang", "Zhizheng Zhuo", "Siyao Xu", "Shan Lv", "Zhaoxi Liu", "Jun Qiu", "Qiuli Wang", "Yaou Liu", "S. Kevin Zhou"], "title": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets", "categories": ["cs.CV"], "comment": "19 pages, 4 figures", "summary": "Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.", "AI": {"tldr": "提出了一种个性化的多模态MRI合成框架PMM-Synth，该框架能够在不同数据集上进行有效的跨数据集泛化。", "motivation": "现有的统一合成模型的训练和评估通常仅限于单个数据集，限制了它们在临床多样性数据集上的应用能力。为了克服这一局限性，提出了PMM-Synth以支持多样的合成任务并实现跨异构数据集的有效推广。", "method": "PMM-Synth通过个性化特征调节模块、模态一致批量调度器和选择性监督损失三项核心创新来实现跨数据集泛化。这些机制共同作用于多个不同模态覆盖、疾病类型和强度分布的多模态MRI数据集，以达到稳定且高效的训练效果。", "result": "在四个临床多模态MRI数据集上的评估显示，PMM-Synth在一对一和多对一合成任务中均优于现有最佳方法，取得了更高的PSNR和SSIM评分。定性结果还表明解剖结构和病理细节得到了更好的保留，并且下游肿瘤分割和放射学报告研究表明PMM-Synth有望支持真实世界中的模态缺失场景下的可靠诊断。", "conclusion": "该研究成功开发了一种能够跨异构数据集泛化的个性化多模态MRI合成框架，有效提升了多模态MRI合成任务的性能，并为临床应用提供了重要的技术支持。"}}
{"id": "2602.19719", "pdf": "https://arxiv.org/pdf/2602.19719", "abs": "https://arxiv.org/abs/2602.19719", "authors": ["Amir Hamza", "Davide Boscaini", "Weihang Li", "Benjamin Busam", "Fabio Poiesi"], "title": "Generative 6D Pose Estimation via Conditional Flow Matching", "categories": ["cs.CV"], "comment": "Project Website : https://tev-fbk.github.io/Flose/", "summary": "Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/", "AI": {"tldr": "该论文提出了一种新的6D姿态估计方法，通过条件流匹配问题在R^3中进行生成式推断。", "motivation": "现有方法直接回归SE(3)中的姿态或间接使用局部特征匹配来估计姿态，前者难以处理对象对称性，后者缺乏明显的本地特征时效果不佳。为解决这些问题，论文提出了Flose方法以综合几何和外观信息消除歧义，并通过RANSAC注册处理离群值。", "method": "Flose利用条件流匹配进行噪声去除过程，结合局部特征的生成式姿态估计。此方法融合了基于外观的语义特征来缓解由对象对称性引起的模糊性，并使用RANSAC算法处理异常情况。", "result": "在BOP基准测试中五个数据集上验证了Flose性能优于先前的方法，平均召回率提高4.5%。", "conclusion": "该方法通过条件流匹配和融合外观信息显著提升了6D姿态估计的准确度，并且在处理对称对象时表现更优。"}}
{"id": "2602.19718", "pdf": "https://arxiv.org/pdf/2602.19718", "abs": "https://arxiv.org/abs/2602.19718", "authors": ["Mateen A. Abbasi", "Tommi J. Mikkonen", "Petri J. Ihantola", "Muhammad Waseem", "Pekka Abrahamsson", "Niko K. Mäkitalo"], "title": "Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages, 1 figure. Preprint version under review", "summary": "The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns.", "AI": {"tldr": "本文提出了一种名为碳感知治理门(CAGG)的架构，旨在减少GenAI开发过程中的碳足迹。", "motivation": "随着Generative AI在软件开发生命周期中的广泛应用，计算需求增加导致了更大的碳排放。为了支持信任、透明度和问责制，组织嵌入了额外的治理机制，但这些机制也增加了能源消耗。", "method": "CAGG架构由三个部分组成：能量和碳追溯账本，碳预算管理器以及绿色验证编排器，通过治理策略和可重用设计模式实现。", "result": "该方法嵌入了碳预算、能量来源追踪以及可持续性感知的验证编排到人机共治层次中。", "conclusion": "CAGG架构有助于确保GenAI开发在保障效率的同时减少其对环境的影响。"}}
{"id": "2602.19715", "pdf": "https://arxiv.org/pdf/2602.19715", "abs": "https://arxiv.org/abs/2602.19715", "authors": ["Kartik Kuckreja", "Parul Gupta", "Muhammad Haris Khan", "Abhinav Dhall"], "title": "Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision", "categories": ["cs.CV"], "comment": "CVPR-2026, Code is available here: https://github.com/KjAeRsTuIsK/DeepfakeJudge", "summary": "Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\\%, outperforming \\texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \\href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.", "AI": {"tldr": "提出了一种名为DeepfakeJudge的框架，用于可扩展的推理监督和评估，以提高深度伪造检测模型中视觉证据合理性的可靠性。", "motivation": "现有的深度伪造检测模型生成的文字解释经常缺乏与视觉证据的支持性，影响了其可靠性和准确性。现有评价方法侧重于分类精度而忽略了推理质量。因此提出了一种新的框架来解决这些问题。", "method": "该框架包含一个分布外基准测试集、人工标注的子集以及一整套评估模型。通过自举生成器-评估器过程，将人类反馈转化为结构化推理监督，并支持点对点和成对评价方式。", "result": "在提出的元评价基准上，基于推理引导的方法达到了96.2%的准确率，超过了30倍大小的基线模型。该框架中的推理法官与人工评分高度相关，在人类标注的子集上达成98.9%的一致性。", "conclusion": "实验结果表明，推理准确性可作为衡量深度伪造检测的重要维度，并且证明了对解释性的监督可以实现大规模扩展。用户研究显示参与者在70%的情况下更偏好该框架生成的理由。"}}
{"id": "2602.19714", "pdf": "https://arxiv.org/pdf/2602.19714", "abs": "https://arxiv.org/abs/2602.19714", "authors": ["Joel Bucher", "Lahari Goswami", "Sverrir Thorgeirsson", "April Yi Wang"], "title": "Git Takes Two: Split-View Awareness for Collaborative Learning of Distributed Workflows in Git", "categories": ["cs.HC", "cs.SE"], "comment": "First two authors contributed equally", "summary": "Git is widely used for collaborative software development, but it can be challenging for newcomers. While most learning tools focus on individual workflows, Git is inherently collaborative. We present GitAcademy, a browser-based learning platform that embeds a full Git environment with a split-view collaborative mode: learners work on their own local repositories connected to a shared remote repository, while simultaneously seeing their partner's actions mirrored in real time. This design is not intended for everyday software development, but rather as a training simulator to build awareness of distributed states, coordination, and collaborative troubleshooting. In a within-subjects study with 13 pairs of learners, we found that the split-view interface enhanced social presence, supported peer teaching, and was consistently preferred over a single-view baseline, even though performance gains were mixed. We further discuss how split-view awareness can serve as a training-only scaffold for collaborative learning of Git and other distributed technical systems.", "AI": {"tldr": "该论文介绍了GitAcademy，一种基于浏览器的学习平台，用于嵌入完整的Git环境，并通过分屏视图帮助学习者理解协作工作流程。", "motivation": "当前大多数学习工具仅关注个人的工作流，而忽视了Git的协作特性。为了更好地支持新用户理解和掌握分布式团队中的协作技术，论文提出了GitAcademy。", "method": "使用了一种具有分屏视图功能的浏览器平台GitAcademy，在此平台上参与者可以同时看到自己和合作伙伴的操作，并通过实证研究验证其效果。", "result": "实验表明，与单屏模式相比，分屏视图增强了社交存在感和支持同伴教学的能力，尽管在某些方面的性能提升不明显。", "conclusion": "论文认为，这种训练工具能够作为协作学习的辅助手段，帮助学习者更好地理解分布式技术系统中的协作问题。"}}
{"id": "2602.19711", "pdf": "https://arxiv.org/pdf/2602.19711", "abs": "https://arxiv.org/abs/2602.19711", "authors": ["Krzysztof Kutt", "Elżbieta Sroka", "Oleksandra Ishchuk", "Luiz do Valle Miranda"], "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs", "categories": ["cs.IR", "cs.DL", "cs.HC"], "comment": "15 pages, 1 figure; submitted to ICCS 2026 conference", "summary": "The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation.", "AI": {"tldr": "本文提出了一种基于知识图谱的三阶段神经符号推荐系统，用于文化遗产资源的推荐。", "motivation": "随着数字化文化遗产数量的增长，需要一种能够解释异构数据实体间语义关系的高级推荐方法。", "method": "该研究在JUHMP知识图谱上实现了一个混合推荐流程，结合了知识图嵌入、近似最近邻搜索和SPARQL驱动的语义过滤。通过实验选择了最佳参数，并最终评估了一种三阶段神经符号推荐器。", "result": "尽管存在稀疏和异构的元数据，该方法能够生成有用且可解释的推荐结果，专家评审也验证了这一点。", "conclusion": "本文展示了如何利用神经符号技术从复杂知识图谱中提取有价值的信息，并为文化遗产资源提供个性化推荐。"}}
{"id": "2602.19710", "pdf": "https://arxiv.org/pdf/2602.19710", "abs": "https://arxiv.org/abs/2602.19710", "authors": ["Haitao Lin", "Hanyang Yu", "Jingshun Huang", "He Zhang", "Yonggen Ling", "Ping Tan", "Xiangyang Xue", "Yanwei Fu"], "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns. To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision. Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.", "AI": {"tldr": "提出了一种新的Vision-Language-Action（VLA）模型Pose-VLA，旨在通过解耦的预训练和后训练阶段解决现有VLA模型的问题。", "motivation": "现有的VLA模型在特征塌陷和低效训练方面存在挑战，因为它们将高层次感知与稀疏、特定于操作的任务结合在一起。这些模型通常依赖于为视觉问答（VQA）优化的视觉语言模型骨架，在语义识别上表现良好但忽略了3D状态变化，从而导致不同的动作模式无法适应。", "method": "提出Pose-VLA框架，包括两个预训练阶段：首先通过姿态提取通用的3D空间先验知识；然后利用轨迹监督进行运动对齐。引入离散姿态令牌作为统一表示将来自不同3D数据集的空间接地与几何级轨迹从机器人示范中结合。", "result": "Pose-VLA在RoboTwin 2.0上的成功率达到了79.5%，LIBERO上表现也非常出色，展示了该框架的强大性能。通过仅使用每个任务100次演示的实验进一步验证了预训练方案的有效性。", "conclusion": "提出的Pose-VLA模型不仅改善了现有VLA模型在特征塌陷和低效训练方面的问题，还提高了对3D状态变化的理解能力，并展示了良好的泛化性能。"}}
{"id": "2602.19708", "pdf": "https://arxiv.org/pdf/2602.19708", "abs": "https://arxiv.org/abs/2602.19708", "authors": ["Hoyoung Kim", "Minwoo Jang", "Jabin Koo", "Sangdoo Yun", "Jungseul Ok"], "title": "ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19706", "pdf": "https://arxiv.org/pdf/2602.19706", "abs": "https://arxiv.org/abs/2602.19706", "authors": ["Yo-Tin Lin", "Su-Kai Chen", "Hou-Ning Hu", "Yen-Yu Lin", "Yu-Lun Liu"], "title": "HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion", "categories": ["cs.CV"], "comment": "WACV 2026. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting", "summary": "Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting", "AI": {"tldr": "论文提出了一种无需训练的HDR重建增强方法，通过扩散填充技术改善过度曝光区域的信息损失问题。", "motivation": "传统的HDR重建在处理过度曝光区域时面临信息完全丢失的问题，导致重建效果不佳。本研究旨在开发一种新的方法来解决这一挑战。", "method": "该论文结合文本引导的扩散模型和SDEdit细化技术，在过度曝光区域生成合理的内容，并通过迭代补偿机制保持不同曝光图像间的亮度一致性。", "result": "实验结果表明，该方法在标准HDR数据集和实际拍摄场景中均显示出显著改进，既提高了感知质量也改善了定量指标。", "conclusion": "论文展示了无需训练的扩散填充技术能够有效恢复挑战性场景中的自然细节，并保持现有HDR重建管线的优点。"}}
{"id": "2602.19702", "pdf": "https://arxiv.org/pdf/2602.19702", "abs": "https://arxiv.org/abs/2602.19702", "authors": ["Adamya Shyam", "Venkateswara Rao Kagita", "Bharti Rana", "Vikas Kumar"], "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.", "AI": {"tldr": "提出了一种统一的多模态推荐框架DReX，利用门控循环单元整合细粒度特征以提升用户和项目表示的一致性。", "motivation": "解决现有方法处理不同模式隔离、需要完整多模态数据训练以及独立学习用户和项目表征等问题。这些问题导致复杂性和潜在表征不一致的风险增加。", "method": "提出DReX框架，利用门控循环单元逐级融合细粒度特征以更新全局表示；通过考虑评论文本作为交互模态自动生成可解释的关键词配置文件。", "result": "实验结果表明，所提方法在三个真实数据集上超越了最先进的多模态推荐系统。", "conclusion": "DReX框架成功解决了现有多模态推荐系统的限制，提高了用户和项目表征的一致性，并增强了模型的鲁棒性和解释性。"}}
{"id": "2602.19699", "pdf": "https://arxiv.org/pdf/2602.19699", "abs": "https://arxiv.org/abs/2602.19699", "authors": ["Elisa Alboni", "Pietro Noah Crestaz", "Elias Fontanari", "Andrea Del Prete"], "title": "CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost trajectories. However, scalability remains a key limitation, as increasing system complexity significantly raises the computational cost of TO. This work introduces CACTO-BIC to address these challenges. CACTO-BIC improves data efficiency by biasing initial-state sampling leveraging a property of the value function associated with locally optimal policies; moreover, it reduces computation time by exploiting GPU acceleration. Empirical evaluations show improved sample efficiency and faster computation compared to CACTO. Comparisons with PPO demonstrate that our approach can achieve similar solutions in less time. Finally, experiments on the AlienGO quadruped robot demonstrate that CACTO-BIC can scale to high-dimensional systems and is suitable for real-time applications.", "AI": {"tldr": "CACTO-BIC通过偏置采样和GPU加速轨迹优化来提高学习效率，解决高维系统计算成本高的问题。", "motivation": "现有方法在处理复杂非凸性时存在数据利用率低、计算时间长的问题。该论文旨在提出一种新的算法以提升效率并减少计算时间。", "method": "CACTO-BIC利用价值函数的特性进行初始状态偏置采样，结合GPU加速优化轨迹，提高学习过程中的样本使用效率和计算速度。", "result": "实验结果表明，与传统方法相比，CACTO-BIC在保持解决方案质量的同时，实现了更高的样本利用率和更快的计算时间。", "conclusion": "通过将偏置采样与GPU加速相结合，CACTO-BIC能够在高维系统中有效运行，并适用于实时应用。"}}
{"id": "2602.19698", "pdf": "https://arxiv.org/pdf/2602.19698", "abs": "https://arxiv.org/abs/2602.19698", "authors": ["Krzysztof Kutt", "Maciej Baczyński"], "title": "Iconographic Classification and Content-Based Recommendation for Digitized Artworks", "categories": ["cs.DL", "cs.AI", "cs.CV", "cs.IR"], "comment": "14 pages, 7 figures; submitted to ICCS 2026 conference", "summary": "We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.", "AI": {"tldr": "本文提出了一种自动化图像分类和基于内容推荐的系统，用于数字化艺术品。", "motivation": "利用计算机视觉技术和Iconclass词汇表实现艺术作品的自动分类与推荐，提高文化遗产库管理效率。", "method": "采用YOLOv8目标检测技术结合算法映射到Iconclass代码，并通过规则推理获得抽象意义。引入三种推荐方法：层次邻近、IDF加权重叠和Jaccard相似度。", "result": "验证了基于计算机视觉的图像分类与推荐方法在文化遗产库中具有潜在应用价值，提高了目录编制效率和导航体验。", "conclusion": "该系统展示了将计算机视觉技术应用于艺术作品分类及推荐的可能性，未来需进一步完善工程化设计。"}}
{"id": "2602.19697", "pdf": "https://arxiv.org/pdf/2602.19697", "abs": "https://arxiv.org/abs/2602.19697", "authors": ["Soumya Mazumdar", "Vineet Kumar Rakesh", "Tapas Samanta"], "title": "BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": null, "summary": "Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF", "AI": {"tldr": "本文提出了一个基于CPU的贝叶斯融合框架BayesFusion-SDF，用于从深度观测中进行概率性距离融合和视图规划。", "motivation": "传统的体积融合方法依赖于启发式权重，并且无法系统地透明传达不确定性。而最近的神经隐含方法虽然可以达到很高的保真度，但通常需要大量GPU算力并且难以理解其决策过程。", "method": "该论文通过创建一个粗略TSDF重建来构建自适应狭窄区域，并使用异方差贝叶斯公式将深度观测合并在一起。采用稀疏线性代数和预处理共轭梯度方法解决融合问题，随机对角估计器可以快速获取后验不确定性。", "result": "该论文表明新方法比TSDF基准更准确，同时提供了有用的不确定性估计以进行主动传感。这种方法为GPU密集型神经重建提供了一种清晰且易于理解的替代方案，并具有可预测的行为。", "conclusion": "BayesFusion-SDF框架在几何精度和不确定性估计方面都优于传统的TSDF方法，是一个具有良好性能、低计算成本并且能解释性强的概率性距离融合解决方案。"}}
{"id": "2602.19695", "pdf": "https://arxiv.org/pdf/2602.19695", "abs": "https://arxiv.org/abs/2602.19695", "authors": ["William Seymour", "Martin J. Kraemer"], "title": "Shifting Engagement With Cybersecurity: How People Discover and Share Cybersecurity Content at Work and at Home", "categories": ["cs.HC"], "comment": "To appear in the extended abstracts of the 2026 ACM CHI Conference on Human Factors in Computing Systems", "summary": "Cybersecurity awareness is shaped by a wide range of professional and personal experiences, including information and training at work and the sharing of news and other content at home. In order to explore how people discover cybersecurity content and the effect that participation in workplace training may have on this we present an online study of 1200 participants from the UK, US, France, and Germany. Those undertaking cybersecurity training at work showed reduced intention to share information at home, shifting the focus towards the workplace. They were also more likely to recall cybersecurity information shared by their employer than from any other source, which in turn correlated with content type and distribution channel. We critically reflect on this shift, highlighting opportunities to improve cybersecurity information sharing at work and at home.", "AI": {"tldr": "探讨人们如何在工作和家庭环境中发现并分享网络安全内容，以及参加工作培训对这种行为的影响。", "motivation": "研究个人的网络安全意识是如何通过职业和个人经历形成的，包括工作中获得的信息和训练以及家中传播新闻和其他内容的方式。同时探索参加工作培训是否会影响在家时信息共享的行为。", "method": "进行了一项在线研究，调查了来自英国、美国、法国和德国共1200名参与者的工作场所网络安全培训对其在家庭环境中的信息安全分享行为的影响。", "result": "发现接受过职场网络安全部门培训的人员更倾向于减少在家的信息共享，并将关注点转移到工作场所。这些人在回忆雇主提供的信息时比其他来源更为频繁，且与内容类型和传播渠道相关联。", "conclusion": "批判性地反思了这一现象，强调有机会改善在工作和家庭环境中网络安全信息分享的方式，以提高整体的安全意识水平。"}}
{"id": "2602.19694", "pdf": "https://arxiv.org/pdf/2602.19694", "abs": "https://arxiv.org/abs/2602.19694", "authors": ["Bo Liu", "Tong Li", "Zhu Xiao", "Ruihui Li", "Geyong Min", "Zhuo Tang", "Kenli Li"], "title": "All Cities are Equal: A Unified Human Mobility Generation Model Enabled by LLMs", "categories": ["cs.ET"], "comment": "under review", "summary": "Synthetic human mobility generation is gaining traction as an ethical and practical approach to supporting the data needs of intelligent urban systems. Existing methods perform well primarily in data-rich cities, while their effectiveness declines significantly in cities with limited data resources. However, the ability to generate reliable human mobility data should not depend on a city's size or available resources, all cities deserve equal consideration. To address this open issue, we propose UniMob, a unified human mobility generation model across cities. UniMob is composed of three main components: an LLM-powered travel planner that derives high-level, temporally-aware, and semantically meaningful travel plans; a unified spatial embedding module that projects the spatial regions of various cities into a shared representation space; and a diffusion-based mobility generator that captures the joint spatiotemporal characteristics of human movement, guided by the derived travel plans. We evaluate UniMob extensively using two real-world datasets covering five cities. Comprehensive experiments show that UniMob significantly outperforms state-of-the-art baselines, achieving improvements of over 30\\% across multiple evaluation metrics. Further analysis demonstrates UniMob's robustness in both zero- and few-shot scenarios, underlines the importance of LLM guidance, verifies its privacy-preserving nature, and showcases its applicability for downstream tasks.", "AI": {"tldr": "提出了一种新的统一人类移动生成模型UniMob，该模型能够跨城市生成可靠的、隐私保护的人类移动数据。", "motivation": "目前的人类移动性生成方法在资源丰富的城市表现良好，在缺乏数据的地区效果差。因此提出了一个可以适用于任何规模的城市的数据生成模型。", "method": "使用LLM驱动的旅行规划器产生高层次的时间感知和语义有意义的行程计划；通过统一的空间嵌入模块将各个城市的区域投影到共享表示空间中；并通过基于扩散的移动性发生器捕捉人类运动的时空特性，由衍生出的行程计划引导。", "result": "在两个包含五个城市的真实数据集上的广泛实验显示，UniMob显著优于最先进的基线模型，在多个评估指标上提高了30%以上。进一步分析表明，该模型在零样本和少量样例场景中都表现出强大的鲁棒性，并验证了其隐私保护特性。", "conclusion": "所提出的UniMob模型不仅解决了不同城市之间的数据生成不平衡问题，而且展示了其广泛的适用性和优越的性能。"}}
{"id": "2602.19690", "pdf": "https://arxiv.org/pdf/2602.19690", "abs": "https://arxiv.org/abs/2602.19690", "authors": ["Qile Wang", "Prerana Khatiwada", "Avinash Chouhan", "Ashrey Mahesh", "Joy Mwaria", "Duy Duc Tran", "Kenneth E. Barner", "Matthew Louis Mauriello"], "title": "\"The explanation makes sense\": An Empirical Study on LLM Performance in News Classification and its Influence on Judgment in Human-AI Collaborative Annotation", "categories": ["cs.HC"], "comment": null, "summary": "The spread of media bias is a significant concern as political discourse shapes beliefs and opinions. Addressing this challenge computationally requires improved methods for interpreting news. While large language models (LLMs) can scale classification tasks, concerns remain about their trustworthiness. To advance human-AI collaboration, we investigate the feasibility of using LLMs to classify U.S. news by political ideology and examine their effect on user decision-making. We first compared GPT models with prompt engineering to state-of-the-art supervised machine learning on a 34k public dataset. We then collected 17k news articles and tested GPT-4 predictions with brief and detailed explanations. In a between-subjects study (N=124), we evaluated how LLM-generated explanations influence human annotation, judgment, and confidence. Results show that AI assistance significantly increases confidence ($p<.001$), with detailed explanations more persuasive and more likely to alter decisions. We highlight recommendations for AI explanations through thematic analysis and provide our dataset for further research.", "AI": {"tldr": "研究探讨了大语言模型（LLM）在新闻分类中的性能及其对人类决策的影响。", "motivation": "通过解决媒体偏见传播的问题，改善新闻解释的计算方法。评估大语言模型与人工标注合作的有效性及信任度。", "method": "比较GPT模型与传统监督学习算法在34k公开数据集上的表现；收集17k篇新闻文章，测试GPT-4的预测和生成简短或详细解释的效果；进行两组间研究评估AI生成说明对人类标注决策的影响。", "result": "结果显示AI支持显著提高信心（p<.001），详细的解释更具说服力且更有可能改变决定。", "conclusion": "提供关于人工智能解释的建议并通过主题分析提出进一步的研究数据集。"}}
{"id": "2602.19685", "pdf": "https://arxiv.org/pdf/2602.19685", "abs": "https://arxiv.org/abs/2602.19685", "authors": ["Xinyu Yuan", "Xixian Liu", "Ya Shi Zhang", "Zuobai Zhang", "Hongyu Guo", "Jian Tang"], "title": "PerturbDiff: Functional Diffusion for Single-Cell Perturbation Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Building Virtual Cells that can accurately simulate cellular responses to perturbations is a long-standing goal in systems biology. A fundamental challenge is that high-throughput single-cell sequencing is destructive: the same cell cannot be observed both before and after a perturbation. Thus, perturbation prediction requires mapping unpaired control and perturbed populations. Existing models address this by learning maps between distributions, but typically assume a single fixed response distribution when conditioned on observed cellular context (e.g., cell type) and the perturbation type. In reality, responses vary systematically due to unobservable latent factors such as microenvironmental fluctuations and complex batch effects, forming a manifold of possible distributions for the same observed conditions. To account for this variability, we introduce PerturbDiff, which shifts modeling from individual cells to entire distributions. By embedding distributions as points in a Hilbert space, we define a diffusion-based generative process operating directly over probability distributions. This allows PerturbDiff to capture population-level response shifts across hidden factors. Benchmarks on established datasets show that PerturbDiff achieves state-of-the-art performance in single-cell response prediction and generalizes substantially better to unseen perturbations. See our project page (https://katarinayuan.github.io/PerturbDiff-ProjectPage/), where code and data will be made publicly available (https://github.com/DeepGraphLearning/PerturbDiff).", "AI": {"tldr": "PerturbDiff 是一种通过扩散过程建模单细胞扰动响应的方法，该方法在概率分布的希尔伯特空间中定义生成模型。", "motivation": "现有模型通常假设给定观测条件下的单一固定反应分布，忽略了因未观察到的因素造成的反应变化。为了准确捕捉这些系统性差异，需要一种新的建模策略来解决这一问题。", "method": "通过将概率分布嵌入希尔伯特空间中的点，并定义一个在概率分布上的扩散生成过程，PerturbDiff 能够有效捕获整个群体的响应位移和变化。", "result": "实验表明，与现有模型相比，PerturbDiff 在单细胞反应预测方面达到了最先进的性能，并且对未见过的扰动具有更好的泛化能力。", "conclusion": "PerturbDiff 提供了一种新颖的方法来建模单细胞中的复杂扰动响应，它通过在概率分布上操作扩散过程成功地克服了现有模型的限制。"}}
{"id": "2602.19680", "pdf": "https://arxiv.org/pdf/2602.19680", "abs": "https://arxiv.org/abs/2602.19680", "authors": ["Fateme Abbasi", "Martin Böhm", "Jarosław Byrka", "Matin Mohammadi", "Yongho Shin"], "title": "Servicing Matched Client Pairs with Facilities", "categories": ["cs.DS"], "comment": null, "summary": "We study Facility Location with Matching, a Facility Location problem where, given additional information about which pair of clients is compatible to be matched, we need to match as many clients as possible and assign each matched client pair to a same open facility at minimum total cost. The problem is motivated by match-making services relevant in, for example, video games or social apps. It naturally generalizes two prominent combinatorial optimization problems -- Uncapacitated Facility Location and Minimum-cost Maximum Matching. Facility Location with Matching also generalizes the Even-constrained Facility Location problem studied by Kim, Shin, and An (Algorithmica 2023). We propose a linear programming (LP) relaxation for this problem, and present a 3.868-approximation algorithm. Our algorithm leverages the work on bifactor-approximation algorithms (Byrka and Aardal, SICOMP 2012); our main technical contribution is a rerouting subroutine that reroutes a fractional solution to be supported on a fixed maximum matching with only small additional cost. For a special case where all clients are matched, we provide a refined algorithm achieving an approximation ratio of 2.218. As our algorithms are based on rounding an optimal solution to the LP relaxation, these approximation results also give the same upper bounds on the integrality gap of the relaxation.", "AI": {"tldr": "研究了设施定位与匹配问题，旨在最小总成本下为尽可能多的客户配对并分配到同一个开放设施。", "motivation": "受视频游戏或社交应用中匹配服务启发，该论文处理的是一个结合了未约束设施位置和最少成本最大匹配的问题。", "method": "提出了线性规划松弛方法，并开发了一个3.868近似算法，利用双因子逼近算法的工作。主要技术贡献是一个重路由子程序，将分数解重新导向到固定的最大匹配上，仅增加少量额外成本。", "result": "对于所有客户都被匹配的特殊情况，提供了一种优化后的算法，达到2.218近似比。这些算法基于线性规划松弛解舍入方法，表明了该松弛模型的积分间隙上限。", "conclusion": "通过提出并证明了一个新的组合优化问题——设施定位与匹配问题，论文展示了有效的近似算法，并提供了理论上的积分间隙上界。"}}
{"id": "2602.19679", "pdf": "https://arxiv.org/pdf/2602.19679", "abs": "https://arxiv.org/abs/2602.19679", "authors": ["Hyeongjin Nam", "Daniel Sungho Jung", "Kyoung Mu Lee"], "title": "TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures", "categories": ["cs.CV", "cs.AI"], "comment": "Published at CVPR 2026, 20 pages including the supplementary material", "summary": "Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.", "AI": {"tldr": "本文提出了一种基于文本引导的3D人体和物体重建框架，旨在解决现有方法在非接触式交互和外观信息捕捉方面的不足。", "motivation": "现有的单图三维人体与物体联合重建技术主要依赖物理接触信息，并且忽视了全局上下文的重要性。为了弥补这些缺陷，本文提出了一种新的框架以支持更广泛的人体-物体互动场景并提高视觉合理性。", "method": "该方法引入了基于文本描述的语义对齐机制，结合外观特征进行三维重建，从而实现更加准确和一致的非接触式人体-物体交互建模。", "result": "实验结果表明本文框架在3D重建任务中达到了最先进的性能水平。", "conclusion": "通过改进现有技术，该研究能够更全面地理解和模拟人类与物体之间的互动方式。"}}
{"id": "2602.19674", "pdf": "https://arxiv.org/pdf/2602.19674", "abs": "https://arxiv.org/abs/2602.19674", "authors": ["Yue Pan", "Xingyao Wang", "Hanyue Zhang", "Liwei Liu", "Changxin Li", "Gang Yang", "Rong Sheng", "Yili Xia", "Ming Chu"], "title": "Continuous Telemonitoring of Heart Failure using Personalised Speech Dynamics", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Remote monitoring of heart failure (HF) via speech signals provides a non-invasive and cost-effective solution for long-term patient management. However, substantial inter-individual heterogeneity in vocal characteristics often limits the accuracy of traditional cross-sectional classification models. To address this, we propose a Longitudinal Intra-Patient Tracking (LIPT) scheme designed to capture the trajectory of relative symptomatic changes within individuals. Central to this framework is a Personalised Sequential Encoder (PSE), which transforms longitudinal speech recordings into context-aware latent representations. By incorporating historical data at each timestamp, the PSE facilitates a holistic assessment of the clinical trajectory rather than modelling discrete visits independently. Experimental results from a cohort of 225 patients demonstrate that the LIPT paradigm significantly outperforms the classic cross-sectional approaches, achieving a recognition accuracy of 99.7% for clinical status transitions. The model's high sensitivity was further corroborated by additional follow-up data, confirming its efficacy in predicting HF deterioration and its potential to secure patient safety in remote, home-based settings. Furthermore, this work addresses the gap in existing literature by providing a comprehensive analysis of different speech task designs and acoustic features. Taken together, the superior performance of the LIPT framework and PSE architecture validates their readiness for integration into long-term telemonitoring systems, offering a scalable solution for remote heart failure management.", "AI": {"tldr": "利用个性化的语音动态长期远程监控心力衰竭。", "motivation": "传统的横断面分类模型因个体间差异而准确度受限，本研究提出一种纵向跟踪方案，以捕捉个体内部相对症状变化的轨迹。", "method": "开发了一种长时程患者追踪（LIPT）框架和个性化序列编码器（PSE），将长期语音记录转换为上下文感知的潜在表示。该模型结合历史数据进行整体评估，而非独立建模每次访问。", "result": "在225名患者的队列中，该方案显著优于传统方法，识别准确率高达99.7%，并能有效预测心力衰竭恶化，证明了其远程监控能力。", "conclusion": "LIPT框架和PSE架构表现出色，为长期远程心力衰竭管理提供了可扩展的解决方案。"}}
{"id": "2602.19672", "pdf": "https://arxiv.org/pdf/2602.19672", "abs": "https://arxiv.org/abs/2602.19672", "authors": ["Jiayu Wang", "Yifei Ming", "Zixuan Ke", "Shafiq Joty", "Aws Albarghouthi", "Frederic Sala"], "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "AI": {"tldr": "SkillOrchestra是一种基于技能的编排框架，用于在多代理系统中进行细粒度的任务分配。", "motivation": "现有路由方法存在两个问题：输入级路由器做出粗略的查询级别决策；强化学习训练的调度器适应性差且容易导致执行崩溃。因此提出SkillOrchestra来解决这些问题。", "method": "SkillOrchestra通过从执行经验中学习细粒度技能，并在部署时根据当前交互的需求选择最佳代理，以此避免直接端到端的学习路由策略。这种方法可以建模每个代理的特定能力和成本。", "result": "实验表明，与最先进的RL基于编排器相比，SkillOrchestra在十个基准测试中的性能提高了22.5%，学习成本分别减少了700倍和300倍。", "conclusion": "通过显式技能建模，SkillOrchestra提供了可扩展、可解释且样本高效的任务分配方法，为数据密集型RL提供了一种有原则的替代方案。"}}
{"id": "2602.19668", "pdf": "https://arxiv.org/pdf/2602.19668", "abs": "https://arxiv.org/abs/2602.19668", "authors": ["He Zhu", "Ren Togo", "Takahiro Ogawa", "Kenji Hirata", "Minghui Tang", "Takaaki Yoshimura", "Hiroyuki Sugimori", "Noriko Nishioka", "Yukie Shimizu", "Kohsuke Kudo", "Miki Haseyama"], "title": "Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation. We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML. Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.", "AI": {"tldr": "本文提出了一种名为FedTAR的框架，该框架通过整合基于人口统计信息的个性化和时间感知全局聚合，解决了纵向医疗报告生成中的隐私约束和疾病进展动态问题。", "motivation": "现有的联邦学习方法忽视了纵向数据的时间演变特性以及患者特定异质性的问题，导致优化不稳定且无法生成高质量的纵向医疗报告。", "method": "FedTAR框架基于人口统计信息生成轻量级LoRA适配器，并执行时间残差聚合，利用元学习策略优化不同访问更新的权重。该方法旨在解决联邦环境中纵向数据的时间演变问题。", "result": "在J-MID和MIMIC-CXR数据集上的实验显示，FedTAR在语言准确性、时间连贯性和跨站点泛化方面均表现出显著改进。", "conclusion": "本文提出的FedTAR框架是一种稳健且保护隐私的方法，在联邦学习环境下解决了纵向医疗报告生成中的关键问题。"}}
{"id": "2602.19657", "pdf": "https://arxiv.org/pdf/2602.19657", "abs": "https://arxiv.org/abs/2602.19657", "authors": ["Duncan Adamson", "Paul G Spirakis"], "title": "Exploration of Always $S$-Connected Temporal Graphs", "categories": ["cs.DS"], "comment": null, "summary": "\\emph{Temporal graphs} are a generalisation of (static) graphs, defined by a sequence of \\emph{snapshots}, each a static graph defined over a common set of vertices. \\emph{Exploration} problems are one of the most fundamental and most heavily studied problems on temporal graphs, asking if a set of $m$ agents can visit every vertex in the graph, with each agent only allowed to traverse a single edge per snapshot. In this paper, we introduce and study \\emph{always $S$-connected} temporal graphs, a generalisation of always connected temporal graphs where, rather than forming a single connected component in each snapshot, we have at most $\\vert S \\vert$ components, each defined by the connection to a single vertex in the set $S$. We use this formulation as a tool for exploring graphs admitting an \\emph{$(r,b)$-division}, a partitioning of the vertex set into disconnected components, each of which is $S$-connected, where $\\vert S \\vert \\leq b$. We show that an always $S$-connected temporal graph with $m = \\vert S \\vert$ and an average degree of $Δ$ can be explored by $m$ agents in $O(n^{1.5} m^3 Δ^{1.5}\\log^{1.5}(n))$ snapshots. Using this as a subroutine, we show that any always-connected temporal graph with treewidth at most $k$ can be explored by a single agent in $O\\left(n^{4/3} k^{5.5}\\log^{2.5}(n)\\right)$ snapshots, improving on the current state-of-the-art for small values of $k$. Further, we show that interval graph with only a small number of large cliques can be explored by a single agent in $O\\left(n^{4/3} \\log^{2.5}(n)\\right)$ snapshots.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19655", "pdf": "https://arxiv.org/pdf/2602.19655", "abs": "https://arxiv.org/abs/2602.19655", "authors": ["Vishnu Subramanian"], "title": "Representation Stability in a Minimal Continual Learning Agent", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 1 figure", "summary": "Continual learning systems are increasingly deployed in environments where retraining or reset is infeasible, yet many approaches emphasize task performance rather than the evolution of internal representations over time. In this work, we study a minimal continual learning agent designed to isolate representational dynamics from architectural complexity and optimization objectives. The agent maintains a persistent state vector across executions and incrementally updates it as new textual data is introduced. We quantify representational change using cosine similarity between successive normalized state vectors and define a stability metric over time intervals. Longitudinal experiments across eight executions reveal a transition from an initial plastic regime to a stable representational regime under consistent input. A deliberately introduced semantic perturbation produces a bounded decrease in similarity, followed by recovery and restabilization under subsequent coherent input. These results demonstrate that meaningful stability plasticity tradeoffs can emerge in a minimal, stateful learning system without explicit regularization, replay, or architectural complexity. The work establishes a transparent empirical baseline for studying representational accumulation and adaptation in continual learning systems.", "AI": {"tldr": "研究了一个最小化持续学习代理的表示稳定性，通过分析其状态向量随时间的变化来评估。", "motivation": "许多持续学习系统更关注任务性能而非内部表征随时间变化的情况。本文旨在研究一个简单代理的表示演化，并定义了稳定的度量标准以理解表示稳定性和可塑性的权衡。", "method": "设计了一个保持状态向量并随新文本数据更新的最小化持续学习代理，使用余弦相似性量化连续状态向量之间的变化，并在多个实验中观察其稳定性。", "result": "实验证明，在一致输入下系统从初始可塑阶段过渡到稳定表示阶段；引入语义扰动后，会有一个有限的变化并随后恢复和再稳态化。", "conclusion": "研究结果表明，即使没有显式正则化、重放或架构复杂性，一个简单的状态学习系统也可以在表征累积和适应中实现有意义的稳定性和可塑性权衡。"}}
{"id": "2602.19654", "pdf": "https://arxiv.org/pdf/2602.19654", "abs": "https://arxiv.org/abs/2602.19654", "authors": ["Rampunit Kumar", "Aditya Maheshwari"], "title": "NEXUS: A compact neural architecture for high-resolution spatiotemporal air quality forecasting in Delhi National Capital Region", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages", "summary": "Urban air pollution in megacities poses critical public health challenges, particularly in Delhi National Capital Region (NCR) where severe degradation affects millions. We present NEXUS (Neural Extraction and Unified Spatiotemporal) architecture for forecasting carbon monoxide, nitrogen oxide, and sulfur dioxide. Working with four years (2018--2021) of atmospheric data across sixteen spatial grids, NEXUS achieves R$^2$ exceeding 0.94 for CO, 0.91 for NO, and 0.95 for SO$_2$ using merely 18,748 parameters -- substantially fewer than SCINet (35,552), Autoformer (68,704), and FEDformer (298,080). The architecture integrates patch embedding, low-rank projections, and adaptive fusion mechanisms to decode complex atmospheric chemistry patterns. Our investigation uncovers distinct diurnal rhythms and pronounced seasonal variations, with winter months experiencing severe pollution episodes driven by temperature inversions and agricultural biomass burning. Analysis identifies critical meteorological thresholds, quantifies wind field impacts on pollutant dispersion, and maps spatial heterogeneity across the region. Extensive ablation experiments demonstrate each architectural component's role. NEXUS delivers superior predictive performance with remarkable computational efficiency, enabling real-time deployment for air quality monitoring systems.", "AI": {"tldr": "本文介绍了NEXUS架构，用于预测印度德里首都区的CO、NO和SO₂浓度。", "motivation": "空气质量恶化对公众健康构成重大挑战。本研究旨在通过开发一种高效且准确的模型来解决这一问题。", "method": "该方法包括了补丁嵌入、低秩投影和自适应融合机制，以预测高分辨率时空空气污染情况。", "result": "NEXUS架构在CO、NO和SO₂的预测中分别达到了0.94、0.91和0.95的相关系数，并且参数数量比其他模型少很多。", "conclusion": "该研究展示了NEXUS架构在预测空气质量方面的优越性和效率，有助于实时监测系统的部署。"}}
{"id": "2602.19653", "pdf": "https://arxiv.org/pdf/2602.19653", "abs": "https://arxiv.org/abs/2602.19653", "authors": ["Bailey Dacre", "Rodrigo Moreno", "Jørn Lambertsen", "Kasper Stoy", "Andrés Faíña"], "title": "Scalable Low-Density Distributed Manipulation Using an Interconnected Actuator Array", "categories": ["cs.RO"], "comment": null, "summary": "Distributed Manipulator Systems, composed of arrays of robotic actuators necessitate dense actuator arrays to effectively manipulate small objects. This paper presents a system composed of modular 3-DoF robotic tiles interconnected by a compliant surface layer, forming a continuous, controllable manipulation surface. The compliant layer permits increased actuator spacing without compromising object manipulation capabilities, significantly reducing actuator density while maintaining robust control, even for smaller objects. We characterize the coupled workspace of the array and develop a manipulation strategy capable of translating objects to arbitrary positions within an N X N array. The approach is validated experimentally using a minimal 2 X 2 prototype, demonstrating the successful manipulation of objects with varied shapes and sizes.", "AI": {"tldr": "本文提出了一种由模块化机器人瓷砖组成的分布式操纵系统，通过弹性表面层互联，实现了低密度的高效物体操控。", "motivation": "密集的执行器阵列是有效操作小物件所必需的，然而这增加了系统的复杂性和成本。因此，本研究旨在开发一种减少执行器密度而不牺牲操控能力的技术。", "method": "该系统由模块化的3自由度机器人瓷砖组成，并通过弹性表面层互联形成一个连续可控的操作面。研究团队还设计了一种操纵策略，能够在N X N阵列中任意位置移动物体。", "result": "实验结果表明，在最小2 X 2原型上成功操控了形状和大小各异的物体。", "conclusion": "该系统展示了在低密度执行器阵列下实现高效小物件操作的能力，并为未来的研究提供了新的方向。"}}
{"id": "2602.19651", "pdf": "https://arxiv.org/pdf/2602.19651", "abs": "https://arxiv.org/abs/2602.19651", "authors": ["Lennart Röstel", "Berthold Bäuml"], "title": "Denoising Particle Filters: Learning State Estimation with Single-Step Objectives", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning-based methods commonly treat state estimation in robotics as a sequence modeling problem. While this paradigm can be effective at maximizing end-to-end performance, models are often difficult to interpret and expensive to train, since training requires unrolling sequences of predictions in time. As an alternative to end-to-end trained state estimation, we propose a novel particle filtering algorithm in which models are trained from individual state transitions, fully exploiting the Markov property in robotic systems. In this framework, measurement models are learned implicitly by minimizing a denoising score matching objective. At inference, the learned denoiser is used alongside a (learned) dynamics model to approximately solve the Bayesian filtering equation at each time step, effectively guiding predicted states toward the data manifold informed by measurements. We evaluate the proposed method on challenging robotic state estimation tasks in simulation, demonstrating competitive performance compared to tuned end-to-end trained baselines. Importantly, our method offers the desirable composability of classical filtering algorithms, allowing prior information and external sensor models to be incorporated without retraining.", "AI": {"tldr": "本文提出了一种新的基于粒子滤波的状态估计方法，通过单步目标训练模型来提高状态估计的效率和可解释性。", "motivation": "传统端到端学习方法虽然有效但难以理解和训练成本高。为了改进这一点，作者提出了一个新算法，利用单步目标训练模型，充分利用马尔科夫属性，并在推理过程中使用测量去噪器与动力学模型协同工作。", "method": "本文提出了一种基于粒子滤波的新方法，在该框架下通过最小化去噪得分匹配目标隐式学习测量模型。在推断阶段，利用学到的去噪器和动力学模型解决贝叶斯过滤方程，指导预测状态接近数据流形。", "result": "实验结果表明所提出的算法在仿真中的性能可与端到端基线相媲美，并且该方法具有经典滤波算法的组合性优势，可以轻松整合先前信息或外部传感器模型而无需重新训练。", "conclusion": "本文通过提出一种基于粒子滤波的新状态估计方法展示了如何利用单步目标优化来改进机器人系统的状态估计，这种方法不仅性能优越而且更加灵活易用。"}}
{"id": "2602.19649", "pdf": "https://arxiv.org/pdf/2602.19649", "abs": "https://arxiv.org/abs/2602.19649", "authors": ["Gabriel Bathie", "Paul Huber", "Guillaume Lagarde", "Akka Zemmari"], "title": "Analyzing and Leveraging the $k$-Sensitivity of LZ77", "categories": ["cs.DS", "cs.CC"], "comment": null, "summary": "We study the sensitivity of the Lempel-Ziv 77 compression algorithm to edits, showing how modifying a string $w$ can deteriorate or improve its compression. Our first result is a tight upper bound for $k$ edits: $\\forall w' \\in B(w,k)$, we have $C_{\\mathrm{LZ77}}(w') \\leq 3 \\cdot C_{\\mathrm{LZ77}}(w) + 4k$. This result contrasts with Lempel-Ziv 78, where a single edit can significantly deteriorate compressibility, a phenomenon known as a *one-bit catastrophe*. We further refine this bound, focusing on the coefficient $3$ in front of $C_{\\mathrm{LZ77}}(w)$, and establish a surprising trichotomy based on the compressibility of $w$. More precisely we prove the following bounds: - if $C_{\\mathrm{LZ77}}(w) \\lesssim k^{3/2}\\sqrt{n}$, the compression may increase by up to a factor of $\\approx 3$, - if $k^{3/2}\\sqrt{n} \\lesssim C_{\\mathrm{LZ77}}(w) \\lesssim k^{1/3}n^{2/3}$, this factor is at most $\\approx 2$, - if $C_{\\mathrm{LZ77}}(w) \\gtrsim k^{1/3}n^{2/3}$, the factor is at most $\\approx 1$. Finally, we present an $\\varepsilon$-approximation algorithm to pre-edit a word $w$ with a budget of $k$ modifications to improve its compression. In favorable scenarios, this approach yields a total compressed size reduction by up to a factor of~$3$, accounting for both the LZ77 compression of the modified word and the cost of storing the edits, $C_{\\mathrm{LZ77}}(w') + k \\log |w|$.", "AI": {"tldr": "研究LZ77压缩算法对编辑的敏感度，给出了经过$k$次编辑后的字符串压缩长度上限，并提出了一个近似算法来优化压缩。", "motivation": "探索LZ77算法在不同条件下修改后的字符串压缩性能变化规律，旨在理解其敏感性并提出改进方案。", "method": "首先证明了对于所有与原串相差$k$次编辑的字符串，其LZ77压缩长度不超过原串压缩长度的三倍加四$k$。然后根据原始数据的不同可压缩性建立了更精细的界限，并设计了一个基于预算$k$修改次数的近似算法来优化压缩。", "result": "证明了对于不同范围内的原始字符串压缩长度，经过编辑后的最大压缩因子分别为约3、2和1；提出的算法在特定场景下能将总压缩大小减少最多三倍。", "conclusion": "该研究揭示了LZ77压缩算法对少量编辑的稳定性和敏感性，为通过预处理改善数据压缩效果提供了理论支持。"}}
{"id": "2602.19634", "pdf": "https://arxiv.org/pdf/2602.19634", "abs": "https://arxiv.org/abs/2602.19634", "authors": ["Jesse Farebrother", "Matteo Pirotta", "Andrea Tirinzoni", "Marc G. Bellemare", "Alessandro Lazaric", "Ahmed Touati"], "title": "Compositional Planning with Jumpy World Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The ability to plan with temporal abstractions is central to intelligent decision-making. Rather than reasoning over primitive actions, we study agents that compose pre-trained policies as temporally extended actions, enabling solutions to complex tasks that no constituent alone can solve. Such compositional planning remains elusive as compounding errors in long-horizon predictions make it challenging to estimate the visitation distribution induced by sequencing policies. Motivated by the geometric policy composition framework introduced in arXiv:2206.08736, we address these challenges by learning predictive models of multi-step dynamics -- so-called jumpy world models -- that capture state occupancies induced by pre-trained policies across multiple timescales in an off-policy manner. Building on Temporal Difference Flows (arXiv:2503.09817), we enhance these models with a novel consistency objective that aligns predictions across timescales, improving long-horizon predictive accuracy. We further demonstrate how to combine these generative predictions to estimate the value of executing arbitrary sequences of policies over varying timescales. Empirically, we find that compositional planning with jumpy world models significantly improves zero-shot performance across a wide range of base policies on challenging manipulation and navigation tasks, yielding, on average, a 200% relative improvement over planning with primitive actions on long-horizon tasks.", "AI": {"tldr": "研究使用跳跃世界模型进行组合规划，以解决智能体在长时间预测中的误差累积问题。", "motivation": "由于长时序预测中误差的积累使得基于策略序列化难以估计访问分布，该论文旨在通过学习多步动态学模型来改进这种状况。", "method": "引入跳跃世界模型捕捉由预训练策略引起的状态占用，并使用时间差分流方法结合一致性目标以改善长时间预测准确性。此外还展示了如何组合这些生成预测以估算任意策略序列的价值。", "result": "实验表明，与基于基本动作的规划相比，在具有挑战性的操纵和导航任务中，跳跃世界模型在零样本性能上平均提高了200%。", "conclusion": "通过引入跳跃世界模型和时间差分流方法，显著改善了组合规划中的长时间预测准确性，并且在实际应用中取得了更好的效果。"}}
{"id": "2602.19633", "pdf": "https://arxiv.org/pdf/2602.19633", "abs": "https://arxiv.org/abs/2602.19633", "authors": ["Jongwon Jeong", "Jungtaek Kim", "Kangwook Lee"], "title": "TAPE: Tool-Guided Adaptive Planning and Constrained Execution in Language Model Agents", "categories": ["cs.AI"], "comment": "Preprint", "summary": "Language Model (LM) agents have demonstrated remarkable capabilities in solving tasks that require multiple interactions with the environment. However, they remain vulnerable in environments where a single error often leads to irrecoverable failure, particularly under strict feasibility constraints. We systematically analyze existing agent frameworks, identifying imperfect planning and stochastic execution as the primary causes. To address these challenges, we propose Tool-guided Adaptive Planning with constrained Execution (TAPE). TAPE enhances planning capability by aggregating multiple plans into a graph and employing an external solver to identify a feasible path. During execution, TAPE employs constrained decoding to reduce sampling noise, while adaptively re-planning whenever environmental feedback deviates from the intended state. Experiments across Sokoban, ALFWorld, MuSiQue, and GSM8K-Hard demonstrate that TAPE consistently outperforms existing frameworks, with particularly large gains on hard settings, improving success rates by 21.0 percentage points on hard settings on average, and by 20.0 percentage points for weaker base models on average. Code and data available at here.", "AI": {"tldr": "TAPE 提出了一种新的语言模型代理框架，旨在改善复杂任务中的规划和执行能力。", "motivation": "现有语言模型代理在面对环境错误时易发生不可恢复的失败，特别是在强约束条件下。主要原因是不完善的计划和随机执行。", "method": "通过将多个计划聚合到图中并使用外部求解器识别可行路径来增强规划能力；执行期间采用受限解码减少采样噪声，并根据环境反馈进行自适应重新规划。", "result": "TAPE 在 Sokoban、ALFWorld、MuSiQue 和 GSM8K-Hard 等任务上的表现优于现有框架，尤其是在困难设置下成功率提高了21.0个百分点。", "conclusion": "TAPE 能有效提升语言模型代理在复杂环境中的性能和鲁棒性。"}}
{"id": "2602.19631", "pdf": "https://arxiv.org/pdf/2602.19631", "abs": "https://arxiv.org/abs/2602.19631", "authors": ["Uichan Lee", "Jeonghyeon Kim", "Sangheum Hwang"], "title": "Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ICLR 2026. The first two authors contributed equally", "summary": "Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.", "AI": {"tldr": "本文提出了一种名为High-Level Representation Misdirection (HiRM)的方法，用于在文本到图像的扩散模型中精确去除目标概念，并且对非目标内容的影响最小。", "motivation": "随着文本到图像生成技术的发展，人们对潜在滥用这种技术产生了担忧。为此，研究者希望开发出一种方法来消除有害、私密或受版权保护的内容，同时保持其他方面的高质量生成能力。", "method": "HiRM通过在文本编码器中将目标概念的高级语义表示误导为指定向量（例如随机方向或定义明确的方向）的方式来实现这一目标。它仅更新包含视觉属性因果状态的早期层，从而实现了对特定概念的精确去除而不会影响到其他未涉及的概念。", "result": "实验表明，在UnlearnCanvas和NSFW等基准测试中，HiRM对于多种类型的目标（如物体、风格、裸体）都能表现出色。此外，它还能够在低训练成本下保持生成能力，并且可以无缝地应用于最新架构而无需额外训练。", "conclusion": "该研究提出了一种高效的概念去除方法——High-Level Representation Misdirection (HiRM)，不仅能够实现对特定视觉概念的有效抑制，而且在保护非目标内容的同时还能保证模型的生成质量。"}}
{"id": "2602.19629", "pdf": "https://arxiv.org/pdf/2602.19629", "abs": "https://arxiv.org/abs/2602.19629", "authors": ["Tatia Codreanu"], "title": "Cooperation After the Algorithm: Designing Human-AI Coexistence Beyond the Illusion of Collaboration", "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 2 tables", "summary": "Generative artificial intelligence systems increasingly participate in research, law, education, media, and governance. Their fluent and adaptive outputs create an experience of collaboration. However, these systems do not bear responsibility, incur liability, or share stakes in downstream consequences. This structural asymmetry has already produced sanctions, professional errors, and governance failures in high-stakes contexts We argue that stable human-AI coexistence is an institutional achievement that depends on governance infrastructure capable of distributing residual risk. Drawing on institutional analysis and evolutionary cooperation theory, we introduce a formal inequality that specifies when reliance on AI yields positive expected cooperative value. The model makes explicit how governance conditions, system policy, and accountability regimes jointly determine whether cooperation is rational or structurally defective. From this formalization we derive a cooperation ecology framework with six design principles: reciprocity contracts, visible trust infrastructure, conditional cooperation modes, defection-mitigation mechanisms, narrative literacy against authority theatre, and an Earth-first sustainability constraint. We operationalize the framework through three policy artefacts: a Human-AI Cooperation Charter, a Defection Risk Register, and a Cooperation Readiness Audit. Together, these elements shift the unit of analysis from the user-AI dyad to the institutional environment that shapes incentives, signals, accountability, and repair. The paper provides a theoretical foundation and practical toolkit for designing human-AI systems that can sustain accountable, trustworthy cooperation over time.", "AI": {"tldr": "设计人类与AI共存的框架，以实现负责任的合作", "motivation": "当前的人机合作存在结构不对称的问题，导致了责任缺失和信任危机。研究者希望通过理论分析和实践工具来解决这些问题，推动长期可持续的合作关系", "method": "基于机构分析和进化合作理论建立模型，并提出六个设计原则和三个政策工具", "result": "提出了一个合作生态框架以及具体的实施方法，旨在促进人类与AI之间的负责任合作", "conclusion": "通过制度设计和治理结构的优化，可以实现人类与AI之间稳定且可持续的合作"}}
{"id": "2602.19624", "pdf": "https://arxiv.org/pdf/2602.19624", "abs": "https://arxiv.org/abs/2602.19624", "authors": ["Jonas Serych", "Jiri Matas"], "title": "Accurate Planar Tracking With Robust Re-Detection", "categories": ["cs.CV"], "comment": null, "summary": "We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM", "AI": {"tldr": "本文提出了SAM-H和WOFTSAM两种新型平面跟踪器，结合了SAM 2提供的长期分割追踪与8自由度的单应性姿态估计。", "motivation": "为了提高平面跟踪的准确性和鲁棒性，特别是在目标外观变化时保持稳定性能。", "method": "SAM-H通过从分割掩模轮廓中估算单应性矩阵来实现高度稳健的目标追踪；WOFTSAM则利用SAM-H提供的丢失目标重检测功能显著提升当前最先进水平。", "result": "在POT-210和PlanarTrack跟踪基准测试上，所提方法均达到了新的最先进性能，在后者上以较大的差距超过第二名，分别提高了+12.4和+15.2百分点。", "conclusion": "本文提出的方法不仅改进了平面追踪的精度，还通过提供更准确的初始位置标注改善了基准测试的评估指标。"}}
{"id": "2602.19623", "pdf": "https://arxiv.org/pdf/2602.19623", "abs": "https://arxiv.org/abs/2602.19623", "authors": ["Injun Baek", "Yearim Kim", "Nojun Kwak"], "title": "PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional \"one-shot\" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.", "AI": {"tldr": "介绍了一个名为PedaCo-Gen的系统，该系统允许教育者与AI协作生成教学视频，并通过中间表示阶段提高视频质量。", "motivation": "现有文本到视频（T2V）生成人工智能模型通常更关注视觉保真度而不是教学效果。此研究旨在引入一种基于Mayer的认知多媒体学习理论（CTML）的教育导向型人机协作视频创作系统，以改善视频的教学质量。", "method": "提出了一个中间表示阶段，使教育者能够与AI进行交互，审查和细化包含脚本和视觉描述的视频草图。通过与23位教育专家的研究，展示了该方法的有效性。", "result": "研究结果表明，PedaCo-Gen在各种主题上提高了视频质量，并且参与者高度认可其生产效率和指南的有效性。", "conclusion": "这些发现强调了通过有原则的合作创作重新获得教学自主权的重要性，并为未来的人工智能创作工具奠定了基础。"}}
{"id": "2602.19622", "pdf": "https://arxiv.org/pdf/2602.19622", "abs": "https://arxiv.org/abs/2602.19622", "authors": ["Jingbo Zhou", "Jun Xia", "Siyuan Li", "Yunfan Liu", "Wenjun Wang", "Yufei Huang", "Changxi Chi", "Mutian Hong", "Zhuoli Ouyang", "Shu Wang", "Zhongqi Wang", "Xingyu Wu", "Chang Yu", "Stan Z. Li"], "title": "VecFormer: Towards Efficient and Generalizable Graph Transformer with Graph Token Attention", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Transformer has demonstrated impressive capabilities in the field of graph representation learning. However, existing approaches face two critical challenges: (1) most models suffer from exponentially increasing computational complexity, making it difficult to scale to large graphs; (2) attention mechanisms based on node-level operations limit the flexibility of the model and result in poor generalization performance in out-of-distribution (OOD) scenarios. To address these issues, we propose \\textbf{VecFormer} (the \\textbf{Vec}tor Quantized Graph Trans\\textbf{former}), an efficient and highly generalizable model for node classification, particularly under OOD settings. VecFormer adopts a two-stage training paradigm. In the first stage, two codebooks are used to reconstruct the node features and the graph structure, aiming to learn the rich semantic \\texttt{Graph Codes}. In the second stage, attention mechanisms are performed at the \\texttt{Graph Token} level based on the transformed cross codebook, reducing computational complexity while enhancing the model's generalization capability. Extensive experiments on datasets of various sizes demonstrate that VecFormer outperforms the existing Graph Transformer in both performance and speed.", "AI": {"tldr": "本文提出了VecFormer模型，旨在解决现有图变换器在大规模图形中的计算复杂度高和泛化能力弱的问题。", "motivation": "现有的图变换器面临两个主要挑战：一是随着图形规模的扩大，计算复杂性呈指数级增长；二是基于节点级别的注意力机制限制了模型灵活性，并导致了出界数据集场景下的性能下降。", "method": "VecFormer采用两阶段训练模式。第一阶段使用两个码本来重构节点特征和图结构以学习丰富的语义图代码。第二阶段在转换后的交叉码书基础上，基于图形标记的注意力机制执行操作，降低计算复杂性并提高模型泛化能力。", "result": "实验结果表明，VecFormer在性能和速度上都优于现有的图变换器。", "conclusion": "通过引入基于矢量量化编码的新方法，VecFormer能够有效解决大规模图形中的效率问题，并展现出更强的泛化能力。"}}
{"id": "2602.19620", "pdf": "https://arxiv.org/pdf/2602.19620", "abs": "https://arxiv.org/abs/2602.19620", "authors": ["Louth Bin Rawshan", "Zhuoyu Wang", "Brian Y Lim"], "title": "Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model", "categories": ["cs.AI"], "comment": null, "summary": "Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.", "AI": {"tldr": "本文提出了CoXAM模型，用于比较和分析不同解释性AI技术的理解效果，并验证了其在模拟人类决策方面的有效性。", "motivation": "缺乏认知框架来比较不同的可解释人工智能（XAI）技术的效果，本研究旨在通过用户研究识别出7种解读规则、权重及其混合的推理策略，并提出CoXAM模型以提供一个基于认知的基础加速调试和基准化不同XAI技术。", "method": "作者进行了一项用户研究，探讨了前向决策任务与反事实决策任务中解释性AI（XAI）方案的理解效果；提出了CoXAM模型，该模型采用共享记忆表示来编码实例属性、线性权重及决策规则，并根据效用和推理时间的权衡选择适当的推理过程。", "result": "在验证研究中，CoXAM模型显示了与人类决策更紧密的一致性，成功复制并解释了多个关键实验发现，包括反事实任务比前向任务更难、决策树规则比线性权重更难以回忆和应用等。此外，还揭示了不同推理策略的有效性。", "conclusion": "通过CoXAM模型提供了一个认知基础来加速调试与基准化不同的可解释人工智能技术，并为进一步改进这些技术提供了方向。"}}
{"id": "2602.19615", "pdf": "https://arxiv.org/pdf/2602.19615", "abs": "https://arxiv.org/abs/2602.19615", "authors": ["Xin Hu", "Haomiao Ni", "Yunbei Zhang", "Jihun Hamm", "Zechen Li", "Zhengming Ding"], "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.", "AI": {"tldr": "本文提出了一种插件式模块，通过优化视觉令牌和增强输入文本提示来提升视觉语言模型对罕见物体的识别能力。", "motivation": "视觉语言模型在广泛的视觉理解任务中表现出色，但在处理罕见物体时仍面临挑战。现有方法如检索额外数据或引入更强的视觉编码器虽有所改善但仍然计算密集且未充分利用原始训练数据。", "method": "本文提出了一种轻量级插件模块，通过从视觉基础模型和同义词增强文本描述中学习多模态类别嵌入来改进VLM对罕见物体的认知。这些嵌入通过基于注意力的增强模块优化视觉令牌，并作为对象感知检测器生成提示信息注入到文本提示中。", "result": "实验显示在两个基准测试上，预训练后的VLM在罕见物体识别和推理方面表现出显著改善。", "conclusion": "该方法增强了VLM聚焦并理解罕见物体的能力。"}}
{"id": "2602.19611", "pdf": "https://arxiv.org/pdf/2602.19611", "abs": "https://arxiv.org/abs/2602.19611", "authors": ["Mingxiu Cai", "Zhe Zhang", "Gaochang Wu", "Tianyou Chai", "Xiatian Zhu"], "title": "RAID: Retrieval-Augmented Anomaly Detection", "categories": ["cs.CV"], "comment": "ef:CVPR 2026", "summary": "Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.", "AI": {"tldr": "RAID是一种基于检索增强的无监督异常检测框架，旨在通过从多层次向量数据库中检索类、语义和实例级别表示来生成细粒度异常图。", "motivation": "现有的无监督异常检测方法主要依赖于图像重建或模板检索，但这些方法在处理测试图片与正常模板之间的匹配时会引入噪声。为解决这一挑战，作者提出了一种新的框架RAID，利用检索样本直接指导异常图的生成以实现去噪。", "method": "RAID通过从多层次向量数据库中检索类、语义和实例级别表示来形成一种粗到细管道，并使用匹配成本体与输入图像相关联。然后，一个引导式的混合专家（MoE）网络利用这些检索样本进行自适应的噪声抑制并生成细粒度异常图。", "result": "RAID在MVTec、VisA、MPDD和BTAD基准测试中实现了最先进的性能，在全样本、小样本以及多数据集设置下均表现出色。", "conclusion": "通过引入检索增强来提高无监督异常检测的鲁棒性和准确性，RAID为解决噪声问题提供了一种有效的解决方案，并在多种数据集上验证了其优越性。"}}
{"id": "2602.19608", "pdf": "https://arxiv.org/pdf/2602.19608", "abs": "https://arxiv.org/abs/2602.19608", "authors": ["Girmaw Abebe Tadesse", "Titien Bartette", "Andrew Hassanali", "Allen Kim", "Jonathan Chemla", "Andrew Zolli", "Yves Ubelmann", "Caleb Robinson", "Inbal Becker-Reshef", "Juan Lavista Ferres"], "title": "Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.", "AI": {"tldr": "论文提出了一个基于卫星图像的机器学习方法，用于识别被盗掘的文化遗址。", "motivation": "保护文化遗迹免受非法挖掘的威胁是一个严峻的问题。监控数千个偏远地区的操作难度大，需要一种有效的手段来检测被盗掘的考古地点。", "method": "通过使用PlanetScope每月拼贴图像和在阿富汗的1943个考古遗址（898个被破坏，1045个未受破坏）的数据集，论文比较了两种方法：一种是直接训练卷积神经网络模型识别原始RGB补丁；另一种则是基于手工制作的空间/纹理特征的传统机器学习方法。研究还展示了使用预训练的ImageNet CNN和空间掩码可以提高性能。", "result": "实验表明，在所有测试中，利用ImageNet预训练CNN结合空间掩码的方法能够达到0.926的F1评分；而传统ML模型（SatCLIP-V+RF+Mean）仅能达到0.710。进一步的研究发现，尽管存在领域偏移问题，但ImageNet预训练和空间掩蔽均能显著提升模型性能。", "conclusion": "论文通过对比研究证明了基于深度学习的方法在识别被盗掘的考古遗址方面比传统机器学习方法更加有效，并强调了盗墓迹象的高度局部化特性。"}}
{"id": "2602.19605", "pdf": "https://arxiv.org/pdf/2602.19605", "abs": "https://arxiv.org/abs/2602.19605", "authors": ["Chunlei Meng", "Guanhong Huang", "Rong Fu", "Runmin Jian", "Zhongxue Gan", "Chun Ouyang"], "title": "CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "This study has been Accepted by CVPR 2026", "summary": "Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.", "AI": {"tldr": "提出了一种跨层次语义协作表示(CLCR)方法，用于多模态学习。", "motivation": "现有方法在将所有模式投影到单个潜在空间以进行融合时忽略了多层次的异步语义结构，导致语义错位和错误传播。为了解决这个问题，提出了CLCR。", "method": "CLCR通过组织每个模式特征形成三层语义层次，并指定跨模态交互的层级约束。采用语义层次编码器来对齐浅、中、深特征；使用IntraCED将特征分解为共享和私有子空间并通过可学习令牌预算限制交叉模态注意机制。", "result": "实验表明，CLCR在情感识别等多个基准测试上的性能优异且跨任务泛化能力强。", "conclusion": "通过明确组织多层次的语义结构并引入特定约束，CLCR方法能够有效提升多模态表示的质量和准确性。"}}
{"id": "2602.19596", "pdf": "https://arxiv.org/pdf/2602.19596", "abs": "https://arxiv.org/abs/2602.19596", "authors": ["Yihang Tao", "Senkang Hu", "Haonan An", "Zhengru Fang", "Hangcheng Cao", "Yuguang Fang"], "title": "Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception", "categories": ["cs.CV"], "comment": "Accepted by CVPR'26", "summary": "Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\\% against state-of-the-art defenses while achieving 47\\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git", "AI": {"tldr": "本文提出了一种基于相互视图信息图的自适应对抗协作感知框架（MVIG攻击），以学习防御性协作系统中的漏洞知识，生成演化伪造风险图，并优化攻击位置、时间和持久性。", "motivation": "当前协作感知系统的防御措施易受更高级别策略的影响，如利用时序和目标区域优化以及通过共享数据中的隐式置信信息无意中披露的脆弱性知识。为了应对这些挑战，本文旨在开发一种新型自适应攻击框架以增强对现有系统漏洞的理解。", "method": "该方法结合了相互视图信息图表示与时间图学习，生成动态伪造风险地图，并使用熵感知搜索策略优化攻击的位置、时间和持久性。", "result": "在OPV2V和Adv-OPV2V数据集上的实验表明，MVIG攻击能将防御成功率降低高达62%，并且对于持续性的攻击检测率降低了47%的同时还能保持每秒处理帧数为29.9 FPS。", "conclusion": "本文展示了自适应对抗协作感知系统中利用相互视图信息图表示的潜在风险，并证明了其对现有防御措施的有效性，揭示了协作感知系统的重大安全漏洞。"}}
{"id": "2602.19591", "pdf": "https://arxiv.org/pdf/2602.19591", "abs": "https://arxiv.org/abs/2602.19591", "authors": ["Yijiashun Qi", "Hanzhe Guo", "Yijiazhen Qi"], "title": "Detecting High-Potential SMEs with Heterogeneous Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Small and Medium Enterprises (SMEs) constitute 99.9% of U.S. businesses and generate 44% of economic activity, yet systematically identifying high-potential SMEs remains an open challenge. We introduce SME-HGT, a Heterogeneous Graph Transformer framework that predicts which SBIR Phase I awardees will advance to Phase II funding using exclusively public data. We construct a heterogeneous graph with 32,268 company nodes, 124 research topic nodes, and 13 government agency nodes connected by approximately 99,000 edges across three semantic relation types. SME-HGT achieves an AUPRC of 0.621 0.003 on a temporally-split test set, outperforming an MLP baseline (0.590 0.002) and R-GCN (0.608 0.013) across five random seeds. At a screening depth of 100 companies, SME-HGT attains 89.6% precision with a 2.14 lift over random selection. Our temporal evaluation protocol prevents information leakage, and our reliance on public data ensures reproducibility. These results demonstrate that relational structure among firms, research topics, and funding agencies provides meaningful signal for SME potential assessment, with implications for policymakers and early-stage investors.", "AI": {"tldr": "本文提出了SME-HGT框架，使用异构图神经网络预测哪些小型企业会获得进一步的资金支持。", "motivation": "系统地识别具有高潜力的小型企业是一个挑战，该研究旨在利用公开数据通过构建异构图来提高识别的准确性。", "method": "采用Heterogeneous Graph Transformer（SME-HGT）框架，并与MLP和R-GCN进行对比实验。", "result": "在测试集上，SME-HGT达到了AUPRC为0.621，优于其他模型；在筛选深度为100家公司时达到89.6%的精度，比随机选择高出2.14倍。", "conclusion": "关系结构对企业潜力评估提供了有意义的信息，对政策制定者和早期投资者具有重要意义。"}}
{"id": "2602.19585", "pdf": "https://arxiv.org/pdf/2602.19585", "abs": "https://arxiv.org/abs/2602.19585", "authors": ["Chunlei Meng", "Jiabin Luo", "Zhenglin Yan", "Zhenyu Yu", "Rong Fu", "Zhongxue Gan", "Chun Ouyang"], "title": "Tri-Subspaces Disentanglement for Multimodal Sentiment Analysis", "categories": ["cs.MM", "cs.AI"], "comment": "This study has been Accepted by CVPR 2026", "summary": "Multimodal Sentiment Analysis (MSA) integrates language, visual, and acoustic modalities to infer human sentiment. Most existing methods either focus on globally shared representations or modality-specific features, while overlooking signals that are shared only by certain modality pairs. This limits the expressiveness and discriminative power of multimodal representations. To address this limitation, we propose a Tri-Subspace Disentanglement (TSD) framework that explicitly factorizes features into three complementary subspaces: a common subspace capturing global consistency, submodally-shared subspaces modeling pairwise cross-modal synergies, and private subspaces preserving modality-specific cues. To keep these subspaces pure and independent, we introduce a decoupling supervisor together with structured regularization losses. We further design a Subspace-Aware Cross-Attention (SACA) fusion module that adaptively models and integrates information from the three subspaces to obtain richer and more robust representations. Experiments on CMU-MOSI and CMU-MOSEI demonstrate that TSD achieves state-of-the-art performance across all key metrics, reaching 0.691 MAE on CMU-MOSI and 54.9% ACC-7 on CMU-MOSEI, and also transfers well to multimodal intent recognition tasks. Ablation studies confirm that tri-subspace disentanglement and SACA jointly enhance the modeling of multi-granular cross-modal sentiment cues.", "AI": {"tldr": "本文提出了一种三子空间解缠框架，用于多模态情感分析，旨在提高表示的表达性和判别性。", "motivation": "现有的方法通常只关注全局共享表征或特定模态特征，忽略了某些模态对之间的共同信号，这限制了多模态表示的能力。因此本文提出了解决这一问题的方法。", "method": "提出了一种三子空间解缠（TSD）框架，并引入了一个去耦合监督器和结构化正则损失来保持这些子空间的纯度和独立性。还设计了一个子空间感知交叉注意模块，以适应地建模和集成来自三个子空间的信息。", "result": "实验结果显示，该方法在CMU-MOSI上达到了0.691 MAE，在CMU-MOSEI上达到了54.9% ACC-7，并且在多模态意图识别任务中表现良好。", "conclusion": "三子空间解缠和SACA模块共同提高了跨模态情感线索的建模能力，实验结果证明了这一点。"}}
{"id": "2602.19584", "pdf": "https://arxiv.org/pdf/2602.19584", "abs": "https://arxiv.org/abs/2602.19584", "authors": ["Biswajit Sadhu", "Kalpak Gupte", "Trijit Sadhu", "S. Anand"], "title": "Interpolation-Driven Machine Learning Approaches for Plume Shine Dose Estimation: A Comparison of XGBoost, Random Forest, and TabNet", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 11 figures, 3 tables", "summary": "Despite the success of machine learning (ML) in surrogate modeling, its use in radiation dose assessment is limited by safety-critical constraints, scarce training-ready data, and challenges in selecting suitable architectures for physics-dominated systems. Within this context, rapid and accurate plume shine dose estimation serves as a practical test case, as it is critical for nuclear facility safety assessment and radiological emergency response, while conventional photon-transport-based calculations remain computationally expensive. In this work, an interpolation-assisted ML framework was developed using discrete dose datasets generated with the pyDOSEIA suite for 17 gamma-emitting radionuclides across varying downwind distances, release heights, and atmospheric stability categories. The datasets were augmented using shape-preserving interpolation to construct dense, high-resolution training data. Two tree-based ML models (Random Forest and XGBoost) and one deep learning (DL) model (TabNet) were evaluated to examine predictive performance and sensitivity to dataset resolution. All models showed higher prediction accuracy with the interpolated high-resolution dataset than with the discrete data; however, XGBoost consistently achieved the highest accuracy. Interpretability analysis using permutation importance (tree-based models) and attention-based feature attribution (TabNet) revealed that performance differences stem from how the models utilize input features. Tree-based models focus mainly on dominant geometry-dispersion features (release height, stability category, and downwind distance), treating radionuclide identity as a secondary input, whereas TabNet distributes attention more broadly across multiple variables. For practical deployment, a web-based GUI was developed for interactive scenario evaluation and transparent comparison with photon-transport reference calculations.", "AI": {"tldr": "该论文使用插值增强的机器学习方法比较了XGBoost、随机森林和TabNet在放射性烟羽照射剂量估计中的性能。", "motivation": "传统光子传输计算对于核设施安全评估及辐射应急响应来说过于耗时，因此研究目的是开发一种快速准确的方法来估算放射性烟云照射剂量。", "method": "使用pyDOSEIA软件套件生成不同距离、释放高度和大气稳定度的17种伽马发射性核素的离散剂量数据集，并通过插值法构造密集高分辨率训练数据，对比XGBoost、随机森林及TabNet模型性能。", "result": "所有模型在插值后的高分辨率数据上表现更好；其中，XGBoost精度最高。解释性分析显示树基模型侧重于主要几何扩散特性，而TabNet则更广泛地分布注意权重。", "conclusion": "该研究通过机器学习方法提高了放射性烟云照射剂量估计的效率和准确性，并开发了交互式网页工具以供实用部署。"}}
{"id": "2602.19577", "pdf": "https://arxiv.org/pdf/2602.19577", "abs": "https://arxiv.org/abs/2602.19577", "authors": ["Kordel K. France", "Ovidiu Daescu", "Latifur Khan", "Rohith Peddi"], "title": "Chasing Ghosts: A Simulation-to-Real Olfactory Navigation Stack with Optional Vision Augmentation", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous odor source localization remains a challenging problem for aerial robots due to turbulent airflow, sparse and delayed sensory signals, and strict payload and compute constraints. While prior unmanned aerial vehicle (UAV)-based olfaction systems have demonstrated gas distribution mapping or reactive plume tracing, they rely on predefined coverage patterns, external infrastructure, or extensive sensing and coordination. In this work, we present a complete, open-source UAV system for online odor source localization using a minimal sensor suite. The system integrates custom olfaction hardware, onboard sensing, and a learning-based navigation policy trained in simulation and deployed on a real quadrotor. Through our minimal framework, the UAV is able to navigate directly toward an odor source without constructing an explicit gas distribution map or relying on external positioning systems. Vision is incorporated as an optional complementary modality to accelerate navigation under certain conditions. We validate the proposed system through real-world flight experiments in a large indoor environment using an ethanol source, demonstrating consistent source-finding behavior under realistic airflow conditions. The primary contribution of this work is a reproducible system and methodological framework for UAV-based olfactory navigation and source finding under minimal sensing assumptions. We elaborate on our hardware design and open source our UAV firmware, simulation code, olfaction-vision dataset, and circuit board to the community. Code, data, and designs will be made available at https://github.com/KordelFranceTech/ChasingGhosts.", "AI": {"tldr": "研究提出了一种基于最小传感器套件的自主嗅觉导航系统，使无人机能够在现实空气流动条件下直接定位气味源。", "motivation": "由于湍流气流、稀疏且延迟的感官信号以及严格的负载和计算限制，空中机器人上的自动气味定位仍然是一个挑战。现有系统依赖预定义覆盖模式、外部基础设施或广泛的传感与协调。", "method": "该研究开发了一个集成定制嗅觉硬件、在板传感器和通过模拟训练并在实际四旋翼飞机上部署的基于学习导航策略的无人机系统，视觉作为可选辅助模态以加速特定条件下的导航。", "result": "在使用乙醇源的真实世界飞行试验中验证了该系统的有效性，在大型室内环境中展示了持续找到气味源的行为。主要贡献是一个可重复的系统和方法论框架，用于基于无人机的嗅觉导航与定位。", "conclusion": "研究提出了一个完整的开放源代码无人机系统，用于在线气味源定位，并公开其硬件设计、无人机固件、模拟代码、嗅觉视觉数据集及电路板。"}}
{"id": "2602.19575", "pdf": "https://arxiv.org/pdf/2602.19575", "abs": "https://arxiv.org/abs/2602.19575", "authors": ["Minseo Kim", "Minchan Kwon", "Dongyeun Lee", "Yunho Jeon", "Junmo Kim"], "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2026", "summary": "Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.", "AI": {"tldr": "本文提出了一种名为ConceptPrism的新框架，通过比较集合内的图像自动分离共享的视觉概念和特定于图像的残差。", "motivation": "个性化文本到图像生成面临概念纠缠问题，即参考图中的无关信息被捕捉，导致概念忠实性和文本对齐之间的权衡。现有的解脱方法依赖人工指导如语言提示或分割掩码，限制了其应用范围并无法完全表达目标概念。", "method": "ConceptPrism通过两个互补的目标来优化目标令牌和图像特定残差令牌：重建损失确保保真度，排斥损失促使残差令牌丢弃共享的概念。此过程允许目标令牌捕获纯概念而无需直接监督。", "result": "实验表明，ConceptPrism有效解决了概念纠缠问题，显著改善了忠实性和对齐之间的权衡。", "conclusion": "本文提出的方法实现了自动化的视觉概念分离，提高了个性化文本到图像生成的性能。"}}
{"id": "2602.19574", "pdf": "https://arxiv.org/pdf/2602.19574", "abs": "https://arxiv.org/abs/2602.19574", "authors": ["Hanwen Liu", "Saierdaer Yusuyin", "Hao Huang", "Zhijian Ou"], "title": "CTC-TTS: LLM-based dual-streaming text-to-speech with CTC alignment", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "Submitted to INTERSPEECH 2026", "summary": "Large-language-model (LLM)-based text-to-speech (TTS) systems can generate natural speech, but most are not designed for low-latency dual-streaming synthesis. High-quality dual-streaming TTS depends on accurate text--speech alignment and well-designed training sequences that balance synthesis quality and latency. Prior work often relies on GMM-HMM based forced-alignment toolkits (e.g., MFA), which are pipeline-heavy and less flexible than neural aligners; fixed-ratio interleaving of text and speech tokens struggles to capture text--speech alignment regularities. We propose CTC-TTS, which replaces MFA with a CTC based aligner and introduces a bi-word based interleaving strategy. Two variants are designed: CTC-TTS-L (token concatenation along the sequence length) for higher quality and CTC-TTS-F (embedding stacking along the feature dimension) for lower latency. Experiments show that CTC-TTS outperforms fixed-ratio interleaving and MFA-based baselines on streaming synthesis and zero-shot tasks. Speech samples are available at https://ctctts.github.io/.", "AI": {"tldr": "该论文提出了CTC-TTS模型，用于基于LLM的双流实时语音合成。", "motivation": "现有的LLM基TTS系统通常无法实现低延迟的双流实时合成。高质量的双流TTS需要准确的文字与语音对齐和良好的训练序列设计以平衡生成质量和延时。传统方法依赖于GMM-HMM强制对齐工具包，这些工具重且不灵活。", "method": "CTC-TTS利用基于CTC的对准器替代了MFA，并引入了一种双词基的交错策略。该模型有两个变体：CTC-TTS-L（令牌按序列长度连接）以提高质量；CTC-TTS-F（嵌入特征维度堆叠）以降低延迟。", "result": "实验结果表明，CTC-TTS在实时合成和零样本任务上优于固定比例交错方法及基于MFA的基线模型。", "conclusion": "CTC-TTS通过改进对齐策略和技术细节实现了高质量与低延迟并存的LLM基TTS系统。"}}
{"id": "2602.19571", "pdf": "https://arxiv.org/pdf/2602.19571", "abs": "https://arxiv.org/abs/2602.19571", "authors": ["Chang Liu", "Yunfan Ye", "Qingyang Zhou", "Xichen Tan", "Mengxuan Luo", "Zhenyu Qiu", "Wei Peng", "Zhiping Cai"], "title": "HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies", "categories": ["cs.CV"], "comment": null, "summary": "Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.", "AI": {"tldr": "HOCA-Bench 是一个用于评估视频大模型在预测性世界建模中的表现的基准，通过将物理异常划分为本体论和因果两类来测试其对基本物理规律的应用能力。", "motivation": "尽管视频-LLMs 在语义感知方面有所进步，但在预测性世界建模上仍然存在不足。为了评估这些模型在应用基本物理规律上的表现，引入了HOCA-Bench。", "method": "通过使用最先进的生成式视频模型作为对抗模拟器构建了一个包含1439个视频（3470对问答）的测试集，并对17种Video-LLMs进行了评测。将异常分为本体论和因果两类，以评估其在识别静态本体论违反与动态因果机制上的表现。", "result": "模型在识别静态本体论违反方面表现出色，但在处理动态因果机制如重力或摩擦上表现较差，性能下降超过20%。启用系统-2“思考”模式能提高推理能力但未能缩小差距。", "conclusion": "目前的架构更擅长识别视觉模式而不是应用基本物理规律，表明在预测性世界建模方面还有很大的提升空间。"}}
{"id": "2602.19570", "pdf": "https://arxiv.org/pdf/2602.19570", "abs": "https://arxiv.org/abs/2602.19570", "authors": ["Nadav Kadvil", "Ayellet Tal"], "title": "VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.", "AI": {"tldr": "本文提出了一种针对大视觉语言模型的多阶段图像攻击检测方法，该方法结合了图像变换和数据整合来恢复正确的模型行为。", "motivation": "大型视觉语言模型可能受到对抗性图像的影响，导致它们输出错误的答案。因此，开发一种有效的、无需训练的防御机制是必要的。", "method": "本文提出了一种两阶段检测机制，首先通过内容保持变换评估图像的一致性，在低计算成本下快速过滤出大多数正常输入；对于更复杂的案例，则在文本嵌入空间中检查差异，并在必要时使用强大的LLM解决攻击引起的分歧。方法的关键思想是整合多个响应。", "result": "该方法实现了最先进的准确率，同时保持了较高的效率：大多数正常图像跳过了昂贵的处理过程，在存在大量对抗性示例的情况下，开销仍然最小。", "conclusion": "本文提出了一种高效的、无需训练的方法来防御LVLM中的图像攻击。通过多阶段检测机制和数据整合策略实现了高准确率和低计算成本的平衡。"}}
{"id": "2602.19569", "pdf": "https://arxiv.org/pdf/2602.19569", "abs": "https://arxiv.org/abs/2602.19569", "authors": ["Wuzhenghong Wen", "Bowen Zhou", "Jinwen Huang", "Xianjie Wu", "Yuwei Sun", "Su Pan", "Liang Li", "Jianting Liu"], "title": "Temporal-Aware Heterogeneous Graph Reasoning with Multi-View Fusion for Temporal Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "6pages", "summary": "Question Answering over Temporal Knowledge Graphs (TKGQA) has attracted growing interest for handling time-sensitive queries. However, existing methods still struggle with: 1) weak incorporation of temporal constraints in question representation, causing biased reasoning; 2) limited ability to perform explicit multi-hop reasoning; and 3) suboptimal fusion of language and graph representations. We propose a novel framework with temporal-aware question encoding, multi-hop graph reasoning, and multi-view heterogeneous information fusion. Specifically, our approach introduces: 1) a constraint-aware question representation that combines semantic cues from language models with temporal entity dynamics; 2) a temporal-aware graph neural network for explicit multi-hop reasoning via time-aware message passing; and 3) a multi-view attention mechanism for more effective fusion of question context and temporal graph knowledge. Experiments on multiple TKGQA benchmarks demonstrate consistent improvements over multiple baselines.", "AI": {"tldr": "提出了一种针对时间敏感查询的新型框架，用于在时空知识图上进行问答。", "motivation": "现有方法处理时间约束、多跳推理和语言与图形表示融合方面存在不足。", "method": "采用时序感知问题编码、多跳图推理以及多视图异构信息融合的方法。", "result": "实验结果显示该方法优于多个基线模型。", "conclusion": "所提出框架有效提升了时空问答系统的性能。"}}
{"id": "2602.19565", "pdf": "https://arxiv.org/pdf/2602.19565", "abs": "https://arxiv.org/abs/2602.19565", "authors": ["Li Zhang", "Mingyu Mei", "Ailing Wang", "Xianhui Meng", "Yan Zhong", "Xinyuan Song", "Liu Liu", "Rujing Wang", "Zaixing He", "Cewu Lu"], "title": "DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICArt (DIsCrete Diffusion for Articulation Pose Estimation), a novel framework that formulates pose estimation as a conditional discrete diffusion process. Instead of operating in a continuous domain, DICArt progressively denoises a noisy pose representation through a learned reverse diffusion procedure to recover the GT pose. To improve modeling fidelity, we propose a flexible flow decider that dynamically determines whether each token should be denoised or reset, effectively balancing the real and noise distributions during diffusion. Additionally, we incorporate a hierarchical kinematic coupling strategy, estimating the pose of each rigid part hierarchically to respect the object's kinematic structure. We validate DICArt on both synthetic and real-world datasets. Experimental results demonstrate its superior performance and robustness. By integrating discrete generative modeling with structural priors, DICArt offers a new paradigm for reliable category-level 6D pose estimation in complex environments.", "AI": {"tldr": "DICArt通过离散扩散过程进行关节对象姿态估计，提高复杂环境下的6D姿态估计的可靠性。", "motivation": "当前方法在连续空间中回归姿态时面临搜索空间大和忽略固有动力学约束的问题。DICArt旨在解决这些问题，提供更准确、鲁棒的姿态估计。", "method": "DICArt将姿态估计视为条件离散扩散过程，通过学习的逆扩散程序逐步去噪以恢复真实姿态，使用灵活流动决策器动态决定每个标记是否被去噪或重置，并采用分层动力学耦合策略估计刚性部分姿态。", "result": "实验结果显示，DICArt在合成和现实世界数据集上表现出优越性能和鲁棒性。", "conclusion": "通过将离散生成模型与结构先验结合，DICArt为复杂环境下的可靠类别级6D姿态估计提供了一种新范式。"}}
{"id": "2602.19562", "pdf": "https://arxiv.org/pdf/2602.19562", "abs": "https://arxiv.org/abs/2602.19562", "authors": ["Joseph Bingham"], "title": "A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data", "categories": ["cs.AI", "cs.CV"], "comment": "19 Pages, 6 figures, preprint", "summary": "Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .", "AI": {"tldr": "本文介绍了一种计算框架，通过结合语言表达与感知表示来建立自然语言和视觉认知之间的稳定映射。", "motivation": "在嘈杂且模糊的感知环境中，人类如何将语言参考固定下来仍然是未解之谜。此研究旨在模拟人类参照解释的核心方面，理解跨模态对齐机制。", "method": "该系统利用尺度不变特征变换（SIFT）与通用质量指数（UQI）相结合的方法来量化相似性，并通过一系列语言预处理和查询转换操作捕捉引用表达的实用变化。模型在斯坦福重复参考游戏语料库上进行评估。", "result": "框架需要比人类交流者少65%的表述次数以建立稳定的映射关系，且能从单一引述中正确识别目标对象41.66%的时间（相比人类为20%）。", "conclusion": "研究表明，相对简单的感知语言对齐机制可以在经典认知基准上产生接近于人的行为，并提供了关于基于地面的通信、感知推理和跨模态概念形成的见解。"}}
{"id": "2602.19560", "pdf": "https://arxiv.org/pdf/2602.19560", "abs": "https://arxiv.org/abs/2602.19560", "authors": ["Kynnedy Simone Smith", "Lydia B. Chilton", "Danielle Bragg"], "title": "Identifying, Explaining, and Correcting Ableist Language with AI", "categories": ["cs.HC"], "comment": "17 pages, 6 figures, Accepted for publication in CHI'26, Barcelona, Spain, April 13 - 17, 2026; CHI '26: ACM CHI Conference on Human Factors in Computing Systems", "summary": "Ableist language perpetuates harmful stereotypes and exclusion, yet its nuanced nature makes it difficult to recognize and address. Artificial intelligence could serve as a powerful ally in the fight against ableist language, offering tools that detect and suggest alternatives to biased terms. This two-part study investigates the potential of large language models (LLMs), specifically ChatGPT, to rectify ableist language and educate users about inclusive communication. We compared GPT-4o generations with crowdsourced annotations from trained disability community members, then invited disabled participants to evaluate both. Participants reported equal agreement with human and AI annotations but significantly preferred the AI, citing its narrative consistency and accessible style. At the same time, they valued the emotional depth and cultural grounding of human annotations. These findings highlight the promise and limits of LLMs in handling culturally sensitive content. Our contributions include a dataset of nuanced ableism annotations and design considerations for inclusive writing tools.", "AI": {"tldr": "本文研究了大型语言模型（LLM）在识别和纠正歧视性语言方面的潜力，以及它们在教育用户进行包容性沟通中的作用。", "motivation": "歧视性语言会传播有害刻板印象并导致排斥，但由于其复杂特性难以被发现和解决。人工智能可以通过提供检测和建议替代偏见术语的工具来帮助对抗这种现象。", "method": "研究分为两个部分：首先比较了GPT-4生成的结果与训练过的残障社区成员提供的众包注释；其次邀请残障参与者评估两者的表现。", "result": "参与者对人类和AI注释的一致性评价相同，但更倾向于AI注释，因为它们具有叙述一致性且风格易于理解。同时他们也重视人类注释的情感深度和文化背景。", "conclusion": "研究展示了LLM在处理文化敏感内容上的潜力与局限，并提供了细微歧视行为注释的数据集以及包容性写作工具的设计考虑因素。"}}
{"id": "2602.19555", "pdf": "https://arxiv.org/pdf/2602.19555", "abs": "https://arxiv.org/abs/2602.19555", "authors": ["Xiaochong Jiang", "Shiqi Yang", "Wenting Yang", "Yichen Liu", "Cheng Ji"], "title": "Agentic AI as a Cybersecurity Attack Surface: Threats, Exploits, and Defenses in Runtime Supply Chains", "categories": ["cs.CR", "cs.AI"], "comment": "9 Pages, 3 figures", "summary": "Agentic systems built on large language models (LLMs) extend beyond text generation to autonomously retrieve information and invoke tools. This runtime execution model shifts the attack surface from build-time artifacts to inference-time dependencies, exposing agents to manipulation through untrusted data and probabilistic capability resolution. While prior work has focused on model-level vulnerabilities, security risks emerging from cyclic and interdependent runtime behavior remain fragmented. We systematize these risks within a unified runtime framework, categorizing threats into data supply chain attacks (transient context injection and persistent memory poisoning) and tool supply chain attacks (discovery, implementation, and invocation). We further identify the Viral Agent Loop, in which agents act as vectors for self-propagating generative worms without exploiting code-level flaws. Finally, we advocate a Zero-Trust Runtime Architecture that treats context as untrusted control flow and constrains tool execution through cryptographic provenance rather than semantic inference.", "AI": {"tldr": "该论文研究了基于大型语言模型（LLM）的代理系统在运行时供应链中的安全威胁，并提出了一种零信任运行时架构来防御这些威胁。", "motivation": "当前的研究主要集中在模型层面的安全漏洞，而由循环和相互依赖的行为所引发的风险仍然没有得到充分关注。论文旨在系统地研究基于LLM的代理系统的运行时供应链攻击面。", "method": "论文提出了一个统一的运行时间框架来分类威胁，并且描述了数据供应链攻击（包括瞬态上下文注入和持久内存中毒）和工具供应链攻击（涉及发现、实现和调用）。还引入了一个称为Viral Agent Loop的概念，其中代理充当自我传播的生成蠕虫载体。", "result": "论文识别出基于LLM的代理系统的运行时安全威胁，并提出了一种零信任运行时间架构来防御这些威胁。该架构通过加密来源而非语义推理来约束工具执行。", "conclusion": "基于大型语言模型（LLM）的代理系统在运行时供应链中面临多种安全威胁，论文建议采用零信任运行时架构以防范这些新出现的风险。"}}
{"id": "2602.19554", "pdf": "https://arxiv.org/pdf/2602.19554", "abs": "https://arxiv.org/abs/2602.19554", "authors": ["Daniel A. Muñoz"], "title": "Sound-first immersive training for blind and low-vision learners: A simulation flow for safe, standardized orientation, mobility, and daily living practice", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Orientation and mobility (O&M) instruction for blind and low-vision learners is effective but difficult to standardize and repeat at scale due to the reliance on instructor availability, physical mock-ups, and variable real-world outdoor conditions. This Technical Note presents a sound-first immersive training flow that uses spatial audio and sonification as the primary channel for action and feedback in pre-street O&M and daily-living practice. The approach specifies parameterized scenario templates (e.g., signalized street crossing, public transport boarding, and kitchen tasks), a compact and consistent cue vocabulary with clear spectral placement and timing to mitigate masking, and a lightweight safety protocol enabling graded exposure, content warnings, seated starts, opt-outs, and structured debriefs. The system assumes a head-mounted device with high-quality binaural rendering and head tracking; 3D scene geometry is used as an invisible scaffold to anchor sources, trigger events, define risk/guidance volumes, and govern physically plausible motion without visuals. Session difficulty is shaped via cue density, event tempo, and task complexity while preserving cue consistency to promote transfer across scenarios. The specification aims to enable safe repetition, reduce instructor burden, and support clearer standards across rehabilitation centers, aligning with evidence that audio-first interaction is essential for blind and visually impaired users and addressing gaps in HRTF personalization, evaluation standards, and accessibility integration. Although no behavioral outcomes are reported here, this implementable flow consolidates auditory science with center-ready design, offering a pragmatic foundation for standardized evaluation and future comparative studies.", "AI": {"tldr": "提出了一种以声音为主的沉浸式培训流程，用于盲人和低视力学习者的导航、移动及日常生活训练。", "motivation": "现行的O&M指导难以标准化和重复进行，因依赖于讲师的存在、物理模型和真实世界的户外条件变化。此方法旨在解决这些问题，提供一个可安全反复使用、减轻教师负担并支持清晰标准的方法。", "method": "该研究设计了一种基于空间音频和声化为主要反馈通道的预街O&M及日常生活实践培训流程，包含参数化的场景模板、一致的声音词汇表以及轻量级的安全协议。系统假设头部佩戴设备具有高质量双耳渲染和头部追踪功能，并利用3D场景几何结构作为不可见支撑。", "result": "此实施框架结合了听觉科学与中心准备设计，为标准化评估提供了实用基础，有助于未来比较研究。", "conclusion": "通过音频首先互动对盲人及视觉受损用户至关重要。该流程能够减轻教师负担，并支持更清晰的标准，有利于未来的标准化评价和比较研究。"}}
{"id": "2602.19552", "pdf": "https://arxiv.org/pdf/2602.19552", "abs": "https://arxiv.org/abs/2602.19552", "authors": ["Kasper Green Larsen", "Markus Engelund Mathiasen", "Chirag Pabbaraju", "Clement Svendsen"], "title": "The Sample Complexity of Replicable Realizable PAC Learning", "categories": ["cs.LG", "cs.CC", "cs.DS"], "comment": null, "summary": "In this paper, we consider the problem of replicable realizable PAC learning. We construct a particularly hard learning problem and show a sample complexity lower bound with a close to $(\\log|H|)^{3/2}$ dependence on the size of the hypothesis class $H$. Our proof uses several novel techniques and works by defining a particular Cayley graph associated with $H$ and analyzing a suitable random walk on this graph by examining the spectral properties of its adjacency matrix. Furthermore, we show an almost matching upper bound for the lower bound instance, meaning if a stronger lower bound exists, one would have to consider a different instance of the problem.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19549", "pdf": "https://arxiv.org/pdf/2602.19549", "abs": "https://arxiv.org/abs/2602.19549", "authors": ["Yibo Yan", "Mingdong Ou", "Yi Cao", "Xin Zou", "Jiahao Huo", "Shuliang Liu", "James Kwok", "Xuming Hu"], "title": "Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework", "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "Under review", "summary": "Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.", "AI": {"tldr": "该论文提出了一个名为Prune-then-Merge的两阶段框架，用于提高多向量视觉文档检索中的效率。", "motivation": "当前最先进方法虽然性能卓越，但计算开销巨大。现有提升效率的方法如剪枝和合并技术无法完美解决压缩率与特征保真度之间的权衡问题。", "method": "该论文提出了一种新的两阶段框架——先通过自适应剪枝去除低信息量补丁，然后在预过滤后的嵌入集合上进行分层合并，从而有效总结语义内容而不丢失关键特征。", "result": "实验结果表明，与现有方法相比，本文提出的Prune-then-Merge框架在多组视觉文档检索数据集上的表现更优，在高压缩率下也能保持较好的性能。", "conclusion": "该论文所提方法成功解决了高效多向量视觉文档检索中的关键问题，并显著扩大了近无损压缩的范围。"}}
{"id": "2602.19542", "pdf": "https://arxiv.org/pdf/2602.19542", "abs": "https://arxiv.org/abs/2602.19542", "authors": ["Yankuan Chi", "Xiang Li", "Zixuan Huang", "James M. Rehg"], "title": "Vinedresser3D: Agentic Text-guided 3D Editing", "categories": ["cs.CV"], "comment": "CVPR 2026, Project website:https://vinedresser3d.github.io/", "summary": "Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.", "AI": {"tldr": "本文提出了一种文本引导的3D编辑框架Vinedresser3D，能够直接在原生3D生成模型的潜在空间中进行高质量编辑。", "motivation": "现有方法难以同时理解复杂的指令、自动定位3D区域以及保持未编辑内容。", "method": "使用多模态大语言模型推断资产描述，并确定编辑类型和位置；选择视角，获取视觉指导；通过基于逆向的修复流填充管道进行3D潜在空间中的编辑。", "result": "实验显示Vinedresser3D在自动评估指标和人类偏好研究中均优于先前基线，在无需掩模的情况下实现精确、连贯的3D编辑。", "conclusion": "Vinedresser3D通过引入创新方法解决了文本引导的3D编辑问题，展示出优越性能。"}}
{"id": "2602.19540", "pdf": "https://arxiv.org/pdf/2602.19540", "abs": "https://arxiv.org/abs/2602.19540", "authors": ["Wei Wang", "Yixing Wu", "C. -C. Jay Kuo"], "title": "A Green Learning Approach to LDCT Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": "Published in IEEE International Conference on Image Processing (ICIP), 2025, pp. 1762-1767. Final version available at IEEE Xplore", "summary": "This work proposes a green learning (GL) approach to restore medical images. Without loss of generality, we use low-dose computed tomography (LDCT) images as examples. LDCT images are susceptible to noise and artifacts, where the imaging process introduces distortion. LDCT image restoration is an important preprocessing step for further medical analysis. Deep learning (DL) methods have been developed to solve this problem. We examine an alternative solution using the Green Learning (GL) methodology. The new restoration method is characterized by mathematical transparency, computational and memory efficiency, and high performance. Experiments show that our GL method offers state-of-the-art restoration performance at a smaller model size and with lower inference complexity.", "AI": {"tldr": "提出了一种绿色学习方法来恢复低剂量CT图像", "motivation": "低剂量CT图像易受噪声和伪影影响，现有深度学习方法虽有效但模型复杂度高。为了提高效率并保持高性能，引入了绿色学习方法", "method": "通过数学透明性、计算及内存效率优化，提出了一种新的LDCT图像恢复算法", "result": "实验结果表明，该绿色学习方法在较小的模型尺寸和更低的推理复杂度下达到了最先进的恢复性能", "conclusion": "绿色学习方法为低剂量CT图像恢复提供了一个高效且高性能的替代方案"}}
{"id": "2602.19539", "pdf": "https://arxiv.org/pdf/2602.19539", "abs": "https://arxiv.org/abs/2602.19539", "authors": ["Xingyu Shen", "Tommy Duong", "Xiaodong An", "Zengqi Zhao", "Zebang Hu", "Haoyu Hu", "Ziyou Wang", "Finn Guo", "Simiao Ren"], "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "13 pages, 6 figures", "summary": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.", "AI": {"tldr": "研究探讨了家庭可用的化妆品修改，如胡须、白发、化妆和皱纹模拟，是否能使AI年龄估计系统误将未成年人判定为成年人。", "motivation": "评估现有年龄估计系统的鲁棒性，并发现其在面对低成本化妆品攻击时存在的漏洞。", "method": "使用VLM图像编辑器（Gemini 2.5 Flash Image）对包含329张面部图像的数据集进行了模拟攻击，然后用八个模型进行测试。引入了Attack Conversion Rate (ACR)作为评估指标。", "result": "单独的虚拟胡须能够使28到69%的未成年人被误判为成年人；所有四种攻击组合后，平均年龄估计增加了7.7岁，最高ACR达到83％；语言视觉模型在全面攻击下的ACR低于专门构建的模型（59至71％对比63至83％）。", "conclusion": "结果表明了当前部署的年龄验证管道存在严重漏洞，并强调对抗鲁棒性评估应成为模型选择的标准之一。"}}
{"id": "2602.19538", "pdf": "https://arxiv.org/pdf/2602.19538", "abs": "https://arxiv.org/abs/2602.19538", "authors": ["Arundhati Banerjee", "Jeff Schneider"], "title": "Cost-Aware Diffusion Active Search", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "In submission", "summary": "Active search for recovering objects of interest through online, adaptive decision making with autonomous agents requires trading off exploration of unknown environments with exploitation of prior observations in the search space. Prior work has proposed information gain and Thompson sampling based myopic, greedy approaches for agents to actively decide query or search locations when the number of targets is unknown. Decision making algorithms in such partially observable environments have also shown that agents capable of lookahead over a finite horizon outperform myopic policies for active search. Unfortunately, lookahead algorithms typically rely on building a computationally expensive search tree that is simulated and updated based on the agent's observations and a model of the environment dynamics. Instead, in this work, we leverage the sequence modeling abilities of diffusion models to sample lookahead action sequences that balance the exploration-exploitation trade-off for active search without building an exhaustive search tree. We identify the optimism bias in prior diffusion based reinforcement learning approaches when applied to the active search setting and propose mitigating solutions for efficient cost-aware decision making with both single and multi-agent teams. Our proposed algorithm outperforms standard baselines in offline reinforcement learning in terms of full recovery rate and is computationally more efficient than tree search in cost-aware active decision making.", "AI": {"tldr": "本文提出了一种利用扩散模型进行采样来解决主动搜索问题的新方法，以平衡探索和利用之间的权衡，并提高多代理团队的决策效率。", "motivation": "传统的方法如信息增益和汤普森抽样在处理目标数量未知的问题时存在局限性。基于有限前瞻的算法虽然表现更好但计算成本高昂。本文旨在通过扩散模型解决主动搜索中的探索与利用权衡问题，提高多代理团队的成本意识决策效率。", "method": "使用扩散模型对前瞻性动作序列进行采样，并平衡主动搜索任务中的探索和利用之间的权衡；改进现有的乐观偏差以适应主动搜索场景，同时提出针对单个代理人和多个代理人团队的有效成本感知决策策略。", "result": "所提出的算法在离线强化学习中表现优于标准基准，在目标完全恢复率方面有优势，并且在成本感知主动决策过程中比树搜索计算效率更高。", "conclusion": "通过扩散模型进行前瞻采样，可以有效地解决主动搜索中的探索-利用权衡问题，提高了多代理团队的成本意识决策能力。该方法在性能和计算效率上都超越了现有的基准算法。"}}
{"id": "2602.19536", "pdf": "https://arxiv.org/pdf/2602.19536", "abs": "https://arxiv.org/abs/2602.19536", "authors": ["Zhiwei Ning", "Xuanang Gao", "Jiaxi Cao", "Runze Yang", "Huiying Xu", "Xinzhong Zhu", "Jie Yang", "Wei Liu"], "title": "Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.", "AI": {"tldr": "本文提出了一种新的基于Mamba的编码器Fore-Mamba3D，用于增强3D物体检测中的前景特征。", "motivation": "先前的方法采用双向编码对整个非空体素序列进行处理，包含大量背景信息。直接编码前景体素会影响检测性能，这归因于线性建模中前景体素响应减弱和上下文表示受限的问题。", "method": "首先根据预测分数采样前景体素，并设计区域到全局滑动窗口（RGSW）传播不同实例间的局部分割信息。此外提出了语义辅助状态空间融合模块（SASFMamba），通过增强Mamba模型内的语义和几何感知来丰富上下文表示。", "result": "实验结果表明，Fore-Mamba3D在多个基准测试中表现出色，证明了其在3D物体检测任务中的有效性。", "conclusion": "本文提出了Fore-Mamba3D方法以解决现有Mamba方法在处理前景体素时遇到的问题，并通过实验证明了该方法的有效性。"}}
{"id": "2602.19535", "pdf": "https://arxiv.org/pdf/2602.19535", "abs": "https://arxiv.org/abs/2602.19535", "authors": ["Thomas Erlebach", "Kelin Luo", "Wen Zhang"], "title": "Minimizing Total Travel Time for Collaborative Package Delivery with Heterogeneous Drones", "categories": ["cs.DS"], "comment": null, "summary": "Given a fleet of drones with different speeds and a set of package delivery requests, the collaborative delivery problem asks for a schedule for the drones to collaboratively carry out all package deliveries, with the objective of minimizing the total travel time of all drones. We show that the best non-preemptive schedule (where a package that is picked up at its source is immediately delivered to its destination by one drone) is within a factor of three of the best preemptive schedule (where several drones can participate in the delivery of a single package). Then, we present a constant-factor approximation algorithm for the problem of computing the best non-preemptive schedule. The algorithm reduces the problem to a tree combination problem and uses a primal-dual approach to solve the latter. We have implemented a version of the algorithm optimized for practical efficiency and report the results of experiments on large-scale instances with synthetic and real-world data, demonstrating that our algorithm is scalable and delivers schedules of excellent quality.", "AI": {"tldr": "本文研究了异构无人机协同包裹投递问题，旨在最小化所有无人机的总行驶时间。", "motivation": "为了提高效率和减少成本，在给定不同速度的无人机队列和一系列包裹投递请求的情况下，研究如何通过协作完成所有的包裹投递任务，并使总的行驶时间最短。", "method": "提出了一种常数因子近似算法来计算最佳非抢占性调度。该算法将问题转化为树组合问题并使用原对偶方法解决后者。", "result": "实验结果表明，优化后的算法在大规模实例上具有可扩展性和高质量的调度能力。", "conclusion": "证明了最优非抢占性调度与最优抢占性调度之间的性能差距，并开发了一个高效的常数因子近似算法来求解非抢占性调度问题。"}}
{"id": "2602.19534", "pdf": "https://arxiv.org/pdf/2602.19534", "abs": "https://arxiv.org/abs/2602.19534", "authors": ["Yousef Emami", "Hao Zhou", "Radha Reddy", "Atefeh Hajijamali Arani", "Biliang Wang", "Kai Li", "Luis Almeida", "Zhu Han"], "title": "Large Language Model-Assisted UAV Operations and Communications: A Multifaceted Survey and Tutorial", "categories": ["cs.RO", "cs.AI"], "comment": "40 pages, 10 figures, 13 tables", "summary": "Uncrewed Aerial Vehicles (UAVs) are widely deployed across diverse applications due to their mobility and agility. Recent advances in Large Language Models (LLMs) offer a transformative opportunity to enhance UAV intelligence beyond conventional optimization-based and learning-based approaches. By integrating LLMs into UAV systems, advanced environmental understanding, swarm coordination, mobility optimization, and high-level task reasoning can be achieved, thereby allowing more adaptive and context-aware aerial operations. This survey systematically explores the intersection of LLMs and UAV technologies and proposes a unified framework that consolidates existing architectures, methodologies, and applications for UAVs. We first present a structured taxonomy of LLM adaptation techniques for UAVs, including pretraining, fine-tuning, Retrieval-Augmented Generation (RAG), and prompt engineering, along with key reasoning capabilities such as Chain-of-Thought (CoT) and In-Context Learning (ICL). We then examine LLM-assisted UAV communications and operations, covering navigation, mission planning, swarm control, safety, autonomy, and network management. After that, the survey further discusses Multimodal LLMs (MLLMs) for human-swarm interaction, perception-driven navigation, and collaborative control. Finally, we address ethical considerations, including bias, transparency, accountability, and Human-in-the-Loop (HITL) strategies, and outline future research directions. Overall, this work positions LLM-assisted UAVs as a foundation for intelligent and adaptive aerial systems.", "AI": {"tldr": "该论文系统地探讨了大型语言模型（LLM）与无人机技术的交集，提出了一种统一框架以整合现有的架构、方法和应用。", "motivation": "为了增强无人机系统的智能性，超越传统的优化和学习方法，该研究通过将大型语言模型集成到无人机系统中，实现高级环境理解、群体协调、移动优化以及高层次任务推理。", "method": "论文首先提出了针对无人机的大型语言模型适应技术分类，包括预训练、微调、检索增强生成（RAG）及提示工程。然后探讨了利用大型语言模型支持的无人机通信和操作方法，覆盖导航、任务规划、群体控制、安全性和自主性以及网络管理。", "result": "该论文讨论了多模态大型语言模型在人类-群组交互、感知驱动导航及协作控制中的应用，并提出了伦理考量和未来研究方向。", "conclusion": "综上所述，本文将基于大型语言模型的无人机视为智能且适应性强的空中系统的基石。"}}
{"id": "2602.19533", "pdf": "https://arxiv.org/pdf/2602.19533", "abs": "https://arxiv.org/abs/2602.19533", "authors": ["Pascal Jr Tikeng Notsawo", "Guillaume Dumas", "Guillaume Rabusseau"], "title": "Grokking Finite-Dimensional Algebra", "categories": ["cs.LG", "cs.AI", "math.RA"], "comment": "34 pages, 13 figures", "summary": "This paper investigates the grokking phenomenon, which refers to the sudden transition from a long memorization to generalization observed during neural networks training, in the context of learning multiplication in finite-dimensional algebras (FDA). While prior work on grokking has focused mainly on group operations, we extend the analysis to more general algebraic structures, including non-associative, non-commutative, and non-unital algebras. We show that learning group operations is a special case of learning FDA, and that learning multiplication in FDA amounts to learning a bilinear product specified by the algebra's structure tensor. For algebras over the reals, we connect the learning problem to matrix factorization with an implicit low-rank bias, and for algebras over finite fields, we show that grokking emerges naturally as models must learn discrete representations of algebraic elements. This leads us to experimentally investigate the following core questions: (i) how do algebraic properties such as commutativity, associativity, and unitality influence both the emergence and timing of grokking, (ii) how structural properties of the structure tensor of the FDA, such as sparsity and rank, influence generalization, and (iii) to what extent generalization correlates with the model learning latent embeddings aligned with the algebra's representation. Our work provides a unified framework for grokking across algebraic structures and new insights into how mathematical structure governs neural network generalization dynamics.", "AI": {"tldr": "研究了神经网络在学习有限维代数中乘法时的grokking现象，探索了其背后的数学结构如何影响泛化能力。", "motivation": "探讨更广泛的代数结构中的grokking现象，并理解不同代数性质对模型泛化的影响。", "method": "通过对比不同的代数组合特性（如交换性、结合性和单位元），以及研究代数的结构张量，实验分析了它们对grokking现象出现的时间和程度的影响。", "result": "发现数学结构显著影响神经网络的学习过程和泛化能力。具体来说，某些代数性质加速或延迟了grokking的发生，并且模型学习与代数组合表示一致的潜在嵌入也会影响其性能。", "conclusion": "提供了一个统一框架来理解不同代数结构中的grokking现象，并揭示了数学结构对神经网络泛化能力的影响机制。"}}
{"id": "2602.19532", "pdf": "https://arxiv.org/pdf/2602.19532", "abs": "https://arxiv.org/abs/2602.19532", "authors": ["William Sharpless", "Oswin So", "Dylan Hirsch", "Sylvia Herbert", "Chuchu Fan"], "title": "Bellman Value Decomposition for Task Logic in Safe Optimal Control", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Real-world tasks involve nuanced combinations of goal and safety specifications. In high dimensions, the challenge is exacerbated: formal automata become cumbersome, and the combination of sparse rewards tends to require laborious tuning. In this work, we consider the innate structure of the Bellman Value as a means to naturally organize the problem for improved automatic performance. Namely, we prove the Bellman Value for a complex task defined in temporal logic can be decomposed into a graph of Bellman Values, connected by a set of well-known Bellman equations (BEs): the Reach-Avoid BE, the Avoid BE, and a novel type, the Reach-Avoid-Loop BE. To solve the Value and optimal policy, we propose VDPPO, which embeds the decomposed Value graph into a two-layer neural net, bootstrapping the implicit dependencies. We conduct a variety of simulated and hardware experiments to test our method on complex, high-dimensional tasks involving heterogeneous teams and nonlinear dynamics. Ultimately, we find this approach greatly improves performance over existing baselines, balancing safety and liveness automatically.", "AI": {"tldr": "通过分解贝尔曼值来解决包含安全性和目标任务的复杂任务", "motivation": "面对高维度下目标和安全性规范组合带来的挑战，现有方法变得笨重且难以调整。本文旨在利用贝尔曼值的内在结构改善自动性能，提出新的算法以适应复杂的、多维的任务并保障其安全性与高效性", "method": "提出了VDPPO算法，该算法将分解后的价值图嵌入到两层神经网络中，并通过隐式依赖进行引导", "result": "在各种模拟和硬件实验中显示了优于现有基准方法的性能，证明能够自动平衡安全性和活性", "conclusion": "这种基于贝尔曼值分解的方法显著改善了复杂高维度任务中的性能表现"}}
{"id": "2602.19531", "pdf": "https://arxiv.org/pdf/2602.19531", "abs": "https://arxiv.org/abs/2602.19531", "authors": ["Dingyi Nie", "Yixing Wu", "C. -C. Jay Kuo"], "title": "A Statistical Approach for Modeling Irregular Multivariate Time Series with Missing Observations", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for publication in APSIPA Transactions on Signal and Information Processing", "summary": "Irregular multivariate time series with missing values present significant challenges for predictive modeling in domains such as healthcare. While deep learning approaches often focus on temporal interpolation or complex architectures to handle irregularities, we propose a simpler yet effective alternative: extracting time-agnostic summary statistics to eliminate the temporal axis. Our method computes four key features per variable-mean and standard deviation of observed values, as well as the mean and variability of changes between consecutive observations to create a fixed-dimensional representation. These features are then utilized with standard classifiers, such as logistic regression and XGBoost. Evaluated on four biomedical datasets (PhysioNet Challenge 2012, 2019, PAMAP2, and MIMIC-III), our approach achieves state-of-the-art performance, surpassing recent transformer and graph-based models by 0.5-1.7% in AUROC/AUPRC and 1.1-1.7% in accuracy/F1-score, while reducing computational complexity. Ablation studies demonstrate that feature extraction-not classifier choice-drives performance gains, and our summary statistics outperform raw/imputed input in most benchmarks. In particular, we identify scenarios where missing patterns themselves encode predictive signals, as in sepsis prediction (PhysioNet, 2019), where missing indicators alone can achieve 94.2% AUROC with XGBoost, only 1.6% lower than using original raw data as input. Our results challenge the necessity of complex temporal modeling when task objectives permit time-agnostic representations, providing an efficient and interpretable solution for irregular time series classification.", "AI": {"tldr": "提出了一种处理不规则多变量时间序列中缺失值的方法，通过提取时间无关的统计特征来简化模型复杂性。", "motivation": "在医疗保健等领域，含有缺失值的不规则多变量时间序列给预测建模带来了挑战。深度学习方法通常专注于时间插补或复杂的架构来处理这些情况，而本文提出了一种更简单有效的方法：通过提取时间无关的统计特征消除时间轴的影响。", "method": "计算每个变量的平均值和标准差以及观察到的变化量的均值与变异性等四种关键特征，形成固定维度表示。然后使用传统的分类器如逻辑回归和XGBoost对这些特征进行处理。", "result": "在四个生物医学数据集上达到了最先进的性能，超越了最近的变压器和图基模型0.5-1.7%的AUROC/AUPRC以及1.1-1.7%的准确率/F1值，并降低了计算复杂度。消融研究表明特征提取是推动表现改进的主要因素。", "conclusion": "结果表明，当任务目标允许时间无关表示时，不需要复杂的时空建模方法，提供了一种高效且可解释的时间序列分类解决方案。"}}
{"id": "2602.19530", "pdf": "https://arxiv.org/pdf/2602.19530", "abs": "https://arxiv.org/abs/2602.19530", "authors": ["Omprakash Chakraborty", "Jose Dolz", "Ismail Ben Ayed"], "title": "ORION: ORthonormal Text Encoding for Universal VLM AdaptatION", "categories": ["cs.CV"], "comment": null, "summary": "Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.", "AI": {"tldr": "介绍了一种名为ORION的文本编码器微调框架，用于改进预训练的视觉语言模型（VLM）的性能。", "motivation": "当前标准零样本分类器由于使用冻结的文本编码器和手工制作的提示词导致类别表示质量差或几何形状不佳，从而限制了特定任务的区分能力。", "method": "ORION通过低秩适应优化了一种新损失函数，该函数包含两个项：一项促进给定任务中类别之间文本表示的正交性；另一项惩罚与初始类别原型偏差的偏离，并且提供了一个概率解释，将此正交罚项连接到最大似然估计（MLE）原则。", "result": "在11个基准测试和三种大型VLM骨干网络上进行了广泛的实验，表明改进后的文本嵌入成为标准CLIP原型的强大替代品，并提高了多种预测设置下的性能。", "conclusion": "ORION作为一个即插即用模块添加到各种最先进的方法中，在零样本、少量样本以及测试时间适应性等不同预测场景下显著提升了模型的性能。"}}
{"id": "2602.19525", "pdf": "https://arxiv.org/pdf/2602.19525", "abs": "https://arxiv.org/abs/2602.19525", "authors": ["Keigo Oka", "Naoki Inaba", "Akira Iino"], "title": "Covering a Polyomino-Shaped Stain with Non-Overlapping Identical Stickers", "categories": ["cs.DS", "cs.CC"], "comment": null, "summary": "You find a stain on the wall and decide to cover it with non-overlapping stickers of a single identical shape (rotation and reflection are allowed). Is it possible to find a sticker shape that fails to cover the stain? In this paper, we consider this problem under polyomino constraints and complete the classification of always-coverable stain shapes (polyominoes). We provide proofs for the maximal always-coverable polyominoes and construct concrete counterexamples for the minimal not always-coverable ones, demonstrating that such cases exist even among hole-free polyominoes. This classification consequently yields an algorithm to determine the always-coverability of any given stain. We also show that the problem of determining whether a given sticker can cover a given stain is $\\NP$-complete, even though exact cover is not demanded. This result extends to the 1D case where the connectivity requirement is removed. As an illustration of the problem complexity, for a specific hexomino (6-cell) stain, the smallest sticker found in our search that avoids covering it has, although not proven minimum, a bounding box of $325 \\times 325$.", "AI": {"tldr": "研究如何用非重叠的相同形状贴纸覆盖墙上的多米诺形污渍，并完成可完全被覆盖的多米诺形状分类。", "motivation": "探究是否存在某种贴纸形状无法覆盖特定污渍，以及解决这种情况下算法的问题复杂性。", "method": "提供证明对于最大可完全覆盖多米诺形状的有效性，并构建最小不可完全覆盖多米诺形的具体反例。此外还展示了一个问题在1D情况下的NP-完备性。", "result": "完成总是能被贴纸覆盖的多米诺形状分类，并给出了一个算法以确定任何给定污渍是否可完全被覆盖。同时，证明了即使不强制精确覆盖时，判断给定贴纸能否覆盖特定污渍的问题是NP-完备的。", "conclusion": "通过详细分类总是能被覆盖的和不能被覆盖的多米诺形状以及展示问题复杂性，研究揭示了一些关于覆盖算法的新见解。"}}
{"id": "2602.19523", "pdf": "https://arxiv.org/pdf/2602.19523", "abs": "https://arxiv.org/abs/2602.19523", "authors": ["Jingyuan Wang", "Li Niu"], "title": "OSInsert: Towards High-authenticity and High-fidelity Image Composition", "categories": ["cs.CV"], "comment": null, "summary": "Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. Some high-authenticity methods can adjust foreground pose/view to be compatible with background, while some high-fidelity methods can preserve the foreground details accurately. However, existing methods can hardly achieve both goals at the same time. In this work, we propose a two-stage strategy to achieve both goals. In the first stage, we use high-authenticity method to generate reasonable foreground shape, serving as the condition of high-fidelity method in the second stage. The experiments on MureCOM dataset verify the effectiveness of our two-stage strategy. The code and model have been released at https://github.com/bcmi/OSInsert-Image-Composition.", "AI": {"tldr": "该论文提出了一种两阶段策略，旨在同时实现高真实性和高质量的图像合成。", "motivation": "现有的方法很难在同一时间达到高真实性和高质量的目标。为了克服这一挑战，作者提出了一个新的方案来同时满足这两个目标。", "method": "第一阶段使用高真实性方法生成合理的前景形状，作为第二阶段高保真度方法的条件。通过这种方式，可以将两者的优点结合起来，提高最终合成图像的质量和真实性。", "result": "实验在MureCOM数据集上验证了该两阶段策略的有效性。", "conclusion": "论文提出了一种有效的两阶段策略，能够在保持高真实性的前提下实现高质量的图像合成。"}}
{"id": "2602.19522", "pdf": "https://arxiv.org/pdf/2602.19522", "abs": "https://arxiv.org/abs/2602.19522", "authors": ["Zhenghao Zhou", "Yiyan Li", "Fei Xie", "Lu Wang", "Bo Wang", "Jiansheng Wang", "Zheng Yan", "Mo-Yuen Chow"], "title": "An LLM-Enabled Frequency-Aware Flow Diffusion Model for Natural-Language-Guided Power System Scenario Generation", "categories": ["eess.SP", "cs.SD"], "comment": null, "summary": "Diverse and controllable scenario generation (e.g., wind, solar, load, etc.) is critical for robust power system planning and operation. As AI-based scenario generation methods are becoming the mainstream, existing methods (e.g., Conditional Generative Adversarial Nets) mainly rely on a fixed-length numerical conditioning vector to control the generation results, facing challenges in user conveniency and generation flexibility. In this paper, a natural-language-guided scenario generation framework, named LLM-enabled Frequency-aware Flow Diffusion (LFFD), is proposed to enable users to generate desired scenarios using plain human language. First, a pretrained LLM module is introduced to convert generation requests described by unstructured natural languages into ordered semantic space. Second, instead of using standard diffusion models, a flow diffusion model employing a rectified flow matching objective is introduced to achieve efficient and high-quality scenario generation, taking the LLM output as the model input. During the model training process, a frequency-aware multi-objective optimization algorithm is introduced to mitigate the frequency-bias issue. Meanwhile, a dual-agent framework is designed to create text-scenario training sample pairs as well as to standardize semantic evaluation. Experiments based on large-scale photovoltaic and load datasets demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "提出了一种基于LLM的频率感知流扩散模型，用于自然语言指导下的电力系统场景生成。", "motivation": "现有AI方法依赖固定长度数值条件向量控制结果，存在用户便利性和生成灵活性的问题。为此，本研究旨在通过使用自然语言来提高场景生成的方法便捷性与多样性。", "method": "引入预训练的LLM模块将非结构化自然语言请求转换为有序语义空间；采用流扩散模型而非传统扩散模型以实现高效的高质量场景生成；在模型训练过程中采用了频率感知多目标优化算法，解决了频率偏置问题。同时设计了双代理框架来创建文本-场景训练样本对和标准化语义评估。", "result": "实验基于大规模光伏及负荷数据集验证了所提方法的有效性。", "conclusion": "本研究成功开发了一种利用自然语言指导电力系统场景生成的方法，提高了用户交互性和灵活性。"}}
{"id": "2602.19519", "pdf": "https://arxiv.org/pdf/2602.19519", "abs": "https://arxiv.org/abs/2602.19519", "authors": ["Yirou Ge", "Yixi Li", "Alec Chiu", "Shivani Shekhar", "Zijie Pan", "Avinash Thangali", "Yun-Shiuan Chuang", "Chaitanya Kulkarni", "Uma Kona", "Linsey Pang", "Prakhar Mehrotra"], "title": "Ada-RS: Adaptive Rejection Sampling for Selective Thinking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly being deployed in cost and latency-sensitive settings. While chain-of-thought improves reasoning, it can waste tokens on simple requests. We study selective thinking for tool-using LLMs and introduce Adaptive Rejection Sampling (Ada-RS), an algorithm-agnostic sample filtering framework for learning selective and efficient reasoning. For each given context, Ada-RS scores multiple sampled completions with an adaptive length-penalized reward then applies stochastic rejection sampling to retain only high-reward candidates (or preference pairs) for downstream optimization. We demonstrate how Ada-RS plugs into both preference pair (e.g. DPO) or grouped policy optimization strategies (e.g. DAPO). Using Qwen3-8B with LoRA on a synthetic tool call-oriented e-commerce benchmark, Ada-RS improves the accuracy-efficiency frontier over standard algorithms by reducing average output tokens by up to 80% and reducing thinking rate by up to 95% while maintaining or improving tool call accuracy. These results highlight that training-signal selection is a powerful lever for efficient reasoning in latency-sensitive deployments.", "AI": {"tldr": "论文提出了自适应拒绝采样（Ada-RS）算法，用于训练选择性思考的语言模型，以提高推理效率。", "motivation": "在成本和延迟敏感的应用场景下，链式思维虽然可以改进推理过程，但对简单请求浪费了令牌。Ada-RS旨在解决这一问题，通过自适应长度惩罚奖励来筛选样本，从而实现高效且准确的推理。", "method": "对于每个给定的上下文，Ada-RS生成多个采样完成项，并使用自适应长度惩罚奖励对其进行评分，随后应用随机拒绝采样以保留高得分候选或偏好对用于后续优化。该方法可以应用于偏好对策略（如DPO）和分组策略优化方法（如DAPO）。", "result": "实验表明，在合成工具调用导向的电子商务基准上，Ada-RS相较于标准算法能显著减少平均输出令牌数，最多可达80%，并降低思考率至多95%，同时保持或提高工具调用准确度。", "conclusion": "该研究证明了训练信号选择是实现延迟敏感部署中有效推理的重要杠杆。"}}
{"id": "2602.19518", "pdf": "https://arxiv.org/pdf/2602.19518", "abs": "https://arxiv.org/abs/2602.19518", "authors": ["Nabanita Dash", "Ayush Kaura", "Shivam Singh", "Ramandeep Singh", "Snehasis Banerjee", "Mohan Sridharan", "K. Madhava Krishna"], "title": "Anticipate, Adapt, Act: A Hybrid Framework for Task Planning", "categories": ["cs.RO"], "comment": "Accepted at IEEE European Conference on Mobile Robots (ECMR)", "summary": "Anticipating and adapting to failures is a key capability robots need to collaborate effectively with humans in complex domains. This continues to be a challenge despite the impressive performance of state of the art AI planning systems and Large Language Models (LLMs) because of the uncertainty associated with the tasks and their outcomes. Toward addressing this challenge, we present a hybrid framework that integrates the generic prediction capabilities of an LLM with the probabilistic sequential decision-making capability of Relational Dynamic Influence Diagram Language. For any given task, the robot reasons about the task and the capabilities of the human attempting to complete it; predicts potential failures due to lack of ability (in the human) or lack of relevant domain objects; and executes actions to prevent such failures or recover from them. Experimental evaluation in the VirtualHome 3D simulation environment demonstrates substantial improvement in performance compared with state of the art baselines.", "AI": {"tldr": "该论文提出了一种混合框架，结合大型语言模型和概率决策系统的能力，以预测并适应任务执行中的潜在故障。", "motivation": "机器人在与人类合作时需要具备预见和应对失败的关键能力。尽管现有的AI规划系统和大型语言模型表现出色，但不确定性和任务结果的不确定性仍然是挑战。", "method": "提出了一种混合框架，该框架结合了大型语言模型的通用预测能力和关系动态影响图语言的概率决策能力，机器人能够根据任务情况和人类操作者的技能进行推理，并采取行动防止或修复潜在失败。", "result": "在VirtualHome 3D仿真环境中进行了实验评估，结果显示与最先进的基线相比有了显著提高。", "conclusion": "该论文提出的方法表明，在不确定性和复杂性的环境中，通过结合大型语言模型和概率决策系统的能力可以有效地预见并适应任务执行中的潜在失败。"}}
{"id": "2602.19517", "pdf": "https://arxiv.org/pdf/2602.19517", "abs": "https://arxiv.org/abs/2602.19517", "authors": ["Chongyang Gao", "Diji Yang", "Shuyan Zhou", "Xichen Yan", "Luchuan Song", "Shuo Li", "Kezhen Chen"], "title": "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19514", "pdf": "https://arxiv.org/pdf/2602.19514", "abs": "https://arxiv.org/abs/2602.19514", "authors": ["Pulak Mehta"], "title": "Security Risks of AI Agents Hiring Humans: An Empirical Marketplace Study", "categories": ["cs.CR", "cs.HC"], "comment": null, "summary": "Autonomous AI agents can now programmatically hire human workers through marketplaces using REST APIs and Model Context Protocol (MCP) integrations. This creates an attack surface analogous to CAPTCHA-solving services but with physical-world reach. We present an empirical measurement study of this threat, analyzing 303 bounties from RENTAHUMAN.AI, a marketplace where agents post tasks and manage escrow payments. We find that 99 bounties (32.7%), originate from programmatic channels (API keys or MCP). Using a dual-coder methodology (\\k{appa} = 0.86 ), we identify six active abuse classes: credential fraud, identity impersonation, automated reconnaissance, social media manipulation, authentication circumvention, and referral fraud, all purchasable for a median of $25 per worker. A retrospective evaluation of seven content-screening rules flags 52 bounties (17.2%) with a single false positive, demonstrating that while basic defenses are feasible, they are currently absent.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19512", "pdf": "https://arxiv.org/pdf/2602.19512", "abs": "https://arxiv.org/abs/2602.19512", "authors": ["Pengxi Liu", "Zeyu Michael Li", "Xiang Cheng"], "title": "Variational Trajectory Optimization of Anisotropic Diffusion Schedules", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce a variational framework for diffusion models with anisotropic noise schedules parameterized by a matrix-valued path $M_t(θ)$ that allocates noise across subspaces. Central to our framework is a trajectory-level objective that jointly trains the score network and learns $M_t(θ)$, which encompasses general parameterization classes of matrix-valued noise schedules. We further derive an estimator for the derivative with respect to $θ$ of the score that enables efficient optimization of the $M_t(θ)$ schedule. For inference, we develop an efficiently-implementable reverse-ODE solver that is an anisotropic generalization of the second-order Heun discretization algorithm. Across CIFAR-10, AFHQv2, FFHQ, and ImageNet-64, our method consistently improves upon the baseline EDM model in all NFE regimes. Code is available at https://github.com/lizeyu090312/anisotropic-diffusion-paper.", "AI": {"tldr": "本文介绍了一种扩散模型的变分框架，通过矩阵值路径分配噪声，优化扩散过程。", "motivation": "为了解决传统等向性扩散模型中噪声分配不理想的问题，提出一种新的方法以提高生成图像的质量和效率。", "method": "提出了一个基于轨迹水平目标函数的变分框架，并发展了一种反向ODE求解器来实现高效推理。", "result": "在CIFAR-10、AFHQv2、FFHQ和ImageNet-64等数据集上，本文方法优于基准EDM模型。", "conclusion": "新提出的变分框架和优化策略能有效改善扩散模型的性能。"}}
{"id": "2602.19509", "pdf": "https://arxiv.org/pdf/2602.19509", "abs": "https://arxiv.org/abs/2602.19509", "authors": ["Arindam Khaled"], "title": "Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 4 figures, 1 table", "summary": "Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.", "AI": {"tldr": "本文提出了一种名为“金字塔MoA”的层次化代理混合架构，旨在通过动态分配复杂任务以优化推理成本。", "motivation": "大型语言模型在推断成本和推理能力之间存在固有折衷。虽然高性能模型（如Llama-3-70B）具有出色的准确性但部署成本高昂；较小的模型则更经济实惠但在处理复杂任务时表现不佳。", "method": "金字塔MoA架构使用轻量级路由器动态分配复杂查询，利用语义一致性与信心校准来识别需要较高性能支持的任务。通过这种方式，在保证准确率的同时显著降低成本。", "result": "在GSM8K基准测试中，该系统实现了93.0%的准确性，并且减少了61%的计算成本，同时仅引入了可忽略不计的延迟开销（+0.82秒）。", "conclusion": "金字塔MoA架构提供了一个有效的方法来平衡性能与预算之间的折衷。"}}
{"id": "2602.19507", "pdf": "https://arxiv.org/pdf/2602.19507", "abs": "https://arxiv.org/abs/2602.19507", "authors": ["David Fraile Navarro", "Mor Peleg"], "title": "Conversational AI for Automated Patient Questionnaire Completion: Development Insights and Design Principles", "categories": ["cs.HC"], "comment": "13 pages", "summary": "Collecting patient-reported outcome measures (PROMs) is essential for clinical care and research, yet traditional form-based approaches are often tedious for patients and burdensome for clinicians. We developed a generative AI conversational agent(CA) using GPT-5 to collect back pain data according to the NIH Task Force's Recommended Minimal Dataset. Unlike prior CAs that ask questions one-by-one, our CA engages users in topic-based conversations, allowing multiple data items to be captured in a single exchange. Through iterative development and pilot testing with clinicians and a consumer panel, we identified key design principles for health data collection CAs. These principles extend established clinical decision support design guidelines to conversational interfaces, addressing: flexibility of interaction style, personality calibration, data quality assurance through confidence visualization, patient safety constraints, and interoperability requirements. We present our prompt design methodology and discuss challenges encountered, including managing conversation length, handling ambiguous responses, and adapting to LLM version changes. Our design principles provide a practical framework for developers creating conversational agents for patient questionnaire completion. The CA is available at https://chatgpt.com/g/g-68f4869548f48191af0544f110ee91c6-backpain-data-collection-assistant (requires ChatGPT registration and subscription for unlimited use).", "AI": {"tldr": "开发了一种基于GPT-5的生成式AI会话代理，用于自动完成患者问卷。", "motivation": "收集患者报告的结果测量对于临床护理和研究至关重要，但传统形式的方法往往令患者感到乏味且给医生带来负担。", "method": "通过迭代开发和试点测试，与医生和消费者小组合作，确定了设计原则，并提出了提示设计方法。处理的挑战包括管理对话长度、应对模棱两可的回答以及适应LLM版本的变化。", "result": "该代理能够根据NIH任务组推荐的最小数据集收集背部疼痛数据，提出了一套实用的设计框架用于创建患者问卷完成会话代理。", "conclusion": "提出的设计原则为开发者提供了实用框架，以创建有效的会话代理来完成患者问卷。"}}
{"id": "2602.19506", "pdf": "https://arxiv.org/pdf/2602.19506", "abs": "https://arxiv.org/abs/2602.19506", "authors": ["Byunggwan Son", "Jeimin Jeon", "Jeongwoo Choi", "Bumsub Ham"], "title": "Relational Feature Caching for Accelerating Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICLR 2026", "summary": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC", "AI": {"tldr": "本文提出了一种新的特征缓存框架，即关系特征缓存（RFC），以提高扩散变压器的性能。", "motivation": "现有基于时间外推的方法在减少重复计算方面有效，但依赖单一的时间外推会导致预测误差，并影响性能。为了改进这一点，作者通过详细分析，引入了输入和输出之间的关系来提高预测准确性。", "method": "提出了一种新的框架关系特征缓存（RFC），该框架利用输入-输出关系进行更准确的特征预测，并提出关系特征估计（RFE）和关系缓存调度（RCS）。", "result": "实验表明，RFC在各种DiT模型上都表现出优于先前方法的结果。", "conclusion": "本文通过引入新的框架和方法改进了扩散变压器中的特征缓存策略，显著提高了预测精度和性能。"}}
{"id": "2602.19505", "pdf": "https://arxiv.org/pdf/2602.19505", "abs": "https://arxiv.org/abs/2602.19505", "authors": ["Mingrui Wu", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Zhiyuan Liu", "Liujuan Cao", "Ming-Ming Cheng", "Rongrong Ji"], "title": "Test-Time Computing for Referring Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.21534", "summary": "We propose ControlMLLM++, a novel test-time adaptation framework that injects learnable visual prompts into frozen multimodal large language models (MLLMs) to enable fine-grained region-based visual reasoning without any model retraining or fine-tuning. Leveraging the insight that cross-modal attention maps intrinsically encode semantic correspondences between textual tokens and visual regions, ControlMLLM++ optimizes a latent visual token modifier during inference via a task-specific energy function to steer model attention towards user-specified areas. To enhance optimization stability and mitigate language prompt biases, ControlMLLM++ incorporates an improved optimization strategy (Optim++) and a prompt debiasing mechanism (PromptDebias). Supporting diverse visual prompt types including bounding boxes, masks, scribbles, and points, our method demonstrates strong out-of-domain generalization and interpretability. The code is available at https://github.com/mrwu-mac/ControlMLLM.", "AI": {"tldr": "论文提出了一种新的测试时适应框架ControlMLLM++，该框架通过注入可学习的视觉提示来增强冻结的大规模多模态语言模型（MLLM）的细粒度区域可视推理能力。", "motivation": "在不重新训练或微调模型的情况下，实现对大规模多模态语言模型进行细粒度的区域可视化推理。", "method": "通过优化一个隐含的视觉标记修改器来引导模型注意力指向用户指定的区域，并引入改进的优化策略和提示去偏机制以增强稳定性和减少语言提示偏差。", "result": "该方法支持多种类型的可视提示，如边界框、掩码、涂鸦点等，并展示了强大的域外泛化能力和解释能力。", "conclusion": "ControlMLLM++框架在无需重新训练或微调模型的情况下提高了大规模多模态语言模型的区域可视化推理能力。"}}
{"id": "2602.19503", "pdf": "https://arxiv.org/pdf/2602.19503", "abs": "https://arxiv.org/abs/2602.19503", "authors": ["Hyun-Ki Jung"], "title": "A Text-Guided Vision Model for Enhanced Recognition of Small Instances", "categories": ["cs.CV"], "comment": "Accepted for publication in Applied Computer Science (2026)", "summary": "As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To address this need, an efficient text-guided object detection model has been developed to enhance the detection of small objects. Specifically, an improved version of the existing YOLO-World model is introduced. The proposed method replaces the C2f layer in the YOLOv8 backbone with a C3k2 layer, enabling more precise representation of local features, particularly for small objects or those with clearly defined boundaries. Additionally, the proposed architecture improves processing speed and efficiency through parallel processing optimization, while also contributing to a more lightweight model design. Comparative experiments on the VisDrone dataset show that the proposed model outperforms the original YOLO-World model, with precision increasing from 40.6% to 41.6%, recall from 30.8% to 31%, F1 score from 35% to 35.5%, and mAP@0.5 from 30.4% to 30.7%, confirming its enhanced accuracy. Furthermore, the model demonstrates superior lightweight performance, with the parameter count reduced from 4 million to 3.8 million and FLOPs decreasing from 15.7 billion to 15.2 billion. These results indicate that the proposed approach provides a practical and effective solution for precise object detection in drone-based applications.", "AI": {"tldr": "提出了一种基于文本引导的视觉模型，用于增强无人机场景中小目标检测的准确性。", "motivation": "随着无人机技术的发展，用户不仅需要识别物体，还需要精准地定位特定的目标。传统的对象检测方法在处理小对象时表现不佳，因此提出了改进的方法来提高精确度和效率。", "method": "该模型基于YOLOv8，通过将C2f层替换为C3k2层以改善局部特征的表示能力，并采用平行处理优化提高了处理速度与效率。此外，参数量从4百万减少到3.8百万，FLOPs也有所降低。", "result": "在VisDrone数据集上的实验显示，改进后的模型显著提升了检测精度、召回率和mAP@0.5指标，并且在参数数量和计算复杂度上都有所优化。", "conclusion": "研究证明了该方法可以在无人机应用场景中提供更精确的对象检测能力，同时保持良好的轻量级特性。"}}
{"id": "2602.19502", "pdf": "https://arxiv.org/pdf/2602.19502", "abs": "https://arxiv.org/abs/2602.19502", "authors": ["Lalitha Pranathi Pulavarthy", "Raajitha Muthyala", "Aravind V Kuruvikkattil", "Zhenan Yin", "Rashmita Kudamala", "Saptarshi Purkayastha"], "title": "Human-Guided Agentic AI for Multimodal Clinical Prediction: Lessons from the AgentDS Healthcare Benchmark", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to the Data Challenge track at the 14th IEEE International Conference on Healthcare Informatics (ICHI) 2026", "summary": "Agentic AI systems are increasingly capable of autonomous data science workflows, yet clinical prediction tasks demand domain expertise that purely automated approaches struggle to provide. We investigate how human guidance of agentic AI can improve multimodal clinical prediction, presenting our approach to all three AgentDS Healthcare benchmark challenges: 30-day hospital readmission prediction (Macro-F1 = 0.8986), emergency department cost forecasting (MAE = $465.13), and discharge readiness assessment (Macro-F1 = 0.7939). Across these tasks, human analysts directed the agentic workflow at key decision points, multimodal feature engineering from clinical notes, scanned PDF billing receipts, and time-series vital signs; task-appropriate model selection; and clinically informed validation strategies. Our approach ranked 5th overall in the healthcare domain, with a 3rd-place finish on the discharge readiness task. Ablation studies reveal that human-guided decisions compounded to a cumulative gain of +0.065 F1 over automated baselines, with multimodal feature extraction contributing the largest single improvement (+0.041 F1). We distill three generalizable lessons: (1) domain-informed feature engineering at each pipeline stage yields compounding gains that outperform extensive automated search; (2) multimodal data integration requires task-specific human judgment that no single extraction strategy generalizes across clinical text, PDFs, and time-series; and (3) deliberate ensemble diversity with clinically motivated model configurations outperforms random hyperparameter search. These findings offer practical guidance for teams deploying agentic AI in healthcare settings where interpretability, reproducibility, and clinical validity are essential.", "AI": {"tldr": "研究探讨了人类指导下的代理AI在多模态临床预测中的应用，特别是在三个AgentDS医疗基准挑战中取得了较好的性能。", "motivation": "纯粹自动化的方法难以提供领域专业知识，因此探索人类如何指导代理人工智能以改进多模态临床预测任务。", "method": "研究人员在关键决策点上引导了代理工作流程，包括从不同数据源进行特征工程、选择合适的模型以及采用临床验证策略。通过这种方式提高了三个AgentDS医疗基准挑战的表现：30天医院再入院预测、急诊费用预测和出院准备度评估。", "result": "研究方法在总体排名中位列第5，在出院准备度任务上排名第3；与自动化基线相比，人类指导决策带来了累积性能提升，尤其是在多模态特征提取方面取得了显著改进。", "conclusion": "该研究揭示了领域知识导向的特征工程在整个工作流程中的累进优势、特定任务的人工判断对于多模态数据集成的重要性以及临床动机配置的模型多样性可以优于随机超参数搜索。"}}
{"id": "2602.19498", "pdf": "https://arxiv.org/pdf/2602.19498", "abs": "https://arxiv.org/abs/2602.19498", "authors": ["Navid Akhavan Attar", "Hesam Asadollahzadeh", "Ling Luo", "Uwe Aickelin"], "title": "Softmax is not Enough (for Adaptive Conformal Classification)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The merit of Conformal Prediction (CP), as a distribution-free framework for uncertainty quantification, depends on generating prediction sets that are efficient, reflected in small average set sizes, while adaptive, meaning they signal uncertainty by varying in size according to input difficulty. A central limitation for deep conformal classifiers is that the nonconformity scores are derived from softmax outputs, which can be unreliable indicators of how certain the model truly is about a given input, sometimes leading to overconfident misclassifications or undue hesitation. In this work, we argue that this unreliability can be inherited by the prediction sets generated by CP, limiting their capacity for adaptiveness. We propose a new approach that leverages information from the pre-softmax logit space, using the Helmholtz Free Energy as a measure of model uncertainty and sample difficulty. By reweighting nonconformity scores with a monotonic transformation of the energy score of each sample, we improve their sensitivity to input difficulty. Our experiments with four state-of-the-art score functions on multiple datasets and deep architectures show that this energy-based enhancement improves the adaptiveness of the prediction sets, leading to a notable increase in both efficiency and adaptiveness compared to baseline nonconformity scores, without introducing any post-hoc complexity.", "AI": {"tldr": "本文提出了一种基于Helmholtz自由能的改进方法，通过预Softmax阶段的信息提高深度置信度评估和预测集自适应性。", "motivation": "传统的softmax输出作为非一致性分数可能导致模型对输入过于自信或犹豫不决，限制了生成预测集合的能力。", "method": "提出了一种新的方法来利用预softmax的logit空间中的信息，使用Helmholtz自由能作为模型不确定性和样本难度的度量。通过用能量得分单调变换后的权重非一致性分数，提高了对输入难度的敏感性。", "result": "实验结果表明，在四个最先进的评分函数上进行测试，所提方法在多个数据集和深度架构中表现出更高的效率和适应性，优于基线非一致得分。", "conclusion": "通过基于能量的方法改进了非一致性分数的生成过程，提高了模型预测集合的自适应性和置信度。"}}
{"id": "2602.19497", "pdf": "https://arxiv.org/pdf/2602.19497", "abs": "https://arxiv.org/abs/2602.19497", "authors": ["Mingrui Wu", "Hang Liu", "Jiayi Ji", "Xiaoshuai Sun", "Rongrong Ji"], "title": "MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models", "categories": ["cs.CV"], "comment": "CVPR2026", "summary": "Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \\textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \\textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.", "AI": {"tldr": "介绍MICON-Bench，一个用于评估多图像上下文生成任务的基准，并提出动态注意力再平衡（DAR）以增强跨图像一致性。", "motivation": "当前统一多模态模型虽有显著进展，但在处理多图像上下文时缺乏相应的挑战性测试。现有基准主要关注单图编辑或文本到图像的任务，因此需要新的评估标准和方法来提升这类任务的表现。", "method": "提出MICON-Bench，包含六个用于评测跨图像组合、推理及身份保持的任务；引入MLLM驱动的通过检查点进行自动验证的方法；以及无需训练且可直接应用的动态注意力再平衡（DAR）机制，以在生成过程中调整注意分配从而提高一致性。", "result": "实验表明MICON-Bench能够有效揭示多图像推理挑战，并证明DAR有助于改善模型在跨图一致性和减少幻觉方面的性能。", "conclusion": "通过引入新的评估标准和方法来提升多图像上下文生成任务的表现，为未来研究奠定了基础。"}}
{"id": "2602.19491", "pdf": "https://arxiv.org/pdf/2602.19491", "abs": "https://arxiv.org/abs/2602.19491", "authors": ["Samuel Bellaire", "Abdalmalek Abu-raddaha", "Natalie Kim", "Nathan Morhan", "William Elliott", "Samir Rawashdeh"], "title": "Botson: An Accessible and Low-Cost Platform for Social Robotics Research", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "5 pages, 7 figures", "summary": "Trust remains a critical barrier to the effective integration of Artificial Intelligence (AI) into human-centric domains. Disembodied agents, such as voice assistants, often fail to establish trust due to their inability to convey non-verbal social cues. This paper introduces the architecture of Botson: an anthropomorphic social robot powered by a large language model (LLM). Botson was created as a low-cost and accessible platform for social robotics research.", "AI": {"tldr": "本文介绍了Botson平台，这是一个基于大型语言模型的拟人化社交机器人，旨在促进社交机器人的研究。", "motivation": "建立信任是将人工智能有效集成到以人为中心领域的重要障碍。无实体代理如语音助手因无法传达非言语社会线索而难以建立信任。", "method": "本文介绍了一个名为Botson的平台架构，它是一个基于大型语言模型的拟人化社交机器人，旨在提供一个低成本且易用的研究工具。", "result": "Botson作为一个低费用和易于获取的平台被提出，用于促进社交机器人的研究。", "conclusion": "通过开发Botson，研究人员可以更有效地探索人工智能在人类环境中的应用，并可能提高人们对这些技术的信任。"}}
{"id": "2602.19489", "pdf": "https://arxiv.org/pdf/2602.19489", "abs": "https://arxiv.org/abs/2602.19489", "authors": ["Bryan Guanrong Shan", "Alysa Ziying Tan", "Han Yu"], "title": "Federated Learning Playground", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present Federated Learning Playground, an interactive browser-based platform inspired by and extends TensorFlow Playground that teaches core Federated Learning (FL) concepts. Users can experiment with heterogeneous client data distributions, model hyperparameters, and aggregation algorithms directly in the browser without coding or system setup, and observe their effects on client and global models through real-time visualizations, gaining intuition for challenges such as non-IID data, local overfitting, and scalability. The playground serves as an easy to use educational tool, lowering the entry barrier for newcomers to distributed AI while also offering a sandbox for rapidly prototyping and comparing FL methods. By democratizing exploration of FL, it promotes broader understanding and adoption of this important paradigm.", "AI": {"tldr": "介绍了一个名为Federated Learning Playground的交互式浏览器平台，旨在教授联邦学习的基本概念和实验。", "motivation": "降低联邦学习的学习门槛，并促进其理解和采用。通过提供一个无需编程或系统设置即可进行实验的工具，帮助用户理解非IID数据、局部过拟合等问题。", "method": "基于TensorFlow Playground开发了一个交互式的浏览器平台，允许用户直接在浏览器中调整客户端的数据分布、模型超参数和聚合算法，并实时观察其效果。", "result": "提供了一个易于使用的教育工具，有助于快速原型设计和比较联邦学习方法，并增强了对这些挑战的理解。", "conclusion": "通过推广探索性实验的民主化，促进了联邦学习的更广泛理解和应用。"}}
{"id": "2602.19487", "pdf": "https://arxiv.org/pdf/2602.19487", "abs": "https://arxiv.org/abs/2602.19487", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Jiang Gui"], "title": "Exploiting Label-Independent Regularization from Spatial Dependencies for Whole Slide Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Whole slide images, with their gigapixel-scale panoramas of tissue samples, are pivotal for precise disease diagnosis. However, their analysis is hindered by immense data size and scarce annotations. Existing MIL methods face challenges due to the fundamental imbalance where a single bag-level label must guide the learning of numerous patch-level features. This sparse supervision makes it difficult to reliably identify discriminative patches during training, leading to unstable optimization and suboptimal solutions. We propose a spatially regularized MIL framework that leverages inherent spatial relationships among patch features as label-independent regularization signals. Our approach learns a shared representation space by jointly optimizing feature-induced spatial reconstruction and label-guided classification objectives, enforcing consistency between intrinsic structural patterns and supervisory signals. Experimental results on multiple public datasets demonstrate significant improvements over state-of-the-art methods, offering a promising direction.", "AI": {"tldr": "提出了一种基于空间依赖性的多实例学习框架，用于改进全滑片图像分析。", "motivation": "当前的多实例学习方法由于数据量大和标注稀疏，难以从单个样本标签中可靠地识别出具有区分度的数据块，导致优化不稳定和次优解。", "method": "该方法通过引入空间重构目标来利用内在的空间关系作为独立于标签的正则化信号，并结合分类目标学习共享表示空间。这种方法强制结构模式与监督信号之间的一致性。", "result": "实验结果表明，该框架在多个公开数据集上显著优于现有技术。", "conclusion": "提出的方法通过引入空间依赖性作为独立于标签的正则化信号来改进全滑片图像分析的有效性和稳定性。"}}
{"id": "2602.19483", "pdf": "https://arxiv.org/pdf/2602.19483", "abs": "https://arxiv.org/abs/2602.19483", "authors": ["Arjun Chatterjee", "Sayeed Sajjad Razin", "John Wu", "Siddhartha Laghuvarapu", "Jathurshan Pradeepkumar", "Jimeng Sun"], "title": "Making Conformal Predictors Robust in Healthcare Settings: a Case Study on EEG Classification", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Under Review", "summary": "Quantifying uncertainty in clinical predictions is critical for high-stakes diagnosis tasks. Conformal prediction offers a principled approach by providing prediction sets with theoretical coverage guarantees. However, in practice, patient distribution shifts violate the i.i.d. assumptions underlying standard conformal methods, leading to poor coverage in healthcare settings. In this work, we evaluate several conformal prediction approaches on EEG seizure classification, a task with known distribution shift challenges and label uncertainty. We demonstrate that personalized calibration strategies can improve coverage by over 20 percentage points while maintaining comparable prediction set sizes. Our implementation is available via PyHealth, an open-source healthcare AI framework: https://github.com/sunlabuiuc/PyHealth.", "AI": {"tldr": "本文研究如何在医疗环境中使符合预测方法鲁棒化，通过EEG癫痫分类案例展示了个性化校准策略能显著提高覆盖度。", "motivation": "量化临床诊断中的不确定性对于高风险任务至关重要。然而，在实际医疗应用中，患者分布变化导致标准符合预测方法的独立同分布假设失效，从而在医疗环境中表现不佳。", "method": "本文评估了几种符合预测方法在EEG癫痫分类任务上的性能，并展示了个性化校准策略能显著提高覆盖度。", "result": "采用个性化校准策略后，符合预测器的覆盖度提高了20多个百分点，同时保持了相似大小的预测集。", "conclusion": "本文证明了通过个性化校准可以有效提升符合预测方法在医疗环境中的鲁棒性和性能。"}}
{"id": "2602.19475", "pdf": "https://arxiv.org/pdf/2602.19475", "abs": "https://arxiv.org/abs/2602.19475", "authors": ["Pao-Hsiung Chiu", "Jian Cheng Wong", "Chin Chun Ooi", "Chang Wei", "Yuchen Fan", "Yew-Soon Ong"], "title": "Scale-PINN: Learning Efficient Physics-Informed Neural Networks Through Sequential Correction", "categories": ["cs.CE", "cs.AI", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a promising mesh-free paradigm for solving partial differential equations, yet adoption in science and engineering is limited by slow training and modest accuracy relative to modern numerical solvers. We introduce the Sequential Correction Algorithm for Learning Efficient PINN (Scale-PINN), a learning strategy that bridges modern physics-informed learning with numerical algorithms. Scale-PINN incorporates the iterative residual-correction principle, a cornerstone of numerical solvers, directly into the loss formulation, marking a paradigm shift in how PINN losses can be conceived and constructed. This integration enables Scale-PINN to achieve unprecedented convergence speed across PDE problems from different physics domain, including reducing training time on a challenging fluid-dynamics problem for state-of-the-art PINN from hours to sub-2 minutes while maintaining superior accuracy, and enabling application to representative problems in aerodynamics and urban science. By uniting the rigor of numerical methods with the flexibility of deep learning, Scale-PINN marks a significant leap toward the practical adoption of PINNs in science and engineering through scalable, physics-informed learning. Codes are available at https://github.com/chiuph/SCALE-PINN.", "AI": {"tldr": "提出了一种学习策略Scale-PINN，通过迭代残差校正原理来提高物理信息神经网络的训练速度和准确性。", "motivation": "现有物理信息神经网络（PINNs）在科学和工程中的应用受限于慢速训练和相对较低的精度。为了解决这一问题，引入了一种新的学习策略Scale-PINN。", "method": "将数值求解器的核心迭代残差校正原理直接融入到损失函数中，通过这种方法提高了物理信息神经网络的学习效率。", "result": "在不同物理领域的偏微分方程问题上，Scale-PINN实现了前所未有的收敛速度；例如，在一个复杂的流体动力学问题上，将最先进的PINNs的训练时间从数小时缩短到了不到2分钟，并且保持了高精度。", "conclusion": "通过结合数值方法的严谨性和深度学习的灵活性，Scale-PINN为物理信息神经网络在科学和工程中的实际应用开辟了一条新的道路。"}}
{"id": "2602.19474", "pdf": "https://arxiv.org/pdf/2602.19474", "abs": "https://arxiv.org/abs/2602.19474", "authors": ["Wei Feng", "Haiyong Zheng"], "title": "Structured Bitmap-to-Mesh Triangulation for Geometry-Aware Discretization of Image-Derived Domains", "categories": ["cs.CG", "cs.CV", "cs.GR"], "comment": "Revised version after peer review; under review at Graphical Models. Earlier version appeared on SSRN", "summary": "We propose a template-driven triangulation framework that embeds raster- or segmentation-derived boundaries into a regular triangular grid for stable PDE discretization on image-derived domains. Unlike constrained Delaunay triangulation (CDT), which may trigger global connectivity updates, our method retriangulates only triangles intersected by the boundary, preserves the base mesh, and supports synchronization-free parallel execution. To ensure determinism and scalability, we classify all local boundary-intersection configurations up to discrete equivalence and triangle symmetries, yielding a finite symbolic lookup table that maps each case to a conflict-free retriangulation template. We prove that the resulting mesh is closed, has bounded angles, and is compatible with cotangent-based discretizations and standard finite element methods. Experiments on elliptic and parabolic PDEs, signal interpolation, and structural metrics show fewer sliver elements, more regular triangles, and improved geometric fidelity near complex boundaries. The framework is well suited for real-time geometric analysis and physically based simulation over image-derived domains.", "AI": {"tldr": "提出了一种基于模板的三角剖分框架，将图像或分割边界嵌入到规则三角网格中，以实现对图像衍生域稳定偏微分方程离散化。", "motivation": "为了在图像衍生域上进行稳定的PDE离散化，并解决传统约束Delaunay三角剖分可能触发全局连接更新的问题，提出了一个更有效的重新三角化的框架。", "method": "通过分类所有局部边界交集配置来实现模板驱动的三角剖分。该方法只对被边界截断的三角形进行重三角剖分，并且支持无同步并行执行。", "result": "实验表明，与传统方法相比，本方法产生更少的细长元素，更加规则的三角形和提高了复杂边界的几何精度。", "conclusion": "该框架适合于实时几何分析及物理基于模拟图像衍生域。"}}
{"id": "2602.19471", "pdf": "https://arxiv.org/pdf/2602.19471", "abs": "https://arxiv.org/abs/2602.19471", "authors": ["Zheang Huai", "Hui Tang", "Hualiang Wang", "Xiaomeng Li"], "title": "Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.", "AI": {"tldr": "提出了一种遗忘抵抗和病变感知的源自由领域自适应方法，用于基于视觉语言模型的眼底图像分析。", "motivation": "现有利用视觉语言模型进行源自由域适应的方法存在两个问题：一是容易忽视目标模型的一些优秀预测结果；二是忽略了视觉语言模型中丰富的细粒度知识。该研究旨在解决这些问题，以提高眼底图像诊断的准确性。", "method": "设计了一个遗忘抵抗适应模块和一个病变感知适应模块，前者用于保持目标模型的自信预测，后者利用视觉语言模型生成的补丁级预测来帮助目标模型识别病变区域并充分利用其细粒度知识。", "result": "实验结果表明，该方法不仅显著优于单一使用视觉语言模型的方法，在多个指标上也优于当前最先进的技术。", "conclusion": "通过引入遗忘抵抗和病变感知模块，可以有效提升基于视觉语言模型的眼底图像诊断性能。"}}
{"id": "2602.19470", "pdf": "https://arxiv.org/pdf/2602.19470", "abs": "https://arxiv.org/abs/2602.19470", "authors": ["Jiazhang Wang", "Hyelim Yang", "Tianyi Wang", "Florian Willomitzer"], "title": "Physics-informed Active Polarimetric 3D Imaging for Specular Surfaces", "categories": ["cs.CV", "physics.optics"], "comment": null, "summary": "3D imaging of specular surfaces remains challenging in real-world scenarios, such as in-line inspection or hand-held scanning, requiring fast and accurate measurement of complex geometries. Optical metrology techniques such as deflectometry achieve high accuracy but typically rely on multi-shot acquisition, making them unsuitable for dynamic environments. Fourier-based single-shot approaches alleviate this constraint, yet their performance deteriorates when measuring surfaces with high spatial frequency structure or large curvature. Alternatively, polarimetric 3D imaging in computer vision operates in a single-shot fashion and exhibits robustness to geometric complexity. However, its accuracy is fundamentally limited by the orthographic imaging assumption. In this paper, we propose a physics-informed deep learning framework for single-shot 3D imaging of complex specular surfaces. Polarization cues provide orientation priors that assist in interpreting geometric information encoded by structured illumination. These complementary cues are processed through a dual-encoder architecture with mutual feature modulation, allowing the network to resolve their nonlinear coupling and directly infer surface normals. The proposed method achieves accurate and robust normal estimation in single-shot with fast inference, enabling practical 3D imaging of complex specular surfaces.", "AI": {"tldr": "提出了一种基于物理信息的深度学习框架，用于单次拍摄复杂镜面表面的3D成像。", "motivation": "现有的光学测量技术在动态环境中不适合使用，而基于傅里叶的方法对具有高空间频率结构或大曲率的表面测量效果较差。极化三维成像方法虽然能够一次完成，但其准确性受到正交成像假设的限制。", "method": "该论文提出了一种结合物理信息的深度学习框架，使用双编码器架构与互调特征处理技术来解析结构照明和偏振线索之间的非线性耦合，并直接推断表面法向量。", "result": "所提出的方法能够准确且稳健地在单次拍摄中进行复杂镜面表面的3D成像，具有快速推理能力。", "conclusion": "该论文通过物理信息引导的深度学习方法，在一次拍摄中实现了对复杂镜面表面的精确和鲁棒性法线估计，使三维成像成为可能。"}}
{"id": "2602.19467", "pdf": "https://arxiv.org/pdf/2602.19467", "abs": "https://arxiv.org/abs/2602.19467", "authors": ["Michael Haman"], "title": "Can Large Language Models Replace Human Coders? Introducing ContentBench", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Project website: https://contentbench.github.io", "summary": "Can low-cost large language models (LLMs) take over the interpretive coding work that still anchors much of empirical content analysis? This paper introduces ContentBench, a public benchmark suite that helps answer this replacement question by tracking how much agreement low-cost LLMs achieve and what they cost on the same interpretive coding tasks. The suite uses versioned tracks that invite researchers to contribute new benchmark datasets. I report results from the first track, ContentBench-ResearchTalk v1.0: 1,000 synthetic, social-media-style posts about academic research labeled into five categories spanning praise, critique, sarcasm, questions, and procedural remarks. Reference labels are assigned only when three state-of-the-art reasoning models (GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1) agree unanimously, and all final labels are checked by the author as a quality-control audit. Among the 59 evaluated models, the best low-cost LLMs reach roughly 97-99% agreement with these jury labels, far above GPT-3.5 Turbo, the model behind early ChatGPT and the initial wave of LLM-based text annotation. Several top models can code 50,000 posts for only a few dollars, pushing large-scale interpretive coding from a labor bottleneck toward questions of validation, reporting, and governance. At the same time, small open-weight models that run locally still struggle on sarcasm-heavy items (for example, Llama 3.2 3B reaches only 4% agreement on hard-sarcasm). ContentBench is released with data, documentation, and an interactive quiz at contentbench.github.io to support comparable evaluations over time and to invite community extensions.", "AI": {"tldr": "介绍了ContentBench，一个公开的基准测试套件，用于评估低成本大型语言模型在解释性编码任务上的表现和成本。", "motivation": "探讨低成本大型语言模型是否能够替代人类编码人员进行内容分析的工作。", "method": "构建了一个包含1000个合成社交媒体帖子的数据集ContentBench-ResearchTalk v1.0，用以评估59种不同语言模型在解释性编码任务上的表现和成本。使用三个最先进的推理模型作为参考基准，通过它们的一致同意来确定标签。", "result": "最佳低成本大型语言模型达到了约97%-99%的标签一致性，远高于早期使用的GPT-3.5 Turbo模型。一些顶级模型可以以较低的成本处理大量文本编码任务，但仍有一些挑战，如对讽刺类内容识别不佳。", "conclusion": "ContentBench提供了一个开放和可扩展的平台，促进了大型语言模型在解释性编码任务上的评估，并推动了大规模文本分析从劳动力瓶颈转向验证、报告和治理问题。"}}
{"id": "2602.19463", "pdf": "https://arxiv.org/pdf/2602.19463", "abs": "https://arxiv.org/abs/2602.19463", "authors": ["Emma Jiren Wang", "Siying Hu", "Zhicong Lu"], "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "19 pages, 8 figures; Accepted by ACM CHI 2026. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (CHI'24)", "summary": "As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.", "AI": {"tldr": "通过双向动作和微叙事，促进亲密沟通。", "motivation": "现有的即时通讯工具无法充分支持双向互动反应、持续性交流以及个性化表达，影响了深度的情感交流和联系。", "method": "介绍了一种名为PuppetChat的双人消息原型系统，该系统采用互惠感知推荐器鼓励响应动作，并从用户故事中生成个性化的微叙事来支持个人历史背景下的互动。进行了为期10天、涉及11对亲密伴侣或朋友的田野研究。", "result": "研究表明这种方法增强了社交存在感、促进了更丰富的自我披露以及持续性和共同记忆的保持。", "conclusion": "PuppetChat通过双向动作和微叙事成功提升了沟通的质量，增强了人际间的联系深度。"}}
{"id": "2602.19461", "pdf": "https://arxiv.org/pdf/2602.19461", "abs": "https://arxiv.org/abs/2602.19461", "authors": ["Zelin Zhao", "Petr Molodyk", "Haotian Xue", "Yongxin Chen"], "title": "Laplacian Multi-scale Flow Matching for Generative Modeling", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to appear in ICLR 2026", "summary": "In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\\times$1024) while maintaining lower computational overhead.", "AI": {"tldr": "提出了一种利用多尺度表示进行图像生成建模的新型框架Laplacian multi-scale flow matching (LapFlow)。", "motivation": "为了改进传统的流匹配方法，通过消除不同尺度间的桥梁过程来提高生成质量、加速采样过程和促进规模扩展。", "method": "将图像分解为拉普拉斯金字塔残差，并通过混合变换器(MoT)架构并行处理不同尺度。该模型采用因果注意力机制，在不需显式重噪声的情况下并行生成多尺度表示。", "result": "实验表明，该方法在CelebA-HQ和ImageNet数据集上实现了更高的样本质量和更快的推理速度，同时减少了计算量。", "conclusion": "LapFlow框架可以有效扩展到高分辨率图像生成（高达1024x1024），且保持较低的计算开销。"}}
{"id": "2602.19458", "pdf": "https://arxiv.org/pdf/2602.19458", "abs": "https://arxiv.org/abs/2602.19458", "authors": ["Ziyang Guo", "Yifan Wu", "Jason Hartline", "Kenneth Holstein", "Jessica Hullman"], "title": "ComplLLM: Fine-tuning LLMs to Discover Complementary Signals for Decision-making", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Multi-agent decision pipelines can outperform single agent workflows when complementarity holds, i.e., different agents bring unique information to the table to inform a final decision. We propose ComplLLM, a post-training framework based on decision theory that fine-tunes a decision-assistant LLM using complementary information as reward to output signals that complement existing agent decisions. We validate ComplLLM on synthetic and real-world tasks involving domain experts, demonstrating how the approach recovers known complementary information and produces plausible explanations of complementary signals to support downstream decision-makers.", "AI": {"tldr": "本文提出了ComplLLM框架，用于通过互补信息的奖励来微调决策辅助语言模型。", "motivation": "多代理决策管道在不同代理提供独特信息时可以优于单代理工作流。作者希望通过引入互补信号改进下游决策者的决定。", "method": "该方法基于决策理论，使用合成和现实世界任务中的互补信息作为奖励来微调决策辅助的LLM。", "result": "实验表明该框架能够恢复已知的互补信息并为下游决策者提供可解释的互补信号。", "conclusion": "ComplLLM通过引入互补信号有效增强了多代理系统的性能，提高了最终决策的质量。"}}
{"id": "2602.19455", "pdf": "https://arxiv.org/pdf/2602.19455", "abs": "https://arxiv.org/abs/2602.19455", "authors": ["Zelin He", "Boran Han", "Xiyuan Zhang", "Shuai Zhang", "Haotian Lin", "Qi Zhu", "Haoyang Fang", "Danielle C. Maddix", "Abdul Fatir Ansari", "Akash Chandrayan", "Abhinav Pradhan", "Bernie Wang", "Matthew Reimherr"], "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Accepted by the 29th International Conference on Artificial Intelligence and Statistics (AISTATS 2026)", "summary": "Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.", "AI": {"tldr": "提出了一种混合知识注入框架，用于改进时间序列推理。", "motivation": "解决GRLM缺乏领域特定知识和TSLM缺乏泛化能力的问题。", "method": "通过将TSLM生成的见解直接注入GRLM的推理轨迹来实现时间序列推理。引入基于强化学习的方法获取知识丰富的轨迹，从而高效地进行知识注入。", "result": "在SenTSR-Bench和其他公开数据集上超越了TSLMs和GRLMs，表现优异。", "conclusion": "通过混合知识注入框架实现了强大且上下文感知的时间序列诊断洞察。"}}
{"id": "2602.19454", "pdf": "https://arxiv.org/pdf/2602.19454", "abs": "https://arxiv.org/abs/2602.19454", "authors": ["Kartik Jhawar", "Lipo Wang"], "title": "HD-TTA: Hypothesis-Driven Test-Time Adaptation for Safer Brain Tumor Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 3 figures, 2 tables", "summary": "Standard Test-Time Adaptation (TTA) methods typically treat inference as a blind optimization task, applying generic objectives to all or filtered test samples. In safety-critical medical segmentation, this lack of selectivity often causes the tumor mask to spill into healthy brain tissue or degrades predictions that were already correct. We propose Hypothesis-Driven TTA, a novel framework that reformulates adaptation as a dynamic decision process. Rather than forcing a single optimization trajectory, our method generates intuitive competing geometric hypotheses: compaction (is the prediction noisy? trim artifacts) versus inflation (is the valid tumor under-segmented? safely inflate to recover). It then employs a representation-guided selector to autonomously identify the safest outcome based on intrinsic texture consistency. Additionally, a pre-screening Gatekeeper prevents negative transfer by skipping adaptation on confident cases. We validate this proof-of-concept on a cross-domain binary brain tumor segmentation task, applying a source model trained on adult BraTS gliomas to unseen pediatric and more challenging meningioma target domains. HD-TTA improves safety-oriented outcomes (Hausdorff Distance (HD95) and Precision) over several state-of-the-art representative baselines in the challenging safety regime, reducing the HD95 by approximately 6.4 mm and improving Precision by over 4%, while maintaining comparable Dice scores. These results demonstrate that resolving the safety-adaptation trade-off via explicit hypothesis selection is a viable, robust path for safe clinical model deployment. Code will be made publicly available upon acceptance.", "AI": {"tldr": "提出了一种新的适应性框架HD-TTA，用于安全的脑肿瘤分割。", "motivation": "现有的测试时间适应方法在关键的安全医学分割任务中缺乏选择性，导致预测不准确或损伤周围健康组织。因此需要一种更加智能化且动态的决策过程来改进这一问题。", "method": "通过引入竞争几何假设（紧凑化和膨胀），HD-TTA将适应视为一个动态决策过程，并使用基于表示的选择器根据纹理一致性自动确定最佳结果，同时Gatekeeper防止在置信度高的情况下进行不必要的调整。", "result": "在跨域二进制脑肿瘤分割任务中，HD-TTA相比其他基线模型，显著提高了安全相关的指标（如Hausdorff距离和精度），其中Hausdorff距离降低了约6.4mm，精度提升了超过4%，且保持了相当的Dice分数。", "conclusion": "该研究证明了解决安全性适应性的权衡问题通过明确假设选择是可行且稳健的方法路径，这对于临床模型的安全部署至关重要。"}}
{"id": "2602.19450", "pdf": "https://arxiv.org/pdf/2602.19450", "abs": "https://arxiv.org/abs/2602.19450", "authors": ["Kunal Mukherjee"], "title": "Red-Teaming Claude Opus and ChatGPT-based Security Advisors for Trusted Execution Environments", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Trusted Execution Environments (TEEs) (e.g., Intel SGX and ArmTrustZone) aim to protect sensitive computation from a compromised operating system, yet real deployments remain vulnerable to microarchitectural leakage, side-channel attacks, and fault injection. In parallel, security teams increasingly rely on Large Language Model (LLM) assistants as security advisors for TEE architecture review, mitigation planning, and vulnerability triage. This creates a socio-technical risk surface: assistants may hallucinate TEE mechanisms, overclaim guarantees (e.g., what attestation does and does not establish), or behave unsafely under adversarial prompting. We present a red-teaming study of two prevalently deployed LLM assistants in the role of TEE security advisors: ChatGPT-5.2 and Claude Opus-4.6, focusing on the inherent limitations and transferability of prompt-induced failures across LLMs. We introduce TEE-RedBench, a TEE-grounded evaluation methodology comprising (i) a TEE-specific threat model for LLM-mediated security work, (ii) a structured prompt suite spanning SGX and TrustZone architecture, attestation and key management, threat modeling, and non-operational mitigation guidance, along with policy-bound misuse probes, and (iii) an annotation rubric that jointly measures technical correctness, groundedness, uncertainty calibration, refusal quality, and safe helpfulness. We find that some failures are not purely idiosyncratic, transferring up to 12.02% across LLM assistants, and we connect these outcomes to secure architecture by outlining an \"LLM-in-the-loop\" evaluation pipeline: policy gating, retrieval grounding, structured templates, and lightweight verification checks that, when combined, reduce failures by 80.62%.", "AI": {"tldr": "本论文研究了大型语言模型（LLM）在可信执行环境（TEE）安全顾问中的潜在风险，并提出了一种针对这些模型的红队测试方法。", "motivation": "随着安全团队越来越多地依赖于大型语言模型作为TEE架构审查、缓解规划和漏洞排查的安全顾问，这给技术系统带来了新的风险。LLM助手可能会错误描述TEE机制、过度声明保证或在对抗性提示下表现出不安全的行为。", "method": "本研究引入了TEE-RedBench评估方法，包括特定于TEE的威胁模型、结构化提示集以及评分规则，以评估大型语言模型作为TEE安全顾问的表现。测试涵盖SGX和TrustZone架构、认证与密钥管理等内容，并通过政策相关的滥用探测进行。", "result": "研究发现一些失败并不是纯粹独立的，在LLM助手之间可以转移高达12.02%，并提出了结合策略控制、检索基础化、结构模板以及轻量级验证检查以减少这些失败的方法，这种方法能够降低80.62%的失败率。", "conclusion": "通过对大型语言模型在TEE安全顾问角色中的评估和改进措施的研究，可以显著提高其安全性和可靠性。"}}
{"id": "2602.19449", "pdf": "https://arxiv.org/pdf/2602.19449", "abs": "https://arxiv.org/abs/2602.19449", "authors": ["Jason Wu", "Tianchen Zhao", "Chang Liu", "Jiarui Cai", "Zheng Zhang", "Zhuowei Li", "Aaditya Singh", "Xiang Xu", "Mani Srivastava", "Jonathan Wu"], "title": "Decoupling Vision and Language: Codebook Anchored Visual Adaptation", "categories": ["cs.CV"], "comment": "17 pages, accepted to CVPR2026 main conference", "summary": "Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.", "AI": {"tldr": "介绍了一种名为CRAFT的方法，用于通过稳定token空间实现视觉编码器的领域自适应，以提升LVLM在特定领域的性能。", "motivation": "现有方法通过连续特征接口调整图像编码器和语言模型之间的连接来实现领域自适应，但这种方法仍然耦合了两个组件，并且需要重新对齐。因此，提出了CRAFT方法，旨在解耦这两个部分并提高特定领域的视觉任务表现。", "method": "CRAFT利用离散代码本锚定视觉表示到稳定的token空间中，通过这种方式进行编码器的微调，使得领域自适应不修改模型其他部分，并能够与具有相同代码本的不同语言架构LVLM无缝结合。", "result": "在10个特定领域的基准测试上，CRAFT实现了平均13.51%的性能提升，并且保持了LLM的语言能力，优于操作连续token的方法。", "conclusion": "通过解耦视觉编码器和语言模型并引入代码本锚定技术，CRAFT方法成功地提高了LVLM在特定领域的表现。"}}
{"id": "2602.19442", "pdf": "https://arxiv.org/pdf/2602.19442", "abs": "https://arxiv.org/abs/2602.19442", "authors": ["Yecheng Zhang", "Rong Zhao", "Zhizhou Sha", "Yong Li", "Lei Wang", "Ce Hou", "Wen Ji", "Hao Huang", "Yunshan Wan", "Jian Yu", "Junhao Xia", "Yuru Zhang", "Chunlei Shi"], "title": "UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment", "categories": ["cs.CV"], "comment": "26 pages", "summary": "Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.", "AI": {"tldr": "本文提出了UrbanAlign，一种不需训练的后处理框架，用于校准视觉语言模型（VLM）的人类偏好对齐。", "motivation": "在特定领域任务中，将视觉语言模型输出与人类偏好的对齐通常需要细调或强化学习，这些方法都需要标注数据和GPU计算资源。本文提出了一种无需训练的方法来解决这一问题。", "method": "该框架包括三个紧密耦合的阶段：概念挖掘、多主体结构评分和几何校准，并通过端到端维度优化循环统一在一起。", "result": "应用UrbanAlign框架，模型在Place Pulse 2.0数据集上取得了72.2%的准确率（κ=0.45），超过了最佳监督基线15.1个百分点，并比未校准的VLM评分高出16.3个百分点。", "conclusion": "该方法能够以零模型权重修改的方式，实现视觉语言模型输出与人类偏好的对齐，在特定任务中表现出色。"}}
{"id": "2602.19441", "pdf": "https://arxiv.org/pdf/2602.19441", "abs": "https://arxiv.org/abs/2602.19441", "authors": ["Costain Nachuma", "Minhaz Zibran"], "title": "When AI Teammates Meet Code Review: Collaboration Signals Shaping the Integration of Agent-Authored Pull Requests", "categories": ["cs.SE", "cs.AI"], "comment": "5 pages, 2 figures, 1 table. Accepted at the 23rd International Conference on Mining Software Repositories (MSR 2026), Rio de Janeiro, Brazil", "summary": "Autonomous coding agents increasingly contribute to software development by submitting pull requests on GitHub; yet, little is known about how these contributions integrate into human-driven review workflows. We present a large empirical study of agent-authored pull requests using the public AIDev dataset, examining integration outcomes, resolution speed, and review-time collaboration signals. Using logistic regression with repository-clustered standard errors, we find that reviewer engagement has the strongest correlation with successful integration, whereas larger change sizes and coordination-disrupting actions, such as force pushes, are associated with a lower likelihood of merging. In contrast, iteration intensity alone provides limited explanatory power once collaboration signals are considered. A qualitative analysis further shows that successful integration occurs when agents engage in actionable review loops that converge toward reviewer expectations. Overall, our results highlight that the effective integration of agent-authored pull requests depends not only on code quality but also on alignment with established review and coordination practices.", "AI": {"tldr": "研究了自动编码代理在GitHub上提交的pull请求如何与人类代码审查工作流程整合，通过分析公开数据集和统计模型评估整合效果。", "motivation": "随着自动化编码代理越来越多地参与到软件开发中，了解它们提交的pull请求是如何融入到人工驱动的审核流程中的变得非常重要。研究目的是探讨自动编码代理贡献的有效性及与人类协作信号的关系。", "method": "通过分析公开数据集AIDev中的代理人代码审查记录，并使用逻辑回归模型（带有仓库聚类标准误差）来评估集成结果、解决速度和审查时间合作信号。", "result": "研究发现，评论者的参与度与成功整合之间有最强的相关性；较大的变更大小及破坏协调的行为如强制推送，则与较低的合并可能性相关。迭代强度在考虑协作信号后解释能力有限。", "conclusion": "有效整合自动编码代理提交的pull请求不仅取决于代码质量，还取决于与既定审核和协同实践的一致性。"}}
{"id": "2602.19439", "pdf": "https://arxiv.org/pdf/2602.19439", "abs": "https://arxiv.org/abs/2602.19439", "authors": ["Ruicheng Ao", "David Simchi-Levi", "Xinshang Wang"], "title": "OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents", "categories": ["cs.AI", "cs.LG", "math.OC"], "comment": "34 pages, 8 figures", "summary": "Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested. Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models. Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what \"rational\" means in their context is the higher-return investment.", "AI": {"tldr": "OptiRepair通过LLM代理实现了供应链优化模型的闭环诊断和修复。", "motivation": "供应链接优化模型经常因建模错误而变得不可行，这需要稀缺的操作研究专业知识。作者探索是否可以利用AI代理来执行该任务。", "method": "OptiRepair将此任务分为两个阶段：通用可行性阶段（迭代IIS引导的任何LP修复）和特定领域的验证阶段（五个基于库存理论的合理性检查）。通过训练两种8B参数模型并使用求解器验证奖励进行自教学习，作者测试了来自七个家族的22个API模型。", "result": "经过训练的模型达到了81.7%的合理恢复率。相比而言，最好的API模型和平均API模型分别达到42.2%和21.3%。差距主要集中在第一阶段修复：API模型平均只有27.6%的成功率，而训练后的模型为97.2%", "conclusion": "当前AI与可靠的模型修复之间存在两个差距：求解器交互（API模型只能恢复27.6%的不可行公式）和运营理据。前者需要有针对性的培训来解决；后者则需明确指定为可验证检查。对于采用AI进行操作规划的组织而言，将“合理”定义为其上下文中的概念是更高回报的投资。"}}
{"id": "2602.19437", "pdf": "https://arxiv.org/pdf/2602.19437", "abs": "https://arxiv.org/abs/2602.19437", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li", "Hong Yu"], "title": "FinSight-Net:A Physics-Aware Decoupled Network with Frequency-Domain Compensation for Underwater Fish Detection in Smart Aquaculture", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition. To address these challenges, we propose FinSight-Net, an efficient and physics-aware detection framework tailored for complex aquaculture environments. FinSight-Net introduces a Multi-Scale Decoupled Dual-Stream Processing (MS-DDSP) bottleneck that explicitly targets frequency-specific information loss via heterogeneous convolutional branches, suppressing backscattering artifacts while compensating distorted biological cues through scale-aware and channel-weighted pathways. We further design an Efficient Path Aggregation FPN (EPA-FPN) as a detail-filling mechanism: it restores high-frequency spatial information typically attenuated in deep layers by establishing long-range skip connections and pruning redundant fusion routes, enabling robust detection of non-rigid fish targets under severe blur and turbidity. Extensive experiments on DeepFish, AquaFishSet, and our challenging UW-BlurredFish benchmark demonstrate that FinSight-Net achieves state-of-the-art performance. In particular, on UW-BlurredFish, FinSight-Net reaches 92.8% mAP, outperforming YOLOv11s by 4.8% while reducing parameters by 29.0%, providing a strong and lightweight solution for real-time automated monitoring in smart aquaculture.", "AI": {"tldr": "提出FinSight-Net框架，利用物理感知的多尺度解耦双流处理和高效路径聚合FPN机制解决水下鱼类检测中的模糊和散射问题。", "motivation": "现有方法在提高准确性的同时增加了计算开销，并忽略了光吸收和散射导致的图像质量下降问题。提出FinSight-Net以减少这些问题，实现更准确且高效的水下鱼类检测。", "method": "利用多尺度解耦双流处理（MS-DDSP）瓶颈来补偿频域信息损失，并设计高效路径聚合FPN机制恢复高频率空间细节。", "result": "在DeepFish、AquaFishSet和UW-BlurredFish数据集上，FinSight-Net达到了92.8%的mAP，优于YOLOv11s，同时参数量减少了29.0%，实现轻量化高效检测。", "conclusion": "FinSight-Net提供了一种强大的、轻量级解决方案，能够实现实时智能养殖中的自动化监测。"}}
{"id": "2602.19432", "pdf": "https://arxiv.org/pdf/2602.19432", "abs": "https://arxiv.org/abs/2602.19432", "authors": ["Yifeng Huang", "Gia Khanh Nguyen", "Minh Hoai"], "title": "CountEx: Fine-Grained Counting via Exemplars and Exclusion", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents CountEx, a discriminative visual counting framework designed to address a key limitation of existing prompt-based methods: the inability to explicitly exclude visually similar distractors. While current approaches allow users to specify what to count via inclusion prompts, they often struggle in cluttered scenes with confusable object categories, leading to ambiguity and overcounting. CountEx enables users to express both inclusion and exclusion intent, specifying what to count and what to ignore, through multimodal prompts including natural language descriptions and optional visual exemplars. At the core of CountEx is a novel Discriminative Query Refinement module, which jointly reasons over inclusion and exclusion cues by first identifying shared visual features, then isolating exclusion-specific patterns, and finally applying selective suppression to refine the counting query. To support systematic evaluation of fine-grained counting methods, we introduce CoCount, a benchmark comprising 1,780 videos and 10,086 annotated frames across 97 category pairs. Experiments show that CountEx achieves substantial improvements over state-of-the-art methods for counting objects from both known and novel categories. The data and code are available at https://github.com/bbvisual/CountEx.", "AI": {"tldr": "该论文提出了CountEx，一个通过示例和排除方法进行精细计数的框架。", "motivation": "现有的基于提示的方法无法明确地排除视觉上相似的干扰物，在混乱场景中有混淆对象类别时容易产生歧义和过计。", "method": "CountEx允许用户通过多模态提示表达包含和排除意图，核心是一个新颖的区分查询优化模块，它通过识别共享视觉特征、隔离排除特定模式并应用选择性抑制来细化计数查询。", "result": "实验显示CountEx在已知和新类别对象计数上比现有最佳方法有显著改进。", "conclusion": "该论文提出了一种新的精细计数框架，解决了现有基于提示的方法的局限性，并通过CoCount基准测试验证了其优越性能。"}}
{"id": "2602.19430", "pdf": "https://arxiv.org/pdf/2602.19430", "abs": "https://arxiv.org/abs/2602.19430", "authors": ["Dong-Guw Lee", "Tai Hyoung Rhee", "Hyunsoo Jang", "Young-Sik Shin", "Ukcheol Shin", "Ayoung Kim"], "title": "TherA: Thermal-Aware Visual-Language Prompting for Controllable RGB-to-Thermal Infrared Translation", "categories": ["cs.CV"], "comment": null, "summary": "Despite the inherent advantages of thermal infrared(TIR) imaging, large-scale data collection and annotation remain a major bottleneck for TIR-based perception. A practical alternative is to synthesize pseudo TIR data via image translation; however, most RGB-to-TIR approaches heavily rely on RGB-centric priors that overlook thermal physics, yielding implausible heat distributions. In this paper, we introduce TherA, a controllable RGB-to-TIR translation framework that produces diverse and thermally plausible images at both scene and object level. TherA couples TherA-VLM with a latent-diffusion-based translator. Given a single RGB image and a user-prompted condition pair, TherA-VLM yields a thermal-aware embedding that encodes scene, object, material, and heat-emission context reflecting the input scene-condition pair. Conditioning the diffusion model on this embedding enables realistic TIR synthesis and fine-grained control across time of day, weather, and object state. Compared to other baselines, TherA achieves state-of-the-art translation performance, demonstrating improved zero-shot translation performance up to 33% increase averaged across all metrics.", "AI": {"tldr": "本文提出了一种可控的RGB到热红外图像转换框架TherA，该框架能够生成符合物理特性的多样化热红外图像。", "motivation": "大规模收集和标注热红外数据存在瓶颈，现有RGB至TIR转换方法忽视了热物理学特性，导致合成结果不合理。因此需要一种新的方法来解决这一问题。", "method": "TherA结合了TherA-VLM（视觉语言模型）和基于潜在扩散的翻译器。通过给定单个RGB图像和用户提示条件对，TherA-VLM生成热感知嵌入，该嵌入编码场景、物体、材料及辐射热信息。以此为条件训练扩散模型可实现真实的TIR合成，并在一天中不同时段、不同天气和物体状态下进行精细化控制。", "result": "实验表明，与现有基线方法相比，TherA实现了最先进的转换性能，在所有评估指标上的零样本翻译性能平均提高了33%。", "conclusion": "TherA框架展示了其在生成多样化且物理合理的热红外图像方面的优越性，并提供了跨时间、天气和物体状态的精细化控制能力。"}}
{"id": "2602.19424", "pdf": "https://arxiv.org/pdf/2602.19424", "abs": "https://arxiv.org/abs/2602.19424", "authors": ["Yuxuan Yang", "Zhonghao Yan", "Yi Zhang", "Bo Yun", "Muxi Diao", "Guowei Zhao", "Kongming Liang", "Wenbin Li", "Zhanyu Ma"], "title": "Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.", "AI": {"tldr": "提出了一种用于肝细胞病理分析的多模态大型语言模型Hepato-LLaVA，通过稀疏拓扑包注意力机制实现了高效的特征聚合。", "motivation": "当前计算方法在处理全切片图像时受限于固定分辨率和低效特征聚合，导致信息损失或冗余。为解决此问题，提出了一种新的多模态大型语言模型。", "method": "引入了稀疏拓扑包注意力机制，该机制有效聚集局部诊断证据，并保持全局上下文，同时克服数据缺乏的问题。", "result": "实验结果表明Hepato-LLaVA在肝细胞癌诊断和描述任务上取得了最先进的性能。", "conclusion": "通过高效特征聚合机制以及新的临床验证的数据集，Hepato-LLaVA在肝细胞病理分析方面实现了显著的改进。"}}
{"id": "2602.19423", "pdf": "https://arxiv.org/pdf/2602.19423", "abs": "https://arxiv.org/abs/2602.19423", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "title": "Prefer-DAS: Learning from Local Preferences and Sparse Prompts for Domain Adaptive Segmentation of Electron Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "Domain adaptive segmentation (DAS) is a promising paradigm for delineating intracellular structures from various large-scale electron microscopy (EM) without incurring extensive annotated data in each domain. However, the prevalent unsupervised domain adaptation (UDA) strategies often demonstrate limited and biased performance, which hinders their practical applications. In this study, we explore sparse points and local human preferences as weak labels in the target domain, thereby presenting a more realistic yet annotation-efficient setting. Specifically, we develop Prefer-DAS, which pioneers sparse promptable learning and local preference alignment. The Prefer-DAS is a promptable multitask model that integrates self-training and prompt-guided contrastive learning. Unlike SAM-like methods, the Prefer-DAS allows for the use of full, partial, and even no point prompts during both training and inference stages and thus enables interactive segmentation. Instead of using image-level human preference alignment for segmentation, we introduce Local direct Preference Optimization (LPO) and sparse LPO (SLPO), plug-and-play solutions for alignment with spatially varying human feedback or sparse feedback. To address potential missing feedback, we also introduce Unsupervised Preference Optimization (UPO), which leverages self-learned preferences. As a result, the Prefer-DAS model can effectively perform both weakly-supervised and unsupervised DAS, depending on the availability of points and human preferences. Comprehensive experiments on four challenging DAS tasks demonstrate that our model outperforms SAM-like methods as well as unsupervised and weakly-supervised DAS methods in both automatic and interactive segmentation modes, highlighting strong generalizability and flexibility. Additionally, the performance of our model is very close to or even exceeds that of supervised models.", "AI": {"tldr": "提出了一种新的域自适应分割方法Prefer-DAS，该方法利用局部偏好和稀疏提示进行电子显微镜图像中的细胞结构分割。", "motivation": "现有的无监督领域适应策略在实际应用中表现有限且具有偏差，本文旨在通过引入局部人类偏好和稀疏点作为弱标签来提高模型的实用性。", "method": "提出了一种可提示的多任务模型Prefer-DAS，该模型结合了自我训练和引导对比学习，并引入了空间变化的人类反馈对齐方案Local direct Preference Optimization (LPO) 和 Sparse LPO (SLPO)，以及无需人类反馈的Unsupervised Preference Optimization (UPO)。", "result": "实验结果表明，在自动和交互式分割模式下，Prefer-DAS模型优于现有的SAM类似方法以及其他无监督和弱监督域自适应分割方法，并且其性能接近甚至超过有监督的方法。", "conclusion": "该研究证明了通过利用局部偏好和稀疏提示可以有效提升电子显微镜图像中的细胞结构分割效果，展示出较强的泛化能力和灵活性。"}}
{"id": "2602.19422", "pdf": "https://arxiv.org/pdf/2602.19422", "abs": "https://arxiv.org/abs/2602.19422", "authors": ["Lingyun Chen", "Qing Xiao", "Zitao Zhang", "Eli Blevis", "Selma Šabanović"], "title": "Positioning Modular Co-Design in Future HRI Design Research", "categories": ["cs.HC", "cs.RO"], "comment": "4 pages, 1 figure, accepted by 3rd Workshop on Designerly HRI at HRI'26", "summary": "Design-oriented HRI is increasingly interested in robots as long-term companions, yet many designs still assume a fixed form and a stable set of functions. We present an ongoing design research program that treats modularity as a designerly medium - a way to make long-term human-robot relationships discussable and material through co-design. Across a series of lifespan-oriented co-design activities, participants repeatedly reconfigured the same robot for different life stages, using modular parts to express changing needs, values, and roles. From these outcomes, we articulate PAS (Personalization-Adaptability-Sustainability) as a human-centered lens on how people enact modularity in practice: configuring for self-expression, adapting across transitions, and sustaining robots through repair, reuse, and continuity. We then sketch next steps toward a fabrication-aware, community-extensible modular platform and propose evaluation criteria for designerly HRI work that prioritize expressive adequacy, lifespan plausibility, repairability-in-use, and responsible stewardship - not only usability or performance.", "AI": {"tldr": "本文研究了通过模块化共设计来探讨长期人机关系的方法，提出了PAS框架并讨论了未来的设计方向。", "motivation": "现有的人机交互设计往往假设机器人具有固定形式和稳定功能，而忽视了其作为长期伴侣的潜力。因此，作者希望通过模块化的手段使这种潜在的人机关系变得可讨论且具象化。", "method": "通过一系列针对不同生活阶段的共设计活动，参与者使用可拆卸部件重新配置同一台机器人以表达不同的需求、价值观和角色。", "result": "提出了PAS（个性化-适应性-可持续性）框架，并阐述了模块化的实践方式：自我表达配置、过渡期适应以及通过修复、再利用实现延续。", "conclusion": "提议未来的研究应关注更具可制造性和社区扩展性的模块化平台，同时提出评价标准，如充分的表达能力、合理的生命周期可行性、实际中的维修性能及负责任的态度。"}}
{"id": "2602.19418", "pdf": "https://arxiv.org/pdf/2602.19418", "abs": "https://arxiv.org/abs/2602.19418", "authors": ["Hefei Mei", "Zirui Wang", "Chang Xu", "Jianyuan Guo", "Minjing Dong"], "title": "PA-Attack: Guiding Gray-Box Attacks on LVLM Vision Encoders with Prototypes and Attention", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are foundational to modern multimodal applications, yet their susceptibility to adversarial attacks remains a critical concern. Prior white-box attacks rarely generalize across tasks, and black-box methods depend on expensive transfer, which limits efficiency. The vision encoder, standardized and often shared across LVLMs, provides a stable gray-box pivot with strong cross-model transfer. Building on this premise, we introduce PA-Attack (Prototype-Anchored Attentive Attack). PA-Attack begins with a prototype-anchored guidance that provides a stable attack direction towards a general and dissimilar prototype, tackling the attribute-restricted issue and limited task generalization of vanilla attacks. Building on this, we propose a two-stage attention enhancement mechanism: (i) leverage token-level attention scores to concentrate perturbations on critical visual tokens, and (ii) adaptively recalibrate attention weights to track the evolving attention during the adversarial process. Extensive experiments across diverse downstream tasks and LVLM architectures show that PA-Attack achieves an average 75.1% score reduction rate (SRR), demonstrating strong attack effectiveness, efficiency, and task generalization in LVLMs. Code is available at https://github.com/hefeimei06/PA-Attack.", "AI": {"tldr": "本文提出了一种针对LVLM视觉编码器的PA-Attack攻击方法，通过原型和注意力机制提高灰盒攻击的有效性和效率。", "motivation": "大型视觉语言模型（LVLM）在多模态应用中至关重要，但其对对抗性攻击的脆弱性仍然是一个关键问题。先前的研究显示白盒攻击难以跨任务泛化，黑盒方法依赖于昂贵的数据迁移，限制了效率。因此，本文旨在通过改进灰盒攻击方法解决这些问题。", "method": "PA-Attack首先使用原型锚定指南提供稳定的攻击方向，并提出一种两阶段注意力增强机制：（1）利用token级注意力分数将扰动集中在关键视觉令牌上；（2）自适应地重新校准注意力权重，以跟踪对抗过程中的注意力变化。", "result": "实验表明PA-Attack在各种下游任务和LVLM架构中实现了平均75.1%的得分减少率(SRR)，展示了其强大的攻击效果、效率和跨任务泛化能力。", "conclusion": "本文提出的PA-Attack有效提高了针对LVLM视觉编码器灰盒攻击的效果，为增强模型安全性提供了新的思路。"}}
{"id": "2602.19416", "pdf": "https://arxiv.org/pdf/2602.19416", "abs": "https://arxiv.org/abs/2602.19416", "authors": ["Mohammad Beigi", "Ming Jin", "Junshan Zhang", "Jiaxin Zhang", "Qifan Wang", "Lifu Huang"], "title": "IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.", "AI": {"tldr": "提出IR3框架，通过对比逆向强化学习解释和修复RLHF模型中的奖励黑客行为。", "motivation": "减少RLHF中引入的奖励黑客问题，并提高对内部化目标的理解以便于检测和纠正这些行为。", "method": "使用对比逆向强化学习（C-IRL）重建隐式奖励函数，通过稀疏自动编码器分解为可解释特征，识别并针对性地减轻有害特征以维持模型能力。", "result": "实验显示IR3在与真实奖励的相关性上达到0.89，并能以超过90%的精度识别黑客特征，在减少危害行为的同时保持原模型性能的约97%。", "conclusion": "IR3框架有效解决了RLHF中的奖励黑客问题，提高了对内部化目标的理解和控制能力。"}}
{"id": "2602.19412", "pdf": "https://arxiv.org/pdf/2602.19412", "abs": "https://arxiv.org/abs/2602.19412", "authors": ["Mingjie Li", "Yizheng Chen", "Md Tauhidul Islam", "Lei Xing"], "title": "Redefining the Down-Sampling Scheme of U-Net for Precision Biomedical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "AAPM 67th", "summary": "U-Net architectures have been instrumental in advancing biomedical image segmentation (BIS) but often struggle with capturing long-range information. One reason is the conventional down-sampling techniques that prioritize computational efficiency at the expense of information retention. This paper introduces a simple but effective strategy, we call it Stair Pooling, which moderates the pace of down-sampling and reduces information loss by leveraging a sequence of concatenated small and narrow pooling operations in varied orientations. Specifically, our method modifies the reduction in dimensionality within each 2D pooling step from $\\frac{1}{4}$ to $\\frac{1}{2}$. This approach can also be adapted for 3D pooling to preserve even more information. Such preservation aids the U-Net in more effectively reconstructing spatial details during the up-sampling phase, thereby enhancing its ability to capture long-range information and improving segmentation accuracy. Extensive experiments on three BIS benchmarks demonstrate that the proposed Stair Pooling can increase both 2D and 3D U-Net performance by an average of 3.8\\% in Dice scores. Moreover, we leverage the transfer entropy to select the optimal down-sampling paths and quantitatively show how the proposed Stair Pooling reduces the information loss.", "AI": {"tldr": "重新定义了U-Net的下采样方案，以提高生物医学图像分割精度", "motivation": "传统下采样技术在追求计算效率的同时牺牲了信息保留度，导致难以捕捉长距离信息。引入Stair Pooling策略以缓解这一问题", "method": "通过一系列串联的小而窄的池化操作来调整每个2D池化步骤中的维度减小比例，从1/4到1/2，并且可以适应3D池化来进一步保留更多信息", "result": "实验结果表明，在三个生物医学图像分割基准测试上，提出的Stair Pooling方法可以使Dice分数平均提高3.8%", "conclusion": "该方法通过减少信息损失增强了U-Net捕捉长距离信息的能力，从而提升了分割精度"}}
{"id": "2602.19410", "pdf": "https://arxiv.org/pdf/2602.19410", "abs": "https://arxiv.org/abs/2602.19410", "authors": ["Duy Anh Ta", "Farnaz Farid", "Farhad Ahamed", "Ala Al-Areqi", "Robert Beutel", "Tamara Watson", "Alana Maurushat"], "title": "BioEnvSense: A Human-Centred Security Framework for Preventing Behaviour-Driven Cyber Incidents", "categories": ["cs.CR", "cs.CY", "cs.HC", "cs.LG"], "comment": ":F.2.2; I.2.7", "summary": "Modern organizations increasingly face cybersecurity incidents driven by human behaviour rather than technical failures. To address this, we propose a conceptual security framework that integrates a hybrid Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model to analyze biometric and environmental data for context-aware security decisions. The CNN extracts spatial patterns from sensor data, while the LSTM captures temporal dynamics associated with human error susceptibility. The model achieves 84% accuracy, demonstrating its ability to reliably detect conditions that lead to elevated human-centred cyber risk. By enabling continuous monitoring and adaptive safeguards, the framework supports proactive interventions that reduce the likelihood of human-driven cyber incidents", "AI": {"tldr": "本文提出了一个基于生物识别和环境数据的混合CNN-LSTM模型，以预防行为驱动的网络事件。", "motivation": "现代组织面临越来越多由人为因素而非技术故障引起的网络安全问题。为了应对这一挑战，作者提出了一种集成了深度学习模型的安全框架。", "method": "该研究使用了一个结合了卷积神经网络（CNN）和长短期记忆网络（LSTM）的混合模型来分析传感器数据中的空间模式和时间动态以预测行为驱动的风险。", "result": "所提出的模型达到了84%的准确率，证明其能够可靠地识别出导致人类网络安全风险上升的条件", "conclusion": "该框架通过持续监控和支持主动干预措施降低了由人为因素引起的网络事件的概率"}}
{"id": "2602.19409", "pdf": "https://arxiv.org/pdf/2602.19409", "abs": "https://arxiv.org/abs/2602.19409", "authors": ["Henry Zhong", "Jörg M. Buchholz", "Julian Maclaren", "Simon Carlile", "Richard F. Lyon"], "title": "AuditoryHuM: Auditory Scene Label Generation and Clustering using Human-MLLM Collaboration", "categories": ["cs.SD"], "comment": null, "summary": "Manual annotation of audio datasets is labour intensive, and it is challenging to balance label granularity with acoustic separability. We introduce AuditoryHuM, a novel framework for the unsupervised discovery and clustering of auditory scene labels using a collaborative Human-Multimodal Large Language Model (MLLM) approach. By leveraging MLLMs (Gemma and Qwen) the framework generates contextually relevant labels for audio data. To ensure label quality and mitigate hallucinations, we employ zero-shot learning techniques (Human-CLAP) to quantify the alignment between generated text labels and raw audio content. A strategically targeted human-in-the-loop intervention is then used to refine the least aligned pairs. The discovered labels are grouped into thematically cohesive clusters using an adjusted silhouette score that incorporates a penalty parameter to balance cluster cohesion and thematic granularity. Evaluated across three diverse auditory scene datasets (ADVANCE, AHEAD-DS, and TAU 2019), AuditoryHuM provides a scalable, low-cost solution for creating standardised taxonomies. This solution facilitates the training of lightweight scene recognition models deployable to edge devices, such as hearing aids and smart home assistants. The project page and code: https://github.com/Australian-Future-Hearing-Initiative", "AI": {"tldr": "本文提出了一种利用人机协作生成和聚类音频场景标签的新框架AuditoryHuM。", "motivation": "手动注释音频数据集耗时且难以平衡标签的粒度与声学可分离性。通过引入AuditoryHuM，可以有效地解决这一问题，并提供标准化分类法以训练轻量级场景识别模型。", "method": "框架利用多模态大型语言模型生成上下文相关的音频标签，采用零样本学习技术量化生成文本标签和原始音频内容的一致性，运用人类干预修正一致性较低的配对。最后使用改进后的轮廓分数将发现的标签分组到主题一致性的聚类中。", "result": "AuditoryHuM在三个不同的音频场景数据集上进行了评估，并提供了一种可扩展、低成本创建标准化分类法的方法。", "conclusion": "通过人机合作，AuditoryHuM能够有效生成高质量且具有主题一致性的音频场景标签，为轻量级音频识别模型的训练提供了可能。"}}
{"id": "2602.19408", "pdf": "https://arxiv.org/pdf/2602.19408", "abs": "https://arxiv.org/abs/2602.19408", "authors": ["Sankardeep Chakraborty", "Roberto Grossi", "Ren Kimura", "Giulia Punzi", "Kunihiko Sadakane", "Wiktor Zuba"], "title": "Variations on the Problem of Identifying Spectrum-Preserving String Sets", "categories": ["cs.DS"], "comment": null, "summary": "In computational genomics, many analyses rely on efficient storage and traversal of $k$-mers, motivating compact representations such as spectrum-preserving string sets (SPSS), which store strings whose $k$-mer spectrum matches that of the input. Existing approaches, including Unitigs, Eulertigs and Matchtigs, model this task as a path cover problem on the deBruijn graph. We extend this framework from paths to branching structures by introducing necklace covers, which combine cycles and tree-like attachments (pendants). We present a greedy algorithm that constructs a necklace cover while guaranteeing, under certain conditions, optimality in the cumulative size of the final representation. Experiments on real genomic datasets indicate that the minimum necklace cover achieves smaller representations than Eulertigs and comparable compression to the Masked Superstrings approach, while maintaining exactness of the $k$-mer spectrum.", "AI": {"tldr": "本文研究了如何通过项链覆盖来构建紧凑的字符串集，以保持输入字符串的$k$-mer频谱。", "motivation": "在计算基因组学中，许多分析依赖于高效地存储和遍历$k$-mers，因此需要高效的压缩表示如光谱保留字符串集(SPPS)。现有方法主要集中在路径覆盖问题上，而本文扩展了这一框架，从路径到分支结构。", "method": "提出了一种贪婪算法来构造项链覆盖，并在一定条件下保证最终表示的累积大小最优。", "result": "实验表明，最小化项链覆盖可以实现更小的表示并且与Masked Superstrings方法相比具有可比压缩效果，同时保持$k$-mer频谱的确切性。", "conclusion": "通过引入项链覆盖的概念并利用贪婪算法构造其结构，本文提供了一种新的方法来构建紧凑的字符串集，并且在实验数据上验证了该方法的有效性和优越性。"}}
{"id": "2602.19404", "pdf": "https://arxiv.org/pdf/2602.19404", "abs": "https://arxiv.org/abs/2602.19404", "authors": ["Tomas Eglinskas"], "title": "One Size Fits None: Modeling NYC Taxi Trips", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 10 figures", "summary": "The rise of app-based ride-sharing has fundamentally changed tipping culture in New York City. We analyzed 280 million trips from 2024 to see if we could predict tips for traditional taxis versus high-volume for-hire services. By testing methods from linear regression to deep neural networks, we found two very different outcomes. Traditional taxis are highly predictable ($R^2 \\approx 0.72$) due to the in-car payment screen. In contrast, app-based tipping is random and hard to model ($R^2 \\approx 0.17$). In conclusion, we show that building one universal model is a mistake and, due to Simpson's paradox, a combined model looks accurate on average but fails to predict tips for individual taxi categories requiring specialized models.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.19401", "pdf": "https://arxiv.org/pdf/2602.19401", "abs": "https://arxiv.org/abs/2602.19401", "authors": ["Grace Barkhuff"], "title": "Reassurance Robots: OCD in the Age of Generative AI", "categories": ["cs.HC"], "comment": "8 pages, 1 figure, conditionally accepted for publication in CHI EA '26: Extended Abstracts of the ACM CHI Conference on Human Factors in Computing Systems April 2026", "summary": "Obsessive Compulsive Disorder (OCD) is a mental health disorder characterized by distressing repetitive patterns of thought, referred to as obsessions, and behaviors aimed to reduce the distress, referred to as compulsions. The explosion of artificial intelligence (AI) into the modern zeitgeist through the introduction of generative AI (GenAI) systems such as ChatGPT has led to novel obsessions and compulsions involving AI in individuals with OCD. Through an exploratory qualitative analysis of 100 Reddit posts related to AI on a popular subreddit for OCD, I examine the ways AI is impacting the presentation of OCD, including novel examples of AI-based obsessions and compulsions. I argue that GenAI in its current form harms individuals with OCD by becoming \"Reassurance Robots,\" and that future designs of GenAI must take OCD into account.", "AI": {"tldr": "探讨生成式AI对患有强迫症个体的影响，特别是如何成为所谓的“安抚机器人”，并通过分析Reddit帖子进行定性研究。", "motivation": "探究生成式AI系统如ChatGPT在现代文化中的普及对患有强迫症个体产生的新型痴迷与行为模式，并呼吁未来的设计应考虑这些影响。", "method": "通过收集并分析100篇关于人工智能的Reddit帖子，采用探索性的定性方法研究AI如何影响强迫症的表现形式。", "result": "揭示了生成式AI系统成为“安抚机器人”，为患有强迫症的人带来新的困扰和行为模式。", "conclusion": "指出当前形态的生成式AI对强迫症患者有害，并强调未来的设计必须考虑这些问题。"}}
{"id": "2602.19400", "pdf": "https://arxiv.org/pdf/2602.19400", "abs": "https://arxiv.org/abs/2602.19400", "authors": ["Tamil Selvan Gurunathan", "Aryya Gangopadhyay"], "title": "Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "We present a coverage framework that integrates Hilbert space-filling priors into decentralized multi-robot learning and execution. We augment DQN and PPO with Hilbert-based spatial indices to structure exploration and reduce redundancy in sparse-reward environments, and we evaluate scalability in multi-robot grid coverage. We further describe a waypoint interface that converts Hilbert orderings into curvature-bounded, time-parameterized SE(2) trajectories (planar (x, y, θ)), enabling onboard feasibility on resource-constrained robots. Experiments show improvements in coverage efficiency, redundancy, and convergence speed over DQN/PPO baselines. In addition, we validate the approach on a Boston Dynamics Spot legged robot, executing the generated trajectories in indoor environments and observing reliable coverage with low redundancy. These results indicate that geometric priors improve autonomy and scalability for swarm and legged robotics.", "AI": {"tldr": "论文提出了一种结合希尔伯特空间填充优先级的分散式多机器人学习和执行框架，以提高覆盖效率。", "motivation": "在稀疏奖励环境中减少探索冗余，并提高多机器人系统的可扩展性。", "method": "使用DQN和PPO算法并引入基于希尔伯特的空间索引来优化覆盖任务。实验中验证了该方法通过转换为可行的路径执行，应用于资源受限的机器人上。", "result": "实验显示在覆盖率、冗余度以及收敛速度方面优于单纯采用DQN/PPO的方法，并且在真实环境下的测试也证明其可靠性。", "conclusion": "基于几何优先级的方法可以提高群集和腿式机器人的自主性和可扩展性。"}}
{"id": "2602.19396", "pdf": "https://arxiv.org/pdf/2602.19396", "abs": "https://arxiv.org/abs/2602.19396", "authors": ["Amirhossein Farzam", "Majid Behabahani", "Mani Malek", "Yuriy Nevmyvaka", "Guillermo Sapiro"], "title": "Hiding in Plain Text: Detecting Concealed Jailbreaks via Activation Disentanglement", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) remain vulnerable to jailbreak prompts that are fluent and semantically coherent, and therefore difficult to detect with standard heuristics. A particularly challenging failure mode occurs when an attacker tries to hide the malicious goal of their request by manipulating its framing to induce compliance. Because these attacks maintain malicious intent through a flexible presentation, defenses that rely on structural artifacts or goal-specific signatures can fail. Motivated by this, we introduce a self-supervised framework for disentangling semantic factor pairs in LLM activations at inference. We instantiate the framework for goal and framing and construct GoalFrameBench, a corpus of prompts with controlled goal and framing variations, which we use to train Representation Disentanglement on Activations (ReDAct) module to extract disentangled representations in a frozen LLM. We then propose FrameShield, an anomaly detector operating on the framing representations, which improves model-agnostic detection across multiple LLM families with minimal computational overhead. Theoretical guarantees for ReDAct and extensive empirical validations show that its disentanglement effectively powers FrameShield. Finally, we use disentanglement as an interpretability probe, revealing distinct profiles for goal and framing signals and positioning semantic disentanglement as a building block for both LLM safety and mechanistic interpretability.", "AI": {"tldr": "该论文提出了一种用于检测大型语言模型（LLM）隐藏的违规行为的方法，通过分离激活中的语义因素来实现。", "motivation": "攻击者可以通过操纵请求的框架来诱导合规性，并且这些攻击在保持恶意意图的同时具有灵活的表现形式。传统的依赖于结构特征或特定目标签名的防御方法可能会失败。", "method": "该研究提出了一种基于自监督的学习框架，用于分离大型语言模型激活中的语义因素对（如目标和框架）。通过构建包含可控的目标和框架变化提示的PromptBench数据集，训练ReDAct模块以提取冻结LLM中的分散表示。然后提出了FrameShield，一个操作于框架表示上的异常检测器。", "result": "该方法在多个大型语言模型家族中改进了模型无关性的检测，并具有最小的计算开销。理论保障和广泛的实验验证表明其分离效果能有效驱动FrameShield。", "conclusion": "语义分解作为一种可解释性探针，揭示了目标和框架信号的独特轮廓，并为大型语言模型的安全性和机制解读奠定了基础"}}
{"id": "2602.19395", "pdf": "https://arxiv.org/pdf/2602.19395", "abs": "https://arxiv.org/abs/2602.19395", "authors": ["Karan Thakkar", "Mounya Elhilali"], "title": "DECAF: Dynamic Envelope Context-Aware Fusion for Speech-Envelope Reconstruction from EEG", "categories": ["cs.SD", "eess.SP"], "comment": "Accepted at ICASSP 2026", "summary": "Reconstructing the speech audio envelope from scalp neural recordings (EEG) is a central task for decoding a listener's attentional focus in applications like neuro-steered hearing aids. Current methods for this reconstruction, however, face challenges with fidelity and noise. Prevailing approaches treat it as a static regression problem, processing each EEG window in isolation and ignoring the rich temporal structure inherent in continuous speech. This study introduces a new, dynamic framework for envelope reconstruction that leverages this structure as a predictive temporal prior. We propose a state-space fusion model that combines direct neural estimates from EEG with predictions from recent speech context, using a learned gating mechanism to adaptively balance these cues. To validate this approach, we evaluate our model on the ICASSP 2023 Stimulus Reconstruction benchmark demonstrating significant improvements over static, EEG-only baselines. Our analyses reveal a powerful synergy between the neural and temporal information streams. Ultimately, this work reframes envelope reconstruction not as a simple mapping, but as a dynamic state-estimation problem, opening a new direction for developing more accurate and coherent neural decoding systems.", "AI": {"tldr": "通过EEG重建语音音包络，提高神经解码系统的准确性。", "motivation": "当前方法在音频重建中的准确性和噪声处理方面存在挑战。传统的静态回归方式忽略了连续语音的时序结构。", "method": "提出了一个结合直接从EEG中估计和最近语音上下文预测的状态空间融合模型，并使用学习到的门控机制来适应性地平衡这些线索。", "result": "在ICASSP 2023刺激重建基准上，该方法显著优于静态基线。", "conclusion": "将音包络重构视为动态状态估计问题，为开发更准确和连贯的神经解码系统打开了新的方向。"}}
{"id": "2602.19390", "pdf": "https://arxiv.org/pdf/2602.19390", "abs": "https://arxiv.org/abs/2602.19390", "authors": ["Philipp Zech", "Istvan David"], "title": "Artificial Intelligence for Modeling & Simulation in Digital Twins", "categories": ["cs.AI"], "comment": null, "summary": "The convergence of modeling & simulation (M&S) and artificial intelligence (AI) is leaving its marks on advanced digital technology. Pertinent examples are digital twins (DTs) - high-fidelity, live representations of physical assets, and frequent enablers of corporate digital maturation and transformation. Often seen as technological platforms that integrate an array of services, DTs have the potential to bring AI-enabled M&S closer to end-users. It is, therefore, paramount to understand the role of M&S in DTs, and the role of digital twins in enabling the convergence of AI and M&S. To this end, this chapter provides a comprehensive exploration of the complementary relationship between these three. We begin by establishing a foundational understanding of DTs by detailing their key components, architectural layers, and their various roles across business, development, and operations. We then examine the central role of M&S in DTs and provide an overview of key modeling techniques from physics-based and discrete-event simulation to hybrid approaches. Subsequently, we investigate the bidirectional role of AI: first, how AI enhances DTs through advanced analytics, predictive capabilities, and autonomous decision-making, and second, how DTs serve as valuable platforms for training, validating, and deploying AI models. The chapter concludes by identifying key challenges and future research directions for creating more integrated and intelligent systems.", "AI": {"tldr": "探讨数字孪生中建模与仿真(M&S)和人工智能(AI)的互补关系，以推动其在企业数字化成熟和转型中的应用。", "motivation": "理解M&S在数字孪生中的作用以及数字孪生如何促进AI和M&S的融合。", "method": "首先概述数字孪生的关键组件、架构层及其商业、开发和操作中的角色；接着分析M&S在数字孪生中扮演的核心角色，并介绍从物理仿真到混合方法的各种建模技术；进一步探讨了AI如何通过高级分析、预测能力和自主决策增强数字孪生，以及数字孪生作为训练、验证和部署AI模型的平台的价值。", "result": "尚未明确指出具体结果，但提出了关键挑战和未来研究方向，以创建更集成和智能系统。", "conclusion": "提出建立更加融合智能系统的必要性，并指出了面临的挑战与未来的探索方向。"}}
{"id": "2602.19387", "pdf": "https://arxiv.org/pdf/2602.19387", "abs": "https://arxiv.org/abs/2602.19387", "authors": ["Marco Knipfer", "Alexander Roman", "Konstantin T. Matchev", "Katia Matcheva", "Sergei Gleyzer"], "title": "AI Agents for Variational Quantum Circuit Design", "categories": ["quant-ph", "cs.ET", "hep-ex", "hep-lat", "hep-ph"], "comment": "43 pages, 12 figures", "summary": "Variational quantum circuits (VQCs) constitute a central building block of near-term quantum machine learning (QML), yet the principled design of expressive and trainable architectures remains a major open challenge. The VQC design space grows combinatorially with the number of qubits, layers, entanglement structures, and gate parameterizations, rendering manual circuit construction inefficient and often suboptimal. We introduce an autonomous agent-based framework for VQC architecture search that integrates high-level reasoning with a quantum simulation environment. The agent proposes candidate circuit architectures, evaluates them through fully automated training and validation pipelines, and iteratively improves its design strategy via performance-driven feedback. Empirically, we show that the agent autonomously evolves circuit architectures from simple initial ansätze toward increasingly expressive designs, progressively trying to improve task performance. This demonstrates that agentic AI can effectively navigate and refine the VQC design landscape with minimal human intervention, providing a scalable methodology for automated quantum model development in the Noisy Intermediate-Scale Quantum (NISQ) regime.", "AI": {"tldr": "介绍了一种基于自主代理的框架，用于设计变分量子电路（VQC），该框架通过高阶推理和自动化训练验证流程来提高任务性能。", "motivation": "由于近似量子机器学习中的VQC设计空间复杂度极高，导致手工构建效率低下且往往次优。因此，提出了一种利用代理进行自主搜索的方法以解决这一挑战。", "method": "引入了集成高阶推理与量子模拟环境的自动代理框架，该代理提出候选电路结构并经过自动化训练验证管道评估其性能，通过反馈迭代改进设计策略。", "result": "实验表明，所提出的代理能够从简单初始模板逐渐演化出更复杂的电路架构，并持续尝试提升任务表现。这说明了在NISQ时代具有最小人工干预的自动量子模型开发方法的有效性与可扩展性。", "conclusion": "展示了一种有效的方法来自主地导航和优化VQC设计领域，该方法可以减少人为干预并提供一种规模化的自动化量子模型发展策略。"}}
{"id": "2602.19385", "pdf": "https://arxiv.org/pdf/2602.19385", "abs": "https://arxiv.org/abs/2602.19385", "authors": ["Minxue Tang", "Yangyang Yu", "Aolin Ding", "Maziyar Baran Pouyan", "Taha Belkhouja Yujia Bao"], "title": "Adaptive Data Augmentation with Multi-armed Bandit: Sample-Efficient Embedding Calibration for Implicit Pattern Recognition", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Recognizing implicit visual and textual patterns is essential in many real-world applications of modern AI. However, tackling long-tail pattern recognition tasks remains challenging for current pre-trained foundation models such as LLMs and VLMs. While finetuning pre-trained models can improve accuracy in recognizing implicit patterns, it is usually infeasible due to a lack of training data and high computational overhead. In this paper, we propose ADAMAB, an efficient embedding calibration framework for few-shot pattern recognition. To maximally reduce the computational costs, ADAMAB trains embedder-agnostic light-weight calibrators on top of fixed embedding models without accessing their parameters. To mitigate the need for large-scale training data, we introduce an adaptive data augmentation strategy based on the Multi-Armed Bandit (MAB) mechanism. With a modified upper confidence bound algorithm, ADAMAB diminishes the gradient shifting and offers theoretically guaranteed convergence in few-shot training. Our multi-modal experiments justify the superior performance of ADAMAB, with up to 40% accuracy improvement when training with less than 5 initial data samples of each class.", "AI": {"tldr": "提出了一种基于多臂老虎机的数据增强策略的高效嵌入校准框架ADAMAB，以改善少样本下的隐式模式识别。", "motivation": "当前预训练模型在处理长尾的隐式图案识别任务时面临挑战，因为缺乏足够的训练数据和高昂的计算成本使得微调变得不可行。因此，需要一种新的方法来解决这个问题。", "method": "ADAMAB通过在其上训练参数不可访问的轻量级校准器并使用基于多臂老虎机机制的数据增强策略来减少对大规模训练数据的需求。修改后的置信上限算法减少了梯度漂移，并提供了理论保证的收敛性，从而在少样本训练中表现出色。", "result": "实验表明ADAMAB具有优越性能，在每个类别的初始数据样本量少于5个时，准确率提高高达40%。", "conclusion": "通过结合多臂老虎机的数据增强和高效的嵌入校准技术，ADAMAB能够显著提升在少量训练样本情况下的模式识别性能。"}}
{"id": "2602.19380", "pdf": "https://arxiv.org/pdf/2602.19380", "abs": "https://arxiv.org/abs/2602.19380", "authors": ["Huayu Wang", "Bahaa Alattar", "Cheng-Yen Yang", "Hsiang-Wei Huang", "Jung Heon Kim", "Linda Shapiro", "Nathan White", "Jenq-Neng Hwang"], "title": "Detector-in-the-Loop Tracking: Active Memory Rectification for Stable Glottic Opening Localization", "categories": ["cs.CV"], "comment": "Accepted to Medical Imaging with Deep Learning (MIDL) 2026", "summary": "Temporal stability in glottic opening localization remains challenging due to the complementary weaknesses of single-frame detectors and foundation-model trackers: the former lacks temporal context, while the latter suffers from memory drift. Specifically, in video laryngoscopy, rapid tissue deformation, occlusions, and visual ambiguities in emergency settings require a robust, temporally aware solution that can prevent progressive tracking errors. We propose Closed-Loop Memory Correction (CL-MC), a detector-in-the-loop framework that supervises Segment Anything Model 2(SAM2) through confidence-aligned state decisions and active memory rectification. High-confidence detections trigger semantic resets that overwrite corrupted tracker memory, effectively mitigating drift accumulation with a training-free foundation tracker in complex endoscopic scenes. On emergency intubation videos, CL-MC achieves state-of-the-art performance, significantly reducing drift and missing rate compared with the SAM2 variants and open loop based methods. Our results establish memory correction as a crucial component for reliable clinical video tracking. Our code will be available in https://github.com/huayuww/CL-MR.", "AI": {"tldr": "提出了一种检测器循环记忆校正框架，用于在复杂内窥镜场景中稳定地定位声门开口。", "motivation": "单帧检测器缺乏时间上下文，基础模型跟踪器存在内存漂移问题，在视频喉镜检查中的快速组织变形、遮挡和视觉模糊需要一种稳健的时间感知解决方案来防止逐步的跟踪错误。", "method": "提出了闭合循环记忆校正（CL-MC），通过信心对齐状态决策监督SAM2，并进行主动记忆修正，高置信度检测触发语义重置以覆盖被污染的跟踪器内存。", "result": "在紧急插管视频上，与SAM2变体和开环方法相比，CL-MC实现了最先进的性能，显著减少了漂移和丢失率。", "conclusion": "结果证明了记忆校正是可靠临床视频跟踪的关键组成部分。"}}
{"id": "2602.19373", "pdf": "https://arxiv.org/pdf/2602.19373", "abs": "https://arxiv.org/abs/2602.19373", "authors": ["Ali Saheb", "Johan Obando-Ceron", "Aaron Courville", "Pouya Bashivan", "Pablo Samuel Castro"], "title": "Stable Deep Reinforcement Learning via Isotropic Gaussian Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep reinforcement learning systems often suffer from unstable training dynamics due to non-stationarity, where learning objectives and data distributions evolve over time. We show that under non-stationary targets, isotropic Gaussian embeddings are provably advantageous. In particular, they induce stable tracking of time-varying targets for linear readouts, achieve maximal entropy under a fixed variance budget, and encourage a balanced use of all representational dimensions--all of which enable agents to be more adaptive and stable. Building on this insight, we propose the use of Sketched Isotropic Gaussian Regularization for shaping representations toward an isotropic Gaussian distribution during training. We demonstrate empirically, over a variety of domains, that this simple and computationally inexpensive method improves performance under non-stationarity while reducing representation collapse, neuron dormancy, and training instability.", "AI": {"tldr": "本文提出了一种通过使用各向同性高斯正则化来稳定深度强化学习训练的方法。", "motivation": "深度强化学习系统通常因非稳态目标而面临训练不稳定的挑战，这些目标导致学习目标和数据分布随时间变化。", "method": "作者引入了各向同性高斯嵌入，并提出了简化版的各向同性高斯正则化方法来塑造表征以趋向于各向同性的高斯分布。", "result": "该方法在多种领域中增强了性能，减少了表示塌陷和训练不稳定性。", "conclusion": "通过使用各向同性高斯嵌入和正则化技术，可以显著提高深度强化学习系统的适应性和稳定性。"}}
{"id": "2602.19372", "pdf": "https://arxiv.org/pdf/2602.19372", "abs": "https://arxiv.org/abs/2602.19372", "authors": ["Yanting Yang", "Shenyuan Gao", "Qingwen Bu", "Li Chen", "Dimitris N. Metaxas"], "title": "Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "ICRA 2026", "summary": "Solving complex, long-horizon robotic manipulation tasks requires a deep understanding of physical interactions, reasoning about their long-term consequences, and precise high-level planning. Vision-Language Models (VLMs) offer a general perceive-reason-act framework for this goal. However, previous approaches using reflective planning to guide VLMs in correcting actions encounter significant limitations. These methods rely on inefficient and often inaccurate implicit learning of state-values from noisy foresight predictions, evaluate only a single greedy future, and suffer from substantial inference latency. To address these limitations, we propose a novel test-time computation framework that decouples state evaluation from action generation. This provides a more direct and fine-grained supervisory signal for robust decision-making. Our method explicitly models the advantage of an action plan, quantified by its reduction in distance to the goal, and uses a scalable critic to estimate. To address the stochastic nature of single-trajectory evaluation, we employ beam search to explore multiple future paths and aggregate them during decoding to model their expected long-term returns, leading to more robust action generation. Additionally, we introduce a lightweight, confidence-based trigger that allows for early exit when direct predictions are reliable, invoking reflection only when necessary. Extensive experiments on diverse, unseen multi-stage robotic manipulation tasks demonstrate a 24.6% improvement in success rate over state-of-the-art baselines, while significantly reducing inference time by 56.5%.", "AI": {"tldr": "本文提出了一种基于价值指导的多路径反射框架，用于优化视觉语言模型在长时机器人操作任务中的策略。", "motivation": "先前的方法依赖于不准确的状态值学习，并且只能评估单一未来的动作序列。为了改进这一不足，本文提出了一个更直接和细化的监督信号方法来提高决策稳健性。", "method": "该方法通过显式建模动作计划的优势并使用可扩展批评者估计状态价值，同时采用束搜索探索多个未来路径以减少预测噪声的影响，并引入了轻量级触发器实现在必要时进行反射。", "result": "实验结果显示，在多阶段机器人操作任务上，成功率提高了24.6%，同时将推理时间减少了56.5%。", "conclusion": "所提出的方法显著改进了视觉语言模型在长时决策中的表现，实现了更高效且准确的动作生成。"}}
{"id": "2602.19368", "pdf": "https://arxiv.org/pdf/2602.19368", "abs": "https://arxiv.org/abs/2602.19368", "authors": ["Hazim AbdElazim", "Shadman Islam", "Mostafa Milani"], "title": "The Human Factor in Data Cleaning: Exploring Preferences and Biases", "categories": ["cs.DB", "cs.HC"], "comment": "Conference submission, 8 pages", "summary": "Data cleaning is often framed as a technical preprocessing step, yet in practice it relies heavily on human judgment. We report results from a controlled survey study in which participants performed error detection, data repair and imputation, and entity matching tasks on census-inspired scenarios with known semantic validity. We find systematic evidence for several cognitive bias mechanisms in data cleaning. Framing effects arise when surface-level formatting differences (e.g., capitalization or numeric presentation) increase false-positive error flags despite unchanged semantics. Anchoring and adjustment bias appears when expert cues shift participant decisions beyond parity, consistent with salience and availability effects. We also observe the representativeness heuristic: atypical but valid attribute combinations are frequently flagged as erroneous, and in entity matching tasks, surface similarity produces a substantial false-positive rate with high confidence. In data repair, participants show a robust preference for leaving values missing rather than imputing plausible values, consistent with omission bias. In contrast, automation-aligned switching under strong contradiction does not exceed a conservative rare-error tolerance threshold at the population level, indicating that deference to automated recommendations is limited in this setting. Across scenarios, bias patterns persist among technically experienced participants and across diverse workflow practices, suggesting that bias in data cleaning reflects general cognitive tendencies rather than lack of expertise. These findings motivate human-in-the-loop cleaning systems that clearly separate representation from semantics, present expert or algorithmic recommendations non-prescriptively, and support reflective evaluation of atypical but valid cases.", "AI": {"tldr": "研究探讨了人在数据清洗中的认知偏差和偏好。", "motivation": "在实际操作中，数据清洗依赖于人类判断，并可能受到多种认知偏误的影响。", "method": "通过受控调查实验让参与者执行错误检测、数据修复与插补以及实体匹配任务来探究这些偏误。", "result": "研究发现几种系统性的人类认知偏差模式，包括框架效应、锚定调整偏见和代表性启发法等。", "conclusion": "鉴于认知偏差的普遍存在，建议设计人机协作的数据清洗系统以减少此类影响。"}}
{"id": "2602.19367", "pdf": "https://arxiv.org/pdf/2602.19367", "abs": "https://arxiv.org/abs/2602.19367", "authors": ["Pratham Yashwante", "Rose Yu"], "title": "Time Series, Vision, and Language: Exploring the Limits of Alignment in Contrastive Representation Spaces", "categories": ["cs.AI", "cs.CV"], "comment": "24 Figures, 12 Tables", "summary": "The Platonic Representation Hypothesis posits that learned representations from models trained on different modalities converge to a shared latent structure of the world. However, this hypothesis has largely been examined in vision and language, and it remains unclear whether time series participate in such convergence. We first examine this in a trimodal setting and find that independently pretrained time series, vision, and language encoders exhibit near-orthogonal geometry in the absence of explicit coupling. We then apply post-hoc alignment by training projection heads over frozen encoders using contrastive learning, and analyze the resulting representations with respect to geometry, scaling behavior, and dependence on information density and input modality characteristics. Our investigation reveals that overall alignment in contrastive representation spaces improves with model size, but this alignment is asymmetric: time series align more strongly with visual representations than with text, and images can act as effective intermediaries between time series and language. We further see that richer textual descriptions improve alignment only up to a threshold; training on denser captions does not lead to further improvement. Analogous effects are observed for visual representations. Our findings shed light on considerations for building multimodal systems involving non-conventional data modalities beyond vision and language.", "AI": {"tldr": "研究时间序列、视觉和语言在对比表示空间中的对齐情况，探讨不同模态数据之间的潜在结构收敛性。", "motivation": "探索时间序列是否参与视觉和语言之间的潜在结构收敛，进一步理解多模态系统中非传统数据模态的融合问题。", "method": "首先通过独立预训练的时间序列、视觉和语言编码器来观察它们在对比表示空间中的几何特性；然后利用对比学习方法训练投影头以进行后对齐处理，并分析对齐后的表示形式及其依赖于信息密度和输入模态特性的行为变化。", "result": "发现在对比表示空间中，整体对齐随着模型规模的增加而提高，但这种对齐是不对称的：时间序列与视觉表示相比更倾向于与文本对齐，且图像可以作为时间序列和语言之间的有效媒介；此外，较丰富的文本描述只在一定程度上改善了对齐效果，更高密度的描述并未进一步提升。", "conclusion": "研究结果为构建涉及非传统数据模态的多模态系统提供了重要启示。"}}
{"id": "2602.19366", "pdf": "https://arxiv.org/pdf/2602.19366", "abs": "https://arxiv.org/abs/2602.19366", "authors": ["Zirui Xu", "Vasileios Tzoumas"], "title": "Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization", "categories": ["eess.SY", "cs.MA", "cs.RO", "math.OC"], "comment": null, "summary": "We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment.", "AI": {"tldr": "研究如何在带宽、数据率和连接性受限的情况下，扩展分布式子模集智优化。", "motivation": "在未知的、部分可观测的资源限制环境中，多智能体任务需要通过智能体间通信进行协调。面对现实中的通信约束，本研究旨在提高协调效率及近似最优行为的实现。", "method": "提出了一种自适应配置网络的方法，仅限于一跳通讯，并保持智能体间消息量小，每个智能体只传输自身行动信息；通过分布式在线集智优化来调整智能体通信范围以满足带宽限制。定义了协调价值（VoC）作为度量标准。", "result": "验证方法在模拟环境中表现出可扩展性和接近最优性：收敛速度更快，并能超越基准，即使面对环境先验知识缺失的情况也能表现优异。", "conclusion": "所提出的方法能够有效应对带宽受限的分布式协调问题，在各种网络拓扑结构中均能保持较优性能。"}}
{"id": "2602.19359", "pdf": "https://arxiv.org/pdf/2602.19359", "abs": "https://arxiv.org/abs/2602.19359", "authors": ["Kevin Qiu", "Yu Zhang", "Marek Cygan", "Josie Hughes"], "title": "Vid2Sid: Videos Can Help Close the Sim2Real Gap", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Calibrating a robot simulator's physics parameters (friction, damping, material stiffness) to match real hardware is often done by hand or with black-box optimizers that reduce error but cannot explain which physical discrepancies drive the error. When sensing is limited to external cameras, the problem is further compounded by perception noise and the absence of direct force or state measurements. We present Vid2Sid, a video-driven system identification pipeline that couples foundation-model perception with a VLM-in-the-loop optimizer that analyzes paired sim-real videos, diagnoses concrete mismatches, and proposes physics parameter updates with natural language rationales. We evaluate our approach on a tendon-actuated finger (rigid-body dynamics in MuJoCo) and a deformable continuum tentacle (soft-body dynamics in PyElastica). On sim2real holdout controls unseen during training, Vid2Sid achieves the best average rank across all settings, matching or exceeding black-box optimizers while uniquely providing interpretable reasoning at each iteration. Sim2sim validation confirms that Vid2Sid recovers ground-truth parameters most accurately (mean relative error under 13\\% vs. 28--98\\%), and ablation analysis reveals three calibration regimes. VLM-guided optimization excels when perception is clean and the simulator is expressive, while model-class limitations bound performance in more challenging settings.", "AI": {"tldr": "Vid2Sid是一种视频驱动的系统识别管道，通过结合基础模型感知与循环优化器来诊断模拟和现实中的具体差异，并提出物理参数更新。", "motivation": "现有的机器人模拟器校准方法通常需要手动调整或使用黑箱优化器，这些方法虽然可以减少错误但无法解释哪些物理差异驱动了这个过程。当只依赖外部摄像机进行感知时，问题变得更加复杂。", "method": "Vid2Sid利用视频数据来诊断模拟和现实之间的具体差距，并通过自然语言提供合理的参数更新建议。该系统结合了基础模型的感知能力和循环优化器的能力。", "result": "在实际应用中，与黑盒优化器相比，Vid2Sid不仅匹配或超过其性能，还能够以每次迭代的形式提供可解释的理由。此外，在模拟到模拟验证过程中，Vid2Sid可以更准确地恢复地面真实参数（平均相对误差低于13%）。", "conclusion": "通过视频驱动的系统识别管道结合循环优化器和基础模型感知，Vid2Sid能够有效解决机器人模拟器与现实硬件之间的物理差异问题，并提供可解释的理由。"}}
{"id": "2602.19358", "pdf": "https://arxiv.org/pdf/2602.19358", "abs": "https://arxiv.org/abs/2602.19358", "authors": ["Fangyi Chen", "Yaojie Shen", "Lu Xu", "Ye Yuan", "Shu Zhang", "Yulei Niu", "Longyin Wen"], "title": "Referring Layer Decomposition", "categories": ["cs.CV"], "comment": "ICLR 2026", "summary": "Precise, object-aware control over visual content is essential for advanced image editing and compositional generation. Yet, most existing approaches operate on entire images holistically, limiting the ability to isolate and manipulate individual scene elements. In contrast, layered representations, where scenes are explicitly separated into objects, environmental context, and visual effects, provide a more intuitive and structured framework for interpreting and editing visual content. To bridge this gap and enable both compositional understanding and controllable editing, we introduce the Referring Layer Decomposition (RLD) task, which predicts complete RGBA layers from a single RGB image, conditioned on flexible user prompts, such as spatial inputs (e.g., points, boxes, masks), natural language descriptions, or combinations thereof. At the core is the RefLade, a large-scale dataset comprising 1.11M image-layer-prompt triplets produced by our scalable data engine, along with 100K manually curated, high-fidelity layers. Coupled with a perceptually grounded, human-preference-aligned automatic evaluation protocol, RefLade establishes RLD as a well-defined and benchmarkable research task. Building on this foundation, we present RefLayer, a simple baseline designed for prompt-conditioned layer decomposition, achieving high visual fidelity and semantic alignment. Extensive experiments show our approach enables effective training, reliable evaluation, and high-quality image decomposition, while exhibiting strong zero-shot generalization capabilities.", "AI": {"tldr": "本文提出了一个名为Referring Layer Decomposition（RLD）的任务，旨在从单个RGB图像中预测出完整的RGBA图层，并通过灵活的用户指令进行控制。", "motivation": "现有的方法大多在对整个图像进行全面处理时缺乏分离和操纵场景元素的能力。而分层表示可以提供更直观、结构化的框架来理解和编辑视觉内容。", "method": "构建了一个大规模数据集RefLade，包含1.11M幅图像-图层-指令三元组，并提出了一种简单的基线模型RefLayer以实现基于提示的图层分解任务。", "result": "实验结果表明该方法能够有效进行训练、可靠评估和高质量图像分解，具有很强的零样本泛化能力。", "conclusion": "通过建立RLD作为研究任务并提供基准数据集以及评价协议，本文为可控图像编辑领域提供了新的研究方向。"}}
{"id": "2602.19357", "pdf": "https://arxiv.org/pdf/2602.19357", "abs": "https://arxiv.org/abs/2602.19357", "authors": ["Nilay Yilmaz", "Maitreya Patel", "Naga Sai Abhiram Kusumba", "Yixuan He", "Yezhou Yang"], "title": "MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Spatial visualization is the mental ability to imagine, transform, and manipulate the spatial characteristics of objects and actions. This intelligence is a part of human cognition where actions and perception are connected on a mental level. To explore whether state-of-the-art Vision-Language Models (VLMs) exhibit this ability, we develop MentalBlackboard, an open-ended spatial visualization benchmark for Paper Folding and Hole Punching tests within two core tasks: prediction and planning. Our prediction experiments reveal that models struggle with applying symmetrical transformations, even when they predict the sequence of unfolding steps correctly. Also, rotations introduce a significant challenge to the physical situational awareness for models. The planning task reveals limitations of models in analyzing symmetrical relationships and in implementing the multi-stage symmetry process, with Claude Opus 4.1 achieving the highest planning score at an accuracy of 10\\%. The top-performing model, o3, attains a peak performance of 71.6\\% on the generalization task, which does not require spatial visualization but transfers spatial data; however, it achieves only 25\\% accuracy on text-based prediction tasks.", "AI": {"tldr": "评估视觉语言模型在空间想象能力方面的表现。", "motivation": "探索最先进的视觉语言模型是否具备人类认知中连接动作和感知的空间想象力。", "method": "开发MentalBlackboard基准，包含纸张折叠和打孔测试的预测与规划任务，以评估模型的空间想象能力。", "result": "实验表明，模型在应用对称变换方面存在困难，尤其是旋转操作。在规划任务中，模型难以分析对称关系并实施多阶段对称过程，最高准确率仅10%。尽管o3模型在无需空间想象力的一般化任务中的表现最好，但在基于文本的预测任务中的准确性仅为25%。", "conclusion": "最先进的视觉语言模型在处理需要空间想象能力的任务时表现出显著限制。"}}
{"id": "2602.19355", "pdf": "https://arxiv.org/pdf/2602.19355", "abs": "https://arxiv.org/abs/2602.19355", "authors": ["David Rawlinson", "Gideon Kowadlo"], "title": "Active perception and disentangled representations allow continual, episodic zero and few-shot learning", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages; 7 figures", "summary": "Generalization is often regarded as an essential property of machine learning systems. However, perhaps not every component of a system needs to generalize. Training models for generalization typically produces entangled representations at the boundaries of entities or classes, which can lead to destructive interference when rapid, high-magnitude updates are required for continual or few-shot learning. Techniques for fast learning with non-interfering representations exist, but they generally fail to generalize. Here, we describe a Complementary Learning System (CLS) in which the fast learner entirely foregoes generalization in exchange for continual zero-shot and few-shot learning. Unlike most CLS approaches, which use episodic memory primarily for replay and consolidation, our fast, disentangled learner operates as a parallel reasoning system. The fast learner can overcome observation variability and uncertainty by leveraging a conventional slow, statistical learner within an active perception system: A contextual bias provided by the fast learner induces the slow learner to encode novel stimuli in familiar, generalized terms, enabling zero-shot and few-shot learning. This architecture demonstrates that fast, context-driven reasoning can coexist with slow, structured generalization, providing a pathway for robust continual learning.", "AI": {"tldr": "本文提出了一种互补学习系统（CLS），该系统包含一个快速学习器和一个慢速学习器，能够在持续的零样本和少量样本学习中表现出色。", "motivation": "传统机器学习模型在边界上生成纠缠表示时会导致干扰问题，从而影响快速更新能力。因此，作者希望通过一种新的架构解决这一问题，即通过非干涉性表示实现快速、持续的学习。", "method": "采用了一种互补学习系统（CLS），其中包含一个不进行泛化的快速学习器和一个慢速的统计学习器。该体系结构结合了主动感知系统，在这种情况下，快学习器为慢学习器提供上下文偏置，以便在新刺激中使用熟悉的通用术语。", "result": "提出的架构能够有效实现持续的零样本和少量样本学习，并且展示了快速、基于上下文驱动推理与缓慢、结构化泛化的共存路径。", "conclusion": "通过互补学习系统（CLS）的设计证明了，快慢两种不同类型的机器学习方法可以并行工作，从而为稳健的持续学习提供了一种可能的方式。"}}
{"id": "2602.19354", "pdf": "https://arxiv.org/pdf/2602.19354", "abs": "https://arxiv.org/abs/2602.19354", "authors": ["Eun Jeong Kang", "Fengyang Lin", "Angel Hsing-Chi Hwang"], "title": "Policy or Community?: Supporting Individual Model Creators' Open Model Development in Model Marketplaces", "categories": ["cs.HC"], "comment": null, "summary": "Lightweight fine-tuning techniques and the rise of 'open' AI model marketplaces have enabled individuals to easily build and release generative models. Yet, this accessibility also raises risks, including the production of harmful and infringing content. While platforms offer policies and responsible AI tools, their effectiveness may be limited, as creators engage with partially open models that vary widely in openness and transparency. To understand how platform governance can better support responsible practices, we conducted semi-structured interviews with 19 individual model creators. We identified three regulatory needs shaped by creators' workflows: reducing downstream harms, recognizing creators' contributions and originality, and securing model ownership. Creators also repurpose RAI tools primarily for self-protection and visibility, and their sense of responsibility is deeply shaped by community norms rather than formal policies. We argue that platforms' governance decisions must consider how policy interventions shape the practices and motivations of individual creators.", "AI": {"tldr": "该论文通过采访个体模型创作者，探讨了平台治理如何更好地支持负责任的AI模型开发。", "motivation": "随着轻量级微调技术的发展和'开放'AI模型市场的兴起，个人可以轻松创建并发布生成式模型。然而，这同时也带来了生产有害或侵权内容的风险。尽管平台提供了政策和责任AI工具，但其有效性可能有限，因此需要研究如何更好地支持个体创作者的负责任实践。", "method": "通过半结构化访谈的形式与19位个体模型创作者进行交流以了解他们的需求及行为模式。", "result": "识别出三个监管需求：减少下游危害、认可创造者的贡献和原创性以及保障模型所有权。发现创建者主要利用责任AI工具来进行自我保护和提高可见度，其责任感受到社区规范的影响更大。", "conclusion": "平台的治理决策应考虑到政策干预对个体创作者实践及动机产生的影响。"}}
{"id": "2602.19352", "pdf": "https://arxiv.org/pdf/2602.19352", "abs": "https://arxiv.org/abs/2602.19352", "authors": ["Varun Shiri", "Charles Liu", "Keyu Yao", "Jin L. C. Guo", "Jinghui Cheng"], "title": "Beyond Privacy Labels: How Users Perceive Different Information Sources for Understanding App's Privacy Practices", "categories": ["cs.HC"], "comment": "5 pages, 1 figure, CHI EA 2026", "summary": "Despite having growing awareness and concerns about privacy, technology users are often insufficiently informed of the data practices of various digital products to protect themselves. Privacy policies and privacy labels, as two conventional ways of communicating data practices, are each criticized for important limitations -- one being lengthy and filled with legal jargon, and the other oversimplified and inaccurate -- causing users significant difficulty in understanding the privacy practices of the products and assessing their impact. To mitigate those issues, we explore ways to enhance privacy labels with the relevant content in complementary sources, including privacy policy, app reviews, and community-curated privacy assessments. Our user study results indicate that perceived usefulness and trust on those information sources are personal and influenced by past experience. Our work highlights the importance of considering various information needs for privacy practice and consolidating different sources for more useful privacy solutions.", "AI": {"tldr": "研究通过结合多种信息源来改进隐私标签，以帮助用户更好地理解应用程序的隐私实践。", "motivation": "由于隐私政策和隐私标签各自存在的缺陷，导致用户难以理解和评估数字产品的隐私行为。因此，该研究旨在探索如何利用补充的信息来源改善用户的体验。", "method": "进行了用户调查研究来分析不同信息源的有用性和信任度，并根据个人过去的经验探讨其影响。", "result": "结果表明，用户对不同信息源的感知有用性和信任度受到个人经历的影响。", "conclusion": "这项工作强调了为了提供更实用的隐私解决方案，需要考虑不同的信息需求并整合各种来源的重要性。"}}
{"id": "2602.19350", "pdf": "https://arxiv.org/pdf/2602.19350", "abs": "https://arxiv.org/abs/2602.19350", "authors": ["Zhilin Guo", "Jing Yang", "Kyle Fogarty", "Jingyi Wan", "Boqiao Zhang", "Tianhao Wu", "Weihao Xia", "Chenliang Zhou", "Sakar Khattar", "Fangcheng Zhong", "Cristina Nader Vasconcelos", "Cengiz Oztireli"], "title": "PoseCraft: Tokenized 3D Body Landmark and Camera Conditioning for Photorealistic Human Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Digitizing humans and synthesizing photorealistic avatars with explicit 3D pose and camera controls are central to VR, telepresence, and entertainment. Existing skinning-based workflows require laborious manual rigging or template-based fittings, while neural volumetric methods rely on canonical templates and re-optimization for each unseen pose. We present PoseCraft, a diffusion framework built around tokenized 3D interface: instead of relying only on rasterized geometry as 2D control images, we encode sparse 3D landmarks and camera extrinsics as discrete conditioning tokens and inject them into diffusion via cross-attention. Our approach preserves 3D semantics by avoiding 2D re-projection ambiguity under large pose and viewpoint changes, and produces photorealistic imagery that faithfully captures identity and appearance. To train and evaluate at scale, we also implement GenHumanRF, a data generation workflow that renders diverse supervision from volumetric reconstructions. Our experiments show that PoseCraft achieves significant perceptual quality improvement over diffusion-centric methods, and attains better or comparable metrics to latest volumetric rendering SOTA while better preserving fabric and hair details.", "AI": {"tldr": "本文提出了一种新的扩散框架PoseCraft，通过编码稀疏的3D地标和相机外参作为离散条件令牌，并将其注入扩散过程中以生成逼真的图像。", "motivation": "现有的皮肤合成方法需要繁琐的手动装配或基于模板的拟合，而神经体积方法依赖于标准模板并在每个未见过的姿态上重新优化。PoseCraft旨在解决这些问题，提供一种新的生成逼真形象的方法。", "method": "PoseCraft采用扩散框架，并将3D地标和相机外参编码为离散条件令牌。通过交叉注意力机制注入扩散过程中，从而避免了在大型姿态和视角变化下的2D重投影模糊性。", "result": "实验表明，与基于扩散的现有方法相比，PoseCraft在感知质量上有了显著改进，并达到了最新的体积渲染SOTA水平或更好，同时更好地保留了织物和头发细节。", "conclusion": "本文提出的PoseCraft框架为生成逼真的3D形象提供了一种新的有效方式，它不仅提高了图像的质量，还能够更好地保持物体的细节。"}}
{"id": "2602.19349", "pdf": "https://arxiv.org/pdf/2602.19349", "abs": "https://arxiv.org/abs/2602.19349", "authors": ["Rohit Mohan", "Florian Drews", "Yakov Miron", "Daniele Cattaneo", "Abhinav Valada"], "title": "UP-Fuse: Uncertainty-guided LiDAR-Camera Fusion for 3D Panoptic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "LiDAR-camera fusion enhances 3D panoptic segmentation by leveraging camera images to complement sparse LiDAR scans, but it also introduces a critical failure mode. Under adverse conditions, degradation or failure of the camera sensor can significantly compromise the reliability of the perception system. To address this problem, we introduce UP-Fuse, a novel uncertainty-aware fusion framework in the 2D range-view that remains robust under camera sensor degradation, calibration drift, and sensor failure. Raw LiDAR data is first projected into the range-view and encoded by a LiDAR encoder, while camera features are simultaneously extracted and projected into the same shared space. At its core, UP-Fuse employs an uncertainty-guided fusion module that dynamically modulates cross-modal interaction using predicted uncertainty maps. These maps are learned by quantifying representational divergence under diverse visual degradations, ensuring that only reliable visual cues influence the fused representation. The fused range-view features are decoded by a novel hybrid 2D-3D transformer that mitigates spatial ambiguities inherent to the 2D projection and directly predicts 3D panoptic segmentation masks. Extensive experiments on Panoptic nuScenes, SemanticKITTI, and our introduced Panoptic Waymo benchmark demonstrate the efficacy and robustness of UP-Fuse, which maintains strong performance even under severe visual corruption or misalignment, making it well suited for robotic perception in safety-critical settings.", "AI": {"tldr": "UP-Fuse是一个用于增强3D全景分割的不确定性引导LiDAR-相机融合框架，能够在相机传感器退化或故障的情况下保持稳健性。", "motivation": "为了应对在恶劣条件下相机传感器降级或失败导致感知系统可靠性降低的问题，提出了一种新的不确定性感知的融合框架UP-Fuse。", "method": "首先将原始LiDAR数据投影到范围视图并通过LiDAR编码器进行编码；同时从摄像头中提取特征并投射到同一共享空间。核心在于采用基于预测不确定性的融合模块动态调节跨模态交互，确保只有可靠的视觉线索影响融合表示。", "result": "实验显示UP-Fuse在Panoptic nuScenes、SemanticKITTI和新引入的Panoptic Waymo基准上表现出色且稳健，在严重的视觉退化或不对齐的情况下仍能保持强大的性能。", "conclusion": "通过不确定性引导的方法，UP-Fuse能够增强3D全景分割任务中的感知系统的鲁棒性。"}}
{"id": "2602.19348", "pdf": "https://arxiv.org/pdf/2602.19348", "abs": "https://arxiv.org/abs/2602.19348", "authors": ["Sirine Bhouri", "Lan Wei", "Jian-Qing Zheng", "Dandan Zhang"], "title": "MultiDiffSense: Diffusion-Based Multi-Modal Visuo-Tactile Image Generation Conditioned on Object Shape and Contact Pose", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by 2026 ICRA", "summary": "Acquiring aligned visuo-tactile datasets is slow and costly, requiring specialised hardware and large-scale data collection. Synthetic generation is promising, but prior methods are typically single-modality, limiting cross-modal learning. We present MultiDiffSense, a unified diffusion model that synthesises images for multiple vision-based tactile sensors (ViTac, TacTip, ViTacTip) within a single architecture. Our approach uses dual conditioning on CAD-derived, pose-aligned depth maps and structured prompts that encode sensor type and 4-DoF contact pose, enabling controllable, physically consistent multi-modal synthesis. Evaluating on 8 objects (5 seen, 3 novel) and unseen poses, MultiDiffSense outperforms a Pix2Pix cGAN baseline in SSIM by +36.3% (ViTac), +134.6% (ViTacTip), and +64.7% (TacTip). For downstream 3-DoF pose estimation, mixing 50% synthetic with 50% real halves the required real data while maintaining competitive performance. MultiDiffSense alleviates the data-collection bottleneck in tactile sensing and enables scalable, controllable multi-modal dataset generation for robotic applications.", "AI": {"tldr": "多模态触觉图像生成，基于物体形状和接触姿态条件下的扩散模型。", "motivation": "获取对齐的视触觉数据集既费时又昂贵，合成方法通常为单模态限制了跨模态学习。为此，提出了一种统一的扩散模型MultiDiffSense以解决这些问题。", "method": "使用基于CAD的深度图和结构化提示进行双条件训练，生成适用于多个视觉触觉传感器的图像，实现可控且物理一致的多模态合成。", "result": "在8个物体上的评估显示，与Pix2Pix cGAN基线相比，在SSIM指标上分别提高了36.3%，134.6%和64.7%。混合50%合成数据可以减少对真实数据的需求同时保持竞争性能。", "conclusion": "MultiDiffSense缓解了触觉传感中的数据收集瓶颈，并为机器人应用提供可扩展、可控的多模态数据集生成方法。"}}
{"id": "2602.19346", "pdf": "https://arxiv.org/pdf/2602.19346", "abs": "https://arxiv.org/abs/2602.19346", "authors": ["Erik Garcia Oyono", "Jialin Lin", "Dandan Zhang"], "title": "Design and Control of Modular Magnetic Millirobots for Multimodal Locomotion and Shape Reconfiguration", "categories": ["cs.RO", "eess.SY"], "comment": "Accepted by 2026 ICRA", "summary": "Modular small-scale robots offer the potential for on-demand assembly and disassembly, enabling task-specific adaptation in dynamic and constrained environments. However, existing modular magnetic platforms often depend on workspace collisions for reconfiguration, employ bulky three-dimensional electromagnetic systems, and lack robust single-module control, which limits their applicability in biomedical settings. In this work, we present a modular magnetic millirobotic platform comprising three cube-shaped modules with embedded permanent magnets, each designed for a distinct functional role: a free module that supports self-assembly and reconfiguration, a fixed module that enables flip-and-walk locomotion, and a gripper module for cargo manipulation. Locomotion and reconfiguration are actuated by programmable combinations of time-varying two-dimensional uniform and gradient magnetic field inputs. Experiments demonstrate closed-loop navigation using real-time vision feedback and A* path planning, establishing robust single-module control capabilities. Beyond locomotion, the system achieves self-assembly, multimodal transformations, and disassembly at low field strengths. Chain-to-gripper transformations succeeded in 90% of trials, while chain-to-square transformations were less consistent, underscoring the role of module geometry in reconfiguration reliability. These results establish a versatile modular robotic platform capable of multimodal behavior and robust control, suggesting a promising pathway toward scalable and adaptive task execution in confined environments.", "AI": {"tldr": "本文设计了一种模块化磁性毫米机器人平台，该平台具有自组装、重构和多模式运动的能力。", "motivation": "目前的模块化磁性平台依赖于工作空间碰撞进行重构，使用笨重的三维电磁系统，并且缺乏单个模块的稳健控制能力。因此，在生物医学环境中应用受到限制。", "method": "提出了一种由三个立方体模块组成的平台，每个模块都嵌入了永久磁铁并设计有独特的功能：自由模块支持自组装和重构；固定模块允许翻转行走运动；抓取器模块用于货物操纵。通过编程组合二维均匀磁场和梯度磁场输入来实现运动和重构。", "result": "实验展示出闭环导航使用实时视觉反馈和A*路径规划，证明了单个模块的稳健控制能力。该系统在低场强下实现了自组装、多模式转换和拆卸，链到抓取器的变换成功率为90%，而链到正方形的变化则不太一致。", "conclusion": "本研究建立了一种多功能模块化机器人平台，具有多模态行为和稳健控制能力，并为在受限环境中实现可扩展和自适应的任务执行提供了有前途的道路。"}}
{"id": "2602.19345", "pdf": "https://arxiv.org/pdf/2602.19345", "abs": "https://arxiv.org/abs/2602.19345", "authors": ["Egor Denisov", "Svetlana Glazyrina", "Maksim Kryzhanovskiy", "Roman Ischenko"], "title": "Smooth Gate Functions for Soft Advantage Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has significantly advanced the training of large language models and enhanced their reasoning capabilities, while it remains susceptible to instability due to the use of hard clipping. Soft Adaptive Policy Optimization (SAPO) addresses this limitation by replacing clipping with a smooth sigmoid-based gate function, which leads to more stable updates. We have decided to push this theory further and investigate the impact of different gate functions on both training stability and final model performance. We formalize the key properties that admissible gates should satisfy and identify several families of such functions for empirical evaluation. This paper presents an analysis of our findings based on experiments conducted with the Qwen2.5-7B-Instruct model on mathematical reasoning tasks. These results provide practical guidance for designing smoother and more robust policy optimization objectives for large language model training.", "AI": {"tldr": "研究不同门函数对大语言模型训练稳定性和最终性能的影响。", "motivation": "解决GRPO中硬裁剪导致的不稳定性问题，探索更平滑且稳健的策略优化目标设计方法。", "method": "通过实验分析多种门函数在Qwen2.5-7B-Instruct模型上的数学推理任务表现，确定满足关键属性的可行门函数家族。", "result": "实验结果表明，不同门函数对训练稳定性和最终性能有显著影响，并提供了设计更平滑和稳健策略优化目标的实际指导。", "conclusion": "基于研究发现，提出了使用合适门函数改善大语言模型训练稳定性与性能的有效方法。"}}
{"id": "2602.19341", "pdf": "https://arxiv.org/pdf/2602.19341", "abs": "https://arxiv.org/abs/2602.19341", "authors": ["Xinling Li", "Gioele Zardini"], "title": "Where Should Robotaxis Operate? Strategic Network Design for Autonomous Mobility-on-Demand", "categories": ["cs.ET", "eess.SY"], "comment": null, "summary": "The emergence of Autonomous Mobility-on-Demand (AMoD) services creates new opportunities to improve the efficiency and reliability of on-demand mobility systems. Unlike human-driven Mobility-on-Demand (MoD), AMoD enables fully centralized fleet control, but it also requires appropriate infrastructure, so that vehicles can operate safely only on a suitably instrumented subnetwork of the roads. Most existing AMoD research focuses on fleet control (matching, rebalancing, ridepooling) on a fixed road network and does not address the joint design of the service network and fleet capacity. In this paper, we formalize this strategic design problem as the Autonomous Mobility-on-Demand Network Design Problem (AMoD-NDP), in which an operator selects an operation subnetwork and routes all passengers, subject to infrastructure and fleet constraints and route-level quality-of-service requirements. We propose a path-based mixed-integer formulation of the AMoD-NDP and develop a column-generation-based algorithm that scales to city-sized networks. The master problem optimizes over a restricted set of paths, while the pricing problem reduces to an elementary shortest path with resource constraints, solved exactly by a tailored label-correcting algorithm. The method provides an explicit certificate of the optimality gap and extends naturally to a robust counterpart under box uncertainty in travel times and demand. Using real-world data from Manhattan, New York City, we show that the framework produces stable and interpretable operation subnetworks, quantifies trade-offs between infrastructure investment and fleet time, and accommodates additional path-level constraints, such as limits on left turns as a proxy for operational risk. These results illustrate how the proposed approach can support strategic planning and policy analysis for future AMoD deployments.", "AI": {"tldr": "论文提出了自动出行需求网络设计问题(AMoD-NDP)，旨在优化自动驾驶出租车运行的子网及其路径规划，以满足服务质量要求。", "motivation": "随着自动出行服务（AMoD）的发展，需要解决如何高效、可靠地部署其基础设施和车辆的问题。现有研究大多集中于车队管理而非网络设计，本文填补了这一空白。", "method": "论文提出了一种基于路径的混合整数模型来表述AMoD-NDP，并开发了一个列生成算法来优化城市规模网络下的子网及路线选择。", "result": "通过纽约曼哈顿的真实数据验证，该方法可以产生稳定且可解释的操作子网络，量化基础设施投资与车队时间之间的权衡，支持未来AMoD的部署规划和政策分析。", "conclusion": "所提出的方法能够为未来的自动出行需求系统的战略规划提供有力的支持。"}}
{"id": "2602.19331", "pdf": "https://arxiv.org/pdf/2602.19331", "abs": "https://arxiv.org/abs/2602.19331", "authors": ["Chaitanya Kapoor", "Alex H. Williams", "Meenakshi Khosla"], "title": "Partial Soft-Matching Distance for Neural Representational Comparison with Partial Unit Correspondence", "categories": ["cs.LG", "cs.NE", "stat.ML"], "comment": null, "summary": "Representational similarity metrics typically force all units to be matched, making them susceptible to noise and outliers common in neural representations. We extend the soft-matching distance to a partial optimal transport setting that allows some neurons to remain unmatched, yielding rotation-sensitive but robust correspondences. This partial soft-matching distance provides theoretical advantages -- relaxing strict mass conservation while maintaining interpretable transport costs -- and practical benefits through efficient neuron ranking in terms of cross-network alignment without costly iterative recomputation. In simulations, it preserves correct matches under outliers and reliably selects the correct model in noise-corrupted identification tasks. On fMRI data, it automatically excludes low-reliability voxels and produces voxel rankings by alignment quality that closely match computationally expensive brute-force approaches. It achieves higher alignment precision across homologous brain areas than standard soft-matching, which is forced to match all units regardless of quality. In deep networks, highly matched units exhibit similar maximally exciting images, while unmatched units show divergent patterns. This ability to partition by match quality enables focused analyses, e.g., testing whether networks have privileged axes even within their most aligned subpopulations. Overall, partial soft-matching provides a principled and practical method for representational comparison under partial correspondence.", "AI": {"tldr": "本文提出了一种部分软匹配距离，用于神经表示的对比分析，并允许某些单元不进行匹配。", "motivation": "传统的相似度衡量方法强制所有单位进行匹配，这使得它们容易受到噪声和异常值的影响。因此，本文提出了一个部分最优传输设置下的软匹配距离，以增强鲁棒性并减少错误影响。", "method": "通过扩展软匹配距离至部分最优传输框架，允许某些神经元保持未配对状态，从而在保持可解释运输成本的同时放松了严格的质量守恒要求。这种方法提供了一种高效的方法来根据跨网络对齐质量排名神经元，无需昂贵的迭代重新计算。", "result": "实验中，该方法能够在存在异常值时保留正确的匹配，并可靠地选择噪声污染识别任务中的正确模型。在fMRI数据上自动排除低可靠性体素并生成与高成本暴力法接近的质量对齐体素排名。此外，在深度网络中，高度匹配的单元表现出相似的最大激发图像，而非匹配的单元则显示出不同的模式。", "conclusion": "部分软匹配提供了一种基于部分对应性的神经表示比较原则性且实用的方法，能够在有噪声的数据集中提高准确性并支持有针对性的研究分析。"}}
{"id": "2602.19328", "pdf": "https://arxiv.org/pdf/2602.19328", "abs": "https://arxiv.org/abs/2602.19328", "authors": ["Bhaskar DasGupta", "Katie Kruzan"], "title": "On Identifying Critical Network Edges via Analyzing Changes in Shapes (Curvatures)", "categories": ["cs.DS", "cs.CC"], "comment": "under submission for publication", "summary": "In recent years extensions of manifold Ricci curvature to discrete combinatorial objects such as graphs and hypergraphs (popularly called as \"network shapes\"), have found a plethora of applications in a wide spectrum of research areas ranging over metabolic systems, transcriptional regulatory networks, protein-protein-interaction networks, social networks and brain networks to deep learning models and quantum computing but, in contrast, they have been looked at by relatively fewer researchers in the algorithms and computational complexity community. As an attempt to bring these network Ricci-curvature related problems under the lens of computational complexity and foster further inter-disciplinary interactions, we provide a formal framework for studying algorithmic and computational complexity issues for detecting critical edges in an undirected graph using Ollivier-Ricci curvatures and provide several algorithmic and inapproximability results for problems in this framework. Our results show some interesting connections between the exact perfect matching and perfect matching blocker problems for bipartite graphs and our problems.", "AI": {"tldr": "本文旨在通过奥利维尔里奇曲率来检测无向图中的关键边，并研究其算法和计算复杂性。", "motivation": "将网络里的几何性质引入到算法及计算复杂度领域，促进跨学科合作与理解。", "method": "提出基于Ollivier-Ricci曲率的框架来检测网络中关键边，并提供多个算法结果及近似不可解问题的结果。", "result": "发现了完美匹配和完美匹配阻挡器问题之间的有趣联系，并提供了这些问题的具体算法解决方案。", "conclusion": "研究证明了在网络形状分析中，使用奥利维尔里奇曲率来检测关键网络边缘的问题具有重要的理论意义与实际应用价值。"}}
{"id": "2602.19327", "pdf": "https://arxiv.org/pdf/2602.19327", "abs": "https://arxiv.org/abs/2602.19327", "authors": ["Svetlana Glazyrina", "Maksim Kryzhanovskiy", "Roman Ischenko"], "title": "Soft Sequence Policy Optimization: Bridging GMPO and SAPO", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.", "AI": {"tldr": "本文提出了一种新的策略优化方法Soft Sequence Policy Optimization，旨在通过软门控函数在序列重要性权重中结合标记级概率比来促进有效的策略探索并保持训练稳定性。", "motivation": "为了提高大语言模型的对齐效果，研究者们聚焦于基于组相对策略优化（GRPO）的新政策优化方法。两个主要方向是：改进序列级别的采样权重以更好地与任务中的序列级奖励一致；以及寻求PPO样式剪辑之外的替代方案，避免训练信号丢失和熵崩溃。", "method": "本文引入了Soft Sequence Policy Optimization，它在序列重要性加权中使用软门控函数来处理标记级别概率比。这种新方法结合了先前工作的优势，即SAPO中的Scopic目标改革以及GMPO的令牌级比率剪辑。", "result": "实验结果表明，所提出的Soft Sequence Policy Optimization能够有效地促进策略探索，并且在训练过程中保持稳定性。", "conclusion": "通过引入新的优化目标和软门控机制，本文成功地结合了现有方法的优点，提供了一种既有效又稳定的策略优化方案。"}}
{"id": "2602.19326", "pdf": "https://arxiv.org/pdf/2602.19326", "abs": "https://arxiv.org/abs/2602.19326", "authors": ["Rui Liu", "Steven Jige Quan", "Zhong-Ren Peng", "Zijun Yao", "Han Wang", "Zhengzhang Chen", "Kunpeng Liu", "Yanjie Fu", "Dongjie Wang"], "title": "City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual effort to redraw geospatial layouts, slowing the iterative planning and decision-making procedure. Motivated by recent advances in agentic systems and multimodal reasoning, we formulate urban renewal as a machine-executable task that iteratively modifies existing urban plans represented in structured geospatial formats. More specifically, we represent urban layouts using GeoJSON and decompose natural-language editing instructions into hierarchical geometric intents spanning polygon-, line-, and point-level operations. To coordinate interdependent edits across spatial elements and abstraction levels, we propose a hierarchical agentic framework that jointly performs multi-level planning and execution with explicit propagation of intermediate spatial constraints. We further introduce an iterative execution-validation mechanism that mitigates error accumulation and enforces global spatial consistency during multi-step editing. Extensive experiments across diverse urban editing scenarios demonstrate significant improvements in efficiency, robustness, correctness, and spatial validity over existing baselines.", "AI": {"tldr": "该论文提出了一个基于代理的框架，用于执行复杂的地理空间城市编辑任务。", "motivation": "为了应对交通拥堵和功能性不平衡等挑战，需要更高效的城市更新方式。当前的小规模改动仍需大量手工绘制工作，影响了迭代规划与决策效率。", "method": "论文利用GeoJSON表示城市布局，并将自然语言指令分解为多级几何意图操作。引入层级代理框架实现多层次计划及执行协调，并通过迭代验证机制减少错误积累和保持全局一致性。", "result": "实验结果表明，该方法在效率、鲁棒性、准确性和地理空间有效性方面优于现有基线。", "conclusion": "城市更新需要更智能的技术支持以提高规划与决策的效率。所提出的方法为解决这一问题提供了一种有效方案。"}}
{"id": "2602.19324", "pdf": "https://arxiv.org/pdf/2602.19324", "abs": "https://arxiv.org/abs/2602.19324", "authors": ["Mohammad Tahmid Noor", "Shayan Abrar", "Jannatul Adan Mahi", "Md Parvez Mia", "Asaduzzaman Hridoy", "Samanta Ghosh"], "title": "RetinaVision: XAI-Driven Augmented Regulation for Precise Retinal Disease Classification using deep learning framework", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 15 figures", "summary": "Early and accurate classification of retinal diseases is critical to counter vision loss and for guiding clinical management of retinal diseases. In this study, we proposed a deep learning method for retinal disease classification utilizing optical coherence tomography (OCT) images from the Retinal OCT Image Classification - C8 dataset (comprising 24,000 labeled images spanning eight conditions). Images were resized to 224x224 px and tested on convolutional neural network (CNN) architectures: Xception and InceptionV3. Data augmentation techniques (CutMix, MixUp) were employed to enhance model generalization. Additionally, we applied GradCAM and LIME for interpretability evaluation. We implemented this in a real-world scenario via our web application named RetinaVision. This study found that Xception was the most accurate network (95.25%), followed closely by InceptionV3 (94.82%). These results suggest that deep learning methods allow effective OCT retinal disease classification and highlight the importance of implementing accuracy and interpretability for clinical applications.", "AI": {"tldr": "本文提出了一种利用深度学习框架和光学相干断层扫描图像进行精确视网膜疾病分类的方法。", "motivation": "早期准确地对视网膜疾病进行分类对于防止视力丧失以及指导临床管理至关重要。", "method": "使用Xception和InceptionV3卷积神经网络架构，采用CutMix、MixUp数据增强技术，结合GradCAM和LIME提高模型解释性，并通过RetinaVision应用程序实现实用化。", "result": "实验结果显示，Xception网络准确率为95.25%，InceptionV3为94.82%，表明深度学习方法在视网膜疾病分类中具有有效性。", "conclusion": "研究结果强调了实现精度和解释性对于临床应用的重要性。"}}
{"id": "2602.19323", "pdf": "https://arxiv.org/pdf/2602.19323", "abs": "https://arxiv.org/abs/2602.19323", "authors": ["Yiran Qiao", "Yiren Lu", "Yunlai Zhou", "Rui Yang", "Linlin Hou", "Yu Yin", "Jing Ma"], "title": "DefenseSplat: Enhancing the Robustness of 3D Gaussian Splatting via Frequency-Aware Filtering", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for real-time and high-fidelity 3D reconstruction from posed images. However, recent studies reveal its vulnerability to adversarial corruptions in input views, where imperceptible yet consistent perturbations can drastically degrade rendering quality, increase training and rendering time, and inflate memory usage, even leading to server denial-of-service. In our work, to mitigate this issue, we begin by analyzing the distinct behaviors of adversarial perturbations in the low- and high-frequency components of input images using wavelet transforms. Based on this observation, we design a simple yet effective frequency-aware defense strategy that reconstructs training views by filtering high-frequency noise while preserving low-frequency content. This approach effectively suppresses adversarial artifacts while maintaining the authenticity of the original scene. Notably, it does not significantly impair training on clean data, achieving a desirable trade-off between robustness and performance on clean inputs. Through extensive experiments under a wide range of attack intensities on multiple benchmarks, we demonstrate that our method substantially enhances the robustness of 3DGS without access to clean ground-truth supervision. By highlighting and addressing the overlooked vulnerabilities of 3D Gaussian Splatting, our work paves the way for more robust and secure 3D reconstructions.", "AI": {"tldr": "通过频率感知过滤技术提升3D高斯点阵的鲁棒性。", "motivation": "3D高斯点阵在输入视图受到对抗性干扰时，会降低渲染质量、增加训练和渲染时间以及内存使用量。为解决这一问题，作者提出了一种基于波浪变换分析低频和高频分量的方法来设计有效的防御策略。", "method": "利用频率感知过滤技术，在训练过程中通过滤除高频率噪声同时保留低频率成分的方式进行视图重建，以此抑制对抗性干扰而不影响原始场景的真实性。", "result": "该方法在不同攻击强度的广泛实验中表现出对3D高斯点阵的强大鲁棒性改进，并且不需要清洁的真实地面监督。", "conclusion": "通过突出并解决3D高斯点阵的未被重视的安全漏洞，这项工作为更安全、更有弹性的3D重建奠定了基础。"}}
{"id": "2602.19322", "pdf": "https://arxiv.org/pdf/2602.19322", "abs": "https://arxiv.org/abs/2602.19322", "authors": ["Ashwath Radhachandran", "Vedrana Ivezić", "Shreeram Athreya", "Ronit Anilkumar", "Corey W. Arnold", "William Speier"], "title": "US-JEPA: A Joint Embedding Predictive Architecture for Medical Ultrasound", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Ultrasound (US) imaging poses unique challenges for representation learning due to its inherently noisy acquisition process. The low signal-to-noise ratio and stochastic speckle patterns hinder standard self-supervised learning methods relying on a pixel-level reconstruction objective. Joint-Embedding Predictive Architectures (JEPAs) address this drawback by predicting masked latent representations rather than raw pixels. However, standard approaches depend on hyperparameter-brittle and computationally expensive online teachers updated via exponential moving average. We propose US-JEPA, a self-supervised framework that adopts the Static-teacher Asymmetric Latent Training (SALT) objective. By using a frozen, domain-specific teacher to provide stable latent targets, US-JEPA decouples student-teacher optimization and pushes the student to expand upon the semantic priors of the teacher. In addition, we provide the first rigorous comparison of all publicly available state-of-the-art ultrasound foundation models on UltraBench, a public dataset benchmark spanning multiple organs and pathological conditions. Under linear probing for diverse classification tasks, US-JEPA achieves performance competitive with or superior to domain-specific and universal vision foundation model baselines. Our results demonstrate that masked latent prediction provides a stable and efficient path toward robust ultrasound representations.", "AI": {"tldr": "US-JEPA是一种针对医学超声图像的自监督框架，通过静态教师提供稳定的潜在目标来改进传统方法。", "motivation": "标准自监督学习方法在低信噪比和随机斑点模式下表现不佳。引入联合嵌入预测架构（JEPAs），以解决像素级重建问题，并提出US-JEPA以克服在线教师依赖的局限性。", "method": "采用静态教师异步潜在训练（SALT）目标，通过冻结领域特定教师提供稳定的目标来解耦学生和教师优化过程。进行首个公开可用超声基础模型UltraBench基准对比测试。", "result": "在多种分类任务下，US-JEPA的性能与领域特定及通用视觉基础模型基线相当或更优。结果显示掩码潜在预测为稳健的超声表示提供了一条稳定且高效的路径。", "conclusion": "US-JEPA通过改进教师和学生优化过程，成功实现了稳定的自监督学习方法，并在多种分类任务中表现出色，证明了其有效性。"}}
{"id": "2602.19320", "pdf": "https://arxiv.org/pdf/2602.19320", "abs": "https://arxiv.org/abs/2602.19320", "authors": ["Dongming Jiang", "Yi Li", "Songtao Wei", "Jinxin Yang", "Ayushi Kishore", "Alysa Zhao", "Dingyi Kang", "Xu Hu", "Feng Chen", "Qiannan Li", "Bingzhe Li"], "title": "Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.", "AI": {"tldr": "本文通过结构化分析介绍了代理记忆系统的架构和系统视角，指出了现有评估基准、评价指标及性能问题，并探讨了改进方向。", "motivation": "尽管代理记忆系统在快速发展中取得了显著成就，但其基础仍然脆弱，现有的基准测试往往规模不足，评价标准与语义效用不一致，性能因模型而异，系统级成本被忽视。因此需要深入分析这些问题并找到解决方案。", "method": "首先介绍了基于四种记忆结构的代理记忆系统的简明分类法；随后从基准饱和效应、度量有效性及裁判敏感性、基础架构依赖准确性以及由于维护记忆引入的延迟和吞吐量开销等多个方面分析了当前系统中的关键痛点问题。", "result": "该论文通过将记忆结构与实证限制联系起来，阐明了现有代理记忆系统的性能为何往往低于其理论承诺，并指出了更可靠评估及可扩展系统设计的方向。", "conclusion": "文章总结了目前代理记忆系统面临的挑战，并提出了针对这些问题的具体解决方案和未来研究方向。"}}
