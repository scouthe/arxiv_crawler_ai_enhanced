{"id": "2512.14658", "pdf": "https://arxiv.org/pdf/2512.14658", "abs": "https://arxiv.org/abs/2512.14658", "authors": ["Alban Puech", "Matteo Mazzonelli", "Celia Cintas", "Tamara R. Govindasamy", "Mangaliso Mngomezulu", "Jonas Weiss", "Matteo Baù", "Anna Varbella", "François Mirallès", "Kibaek Kim", "Le Xie", "Hendrik F. Hamann", "Etienne Vos", "Thomas Brunschwiler"], "title": "gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation", "categories": ["cs.LG", "cs.AI", "eess.SY", "math.OC"], "comment": "Main equal contributors: Alban Puech, Matteo Mazzonelli. Other equal contributors: Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss", "summary": "We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.", "AI": {"tldr": "gridfm-datakit-v1是一个Python库，用于生成电力系统负荷流动和最优潮流的多样化且现实的数据集。", "motivation": "现有数据集面临缺乏真实随机负载和网络扰动、局限于可行点以及固定发电成本函数的问题。这些问题限制了机器学习求解器的真实性和泛化能力。", "method": "通过结合全球负载缩放和本地噪声，支持任意N-k拓扑扰动生成多样化且现实的数据集；生成超出操作极限的电力流动样本，并生成具有不同发电机成本的最佳潮流数据。", "result": "比较了OPFData、OPF-Learn、PGLearn和PF$Δ$等方法，显示出了在大规模电网（至多10,000个节点）上高效生成数据的能力。", "conclusion": "gridfm-datakit-v1通过解决现有数据集的问题提供了更真实且多样化的电力系统模拟数据，支持机器学习求解器的训练和测试。"}}
{"id": "2512.14657", "pdf": "https://arxiv.org/pdf/2512.14657", "abs": "https://arxiv.org/abs/2512.14657", "authors": ["Yiwen Zhao", "Jiatong Shi", "Jinchuan Tian", "Yuxun Tang", "Jiarui Hai", "Jionghao Han", "Shinji Watanabe"], "title": "Adapting Speech Language Model to Singing Voice Synthesis", "categories": ["cs.SD"], "comment": "Accepted by NeurIPS 2025 workshop AI for Music", "summary": "Speech Language Models (SLMs) have recently emerged as a unified paradigm for addressing a wide range of speech-related tasks, including text-to-speech (TTS), speech enhancement (SE), and automatic speech recognition (ASR). However, the generalization capability of large-scale pre-trained SLMs remains underexplored. In this work, we adapt a 1.7B parameter TTS pretrained SLM for singing voice synthesis (SVS), using only a 135-hour synthetic singing corpus, ACE-Opencpop. Building upon the ESPNet-SpeechLM, our recipe involves the following procedure: (1) tokenization of music score conditions and singing waveforms, (2) multi-stream language model token prediction, (3) conditional flow matching-based mel-spectrogram generation. (4) a mel-to-wave vocoder. Experimental results demonstrate that our adapted SLM generalizes well to SVS and achieves performance comparable to leading discrete token-based SVS models.", "AI": {"tldr": "本论文将一个预训练的文本转语音模型适应到唱歌声音合成任务中。", "motivation": "探索大规模预训练的语言模型在歌唱声音合成中的泛化能力，以解决当前歌唱声音合成技术的问题。", "method": "采用ESPNet-SpeechLM模型，通过音符和声波的标记化、多流语言模型标记预测、基于条件流动匹配的梅尔光谱生成以及从梅尔到波形的编码器来实现歌唱声音合成。", "result": "实验结果表明该适应后的语言模型在歌唱声音合成中表现出良好的泛化能力，并且性能与现有的基于离散令牌的歌唱声音合成模型相当。", "conclusion": "证明了预训练的语言模型可以有效地适应到歌唱声音合成任务，具有广泛的潜在应用价值。"}}
{"id": "2512.14656", "pdf": "https://arxiv.org/pdf/2512.14656", "abs": "https://arxiv.org/abs/2512.14656", "authors": ["Gabriele Accarino", "Viviana Acquaviva", "Sara Shamekh", "Duncan Watson-Parris", "David Lawrence"], "title": "WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields", "categories": ["physics.ao-ph", "cs.CV", "physics.data-an"], "comment": ":42C40; 65T60; 86A08;ACM Class:I.4.10; I.4.7; I.5.4; I.2.10", "summary": "We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.", "AI": {"tldr": "本文介绍了WaveSim，一种基于小波变换的多尺度相似性度量方法，用于评估天气和气候应用中的空间场。", "motivation": "传统点对点相似性度量缺乏将误差归因于物理尺度或不同模式的方法。通过在小波域内工作并沿独立轴分解信号，WaveSim克服了这些限制，并提供了可解释且丰富的框架来评估复杂字段的相似性。", "method": "利用小波变换将输入场分解为特定规模的小波系数。该度量由三个正交分量组成：幅度、位移和结构。每个分量提供0（无相似性）到1（完美相似性）之间的尺度特异性的相似性得分，这些得分随后在不同尺度下组合以生成总体相似性测量。", "result": "通过合成测试案例评估WaveSim，并应用受控的空间和时间扰动系统地评估其敏感性和预期行为。然后使用地球系统的模型中关键气候变异性模式的物理相关案例研究来展示其适用性。", "conclusion": "与传统的点对点度量相比，WaveSim提供了一个解释性强且具有诊断性的框架来评估复杂字段中的相似性，并允许用户根据特定尺度或组件进行强调。"}}
{"id": "2512.14654", "pdf": "https://arxiv.org/pdf/2512.14654", "abs": "https://arxiv.org/abs/2512.14654", "authors": ["Lihong Wang", "Liangqi Li", "Weiwei Feng", "Jiamin Wu", "Changtao Miao", "Tieru Wu", "Rui Ma", "Bo Zhang", "Zhe Li"], "title": "ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/Leon-LihongWang/ViRC", "summary": "CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.", "AI": {"tldr": "提出了一种名为ViRC的框架，旨在通过引入Reason Chunking机制来增强视觉交错数学CoT，并生成了一个CRUX数据集以支持结构化推理。", "motivation": "现有MLLMs在处理多模态领域特别是数学任务时面临挑战。传统的LLMs只能从单个静态数学图像中进行文本推理，忽略了动态的视觉获取过程。人类通过分步骤验证中间命题的方式来解决数学问题，这符合认知科学中的Miller定律。", "method": "提出了一个ViRC框架以及CRUX数据集，并设计了Instructional SFT、Practice SFT和Strategic RL三种训练策略来增强模型的原因拆分能力。", "result": "通过在多个数学基准上的实验表明，生成的ViRC-7B模型相较于基线模型平均提高了18.8％的表现。", "conclusion": "该框架能够有效地模仿人类专家解决问题的方式，并且在处理多模态数学任务上表现出色。"}}
{"id": "2512.14653", "pdf": "https://arxiv.org/pdf/2512.14653", "abs": "https://arxiv.org/abs/2512.14653", "authors": ["Yiwen Zhao", "Jiatong Shi", "Yuxun Tang", "William Chen", "Shinji Watanabe"], "title": "Robust Training of Singing Voice Synthesis Using Prior and Posterior Uncertainty", "categories": ["cs.SD"], "comment": "Accepted by ASRU 2025", "summary": "Singing voice synthesis (SVS) has seen remarkable advancements in recent years. However, compared to speech and general audio data, publicly available singing datasets remain limited. In practice, this data scarcity often leads to performance degradation in long-tail scenarios, such as imbalanced pitch distributions or rare singing styles. To mitigate these challenges, we propose uncertainty-based optimization to improve the training process of end-to-end SVS models. First, we introduce differentiable data augmentation in the adversarial training, which operates in a sample-wise manner to increase the prior uncertainty. Second, we incorporate a frame-level uncertainty prediction module that estimates the posterior uncertainty, enabling the model to allocate more learning capacity to low-confidence segments. Empirical results on the Opencpop and Ofuton-P, across Chinese and Japanese, demonstrate that our approach improves performance in various perspectives.", "AI": {"tldr": "本文提出了一种基于不确定性优化的方法，以增强歌唱合成模型的训练过程。", "motivation": "由于公开可用的歌唱数据集相对有限，导致了在长尾场景下的性能下降问题，如不平衡的音高分布或罕见的演唱风格。为了解决这个问题，研究者提出了不确定性的方法来改进端到端歌唱合成模型的训练过程。", "method": "本文介绍了两种技术：第一种是在对抗性训练中的可微数据增强，以增加样本级别的先验不确定性；第二种是加入了一个帧级别上的不确定性预测模块，用于估计后验不确定性，让模型能够将更多的学习能力分配给低置信度片段。", "result": "在Opencpop和Ofuton-P数据集上进行了测试，结果表明该方法从多个角度提升了歌唱合成的性能。", "conclusion": "研究证明了基于不确定性的优化技术可以显著改善端到端歌唱合成模型的学习能力和泛化能力，在有限的数据下表现出更优的结果。"}}
{"id": "2512.14652", "pdf": "https://arxiv.org/pdf/2512.14652", "abs": "https://arxiv.org/abs/2512.14652", "authors": ["Pawel Swietojanski", "Xinwei Li", "Mingbin Xu", "Takaaki Hori", "Dogan Can", "Xiaodan Zhuang"], "title": "Segmental Attention Decoding With Long Form Acoustic Encodings", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 1 fig", "summary": "We address the fundamental incompatibility of attention-based encoder-decoder (AED) models with long-form acoustic encodings. AED models trained on segmented utterances learn to encode absolute frame positions by exploiting limited acoustic context beyond segment boundaries, but fail to generalize when decoding long-form segments where these cues vanish. The model loses ability to order acoustic encodings due to permutation invariance of keys and values in cross-attention. We propose four modifications: (1) injecting explicit absolute positional encodings into cross-attention for each decoded segment, (2) long-form training with extended acoustic context to eliminate implicit absolute position encoding, (3) segment concatenation to cover diverse segmentations needed during training, and (4) semantic segmentation to align AED-decoded segments with training segments. We show these modifications close the accuracy gap between continuous and segmented acoustic encodings, enabling auto-regressive use of the attention decoder.", "AI": {"tldr": "提出了四种方法来解决注意力编码器解码器模型与长形式声学编码不兼容的问题，使模型能够更好地处理连续的语音段。", "motivation": "现有的注意力编码器解码器模型在处理分段语音时可以通过绝对帧位置进行学习，但在处理长时间段落时这些线索消失导致性能下降。因此需要改进以解决这一问题。", "method": "提出了四种改进方法：（1）向交叉注意中注入显式的绝对位置编码；（2）使用延长的声学上下文进行长形式训练以消除隐含的位置编码；（3）段拼接以便在训练期间覆盖不同分割；（4）语义分段使解码器生成的片段与训练中的片段对齐。", "result": "这些改进措施能够缩小连续和分段声学编码之间的准确率差距，使得注意力解码器可以进行自回归操作。", "conclusion": "所提出的四种方法成功解决了长形式语音数据在注意力编码器-解码器模型中不兼容的问题，并提高了模型对长时间语音片段的处理能力。"}}
{"id": "2512.14648", "pdf": "https://arxiv.org/pdf/2512.14648", "abs": "https://arxiv.org/abs/2512.14648", "authors": ["Daniel Capellán-Martín", "Abhijeet Parida", "Zhifan Jiang", "Nishad Kulkarni", "Krithika Iyer", "Austin Tapp", "Syed Muhammad Anwar", "María J. Ledesma-Carbayo", "Marius George Linguraru"], "title": "Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble", "categories": ["cs.CV", "eess.IV"], "comment": "12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025", "summary": "Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.", "AI": {"tldr": "提出了一种灵活且适应性强的脑肿瘤分割流程，结合了多种模型和基于病变级别的处理方法，并利用影像组学特征进行肿瘤亚型检测。", "motivation": "多参数磁共振成像（MRI）中脑肿瘤的鲁棒性和泛化分割仍然具有挑战性，因为不同类型的肿瘤差异较大。为了解决这一问题，研究提出了一个灵活、模块化的流程来提高各类脑肿瘤的分割性能。", "method": "该方法通过选择和结合最先进的模型，并在训练前后应用基于病变级别的处理步骤以改进分割表现。利用MRI影像组学特征帮助检测肿瘤亚型，确保了训练数据更为平衡。定制的病变级别性能指标决定每种模型的影响并在后期优化细化预测结果。", "result": "该流程在BraTS测试集上实现了与顶尖算法相当的表现，证明了基于病变级别的处理和模型选择可以产生鲁棒性较强的分割效果。", "conclusion": "研究提出的方法具有临床应用潜力，能够支持肿瘤的量化测量、诊断及预后。"}}
{"id": "2512.14641", "pdf": "https://arxiv.org/pdf/2512.14641", "abs": "https://arxiv.org/abs/2512.14641", "authors": ["Abhinav Choudhry", "Bashab Mazumder", "Lauren Alyssa Marks", "Roqaya Elmenshawy", "Devorah Kletenik", "Sean Mullen", "Rachel F. Adler"], "title": "Creating Opportunities: Co-designing an mHealth App with Older Adults", "categories": ["cs.HC"], "comment": "6 pages, 1 figure, 1 table, submitted to CSCWD 2026", "summary": "We conducted a qualitative co-design study with four adults aged 60+ to gather design insights on a Figma prototype and a generative AI (GenAI) chatbot for an app aimed at providing an AI coach to support older adults' physical activity. The initial design for both incorporates several novel aspects: a curated health knowledge base, personalised responses based on goals and health history, privacy considerations, integration with wearables for physical activity context, as well as dynamic context injection. The study yielded feedback on improving both the proposed user experience in the app and the conversation flow with the chatbot, and it will aid future work aimed at implementing a GenAI-powered health coach for older adults.", "AI": {"tldr": "本论文通过与四名60岁以上的成年人合作，对一款旨在为老年人提供AI教练以支持他们进行体育活动的移动健康应用程序进行了定性共设计研究。", "motivation": "为了了解如何更好地设计和优化一款面向老年人群体的移动健康应用及其聊天机器人功能，从而提高其使用体验和支持效果。", "method": "与四名60岁以上的成年人合作开展了一项定性的共设计研究，并对Figma原型和生成式AI（GenAI）聊天机器人的设计方案进行了探讨。", "result": "获得了关于如何改进应用程序中的用户体验以及聊天机器人的对话流程的反馈，这些发现将有助于未来的实施工作。", "conclusion": "通过与老年人的合作研究，可以更好地了解他们的需求并改善设计，从而为他们提供更有效的健康支持。"}}
{"id": "2512.14640", "pdf": "https://arxiv.org/pdf/2512.14640", "abs": "https://arxiv.org/abs/2512.14640", "authors": ["Rao Muhammad Umer", "Daniel Sens", "Jonathan Noll", "Christian Matek", "Lukas Wolfseher", "Rainer Spang", "Ralf Huss", "Johannes Raffler", "Sarah Reinke", "Wolfram Klapper", "Katja Steiger", "Kristina Schwamborn", "Carsten Marr"], "title": "A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.", "AI": {"tldr": "本论文通过基准测试评估了五种公开的病理基础模型在淋巴瘤亚型分类中的性能。", "motivation": "准确及时地诊断淋巴瘤对于指导癌症治疗至关重要。现有的标准诊断方法依赖于多种昂贵且耗时的技术，而深度学习可以从常规HE染色切片中提取诊断信息来辅助病理学家。", "method": "论文使用了四个常见的淋巴瘤亚型和健康对照组织的多中心数据集，评估了五种公开的病理基础模型结合注意力机制和Transformer聚合器的方法。", "result": "在内部测试集中，所有模型的平衡准确率超过80%，不同放大倍数下性能相似。但在外部测试集上，性能显著下降到约60%左右。", "conclusion": "研究揭示了深度学习方法在淋巴瘤亚型分类中的潜力及面临的挑战，并建议未来需要更大规模的研究来覆盖更多的罕见淋巴瘤类型。"}}
{"id": "2512.14639", "pdf": "https://arxiv.org/pdf/2512.14639", "abs": "https://arxiv.org/abs/2512.14639", "authors": ["Fei Wu", "Marcel Dreier", "Nora Gourmelon", "Sebastian Wind", "Jianlin Zhang", "Thorsten Seehaus", "Matthias Braun", "Andreas Maier", "Vincent Christlein"], "title": "AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation", "categories": ["cs.CV"], "comment": "ef:IEEE Transactions on Geoscience and Remote Sensing (2025)", "summary": "The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.", "AI": {"tldr": "AMD-HookNet++ 提出了一种用于冰川分割和剥离前沿界定的混合 CNN-Transformer 特征增强方法。", "motivation": "为了更有效地监测冰川条件，需要持续估算冰川剥离前沿的位置变化。传统的纯 CNN 方法虽然能捕捉到低级细节，但在保持长距离依赖方面存在局限性。", "method": "提出了一种基于混合 CNN-Transformer 的结构，其中 Transformer 分支用于捕获长范围依赖关系并提供全局上下文信息，而 CNN 分支则保留局部细节。通过增强的时空通道注意模块来加强连接的混合特征表示，并采用像素到像素对比深度监督以优化模型。", "result": "在 CaFFe 数据集上的实验表明，AMD-HookNet++ 达到了新的最佳状态，IoU 为 78.2%，HD95 为 1,318 米，并保持了竞争力的 MDE 367 米。此外，模型产生的剥离前沿界定更加平滑。", "conclusion": "AMD-HookNet++ 成功解决了传统方法中存在的问题，提高了冰川分割和剥离前沿界定的质量。"}}
{"id": "2512.14629", "pdf": "https://arxiv.org/pdf/2512.14629", "abs": "https://arxiv.org/abs/2512.14629", "authors": ["Yash Vishe", "Eric Xue", "Xunyi Jiang", "Zachary Novack", "Junda Wu", "Julian McAuley", "Xin Xu"], "title": "MuseCPBench: an Empirical Study of Music Editing Methods through Music Context Preservation", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Music editing plays a vital role in modern music production, with applications in film, broadcasting, and game development. Recent advances in music generation models have enabled diverse editing tasks such as timbre transfer, instrument substitution, and genre transformation. However, many existing works overlook the evaluation of their ability to preserve musical facets that should remain unchanged during editing a property we define as Music Context Preservation (MCP). While some studies do consider MCP, they adopt inconsistent evaluation protocols and metrics, leading to unreliable and unfair comparisons. To address this gap, we introduce the first MCP evaluation benchmark, MuseCPBench, which covers four categories of musical facets and enables comprehensive comparisons across five representative music editing baselines. Through systematic analysis along musical facets, methods, and models, we identify consistent preservation gaps in current music editing methods and provide insightful explanations. We hope our findings offer practical guidance for developing more effective and reliable music editing strategies with strong MCP capability", "AI": {"tldr": "本文提出了MuseCPBench，一个用于评估音乐编辑方法中音乐上下文保存能力的基准。", "motivation": "现有的音乐生成模型虽然可以执行多种编辑任务，但往往忽视了它们在保持某些应保持不变的音乐属性方面的能力，即音乐上下文保存（MCP）。这些研究采用不一致的评价协议和指标，导致不公平比较。因此本文希望通过建立一个基准来解决这个问题。", "method": "本文设计了一个涵盖四种音乐特性的MuseCPBench，并通过系统性分析不同方法与模型下音乐编辑的效果，识别当前方法中存在的保存差距。", "result": "通过对五种代表性基线的全面对比，本研究发现现有音乐编辑方法在保持某些特定特性方面存在一致的不足，提供了有洞察力的解释。", "conclusion": "希望该基准和分析结果能为开发具有更强MCP能力的有效可靠音乐编辑策略提供实际指导。"}}
{"id": "2512.14621", "pdf": "https://arxiv.org/pdf/2512.14621", "abs": "https://arxiv.org/abs/2512.14621", "authors": ["Zhenghao Zhao", "Haoxuan Wang", "Kai Wang", "Yuzhang Shang", "Yuan Hong", "Yan Yan"], "title": "Distill Video Datasets into Images", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.", "AI": {"tldr": "将视频数据集浓缩成信息丰富的图像帧，以实现模型训练效果与完整视频数据集相当。", "motivation": "现有视频数据集蒸馏方法在处理视频时间维度时遇到挑战，导致性能不佳。希望通过新的框架解决这一问题。", "method": "提出Single-Frame Video Set Distillation (SFVD) 方法，通过区分插值将每类视频浓缩成高度信息丰富的单帧，并将其转换为视频序列进行匹配。", "result": "在多个基准测试中表现优于先前方法，MiniUCF上提升可达5.3%。", "conclusion": "该框架提供了一种更有效的视频数据集蒸馏解决方案。"}}
{"id": "2512.14620", "pdf": "https://arxiv.org/pdf/2512.14620", "abs": "https://arxiv.org/abs/2512.14620", "authors": ["Atsuyuki Miyai", "Shota Onohara", "Jeonghun Baek", "Kiyoharu Aizawa"], "title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project page: https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/", "summary": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.", "AI": {"tldr": "介绍了一种基于图像的日语多学科跨模态理解基准JMMMU-Pro及其构建方法Vibe Benchmark Construction。", "motivation": "旨在通过创建一个高质量的、可广泛应用于不同背景和布局设计的评估工具，来更严格地评价大规模语言模型的日语能力，并指导开源社区未来的努力方向。", "method": "利用图像生成模型Nano Banana Pro产生候选视觉问题，然后由人工验证输出并进行必要调整以确保质量，构建了一个综合了视觉与文本理解需求的新基准。", "result": "实验表明，所有开源的大规模语言模型在JMMMU-Pro上表现不佳，强调了该基准的重要性。", "conclusion": "提出了一个严格的评估工具JMMMU-Pro用于测试大规模语言模型的日语能力，并提出了一种高效的图像基视觉问答基准构建指南。"}}
{"id": "2512.14617", "pdf": "https://arxiv.org/pdf/2512.14617", "abs": "https://arxiv.org/abs/2512.14617", "authors": ["Alessandro Trapasso", "Luca Iocchi", "Fabio Patrizi"], "title": "Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 32 figures, includes appendix", "summary": "Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.", "AI": {"tldr": "提出了一种新型的基于模型的强化学习算法QR-MAX，用于处理非马尔可夫奖励决策过程（NMRDPs）中的离散动作问题。", "motivation": "许多实际的任务成功取决于整个系统的历史而非到达特定状态。现有的Markovian强化学习方法在这种情况下并不适用。因此，需要一种新型的基于模型的学习算法来解决此类任务的优化和样本效率问题。", "method": "QR-MAX通过将马尔可夫转移学习与非马尔科夫奖励处理分离，并利用奖励机器进行因子化。该方法适用于离散动作NMRDPs并提供了PAC收敛到ε-最优策略的能力，同时具有多项式样本复杂度。对于连续状态空间，则采用Bucket-QR-MAX通过SimHash技术对连续状态进行了离散化。", "result": "在实验中比较了QR-MAX及其变体与现代的最先进的基于模型的强化学习方法，并发现在处理越来越复杂的环境时，该算法具有显著提高的样本效率和找到最优策略的能力。", "conclusion": "通过引入QR-MAX及其连续状态空间扩展Bucket-QR-MAX，解决了非马尔科夫奖励决策过程中存在的优化和样本复杂度问题，实现了在多项式时间内的PAC收敛性，并展示了优于现有方法的优势。"}}
{"id": "2512.14614", "pdf": "https://arxiv.org/pdf/2512.14614", "abs": "https://arxiv.org/abs/2512.14614", "authors": ["Wenqiang Sun", "Haiyu Zhang", "Haoyuan Wang", "Junta Wu", "Zehan Wang", "Zhenwei Wang", "Yunhong Wang", "Jun Zhang", "Tengfei Wang", "Chunchao Guo"], "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "categories": ["cs.CV", "cs.GR"], "comment": "project page: https://3d-models.hunyuan.tencent.com/world/, demo: https://3d.hunyuan.tencent.com/sceneTo3D", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "AI": {"tldr": "本文提出了WorldPlay，一种实时交互式世界建模的流视频扩散模型，解决了当前方法中速度与内存之间的权衡问题。", "motivation": "现有方法在实时交互式世界建模中的速度和内存之间存在权衡问题，本文旨在通过新的创新技术来解决这些问题，以实现实时、具有长期几何一致性的建模。", "method": "WorldPlay采用了三方面的关键技术：1）使用双重动作表示使模型能够根据用户的键盘和鼠标输入进行稳健的动作控制；2）利用重建上下文记忆从过去的帧中动态重组上下文，并通过时间重帧保持重要但久远的帧可访问，从而缓解内存衰减；3）提出了一种新颖的记忆感知模型蒸馏方法——上下文强制。", "result": "实验结果表明，WorldPlay能够在24 FPS下生成长时间跨度的720p流视频，并且具有更高的几何一致性，在多样化的场景中表现出强大的泛化能力。", "conclusion": "通过这些创新技术，WorldPlay实现了在实时速度和长期几何一致性的平衡，超越了现有的方法。"}}
{"id": "2512.14613", "pdf": "https://arxiv.org/pdf/2512.14613", "abs": "https://arxiv.org/abs/2512.14613", "authors": ["Cristiano Welter", "Kleinner Farias"], "title": "MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development", "categories": ["cs.SE", "cs.ET", "cs.HC"], "comment": "20 pages, 23 figures, 4 tables", "summary": "The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.", "AI": {"tldr": "提出了MoT模型驱动的低代码方法，简化了云物联网应用开发。", "motivation": "现有云物联网（CoT）应用程序开发存在技术门槛高和缺乏标准化模型驱动的方法的问题。此研究旨在降低这些障碍，提高自动化、互操作性和效率。", "method": "引入了一种基于模型的方法MoT，并设计了一个用于IoT和云计算服务的自定义UML配置文件。通过案例研究和技术接受度模型（TAM）问卷评估了该方法的有效性。", "result": "结果显示，MoT能够简化云物联网应用开发和部署过程，用户认为其易于使用且具有高效率，即使是那些拥有有限IoT经验的人也能轻松上手。", "conclusion": "研究表明，通过降低进入门槛并促进自动化，MoT为CoT应用程序的开发提供了一种有前景的方法。这代表了向更用户友好框架迈出的重要一步，有助于广泛采用云物联网技术。"}}
{"id": "2512.14602", "pdf": "https://arxiv.org/pdf/2512.14602", "abs": "https://arxiv.org/abs/2512.14602", "authors": ["Lukáš Samuel Marták", "Patricia Hu", "Gerhard Widmer"], "title": "Sound and Music Biases in Deep Music Transcription Models: A Systematic Analysis", "categories": ["cs.SD", "cs.LG"], "comment": "pre-print of the upcoming EURASIP JASM journal article", "summary": "Automatic Music Transcription (AMT) -- the task of converting music audio into note representations -- has seen rapid progress, driven largely by deep learning systems. Due to the limited availability of richly annotated music datasets, much of the progress in AMT has been concentrated on classical piano music, and even a few very specific datasets. Whether these systems can generalize effectively to other musical contexts remains an open question. Complementing recent studies on distribution shifts in sound (e.g., recording conditions), in this work we investigate the musical dimension -- specifically, variations in genre, dynamics, and polyphony levels. To this end, we introduce the MDS corpus, comprising three distinct subsets -- (1) Genre, (2) Random, and (3) MAEtest -- to emulate different axes of distribution shift. We evaluate the performance of several state-of-the-art AMT systems on the MDS corpus using both traditional information-retrieval and musically-informed performance metrics. Our extensive evaluation isolates and exposes varying degrees of performance degradation under specific distribution shifts. In particular, we measure a note-level F1 performance drop of 20 percentage points due to sound, and 14 due to genre. Generally, we find that dynamics estimation proves more vulnerable to musical variation than onset prediction. Musically informed evaluation metrics, particularly those capturing harmonic structure, help identify potential contributing factors. Furthermore, experiments with randomly generated, non-musical sequences reveal clear limitations in system performance under extreme musical distribution shifts. Altogether, these findings offer new evidence of the persistent impact of the Corpus Bias problem in deep AMT systems.", "AI": {"tldr": "这项研究通过引入MDS语料库，评估了自动音乐转录系统在不同音乐风格、动态和复调水平下的性能。", "motivation": "鉴于自动音乐转录系统的快速进展主要集中在古典钢琴音乐上，且数据集有限，作者希望通过引入MDS语料库来探讨这些系统能否有效地适应不同的音乐背景。", "method": "通过创建三个不同子集的MDS语料库（1）风格、（2）随机和（3）MAEtest，研究团队使用传统的信息检索以及音乐导向性能指标评估了多个最先进的自动音乐转录系统的性能。", "result": "实验显示，在声音变化下注解准确率下降20个百分点；在类型变化下降低14个百分点。动态估计比音符预测对音乐变化更为敏感。", "conclusion": "研究表明，深度自动音乐转录系统仍然受到数据集偏见的影响，并且在面对极端的音乐分布移位时表现出明显的局限性。"}}
{"id": "2512.14601", "pdf": "https://arxiv.org/pdf/2512.14601", "abs": "https://arxiv.org/abs/2512.14601", "authors": ["Zhaolun Li", "Jichang Li", "Yinqi Cai", "Junye Chen", "Xiaonan Luo", "Guanbin Li", "Rushi Lan"], "title": "FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.", "AI": {"tldr": "FakeRadar 是一种新型的深度伪造视频检测框架，旨在解决跨领域泛化问题。", "motivation": "现有的检测方法依赖于特定操作的线索，在已知的伪造类型上表现良好，但在面对新的篡改技术时存在严重限制。为了克服这一挑战，研究人员提出了一种新方法来模拟未知的伪造模式。", "method": "FakeRadar 引入了Forgery Outlier Probing，并设计了Outlier-Guided Tri-Training 方法来优化检测器，使其能够区分真实视频、伪造视频和异常样本。", "result": "实验表明，FakeRadar 在多个基准数据集上的性能优于现有方法，在跨域评估中特别有效。", "conclusion": "通过创新的 Forgery Outlier Probing 和Outlier-Guided Tri-Training 方法，FakeRadar 能够更有效地检测未知的深度伪造视频。"}}
{"id": "2512.14595", "pdf": "https://arxiv.org/pdf/2512.14595", "abs": "https://arxiv.org/abs/2512.14595", "authors": ["Mengyu Li", "Xingcheng Zhou", "Guang Chen", "Alois Knoll", "Hu Cao"], "title": "TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.", "AI": {"tldr": "本文提出了一个针对智能交通系统的事件驱动多目标跟踪数据集和基准线。", "motivation": "现有基于帧的摄像头在暗光和高速运动条件下表现不佳，而事件摄像头具有低延迟、高动态范围和高时间分辨率的特点，可以弥补这些不足。为了填补研究空白，本文创建了一个针对智能交通系统的事件驱动视觉初步试点数据集。", "method": "通过构建一个基于该数据集的跟踪-检测基准线，并采用专门设计的特征提取器来实现性能优化。", "result": "在所建立的数据集和基准线上取得了优异的成绩。", "conclusion": "引入了面向智能交通系统的事件驱动多目标跟踪初步试点数据集，通过建立跟踪-检测基准线并使用特定的设计方法实现了优秀的性能表现。"}}
{"id": "2512.14594", "pdf": "https://arxiv.org/pdf/2512.14594", "abs": "https://arxiv.org/abs/2512.14594", "authors": ["Chenyu Zhao", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Hao Chen"], "title": "LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.", "AI": {"tldr": "提出了一种基于大语言模型的知识增强多模态癌症生存预测模型KEMM。", "motivation": "当前的多模态生存预测方法难以从高维度、冗余的数据中提取区分性特征，且单一的生存随访标签不足以监督复杂任务。为解决这些问题，该论文提出了KEMM模型。", "method": "引入了专家报告和预后背景知识，并使用KECM注意力模块来指导网络聚焦于区分性和与生存相关的特征。", "result": "实验表明，KEMM在五个数据集上的表现优于现有方法。", "conclusion": "提出了KEMM模型，有效解决了多模态癌症生存预测中的挑战性问题。"}}
{"id": "2512.14585", "pdf": "https://arxiv.org/pdf/2512.14585", "abs": "https://arxiv.org/abs/2512.14585", "authors": ["Adarsha Shrestha", "Basanta Pokharel", "Binit Shrestha", "Smriti Adhikari", "Dinesh Gothe"], "title": "Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Nepali, a low-resource language spoken by over 32 million people, continues to face challenges in natural language processing (NLP) due to its complex grammar, agglutinative morphology, and limited availability of high-quality corpora. Most efforts to date have centered on basic encoder architectures; they remain insufficient for Nepali-specific text generation. This study presents a GPT-2-based Nepali language model trained using several training strategies inspired by GPT-3, including optimized learning rate schedules, batch scaling, and architectural refinements. A custom 16k Byte-Pair Encoding (BPE) tokenizer was trained exclusively on Nepali text to ensure more consistent segmentation and improved input representation. The model was pretrained on a combined dataset comprising a 10.75GB cleaned NepBERTa corpus and additional web-scraped Nepali news articles. FlashAttention was integrated to reduce memory usage and stabilize training. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its capability to generate coherent Nepali news-style text.", "AI": {"tldr": "本文使用GPT-2架构训练了针对尼泊尔语的大型语言模型，采用了定制的BPE分词器和多种优化策略。", "motivation": "尼泊尔语作为一种低资源语言，在自然语言处理领域面临着复杂语法、黏着形态学及高质量语料库缺乏的问题。现有的研究多集中在基本编码架构上，但这些方法不足以满足尼泊尔语特定的文本生成需求。", "method": "本文基于GPT-2架构训练了一个针对尼泊尔语的语言模型，并使用了定制的16k BPE分词器、优化的学习率调度策略和批处理规模。预训练数据包括经清洗后的NepBERTa语料库以及额外从网络抓取的尼泊尔新闻文章。", "result": "经过两次完整训练周期后，该模型达到了3.168177的训练损失值、3.081982的验证损失值和21.80的最终困惑度，显示出其生成连贯的尼泊尔语新闻文本的能力。", "conclusion": "通过定制化的方法和技术手段，本文成功开发出一个能够有效处理尼泊尔语的语言模型，为低资源语言的研究提供了新的方向。"}}
{"id": "2512.14576", "pdf": "https://arxiv.org/pdf/2512.14576", "abs": "https://arxiv.org/abs/2512.14576", "authors": ["Ekaterina Artemova", "Laurie Burchell", "Daryna Dementieva", "Shu Okabe", "Mariya Shmatova", "Pedro Ortiz Suarez"], "title": "Low-Resource, High-Impact: Building Corpora for Inclusive Language Technologies", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial is accepted to LREC2026", "summary": "This tutorial (https://tum-nlp.github.io/low-resource-tutorial) is designed for NLP practitioners, researchers, and developers working with multilingual and low-resource languages who seek to create more equitable and socially impactful language technologies. Participants will walk away with a practical toolkit for building end-to-end NLP pipelines for underrepresented languages -- from data collection and web crawling to parallel sentence mining, machine translation, and downstream applications such as text classification and multimodal reasoning. The tutorial presents strategies for tackling the challenges of data scarcity and cultural variance, offering hands-on methods and modeling frameworks. We will focus on fair, reproducible, and community-informed development approaches, grounded in real-world scenarios. We will showcase a diverse set of use cases covering over 10 languages from different language families and geopolitical contexts, including both digitally resource-rich and severely underrepresented languages.", "AI": {"tldr": "构建用于低资源语言的包容性语言技术的语料库", "motivation": "为了创建更公平和有社会影响力的语言技术，帮助处理数据稀缺和文化差异的问题", "method": "介绍从数据收集、网络爬取到平行句子挖掘、机器翻译及下游应用等端到端NLP管道构建策略，并提供实战方法与建模框架", "result": "展示了覆盖超过10种来自不同语系和地缘政治背景的语言的多样化应用场景，包括数字资源丰富和严重被忽视的语言", "conclusion": "强调了公平、可重复且社区导向的发展方式的重要性"}}
{"id": "2512.14574", "pdf": "https://arxiv.org/pdf/2512.14574", "abs": "https://arxiv.org/abs/2512.14574", "authors": ["Mitsuki Watanabe", "Sosuke Amano", "Kiyoharu Aizawa", "Yoko Yamakata"], "title": "FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.", "AI": {"tldr": "构建了一个名为FoodLogAthl-218的真实世界食物图像数据集，用于提高饮食管理应用程序中的食品分类模型的准确性。", "motivation": "现有的食品图像数据集通常依赖于从网络爬取的数据，这些数据与用户实际用餐的照片存在差异。为了改善这一状况并提升饮食管理应用的效果，作者开发了FoodLogAthl-218，旨在提供一个更加贴近真实生活的食物图像集合。", "method": "通过饮食管理应用程序FoodLog Athl收集真实的餐饮记录，构建了一个包含6925张图片、覆盖218个食品类别的数据集，并且为每一张图片提供了丰富的元数据信息。该方法不同于传统数据集的网络爬虫方式，而是从用户上传的照片开始，之后进行标注。", "result": "引入了两个特定于FoodLog的任务：增量微调协议和基于上下文的食物分类任务，使用大型多模态模型（LMMs）评估这些新任务。该数据集已公开发布在指定网址上。", "conclusion": "通过构建真实世界的食物图像数据集FoodLogAthl-218，研究者们提供了更高质量的训练资源给食品分类领域的模型开发工作，并为饮食管理应用的发展奠定了基础。"}}
{"id": "2512.14563", "pdf": "https://arxiv.org/pdf/2512.14563", "abs": "https://arxiv.org/abs/2512.14563", "authors": ["Tejaswani Dash", "Gautam Datla", "Anudeep Vurity", "Tazeem Ahmad", "Mohd Adnan", "Saima Rafi", "Saisha Patro", "Saina Patro"], "title": "Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in IEEE Bigdata 2025- Learning Representations with Limited Supervision", "summary": "Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.", "AI": {"tldr": "本文提出了一种轻量级的混合循环注意力模型（Residual GRU+MHSA）用于心血管疾病的检测。", "motivation": "心血管疾病是全球主要死因之一，需要可靠且高效的预测工具。传统方法依赖手工特征和临床专家知识，而机器学习方法虽然提高了可重复性但难以处理复杂数据。", "method": "本文提出了一种结合残差双向门控循环单元、通道重加权块以及多头自注意力机制的模型，并在UCI心脏病数据集上进行了评估。与传统及现代深度学习基线相比，该方法表现出色。", "result": "该模型达到了0.861的精度和0.908的ROC-AUC值，在所有基准中表现最优。", "conclusion": "结果表明轻量级混合循环注意力架构在临床风险预测中的准确性与效率之间实现了良好平衡，适合资源有限的医疗环境中应用。"}}
{"id": "2512.14562", "pdf": "https://arxiv.org/pdf/2512.14562", "abs": "https://arxiv.org/abs/2512.14562", "authors": ["Tejaswani Dash", "Dinesh Karri", "Anudeep Vurity", "Gautam Datla", "Tazeem Ahmad", "Saima Rafi", "Rohith Tangudu"], "title": "Polypersona: Persona-Grounded LLM for Synthetic Survey Responses", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE Bigdata 2025- LLMs4ALL", "summary": "This paper introduces PolyPersona, a generative framework for synthesizing persona-conditioned survey responses across multiple domains. The framework instruction-tunes compact chat models using parameter-efficient LoRA adapters with 4-bit quantization under a resource-adaptive training setup. A dialogue-based data pipeline explicitly preserves persona cues, ensuring consistent behavioral alignment across generated responses. Using this pipeline, we construct a dataset of 3,568 synthetic survey responses spanning ten domains and 433 distinct personas, enabling controlled instruction tuning and systematic multi-domain evaluation. We evaluate the generated responses using a multi-metric evaluation suite that combines standard text generation metrics, including BLEU, ROUGE, and BERTScore, with survey-specific metrics designed to assess structural coherence, stylistic consistency, and sentiment alignment.Experimental results show that compact models such as TinyLlama 1.1B and Phi-2 achieve performance comparable to larger 7B to 8B baselines, with a highest BLEU score of 0.090 and ROUGE-1 of 0.429. These findings demonstrate that persona-conditioned fine-tuning enables small language models to generate reliable and coherent synthetic survey data. The proposed framework provides an efficient and reproducible approach for survey data generation, supporting scalable evaluation while facilitating bias analysis through transparent and open protocols.", "AI": {"tldr": "介绍了一种生成框架PolyPersona，用于合成带有特定人格条件的问卷答复。", "motivation": "为了在多个领域中创造一致且个性化的虚拟角色行为反应，提高小模型在生成可靠和连贯的人工问卷数据方面的性能。", "method": "使用参数高效LoRA适配器对紧凑型对话模型进行有条件指令调优，并结合4位量化技术，在资源适应性训练环境中合成包含人格线索的多领域调查答复。", "result": "实验表明，小型语言模型如TinyLlama 1.1B和Phi-2在性能上与更大的7B到8B基线相当，BLEU得分为0.090，ROUGE-1为0.429。证明了有条件人格调整使小模型能够生成可靠且连贯的人工问卷数据。", "conclusion": "提出的框架提供了一种高效和可重复的方法来生成调查数据，支持大规模评估，同时通过透明协议进行偏差分析。"}}
{"id": "2512.14560", "pdf": "https://arxiv.org/pdf/2512.14560", "abs": "https://arxiv.org/abs/2512.14560", "authors": ["Xianwei Cao", "Dou Quan", "Shuang Wang", "Ning Huyan", "Wei Wang", "Yunan Li", "Licheng Jiao"], "title": "CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.", "AI": {"tldr": "本文提出了CLNet框架，用于解决基于图像检索的跨视图地理定位问题，通过显式建模不同视角间的对应关系来提高准确度。", "motivation": "现有的方法主要依赖于学习鲁棒的全局表示或隐式的特征对齐，但这些方法往往无法有效模型显式的空间对应关系，这对于精确的地理定位至关重要。CLNet旨在填补这一空白。", "method": "CLNet将视角对准过程分解为三个可学习且互补的模块：神经对应图（NCM），非线性嵌入转换器（NEC）和全局特征再校准（GFR）。这些模块共同捕捉高层次语义与细粒度对齐。", "result": "在四个公开基准测试CVUSA、CVACT、VIGOR和University-1652上的广泛实验表明，所提出的CLNet达到了最先进的性能，并提供了更好的可解释性和泛化能力。", "conclusion": "CLNet通过显式建模不同视角间的空间对应关系，有效提高了基于图像检索的跨视图地理定位任务中的准确性。"}}
{"id": "2512.14556", "pdf": "https://arxiv.org/pdf/2512.14556", "abs": "https://arxiv.org/abs/2512.14556", "authors": ["Sneha Sree C.", "Dattesh Shanbhag", "Sudhanya Chatterjee"], "title": "Test Time Optimized Generalized AI-based Medical Image Registration Method", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.", "AI": {"tldr": "该论文提出了一种新的AI驱动框架，用于跨多种成像模态和解剖区域的三维非刚性配准。", "motivation": "现有技术在参数调优和计算成本方面存在挑战，限制了其临床实用性。深度学习方法虽然有前景但依赖于特定任务的再训练，导致可扩展性和适应性受限。", "method": "该框架采用了一种AI驱动的方法，旨在消除解剖特异或模态特异定制化需求，实现了跨多种成像模式和解剖区域的一般化配准。", "result": "结果表明，所提出方法具备处理不同成像环境的能力，并且简化了临床应用流程。", "conclusion": "本文引入了一种创新的AI驱动框架，克服了传统非刚性图像注册技术中的限制，实现了高效、通用化的跨模态配准。"}}
{"id": "2512.14554", "pdf": "https://arxiv.org/pdf/2512.14554", "abs": "https://arxiv.org/abs/2512.14554", "authors": ["Nguyen Tien Dong", "Minh-Anh Nguyen", "Thanh Dat Hoang", "Nguyen Tuan Ngoc", "Dao Xuan Quang Minh", "Phan Phi Hai", "Nguyen Thi Ngoc Anh", "Dang Van Tu", "Binh Vu"], "title": "VLegal-Bench: Cognitively Grounded Benchmark for Vietnamese Legal Reasoning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled new possibilities for applying artificial intelligence within the legal domain. Nonetheless, the complexity, hierarchical organization, and frequent revisions of Vietnamese legislation pose considerable challenges for evaluating how well these models interpret and utilize legal knowledge. To address this gap, Vietnamese Legal Benchmark (VLegal-Bench) is introduced, the first comprehensive benchmark designed to systematically assess LLMs on Vietnamese legal tasks. Informed by Bloom's cognitive taxonomy, VLegal-Bench encompasses multiple levels of legal understanding through tasks designed to reflect practical usage scenarios. The benchmark comprises 10,450 samples generated through a rigorous annotation pipeline, where legal experts label and cross-validate each instance using our annotation system to ensure every sample is grounded in authoritative legal documents and mirrors real-world legal assistant workflows, including general legal questions and answers, retrieval-augmented generation, multi-step reasoning, and scenario-based problem solving tailored to Vietnamese law. By providing a standardized, transparent, and cognitively informed evaluation framework, VLegal-Bench establishes a solid foundation for assessing LLM performance in Vietnamese legal contexts and supports the development of more reliable, interpretable, and ethically aligned AI-assisted legal systems.", "AI": {"tldr": "提出了VLegal-Bench，这是一个用于评估大型语言模型在越南法律任务上的表现的基准测试。", "motivation": "由于越南海法规则的复杂性、层级结构和频繁修订，现有的大型语言模型难以有效地理解和使用这些规则。因此，需要一个专门针对越南法律任务设计的全面基准来解决这一问题。", "method": "VLegal-Bench根据布卢姆认知分类法包括多个层次的法律理解，并通过严格的注释流程生成了10,450个样本，确保每个样本都基于权威的法律文件并在实际的法律助理工作流程中反映。", "result": "尚未报告具体的结果。该论文主要描述了一个基准测试工具的设计和开发过程，旨在评估大型语言模型在越南法律任务上的表现。", "conclusion": "VLegal-Bench为评估大型语言模型在越南法律环境中的性能提供了标准化、透明且基于认知的框架，并支持了更可靠、可解释性和伦理一致的人工智能辅助法律系统的研发。"}}
{"id": "2512.14550", "pdf": "https://arxiv.org/pdf/2512.14550", "abs": "https://arxiv.org/abs/2512.14550", "authors": ["Zhiwen Yang", "Jiaju Zhang", "Yang Yi", "Jian Liang", "Bingzheng Wei", "Yan Xu"], "title": "TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration", "categories": ["cs.CV"], "comment": "This paper has been accepted by MICCAI 2025", "summary": "Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.", "AI": {"tldr": "提出了一种任务自适应Transformer（TAT），以解决多模态医学图像复原中任务干扰和任务不平衡的问题。", "motivation": "为了同时处理多种不同类型的医学图像复原任务，需要考虑任务之间的干扰以及学习难度的不均衡问题。这些问题可能导致模型性能下降。", "method": "通过引入任务自适应权重生成策略来减少任务间的干扰，并采用任务自适应损失平衡策略动态调整任务损失权重，以解决任务不平衡的问题。", "result": "在PET合成、CT去噪和MRI超分辨率等三个医学图像复原任务中，TAT框架展示了优于现有方法的性能表现。", "conclusion": "该研究通过引入任务自适应机制，有效地解决了多模态医学图像复原中的任务干扰和任务不平衡问题。"}}
{"id": "2512.14549", "pdf": "https://arxiv.org/pdf/2512.14549", "abs": "https://arxiv.org/abs/2512.14549", "authors": ["David Samuel", "Lucas Georges Gabriel Charpentier"], "title": "Dual Language Models: Balancing Training Efficiency and Overfitting Resilience", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal ratio between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal ratio is similar whether targeting autoregressive or masked-diffusion downstream performance.", "AI": {"tldr": "本文提出了结合自回归和掩码扩散训练目标的双语言模型，无需架构修改即可提高性能并减少过拟合。", "motivation": "旨在平衡高效训练与降低过拟合风险之间的矛盾，实现灵活性更强的语言模型。", "method": "通过调整两种训练方式的比例来优化模型，并在不同数据重复设置下评估50个语言模型的表现。", "result": "双目标训练优于单一目标训练，在所有评估的场景中表现最佳；最优比例对于自回归和掩码扩散下游性能而言相似。", "conclusion": "该方法能有效结合两者的优点，提供更强大的语言模型。"}}
{"id": "2512.14542", "pdf": "https://arxiv.org/pdf/2512.14542", "abs": "https://arxiv.org/abs/2512.14542", "authors": ["Yifang Xu", "Benxiang Zhai", "Yunzhuo Sun", "Ming Li", "Yang Li", "Sidan Du"], "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.", "AI": {"tldr": "HiFi-Portrait是一种零样本生成高保真身份保留肖像的方法，通过融合多张面孔特征并利用3D人脸地标进行对齐。", "motivation": "现有方法在使用同一ID的多个参考图像时会产生较低保真的肖像，并且难以精确定制面部属性。该研究旨在解决这些问题，提高身份和面部控制的质量。", "method": "通过引入面部细化器和地标生成器获取细粒度多脸特征及3D感知的人脸地标；设计HiFi-Net融合这些特征并根据地标进行对齐，以提升身份保真度和面部控制能力；构建基于ID的数据集用于训练模型。", "result": "实验结果表明，该方法在人脸相似性和可控制性方面超过了现有最先进的技术。", "conclusion": "提出的方法能有效生成高保真且具有高度自定义性的肖像，并与之前的SDXL工作兼容。"}}
{"id": "2512.14541", "pdf": "https://arxiv.org/pdf/2512.14541", "abs": "https://arxiv.org/abs/2512.14541", "authors": ["Subrata Das", "Archisman Ghosh", "Swaroop Ghosh"], "title": "A Graph-Based Forensic Framework for Inferring Hardware Noise of Cloud Quantum Backend", "categories": ["quant-ph", "cs.ET"], "comment": "11 pages, 5 figures, conference", "summary": "Cloud quantum platforms give users access to many backends with different qubit technologies, coupling layouts, and noise levels. The execution of a circuit, however, depends on internal allocation and routing policies that are not observable to the user. A provider may redirect jobs to more error-prone regions to conserve resources, balance load or for other opaque reasons, causing degradation in fidelity while still presenting stale or averaged calibration data. This lack of transparency creates a security gap: users cannot verify whether their circuits were executed on the hardware for which they were charged. Forensic methods that infer backend behavior from user-visible artifacts are therefore becoming essential. In this work, we introduce a Graph Neural Network (GNN)-based forensic framework that predicts per-qubit and per-qubit link error rates of an unseen backend using only topology information and aggregated features extracted from transpiled circuits. We construct a dataset from several IBM 27-qubit devices, merge static calibration features with dynamic transpilation features and train separate GNN regressors for one- and two-qubit errors. At inference time, the model operates without access to calibration data from the target backend and reconstructs a complete error map from the features available to the user. Our results on the target backend show accurate recovery of backend error rate, with an average mismatch of approximately 22% for single-qubit errors and 18% for qubit-link errors. The model also exhibits strong ranking agreement, with the ordering induced by predicted error values closely matching that of the actual calibration errors, as reflected by high Spearman correlation. The framework consistently identifies weak links and high-noise qubits and remains robust under realistic temporal noise drift.", "AI": {"tldr": "提出了一种基于图神经网络的框架，用于从用户可见的转译电路特征中预测云量子后端的硬件噪声。", "motivation": "在云计算环境中，由于内部分配和路由策略不可见，用户的量子计算任务可能被分配到错误率更高的区域执行。这导致了透明度问题，用户无法验证他们支付的服务是否与实际使用的设备一致。因此需要一种方法来通过用户可观察的特征推断后端行为。", "method": "使用图神经网络框架，结合拓扑信息和从转译电路中提取的动态特性，预测未见过的后端每个量子比特以及连边的错误率。该模型在没有目标后端校准数据的情况下进行推理，并重建出完整的误差分布图。", "result": "实验结果表明，单个量子比特和两量子比特链接的误差恢复精度分别为大约22%和18%，并且预测值与实际校准误差之间的顺序高度一致。该模型还能准确识别高噪声区域，在噪声漂移的情况下表现稳健。", "conclusion": "所提出的基于图神经网络的方法能够有效推断出云量子后端的实际硬件噪声分布，对于提高云计算环境中的透明度和安全性具有重要意义。"}}
{"id": "2512.14540", "pdf": "https://arxiv.org/pdf/2512.14540", "abs": "https://arxiv.org/abs/2512.14540", "authors": ["Andreas Lolos", "Theofilos Christodoulou", "Aris L. Moustakas", "Stergios Christodoulidis", "Maria Vakalopoulou"], "title": "CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning", "categories": ["cs.CV", "cs.AI"], "comment": "24 pages, 12 Figures, 4 Tables", "summary": "In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL", "AI": {"tldr": "提出了一种名为CAPRMIL的新方法，用于计算病理学中的弱监督学习问题。", "motivation": "通过引入一个高效的聚合器无关框架来简化多实例学习（MIL）过程，并减少复杂的相关性学习的需求，以应对WSI的巨大规模和像素级注释的稀缺性。", "method": "CAPRMIL通过将提取到的补丁特征投影到一组全局上下文/形态感知令牌中并使用多头自注意力机制来注入全局上下文。该方法利用了预训练但冻结的补丁编码器，且计算复杂度与包大小线性相关。", "result": "CAPRMIL在多个公开病理学基准测试上达到了顶级的切片级性能，并减少了48%-92.8%的可训练参数数量及降低了52%-99%的推理FLOPs。同时，在GPU内存效率和训练时间方面也表现优异。", "conclusion": "学习丰富的、上下文感知的实例表示在聚合之前是一种有效且可扩展的替代复杂池化技术的方法，适用于整体滑片分析。"}}
{"id": "2512.14536", "pdf": "https://arxiv.org/pdf/2512.14536", "abs": "https://arxiv.org/abs/2512.14536", "authors": ["Yiheng Huang", "Junhong Chen", "Anqi Ning", "Zhanhong Liang", "Nick Michiels", "Luc Claesen", "Wenyin Liu"], "title": "DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.", "AI": {"tldr": "提出了DASP框架，利用时空先验进行夜间单目深度估计。", "motivation": "现有自监督单目深度估计算法在白天表现良好，在夜晚由于低可见度和光照变化性能下降明显。提出一种解决夜间纹理缺失及动态物体模糊区域的方法。", "method": "设计了一个包含对抗分支和自监督分支的DASP框架，其中对抗分支通过四个时空先验学习块（SPLB）提取日间先验，并且包含了利用正交差分技术的时间轴运动变化提取模块（STLM），以及采用局部不对称卷积和全局轴向注意力捕获多尺度结构信息的空间轴向学习模块（ASLM）。自监督分支使用3D一致性投影损失优化三维结构性一致性和日间先验。", "result": "在牛津RobotCar和nuScenes数据集上实验表明，该方法夜间深度估计效果达到了业界领先水平。", "conclusion": "通过结合时空先验学习模块和自监督损失，DASP框架能够有效解决夜间单目深度估计问题。"}}
{"id": "2512.14527", "pdf": "https://arxiv.org/pdf/2512.14527", "abs": "https://arxiv.org/abs/2512.14527", "authors": ["Shreyas Subramanian", "Bala Krishnamoorthy", "Pranav Murthy"], "title": "Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence", "categories": ["cs.AI"], "comment": null, "summary": "Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \\emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.", "AI": {"tldr": "本文提出了一种基于当前损失动态调整学习率的新型调度器GreedyLR，并通过实验验证了其在NLP、CV和LLM任务中的优越性。", "motivation": "尽管优化器已有显著进步，但大多数研究工作仍采用标准的学习率调度策略。作者希望开发出一种更加适应性的学习率调度方法，以提高训练速度和收敛效率。", "method": "提出了一种名为GreedyLR的新型动态学习率调整算法，该算法通过当前损失值的变化来决定如何调整学习率，并提供了理论分析和最优缩放因子F的推导。", "result": "实验结果显示，在NLP、CV和LLM任务中，GreedyLR在准确性、速度和收敛性方面优于多种最先进的调度器。此外，还进行了鲁棒性测试以验证算法的有效性。", "conclusion": "GreedyLR不仅易于实现且计算效率高，并且可以作为训练时的默认调度器使用。"}}
{"id": "2512.14506", "pdf": "https://arxiv.org/pdf/2512.14506", "abs": "https://arxiv.org/abs/2512.14506", "authors": ["Marianne de Heer Kloots", "Paul Boersma", "Willem Zuidema"], "title": "Linguists should learn to love speech-based deep learning models", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Commentary on Futrell, R., & Mahowald, K. arXiv:2501.17047 (in press). How Linguistics Learned to Stop Worrying and Love the Language Models. Behavioural and Brain Sciences", "summary": "Futrell and Mahowald present a useful framework bridging technology-oriented deep learning systems and explanation-oriented linguistic theories. Unfortunately, the target article's focus on generative text-based LLMs fundamentally limits fruitful interactions with linguistics, as many interesting questions on human language fall outside what is captured by written text. We argue that audio-based deep learning models can and should play a crucial role.", "AI": {"tldr": "提出语音基础的深度学习模型在语言研究中的重要作用。", "motivation": "认为现有的基于文本的语言模型忽略了人类语言中许多有趣的问题，建议采用音频为基础的深度学习模型进行更全面的研究。", "method": "讨论并推荐使用语音数据训练和分析的深度学习方法。", "result": "展示了语音基础的深度学习模型在理解人类语言方面的潜力。", "conclusion": "强调了基于语音的深度学习模型对语言学研究的重要性和必要性。"}}
{"id": "2512.14499", "pdf": "https://arxiv.org/pdf/2512.14499", "abs": "https://arxiv.org/abs/2512.14499", "authors": ["Jia Guo", "Jiawei Du", "Shengzhu Yang", "Shuai Lu", "Wenquan Cheng", "Kaiwen Zhang", "Yihua Sun", "Chuhong Yang", "Weihang Zhang", "Fang Chen", "Yilan Wu", "Lie Ju", "Guochen Ning", "Longfei Ma", "Huiping Yao", "Jinyuan Wang", "Peilun Shi", "Yukun Zhou", "Jie Xu", "Pearse A. Keane", "Hanruo Liu", "Hongen Liao", "Ningli Wang", "Huiqi Li"], "title": "Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.", "AI": {"tldr": "构建了一个名为ReVision的视网膜基础模型，该模型直接从大型远程医疗项目中的真实医学实践中学习。", "motivation": "当前的视网膜基础模型受限于经过精心整理的研究数据集，缺乏真实的临床环境，并且每个应用程序都需要大量的特定任务优化，限制了其在资源有限情况下的部署效率。作者希望通过使用实际临床档案来克服这些障碍。", "method": "ReVision从跨越中国162个医疗机构的十年远程医疗项目中积累的485,980张彩色眼底照片及其相应的诊断报告之间自然对齐的数据学习，无需进一步注释即可直接提取临床原生智能。", "result": "在广泛的评估中，ReVision展示了部署效率，仅需最少本地资源。它实现了无监督疾病的检测，平均AUROC为12个公共基准的0.946和3个独立临床队列的0.952，并且与完全微调的替代方案相比，在需要更少可训练参数和标记示例的情况下达到了相同的性能。", "conclusion": "研究表明直接从临床档案中提取临床原生智能可以构建适合各种资源受限环境的医学AI系统。"}}
{"id": "2512.14491", "pdf": "https://arxiv.org/pdf/2512.14491", "abs": "https://arxiv.org/abs/2512.14491", "authors": ["Cheng-Han Lu", "Pei-Hsuan Tsai"], "title": "Sparse Multi-Modal Transformer with Masking for Alzheimer's Disease Classification", "categories": ["cs.AI"], "comment": "8 pages, 7 figures", "summary": "Transformer-based multi-modal intelligent systems often suffer from high computational and energy costs due to dense self-attention, limiting their scalability under resource constraints. This paper presents SMMT, a sparse multi-modal transformer architecture designed to improve efficiency and robustness. Building upon a cascaded multi-modal transformer framework, SMMT introduces cluster-based sparse attention to achieve near linear computational complexity and modality-wise masking to enhance robustness against incomplete inputs. The architecture is evaluated using Alzheimer's Disease classification on the ADNI dataset as a representative multi-modal case study. Experimental results show that SMMT maintains competitive predictive performance while significantly reducing training time, memory usage, and energy consumption compared to dense attention baselines, demonstrating its suitability as a resource-aware architectural component for scalable intelligent systems.", "AI": {"tldr": "提出了一种稀疏多模态变压器架构（SMMT），用于提高阿尔茨海默病分类的效率和鲁棒性。", "motivation": "传统Transformer因密集自注意力机制导致高计算成本，限制了其在资源受限条件下的应用。为了解决这个问题并提升系统的可扩展性，本文提出了SMMT。", "method": "基于级联多模态变压器框架，引入簇基稀疏注意力和模态特定掩码来提高效率和鲁棒性。", "result": "实验表明，与密集注意基准相比，SMMT在保持竞争力的预测性能的同时显著减少了训练时间、内存使用量和能耗。", "conclusion": "提出的架构适合资源受限场景下的大规模智能系统应用。"}}
{"id": "2512.14489", "pdf": "https://arxiv.org/pdf/2512.14489", "abs": "https://arxiv.org/abs/2512.14489", "authors": ["Alessia Micieli", "Giovanni Maria Farinella", "Francesco Ragusa"], "title": "SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.", "AI": {"tldr": "本文介绍了SignIT数据集，该数据集用于研究意大利手语识别任务。", "motivation": "为了解决意大利手语识别中的挑战，并提供一个全面的数据集和多模态分析框架。", "method": "构建了一个包含644个视频、3.33小时的SignIT数据集。手动注释了涉及94种不同手势类别的视频，这些类别属于5大宏观分类：动物、食物、颜色、情感和家庭。提取了用户的手部、面部和身体的2D关键点。", "result": "实验结果表明，现有的模型在处理这个具有挑战性的意大利手语数据集时存在局限性。", "conclusion": "公开发布了SignIT数据集及其注释以供研究使用。"}}
{"id": "2512.14481", "pdf": "https://arxiv.org/pdf/2512.14481", "abs": "https://arxiv.org/abs/2512.14481", "authors": ["Shizhuo Mao", "Song Chen", "Yi Kang"], "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.", "AI": {"tldr": "本文提出了SASQ，一种轻量级的量化感知训练框架，专门针对激活量化因子进行优化。", "motivation": "大规模语言模型由于其不断增长的规模和有限的GPU内存之间的矛盾而面临部署挑战。现有的量化方法要么计算开销高，要么牺牲精度，或者在权重训练成本方面存在不足。", "method": "SASQ只优化量化因子（不改变预训练权重），实现静态推断的同时保持高效部署，并且通过自适应截断一些异常值来降低量化难度并保留激活的分布特性。", "result": "实验表明，SASQ在LLaMA2-7B模型上，在WikiText2数据集上的困惑度比QuaRot低5.2%，比FP16模型低4.7%。", "conclusion": "SASQ不仅超越了现有的最先进量化方案，还优于相应的FP16模型。"}}
{"id": "2512.14480", "pdf": "https://arxiv.org/pdf/2512.14480", "abs": "https://arxiv.org/abs/2512.14480", "authors": ["Weiheng Zhao", "Zilong Huang", "Jiashi Feng", "Xinggang Wang"], "title": "SuperCLIP: CLIP with Simple Classification Supervision", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2025. Code: https://github.com/hustvl/SuperCLIP", "summary": "Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.", "AI": {"tldr": "SuperCLIP是一种通过简单分类监督增强对比学习的框架，旨在改进图像和文本之间的细粒度对齐。", "motivation": "现有的CLIP模型在处理长且详细的描述时未能充分利用文本中的细粒度语义信号。这是因为CLIP的训练目标仅优化全局图像-文本相似性而忽略了词级监督。", "method": "通过向视觉编码器添加一个轻量级的线性层，SuperCLIP利用词级提示来改进视觉-文本对齐。", "result": "实验表明，SuperCLIP在零样本分类、图像-文本检索和纯视觉任务上都有显著提升。这些改善无论是在原始网络数据还是丰富重标注数据上都存在，证明了其恢复文本监督的能力。此外，它还能缓解CLIP的小批量性能下降问题。", "conclusion": "通过引入简单的分类监督，SuperCLIP能够改进现有的CLIP模型在细粒度视觉-文本对齐方面的能力，并且无需额外的注释数据和大批次大小的需求。"}}
{"id": "2512.14477", "pdf": "https://arxiv.org/pdf/2512.14477", "abs": "https://arxiv.org/abs/2512.14477", "authors": ["Andreas Sjölander", "Valeria Belloni", "Robel Fekadu", "Andrea Nascetti"], "title": "TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.", "AI": {"tldr": "本文介绍了TACK Tunnel Data (TTD) 数据集，用于隧道缺陷检测的深度学习方法。", "motivation": "随着隧道老化和破损问题加剧，传统的人工检查耗时且成本高。为提高自动化视觉检查效率，需要专门的数据集来支持深度学习模型。", "method": "该数据集包含三种不同类型的隧道衬砌图像及其缺陷标注，如裂缝、渗水等。设计用于监督、半监督和无监督的深度学习方法进行检测和分割。", "result": "TTD 数据集能够促进研究模型在不同类型隧道中的泛化性和迁移性，从而提高自动化检查技术。", "conclusion": "通过提供特定领域的数据，TTD 数据集有助于推动隧道自动检查技术和更安全、高效的基础设施维护策略的发展。"}}
{"id": "2512.14474", "pdf": "https://arxiv.org/pdf/2512.14474", "abs": "https://arxiv.org/abs/2512.14474", "authors": ["Annu Rana", "Gaurav Kumar"], "title": "Model-First Reasoning LLM Agents: Reducing Hallucinations through Explicit Problem Modeling", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex multi-step planning tasks, showing high rates of constraint violations and inconsistent solutions. Existing strategies such as Chain-of-Thought and ReAct rely on implicit state tracking and lack an explicit problem representation. Inspired by classical AI planning, we propose Model-First Reasoning (MFR), a two-phase paradigm in which the LLM first constructs an explicit model of the problem, defining entities, state variables, actions, and constraints, before generating a solution plan. Across multiple planning domains, including medical scheduling, route planning, resource allocation, logic puzzles, and procedural synthesis, MFR reduces constraint violations and improves solution quality compared to Chain-of-Thought and ReAct. Ablation studies show that the explicit modeling phase is critical for these gains. Our results suggest that many LLM planning failures stem from representational deficiencies rather than reasoning limitations, highlighting explicit modeling as a key component for robust and interpretable AI agents. All prompts, evaluation procedures, and task datasets are documented to facilitate reproducibility.", "AI": {"tldr": "论文提出了一种新的方法Model-First Reasoning（MFR），旨在通过构建问题的显式模型来减少大型语言模型在复杂多步骤规划任务中的幻觉现象。", "motivation": "现有的策略如Chain-of-Thought和ReAct依赖隐式状态跟踪，缺乏显式的任务表示。这些策略会导致大量的约束违反和不一致的解决方案。因此，作者提出了一种基于经典人工智能规划的方法来改进大型语言模型的表现。", "method": "MFR由两阶段组成：第一阶段是建立问题的显式模型，定义实体、状态变量、动作和约束；第二阶段是在构建完模型后生成解决计划。", "result": "在包括医疗调度、路线规划、资源分配、逻辑谜题和程序合成等多个规划领域中，MFR相比Chain-of-Thought和ReAct减少了约束违反并提高了解决方案的质量。研究还表明显式建模步骤对于这些改进至关重要。", "conclusion": "研究表明许多大型语言模型的规划失败是由于表示缺陷而非推理能力不足，因此显式建模可以作为创建稳健且可解释的人工智能代理的关键组成部分。"}}
{"id": "2512.14465", "pdf": "https://arxiv.org/pdf/2512.14465", "abs": "https://arxiv.org/abs/2512.14465", "authors": ["Siyuan Zhu", "Chengdong Xu", "Kaiqiang Ke", "Chao Yu"], "title": "Context-Picker: Dynamic context selection using multi-stage reinforcement learning", "categories": ["cs.AI"], "comment": null, "summary": "In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \\emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \\emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \\emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines \"minimal sufficient sets\" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.", "AI": {"tldr": "论文提出了一个名为Context-Picker的框架，用于在长文本问答中动态选择最优上下文。", "motivation": "传统方法如固定Top-K检索和单一阶段重排序，在确定给定查询的最佳上下文数量时面临挑战。引入冗余或遗漏关键信息会影响答案质量。", "method": "Context-Picker通过多阶段强化学习优化上下文选择，首先采用召回导向策略覆盖推理链，其次使用精度导向策略减少冗余以提炼紧凑的证据集。", "result": "实验结果显示，在五个长文本和多跳问答基准测试中，Context-Picker显著优于强大的RAG基线，实现了更准确的答案且具有相似或更短的上下文长度。", "conclusion": "Context-Picker证明了在长文本问答中通过动态选择最优上下文可以提高答案质量。其方法包括两阶段优化策略、冗余感知奖励塑造和理由引导格式，均对结果有显著贡献。"}}
{"id": "2512.14457", "pdf": "https://arxiv.org/pdf/2512.14457", "abs": "https://arxiv.org/abs/2512.14457", "authors": ["Jingyang Zhao", "Mingyu Xiao"], "title": "An Improved Approximation Algorithm for Maximum Weight 3-Path Packing", "categories": ["cs.DS"], "comment": null, "summary": "Given a complete graph with $n$ vertices and non-negative edge weights, where $n$ is divisible by 3, the maximum weight 3-path packing problem is to find a set of $n/3$ vertex-disjoint 3-paths such that the total weight of the 3-paths in the packing is maximized. This problem is closely related to the classic maximum weight matching problem. In this paper, we propose a $10/17$-approximation algorithm, improving the best-known $7/12$-approximation algorithm (ESA 2015). Our result is obtained by making a trade-off among three algorithms. The first is based on the maximum weight matching of size $n/2$, the second is based on the maximum weight matching of size $n/3$, and the last is based on an approximation algorithm for star packing. Our first algorithm is the same as the previous $7/12$-approximation algorithm, but we propose a new analysis method -- a charging method -- for this problem, which is not only essential to analyze our second algorithm but also may be extended to analyze algorithms for some related problems.", "AI": {"tldr": "提出了一种改进的近似算法，用于解决最大权重3路径打包问题。", "motivation": "为了提高最大权重匹配问题中已知的最佳7/12近似算法，并寻找更好的解决方案以优化完整图中的3-路径打包问题。", "method": "通过权衡三种算法，包括基于大小为n/2的最大权重匹配、基于大小为n/3的最大权重匹配以及星形打包的近似算法。提出了新的分析方法——充电法。", "result": "提出了一种10/17的近似算法，超过了之前的最佳7/12近似算法。", "conclusion": "通过引入新分析方法并优化三种算法之间的权衡关系，成功提高了最大权重3路径打包问题的解的质量。"}}
{"id": "2512.14450", "pdf": "https://arxiv.org/pdf/2512.14450", "abs": "https://arxiv.org/abs/2512.14450", "authors": ["Riccardo Busetto", "Elia Cereda", "Marco Forgione", "Gabriele Maroni", "Dario Piga", "Daniele Palossi"], "title": "Nonlinear System Identification Nano-drone Benchmark", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We introduce a benchmark for system identification based on 75k real-world samples from the Crazyflie 2.1 Brushless nano-quadrotor, a sub-50g aerial vehicle widely adopted in robotics research. The platform presents a challenging testbed due to its multi-input, multi-output nature, open-loop instability, and nonlinear dynamics under agile maneuvers. The dataset comprises four aggressive trajectories with synchronized 4-dimensional motor inputs and 13-dimensional output measurements. To enable fair comparison of identification methods, the benchmark includes a suite of multi-horizon prediction metrics for evaluating both one-step and multi-step error propagation. In addition to the data, we provide a detailed description of the platform and experimental setup, as well as baseline models highlighting the challenge of accurate prediction under real-world noise and actuation nonlinearities. All data, scripts, and reference implementations are released as open-source at https://github.com/idsia-robotics/nanodrone-sysid-benchmark to facilitate transparent comparison of algorithms and support research on agile, miniaturized aerial robotics.", "AI": {"tldr": "本文介绍了基于Crazyflie 2.1 Brushless纳米四旋翼的系统识别基准，提供了75k真实世界样本。", "motivation": "由于其多输入、多输出特性及在敏捷操作下的非线性动态和开环不稳定性质，该平台为系统识别提供了一个极具挑战性的测试床。此外，还提供了一系列预测指标以评估方法的有效性和透明度。", "method": "基准包括了四条激进轨迹的同步4维电机输入和13维度输出测量数据，用于多步误差传播评估。", "result": "发布了所有数据、脚本及参考实现为开源项目，支持研究敏捷微型航空机器人系统识别方法。", "conclusion": "该基准为研究人员提供了全面的数据集以及实验环境描述，促进了透明的算法比较和学术交流。"}}
{"id": "2512.14448", "pdf": "https://arxiv.org/pdf/2512.14448", "abs": "https://arxiv.org/abs/2512.14448", "authors": ["Xingfu Zhou", "Pengfei Wang"], "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.", "AI": {"tldr": "提出了一种针对大型语言模型代理的新型过程级攻击方法，通过操纵代理处理信息的方式而非其内容来实现。", "motivation": "现有对抗性攻击主要集中在内容伪造或指令注入上，而本文关注的是代理处理信息方式的新颖攻击面：推理风格。", "method": "提出Reasoning-Style Poisoning (RSP) 方法，通过Generative Style Injection (GSI) 攻击将检索文档重写成病态语气（分析瘫痪或认知急躁）而不改变底层事实。开发了Reasoning Style Vector (RSV)，用于量化这些变化，并在HotpotQA和FEVER上对ReAct、Reflection和Tree of Thoughts架构进行了实验。", "result": "GSI显著降低了性能，推理步骤增加了4.4倍或者提前犯错，成功绕过了最先进的内容过滤器。还提出了RSP-M实时监控方法，通过计算RSV指标并在超出安全阈值时触发警报来保障安全性。", "conclusion": "证明了推理风格是一个独立且可被利用的漏洞，需要超越静态内容分析的过程级防御措施"}}
{"id": "2512.14442", "pdf": "https://arxiv.org/pdf/2512.14442", "abs": "https://arxiv.org/abs/2512.14442", "authors": ["Zixin Zhang", "Kanghao Chen", "Hanqing Wang", "Hongfei Zhang", "Harold Haodong Chen", "Chenfei Liao", "Litao Guo", "Ying-Cong Chen"], "title": "A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\\textbf{Dreamer}$ that employs generative models to visualize $\\textit{how}$ an interaction would look; (2) a $\\textbf{Thinker}$ that utilizes large vision-language models to decide $\\textit{what}$ object part to interact with; and (3) a $\\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.", "AI": {"tldr": "提出了一种零样本框架A4-Agent，用于基于语言指令预测对象的交互区域。", "motivation": "现有方法依赖于大规模标注数据集训练，导致在新物体和环境下的泛化能力较差。本文旨在通过无需任务特定微调的预训练模型解耦处理来解决这一问题。", "method": "引入了A4-Agent框架，包含三个阶段：Dreamer（使用生成模型可视化交互），Thinker（利用视觉语言大型模型决定要与哪个部分互动）和Spotter（协调视觉基础模型以精确确定互动区域）。", "result": "该零样本方法在多个基准上超过了最先进的监督学习方法，并显示了对真实世界场景的鲁棒泛化能力。", "conclusion": "通过提出A4-Agent框架，证明了无需训练也能实现准确且具有高度泛化的交互预测。"}}
{"id": "2512.14440", "pdf": "https://arxiv.org/pdf/2512.14440", "abs": "https://arxiv.org/abs/2512.14440", "authors": ["Leon Sick", "Lukas Hoyer", "Dominik Engel", "Pedro Hermosilla", "Timo Ropinski"], "title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation", "categories": ["cs.CV"], "comment": "Project Page with Code/Models/Demo: https://leonsick.github.io/s2d/", "summary": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.", "AI": {"tldr": "本文提出了一种仅使用真实视频数据进行无监督视频实例分割的方法。", "motivation": "当前最先进的无监督视频实例分割方法依赖于人工合成的视频，这些视频未能准确模拟现实中的运动变化。为了改进这一点，文章致力于开发一种基于真实视频训练的技术。", "method": "该研究从单帧分割开始，通过利用深度运动先验识别高质量的关键掩码，并引入稀疏到密集知识蒸馏方法以及时间DropLoss来提高模型在多帧上的表现。", "result": "所提出的方法在多个基准测试中超越了现有最先进水平。", "conclusion": "该技术成功地提高了无监督视频实例分割的质量，特别是在基于真实数据集的训练方面取得了显著进展。"}}
{"id": "2512.14439", "pdf": "https://arxiv.org/pdf/2512.14439", "abs": "https://arxiv.org/abs/2512.14439", "authors": ["Quan Yuan", "Zhikun Zhang", "Linkang Du", "Min Chen", "Mingyang Sun", "Yunjun Gao", "Shibo He", "Jiming Chen"], "title": "VICTOR: Dataset Copyright Auditing in Video Recognition Systems", "categories": ["cs.CR", "cs.CV"], "comment": "To appear in the NDSS Symposium 2026, February 2026, San Diego, CA, USA", "summary": "Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods. In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.", "AI": {"tldr": "提出VICTOR，一种针对视频识别系统的数据集版权审计方法", "motivation": "现有数据集版权解决方案主要集中在图像领域，而视频数据的复杂性使得该问题在视频领域的研究较少。本文旨在解决视频数据集中未经授权使用的问题", "method": "通过修改一小部分样本（如1%），VICTOR增强目标模型输出差异，并利用这种差异进行数据集审计", "result": "实验表明VICTOR在多种模型和数据集上表现优异，且对训练视频或目标模型的扰动具有鲁棒性", "conclusion": "VICTOR为视频识别系统的数据集版权审计提供了一种有效的方法"}}
{"id": "2512.14435", "pdf": "https://arxiv.org/pdf/2512.14435", "abs": "https://arxiv.org/abs/2512.14435", "authors": ["Chang Cai", "Hao Jiang", "Xiaojun Yuan", "Ying-Jun Angela Zhang"], "title": "Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.", "AI": {"tldr": "提出了一种基于评分的快速消息传递算法（STMP）用于压缩成像，结合了消息传递和评分生成先验的优势。", "motivation": "传统插件式方法在高度欠定情况下重建性能不佳，新型评分模型难以直接应用于后验采样。为此开发一种新的融合技术以优化图像恢复。", "method": "利用评分模型与经验贝叶斯去噪之间的关联性设计消息传递框架，引入基于评分的最小均方误差去噪器处理压缩成像恢复。对于量化测量系统提出量化的STMP（Q-STMP）。", "result": "实验显示STMP在性能和复杂度之间找到了更好的平衡点，并且即使在1位量化下仍保持稳定。", "conclusion": "提出的评分消息传递算法显著改善了图像重建效果，展示了一种解决高度欠定情况的有效方法。"}}
{"id": "2512.14434", "pdf": "https://arxiv.org/pdf/2512.14434", "abs": "https://arxiv.org/abs/2512.14434", "authors": ["Quan Yuan", "Daqian Cao", "Weibang Bai"], "title": "Geometric Parameter Optimization of a Novel 3-(PP(2-(UPS))) Redundant Parallel Mechanism based on Workspace Determination", "categories": ["cs.RO"], "comment": "7 pages, 5 figures", "summary": "Redundant parallel robots are normally employed in scenarios requiring good precision, high load capability, and large workspace compared to traditional parallel mechanisms. However, the elementary robotic configuration and geometric parameter optimization are still quite challenging. This paper proposes a novel 3-(PP(2-(UPS))) redundant parallel mechanism, with good generalizability first, and further investigates the kinematic optimization issue by analyzing and investigating how its key geometric parameters influence the volume, shape, boundary completeness, and orientation capabilities of its workspace. The torsional capability index TI_1 and tilting capability index TI_2 are defined to evaluate the orientation performance of the mechanism. Numerical simulation studies are completed to indicate the analysis, providing reasonable but essential references for the parameter optimization of 3-(PP(2-(UPS))) and other similar redundant parallel mechanisms.", "AI": {"tldr": "该论文提出了一种新型的3-(PP(2-(UPS)))冗余并联机制，并通过分析其关键几何参数对其工作空间的影响，完成了对该机制的姿态性能评估。", "motivation": "为了提高精度、载荷能力和工作空间，在传统并联机构的基础上开发一种新的冗余并联机器人。研究目的是解决基本机械配置和几何参数优化的挑战性问题。", "method": "定义了扭转能力指数TI_1和倾斜能力指数TI_2来评估该机制的姿态性能，并通过数值仿真研究展示了分析结果。", "result": "研究表明，关键几何参数对工作空间的体积、形状、边界完整性和方向性能力有显著影响。仿真结果显示了一种合理的但必要的参考方法。", "conclusion": "通过对3-(PP(2-(UPS)))冗余并联机构的关键几何参数进行优化分析，论文为其他类似的冗余并联机制提供了有价值的参考依据。"}}
{"id": "2512.14429", "pdf": "https://arxiv.org/pdf/2512.14429", "abs": "https://arxiv.org/abs/2512.14429", "authors": ["Yukun Ren", "Siwei Yu", "Kai Chen", "Jianwei Ma"], "title": "Seismology modeling agent: A smart assistant for geophysical researchers", "categories": ["cs.AI", "cs.SE"], "comment": "26 pages, 15 figures. Code available at https://github.com/RenYukun1563/specfem-mcp", "summary": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.", "AI": {"tldr": "本文提出了一种基于大语言模型的智能交互式工作流，旨在降低主流开源地震波模拟软件SPECFEM的学习曲线和复杂操作需求。", "motivation": "针对传统SPECFEM软件学习难度高、依赖复杂的文件编辑和命令行操作的问题，引入了Model Context Protocol（MCP）服务器套件，以简化用户交互并提高工作效率。", "method": "通过将整个模拟过程分解为独立的代理可执行工具，并实现了从参数生成到求解器执行及可视化的一系列步骤自动化，构建了一个新型智能工作流。", "result": "该方法经过多案例验证，在自主模式和互动模式下均能平稳运行并产生高保真度的结果。", "conclusion": "此研究显著降低了进入门槛，增强了可重复性，并为计算地震学向AI辅助和自动化科研迈进提供了新路径。"}}
{"id": "2512.14428", "pdf": "https://arxiv.org/pdf/2512.14428", "abs": "https://arxiv.org/abs/2512.14428", "authors": ["Aaron Kurda", "Simon Steuernagel", "Lukas Jung", "Marcus Baum"], "title": "Odyssey: An Automotive Lidar-Inertial Odometry Dataset for GNSS-denied situations", "categories": ["cs.RO"], "comment": "9 pages, 4 figures, submitted to International Journal of Robotics Research (IJRR)", "summary": "The development and evaluation of Lidar-Inertial Odometry (LIO) and Simultaneous Localization and Mapping (SLAM) systems requires a precise ground truth. The Global Navigation Satellite System (GNSS) is often used as a foundation for this, but its signals can be unreliable in obstructed environments due to multi-path effects or loss-of-signal. While existing datasets compensate for the sporadic loss of GNSS signals by incorporating Inertial Measurement Unit (IMU) measurements, the commonly used Micro-Electro-Mechanical Systems (MEMS) or Fiber Optic Gyroscope (FOG)-based systems do not permit the prolonged study of GNSS-denied environments. To close this gap, we present Odyssey, a LIO dataset with a focus on GNSS-denied environments such as tunnels and parking garages as well as other underrepresented, yet ubiquitous situations such as stop-and-go-traffic, bumpy roads and wide open fields. Our ground truth is derived from a navigation-grade Inertial Navigation System (INS) equipped with a Ring Laser Gyroscope (RLG), offering exceptional bias stability characteristics compared to IMUs used in existing datasets and enabling the prolonged and accurate study of GNSS-denied environments. This makes Odyssey the first publicly available dataset featuring a RLG-based INS. Besides providing data for LIO, we also support other tasks, such as place recognition, through the threefold repetition of all trajectories as well as the integration of external mapping data by providing precise geodetic coordinates. All data, dataloader and other material is available online at https://odyssey.uni-goettingen.de/ .", "AI": {"tldr": "奥德赛数据集用于开发和评估在GPS信号缺失的情况下，基于激光雷达与惯性测量单元的定位系统。", "motivation": "现有数据集中使用的惯性测量单元无法长时间研究GNSS信号缺失的情况。为此，提出了一种新的数据集来弥补这一不足。", "method": "使用高精度环形激光陀螺仪作为地面真实值，记录了车辆在隧道、停车场以及交通拥堵、颠簸道路和开阔地带等环境下的运动轨迹，并且提供了精确的地理坐标以支持其他任务如场景识别。", "result": "奥德赛数据集是首个公开提供基于环形激光陀螺仪惯性导航系统的LIO数据集，为研究GNSS信号缺失情况下的定位系统开发与评估提供了可能。", "conclusion": "该数据集填补了现有数据集无法长时间研究GNSS信号缺失环境的空白，并促进了更广泛的场景识别任务的研究。"}}
{"id": "2512.14427", "pdf": "https://arxiv.org/pdf/2512.14427", "abs": "https://arxiv.org/abs/2512.14427", "authors": ["Gabriele Prato", "Shagun Sodhani", "Alessandro Sordoni", "Sarath Chandar"], "title": "Effect of Document Packing on the Latent Multi-Hop Reasoning Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The standard practice for training large language models involves packing multiple documents together to optimize computational efficiency. However, the impact of this process on the models' capabilities remains largely unexplored. To address this gap, we investigate how different document-packing strategies influence the latent multi-hop reasoning abilities of LLMs. Our findings indicate that packing can improve model performance compared to training on individual documents, at the expense of more compute. To further understand the underlying mechanisms, we conduct an ablation study, identifying key factors that explain the advantages of packing. Ultimately, our research deepens the understanding of LLM training dynamics and provides practical insights for optimizing model development.", "AI": {"tldr": "研究不同文档打包策略对大型语言模型潜在的多跳推理能力的影响。", "motivation": "探讨文档打包训练方法对模型性能的具体影响，填补相关领域的研究空白。", "method": "通过比较不同类型文档打包策略在优化计算效率和提升模型多步推理能力上的效果，并进行消融实验以识别关键因素。", "result": "发现适当文档打包能提高大型语言模型的性能，但需要更多的计算资源。同时确定了影响这种性能增强的关键因素。", "conclusion": "该研究加深了对大规模语言模型训练动态的理解，为优化模型开发提供了实际见解"}}
{"id": "2512.14426", "pdf": "https://arxiv.org/pdf/2512.14426", "abs": "https://arxiv.org/abs/2512.14426", "authors": ["Simon Steuernagel", "Marcus Baum"], "title": "Quadratic Kalman Filter for Elliptical Extended Object Tracking based on Decoupling State Components", "categories": ["eess.SP", "cs.RO"], "comment": "13 pages, 8 figures, submitted to IEEE Transactions on Aerospace and Electronic Systems", "summary": "Extended object tracking involves estimating both the physical extent and kinematic parameters of a target object, where typically multiple measurements are observed per time step. In this article, we propose a deterministic closed-form elliptical extended object tracker, based on decoupling of the kinematics, orientation, and axis lengths. By disregarding potential correlations between these state components, fewer approximations are required for the individual estimators than for an overall joint solution. The resulting algorithm outperforms existing algorithms, reaching the accuracy of sampling-based procedures. Additionally, a batch-based variant is introduced, yielding highly efficient computation while outperforming all comparable state-of-the-art algorithms. This is validated both by a simulation study using common models from literature, as well as an extensive quantitative evaluation on real automotive radar data.", "AI": {"tldr": "本文提出了一种基于状态分量解耦的椭圆扩展目标跟踪算法，该算法通过忽略各状态分量之间的相关性减少了近似处理，从而提高了准确性并实现了高效的计算。", "motivation": "现有的扩展对象跟踪方法在估计目标物理尺寸和运动参数时存在较多近似误差。本文旨在通过解耦技术减少这些误差，并提出一种高效且准确的算法。", "method": "基于状态分量解耦的思想，设计了一种确定性的闭合形式椭圆扩展目标跟踪器，该方法将目标的动力学、姿态及轴长进行分离处理，从而减少了各分量之间的近似误差。此外，还引入了批次处理版本以实现更高效的计算。", "result": "通过仿真研究和实车雷达数据的定量评估验证了所提算法的有效性，结果表明该算法达到了采样方法的精度，并且在效率方面优于现有所有同类最先进的算法。", "conclusion": "本文提出的基于状态分量解耦的椭圆扩展目标跟踪算法，不仅提高了估计准确性，而且实现了高效的计算性能，是一种具有实际应用价值的方法。"}}
{"id": "2512.14423", "pdf": "https://arxiv.org/pdf/2512.14423", "abs": "https://arxiv.org/abs/2512.14423", "authors": ["Zhuo Chen", "Fanyue Wei", "Runze Xu", "Jingjing Li", "Lixin Duan", "Angela Yao", "Wen Li"], "title": "The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy", "categories": ["cs.CV"], "comment": "Project page:https://synps26.github.io/", "summary": "Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.", "AI": {"tldr": "本文提出了一种改进复杂非刚性图像编辑忠实性的方法SynPS，通过协同利用位置嵌入和语义信息来避免过度或不足的编辑。", "motivation": "随着大型扩散模型在训练自由图像编辑中的应用变得实用化，进行复杂的非刚性编辑（如姿态或形状变化）仍然非常具有挑战性。作者发现现有注意力共享机制中存在的注意崩溃问题会导致过度或不足编辑的问题。", "method": "SynPS通过设计一个动态调节位置嵌入影响的注意力协同管道来解决这一问题，并基于编辑测量量制定该方法，使模型能够平衡语义修改与忠实度保持。", "result": "实验显示，在公共和新整理的基准上，我们的方法表现出优越性能并提高图像编辑的忠实性。", "conclusion": "本文通过提出SynPS有效解决了复杂非刚性图像编辑过程中的过度或不足编辑问题，并显著提升了图像编辑结果的忠实度。"}}
{"id": "2512.14421", "pdf": "https://arxiv.org/pdf/2512.14421", "abs": "https://arxiv.org/abs/2512.14421", "authors": ["Mischa Dombrowski", "Felix Nützel", "Bernhard Kainz"], "title": "LCMem: A Universal Model for Robust Image Memorization Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.", "AI": {"tldr": "该论文提出了一种用于图像记忆检测的通用模型LCMem，通过两阶段训练策略实现跨领域的身份一致性和抗增强复制检测。", "motivation": "现有隐私保护方法在领域间缺乏可靠的记忆检测机制、定量评估不足且泛化能力弱，作者希望解决这些障碍，提高大规模数据分享中的隐私保护效果。", "method": "LCMem通过将记忆检测视为重识别和拷贝检测的统一问题，在两阶段训练策略中首先学习身份一致性然后集成抗增强复制检测，从而实现跨领域的可靠记忆检测。", "result": "在六个基准数据集上，LCMem实现了高达16个百分点的身份一致性和30个百分点的复制检测改进，展示了更强的记忆检测性能和鲁棒性。", "conclusion": "研究表明现有隐私过滤器提供有限的性能与稳健性，而LCMem通过跨领域隐私审计确立了新的标准，为大规模可靠记忆检测提供了有力支持。"}}
{"id": "2512.14420", "pdf": "https://arxiv.org/pdf/2512.14420", "abs": "https://arxiv.org/abs/2512.14420", "authors": ["Nakamasa Inoue", "Kanoko Goto", "Masanari Oi", "Martyna Gruszka", "Mahiro Ukai", "Takumi Hirose", "Yusuke Sekikawa"], "title": "DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning", "categories": ["cs.CV", "cs.AI"], "comment": "Paper accepted to AAAI 2026", "summary": "Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.", "AI": {"tldr": "论文提出了一种新的无微调方法DISCODE，用于图像描述的鲁棒自动评估。", "motivation": "大规模视觉-语言模型在多模态任务中表现出色，但在领域偏移场景下的图像描述评价仍然具有挑战性。为此引入了分布感知评分解码器（DISCODE）来生成更贴近人类判断的标准分数。", "method": "DISCODE通过测试时间自适应评估方法和Adaptive Test-Time（ATT）损失函数，利用高斯先验分布改善评分估计的鲁棒性，并提出了一个新的跨域图像描述评价基准MCEval。", "result": "在MCEval以及四个代表性的现有基准上，DISCODE作为无参考标准显示了最先进的性能。", "conclusion": "论文通过引入DISCODE和MCEval证明了其方法的有效性和鲁棒性。"}}
{"id": "2512.14417", "pdf": "https://arxiv.org/pdf/2512.14417", "abs": "https://arxiv.org/abs/2512.14417", "authors": ["Jia Hu", "Junqi Li", "Weimeng Lin", "Peng Jia", "Yuxiong Ji", "Jintao Lai"], "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals", "categories": ["cs.AI"], "comment": null, "summary": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created", "AI": {"tldr": "开发PortAgent，一种基于大型语言模型的港口终端车辆调度代理，以自动化车辆调度系统（VDS）的工作流程。", "motivation": "传统车辆调度系统的低可转移性问题限制了其商业化应用，主要挑战包括高度依赖于港口运营专家、需要大量特定终端的数据以及手动部署耗时长。", "method": "使用虚拟专家团队(VET)，结合知识检索者、建模师、编码员和调试员等四个虚拟专家角色，采用少量样本学习方法，并通过检索增强生成(RAG)机制来减少对特定终端数据的需求。此外，创建了一个自我校正循环的工作流程以避免额外的人工干预。", "result": "PortAgent能够消除对港口运营专家的依赖，降低特定终端的数据需求，并实现快速部署和自动化VDS工作流程。", "conclusion": "通过使用大型语言模型，PortAgent成功解决了传统车辆调度系统在不同港口之间可转移性的难题，展示了其强大的应用潜力。"}}
{"id": "2512.14411", "pdf": "https://arxiv.org/pdf/2512.14411", "abs": "https://arxiv.org/abs/2512.14411", "authors": ["Mohammed Ayman Habib", "Aldo Petruzzelli"], "title": "Synthetic Data Pipelines for Adaptive, Mission-Ready Militarized Humanoids", "categories": ["cs.RO"], "comment": "6 pages; xTech Humanoid white paper submission", "summary": "Omnia presents a synthetic data driven pipeline to accelerate the training, validation, and deployment readiness of militarized humanoids. The approach converts first-person spatial observations captured from point-of-view recordings, smart glasses, augmented reality headsets, and spatial browsing workflows into scalable, mission-specific synthetic datasets for humanoid autonomy. By generating large volumes of high-fidelity simulated scenarios and pairing them with automated labeling and model training, the pipeline enables rapid iteration on perception, navigation, and decision-making capabilities without the cost, risk, or time constraints of extensive field trials. The resulting datasets can be tuned quickly for new operational environments and threat conditions, supporting both baseline humanoid performance and advanced subsystems such as multimodal sensing, counter-detection survivability, and CBRNE-relevant reconnaissance behaviors. This work targets faster development cycles and improved robustness in complex, contested settings by exposing humanoid systems to broad scenario diversity early in the development process.", "AI": {"tldr": "Omnia 提出了一种基于合成数据的管道，用于加速军事化的人形机器人的训练、验证和部署准备。", "motivation": "为了降低成本、减少风险并缩短时间限制，Omnia 使用合成数据来提高人形机器人在复杂环境中的感知、导航和决策能力。", "method": "通过将第一人称空间观察结果转换为可扩展的、任务特定的合成数据集，并结合自动化标记和模型训练，生成大量高保真的模拟场景。", "result": "该方法能够快速调整新操作环境和威胁条件下的数据集，支持人形机器人的基础性能和高级子系统。", "conclusion": "这种方法加快了开发周期并提高了在复杂环境中的鲁棒性。"}}
{"id": "2512.14409", "pdf": "https://arxiv.org/pdf/2512.14409", "abs": "https://arxiv.org/abs/2512.14409", "authors": ["Michelle Döring", "Jannes Malanowski", "Stefan Neubert"], "title": "Cost-Free Neutrality for the River Method", "categories": ["cs.DS"], "comment": "appears at AAAI 2026", "summary": "Recently, the River Method was introduced as novel refinement of the Split Cycle voting rule. The decision-making process of River is closely related to the well established Ranked Pairs Method. Both methods consider a margin graph computed from the voters' preferences and eliminate majority cycles in that graph to choose a winner. As ties can occur in the margin graph, a tiebreaker is required along with the preferences. While such a tiebreaker makes the computation efficient, it compromises the fundamental property of neutrality: the voting rule should not favor alternatives in advance. One way to reintroduce neutrality is to use Parallel-Universe Tiebreaking (PUT), where each alternative is a winner if it wins according to any possible tiebreaker. Unfortunately, computing the winners selected by Ranked Pairs with PUT is NP-complete. Given the similarity of River to Ranked Pairs, one might expect River to suffer from the same complexity. Surprisingly, we show the opposite: We present a polynomial-time algorithm for computing River winners with PUT, highlighting significant structural advantages of River over Ranked Pairs. Our Fused-Universe (FUN) algorithm simulates River for every possible tiebreaking in one pass. From the resulting FUN diagram one can then directly read off both the set of winners and, for each winner, a certificate that explains how this alternative dominates the others.", "AI": {"tldr": "本文提出了一种名为Fused-Universe（FUN）的新算法，该算法可以在多项式时间内为River方法计算出在Parallel-Universe Tiebreaking下的获胜者。", "motivation": "尽管使用Parallel-Universe Tiebreaking可以恢复中立性，但Ranked Pairs方法在这种情况下需要NP完全的复杂度来选择获胜者。本文旨在探索River方法是否也能以类似方式保持多项式时间内的有效性。", "method": "通过一种称为Fused-Universe（FUN）的新算法，在一次遍历中模拟所有可能的平局解决策略下的River选择，从而直接确定获胜者的集合以及每个获胜者如何超过其他选项。", "result": "本文提出了一种新的多项式时间复杂度算法，可以有效计算出在Parallel-Universe Tiebreaking下使用River方法时的选择结果，展示了River与Ranked Pairs相比的结构优势。", "conclusion": "通过展示FUN算法的有效性，表明了River方法不仅可以在保持中立性的前提下选择获胜者，而且其计算复杂度远低于Ranked Pairs。"}}
{"id": "2512.14406", "pdf": "https://arxiv.org/pdf/2512.14406", "abs": "https://arxiv.org/abs/2512.14406", "authors": ["Le Jiang", "Shaotong Zhu", "Yedi Luo", "Shayda Moezzi", "Sarah Ostadabbas"], "title": "Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.", "AI": {"tldr": "本文提出了一种名为Expanded Dynamic NeRF（ExpanDyNeRF）的单目神经辐射场框架，以解决动态场景在大角度旋转视角下合成不稳定和不真实的问题。", "motivation": "传统的动态神经辐射场系统在显著的角度偏移下无法生成稳定的现实渲染。为了解决这一问题，本文提出了一个改进的方法来提高动态场景下的视点合成能力。", "method": "ExpanDyNeRF利用高斯散射先验和伪地面真实生成策略，在挑战性的视角条件下优化密度和颜色特征以改善场景重建。", "result": "在SynDM及实际数据集上，ExpanDyNeRF的渲染质量显著优于现有的动态神经辐射场方法。", "conclusion": "ExpanDyNeRF能够提高视点合成的真实性和稳定性，在大角度旋转视角下表现出色。"}}
{"id": "2512.14395", "pdf": "https://arxiv.org/pdf/2512.14395", "abs": "https://arxiv.org/abs/2512.14395", "authors": ["Wentao Wan", "Qiqing Lao", "Zhiwei Xie", "Hefeng Wu", "Runnan Lin", "Liang Lin", "Keze Wang"], "title": "Massive Editing for Large Language Models Based on Dynamic Weight Generation", "categories": ["cs.AI"], "comment": "27 pages, 8 figures", "summary": "Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.", "AI": {"tldr": "提出了一种基于动态权重生成的大型语言模型大规模编辑方法（MeG），以实现知识修改的同时保持可靠性、通用性和局部性。", "motivation": "当前在大型语言模型中进行大规模编辑时，需要确保编辑的可靠性、通用性和局部性是一个挑战。为此，论文提出了一个新方案来应对这一问题。", "method": "通过将动态权重神经元附加到LLM特定层，并利用扩散模型基于输入查询条件生成该神经元的权重来实现大规模知识编辑。", "result": "实验表明，MeG方法在可靠性、通用性和局部性指标上显著优于现有知识编辑方法，特别是在局部性指标上的绝对值指数有大幅增加。", "conclusion": "研究提出了一种新的基于动态权重生成的大规模知识编辑方法（MeG），有效解决了大规模编辑中保持各指标的问题，并展示了明显的优势。"}}
{"id": "2512.14391", "pdf": "https://arxiv.org/pdf/2512.14391", "abs": "https://arxiv.org/abs/2512.14391", "authors": ["Huayang Li", "Tianyu Zhao", "Richard Sproat"], "title": "RePo: Language Models with Context Re-Positioning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.", "AI": {"tldr": "本文提出了一种新的语言模型机制RePo，通过调整上下文中的词位置来减少额外的认知负荷，并提高在涉及嘈杂上下文、结构化数据和较长上下文长度的任务上的性能。", "motivation": "现有的大型语言模型架构对上下文结构施加了固定的线性或恒定的位置索引，这会增加不必要的认知负荷。受认知负载理论启发，作者提出了一种新的机制来减少这种额外的认知负担，从而提高深度推理和注意力分配的效率。", "method": "RePo利用可微分模块$f_φ$为令牌分配位置，以捕捉上下文依赖关系而不是使用预定义的整数范围。通过不断在OLMo-2 1B骨干网中进行预训练来展示其效果。", "result": "实验证明，与标准方法相比，RePo显著提高了处理嘈杂上下文、结构化数据和较长上下文长度任务的能力，并且在一般短上下文任务上也能保持竞争力。进一步分析显示，RePo能更有效地关注到远距离但相关的信息，并以密集而非线性的方式分配位置。", "conclusion": "通过重新定位上下文中的令牌位置来减少认知负荷的RePo方法，在处理复杂和长上下文的任务中表现出了优越性能，展示了其在语言模型优化方面的潜力。"}}
{"id": "2512.14390", "pdf": "https://arxiv.org/pdf/2512.14390", "abs": "https://arxiv.org/abs/2512.14390", "authors": ["Jakub Balabán"], "title": "Finding $b$-colorings Using Feedback Edges", "categories": ["cs.DS"], "comment": null, "summary": "A $b$-coloring of a graph is a proper vertex coloring such that each color class contains a vertex that sees all other colors in its neighborhood. The $b$-coloring problem, in which the task is to decide whether a graph admits a $b$-coloring with $k$ colors, is NP-complete in general but polytime solvable on trees. Moreover, it is known that $b$-coloring is in XP but W[$t$]-hard for all $t \\in \\mathbb{N}$ when parameterized by tree-width. In fact, only very few parameters, such as the vertex cover number, were known to admit an FPT algorithm for $b$-coloring. In this paper, we consider a more restrictive parameter measuring similarity to trees than tree-width, namely the feedback edge number, and show that $b$-coloring is fixed-parameter tractable under this parameterization. Our algorithm combines standard techniques used in parameterized algorithmics with the problem-specific ideas used in the polytime algorithm for trees. In addition, we present an FPT algorithm for $b$-coloring parameterized by distance to co-cluster, which is a parameter measuring similarity to complete multipartite graphs. Finally, we make several observations based on known results, including that $b$-coloring is W[$1$]-hard when parameterized by tree-depth.", "AI": {"tldr": "研究了使用反馈边数作为参数的$b$着色问题，并提出了相应的固定参数可解算法", "motivation": "探讨在更接近树结构或完全多重图的图上解决$b$着色问题的可能性，以寻找新的参数化方法来提高问题的求解效率", "method": "结合标准参数化算法技术和针对树结构优化的方法，设计了基于反馈边数和距离到共团的固定参数可解算法", "result": "证明了$b$着色问题在使用反馈边数作为参数时是固定参数可解的，并提出了相应的算法；同时提供了基于距离到共团的$b$着色问题的新算法", "conclusion": "提出的方法为解决更接近树结构或完全多重图的图上的$b$着色问题提供了一种新的视角和工具"}}
{"id": "2512.14373", "pdf": "https://arxiv.org/pdf/2512.14373", "abs": "https://arxiv.org/abs/2512.14373", "authors": ["Martin Röhn", "Nora Gourmelon", "Vincent Christlein"], "title": "EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities", "categories": ["cs.CV"], "comment": null, "summary": "Climate adaptation is vital for the sustainability and sometimes the mere survival of our urban areas. However, small cities often struggle with limited personnel resources and integrating vast amounts of data from multiple sources for a comprehensive analysis. To overcome these challenges, this paper proposes a multi-layered system combining specialized LLMs, satellite imagery analysis and a knowledge base to aid in developing effective climate adaptation strategies. The corresponding code can be found at https://github.com/Photon-GitHub/EcoScapes.", "AI": {"tldr": "提出一个结合专业LLM、卫星图像分析和知识库的多层系统，以帮助小城市制定有效的气候适应策略。", "motivation": "解决小城市在人员资源有限的情况下难以从多个来源整合大量数据进行综合分析的问题，从而实现可持续发展与生存。", "method": "利用专业的大规模语言模型（LLM）、卫星图像分析技术和知识库来提供有关如何构建可持续城市的建议。", "result": "该系统能够为小城市提供有效的气候适应策略建议，提高其应对气候变化的能力。", "conclusion": "通过结合先进的AI技术与综合数据源，可以有效地支持小城市制定和实施针对气候变化的适应措施。"}}
{"id": "2512.14367", "pdf": "https://arxiv.org/pdf/2512.14367", "abs": "https://arxiv.org/abs/2512.14367", "authors": ["Georg Volk", "Jörg Gamerdinger", "Alexander von Bernuth", "Oliver Bringmann"], "title": "A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at IEEE ITSC 2020", "summary": "Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.", "AI": {"tldr": "提出了一种新的安全评估指标，用于评价自动驾驶系统中的物体感知。", "motivation": "当前的物体感知评估标准未能充分考虑不同物体的重要性和潜在风险。因此，需要一个综合的安全评估指标来全面评价感知系统的安全性。", "method": "提出了一个新的安全度量方法，该方法不仅基于传统的感知准确性，还考虑了速度、方向、距离和大小等参数，并应用于实际数据集和虚拟数据集进行测试。", "result": "新提出的安全度量方法在评估结果上优于现有的标准。", "conclusion": "通过引入新的综合安全评估指标，可以更全面地评价自动驾驶车辆的感知系统安全性。"}}
{"id": "2512.14366", "pdf": "https://arxiv.org/pdf/2512.14366", "abs": "https://arxiv.org/abs/2512.14366", "authors": ["Julian McGinnis", "Florian A. Hölzl", "Suprosanna Shit", "Florentin Bieder", "Paul Friedrich", "Mark Mühlau", "Björn Menze", "Daniel Rueckert", "Benedikt Wiestler"], "title": "Optimizing Rank for High-Fidelity Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).", "AI": {"tldr": "本文挑战了基于vanilla MLP的INR无法表达高频信号的观点，提出通过调节网络秩来提升其表现力。", "motivation": "传统观点认为vanilla MLP缺乏表示高频内容的能力。作者质疑这种观点，并提出通过训练期间控制网络的秩可以改善这一问题。", "method": "本文采用Muon等优化器进行高秩、近正交更新，从而调节网络秩，提高INR的学习信号保真度。", "result": "实验证明该方法在自然图像和医学图像以及新颖视图合成等多个领域内均有显著提升，PSNR最高提升了9分贝。", "conclusion": "通过训练时控制MLP的秩可以有效增强其对高频内容的表现力。"}}
{"id": "2512.14364", "pdf": "https://arxiv.org/pdf/2512.14364", "abs": "https://arxiv.org/abs/2512.14364", "authors": ["Sebastian Koch", "Johanna Wald", "Hide Matsuki", "Pedro Hermosilla", "Timo Ropinski", "Federico Tombari"], "title": "Unified Semantic Transformer for 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Project page: https://unite-page.github.io/", "summary": "Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io", "AI": {"tldr": "本文介绍了UNITE，一种用于3D场景理解的统一语义变换器。", "motivation": "当前模型主要针对特定任务设计且难以泛化到新的未见过的场景。因此，研究者提出了一个单一模型以适应多种不同的3D语义任务。", "method": "该方法是一个前馈神经网络UNITE，能够在仅使用RGB图像的情况下直接预测多个语义属性，并通过2D蒸馏、自我监督和新颖的多视角损失训练。", "result": "实验显示，UNITE在几种不同语义任务中达到了最佳性能，在某些情况下甚至超过了特定任务模型以及操作真实3D几何的方法。", "conclusion": "该工作展示了一种新的统一方法来处理各种3D场景理解任务，并证明了其优越性。"}}
{"id": "2512.14361", "pdf": "https://arxiv.org/pdf/2512.14361", "abs": "https://arxiv.org/abs/2512.14361", "authors": ["Nicholas Tagliapietra", "Katharina Ensinger", "Christoph Zimmer", "Osman Mian"], "title": "Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis", "categories": ["cs.LG", "cs.AI", "math.DS"], "comment": "Accepted as Oral at AAAI 2026 Conference", "summary": "Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.", "AI": {"tldr": "提出了一种名为CaDyT的新方法，用于在动力系统中进行因果关系学习。", "motivation": "现有的因果发现方法要么将时间离散化导致对不规则采样数据表现不佳，要么忽略了潜在的因果性。因此，需要一种新的方法来解决这些问题。", "method": "CaDyT 方法基于差分模型，通过精确的高斯过程推理建模连续时间动态，并利用算法马尔科夫条件和最小描述长度原则进行贪婪搜索以确定因果结构。", "result": "实验证明了CaDyT方法在规则采样和不规则采样数据上均优于现有方法，更接近于发现真正的潜在动力学网络。", "conclusion": "通过引入基于差分模型的连续时间动态建模，并使用精确高斯过程推理及贪婪搜索策略，CaDyT在因果关系学习方面取得了显著进展。"}}
{"id": "2512.14360", "pdf": "https://arxiv.org/pdf/2512.14360", "abs": "https://arxiv.org/abs/2512.14360", "authors": ["Ankita Raj", "Kaashika Prajaapat", "Tapan Kumar Gandhi", "Chetan Arora"], "title": "Mimicking Human Visual Development for Learning Robust Image Representations", "categories": ["cs.CV"], "comment": "Accepted to ICVGIP 2025", "summary": "The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.", "AI": {"tldr": "本文提出了一种模仿人类视觉发展的渐进模糊训练方法，以提高CNN的鲁棒性和泛化能力。", "motivation": "现代卷积神经网络在面对输入分布变化时仍难以适应，作者受到人类视觉发展过程的启发，通过模拟婴儿出生后视力逐渐增强的过程来改进CNN模型。", "method": "从高模糊图像开始训练CNN，在训练过程中逐步减少模糊程度。这种方法鼓励网络优先关注全局结构而非高频细节，从而提高对数据分布变化和噪声输入的鲁棒性。", "result": "该方法在CIFAR-10-C和ImageNet-100-C上分别降低了8.30%和4.43%的平均破坏误差。相比静态模糊增强技术，这种方法带来了更稳定的效果，并且与其它数据增强技术如CutMix和MixUp兼容。", "conclusion": "通过模仿人类视觉发展的过程，本文提出的方法有效提高了CNN模型的鲁棒性和泛化能力，在多个数据集上取得了优于标准训练的结果。"}}
{"id": "2512.14358", "pdf": "https://arxiv.org/pdf/2512.14358", "abs": "https://arxiv.org/abs/2512.14358", "authors": ["Qizhi Wang"], "title": "TiCard: Deployable EXPLAIN-only Residual Learning for Cardinality Estimation", "categories": ["cs.AI", "cs.DB"], "comment": "16 pages(/wo references), 4 figures, 10 tables", "summary": "Cardinality estimation is a key bottleneck for cost-based query optimization, yet deployable improvements remain difficult: classical estimators miss correlations, while learned estimators often require workload-specific training pipelines and invasive integration into the optimizer. This paper presents TiCard, a low intrusion, correction-based framework that augments (rather than replaces) a database's native estimator. TiCard learns multiplicative residual corrections using EXPLAIN-only features, and uses EXPLAIN ANALYZE only for offline labels. We study two practical instantiations: (i) a Gradient Boosting Regressor for sub-millisecond inference, and (ii) TabPFN, an in-context tabular foundation model that adapts by refreshing a small reference set without gradient retraining. On TiDB with TPCH and the Join Order Benchmark, in a low-trace setting (263 executions total; 157 used for learning), TiCard improves operator-level tail accuracy substantially: P90 Q-error drops from 312.85 (native) to 13.69 (TiCard-GBR), and P99 drops from 37,974.37 to 3,416.50 (TiCard-TabPFN), while a join-only policy preserves near-perfect median behavior. We position TiCard as an AI4DB building block focused on deployability: explicit scope, conservative integration policies, and an integration roadmap from offline correction to in-optimizer use.", "AI": {"tldr": "本文提出了TiCard，一种低侵入性、基于修正的框架，用于改进数据库中的基数估计。", "motivation": "传统的基数估计算法常常忽略数据间的相关性，而机器学习方法通常需要特定工作负载的训练流程和侵入性的集成。因此，本文旨在开发一个既不需要替换现有估计算法又能够显著改善精度的方法。", "method": "TiCard通过EXPLAIN-only特征进行乘积修正残差的学习，并使用EXPLAIN ANALYZE仅用于离线标签生成。它提供了两个实例：（i）使用梯度提升回归器实现亚毫秒级的推断，以及（ii）使用TabPFN这样的上下文适应性模型来刷新一个小型参考集而无需重新训练。", "result": "在TiDB上运行TPCH和Join Order Benchmark时，在低追踪设置下（总共263次执行；157用于学习），TiCard显著提高了操作级别的尾部精度：P90 Q误差从原生方法的312.85降至TiCard-GBR的13.69，而P99则从37,974.37降低至TiCard-TabPFN的3,416.50。", "conclusion": "本文将TiCard定位为AI4DB的基础模块，并强调了其可部署性：明确的范围、保守集成策略以及从离线修正到在优化器中的使用的发展路线图。"}}
{"id": "2512.14355", "pdf": "https://arxiv.org/pdf/2512.14355", "abs": "https://arxiv.org/abs/2512.14355", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Georg Volk", "Oliver Bringmann"], "title": "CoLD Fusion: A Real-time Capable Spline-based Fusion Algorithm for Collective Lane Detection", "categories": ["cs.RO"], "comment": "Accepted at IEEE IV 2023", "summary": "Comprehensive environment perception is essential for autonomous vehicles to operate safely. It is crucial to detect both dynamic road users and static objects like traffic signs or lanes as these are required for safe motion planning. However, in many circumstances a complete perception of other objects or lanes is not achievable due to limited sensor ranges, occlusions, and curves. In scenarios where an accurate localization is not possible or for roads where no HD maps are available, an autonomous vehicle must rely solely on its perceived road information. Thus, extending local sensing capabilities through collective perception using vehicle-to-vehicle communication is a promising strategy that has not yet been explored for lane detection. Therefore, we propose a real-time capable approach for collective perception of lanes using a spline-based estimation of undetected road sections. We evaluate our proposed fusion algorithm in various situations and road types. We were able to achieve real-time capability and extend the perception range by up to 200%.", "AI": {"tldr": "本文提出了一种基于样条曲线估计的实时集体车道检测融合算法。", "motivation": "在传感器范围有限、遮挡或弯道情况下，完全感知其他车辆和静态物体（如交通标志或车道）是困难的。为了提高自主驾驶的安全性，作者提出了利用车辆间通信进行集体感知的方法来扩展局部感知能力。", "method": "通过使用样条曲线估计未检测到的道路部分，提出了一种实时可行的融合算法以实现车道的集体感知。", "result": "该方法在多种道路类型和场景中进行了评估，并且实现了接近200%的感知范围扩展及实时性能。", "conclusion": "通过车辆间通信进行的集体车道检测不仅可以提高自主驾驶的安全性，而且可以实现实时能力。"}}
{"id": "2512.14354", "pdf": "https://arxiv.org/pdf/2512.14354", "abs": "https://arxiv.org/abs/2512.14354", "authors": ["Kanglong Fan", "Yunqiao Yang", "Chen Ma"], "title": "Enhancing Interpretability for Vision Models via Shapley Value Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI2026", "summary": "Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.", "AI": {"tldr": "提出了一种新的自我解释框架，通过在训练过程中将Shapley值估计作为辅助任务来提高视觉模型的可解释性。", "motivation": "现有解释方法无法准确反映深度神经网络的行为，并且专门设计的自我解释模型会牺牲性能和兼容性。为了解决这些问题，该研究提出了一种新的框架。", "method": "在训练过程中整合Shapley值估计作为辅助任务，使模型预测分数公平分配给图像块，以确保解释与模型决策逻辑一致；通过轻微结构修改提升可解释性，并保持模型性能和兼容性。", "result": "实验表明该方法在多个基准上达到了最先进的可解释性水平。", "conclusion": "提出的框架能够提高视觉模型的可解释性，同时保持其性能和兼容性。"}}
{"id": "2512.14352", "pdf": "https://arxiv.org/pdf/2512.14352", "abs": "https://arxiv.org/abs/2512.14352", "authors": ["Kaizhe Zhang", "Yijie Zhou", "Weizhan Zhang", "Caixia Yan", "Haipeng Du", "yugui xie", "Yu-Hui Wen", "Yong-Jin Liu"], "title": "HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis", "categories": ["cs.CV", "cs.CG"], "comment": "11 pages, 9 figures", "summary": "Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.", "AI": {"tldr": "提出了一种混合高斯点阵(HGS)框架，用于动态视图合成。", "motivation": "现有的动态新型视图合成方法虽然有所进步，但由于模型复杂度和参数冗余导致效率低下，不适合实时应用特别是资源受限的设备。因此需要一种更高效的方法来减少冗余参数。", "method": "通过静态-动态分解(SDD)策略，利用径向基函数(RBF)建模高斯原语，并引入两阶段训练策略以提高边界处的时间一致性。", "result": "该方法可将模型大小减小多达98%，在RTX3090上实现高达125FPS的实时渲染，在4K分辨率下，且能维持更高的帧率和视觉保真度。", "conclusion": "HGS框架提供了一种高效的动态视图合成方案，不仅减少了模型大小和参数冗余，还能保持高质量的渲染效果和高频率细节处理能力。"}}
{"id": "2512.14350", "pdf": "https://arxiv.org/pdf/2512.14350", "abs": "https://arxiv.org/abs/2512.14350", "authors": ["Henrik Hose", "Paul Brunzema", "Alexander von Rohr", "Alexander Gräfe", "Angela P. Schoellig", "Sebastian Trimpe"], "title": "Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Approximate model-predictive control (AMPC) aims to imitate an MPC's behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC's optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.", "AI": {"tldr": "本文提出了一种使用贝叶斯优化自动调整AMPC参数的方法，以提高其在硬件上的性能。", "motivation": "传统AMPC方法需要重新生成数据集并重新训练神经网络来微调控制参数，这既耗时又复杂。因此，提出了使用贝叶斯优化来自动生成和优化这些参数。", "method": "通过结合模型预测控制与基于实验数据的直接局部学习，利用贝叶斯优化自动调整AMPC策略中的参数。", "result": "该方法在硬件试验中展示了对倒立摆和欠驱动平衡独轮车的控制性能优于传统AMPC。", "conclusion": "证明了所提出的使用贝叶斯优化的方法能够有效地自适应调整AMPC以应对新的系统实例并适应难以直接实现的成本函数。"}}
{"id": "2512.14349", "pdf": "https://arxiv.org/pdf/2512.14349", "abs": "https://arxiv.org/abs/2512.14349", "authors": ["Federico Califano", "Camilla Rota", "Riccardo Zanella", "Antonio Franchi"], "title": "A Geometric Task-Space Port-Hamiltonian Formulation for Redundant Manipulators", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "We present a novel geometric port-Hamiltonian formulation of redundant manipulators performing a differential kinematic task $η=J(q)\\dot{q}$, where $q$ is a point on the configuration manifold, $η$ is a velocity-like task space variable, and $J(q)$ is a linear map representing the task, for example the classical analytic or geometric manipulator Jacobian matrix. The proposed model emerges from a change of coordinates from canonical Hamiltonian dynamics, and splits the standard Hamiltonian momentum variable into a task-space momentum variable and a null-space momentum variable. Properties of this model and relation to Lagrangian formulations present in the literature are highlighted. Finally, we apply the proposed model in an \\textit{Interconnection and Damping Assignment Passivity-Based Control} (IDA-PBC) design to stabilize and shape the impedance of a 7-DOF Emika Panda robot in simulation.", "AI": {"tldr": "提出了一种冗余机械臂执行微分运动任务的几何端口哈密顿模型", "motivation": "为了更好地理解和控制冗余机械臂在特定任务空间中的动力学行为，通过改变坐标从经典哈密顿动力学到新的几何端口哈密顿形式，以分离标准哈密顿动量变量到任务空间动量和零空间动量。", "method": "基于变分原理及几何结构设计了一种用于冗余机械臂的新模型，并应用到了IDA-PBC控制策略中来实现稳定性和阻抗塑形的设计目标", "result": "在7自由度Emika Panda机器人的仿真中验证了所提出的模型的有效性，实现了任务空间中的稳定性与柔顺性的良好平衡。", "conclusion": "新提出的几何端口哈密顿框架为冗余机械臂提供了一种有效的动力学描述和控制策略，有助于进一步提高机器人系统的性能和适应性"}}
{"id": "2512.14341", "pdf": "https://arxiv.org/pdf/2512.14341", "abs": "https://arxiv.org/abs/2512.14341", "authors": ["Jie Zhang", "Shuai Dong", "Shiguang Shan", "Xilin Chen"], "title": "Towards Transferable Defense Against Malicious Image Edits", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.", "AI": {"tldr": "本文提出了一种新的双模态框架TDAE，通过视觉和文本优化增强图像抵御恶意编辑的能力。", "motivation": "现有方法在跨模型评估中存在转移性问题，即不能有效抵抗未知的编辑模型。为了提高这种转移性，研究者们设计了新的防御机制。", "method": "提出了FlatGrad Defense Mechanism (FDM)和Dynamic Prompt Defense (DPD)，通过优化图像和文本来增强免疫性能，并实现跨模型迁移。", "result": "实验结果表明，在内模型与跨模型评估中，TDAE能够有效减轻恶意编辑的影响，达到最先进的防御效果。", "conclusion": "该论文提出了一种新的方法，解决了现有防御技术在转移性上的不足，展示了强大的抵抗恶意图像编辑的能力。"}}
{"id": "2512.14340", "pdf": "https://arxiv.org/pdf/2512.14340", "abs": "https://arxiv.org/abs/2512.14340", "authors": ["Aleksi Karhunen", "Teemu Hakala", "Väinö Karjalainen", "Eija Honkavaara"], "title": "Field evaluation and optimization of a lightweight lidar-based UAV navigation system for dense boreal forest environments", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The interest in the usage of uncrewed aerial vehicles (UAVs) for forest applications has increased in recent years. While above-canopy flight has reached a high level of autonomy, navigating under-canopy remains a significant challenge. The use of autonomous UAVs could reduce the burden of data collection, which has motivated the development of numerous solutions for under-canopy autonomous flight. However, the experiments conducted in the literature and their reporting lack rigor. Very rarely, the density and the difficulty of the test forests are reported, or multiple flights are flown, and the success rate of those flights is reported. The aim of this study was to implement an autonomously flying quadrotor based on a lightweight lidar using openly available algorithms and test its behavior in real forest environments. A set of rigorous experiments was conducted with a quadrotor prototype utilizing the IPC path planner and LTA-OM SLAM algorithm. Based on the results of the first 33 flights, the original system was further enhanced. With the optimized system, 60 flights were performed, resulting in a total of 93 test flights. The optimized system performed significantly better in terms of reliability and flight mission completion times, achieving success rates of 12/15 in a medium-density forest and 15/15 in a dense forest, at a target flight velocity of 1 m/s. At a target flight velocity of 2 m/s, it had a success rate of 12/15 and 5/15, respectively. Furthermore, a standardized testing setup and evaluation criteria were proposed, enabling consistent performance comparisons of autonomous under-canopy UAV systems, enhancing reproducibility, guiding system improvements, and accelerating progress in forest robotics.", "AI": {"tldr": "本文评估并优化了一种基于轻量级激光雷达的四旋翼无人机导航系统，以适应密集森林环境。", "motivation": "近年来人们对无人机在林业应用中的兴趣增加。尽管高空自主飞行已达到较高水平，但林下自主航行仍是一个重大挑战。因此开发了多种解决方案来解决这一问题，但实验缺乏严谨性。本文旨在实现并测试一种基于轻量级激光雷达的四旋翼自主导航系统，并提出了一套标准化测试和评估标准。", "method": "利用IPC路径规划器和LTA-OM SLAM算法进行了一系列严格的飞行试验，初始33次飞行后对系统进行了优化。使用该优化后的系统完成了60次飞行任务，总共93个测试飞行。", "result": "在中密度森林环境中以1m/s的速度飞行时成功率为80%，而在密集森林环境中的成功率达到了100%；当速度提升到2m/s时，在相同环境下分别取得80%和33.3%的成功率。", "conclusion": "本文提出的系统通过优化在自主性、任务完成度及可靠性方面表现出色，并提供了一套标准化的测试框架，促进了林业无人机技术的进步。"}}
{"id": "2512.14336", "pdf": "https://arxiv.org/pdf/2512.14336", "abs": "https://arxiv.org/abs/2512.14336", "authors": ["Jooyeol Yun", "Jaegul Choo"], "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure", "categories": ["cs.CV"], "comment": "yeolj00.github.io/personal-projects/vector-prism", "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.", "AI": {"tldr": "介绍了一种框架，通过恢复SVG的语义结构来实现更连贯的动画。", "motivation": "现有视觉语言模型在处理SVG时容易出错，因为这些模型无法理解哪些元素应该一起移动。因此，需要一种方法来改进这一过程。", "method": "使用统计聚合多个弱部分预测的方法稳定地从嘈杂的预测中推断语义，并通过重组SVG使其按语义分组，从而提高VLM生成动画的一致性。", "result": "实验表明所提出的方法相比现有方法有了显著改进，证明了语义恢复是实现稳健SVG动画的关键步骤。", "conclusion": "该框架可以有效地利用视觉语言模型进行更连贯的SVG动画，支持更可解释的交互。"}}
{"id": "2512.14333", "pdf": "https://arxiv.org/pdf/2512.14333", "abs": "https://arxiv.org/abs/2512.14333", "authors": ["Jie Zhang", "Shuai Dong", "Shiguang Shan", "Xilin Chen"], "title": "Dual Attention Guided Defense Against Malicious Edits", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "11 pages, 7 figures", "summary": "Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.", "AI": {"tldr": "提出了一种名为DANP的双注意力引导噪声扰动免疫方法，旨在通过添加几乎不可察觉的干扰来抵御恶意图像编辑。", "motivation": "随着文本到图像扩散模型的进步，出现了利用文本提示进行图像编辑的可能性，同时也带来了制造欺骗性或有害内容的风险。现有的防御措施效果有限，因此需要更有效的保护手段。", "method": "DANP方法在多个时间步上操作，通过动态阈值生成掩码来区分相关和不相关的文本区域，并调整注意力权重以误导模型生成错误的编辑结果并保护预期目标。", "result": "实验表明，DANP方法能显著提高对恶意图像编辑的抵抗力，在对抗攻击方面表现出色，达到了最先进的性能水平。", "conclusion": "通过结合双注意力引导和噪声预测干扰机制，DANP为文本指导的图像编辑提供了一种有效的免疫策略。"}}
{"id": "2512.14332", "pdf": "https://arxiv.org/pdf/2512.14332", "abs": "https://arxiv.org/abs/2512.14332", "authors": ["Yannis Belkhiter", "Seshu Tirupathi", "Giulio Zizzo", "John D. Kelleher"], "title": "Step-Tagging: Toward controlling the generation of Language Reasoning Models through step monitoring", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The field of Language Reasoning Models (LRMs) has been very active over the past few years with advances in training and inference techniques enabling LRMs to reason longer, and more accurately. However, a growing body of studies show that LRMs are still inefficient, over-generating verification and reflection steps. To address this challenge, we introduce the Step-Tagging framework, a lightweight sentence-classifier enabling real-time annotation of the type of reasoning steps that an LRM is generating. To monitor reasoning behaviors, we introduced ReasonType: a novel taxonomy of reasoning steps. Building on this framework, we demonstrated that online monitoring of the count of specific steps can produce effective interpretable early stopping criteria of LRM inferences. We evaluate the Step-tagging framework on three open-source reasoning models across standard benchmark datasets: MATH500, GSM8K, AIME and non-mathematical tasks (GPQA and MMLU-Pro). We achieve 20 to 50\\% token reduction while maintaining comparable accuracy to standard generation, with largest gains observed on more computation-heavy tasks. This work offers a novel way to increase control over the generation of LRMs, and a new tool to study behaviors of LRMs.", "AI": {"tldr": "提出Step-Tagging框架，通过实时监控语言推理模型生成的步骤类型来控制其行为。", "motivation": "解决语言推理模型过度产生验证和反思步骤的问题，提高模型效率。", "method": "引入ReasonType分类法来标注推理步骤，并利用Step-Tagging框架进行在线监测以设定有效的早期停止准则。", "result": "在多个基准数据集上实现20到50%的标记减少，同时保持与标准生成相仿的准确性，在计算密集型任务中效果更佳。", "conclusion": "提供了一种新的方法来增强对语言推理模型生成过程的控制，并引入了一个研究这些模型行为的新工具。"}}
{"id": "2512.14331", "pdf": "https://arxiv.org/pdf/2512.14331", "abs": "https://arxiv.org/abs/2512.14331", "authors": ["Rishabh Dev Yadav", "Avirup Das", "Hongyu Song", "Samuel Kaski", "Wei Pan"], "title": "ARCADE: Adaptive Robot Control with Online Changepoint-Aware Bayesian Dynamics Learning", "categories": ["cs.RO"], "comment": null, "summary": "Real-world robots must operate under evolving dynamics caused by changing operating conditions, external disturbances, and unmodeled effects. These may appear as gradual drifts, transient fluctuations, or abrupt shifts, demanding real-time adaptation that is robust to short-term variation yet responsive to lasting change. We propose a framework for modeling the nonlinear dynamics of robotic systems that can be updated in real time from streaming data. The method decouples representation learning from online adaptation, using latent representations learned offline to support online closed-form Bayesian updates. To handle evolving conditions, we introduce a changepoint-aware mechanism with a latent variable inferred from data likelihoods that indicates continuity or shift. When continuity is likely, evidence accumulates to refine predictions; when a shift is detected, past information is tempered to enable rapid re-learning. This maintains calibrated uncertainty and supports probabilistic reasoning about transient, gradual, or structural change. We prove that the adaptive regret of the framework grows only logarithmically in time and linearly with the number of shifts, competitive with an oracle that knows timings of shift. We validate on cartpole simulations and real quadrotor flights with swinging payloads and mid-flight drops, showing improved predictive accuracy, faster recovery, and more accurate closed-loop tracking than relevant baselines.", "AI": {"tldr": "提出了一种适应性机器人控制框架，用于在线实时动态调整。", "motivation": "由于操作条件的变化、外部干扰和未建模效应，现实世界中的机器人的工作环境是不断变化的。这些因素可能会导致渐进式漂移、瞬时波动或突变，需要一种能够应对这种变化且快速响应持久改变的方法。", "method": "该方法提出了一个框架来在线实时更新非线性动态模型，并引入了一个基于数据似然性的隐变量机制，用于检测连续还是转变。当认为是连续情况时，证据会积累以改进预测；如果检测到突变，则过去的信息会被抑制以便于快速重新学习。", "result": "实验结果表明，在模拟和实际飞行测试中，该方法比相关基准提高了预测精度、恢复速度和闭环跟踪准确性。", "conclusion": "通过引入一种在线适应性机制来处理不断变化的操作条件，实现了更准确的预测和更快的响应时间。"}}
{"id": "2512.14330", "pdf": "https://arxiv.org/pdf/2512.14330", "abs": "https://arxiv.org/abs/2512.14330", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Criminal Liability in AI-Enabled Autonomous Vehicles: A Comparative Study", "categories": ["cs.CY", "cs.AI", "cs.CR"], "comment": "Published in Journal of University Institute of Legal Studies, Vol. 18, Issue 1, pp. 57-78, 2025", "summary": "AI revolutionizes transportation through autonomous vehicles (AVs) but introduces complex criminal liability issues regarding infractions. This study employs a comparative legal analysis of primary statutes, real-world liability claims, and academic literature across the US, Germany, UK, China, and India; jurisdictions selected for their technological advancement and contrasting regulatory approaches. The research examines the attribution of human error, AI moral agency, and the identification of primary offenders in AV incidents. Findings reveal fragmented regulatory landscapes: India and the US rely on loose networks of state laws, whereas the UK enacted the pioneering Automated and Electric Vehicles Act 2018. Germany enforces strict safety standards, distinguishing liability based on the vehicle's operating mode, while China similarly aims for a stringent liability regime. The study concludes that globally harmonized legal standards are essential to foster technological innovation while ensuring minimum risk and clear liability attribution.", "AI": {"tldr": "研究通过比较分析主要法律条文、实际责任索赔案例及学术文献，探讨自动驾驶汽车（AV）引发的犯罪法律责任问题。", "motivation": "随着AI技术的发展，自动驾驶车辆引入了复杂的刑事责任问题，特别是在违规行为的责任归属上存在挑战，因此需要跨法域的研究来明确相关规则。", "method": "采用比较法律分析方法研究美国、德国、英国、中国和印度五个国家的法律条文及实际案例，并结合学术文献进行综合评估。", "result": "发现不同司法管辖区对AI责任的认定存在较大差异：例如，印度和美国依赖松散的州法律法规；而英国则通过《2018年自动化和电动汽车法案》开创先河。德国实行严格的车辆安全标准以区分责任归属，中国亦追求类似的严格责任制度。", "conclusion": "在全球范围内制定统一的法律标准对于促进技术革新同时确保最低风险及明确的责任归属至关重要。"}}
{"id": "2512.14329", "pdf": "https://arxiv.org/pdf/2512.14329", "abs": "https://arxiv.org/abs/2512.14329", "authors": ["Yanning Dai", "Chenyu Tang", "Ruizhi Zhang", "Wenyu Yang", "Yilan Zhang", "Yuhui Wang", "Junliang Chen", "Xuhang Chen", "Ruimou Xie", "Yangyue Cao", "Qiaoying Li", "Jin Cao", "Tao Li", "Hubin Zhao", "Yu Pan", "Arokia Nathan", "Xin Gao", "Peter Smielewski", "Shuo Gao"], "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data", "categories": ["cs.CE", "cs.AI"], "comment": "26 pages, 6 figures", "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.", "AI": {"tldr": "本文开发了一种数据物理混合生成模型，用于预测中风患者的特定任务行走能力。", "motivation": "当前的评估只能提供静态的损伤评分，并不能确定患者是否可以安全地执行如斜坡步行或楼梯攀爬等具体任务。因此需要一种动态预测方法来个性化康复策略。", "method": "该模型结合了可穿戴传感器运动学、比例导数物理控制器、健康动作图谱和目标条件下的深度强化学习，生成特定患者的行走模拟。", "result": "在11名中风患者中，个性化控制方案保持了个性化的步态模式，并且提高了关节角度和终端精度。多中心试点研究显示使用此模型指导任务选择的临床医生获得更大康复收益。", "conclusion": "该生成预测框架可以增强中风后步态康复中的临床决策制定，并为动态个性化运动恢复策略提供了一个模板"}}
{"id": "2512.14320", "pdf": "https://arxiv.org/pdf/2512.14320", "abs": "https://arxiv.org/abs/2512.14320", "authors": ["Shuai Dong", "Jie Zhang", "Guoying Zhao", "Shiguang Shan", "Xilin Chen"], "title": "Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "11 pages, 4 figures", "summary": "Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.", "AI": {"tldr": "本文提出了一种新的方法SIFM，用于通过干扰扩散模型的中间特征来保护图像免受恶意编辑的影响，并引入了ISR指标以评估免疫效果。", "motivation": "现有文本引导的图像编辑技术存在被滥用的风险，现有的评估免疫成功的方法仅关注输出图像与原图之间的视觉差异，忽略了真正需要的是破坏语义上的对齐以及引起感知退化。本文旨在提出一种新的方法来更准确地保护图像免受恶意攻击。", "method": "提出了SIFM（协同中间特征操控）方法，通过两个目标同时操作扩散模型的中间特征：最大化的偏离原编辑轨迹以打破与期望编辑的语义对齐，并最小化特征范数以引起感知退化。此外，还引入了ISR（免疫成功率）指标来量化保护效果。", "result": "实验表明，SIFM在防护视觉内容免受基于扩散模型的恶意操纵方面达到了最先进的性能。", "conclusion": "本文提出的SIFM方法和ISR评估指标为图像编辑免疫提供了一种新的视角，并展示了其优越的效能。"}}
{"id": "2512.14319", "pdf": "https://arxiv.org/pdf/2512.14319", "abs": "https://arxiv.org/abs/2512.14319", "authors": ["Marc Barthelemy"], "title": "Not all Chess960 positions are equally complex", "categories": ["physics.soc-ph", "cond-mat.dis-nn", "cs.HC"], "comment": "11 pages, 7 figures", "summary": "We analyze strategic complexity across all 960 Chess960 (Fischer Random Chess) starting positions. Stockfish evaluations show a near-universal first-move advantage for White ($\\langle E \\rangle = +0.30 \\pm 0.14$ pawns), indicating that the advantage conferred by moving first is a robust structural feature of the game. To quantify decision difficulty, we introduce an information-based measure $S(n)$ describing the cumulative information required to identify optimal moves over the first $n$ plies. This measure decomposes into contributions from White and Black, $S_W$ and $S_B$, yielding a total opening complexity $S_{\\mathrm{tot}} = S_W + S_B$ and a decision asymmetry $A=S_B-S_W$. Across the ensemble, $S_{\\mathrm{tot}}$ varies by a factor of three, while $A$ spans from $-2.5$ to $+1.8$ bits, showing that some openings burden White and others Black. The mean $\\langle A \\rangle = -0.25$ bits indicates a slight tendency for White to face harder opening decisions. Standard chess (position \\#518, \\texttt{RNBQKBNR}) exhibits above-average asymmetry (91st percentile) but typical overall complexity (47th percentile). The most complex opening is \\#226 (\\texttt{BNRQKBNR}), whereas \\#198 (\\texttt{QNBRKBNR})is the most balanced, with both evaluation and asymmetry near zero. These results reveal a highly heterogeneous Chess960 landscape in which small rearrangements of the back-rank pieces can significantly alter strategic depth and competitive fairness. Remarkably, the classical starting position-despite centuries of cultural selection-lies far from the most balanced configuration.", "AI": {"tldr": "本文分析了所有960种Chess960开局位置的战略复杂度，引入了一个基于信息量的衡量标准来评估每个开局决策难度。", "motivation": "通过量化各棋局开始时的战略复杂性和公平性，揭示Chess960的不同开局配置如何影响游戏深度和平衡性。", "method": "使用Stockfish评估器分析了所有可能的960种Chess960开局位置，并引入了一个基于信息量的衡量标准来评估每个开局决策难度。该措施分为白方和黑方的信息贡献，以量化总开局复杂度及决策偏移情况。", "result": "研究发现，不同开局配置的战略复杂性和公平性差异显著；标准国际象棋开局（位置518）具有较高的决策不对称性，但总体复杂度处于平均水平。最复杂的开局是编号226的位置，而最平衡的则是编号198的位置。", "conclusion": "研究表明，在Chess960中，即使是轻微的后行棋子调整也能显著改变游戏的战略深度和公平性；尽管传统国际象棋经过了数百年的选择优化，但并不一定是最平衡的开局配置。"}}
{"id": "2512.14312", "pdf": "https://arxiv.org/pdf/2512.14312", "abs": "https://arxiv.org/abs/2512.14312", "authors": ["Akila Premarathna", "Kanishka Hewageegana", "Garcia Andarcia Mariangel"], "title": "From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 9 figures", "summary": "In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.", "AI": {"tldr": "该论文旨在使用卫星图像在中东和北非地区精确识别废水处理厂，通过对比YOLOv8与几种视觉语言模型（VLMs）的零样本和少量样本检测性能。", "motivation": "传统的YOLOv8分割方法需要大量的手动标注，效率较低。研究表明，视觉语言模型可以作为更有效的替代方案来实现相同或更好的结果，并且通过内在推理和注释减少人工参与。", "method": "使用政府数据集中来自埃及、沙特阿拉伯和阿联酋的高分辨率卫星图像训练YOLOv8；对比LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini和Pixtral 12B（Mistral）等VLMs，利用专家提示识别废水处理厂组件，并输出包含置信度与描述的JSON结果。", "result": "在零样本评估中，多种视觉语言模型优于YOLOv8的真实阳性率，其中Gemma-3表现最佳。实验表明，特别是在零样本情况下，这些VLMs可以替代YOLOv8进行无标注、高效的废水处理厂分类，有助于大规模的远程感应。", "conclusion": "研究确认了视觉语言模型在不需要大量人工注释的情况下，在零样本和少量样本检测中能够有效识别废水处理设施。这证明了它们在环境监测中的潜在应用价值。"}}
{"id": "2512.14309", "pdf": "https://arxiv.org/pdf/2512.14309", "abs": "https://arxiv.org/abs/2512.14309", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.", "AI": {"tldr": "PSMamba是一种逐步的无监督学习框架，用于植物疾病识别。", "motivation": "现有的无监督学习方法大多注重全局对齐，难以捕捉植物病害图像中的层级、多尺度病变模式。为了弥补这一不足，提出了PSMamba。", "method": "PSMamba采用Vision Mamba进行高效的序列建模，并结合了双学生分层蒸馏策略。该框架包含一个共享的全局教师和两个专门的学生：一个处理中等规模视图以捕捉病斑分布和叶脉结构；另一个关注局部视图，捕捉诸如纹理不规则性和早期阶段病变等细粒度线索。", "result": "实验证明PSMamba在三个基准数据集上表现优于最先进的无监督学习方法，在域偏移和细粒度场景中表现出色。", "conclusion": "PSMamba通过结合全局教师与双学生结构，能够更好地捕捉植物病害图像中的多尺度病变模式，提高了识别准确性。"}}
{"id": "2512.14297", "pdf": "https://arxiv.org/pdf/2512.14297", "abs": "https://arxiv.org/abs/2512.14297", "authors": ["Agrippina Mwangi", "León Navarro-Hilfiker", "Lukasz Brewka", "Mikkel Gryning", "Elena Fumagalli", "Madeleine Gibescu"], "title": "A Threshold-Triggered Deep Q-Network-Based Framework for Self-Healing in Autonomic Software-Defined IIoT-Edge Networks", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.PF", "hep-ex"], "comment": null, "summary": "Stochastic disruptions such as flash events arising from benign traffic bursts and switch thermal fluctuations are major contributors to intermittent service degradation in software-defined industrial networks. These events violate IEC~61850-derived quality-of-service requirements and user-defined service-level agreements, hindering the reliable and timely delivery of control, monitoring, and best-effort traffic in IEC~61400-25-compliant wind power plants. Failure to maintain these requirements often results in delayed or lost control signals, reduced operational efficiency, and increased risk of wind turbine generator downtime. To address these challenges, this study proposes a threshold-triggered Deep Q-Network self-healing agent that autonomically detects, analyzes, and mitigates network disruptions while adapting routing behavior and resource allocation in real time. The proposed agent was trained, validated, and tested on an emulated tri-clustered switch network deployed in a cloud-based proof-of-concept testbed. Simulation results show that the proposed agent improves disruption recovery performance by 53.84% compared to a baseline shortest-path and load-balanced routing approach and outperforms state-of-the-art methods, including the Adaptive Network-based Fuzzy Inference System by 13.1% and the Deep Q-Network and traffic prediction-based routing optimization method by 21.5%, in a super-spine leaf data-plane architecture. Additionally, the agent maintains switch thermal stability by proactively initiating external rack cooling when required. These findings highlight the potential of deep reinforcement learning in building resilience in software-defined industrial networks deployed in mission-critical, time-sensitive application scenarios.", "AI": {"tldr": "本文提出了一种基于深度Q网络的阈值触发自愈框架，用于解决软件定义工业物联网边缘网络中的间歇性服务质量降级问题。", "motivation": "闪存事件和交换机热波动等随机干扰导致服务降级，违反了IEC61850规定的质量要求和服务水平协议。这些故障可能导致控制信号延迟或丢失，降低风电场的运营效率并增加风力发电机停机的风险。", "method": "本文提出了一种阈值触发的深度Q网络自愈代理，该代理自主检测、分析和缓解网络中断，并实时调整路由行为和资源分配。实验在云端概念验证测试平台上的模拟三集群交换网络上进行。", "result": "仿真结果显示，所提出的代理提高了53.84%的干扰恢复性能，比基线最短路径负载均衡路由方法更优，且优于现有的自适应模糊推理系统方法13.1%，以及深度Q网络和流量预测路由优化法21.5%。", "conclusion": "这些发现强调了在软件定义工业网络部署中应用深度强化学习以构建弹性系统的潜力。"}}
{"id": "2512.14291", "pdf": "https://arxiv.org/pdf/2512.14291", "abs": "https://arxiv.org/abs/2512.14291", "authors": ["Jiayan Cui", "Zhihan Yang", "Naihan Li", "Jiankun Tian", "Xingyu Ma", "Yi Zhang", "Guangyu Chen", "Runxuan Yang", "Yuqing Cheng", "Yizhi Zhou", "Guochen Yu", "Xiaotao Gu", "Jie Tang"], "title": "GLM-TTS Technical Report", "categories": ["cs.SD"], "comment": null, "summary": "This work proposes GLM-TTS, a production-level TTS system designed for efficiency, controllability, and high-fidelity speech generation. GLM-TTS follows a two-stage architecture, consisting of a text-to-token autoregressive model and a token-to-waveform diffusion model. With only 100k hours of training data, GLM-TTS achieves state-of-the-art performance on multiple open-source benchmarks. To meet production requirements, GLM-TTS improves speech quality through an optimized speech tokenizer with fundamental frequency constraints and a GRPO-based multi-reward reinforcement learning framework that jointly optimizes pronunciation, speaker similarity, and expressive prosody. In parallel, the system enables efficient and controllable deployment via parameter-efficient LoRA-based voice customization and a hybrid phoneme-text input scheme that provides precise pronunciation control. Our code is available at https://github.com/zai-org/GLM-TTS. Real-time speech synthesis demos are provided via Z.ai (audio.z.ai), the Zhipu Qingyan app/web (chatglm.cn).", "AI": {"tldr": "GLM-TTS是一种高效、可控且高质量的文本到语音(TTS)系统，采用两阶段架构并在生产环境中进行优化。", "motivation": "提高TTS系统的效率和质量，满足生产需求。通过使用改进的声学模型和训练策略来提升语音合成的质量。", "method": "GLM-TTS采用两阶段架构：文本到令牌的自回归模型以及令牌到波形的扩散模型；并优化了语音标记器以提高音质，同时引入了多奖励强化学习框架进行联合优化。此外，还提供了参数高效的LoRA方法来定制声音。", "result": "GLM-TTS在多种开源基准测试中达到了最先进的性能水平，并且在生产部署方面表现出色。", "conclusion": "该研究展示了如何设计并实现一个高效、高质量的TTS系统，适用于实际应用。"}}
{"id": "2512.14288", "pdf": "https://arxiv.org/pdf/2512.14288", "abs": "https://arxiv.org/abs/2512.14288", "authors": ["Georgios Bouchouras", "Dimitrios Doumanas", "Andreas Soularidis", "Konstantinos Kotis", "George A. Vouros"], "title": "Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting", "categories": ["cs.AI"], "comment": null, "summary": "This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration. Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy. X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts. Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies. Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.", "AI": {"tldr": "探索大型语言模型在帕金森病监测和预警本体工程中的应用，评估其自主构建能力和与人类协作的效果。", "motivation": "探讨LLM是否能独立生成全面的PD监测和警报本体，并通过人机合作改善这一过程。", "method": "采用OS、CoT提示方法以及X-HCOME、SimX-HCOME+等策略，研究LLM在自主构建本体中的效果及其与人类协作提高本体准确性和完整性的能力。", "result": "LLM可以通过OS和CoT提示生成初步的PD监测和警报本体，但需大量人工修正才能达到较高精度。结合人机合作方法（如X-HCOME）显著提高了本体的质量，而SimX-HCOME+进一步优化了这一过程。", "conclusion": "人类与LLM的合作在复杂领域中的本体工程具有巨大潜力，并为未来的PD监测和警报系统开发提供了新方向。"}}
{"id": "2512.14284", "pdf": "https://arxiv.org/pdf/2512.14284", "abs": "https://arxiv.org/abs/2512.14284", "authors": ["Zhibing Li", "Mengchen Zhang", "Tong Wu", "Jing Tan", "Jiaqi Wang", "Dahua Lin"], "title": "SS4D: Native 4D Generative Model via Structured Spacetime Latents", "categories": ["cs.CV"], "comment": "ToG(Siggraph Asia 2025)", "summary": "We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion", "AI": {"tldr": "提出SS4D模型，直接从单目视频生成动态的三维对象。", "motivation": "解决现有方法通过优化三维或视频生成模型构建四维表示的问题，提供高保真度、时间一致性和结构一致性。", "method": "利用压缩的时空潜在变量集进行训练；采用预训练的一张图像到3D模型来保持空间一致性；引入专用的时间层以保证时间一致性；使用因子化的4D卷积和时间下采样块来压缩潜在序列，支持长时间视频序列的有效训练和推理。", "result": "实现了高保真度、时间一致性和结构一致性。", "conclusion": "SS4D模型展示了在生成动态3D对象方面的优越性能，特别是在处理复杂场景中的遮挡问题时。"}}
{"id": "2512.14278", "pdf": "https://arxiv.org/pdf/2512.14278", "abs": "https://arxiv.org/abs/2512.14278", "authors": ["Marvin Kopka", "Azeem Majeed", "Gabriella Spinelli", "Austen El-Osta", "Markus Feufel"], "title": "The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.", "AI": {"tldr": "开发并验证了评估用户对AI生成健康建议信任度的TAIGHA量表及其简化版TAIGHA-S。", "motivation": "现有的工具无法专门测量用户对AI生成的健康建议的信任度，因此需要一个新的标准化仪器来填补这一空白。", "method": "通过理论基础设计题目，专家内容验证，普通参与者面部验证和心理计量学验证385名英国参与者的数据。最终保留了28个题目并缩减为10项，通过CFA确认两因素模型，内部一致性高（α=0.95）。", "result": "TAIGHA量表具有优秀的内容有效度（S-CVI/Ave=0.99），结构良好（CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03）。简化版TAIGHA-S与完整版本有高相关性(r=0.96)，并且具有良好的可靠性(α=0.88)。", "conclusion": "TAIGHA和TAIGHA-S是评估用户对AI生成健康建议的信任度和不信任度的有效工具，有助于全面评价AI干预措施，并且简化版适用于时间受限的场景。"}}
{"id": "2512.14277", "pdf": "https://arxiv.org/pdf/2512.14277", "abs": "https://arxiv.org/abs/2512.14277", "authors": ["Panayiotis Smeros", "Vincent Emonet", "Ruijie Wang", "Ana-Claudia Sima", "Tarcisio Mendes de Farias"], "title": "SPARQL-LLM: Real-Time SPARQL Query Generation from Natural Language Questions", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "17 pages, 8 figures, 1 table. Under Review", "summary": "The advent of large language models is contributing to the emergence of novel approaches that promise to better tackle the challenge of generating structured queries, such as SPARQL queries, from natural language. However, these new approaches mostly focus on response accuracy over a single source while ignoring other evaluation criteria, such as federated query capability over distributed data stores, as well as runtime and cost to generate SPARQL queries. Consequently, they are often not production-ready or easy to deploy over (potentially federated) knowledge graphs with good accuracy. To mitigate these issues, in this paper, we extend our previous work and describe and systematically evaluate SPARQL-LLM, an open-source and triplestore-agnostic approach, powered by lightweight metadata, that generates SPARQL queries from natural language text. First, we describe its architecture, which consists of dedicated components for metadata indexing, prompt building, and query generation and execution. Then, we evaluate it based on a state-of-the-art challenge with multilingual questions, and a collection of questions from three of the most prevalent knowledge graphs within the field of bioinformatics. Our results demonstrate a substantial increase of 24% in the F1 Score on the state-of-the-art challenge, adaptability to high-resource languages such as English and Spanish, as well as ability to form complex and federated bioinformatics queries. Furthermore, we show that SPARQL-LLM is up to 36x faster than other systems participating in the challenge, while costing a maximum of $0.01 per question, making it suitable for real-time, low-cost text-to-SPARQL applications. One such application deployed over real-world decentralized knowledge graphs can be found at https://www.expasy.org/chat.", "AI": {"tldr": "本文介绍了SPARQL-LLM，一种通过自然语言生成SPARQL查询的开源方法。", "motivation": "现有的生成结构化查询的方法大多关注单一数据源上的响应准确性，并忽视了联邦查询能力、运行时间和成本等其他评估标准。因此，这些方法往往无法满足生产环境的需求或难以部署在知识图谱上。", "method": "SPARQL-LLM采用一种轻量级的元数据方案，包括专用组件用于元数据索引、提示构建和查询生成与执行。它针对多语言挑战进行了评估，并通过生物信息学中常用的三个知识图谱进行测试。", "result": "实验结果显示，在最新的多语言挑战上F1分数提高了24%，适应高资源语言如英语和西班牙语，能够形成复杂的联邦生物信息学查询。与其他系统相比，SPARQL-LLM速度更快（最高快36倍），且每个问题成本不超过$0.01。", "conclusion": "SPARQL-LLM是一个适合实时、低成本的文本到SPARQL应用的方法，展示了其在实际部署中的潜力和优势。"}}
{"id": "2512.14274", "pdf": "https://arxiv.org/pdf/2512.14274", "abs": "https://arxiv.org/abs/2512.14274", "authors": ["Yu Chen", "Hongwei Lin"], "title": "TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning", "categories": ["cs.CV", "cs.LG", "math.AT"], "comment": null, "summary": "Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.", "AI": {"tldr": "提出了一种用于自动检测持久性图中显著点的深度学习网络TUN。", "motivation": "在处理点云时，理解其拓扑结构至关重要。然而，在持久性图中识别哪些点代表真实信号仍是一项挑战，这阻碍了拓扑数据分析在实际应用中的广泛采用。", "method": "提出了一种结合增强的持久性描述符、自我注意机制、PointNet风格的点云编码器以及稳定预处理和不平衡感知训练的多模态网络TUN。", "result": "实验表明，与经典方法相比，TUN在检测持久性图中的显著点方面表现更佳。", "conclusion": "TUN提供了一种自动化且有效的解决方案来识别持久性图中的关键特征点，这对下游应用至关重要。"}}
{"id": "2512.14273", "pdf": "https://arxiv.org/pdf/2512.14273", "abs": "https://arxiv.org/abs/2512.14273", "authors": ["Xiaoqian Shen", "Min-Hung Chen", "Yu-Chiang Frank Wang", "Mohamed Elhoseiny", "Ryo Hachiuma"], "title": "Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in", "categories": ["cs.CV"], "comment": "Project page: https://xiaoqian-shen.github.io/Zoom-Zero/", "summary": "Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\\% on NExT-GQA and 4.6\\% on ReXTime, while also enhancing average answer accuracy by 2.4\\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\\% on long-video benchmarks.", "AI": {"tldr": "提出了一种名为Zoom-Zero的粗到细框架，用于改进视频中的时间定位和答案生成。", "motivation": "现有的基于GRPO的方法在解决GVQA任务时存在时间定位不准和幻觉问题，需要进一步提高其准确性。", "method": "通过引入zoom-in准确度奖励来验证时间定位预测的保真度，并进行细粒度视觉验证；采用令牌选择性信用分配机制来将奖励归因于负责时间定位或答案生成的令牌。", "result": "改进了NExT-GQA和ReXTime上的时间定位，分别提高了5.2%和4.6%，提升了平均回答准确性2.4%；在长视频理解中表现更优，平均改善了6.4%。", "conclusion": "Zoom-Zero方法显著改善了GVQA任务中的时间接地问题，并且在细粒度视觉验证方面表现出色。"}}
{"id": "2512.14270", "pdf": "https://arxiv.org/pdf/2512.14270", "abs": "https://arxiv.org/abs/2512.14270", "authors": ["Zixin Tang", "Yiming Chen", "Quentin Rouxel", "Dianxi Li", "Shuang Wu", "Fei Chen"], "title": "CaFe-TeleVision: A Coarse-to-Fine Teleoperation System with Immersive Situated Visualization for Enhanced Ergonomics", "categories": ["cs.RO"], "comment": null, "summary": "Teleoperation presents a promising paradigm for remote control and robot proprioceptive data collection. Despite recent progress, current teleoperation systems still suffer from limitations in efficiency and ergonomics, particularly in challenging scenarios. In this paper, we propose CaFe-TeleVision, a coarse-to-fine teleoperation system with immersive situated visualization for enhanced ergonomics. At its core, a coarse-to-fine control mechanism is proposed in the retargeting module to bridge workspace disparities, jointly optimizing efficiency and physical ergonomics. To stream immersive feedback with adequate visual cues for human vision systems, an on-demand situated visualization technique is integrated in the perception module, which reduces the cognitive load for multi-view processing. The system is built on a humanoid collaborative robot and validated with six challenging bimanual manipulation tasks. User study among 24 participants confirms that CaFe-TeleVision enhances ergonomics with statistical significance, indicating a lower task load and a higher user acceptance during teleoperation. Quantitative results also validate the superior performance of our system across six tasks, surpassing comparative methods by up to 28.89% in success rate and accelerating by 26.81% in completion time. Project webpage: https://clover-cuhk.github.io/cafe_television/", "AI": {"tldr": "本文提出了CaFe-TeleVision系统，该系统结合了粗到细控制机制和沉浸式场景可视化技术来增强遥操作的效率和人体工学。", "motivation": "当前的遥操作系统在复杂环境中仍然面临低效和不舒适的人机交互问题。为了提高远程操控的任务成功率并减少完成时间，本文提出了一种新的系统CaFe-TeleVision。", "method": "该方法包含两个主要模块：粗到细重定向模块和沉浸式感知模块。前者通过优化工作空间差异来提高操作效率和人体工学；后者则提供足够的视觉线索以降低多视图处理的认知负担。", "result": "实验结果显示，CaFe-TeleVision系统在六个任务中表现出优越的性能，在成功率上超越了比较方法28.89%，并使完成时间加快了26.81%。用户研究也显示该系统具有显著的人体工学优势。", "conclusion": "本文所提出的方法证明了其能够有效提高遥操作系统的效率和人体工学，为未来的应用提供了新的思路与解决方案。"}}
{"id": "2512.14266", "pdf": "https://arxiv.org/pdf/2512.14266", "abs": "https://arxiv.org/abs/2512.14266", "authors": ["Shreedhar Govil", "Didier Stricker", "Jason Rambach"], "title": "DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\\circ$ field of view driver attention dataset, containing $\\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.", "AI": {"tldr": "开发了一种新的全方位驾驶员注意力预测系统DriverGaze360，包含大规模数据集和改进的预测模型。", "motivation": "现有工作受限于窄视野和驾驶多样性不足，难以捕捉全面的空间环境信息。因此，需要一个能够提供全向视野和丰富多样性的新方法。", "method": "提出DriverGaze360大型全方位驾驶员注意力数据集和全景注意力预测模型DriverGaze360-Net，该模型通过辅助语义分割头联合学习注意图和被注意的物体。", "result": "在多种指标上实现全景驾驶图像中的状态最佳注意力预测性能。", "conclusion": "新方法能够有效提升对全方位驾驶环境的理解与驾驶员行为分析。"}}
{"id": "2512.14263", "pdf": "https://arxiv.org/pdf/2512.14263", "abs": "https://arxiv.org/abs/2512.14263", "authors": ["Nick Leenders", "Thomas Quadt", "Boris Cule", "Roy Lindelauf", "Herman Monsuur", "Joost van Oijen", "Mark Voskuijl"], "title": "Explainable Preference Learning: a Decision Tree-based Surrogate Model for Preferential Bayesian Optimization", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Current Preferential Bayesian Optimization methods rely on Gaussian Processes (GPs) as surrogate models. These models are hard to interpret, struggle with handling categorical data, and are computationally complex, limiting their real-world usability. In this paper, we introduce an inherently interpretable decision tree-based surrogate model capable of handling both categorical and continuous data, and scalable to large datasets. Extensive numerical experiments on eight increasingly spiky optimization functions show that our model outperforms GP-based alternatives on spiky functions and has only marginally lower performance for non-spiky functions. Moreover, we apply our model to the real-world Sushi dataset and show its ability to learn an individual's sushi preferences. Finally, we show some initial work on using historical preference data to speed up the optimization process for new unseen users.", "AI": {"tldr": "介绍了基于决策树的替代模型，以解释偏好的贝叶斯优化方法。", "motivation": "当前基于高斯过程的方法难以理解、处理类别数据有困难且计算复杂度高，限制了其实用性。", "method": "提出了一种可解释性强，能处理分类和连续数据，并且在大数据集上具有扩展性的决策树替代模型。", "result": "实验表明该模型优于基于GP的模型，尤其擅长处理尖刺优化函数。并且在真实世界的数据集中表现出色。", "conclusion": "提出的模型提高了偏好的贝叶斯优化方法的可解释性，并展示了其在实际应用中的潜力。"}}
{"id": "2512.14257", "pdf": "https://arxiv.org/pdf/2512.14257", "abs": "https://arxiv.org/abs/2512.14257", "authors": ["Wentao Wan", "Kaiyu Wu", "Qingyang Ma", "Nan Kang", "Yunjie Chen", "Liang Lin", "Keze Wang"], "title": "Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs", "categories": ["cs.CV"], "comment": "13 Pages, 12 figures", "summary": "Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.", "AI": {"tldr": "通过构建概率图增强视觉编程，使非可微分的执行过程变为可微分的概率推理过程，从而利用最终标签进行端到端学习", "motivation": "现有的改进方法主要集中在提高LLM生成的程序质量，但忽视了对VP调用的预训练模型的优化。由于缺乏子任务标签和非可微性问题，难以直接使用高效的梯度下降优化法进行端到端的学习", "method": "提出EVPG方法，通过构建基于变量依赖关系的概率图来实现非可微分过程的可微分化，使VP可以利用最终标签进行高效学习", "result": "在GQA、NLVRv2和Open Images等复杂视觉推理任务上表现出色，显示出显著性能改进", "conclusion": "EVPG有效地增强了VP框架，提高了其在复杂视觉推理任务中的表现"}}
{"id": "2512.14252", "pdf": "https://arxiv.org/pdf/2512.14252", "abs": "https://arxiv.org/abs/2512.14252", "authors": ["Kelly J. Davis"], "title": "Gödel's Poetry", "categories": ["cs.AI", "cs.LG"], "comment": "24 pages, 1 figure", "summary": "Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.", "AI": {"tldr": "该论文提出了一种新的计算机定理证明方法，结合了Lean4证明生成的语言模型和复杂的定理分解成简单命题的递归技术。", "motivation": "长期以来，形式化的自动定理证明一直被视为人工智能的一个挑战。为了提高自动化证明系统的性能并解决复杂问题，研究人员开发了一种新的多代理架构系统，通过语言模型协调来实现自动形式化、证明生成和复杂的定理解构。", "method": "该方法引入了一种专门的语言模型用于Lean4的证明生成，并结合了将困难的定理分解为更简单命题的递归技术。此过程使用抽象语法树（AST）解析能力进行自动化和递归证明解构，通过多代理架构实现这些任务之间的协调。", "result": "在没有解构的情况下，在miniF2F数据集上达到了90.4%的成功率；引入了定理分解后，系统的表现有了显著提高。", "conclusion": "该研究提出了一种创新的自动证明方法，并通过实验证明其有效性。所开发的系统作为goedels-poetry发布在PyPI和GitHub上，便于进一步的研究与扩展。"}}
{"id": "2512.14244", "pdf": "https://arxiv.org/pdf/2512.14244", "abs": "https://arxiv.org/abs/2512.14244", "authors": ["Yiqing Zhou", "Yu Lei", "Shuzheng Si", "Qingyan Sun", "Wei Wang", "Yifei Wu", "Hao Wen", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.", "AI": {"tldr": "该论文提出了基于EDU的上下文压缩框架，用于改进大型语言模型在处理长文档问题和自主代理时对大量上下文的管理。", "motivation": "现有的上下文压缩技术存在破坏局部连贯性的问题，并且依赖于隐式的潜在编码方式容易引入位置偏差。为解决这些问题，提出了一种新的基于EDU的显式压缩框架以保持全局结构和细粒度细节。", "method": "该方法通过将线性文本转化为由基本话语单元（EDUs）组成的结构关系树，并使用轻量级排名模块选择与查询相关的子树进行线性化，从而实现上下文压缩。同时发布了一个名为StructBench的手动注释数据集以评估结构理解。", "result": "实验结果表明该方法在结构预测准确性方面达到了最先进的水平，并且显著优于前沿的大型语言模型。此外，在从长文本任务到复杂的深度搜索场景等下游任务中，其性能也得到了提升。", "conclusion": "所提出的方法有效解决了现有上下文压缩技术的问题，不仅提高了结构理解精度，还在多种下游任务中表现出色。"}}
{"id": "2512.14241", "pdf": "https://arxiv.org/pdf/2512.14241", "abs": "https://arxiv.org/abs/2512.14241", "authors": ["Salvatore Romano", "Marco Grassia", "Giuseppe Mangioni"], "title": "Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning", "categories": ["cs.LG", "cs.AI", "physics.soc-ph"], "comment": "16 pages, 4 figures", "summary": "Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.", "AI": {"tldr": "本文提出了一种新的评估图生成模型的方法，克服了最大均值差异（MMD）作为度量标准的局限性，并通过几何深度学习模型训练在合成和现实世界图形上的定制数据集来评估两种最先进的图生成模型。", "motivation": "现有方法依赖于最大均值差异（MMD），这不能全面评估生成的图是否具有与真实网络相同的结构特性，因此需要一个更有效的评估标准。", "method": "引入了一种新的称为RGM的方法来评估GGMs，这种方法利用几何深度学习模型训练在合成和现实世界图形上的定制数据集进行比较。", "result": "发现GRAN和EDGE虽然可以生成具有某些拓扑特性的图，但在保持区分不同图域的结构特性方面存在显著局限性。同时指出MMD作为评估指标对于GGMs来说是不足的，并建议未来研究中应考虑替代方法。", "conclusion": "该研究表明了RGM作为一个更有效的评估工具的重要性，并指出了现有模型在生成具有复杂结构特性的图时存在的问题，为未来的改进提供了方向。"}}
{"id": "2512.14236", "pdf": "https://arxiv.org/pdf/2512.14236", "abs": "https://arxiv.org/abs/2512.14236", "authors": ["Nando Metzger", "Prune Truong", "Goutam Bhat", "Konrad Schindler", "Federico Tombari"], "title": "Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding", "categories": ["cs.CV"], "comment": "Project page: elastic3d.github.io", "summary": "The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.", "AI": {"tldr": "该论文提出了一种将单目视频转换为双目立体视频的自动化方法Elastic3D。", "motivation": "随着沉浸式3D内容需求的增长，需要一种自动化的单目到立体视频转换的方法来提供高质量的3D体验。", "method": "基于条件隐变量扩散模型和新颖的引导VAE解码器，提出了一种直接端到端的转换方法，避免了显式深度估计和扭曲带来的伪影，并通过一个直观的标量调谐旋钮在推断时允许用户控制立体效果强度。", "result": "实验表明，该方法在三个真实世界立体视频数据集上的表现优于传统的基于扭曲的方法以及最近的非扭曲基线方法，确立了可靠、可控的立体视频转换的新标准。", "conclusion": "Elastic3D提供了一种新的自动化单目到双目立体视频转换的方法，提高了立体视频的质量和用户体验。"}}
{"id": "2512.14235", "pdf": "https://arxiv.org/pdf/2512.14235", "abs": "https://arxiv.org/abs/2512.14235", "authors": ["Jimmie Kwok", "Holger Caesar", "Andras Palffy"], "title": "4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation", "categories": ["cs.CV"], "comment": null, "summary": "Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.", "AI": {"tldr": "本文提出了一种生成4D雷达点云的新框架，以解决标注雷达数据稀缺的问题。", "motivation": "由于汽车雷达在恶劣天气条件下的成本效益和鲁棒性，其环境感知能力得到了发展。但是，缺乏标注的雷达数据限制了基于雷达的感知系统的进步。因此提出了此方法来生成高质量的雷达注释和现实场景以供训练使用。", "method": "4D-RaDiff通过扩散一个潜在的点云表示并对其进行物体级别或场景级别的条件控制，在这个潜在空间中生成4D雷达点云，并将未标记的边界框转换为高质量的雷达注释，同时也可以将现有的LiDAR点云数据转换成真实的雷达场景。", "result": "实验表明，与仅使用真实数据训练相比，通过在训练过程中加入由4D-RaDiff合成的数据作为数据增强方法可以持续提高物体检测性能。并且，在预训练阶段使用合成数据可以减少90%的标注雷达数据需求，同时实现相比较的对象检测性能。", "conclusion": "提出的框架证明了它能有效生成高质量的雷达注释，并且通过减少对大量标注雷达数据的需求，提高了基于雷达感知系统的性能和效率。"}}
{"id": "2512.14234", "pdf": "https://arxiv.org/pdf/2512.14234", "abs": "https://arxiv.org/abs/2512.14234", "authors": ["Juze Zhang", "Changan Chen", "Xin Chen", "Heng Yu", "Tiange Xiang", "Ali Sartaz Khan", "Shrinidhi K. Lakshmikanth", "Ehsan Adeli"], "title": "ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body", "categories": ["cs.CV"], "comment": "Project page: https://ai.stanford.edu/~juze/ViBES/", "summary": "Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond \"speech-conditioned motion generation\" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/", "AI": {"tldr": "ViBES 是一个可以进行多轮对话并根据对话内容生成身体动作的虚拟代理。", "motivation": "大多数先前系统将人类行为建模为固定语言到运动片段的任务，缺乏对何时移动、做什么以及如何适应多回合对话中变化的决策。这种模型导致了脆弱的时间同步性、弱社会联系和分离的操作栈。", "method": "ViBES 是一个混合模式专家 (MoME) 模型：它由语音、面部表情和身体动作的变压器专家组成，这些专家在模态上进行分区，并通过跨专家注意力共享信息。该模型处理交织的多模态标记流，并通过硬路由按模态分离参数。", "result": "ViBES 在多轮对话中表现出更好的对话-运动对齐度和行为质量，超过了强基线。", "conclusion": "ViBES 展示了语音、韵律和动作协同生成的能力，使虚拟身体成为可能，并且可以进行可控制的社交互动。"}}
{"id": "2512.14233", "pdf": "https://arxiv.org/pdf/2512.14233", "abs": "https://arxiv.org/abs/2512.14233", "authors": ["Ruozhao Yang", "Mingfei Cheng", "Gelei Deng", "Tianwei Zhang", "Junjie Wang", "Xiaofei Xie"], "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": "13 pages, 6 figures", "summary": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.", "AI": {"tldr": "PentestEval是一款用于评估大型语言模型在渗透测试不同阶段性能的基准工具", "motivation": "当前的渗透测试工作流高度依赖人工操作，难以规模化。虽然大语言模型（LLM）提供了自动化机会，但现有应用缺乏任务分解和领域适应能力，导致表现不可靠。因此需要一个全面的基准来评估这些模型在渗透测试中的实际效果", "method": "PentestEval设计了六个阶段的任务：信息收集、弱点搜集与过滤、攻击决策、漏洞利用及修正，并通过自动化的评价管道对九种广泛使用的LLM进行评测，涵盖了12个真实的脆弱场景下346项任务的全面评估", "result": "整体成功率仅达到31%，显示当前模型在各个阶段中的性能较弱。现有的基于LLM的系统表现类似，自主代理几乎完全失效", "conclusion": "此研究揭示了渗透测试自动化对结构化推理能力的需求，并指出了模块化的改进方向以提高各阶段和整体性能"}}
{"id": "2512.14232", "pdf": "https://arxiv.org/pdf/2512.14232", "abs": "https://arxiv.org/abs/2512.14232", "authors": ["Rawan Alyahya", "Asrar Alruwayqi", "Atheer Alqarni", "Asma Alkhaldi", "Metab Alkubeyyer", "Xin Gao", "Mona Alshahrani"], "title": "Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients", "categories": ["cs.CV"], "comment": null, "summary": "The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.", "AI": {"tldr": "该论文提出了一种基于多视图MRI的分类方法，用于检测胶质母细胞瘤患者的MGMT甲基化状态。", "motivation": "目前确认MGMT启动子甲基化的标准方法是进行侵入性的脑肿瘤组织活检。此研究旨在探索使用医学影像提取遗传标记的方法，以实现非侵入性诊断，并提高精准医疗水平。", "method": "通过MRI扫描和深度学习模型提出了一种新的多视图方法来检测MGMT甲基化状态，该方法从所有三个视角中提取信息而不依赖复杂的3D深度学习模型。同时引入了肿瘤切片抽取的新技术，并且展示了其优越性。", "result": "在与当前最先进模型的比较中证明了这种方法的有效性，并分享了一个可复制的模型发布管道，以促进透明度和稳健诊断工具的发展。", "conclusion": "研究强调了非侵入式方法用于检测MGMT启动子甲基化的潜力，并为胶质母细胞瘤治疗中的精准医疗做出了贡献。"}}
{"id": "2512.14228", "pdf": "https://arxiv.org/pdf/2512.14228", "abs": "https://arxiv.org/abs/2512.14228", "authors": ["Aneesha Fernando", "Surangika Ranathunga", "Kristin Stock", "Raj Prasanna", "Christopher B. Jones"], "title": "Georeferencing complex relative locality descriptions with large language models", "categories": ["cs.AI"], "comment": "Provisionally accepted for publication in the International Journal of Geographical Information Science", "summary": "Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.", "AI": {"tldr": "本文研究了使用大型语言模型自动地理参考复杂的地点描述的方法，特别是在生物多样性收藏领域。", "motivation": "地理参考文本通常依赖地名或具有地理指示性的词语，但对于基于空间关系的相对位置描述准确性不足。生物标本采集记录中常见的叙述性地点描述需要更准确、高效的自动化地理参考解决方案。", "method": "本文首先确定了有效的提示模式，并通过量化低秩适应（QLoRA）在多种区域和语言的生物多样性数据集上微调大型语言模型。然后评估其性能以解决复杂相对位置描述的问题。", "result": "该方法表现出色，平均而言，在10公里范围内记录准确率为65%，最佳情况下（纽约州）则达到了85%的准确性，并且在更小的距离内表现也较好，例如1公里范围内的准确率为67%。", "conclusion": "大型语言模型对复杂的地理位置描述有良好的地理参考性能，显示了其在这方面的巨大潜力。"}}
{"id": "2512.14225", "pdf": "https://arxiv.org/pdf/2512.14225", "abs": "https://arxiv.org/abs/2512.14225", "authors": ["Tao Tang", "Enhui Ma", "xia zhou", "Letian Wang", "Tianyi Yan", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "XianPeng Lang", "Jia-Wang Bian", "Kaicheng Yu", "Xiaodan Liang"], "title": "OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving", "categories": ["cs.CV"], "comment": "ACM MM 2025", "summary": "Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.", "AI": {"tldr": "本文提出了OmniGen，一种用于自驾车的统一多模态传感器数据生成框架。", "motivation": "现有的生成模型主要集中在单模态数据合成上，导致多模态传感器数据之间的不一致性和效率低下。因此需要一个能够生成一致性高且灵活调整的多模态传感器数据的方法。", "method": "OmniGen利用共享鸟瞰图空间统一多模态特征，并设计了一个新的可泛化的多模态重建方法UAE，通过体积渲染实现准确和灵活的数据解码；同时引入了具有控制分支的Diffusion Transformer以支持可控性生成。", "result": "实验结果表明，OmniGen在多模态传感器数据的一致性和灵活性方面都取得了预期的效果。", "conclusion": "本文提出的OmniGen框架能够有效地解决自驾车中多模态传感器数据合成的问题，并且其性能优于现有的方法。"}}
{"id": "2512.14222", "pdf": "https://arxiv.org/pdf/2512.14222", "abs": "https://arxiv.org/abs/2512.14222", "authors": ["Xichen Ding", "Jianzhe Gao", "Cong Pan", "Wenguan Wang", "Jie Qin"], "title": "History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.", "AI": {"tldr": "本文提出了一种历史增强的两阶段变压器框架，用于处理基于语言指令在大型城市环境中定位目标的空中视觉与语言导航任务。", "motivation": "现有的无人机代理通常采用单一粒度框架，在全局环境推理和局部场景理解之间难以平衡。为此，本研究提出了一个能够整合这两方面的解决方案。", "method": "通过历史增强的两阶段变压器（HETT）框架，该方法首先根据空间地标和历史上下文预测粗略的目标位置，然后利用精细的视觉分析细化动作。此外，还设计了一个动态聚合视觉特征到结构化空间记忆的历史栅格地图。", "result": "实验表明，HETT在经过精修的城市导航数据集上取得了显著的性能提升，并通过广泛的消融研究验证了每个组件的有效性。", "conclusion": "该论文提出了一种能够有效处理空中视觉与语言导航任务的新方法，证明其框架比现有单一粒度模型更优越。"}}
{"id": "2512.14220", "pdf": "https://arxiv.org/pdf/2512.14220", "abs": "https://arxiv.org/abs/2512.14220", "authors": ["Marthe Ballon", "Andres Algaba", "Brecht Verbeken", "Vincent Ginis"], "title": "Estimating problem difficulty without ground truth using Large Language Model comparisons", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \\geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\\%$ degradation in Pearson correlation for $10\\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.14217", "pdf": "https://arxiv.org/pdf/2512.14217", "abs": "https://arxiv.org/abs/2512.14217", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.", "AI": {"tldr": "DRAW2ACT是一种基于深度编码轨迹生成机器人示范视频的框架，提高了视频生成在机器人操控中的可控性和一致性。", "motivation": "现有的视频扩散模型和基于轨迹的条件视频生成方法受限于对机器人操作的控制能力。DRAW2ACT旨在通过引入多模态输入以提高生成视频的质量和机器人的执行成功率。", "method": "该框架从输入轨迹中提取多个正交表示，包括深度、语义、形状和运动，并将其注入扩散模型中。此外，它同时生成RGB和深度视频，并使用跨模式注意机制及深度监督来增强空间时间一致性。", "result": "实验结果表明，与现有基线相比，DRAW2ACT在视觉保真度和一致性的表现上更为优异，并且能够提升机器人的操作成功率。", "conclusion": "通过改进的多模态输入方法和跨模式注意机制，DRAW2ACT展示了其在生成高质量机器人示范视频方面的强大能力。"}}
{"id": "2512.14211", "pdf": "https://arxiv.org/pdf/2512.14211", "abs": "https://arxiv.org/abs/2512.14211", "authors": ["Mengxue Zhang", "Qingrui Cai", "Yinyin Chen", "Hang Jin", "Jianjun Zhou", "Qiu Guo", "Peijun Zhao", "Zhiping Mao", "Xingxing Zhang", "Yuyu Xia", "Xianwang Jiang", "Qin Xu", "Chunyan Xiong", "Yirong Zhou", "Chengyan Wang", "Xiaobo Qu"], "title": "Error Bound Analysis of Physics-Informed Neural Networks-Driven T2 Quantification in Cardiac Magnetic Resonance Imaging", "categories": ["physics.bio-ph", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINN) are emerging as a promising approach for quantitative parameter estimation of Magnetic Resonance Imaging (MRI). While existing deep learning methods can provide an accurate quantitative estimation of the T2 parameter, they still require large amounts of training data and lack theoretical support and a recognized gold standard. Thus, given the absence of PINN-based approaches for T2 estimation, we propose embedding the fundamental physics of MRI, the Bloch equation, in the loss of PINN, which is solely based on target scan data and does not require a pre-defined training database. Furthermore, by deriving rigorous upper bounds for both the T2 estimation error and the generalization error of the Bloch equation solution, we establish a theoretical foundation for evaluating the PINN's quantitative accuracy. Even without access to the ground truth or a gold standard, this theory enables us to estimate the error with respect to the real quantitative parameter T2. The accuracy of T2 mapping and the validity of the theoretical analysis are demonstrated on a numerical cardiac model and a water phantom, where our method exhibits excellent quantitative precision in the myocardial T2 range. Clinical applicability is confirmed in 94 acute myocardial infarction (AMI) patients, achieving low-error quantitative T2 estimation under the theoretical error bound, highlighting the robustness and potential of PINN.", "AI": {"tldr": "本文提出了基于PINN的T2参数量化方法，并通过推导误差上界建立了理论基础。", "motivation": "现有深度学习方法在MRI中进行T2参数估计时，需要大量训练数据且缺乏理论支持。为此，作者提出利用PINN结合Bloch方程进行T2估计，提高准确性并建立理论框架。", "method": "通过将Bloch方程嵌入到PINN的损失函数中，并推导出T2估计误差和一般化误差的上界，以评估PINN的定量精度。方法仅依赖于目标扫描数据而不需要预定义的训练数据库。", "result": "在数值心脏模型及水模上验证了该方法在心肌T2范围内的精确度，并通过94例急性心梗患者的临床应用证明其低误差和理论误差界限内的量化估计准确性。", "conclusion": "本文提出的基于PINN的MRI T2参数量化方法具备高精度、良好的临床适用性及强大的理论支持，展示了该技术的稳健性和潜在价值。"}}
{"id": "2512.14206", "pdf": "https://arxiv.org/pdf/2512.14206", "abs": "https://arxiv.org/abs/2512.14206", "authors": ["Mayank Sewlia", "Christos K. Verginis", "Dimos V. Dimarogonas"], "title": "Trajectory Tracking for Multi-Manipulator Systems in Constrained Environments", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "We consider the problem of cooperative manipulation by a mobile multi-manipulator system operating in obstacle-cluttered and highly constrained environments under spatio-temporal task specifications. The task requires transporting a grasped object while respecting both continuous robot dynamics and discrete geometric constraints arising from obstacles and narrow passages. To address this hybrid structure, we propose a multi-rate planning and control framework that combines offline generation of an STL-satisfying object trajectory and collision-free base footprints with online constrained inverse kinematics and continuous-time feedback control. The resulting closed-loop system enables coordinated reconfiguration of multiple manipulators while tracking the desired object motion. The approach is evaluated in high-fidelity physics simulations using three Franka Emika Panda mobile manipulators rigidly grasping an object.", "AI": {"tldr": "多机械臂系统在受限环境中的轨迹跟踪问题", "motivation": "解决移动多机械臂系统在障碍物密集且高度约束的环境中执行空间和时间任务要求时面临的挑战", "method": "提出了一种结合离线生成满足STL规范的对象轨迹和无碰撞基座足迹、在线受约束逆运动学以及连续时间反馈控制的多速率规划与控制框架", "result": "通过使用三个Franka Emika Panda移动机械臂牢固抓握物体，在高保真物理模拟中对该方法进行了评估，并实现了多个机械臂的同时重新配置，以跟踪所需对象运动", "conclusion": "所提出的方案有效解决了多机械臂系统在受限环境中的轨迹跟踪问题"}}
{"id": "2512.14202", "pdf": "https://arxiv.org/pdf/2512.14202", "abs": "https://arxiv.org/abs/2512.14202", "authors": ["Timo Klein", "Thomas Lang", "Andrii Shkabrii", "Alexander Sturm", "Kevin Sidak", "Lukas Miklautz", "Claudia Plant", "Yllka Velaj", "Sebastian Tschiatschek"], "title": "Understanding and Improving Hyperbolic Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The performance of reinforcement learning (RL) agents depends critically on the quality of the underlying feature representations. Hyperbolic feature spaces are well-suited for this purpose, as they naturally capture hierarchical and relational structure often present in complex RL environments. However, leveraging these spaces commonly faces optimization challenges due to the nonstationarity of RL. In this work, we identify key factors that determine the success and failure of training hyperbolic deep RL agents. By analyzing the gradients of core operations in the Poincaré Ball and Hyperboloid models of hyperbolic geometry, we show that large-norm embeddings destabilize gradient-based training, leading to trust-region violations in proximal policy optimization (PPO). Based on these insights, we introduce Hyper++, a new hyperbolic PPO agent that consists of three components: (i) stable critic training through a categorical value loss instead of regression; (ii) feature regularization guaranteeing bounded norms while avoiding the curse of dimensionality from clipping; and (iii) using a more optimization-friendly formulation of hyperbolic network layers. In experiments on ProcGen, we show that Hyper++ guarantees stable learning, outperforms prior hyperbolic agents, and reduces wall-clock time by approximately 30%. On Atari-5 with Double DQN, Hyper++ strongly outperforms Euclidean and hyperbolic baselines. We release our code at https://github.com/Probabilistic-and-Interactive-ML/hyper-rl .", "AI": {"tldr": "本文研究如何改进在双曲空间中训练强化学习代理的方法，提出了Hyper++框架。", "motivation": "双曲特征空间适用于捕捉复杂环境中的层级和关系结构。然而，在非平稳环境下进行优化面临挑战，导致训练不稳定。", "method": "通过分析Poincaré球体和超球模型的核心操作的梯度，确定了大范数嵌入会导致训练不稳定的因素，并提出Hyper++框架以解决这些问题。", "result": "实验显示，Hyper++提高了学习稳定性，在ProcGen基准测试中优于先前的双曲代理并减少了大约30%的时间。在Atari-5上，Hyper++也显著超越了欧氏和双曲基线。", "conclusion": "通过改进训练方法，可以在复杂环境中实现更高效的强化学习。"}}
{"id": "2512.14200", "pdf": "https://arxiv.org/pdf/2512.14200", "abs": "https://arxiv.org/abs/2512.14200", "authors": ["Zhuoxiao Li", "Wenzong Ma", "Taoyu Wu", "Jinjing Zhu", "Zhenchao Q", "Shuai Zhang", "Jing Ou", "Yinrui Ren", "Weiqing Qi", "Guobin Shen", "Hui Xiong", "Wufan Zhao"], "title": "Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.", "AI": {"tldr": "本文介绍SkyLume，一个大规模的无人机数据集，用于研究在不同光照条件下进行稳健的三维重建和逆向渲染。", "motivation": "当前缺乏系统性地捕捉同一区域在不同时段照明条件下的无人机数据集，这导致了颜色伪影、几何错误及外观不一致等问题。SkyLume旨在填补这一空白，并促进大规模逆向渲染、几何重建及新颖视图合成的研究与实际评估。", "method": "通过收集来自10个城区的超过10万张高分辨率无人机图像（四个倾斜视角和正射视角），并在一天中的三个时段系统性地隔离照明变化。同时提供每场景点云扫描和精确三维地面真实值，以支持几何和外观的精准评估。此外，引入了时间一致性系数TCC，用于直接衡量逆向渲染任务中光与材料分离的稳健性。", "result": "SkyLume数据集提供了大规模逆向渲染、几何重建及新颖视图合成研究所需的资源，并且为跨时域反照率稳定性提供了一个新的评估标准。", "conclusion": "通过引入SkyLume，作者希望促进研究界在大规模逆向渲染和三维重建领域的进展。此数据集将成为一个重要的基准平台，用于评估算法的性能以及探索实际应用中的照明鲁棒性问题。"}}
{"id": "2512.14196", "pdf": "https://arxiv.org/pdf/2512.14196", "abs": "https://arxiv.org/abs/2512.14196", "authors": ["Cassandra Krause", "Mattias P. Heinrich", "Ron Keuth"], "title": "Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity", "categories": ["cs.CV"], "comment": "Accepted as poster at the German Conference on Medical Image Computing 2026", "summary": "Between $15\\,\\%$ and $45\\,\\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\\,\\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.", "AI": {"tldr": "提出了一种自动分配AO码的方法，以提取骨折形态，并通过将多标签任务转化为局部多分类问题来提高准确率。", "motivation": "儿童在成长期间易发生骨折，需要准确的诊断。骨折形态是关键的诊断特征之一。此方法旨在改善现有技术在诊断中的准确性。", "method": "利用公共数据集和自动分配AO码的方法为骨折边界框确定全球AO代码，将全局多标签任务转换为局部多类任务。", "result": "这种方法使平均F1分数提高了7.89%，但是使用不完美的骨折检测器时性能下降。", "conclusion": "该研究提出了一种有效的提取骨折形态的方法，并将其开源。尽管存在一些挑战，但结果表明其具有较高的准确性和应用价值。"}}
{"id": "2512.14189", "pdf": "https://arxiv.org/pdf/2512.14189", "abs": "https://arxiv.org/abs/2512.14189", "authors": ["Johannes A. Gaus", "Daniel Häufle", "Woo-Jeong Baek"], "title": "SUPER -- A Framework for Sensitivity-based Uncertainty-aware Performance and Risk Assessment in Visual Inertial Odometry", "categories": ["cs.RO"], "comment": null, "summary": "While many visual odometry (VO), visual-inertial odometry (VIO), and SLAM systems achieve high accuracy, the majority of existing methods miss to assess risks at runtime. This paper presents SUPER (Sensitivity-based Uncertainty-aware PErformance and Risk assessment) that is a generic and explainable framework that propagates uncertainties via sensitivities for real-time risk assessment in VIO. The scientific novelty lies in the derivation of a real-time risk indicator that is backend-agnostic and exploits the Schur complement blocks of the Gauss-Newton normal matrix to propagate uncertainties. Practically, the Schur complement captures the sensitivity that reflects the influence of the uncertainty on the risk occurrence. Our framework estimates risks on the basis of the residual magnitudes, geometric conditioning, and short horizon temporal trends without requiring ground truth knowledge. Our framework enables to reliably predict trajectory degradation 50 frames ahead with an improvement of 20% to the baseline. In addition, SUPER initiates a stop or relocalization policy with 89.1% recall. The framework is backend agnostic and operates in real time with less than 0.2% additional CPU cost. Experiments show that SUPER provides consistent uncertainty estimates. A SLAM evaluation highlights the applicability to long horizon mapping.", "AI": {"tldr": "本文提出了SUPER框架，用于在视觉惯性里程计中进行基于敏感性的不确定性感知性能和风险评估。", "motivation": "许多VO、VIO和SLAM系统虽然具有高精度，但缺少实时风险评估。现有的方法大多数忽略了风险的动态传播过程。", "method": "该框架通过利用Gauss-Newton正则矩阵中的Schur补块来传递不确定性，并基于残差幅度、几何条件和短期时间趋势估计风险，无需地面真值知识。", "result": "实验表明，与基准相比，该框架可提前50帧可靠预测轨迹退化，召回率达到89.1%，并提供一致的不确定性估计。此外，它能在实时环境下运行，并且仅增加不到0.2%的CPU成本。", "conclusion": "SUPER是一个通用且解释性强的框架，适用于视觉惯性里程计中的实时风险评估和不确定性传播，可应用于长时程地图构建中。"}}
{"id": "2512.14187", "pdf": "https://arxiv.org/pdf/2512.14187", "abs": "https://arxiv.org/abs/2512.14187", "authors": ["Jianwei Sun", "Xiaoning Lei", "Wenhao Cai", "Xichen Xu", "Yanshu Wang", "Hu Gao"], "title": "Establishing Stochastic Object Models from Noisy Data via Ambient Measurement-Integrated Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Task-based measures of image quality (IQ) are critical for evaluating medical imaging systems, which must account for randomness including anatomical variability. Stochastic object models (SOMs) provide a statistical description of such variability, but conventional mathematical SOMs fail to capture realistic anatomy, while data-driven approaches typically require clean data rarely available in clinical tasks. To address this challenge, we propose AMID, an unsupervised Ambient Measurement-Integrated Diffusion with noise decoupling, which establishes clean SOMs directly from noisy measurements. AMID introduces a measurement-integrated strategy aligning measurement noise with the diffusion trajectory, and explicitly models coupling between measurement and diffusion noise across steps, an ambient loss is thus designed base on it to learn clean SOMs. Experiments on real CT and mammography datasets show that AMID outperforms existing methods in generation fidelity and yields more reliable task-based IQ evaluation, demonstrating its potential for unsupervised medical imaging analysis.", "AI": {"tldr": "提出了一种从噪声数据中直接建立清洁的随机对象模型（SOM）的方法AMID，用于医疗成像系统的任务驱动图像质量评价。", "motivation": "传统的数学SOM无法准确描述现实中的解剖学变异性，而基于数据驱动的方法通常需要干净的数据，而这在临床实践中很少见。因此提出了一种新的无监督方法来解决这个问题。", "method": "AMID利用测量集成策略使测量噪声与扩散轨迹对齐，并明确建模了测量和扩散噪声之间的耦合关系，在此基础上设计了一个基于环境损失的学习清洁SOM的方法。", "result": "实验结果表明，AMID在生成忠实度上优于现有方法，并能够提供更可靠的基于任务的图像质量评估。", "conclusion": "提出的AMID方法能够在不依赖干净数据的情况下，从噪声测量中建立准确的随机对象模型，展示了其在无监督医疗成像分析中的潜力。"}}
{"id": "2512.14185", "pdf": "https://arxiv.org/pdf/2512.14185", "abs": "https://arxiv.org/abs/2512.14185", "authors": ["Emanuele Artioli", "Farzad Tashtarian", "Christian Timmerer"], "title": "End-to-End Learning-based Video Streaming Enhancement Pipeline: A Generative AI Approach", "categories": ["cs.MM", "cs.AI"], "comment": "The 35th edition of the Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV '25), March 31-April 4, 2025, Stellenbosch, South Africa", "summary": "The primary challenge of video streaming is to balance high video quality with smooth playback. Traditional codecs are well tuned for this trade-off, yet their inability to use context means they must encode the entire video data and transmit it to the client. This paper introduces ELVIS (End-to-end Learning-based VIdeo Streaming Enhancement Pipeline), an end-to-end architecture that combines server-side encoding optimizations with client-side generative in-painting to remove and reconstruct redundant video data. Its modular design allows ELVIS to integrate different codecs, inpainting models, and quality metrics, making it adaptable to future innovations. Our results show that current technologies achieve improvements of up to 11 VMAF points over baseline benchmarks, though challenges remain for real-time applications due to computational demands. ELVIS represents a foundational step toward incorporating generative AI into video streaming pipelines, enabling higher quality experiences without increased bandwidth requirements.", "AI": {"tldr": "提出了ELVIS系统，通过结合服务器端编码优化和客户端生成式填充技术来提高视频流质量。", "motivation": "传统编解码器难以利用上下文信息进行高效传输，导致带宽浪费。因此本文提出了一种新的架构以改善这一问题。", "method": "设计了一个名为ELVIS的端到端框架，该框架结合了服务器端编码优化和客户端生成式填充技术来去除冗余数据并重建视频流。", "result": "实验结果显示，与基线相比，新技术在VMAF指标上提高了11个点。尽管存在计算需求高的问题，但仍显著改善了用户体验。", "conclusion": "ELVIS为将生成AI整合到视频流管道中提供了基础步骤，并且能够实现更高的质量而无需增加带宽要求。"}}
{"id": "2512.14181", "pdf": "https://arxiv.org/pdf/2512.14181", "abs": "https://arxiv.org/abs/2512.14181", "authors": ["Shaolun Ruan", "Feng Liang", "Rohan Ramakrishna", "Chao Ren", "Rudai Yan", "Qiang Guan", "Jiannan Li", "Yong Wang"], "title": "Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization", "categories": ["quant-ph", "cs.AI", "cs.HC"], "comment": "9 pages, 6 figures, accepted by TVCG 2026, not published yet", "summary": "Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.", "AI": {"tldr": "介绍了一种可视化工具XQAI-Eyes，用于评估和优化量子神经网络中的编码器选择。", "motivation": "由于缺乏系统指导，当前的量子编码器选择依赖于试错方法，本研究旨在通过引入一种新型可视化工具解决这一问题，以提高量子编码器的选择效率和效果。", "method": "开发了一种名为XQAI-Eyes的工具，该工具可以比较经典数据特征与对应的量子态，并分析混合量子态在不同类之间的区别。", "result": "通过对多种数据集和编码设计进行评估，展示了XQAI-Eyes支持探索编码器设计与量子神经网络效果之间关系的能力。", "conclusion": "通过提供透明且全面的方法来优化量子编码器的设计，本研究提出了两个关键实践：模式保留和特征映射。"}}
{"id": "2512.14180", "pdf": "https://arxiv.org/pdf/2512.14180", "abs": "https://arxiv.org/abs/2512.14180", "authors": ["Francesco Di Sario", "Daniel Rebain", "Dor Verbin", "Marco Grangetto", "Andrea Tagliasacchi"], "title": "Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere", "categories": ["cs.CV"], "comment": null, "summary": "Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.", "AI": {"tldr": "提出了一种用于三维高斯点状渲染的外观模型，即球面Voronoi图。", "motivation": "现有的球谐函数和球面高斯模型在处理高频信号、反走样问题以及镜面反射时存在局限性。", "method": "通过将方向空间划分为具有光滑边界的可学习区域，提出了一种统一的框架来表示三维高斯点状渲染中的外观。", "result": "该方法在合成和真实数据集上达到了最先进的结果，并且优化比现有替代方案更简单。", "conclusion": "球面Voronoi图提供了一个原理性、效率高和通用的解决方案用于显式3D表示中的外观建模。"}}
{"id": "2512.14179", "pdf": "https://arxiv.org/pdf/2512.14179", "abs": "https://arxiv.org/abs/2512.14179", "authors": ["K. M. Jubair Sami", "Dipto Sumit", "Ariyan Hossain", "Farig Sadeque"], "title": "A Comparative Analysis of Retrieval-Augmented Generation Techniques for Bengali Standard-to-Dialect Machine Translation Using LLMs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to the Second Workshop on Bangla Language Processing (BLP) at IJCNLP-AACL 2025. 14 pages, 9 figures, 6 tables", "summary": "Translating from a standard language to its regional dialects is a significant NLP challenge due to scarce data and linguistic variation, a problem prominent in the Bengali language. This paper proposes and compares two novel RAG pipelines for standard-to-dialectal Bengali translation. The first, a Transcript-Based Pipeline, uses large dialect sentence contexts from audio transcripts. The second, a more effective Standardized Sentence-Pairs Pipeline, utilizes structured local\\_dialect:standard\\_bengali sentence pairs. We evaluated both pipelines across six Bengali dialects and multiple LLMs using BLEU, ChrF, WER, and BERTScore. Our findings show that the sentence-pair pipeline consistently outperforms the transcript-based one, reducing Word Error Rate (WER) from 76\\% to 55\\% for the Chittagong dialect. Critically, this RAG approach enables smaller models (e.g., Llama-3.1-8B) to outperform much larger models (e.g., GPT-OSS-120B), demonstrating that a well-designed retrieval strategy can be more crucial than model size. This work contributes an effective, fine-tuning-free solution for low-resource dialect translation, offering a practical blueprint for preserving linguistic diversity.", "AI": {"tldr": "本文比较了两种用于标准到方言的孟加拉语机器翻译的检索增强生成技术，提出了更有效的句子对管道，并使用多种LLM进行了评估。", "motivation": "由于数据稀缺和语言差异，在从标准语言翻译到其区域方言时存在显著的语言处理挑战，特别是在孟加拉语中。这项研究旨在提出并比较两种新的RAG流程来解决这一问题。", "method": "论文提出了两种方法：基于转录的管道使用大量的方言句子上下文，而标准化句子对管道则利用结构化的方言-标准孟加拉语句子对，并在六种孟加拉语方言和多个LLM上进行了评估。", "result": "实验结果表明，句子对管道比基于转录的方法更有效，在减少词错误率方面尤为显著。例如，对于查查格角方言，从76%降低到55%。", "conclusion": "本研究展示了一种无需微调的有效解决方案，以解决低资源的方言翻译问题，并为保护语言多样性提供了实用蓝图。"}}
{"id": "2512.14177", "pdf": "https://arxiv.org/pdf/2512.14177", "abs": "https://arxiv.org/abs/2512.14177", "authors": ["Joseph Hoche", "Andrei Bursuc", "David Brellmann", "Gilles Louppe", "Pavel Izmailov", "Angela Yao", "Gianni Franchi"], "title": "Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.", "AI": {"tldr": "本文提出了一种新的方法SGPU，用于量化LVLM的语义不确定性。", "motivation": "现有的语义不确定性估计方法过于脆弱，容易受到语言表达变化的影响，导致不准确的结果。因此需要一种更稳健的方法来解决这一问题。", "method": "SGPU通过分析答案嵌入的空间几何结构，绕过了脆弱的聚类过程，使用高斯过程分类器将语义一致性模式映射到预测不确定性。", "result": "在六种LLM和LVLM模型上进行测试后，SGPU表现出色，在校准性和区分性方面都达到了最先进的性能。", "conclusion": "本文提出的SGPU方法不仅有效解决了现有方法的脆弱问题，还证明了其跨模型和模式的转移能力。"}}
{"id": "2512.14166", "pdf": "https://arxiv.org/pdf/2512.14166", "abs": "https://arxiv.org/abs/2512.14166", "authors": ["Yunhao Yao", "Zhiqiang Wang", "Haoran Cheng", "Yihang Cheng", "Haohua Du", "Xiang-Yang Li"], "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol", "categories": ["cs.CR", "cs.AI"], "comment": "12 pages, 6 figures", "summary": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.", "AI": {"tldr": "本文提出了一个名为IntentMiner的框架，用于通过工具调用分析来重建用户意图。", "motivation": "随着大型语言模型向自主代理发展，引入了Model Context Protocol（MCP），该协议虽然提高了可扩展性但增加了隐私风险。MCP服务器可以通过合法的工具调用来尝试重构用户的私人意图，从而导致隐私泄露问题。", "method": "IntentMiner框架结合层次信息隔离和三维语义分析技术，从工具目的、调用语句及返回结果三个维度来准确推断用户在每一步操作中的意图。", "result": "实验显示，IntentMiner能够以超过85%的语义一致性与原始用户的查询相匹配，显著优于基线方法。这表明解耦代理架构存在固有的隐私风险，即使是看似无害的工具调用日志也可能成为暴露用户秘密的强大载体。", "conclusion": "研究揭示了在MCP体系下，通过工具调用分析来推断用户意图的可能性，并展示了IntentMiner框架的有效性。这强调了解耦代理架构中的隐私威胁，为未来的工作提供了新的视角和方向。"}}
{"id": "2512.14162", "pdf": "https://arxiv.org/pdf/2512.14162", "abs": "https://arxiv.org/abs/2512.14162", "authors": ["Qingyuan Cai", "Linxin Zhang", "Xuecai Hu", "Saihui Hou", "Yongzhen Huang"], "title": "FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE", "AI": {"tldr": "提出了一种统一高效的三维人体姿态估计框架Fast3DHPE，以及在此框架下实现的模型FastDDHPose。", "motivation": "现有的三维人体姿态估计算法缺乏统一的标准和公平对比的方法，同时在训练效率和误差累积方面存在不足。", "method": "提出了一个模块化框架Fast3DHPE，该框架标准化了训练和评估协议，并引入了一种基于扩散模型的解耦三维人体姿态估计方法FastDDHPose。", "result": "实验表明，在Human3.6M和MPI-INF-3DHP数据集上，所提出的框架能够公平地比较不同方法并显著提高训练效率；同时，FastDDHPose达到了最先进的性能，并在野外场景中表现出强大的泛化能力和鲁棒性。", "conclusion": "该研究提供了一种统一高效的三维人体姿态估计框架，实现了更高的精度和更强的鲁棒性。"}}
{"id": "2512.14158", "pdf": "https://arxiv.org/pdf/2512.14158", "abs": "https://arxiv.org/abs/2512.14158", "authors": ["Shuxin Zhao", "Bo Lang", "Nan Xiao", "Yilang Zhang"], "title": "CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.", "AI": {"tldr": "本文提出了一种新的针对目标检测模型的后门攻击方法CIS-BA，通过连续交互空间中的多触发器和多对象映射机制来增强攻击的成功率。", "motivation": "现有后门攻击方法受限于单一触发器与单个对象之间的对应关系以及脆弱的像素级线索，导致其在复杂现实场景下的有效性和鲁棒性不足。本文旨在通过设计基于连续交互空间的新式攻击模式来提高后门攻击的能力和稳健性。", "method": "CIS-BA通过分析物体间的持续互动模式，并将其形式化为几何约束用于样本污染，在模型训练中嵌入后门，实现单个目标或多个目标的同时攻击。", "result": "实验表明，CIS-BA在复杂环境下具有超过97%的攻击成功率，在动态多触发条件下仍能保持至少95%的效果，并成功规避了三种最先进的防御措施。", "conclusion": "CIS-BA扩展了交互密集场景中后门攻击的领域，并为对象检测系统的安全性提供了新的见解。"}}
{"id": "2512.14157", "pdf": "https://arxiv.org/pdf/2512.14157", "abs": "https://arxiv.org/abs/2512.14157", "authors": ["Yankai Jiang", "Yujie Zhang", "Peng Zhang", "Yichen Li", "Jintai Chen", "Xiaoming Shi", "Shihui Zhen"], "title": "Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely \"think with images\" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.", "AI": {"tldr": "介绍了一种名为Ophiuchus的框架，用于通过工具增强推理来改善医学图像分析。", "motivation": "现有基于推理的医学多模态语言模型在处理需要动态和迭代聚焦于细粒度视觉区域的任务时表现不佳。因此，提出了一种新的方法来提高精准定位和诊断能力。", "method": "Ophiuchus通过三阶段训练策略实现：冷启动训练、自我反思微调以及代理工具强化学习，以优化任务特定奖励并模拟专家级诊断行为。", "result": "实验结果显示Ophiuchus在多个医学基准测试中超越了现有的最佳方法。", "conclusion": "展示了通过集成工具增强推理的路径，使医学AI能够真正地“用图像思考”。"}}
{"id": "2512.14150", "pdf": "https://arxiv.org/pdf/2512.14150", "abs": "https://arxiv.org/abs/2512.14150", "authors": ["Zhijie Zhong", "Zhiwen Yu", "Pengyu Li", "Jianming Lv", "C. L. Philip Chen", "Min Chen"], "title": "PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario", "categories": ["cs.LG", "cs.AI"], "comment": "34 pages, 14 figures, 4 tables. Under review", "summary": "Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.", "AI": {"tldr": "提出PathFinder架构，以改进单发射机到多发射机场景下的路径损耗预测。", "motivation": "当前基于深度学习的路径损耗预测方法存在环境建模被动、单一发射机模拟不足以及分布偏移鲁棒性差的问题。为此，本文提出了一个新型架构来解决这些问题。", "method": "PathFinder通过分离特征编码主动建模建筑物和发射机，并采用掩码引导低秩注意力机制分别聚焦接收器和建筑物区域。此外，还引入了针对多发射机场景的训练策略Transmitter-Oriented Mixup及一个新的评估基准S2MT-RPP。", "result": "实验结果表明PathFinder在单发射机到多发射机的路径损耗预测任务中优于现有方法，特别是在挑战性大的多发射机场景下表现尤为突出。", "conclusion": "通过主动环境建模和针对性训练策略，PathFinder能够更准确地预测多发射机下的路径损耗，为优化5G网络及物联网应用提供了有力支持。"}}
{"id": "2512.14141", "pdf": "https://arxiv.org/pdf/2512.14141", "abs": "https://arxiv.org/abs/2512.14141", "authors": ["Hanning Chen", "Keyu Man", "Kevin Zhu", "Chenguang Zhu", "Haonan Li", "Tongbo Luo", "Xizhou Feng", "Wei Sun", "Sreen Tallam", "Mohsen Imani", "Partha Kanuparthy"], "title": "TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.", "AI": {"tldr": "提出了一种新的基准数据集TorchTraceAP，用于评估和改进ML模型检测性能反模式的能力。", "motivation": "识别并解决机器学习模型中的性能反模式对于高效的训练和推理至关重要。然而，这种工作通常需要系统基础设施、机器学习模型和内核开发方面的专业知识，而这些资源在大多数计算机视觉研究者中是不可用的。", "method": "提出了一种轻量级ML模型与大语言模型相结合的新方法来检测性能反模式。该方法首先使用轻量级ML模型识别具有反模式的跟踪段，然后通过大语言模型进行精细化分类和针对性反馈。", "result": "实验结果显示，该方法在识别反模式区域方面比无监督聚类和基于规则的统计技术表现更好，并且能够有效弥补LLM上下文长度限制和推理效率低下的问题。", "conclusion": "TorchTraceAP数据集为评估ML模型检测性能反模式的能力提供了新的标准。提出的迭代方法显著提高了识别准确性，展示了在解决复杂计算机视觉任务中的应用潜力。"}}
{"id": "2512.14140", "pdf": "https://arxiv.org/pdf/2512.14140", "abs": "https://arxiv.org/abs/2512.14140", "authors": ["Han Zou", "Yan Zhang", "Ruiqi Yu", "Cong Xie", "Jie Huang", "Zhenpeng Zhan"], "title": "SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing", "categories": ["cs.CV"], "comment": null, "summary": "Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.", "AI": {"tldr": "SketchAssist是一种交互式草图绘制助手，通过统一的指令引导全局编辑与线条指导区域重绘来加速创作。", "motivation": "现有的图像编辑系统在保持线艺术的稀疏和风格敏感结构的同时，难以支持高层次语义变化和精确局部重绘。因此，需要一种能够同时满足这些需求的方法。", "method": "提出了一种可控的数据生成管道，并利用RGB通道编码输入，使得SketchAssist可以在单一界面内无缝切换指令引导编辑与线条指导重绘。此外，通过引入任务导向的专家混合模型来提高语义控制性、结构保真度和风格保存。", "result": "实验结果表明，SketchAssist在两个任务上均达到了最先进的水平，在指令遵守性和风格/结构保持方面优于最近的基础线。", "conclusion": "通过数据集和SketchAssist的结合，提供了一种实用且可控的助手来支持草图创作与修改。"}}
{"id": "2512.14138", "pdf": "https://arxiv.org/pdf/2512.14138", "abs": "https://arxiv.org/abs/2512.14138", "authors": ["So Kuroki", "Manami Nakagawa", "Shigeo Yoshida", "Yuki Koyama", "Kozuno Tadashi"], "title": "LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.", "AI": {"tldr": "LAPPI 是一种通过自然语言对话帮助用户将模糊偏好转化为优化问题实例的交互式方法，进而生成可行解决方案。", "motivation": "许多现实任务如行程规划和餐点计划可以被定义为组合优化问题，但使用传统优化求解器要求精确的问题实例化。这需要用户指定候选项目、分配偏好数值并设置约束条件，对于普通用户而言较为困难。LAPPI 目的是简化这一过程。", "method": "通过自然语言对话与大模型结合的方式，LAPPI 帮助用户将模糊偏好转化为精确的优化问题实例，并传递给现有的求解器生成解决方案。", "result": "在行程规划的研究中，该方法成功捕捉到了用户的偏好并产生了优于传统和提示工程方案的可行计划。此外，在额外的应用场景中也展示了其灵活性。", "conclusion": "LAPPI 方法通过交互式优化过程克服了普通用户难以使用传统优化求解器的问题，并证明了其在多种应用场景中的有效性和灵活性。"}}
{"id": "2512.14137", "pdf": "https://arxiv.org/pdf/2512.14137", "abs": "https://arxiv.org/abs/2512.14137", "authors": ["Ashish Mishra", "Tarun Kumar", "Gyanaranjan Nayak", "Arpit Shah", "Suparna Bhattacharya", "Martin Foltin"], "title": "Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.", "AI": {"tldr": "介绍了一种针对CLIP模型的非破坏性零样本类别删除方法，通过投影到目标文本嵌入的空间正交基来擦除特定类别的信息。", "motivation": "解决多模态预训练模型中的数据隐私和安全性问题，提供一种高效精确的类别删除技术。", "method": "利用空洞投影技术，在最终投影层上消除特定类别信息，无需重新训练或使用目标类的数据样本。", "result": "实验表明该方法能够有效降低零样本性能对指定类别的影响，并保留整体模型知识。", "conclusion": "提出的方法在处理多模态模型中隐私保护和数据删除方面具有显著优势。"}}
{"id": "2512.14130", "pdf": "https://arxiv.org/pdf/2512.14130", "abs": "https://arxiv.org/abs/2512.14130", "authors": ["Amirmohammad Pasdar", "Toby Murray", "Van-Thuan Pham"], "title": "UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis", "categories": ["cs.CR", "cs.AI"], "comment": "15 pages", "summary": "We introduce UIXPOSE, a source-code-agnostic framework that operates on both compiled and open-source apps. This framework applies Intention Behaviour Alignment (IBA) to mobile malware analysis, aligning UI-inferred intent with runtime semantics. Previous work either infers intent statically, e.g., permission-centric, or widget-level or monitors coarse dynamic signals (endpoints, partial resource usage) that miss content and context. UIXPOSE infers an intent vector from each screen using vision-language models and knowledge structures and combines decoded network payloads, heap/memory signals, and resource utilisation traces into a behaviour vector. Their alignment, calculated at runtime, can both detect misbehaviour and highlight exploration of behaviourally rich paths. In three real-world case studies, UIXPOSE reveals covert exfiltration and hidden background activity that evade metadata-only baselines, demonstrating how IBA improves dynamic detection.", "AI": {"tldr": "UIXPOSE是一种基于意图行为对齐的移动恶意软件检测框架，通过视觉语言模型和知识结构来推断用户界面意图，并结合网络数据包、内存信号等动态信息进行恶意行为分析。", "motivation": "传统的恶意软件检测方法要么静态地根据权限或控件级别的特征来推测意图，要么监控粗略的动态信号（如端点使用），这些方法容易错过内容和上下文。因此，需要一种新的方法结合用户界面和实际运行时的行为进行更精确的检测。", "method": "UIXPOSE框架首先从每个屏幕上推断出一个意图向量，然后将解码后的网络负载、堆/内存信号及资源使用痕迹组合成行为向量。通过对比这两个向量在运行时刻的真实对齐程度来发现恶意软件的行为异常。", "result": "在三个真实世界案例研究中，UIXPOSE检测到了隐秘的数据泄露和隐藏的后台活动，这些活动能够逃避仅基于元数据的方法。", "conclusion": "IBA方法通过将用户界面意图与实际行为进行对齐分析，显著提高了动态恶意软件检测的有效性。"}}
{"id": "2512.14126", "pdf": "https://arxiv.org/pdf/2512.14126", "abs": "https://arxiv.org/abs/2512.14126", "authors": ["Junyi Wu", "Van Nguyen Nguyen", "Benjamin Planche", "Jiachen Tao", "Changchang Sun", "Zhongpai Gao", "Zhenghao Zhao", "Anwesa Choudhuri", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "Consistent Instance Field for Dynamic Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.", "AI": {"tldr": "本文提出了一种名为一致实例场的连续和概率性时空表示方法，用于动态场景理解。", "motivation": "前人的方法依赖于离散跟踪或视图相关特征，难以在复杂场景中保持物体的一致身份识别。为了解决这个问题，论文引入了一种新的基于变形3D高斯的方法来建模每个空间时间点的占用概率和条件实例分布。", "method": "通过可微分栅格化直接从RGB图像和实例掩码中学习出一种新型的嵌入式表示方法，并引入机制校准每个高斯的身份以及重新采样活跃区域的高斯，从而实现一致的空间时间和时间上的实例表征。", "result": "在HyperNeRF和Neu3D数据集上进行实验的结果表明，该方法显著优于现有的全景分割和开放词汇4D查询任务的方法。", "conclusion": "论文提出了一种新的连续和概率性时空表示方法来解决动态场景理解中的一致实例表征问题，并展示了其在多个数据集上的优越性能。"}}
{"id": "2512.14121", "pdf": "https://arxiv.org/pdf/2512.14121", "abs": "https://arxiv.org/abs/2512.14121", "authors": ["Wenbo Tian", "Ruting Lin", "Hongxian Zheng", "Yaodong Yang", "Geng Wu", "Zihao Zhang", "Zhang Zhang"], "title": "SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing intelligent sports analysis systems mainly focus on \"scoring and visualization,\" often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.", "AI": {"tldr": "提出了SportsGPT框架，利用LLM进行可解释的体育运动评估和训练指导。", "motivation": "现有的智能体育分析系统主要关注评分和可视化，缺乏自动性能诊断和可解释性训练指导。通过结合大规模语言模型（LLMs）和动作分析技术的新进展来解决这些问题。", "method": "提出了MotionDTW算法用于准确的关键帧提取；设计了KISMAM模型获取可解释的评估指标；开发了基于Qwen3的SportsRAG训练指导模型，利用领域特定的知识库生成专业性建议。", "result": "实验结果显示，MotionDTW在时间误差和IoU得分方面优于传统方法。消融研究表明KISMAM和SportsRAG有效提升了诊断准确性和专业性。", "conclusion": "SportsGPT框架能够提供更精确的性能评估和专业的训练指导，超越了一般的LLMs表现。"}}
{"id": "2512.14115", "pdf": "https://arxiv.org/pdf/2512.14115", "abs": "https://arxiv.org/abs/2512.14115", "authors": ["Ramesh Gundluru", "Shubham Gupta", "Sri Rama Murty K"], "title": "Joint Multimodal Contrastive Learning for Robust Spoken Term Detection and Keyword Spotting", "categories": ["cs.SD", "cs.LG"], "comment": null, "summary": "Acoustic Word Embeddings (AWEs) improve the efficiency of speech retrieval tasks such as Spoken Term Detection (STD) and Keyword Spotting (KWS). However, existing approaches suffer from limitations, including unimodal supervision, disjoint optimization of audio-audio and audio-text alignment, and the need for task-specific models. To address these shortcomings, we propose a joint multimodal contrastive learning framework that unifies both acoustic and cross-modal supervision in a shared embedding space. Our approach simultaneously optimizes: (i) audio-text contrastive learning, inspired by the CLAP loss, to align audio and text representations and (ii) audio-audio contrastive learning, via Deep Word Discrimination (DWD) loss, to enhance intra-class compactness and inter-class separation. The proposed method outperforms existing AWE baselines on word discrimination task while flexibly supporting both STD and KWS. To our knowledge, this is the first comprehensive approach of its kind.", "AI": {"tldr": "提出了联合多模态对比学习框架，用于改进声学词嵌入以提高语音检索任务的效率。", "motivation": "现有方法在单模态监督、音频-文本对齐优化和任务特定模型需求方面存在局限性。为了解决这些问题，提出了一种新的联合多模态对比学习框架。", "method": "采用统一的嵌入空间同时优化音频-文本对比学习（基于CLAP损失）和音频-音频对比学习（通过深度词区分损失），以增强类内紧凑性和类间分离。", "result": "所提方法在单词区分任务上超越了现有的声学词嵌入基线，并灵活支持术语检测和关键词识别。", "conclusion": "该方法是首个综合性的联合多模态对比学习框架，提高了语音检索任务的效率。"}}
{"id": "2512.14114", "pdf": "https://arxiv.org/pdf/2512.14114", "abs": "https://arxiv.org/abs/2512.14114", "authors": ["Rui-Yang Ju", "KokSheik Wong", "Yanlin Jin", "Jen-Shiun Chiang"], "title": "MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction", "categories": ["cs.CV"], "comment": "Extended Journal Version of APSIPA ASC 2025", "summary": "Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.", "AI": {"tldr": "提出了一种基于多尺度特征提取的高效生成对抗网络框架MFE-GAN，用于文档图像增强和二值化。", "motivation": "为了减少训练和推断时间，并提高光学字符识别系统的效率和准确性，同时处理彩色文档图像中的阴影和噪声问题。", "method": "引入了Haar小波变换和归一化处理文档图像，设计了新的生成器、判别器和损失函数以优化模型性能。并进行了消融研究来证明其有效性。", "result": "实验结果表明，在Benchmark、Nabuco和CMATERdb数据集上，所提出的MFE-GAN在减少总训练和推断时间的同时，保持了与最先进方法相当的性能。", "conclusion": "通过结合多尺度特征提取和Haar小波变换，MFE-GAN实现了高效的文档图像增强和二值化处理，并减少了模型训练和推理的时间。"}}
{"id": "2512.14113", "pdf": "https://arxiv.org/pdf/2512.14113", "abs": "https://arxiv.org/abs/2512.14113", "authors": ["Ashish Mishra", "Gyanaranjan Nayak", "Tarun Kumar", "Arpit Shah", "Suparna Bhattacharya", "Martin Foltin"], "title": "Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or \"unlearning\") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.", "AI": {"tldr": "提出了一种无需重新训练和额外数据的预训练模型遗忘框架，可以针对性地移除特定对象类别。", "motivation": "在实际应用中需要能够去除特定类别的信息而不影响其他任务的表现，现有的基于重训练的方法存在局限性。", "method": "通过利用CLIP的多模态零空间，结合文本提示和合成视觉原型，有效地移除了不需要的分类信息并保留了原有知识。", "result": "实验表明该方法能够高效地实现选择性的遗忘，并且在未被忘记的任务上保持高性能。", "conclusion": "所提出的方法提供了一种灵活、计算效率高的解决方案来控制模型遗忘。"}}
{"id": "2512.14112", "pdf": "https://arxiv.org/pdf/2512.14112", "abs": "https://arxiv.org/abs/2512.14112", "authors": ["Chunan Tong"], "title": "Optimizing Multi-Tier Supply Chain Ordering with a Hybrid Liquid Neural Network and Extreme Gradient Boosting Model", "categories": ["cs.AI"], "comment": null, "summary": "Supply chain management (SCM) faces significant challenges like demand fluctuations and the bullwhip effect. Traditional methods and even state-of-the-art LLMs struggle with benchmarks like the Vending Machine Test, failing to handle SCM's complex continuous time-series data. While ML approaches like LSTM and XGBoost offer solutions, they are often limited by computational inefficiency. Liquid Neural Networks (LNN), known for their adaptability and efficiency in robotics, remain untapped in SCM. This study proposes a hybrid LNN+XGBoost model for multi-tier supply chains. By combining LNN's dynamic feature extraction with XGBoost's global optimization, the model aims to minimize the bullwhip effect and increase profitability. This innovative approach addresses the need for efficiency and adaptability, filling a critical gap in intelligent SCM.", "AI": {"tldr": "提出了一种结合液体神经网络（LNN）和极端梯度提升模型（XGBoost）的混合方法，用于优化多级供应链中的订单管理。", "motivation": "传统方法及最新的LLMs在处理供应链复杂的时间序列数据时面临挑战，并且难以应对需求波动和牛鞭效应等问题。提出一种新方法旨在解决现有技术效率低下的问题，提高供应链智能化管理水平。", "method": "结合LNN的动态特征提取能力和XGBoost的整体优化能力，以减少牛鞭效应并增加盈利能力。", "result": "创新的方法能更有效地处理供应链中的复杂数据，并提升整体管理效果。", "conclusion": "混合模型在智能供应链管理中具有重要应用价值，填补了效率与适应性方面的空白。"}}
{"id": "2512.14111", "pdf": "https://arxiv.org/pdf/2512.14111", "abs": "https://arxiv.org/abs/2512.14111", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Tao Teng", "Sylvain Calinon", "Darwin Caldwell", "Fei Chen"], "title": "Interactive Motion Planning for Human-Robot Collaboration Based on Human-Centric Configuration Space Ergonomic Field", "categories": ["cs.RO"], "comment": "10 pages, 9 figures", "summary": "Industrial human-robot collaboration requires motion planning that is collision-free, responsive, and ergonomically safe to reduce fatigue and musculoskeletal risk. We propose the Configuration Space Ergonomic Field (CSEF), a continuous and differentiable field over the human joint space that quantifies ergonomic quality and provides gradients for real-time ergonomics-aware planning. An efficient algorithm constructs CSEF from established metrics with joint-wise weighting and task conditioning, and we integrate it into a gradient-based planner compatible with impedance-controlled robots. In a 2-DoF benchmark, CSEF-based planning achieves higher success rates, lower ergonomic cost, and faster computation than a task-space ergonomic planner. Hardware experiments with a dual-arm robot in unimanual guidance, collaborative drilling, and bimanual cocarrying show faster ergonomic cost reduction, closer tracking to optimized joint targets, and lower muscle activation than a point-to-point baseline. CSEF-based planning method reduces average ergonomic scores by up to 10.31% for collaborative drilling tasks and 5.60% for bimanual co-carrying tasks while decreasing activation in key muscle groups, indicating practical benefits for real-world deployment.", "AI": {"tldr": "提出了一种基于人体中心配置空间的力学场（CSEF）的方法，用于实现人机协作中的运动规划。", "motivation": "工业中的人机合作需要碰撞避免、响应快且符合人体工学的安全性以减少疲劳和肌肉骨骼风险。", "method": "构建了一个连续可微分的力学场（CSEF），该场量化学科质量并提供实时学科意识规划所需的梯度。一种高效的算法从已建立的指标中构造了CSEF，并将其集成到与阻抗控制机器人兼容的基于梯度的规划器中。", "result": "在2-自由度基准测试中，基于CSEF的规划实现了更高的成功率、更低的学科成本和更快的计算速度。硬件实验显示，在引导、协作钻孔和双手搬运任务中，基于CSEF的规划方法比点到点基线更快地减少学科成本，并且肌肉激活较低。", "conclusion": "通过降低关键肌群的激活水平，表明了该方法在实际部署中的潜在效益。"}}
{"id": "2512.14106", "pdf": "https://arxiv.org/pdf/2512.14106", "abs": "https://arxiv.org/abs/2512.14106", "authors": ["Ijaz Ul Haq", "Byung Suk Lee", "Julia N. Perdrial", "David Baude"], "title": "HydroGEM: A Self Supervised Zero Shot Hybrid TCN Transformer Foundation Model for Continental Scale Streamflow Quality Control", "categories": ["cs.AI"], "comment": "Supplementary materials, datasets, and implementation code will be made publicly available upon acceptance for publication in a peer-reviewed journal", "summary": "Real-time streamflow monitoring networks generate millions of observations annually, yet maintaining data quality across thousands of remote sensors remains labor-intensive. We introduce HydroGEM (Hydrological Generalizable Encoder for Monitoring), a foundation model for continental-scale streamflow quality control. HydroGEM uses two-stage training: self-supervised pretraining on 6.03 million sequences from 3,724 USGS stations learns hydrological representations, followed by fine-tuning with synthetic anomalies for detection and reconstruction. A hybrid TCN-Transformer architecture (14.2M parameters) captures local temporal patterns and long-range dependencies, while hierarchical normalization handles six orders of magnitude in discharge. On held-out synthetic tests comprising 799 stations with 18 expert-validated anomaly types, HydroGEM achieves F1 = 0.792 for detection and 68.7% reconstruction-error reduction, a 36.3% improvement over existing methods. Zero-shot transfer to 100 Environment and Climate Change Canada stations yields F1 = 0.586, exceeding all baselines and demonstrating cross-national generalization. The model maintains consistent detection across correction magnitudes and aligns with operational seasonal patterns. HydroGEM is designed for human-in-the-loop workflows - outputs are quality control suggestions requiring expert review, not autonomous corrections.", "AI": {"tldr": "本文介绍了一种用于大陆规模水流质量控制的基础模型HydroGEM。", "motivation": "实时水流监测网络每年生成数百万观测值，但在数千个远程传感器上维持数据质量仍然是劳动密集型的工作。因此，该研究旨在开发一种能够解决大规模、跨国家的数据质量问题的自动化系统。", "method": "HydroGEM采用了两阶段训练：自监督预训练在来自3724个USGS站的603万个序列上学到水流表示，随后通过合成异常进行微调以实现检测和重建。采用混合TCN-Transformer架构（14.2百万参数）捕捉局部时间模式和长程依赖性，同时分层归一化处理六个数量级的排水量。", "result": "在包含799个站点的18种专家验证异常类型的留出合成测试中，HydroGEM实现了F1 = 0.792的检测率和68.7%的重建误差减少，比现有方法高出36.3%。零样本迁移至Environment and Climate Change Canada的100个站点达到F1 = 0.586，超过所有基准。", "conclusion": "HydroGEM展示了跨国家的数据质量控制能力，并设计为人机协作工作流程——输出为需要专家审查的质量控制建议，而不是自动修正。"}}
{"id": "2512.14102", "pdf": "https://arxiv.org/pdf/2512.14102", "abs": "https://arxiv.org/abs/2512.14102", "authors": ["Emanuele Mezzi", "Gertjan Burghouts", "Maarten Kruithof"], "title": "Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": null, "summary": "Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.", "AI": {"tldr": "本文提出了一种结合大语言模型和神经符号AI的方法RUNE，用于基于复杂查询的遥感文本到图像检索。", "motivation": "现有的遥感大型视觉-语言模型在解释性和处理复杂空间关系方面存在不足，影响了其实际应用。因此，提出了RUNE方法以提高性能、可解释性及鲁棒性。", "method": "RUNE通过利用大语言模型生成一阶逻辑表达式并结合神经符号推理模块来检索图像。为了解决大规模问题，引入了一种逻辑分解策略以减少执行时间。该方法仅使用基础模型生成FOL表达式，而不是用于端到端的检索。", "result": "实验结果显示RUNE在复杂遥感文本到图像检索任务中优于联合嵌入模型，同时提高了性能、鲁棒性和可解释性。", "conclusion": "本文展示了RUNE方法在处理复杂查询和不确定性时的优势，并通过洪水后卫星图像检索的应用案例证明了其在现实世界中的潜力。"}}
{"id": "2512.14099", "pdf": "https://arxiv.org/pdf/2512.14099", "abs": "https://arxiv.org/abs/2512.14099", "authors": ["Ruishu Zhu", "Zhihao Huang", "Jiacheng Sun", "Ping Luo", "Hongyuan Zhang", "Xuelong Li"], "title": "ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.", "AI": {"tldr": "本文提出了一种名为ViewMask-1-to-3的多视角图像生成方法，通过离散扩散模型实现从单幅图像和文本描述生成多个视图。", "motivation": "多视角图像生成任务挑战性在于保持不同视角之间的几何一致性。现有方法需要依赖复杂的三维结构或大量多视角训练数据。", "method": "ViewMask-1-to-3将多视角合成视为离散序列建模问题，利用MAGVIT-v2进行视觉标记化，并通过随机掩码和自注意力机制实现跨视图一致性的生成。", "result": "该方法在GSO和3D-FUTURE数据集上以PSNR、SSIM和LPIPS指标排名第一，展示了离散扩散模型在多视角图像生成任务中的可行性和简洁性。", "conclusion": "通过展示离散扩散模型的有效性，证明其为现有复杂三维几何约束或特殊注意架构的替代方案。"}}
{"id": "2512.14096", "pdf": "https://arxiv.org/pdf/2512.14096", "abs": "https://arxiv.org/abs/2512.14096", "authors": ["Ruitong Sun", "Tianze Yang", "Wei Niu", "Jin Sun"], "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration", "categories": ["cs.CV"], "comment": "29 pages", "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.", "AI": {"tldr": "OUSAC通过优化和自适应缓存策略加速扩散模型（DiT）的生成过程，减少计算成本同时提高图像质量。", "motivation": "为了克服扩散模型中Classifier-Free Guidance (CFG)带来的高计算开销，以及标准缓存方法在变尺度指导下的无效性问题，提出了一种优化框架来提高模型效率和效果。", "method": "OUSAC采用两阶段策略：第一阶段利用进化算法优化哪些时间点可以省略无条件前向传递并调整相应的指导规模；第二阶段引入自适应秩分配技术以解决不同变换器块在动态条件下受影响程度不同的问题，保持缓存的有效性。", "result": "实验表明，OUSAC相比现有的加速方法有显著优势，在DiT-XL/2（ImageNet 512x512）上节省了53%的计算成本并提高了15%的质量；在PixArt-alpha（MSCOCO）上实现了60%的成本节约和16.1%的质量提升，同时FLUX模型加速达到5倍。", "conclusion": "OUSAC通过系统优化显著提升了扩散模型DiT的生成效率与效果，展示了在多种场景下的优越性。"}}
{"id": "2512.14095", "pdf": "https://arxiv.org/pdf/2512.14095", "abs": "https://arxiv.org/abs/2512.14095", "authors": ["Sisi Dai", "Kai Xu"], "title": "AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation", "categories": ["cs.CV"], "comment": "AAAI 2026", "summary": "Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.", "AI": {"tldr": "提出了一种名为AnchorHOI的新框架，通过锚点基优先级提取策略生成4D人与物体交互场景。", "motivation": "克服现有零样本4D HOI生成方法中由于缺乏大规模数据集而导致的可扩展性限制问题。", "method": "引入基于锚点的优先级提取策略，使用视频扩散模型而非单纯图像扩散模型进行4D HOI生成。", "result": "实验表明AnchorHOI在多样性与泛化能力方面优于先前方法。", "conclusion": "通过锚定神经辐射场和关键点设计，AnchorHOI能够实现更加多样且具现实感的零样本4D人与物体交互场景生成。"}}
{"id": "2512.14093", "pdf": "https://arxiv.org/pdf/2512.14093", "abs": "https://arxiv.org/abs/2512.14093", "authors": ["Nhi Nguyen", "Constantino Álvarez Casado", "Le Nguyen", "Manuel Lage Cañellas", "Miguel Bordallo López"], "title": "Quality-Aware Framework for Video-Derived Respiratory Signals", "categories": ["cs.CV", "eess.SP"], "comment": "6 pages, 1 figure, 2 tables, conference", "summary": "Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.", "AI": {"tldr": "提出了一个预测性的质量感知框架，用于视频衍生的呼吸信号估计。", "motivation": "视频基础的呼吸率（RR）估计由于提取方法的质量不一致而经常不可靠。本文旨在通过整合不同信号源并动态评估可靠性来提高准确性。", "method": "从面部远程光电容积描记法（rPPG），上身运动和深度学习管道中抽取了十个信号，使用Welch方法、MUSIC、快速傅里叶变换（FFT）和峰值检测四种谱估计器进行分析。然后利用片段级别的质量指数来训练机器学习模型以预测准确度或选择最可靠的信号。", "result": "在三个公开数据集上的实验表明，所提出的框架在大多数情况下实现了比单独方法更低的RR估算误差，并且性能提升取决于数据集特性。", "conclusion": "该研究证明了质量驱动的预测建模对于提供可扩展和通用化的视频基础呼吸监测解决方案具有潜力。"}}
{"id": "2512.14092", "pdf": "https://arxiv.org/pdf/2512.14092", "abs": "https://arxiv.org/abs/2512.14092", "authors": ["Felix Holm", "Ghazal Ghazaei", "Nassir Navab"], "title": "ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes", "categories": ["cs.CV", "cs.AI"], "comment": ":I.2.10", "summary": "Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner. Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis. Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications. Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.", "AI": {"tldr": "提出了一种名为ProtoFlow的框架，用于学习动态场景图原型以解释性且鲁棒的方式建模复杂的手术工作流程。", "motivation": "详细手术识别对于推进AI辅助手术至关重要，然而进展受到高标注成本、数据稀缺以及缺乏可解释模型的影响。虽然场景图提供了对手术事件的结构化抽象，但它们的全部潜力尚未充分利用。", "method": "ProtoFlow利用图神经网络（GNN）编码器-解码器架构，结合自监督预训练以进行丰富的表示学习，并通过基于原型的微调阶段发现和细化核心原型。这些原型封装了反复出现、临床意义重大的手术互动模式，为工作流程分析提供了可解释的基础。", "result": "在细粒度CAT-SG数据集上评估该方法，ProtoFlow不仅超越标准GNN基线的整体准确性，并且在有限数据、少量样本场景中也表现出色，在仅基于一个手术视频的训练下仍能保持较强性能。定性分析进一步表明，学习到的原型能够识别不同的手术子技术并提供清晰可解释的工作流程偏差和罕见并发症洞察。", "conclusion": "通过将强大的表示学习与内在的可解释性相结合，ProtoFlow代表了向开发更透明、可靠且数据高效的AI系统的重大进展，加速其在手术培训、实时决策支持以及工作流优化方面的临床应用潜力。"}}
{"id": "2512.14090", "pdf": "https://arxiv.org/pdf/2512.14090", "abs": "https://arxiv.org/abs/2512.14090", "authors": ["Taig Singh", "Shreshth Rajan", "Nikhil Iyer"], "title": "Arithmetic-Intensity-Aware Quantization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.", "AI": {"tldr": "提出了一种基于算术强度的量化框架，以在保持精度的同时最大化推理吞吐量。", "motivation": "随着现代神经网络变得越来越内存受限，推断吞吐量受到DRAM带宽而不是计算能力的限制。通过调整各层的比特宽度来提高算术强度并减少精度损失是关键。", "method": "AIQ是一种后训练量化方法，使用搜索算法在各层量化方案上进行优化，以最小化加权算术强度和准确率的损失。", "result": "在ResNet-20/CIFAR-10数据集上，AIQ相对于FP32基线提高了约50%的算术强度，并将测试精度保持在约1个百分点内；在MobileNetV2架构中，AIQ配置比FP32基线高1.66倍。", "conclusion": "通过选择各层合适的比特宽度，可以有效提高神经网络的推理吞吐量，同时保持较高的准确率。"}}
{"id": "2512.14087", "pdf": "https://arxiv.org/pdf/2512.14087", "abs": "https://arxiv.org/abs/2512.14087", "authors": ["Yang Yang", "Risa Shinoda", "Hiroaki Santo", "Fumio Okura"], "title": "GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants", "categories": ["cs.CV"], "comment": "Submitted to IEEE TPAMI, under review", "summary": "We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.", "AI": {"tldr": "本文提出了一种名为GaussianPlant的方法，用于从多视角图像中联合恢复植物的外观和内部结构。", "motivation": "传统的3D高斯点云（3DGS）在场景外观重建方面表现良好，但在表示如植物分支模式等内部结构时存在局限性。因此，需要一种方法来同时实现高保真外观和精确结构重建。", "method": "GaussianPlant通过引入层次化的3DGS表示法，使用结构原语（StPs）和外观原语（ApPs），分别用于模型植物的几何形状和外观。StPs采用简化形式表示枝干和叶片，并通过自组织方式优化其属性以准确区分分支与叶子；ApPs则绑定到每个StP上，代表分支或叶片的外观。", "result": "实验结果显示GaussianPlant能够实现高保真的外观重建并通过StPs进行精确结构重建，从而提取出分枝结构和叶片实例。", "conclusion": "该方法在植物外观与内部结构的联合重建方面取得了显著效果。"}}
{"id": "2512.14085", "pdf": "https://arxiv.org/pdf/2512.14085", "abs": "https://arxiv.org/abs/2512.14085", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Taiga Mori", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study", "categories": ["cs.CL", "cs.HC", "cs.SD"], "comment": "This paper has been accepted for presentation at International Workshop on Spoken Dialogue Systems Technology 2026 (IWSDS 2026) and represents the author's version of the work", "summary": "We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.", "AI": {"tldr": "构建一个多语言的、连续背景音预测模型，用于研究不同语言之间的时序行为。", "motivation": "为了理解跨语种背景下回应的时间差异，并设计更加自然和文化敏感的对话系统。", "method": "基于Transformer框架，开发了一个多语言、帧级操作的背景音预测模型，并通过辅助任务联合训练大约300小时双人会话数据。", "result": "结果显示，该模型在三种语言中均超过了单语基线；零样本迁移显示两种语言训练受限，表明跨语种差异显著；扰动分析揭示了不同语言对线索的依赖性差异；上下文长度的研究进一步展示了不同语言对于背景音预测的适应性。", "conclusion": "研究提供了一个统一的模型和实证证据，说明了背景音时间上的语言差异，并为设计更加自然、文化敏感的对话系统提供了依据。"}}
{"id": "2512.14083", "pdf": "https://arxiv.org/pdf/2512.14083", "abs": "https://arxiv.org/abs/2512.14083", "authors": ["Sungnyun Kim"], "title": "Scalable Frameworks for Real-World Audio-Visual Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG"], "comment": "PhD Dissertation", "summary": "The practical deployment of Audio-Visual Speech Recognition (AVSR) systems is fundamentally challenged by significant performance degradation in real-world environments, characterized by unpredictable acoustic noise and visual interference. This dissertation posits that a systematic, hierarchical approach is essential to overcome these challenges, achieving the robust scalability at the representation, architecture, and system levels. At the representation level, we investigate methods for building a unified model that learns audio-visual features inherently robust to diverse real-world corruptions, thereby enabling generalization to new environments without specialized modules. To address architectural scalability, we explore how to efficiently expand model capacity while ensuring the adaptive and reliable use of multimodal inputs, developing a framework that intelligently allocates computational resources based on the input characteristics. Finally, at the system level, we present methods to expand the system's functionality through modular integration with large-scale foundation models, leveraging their powerful cognitive and generative capabilities to maximize final recognition accuracy. By systematically providing solutions at each of these three levels, this dissertation aims to build a next-generation, robust, and scalable AVSR system with high reliability in real-world applications.", "AI": {"tldr": "构建面向现实世界的音频视觉语音识别系统的可扩展框架", "motivation": "解决实际环境中由于不可预测的声学噪声和视觉干扰而导致的性能下降问题，推动音频视觉语音识别系统在真实环境中的应用。", "method": "通过层级化方法，在特征表示、架构设计及系统集成三个层面提出解决方案：开发统一模型以提高鲁棒性；探索多模态输入的高效扩展与资源分配策略；利用大规模基础模型增强系统的认知和生成能力，提升最终识别精度。", "result": "该研究成功构建了一个具备高可靠性的下一代音频视觉语音识别系统框架。", "conclusion": "通过在特征表示、架构设计及系统集成三个层面提供解决方案，该论文提出了一种面向现实世界应用的可扩展音频视觉语音识别系统。"}}
{"id": "2512.14080", "pdf": "https://arxiv.org/pdf/2512.14080", "abs": "https://arxiv.org/abs/2512.14080", "authors": ["Wentao Guo", "Mayank Mishra", "Xinle Cheng", "Ion Stoica", "Tri Dao"], "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.", "AI": {"tldr": "本文提出了SonicMoE，一种用于加速混合专家（MoE）模型的新方法。", "motivation": "近年来，高精细度的混合专家模型和高度稀疏化的混合专家模型显示出在不显著增加计算成本的情况下提高模型质量的趋势。然而，这些模型面临着激活内存开销增大和硬件效率降低的问题，以及由于分组GEMM内核填充导致的浪费计算问题。", "method": "提出了一个内存高效的算法来实现前向和后向传递中的混合专家模型，并设计了重叠内存I/O与计算的GPU内核。此外，还提出了一种新的“令牌舍入”方法，以最小化由于分组GEMM内核填充导致的浪费计算。", "result": "SonicMoE相比ScatterMoE在70亿参数模型上实现了45%的激活内存减少和1.86倍的Hopper GPU计算吞吐量提高。使用64个H100 GPU时，训练吞吐量达到2130亿token/天，与96个H100 GPU上的ScatterMoE相当。", "conclusion": "SonicMoE有效解决了混合专家模型在高精细度和高度稀疏化设置下的内存开销和计算效率问题，并通过开源所有内核加速了混合专家模型的训练。"}}
{"id": "2512.14079", "pdf": "https://arxiv.org/pdf/2512.14079", "abs": "https://arxiv.org/abs/2512.14079", "authors": ["Mayank Singh", "Vikas Yadav", "Shiva Krishna Reddy Malay", "Shravan Nayak", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Eduardo Blanco"], "title": "Grammar Search for Multi-Agent Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.", "AI": {"tldr": "提出了一种通过固定组件集进行多智能体系统搜索的结构化框架，该框架在数学和问答任务上的性能优于现有方法。", "motivation": "当前多智能体系统的自动搜索主要依赖于大型语言模型的自由形式搜索。这种方法缺乏结构性且成本较高。因此，需要一种更高效、可解释性更强的方法来生成多智能体系统。", "method": "通过使用一组固定但可组合的基本组件来进行候选生成，而不是依赖于大型语言模型的自由形式搜索方法。", "result": "该方法在四个评估基准中表现出色，并且比现有技术更具成本效益。此外，它产生的多智能体系统更加模块化和易于理解。", "conclusion": "提出的基于固定组件集的方法不仅提高了性能，还提供了更高效的搜索过程以及可解释性更好的结果。"}}
{"id": "2512.14069", "pdf": "https://arxiv.org/pdf/2512.14069", "abs": "https://arxiv.org/abs/2512.14069", "authors": ["Junjie Ma", "Jinlong Li"], "title": "RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees", "categories": ["cs.AI"], "comment": "5 pages, 2 figures", "summary": "Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.", "AI": {"tldr": "RADAR是一种基于强化学习的动态预选树生成方法，用于加速大规模语言模型中的推断过程。", "motivation": "现代大型语言模型（LLMs）在推断过程中消耗大量资源并且速度慢。现有的投机采样方法存在超参数固定的问题，缺乏灵活性。", "method": "RADAR将预选树的生成过程描述为马尔可夫决策过程，并通过离线强化学习训练预测模型，实现对预选模型调用的实时决策，减少冗余计算并进一步加速推断。", "result": "实验结果显示，与自回归解码基线相比，RADAR在三个大型语言模型和四个任务上的速度提高了3.17倍到4.82倍。", "conclusion": "该方法通过引入动态决策机制有效提升了大规模语言模型推断的速度。"}}
{"id": "2512.14068", "pdf": "https://arxiv.org/pdf/2512.14068", "abs": "https://arxiv.org/abs/2512.14068", "authors": ["Shuang Cheng", "Yuhua Jiang", "Zineng Zhou", "Dawei Liu", "Wang Tao", "Linfeng Zhang", "Biqing Qi", "Bowen Zhou"], "title": "SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \\textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \\emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \\textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \\textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \\textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \\emph{training efficiency}, \\emph{convergence stability}, and \\emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.", "AI": {"tldr": "提出了一种新的块级离散扩散方法SDAR-VL，用于大规模视觉语言理解任务。", "motivation": "现有的块级离散扩散方法面临高训练成本、慢收敛速度和不稳定的问题，限制了其在实际应用中的采用。本文旨在解决这些问题并提高其性能。", "method": "通过异步块级噪声调度、有效的掩码比例缩放以及逐步贝塔噪声课程来统一框架，以实现更高效的稳定训练。", "result": "实验结果表明，SDAR-VL方法在单图像、多图像和视频基准上表现出一致的改进效果，在训练效率、收敛稳定性及任务性能方面超过传统块级扩散模型，并且达到或超过了强大的自回归基线。", "conclusion": "通过引入新的框架和技术，SDAR-VL解决了块级离散扩散方法存在的问题并取得了显著的效果，确立了其作为视觉语言理解任务中的实用骨干网络的地位。"}}
{"id": "2512.14067", "pdf": "https://arxiv.org/pdf/2512.14067", "abs": "https://arxiv.org/abs/2512.14067", "authors": ["Yonggan Fu", "Lexington Whalen", "Zhifan Ye", "Xin Dong", "Shizhe Diao", "Jingyu Liu", "Chengyue Wu", "Hao Zhang", "Enze Xie", "Song Han", "Maksim Khadkevich", "Jan Kautz", "Yingyan Celine Lin", "Pavlo Molchanov"], "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.", "AI": {"tldr": "本文研究了将自回归（AR）模型转换为高效的扩散语言模型（dLM），以提高生成速度并保持任务准确性。", "motivation": "虽然扩散语言模型因其可以实现并行、非自回归生成而前景广阔，但在从头训练时其学习效率不及自回归语言模型。本文旨在通过研究AR到dLM的转换来解决这一问题，并提出更有效的转化原则和方法论，以提高速度同时保持准确性。", "method": "首先系统地对比了不同的注意力模式并发现维护预训练的AR权重分布对于有效AR到dLM转换至关重要。其次提出了一个连续预训练方案结合块式注意力模式，在每个块内实现双向建模的同时保持跨块因果性，解决了预训练AR模型在完全双向建模中的权重分布问题，并引入位置依赖的令牌掩码策略以解决掩码令牌分布的训练测试差距。", "result": "本文提出的Efficient-DLM家族不仅超越了最先进的自回归（AR）和扩散语言模型，而且在效率上也有显著提升。例如，Efficient-DLM 8B相较于Dream 7B和Qwen3 4B分别实现了5.4%/2.7%更高的准确性以及4.5x/2.7x的更高吞吐量。", "conclusion": "本文通过系统地研究了dLMs的注意力模式、训练动态和其他设计选择，提供了可操作的见解以实现可扩展的AR到dLM转换。Efficient-DLM家族在保持准确性的同时极大地提升了速度，并超越了现有模型的表现。"}}
{"id": "2512.14061", "pdf": "https://arxiv.org/pdf/2512.14061", "abs": "https://arxiv.org/abs/2512.14061", "authors": ["Hao Chen", "Junyang Chen", "Jinshan Pan", "Jiangxin Dong"], "title": "Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution", "categories": ["cs.CV"], "comment": "Project page: https://github.com/Chanson94/CODSR", "summary": "Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.", "AI": {"tldr": "本文提出了CODSR，一种基于可控一步扩散的图像超分辨率方法。", "motivation": "当前的一步法在图像超分辨率领域取得了显著进展，但仍受限于信息损失、区域辨识度不足和文本提示与语义区域不匹配等问题。CODSR旨在解决这些问题，提高图像超分辨率的质量。", "method": "CODSR通过LQ引导特征调制模块利用低质量输入的原始未压缩信息来提供高保真条件；开发了区域自适应生成先验激活方法以增强感知丰富度而不牺牲局部结构保真度；采用文本匹配指导策略充分利用文本提示的条件潜力。", "result": "CODSR在感知质量和竞争性保真度方面优于最先进的方法，并且具有高效的一步推理能力。", "conclusion": "本文提出的CODSR能够在保持高效计算的同时，显著提高图像超分辨率的质量和保真度。"}}
{"id": "2512.14058", "pdf": "https://arxiv.org/pdf/2512.14058", "abs": "https://arxiv.org/abs/2512.14058", "authors": ["Zulin Zhuang", "Yu Bian"], "title": "Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.", "AI": {"tldr": "该论文提出了一个用于实时预测室内工作面光照分布的多模态深度学习框架，以支持日光联动控制。", "motivation": "在建筑中利用日光联动控制系统具有显著节能潜力，尤其是在能够准确实时预测室内工作面光照强度的情况下。然而，大多数现有的研究都是针对静态场景进行开发和测试的。", "method": "该研究提出了一种多模态深度学习框架，从非侵入性图像的时间空间特征中实时预测室内工作面光照分布，并只提取侧窗区域的图像特征而非内部像素，使其适用于动态占用的室内空间。在广州市的一个测试房间进行了现场实验并收集了17344个样本用于模型训练和验证。", "result": "该模型在相同分布的测试集上达到了R2>0.98，RMSE<0.14，在未见过的一天的测试集中也表现良好，即R2>0.82，RMSE<0.17，显示出高准确性和可接受的时间泛化能力。", "conclusion": "该研究成功开发了一种能够实时预测室内工作面光照分布的多模态深度学习框架，这对实施日光联动控制系统具有重要意义。"}}
{"id": "2512.14057", "pdf": "https://arxiv.org/pdf/2512.14057", "abs": "https://arxiv.org/abs/2512.14057", "authors": ["Amir M. Soufi Enayati", "Homayoun Honari", "Homayoun Najjaran"], "title": "Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning", "categories": ["cs.RO"], "comment": ":68T40 (Primary) 68T30 (Secondary)", "summary": "Reinforcement learning (RL) enables robots to operate in uncertain environments, but standard approaches often struggle with poor generalization to unseen tasks. Context-adaptive meta reinforcement learning addresses these limitations by conditioning on the task representation, yet they mostly rely on complete action information in the experience making task inference tightly coupled to a specific policy. This paper introduces Context Representation via Action Free Transformer encoder decoder (CRAFT), a belief model that infers task representations solely from sequences of states and rewards. By removing the dependence on actions, CRAFT decouples task inference from policy optimization, supports modular training, and leverages amortized variational inference for scalable belief updates. Built on a transformer encoder decoder with rotary positional embeddings, the model captures long range temporal dependencies and robustly encodes both parametric and non-parametric task variations. Experiments on the MetaWorld ML-10 robotic manipulation benchmark show that CRAFT achieves faster adaptation, improved generalization, and more effective exploration compared to context adaptive meta--RL baselines. These findings highlight the potential of action-free inference as a foundation for scalable RL in robotic control.", "AI": {"tldr": "提出了CRAFT模型，通过状态和奖励序列而非动作信息来推断任务表示。", "motivation": "传统的强化学习方法在处理未见任务时泛化能力较差。现有的上下文适应性元强化学习依赖于完整的行动信息，导致任务推理与特定策略紧密耦合，难以实现模块化训练。", "method": "CRAFT模型利用Transformer编码器解码器从状态和奖励序列中推断出任务表示，并采用旋转位置嵌入捕捉长期时间依赖关系。它通过概率变分推理进行可扩展的信念更新。", "result": "实验表明，相比于上下文适应性元强化学习基线方法，CRAFT模型在MetaWorld ML-10机器人操作基准测试中实现了更快的学习速度、更好的泛化能力和更有效的探索策略。", "conclusion": "该研究展示了无动作推理作为大规模机器人控制中可扩展强化学习基础的潜力。"}}
{"id": "2512.14056", "pdf": "https://arxiv.org/pdf/2512.14056", "abs": "https://arxiv.org/abs/2512.14056", "authors": ["Kim Sung-Bin", "Joohyun Chang", "David Harwath", "Tae-Hyun Oh"], "title": "FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://facedit.github.io/", "summary": "Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.", "AI": {"tldr": "本文提出了一种统一的说话人脸编辑和生成方法FacEDiT，通过面部动作填充来实现两者之间的无缝过渡。", "motivation": "当前的人脸编辑和生成任务通常被视为独立的问题。作者认为这两个问题可以通过一个统一的形式化——基于语音条件的面部运动填充来解决，并引入了一个新的数据集来评估这些任务。", "method": "提出了一种基于语音条件的扩散转换器FacEDiT，使用流匹配进行训练。该模型通过学习合成被屏蔽的面部动作以适应周围的动作和语音，实现了局部生成和编辑的能力，并通过偏置注意力和时间平滑性约束提高了边界连续性和唇部同步。", "result": "实验结果表明，说话人脸编辑和生成可以作为基于语音条件的运动填充任务的一部分；FacEDiT能够产生准确、与语音对齐的人脸修改，同时保持身份识别和视觉上的平滑连续性，并有效推广到说话人脸生成任务中。", "conclusion": "本文提出了一种新的方法来统一处理说话人脸编辑和生成问题，通过引入面部动作填充作为核心任务，以及一个专门用于评估的基准数据集，为这一领域的发展做出了贡献。"}}
{"id": "2512.14054", "pdf": "https://arxiv.org/pdf/2512.14054", "abs": "https://arxiv.org/abs/2512.14054", "authors": ["Humaira Tasnim", "Ashik E Rasul", "Bruce Jo", "Hyung-Jin Yoon"], "title": "Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.", "AI": {"tldr": "论文提出了一个基于双专家的感知框架，用于增强自主空中车辆（AAV）在GPS受限或视觉条件退化情况下的可靠着陆。", "motivation": "现代检测器如YOLOv8虽然表现良好，但在极端尺度转换下（即从高空到接近地面时），单一模型管道难以保持稳健性。为解决这一问题，论文提出了一种双专家感知框架以提高AAV在不同高度范围内的着陆精度和稳定性。", "method": "通过训练两个专门针对远距离和近距离检测的YOLOv8专家，并利用几何门控机制选择与AAV视角最一致的预测结果，在闭环着陆环境中测试了该方法的有效性。", "result": "实验结果显示，相比于单一检测器基线模型，提出的双专家感知模块在着陆精度、稳定性以及整体稳健性方面有了显著提高。", "conclusion": "通过引入针对尺度敏感问题优化的多专家AAV框架策略，论文为自主下降阶段中基于视觉感知的任务提供了新的解决方案和坚实基础。"}}
{"id": "2512.14052", "pdf": "https://arxiv.org/pdf/2512.14052", "abs": "https://arxiv.org/abs/2512.14052", "authors": ["HyperAI Team", "Yuchen Liu", "Kaiyang Han", "Zhiqiang Xia", "Yuhang Dong", "Chen Song", "Kangyu Tang", "Jiaming Xu", "Xiushi Feng", "WenXuan Yu", "Li Peng", "Mingyang Wang", "Kai Wang", "Changpeng Yang", "Yang Li", "Haoyu Lu", "Hao Wang", "Bingna Xu", "Guangyao Liu", "Long Huang", "Kaibin Guo", "Jinyang Wu", "Dan Wu", "Hongzhen Wang", "Peng Zhou", "et al. (4 additional authors not shown)"], "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices", "categories": ["cs.CV", "cs.CL"], "comment": "Technical report of Xiaomi HyperAI Team", "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.", "AI": {"tldr": "本文提出了一种高效的多模态大语言模型HyperVL，旨在解决当前多模态模型在设备端部署时的计算和内存需求问题。", "motivation": "目前的多模态大语言模型虽然拥有强大的感知和推理能力，但由于高计算量和内存消耗，在边缘设备上直接部署非常困难。标准视觉变换器（ViT）编码器处理高分辨率输入时存在延迟过长和内存消耗过大的瓶颈问题。", "method": "HyperVL采用图像平铺策略限制峰值内存使用，并引入两种新技术：1. 视觉分辨率压缩器(VRC)，预测最优编码分辨率以减少冗余计算；2. 双一致性学习(DCL)将多尺度ViT编码器统一在一个框架中，支持在共享LLM内动态切换视觉分支。", "result": "实验表明，HyperVL在同类模型中的多个基准测试上达到最先进的性能，并且显著减少了真实移动设备上的延迟和功耗，展示了其边缘设备端推理的实用性。", "conclusion": "本文提出的HyperVL通过引入高效的方法解决了多模态大语言模型在设备端部署的问题，为未来的研究提供了一个良好的框架。"}}
{"id": "2512.14051", "pdf": "https://arxiv.org/pdf/2512.14051", "abs": "https://arxiv.org/abs/2512.14051", "authors": ["Mengzhang Cai", "Xin Gao", "Yu Li", "Honglin Lin", "Zheng Liu", "Zhuoshi Pan", "Qizhi Pei", "Xiaoran Shang", "Mengyuan Sun", "Zinan Tang", "Xiaoyang Wang", "Zhanping Zhong", "Yun Zhu", "Dahua Lin", "Conghui He", "Lijun Wu"], "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value", "categories": ["cs.AI"], "comment": null, "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.", "AI": {"tldr": "本文提出了OpenDataArena（ODA），一个用于评估后训练数据集价值的开放平台。", "motivation": "模型性能依赖于高质量和多样化的后训练数据集，但目前缺乏对这些数据集系统的评测方法。这阻碍了研究的可重复性，并掩盖了数据特征与模型行为之间的因果关系。", "method": "ODA提供了一个统一的培训评估流程、多维评分框架、交互式数据谱系探索器以及开源工具包，以促进对数据集的研究和评价。", "result": "实验结果揭示了数据复杂性与任务性能间的内在权衡，发现流行基准中的冗余，并绘制了数据集之间的基因关系图。所有这些成果被公开发布，以便让更多人访问高质量的数据评估。", "conclusion": "ODA推动从试错式的数据管理转向基于科学的Data-Centric AI研究方向，为未来关于数据混合定律和基础模型组成的策略性组合提供了坚实的依据。"}}
{"id": "2512.14050", "pdf": "https://arxiv.org/pdf/2512.14050", "abs": "https://arxiv.org/abs/2512.14050", "authors": ["Wenjun Liu", "Qian Wu", "Yifeng Hu", "Yuke Li"], "title": "SELECT: Detecting Label Errors in Real-world Scene Text Data", "categories": ["cs.CV"], "comment": null, "summary": "We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.", "AI": {"tldr": "SELECT是一种用于检测真实世界场景文本数据标签错误的新方法，通过多模态训练提升了准确性。", "motivation": "现有技术在处理真实世界中复杂的变量长度序列和字符级错误方面存在不足。因此，提出了一种新方法来改进这些问题并提高准确性。", "method": "利用图像文本编码器和字符级别分词器，并引入SSLC过程以模拟训练标签中的实际错误场景。", "result": "实验结果表明SELECT在检测真实世界文本数据集的标签错误以及提高STR准确率方面有效且实用。", "conclusion": "SELECT是第一个成功处理变量长度标签并检测真实世界场景文本数据集中标记错误的方法，展示了其在实践中的应用价值。"}}
{"id": "2512.14048", "pdf": "https://arxiv.org/pdf/2512.14048", "abs": "https://arxiv.org/abs/2512.14048", "authors": ["Shen Li", "Li Huang", "Shaoxiong Zhan", "Weifeng Sun", "Tao Yin", "Zhongxin Liu", "Meng Yan"], "title": "Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation", "categories": ["cs.AI"], "comment": "Accepted at AAAI-2026", "summary": "Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.", "AI": {"tldr": "论文提出了RoutingGen框架，该框架通过动态路由适应不同任务的提示策略，提升代码生成性能。", "motivation": "现有链式思维（CoT）提示方法存在过度思考简单任务和缺乏意图抽象的问题。为了优化模型在代码生成中的表现，引入了难度感知的动态路由机制。", "method": "提出RoutingGen框架：对于简单的任务采用少样本提示；对于复杂任务则使用ICoT策略，该策略指导模型捕捉核心算法逻辑及时间复杂度。", "result": "实验表明，在六个标准代码生成基准测试中，RoutingGen在大多数情况下达到了最先进的性能，并将总令牌使用量平均减少了46.37%。", "conclusion": "提出的ICoT和动态路由方法有效提升了代码生成任务的性能并降低了计算资源消耗。"}}
{"id": "2512.14046", "pdf": "https://arxiv.org/pdf/2512.14046", "abs": "https://arxiv.org/abs/2512.14046", "authors": ["Boyang Li", "Zhongpeng Jin", "Shuai Zhao", "Jiahui Liao", "Tian Liu", "Han Liu", "Yuanhai Zhang", "Kai Huang"], "title": "E-Navi: Environmental Adaptive Navigation for UAVs on Resource Constrained Platforms", "categories": ["cs.RO"], "comment": null, "summary": "The ability to adapt to changing environments is crucial for the autonomous navigation systems of Unmanned Aerial Vehicles (UAVs). However, existing navigation systems adopt fixed execution configurations without considering environmental dynamics based on available computing resources, e.g., with a high execution frequency and task workload. This static approach causes rigid flight strategies and excessive computations, ultimately degrading flight performance or even leading to failures in UAVs. Despite the necessity for an adaptive system, dynamically adjusting workloads remains challenging, due to difficulties in quantifying environmental complexity and modeling the relationship between environment and system configuration. Aiming at adapting to dynamic environments, this paper proposes E-Navi, an environmental-adaptive navigation system for UAVs that dynamically adjusts task executions on the CPUs in response to environmental changes based on available computational resources. Specifically, the perception-planning pipeline of UAVs navigation system is redesigned through dynamic adaptation of mapping resolution and execution frequency, driven by the quantitative environmental complexity evaluations. In addition, E-Navi supports flexible deployment across hardware platforms with varying levels of computing capability. Extensive Hardware-In-the-Loop and real-world experiments demonstrate that the proposed system significantly outperforms the baseline method across various hardware platforms, achieving up to 53.9% navigation task workload reduction, up to 63.8% flight time savings, and delivering more stable velocity control.", "AI": {"tldr": "该论文提出了一种环境自适应导航系统E-Navi，旨在根据可用计算资源动态调整任务执行以应对不断变化的环境。", "motivation": "现有的自主导航系统采用固定配置，在面对环境动态时无法灵活调整任务负载和执行频率，导致飞行性能下降或失败。因此，需要一种能够量化工况复杂度并自适应地调整系统的解决方案。", "method": "E-Navi通过重新设计感知-规划管道中的映射分辨率和执行频率来实现动态适应性，并利用硬件在环和实地实验验证了其有效性。", "result": "实验证明，相较于基线方法，E-Navi能够在多种硬件平台上显著降低导航任务负载（最多减少53.9%），节省飞行时间（最多63.8%），并提高速度控制的稳定性。", "conclusion": "论文展示了E-Navi在处理环境动态变化时的优势，并证明了其能够通过自适应策略优化资源利用，从而提升UAV导航系统的性能。"}}
{"id": "2512.14044", "pdf": "https://arxiv.org/pdf/2512.14044", "abs": "https://arxiv.org/abs/2512.14044", "authors": ["Zhenguo Zhang", "Haohan Zhen", "Yishen Wang", "Le Xu", "Tianchen Deng", "Xuefeng Chen", "Qu Chen", "Bo Zhang", "Wuxiong Huang"], "title": "OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and \"zoom in\" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.", "AI": {"tldr": "OmniDrive-R1是一种用于自动驾驶的端到端视觉语言模型，通过强化学习驱动的多模态链式推理机制解决物体幻觉问题。", "motivation": "现有模型在自动驾驶中存在可靠性问题，特别是在关键领域中的物体现象误判。本文旨在通过统一感知和推理解耦过程来提高模型的准确性和鲁棒性。", "method": "提出了一种基于强化学习的视觉定位能力，通过纯两阶段强化学习训练流程及Clip-GRPO算法实现自适应注意力机制。", "result": "在DriveLMM-o1数据集上实验显示，与基线Qwen2.5VL-7B相比，OmniDrive-R1将推理准确率从51.77%提高至80.35%，最终答案准确性提升至73.62%。", "conclusion": "通过强化学习驱动的多模态链式推理机制，OmniDrive-R1显著提高了自动驾驶中视觉语言模型的可靠性和精度。"}}
{"id": "2512.14043", "pdf": "https://arxiv.org/pdf/2512.14043", "abs": "https://arxiv.org/abs/2512.14043", "authors": ["Enhong Liu", "Haiyu Yang", "Miel Hostens"], "title": "Evaluating Small Language Models for Agentic On-Farm Decision Support Systems", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLM) hold potential to support dairy scholars and farmers by supporting decision-making and broadening access to knowledge for stakeholders with limited technical expertise. However, the substantial computational demand restricts access to LLM almost exclusively through cloud-based service, which makes LLM-based decision support tools impractical for dairy farming. To address this gap, lightweight alternatives capable of running locally on farm hardware are required. In this work, we benchmarked 20 open-source Small Language Models (SLM) available on HuggingFace under farm-realistic computing constraints. Building on our prior work, we developed an agentic AI system that integrates five task-specific agents: literature search, web search, SQL database interaction, NoSQL database interaction, and graph generation following predictive models. Evaluation was conducted in two phases. In the first phase, five test questions were used for the initial screening to identify models capable of following basic dairy-related instructions and performing reliably in a compute-constrained environment. Models that passed this preliminary stage were then evaluated using 30 questions (five per task category mentioned above, plus one category addressing integrity and misconduct) in phase two. In results, Qwen-4B achieved superior performance across most of task categories, although showed unstable effectiveness in NoSQL database interactions through PySpark. To our knowledge, this is the first work explicitly evaluating the feasibility of SLM as engines for dairy farming decision-making, with central emphases on privacy and computational efficiency. While results highlight the promise of SLM-assisted tools for practical deployment in dairy farming, challenges remain, and fine-tuning is still needed to refine SLM performance in dairy-specific questions.", "AI": {"tldr": "评估小型语言模型在农场决策支持系统中的应用可行性。", "motivation": "大型语言模型虽然有潜力辅助乳品学者和农民做出更好的决策，但由于计算需求大，只能通过云端服务访问，不适合在农场直接使用。因此需要开发可以本地运行的小型语言模型作为替代方案。", "method": "选取20个开源小型语言模型，在符合农场实际情况的计算限制下进行评估。这些模型被集成到一个代理AI系统中，该系统包含五个任务特定代理：文献搜索、网络搜索、SQL数据库交互、NoSQL数据库交互和生成预测模型的图表。通过两阶段测试流程筛选并评估模型性能。", "result": "Qwen-4B在大多数任务类别中表现出色，但在处理某些NoSQL数据库交互时稳定性较差。这项工作展示了小型语言模型作为乳品农场决策支持系统引擎的潜力。", "conclusion": "尽管结果表明了小型语言模型辅助工具的实际部署前景，但仍面临挑战，需要进一步微调以改进在特定问题上的表现。"}}
{"id": "2512.14040", "pdf": "https://arxiv.org/pdf/2512.14040", "abs": "https://arxiv.org/abs/2512.14040", "authors": ["Boran Wang", "Xinming Wang", "Yi Chen", "Xiang Li", "Jian Xu", "Jing Yuan", "Chenglin Liu"], "title": "ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.", "AI": {"tldr": "ChartAgent是一种基于工具集成推理的图表理解框架，旨在解决现有模型在缺乏关键数字时性能下降的问题。", "motivation": "当前多模态大语言模型虽然在自动化图表理解方面取得了一些进展，但在缺少明确文本注释的情况下表现不佳。为了解决这一问题，提出了ChartAgent。", "method": "ChartAgent将复杂的图分析分解成一系列可观察、可回放的步骤，并通过一个包含多种核心工具（如关键元素检测、实例分割和光学字符识别）的模块化工具库来实现系统化的视觉解析。", "result": "实验结果表明，ChartAgent在注释稀疏的情况下性能显著提升，提供了一种值得信赖且具有扩展性的图表理解系统的实际路径。", "conclusion": "通过透明性和可验证性，ChartAgent超越了黑盒子范式，并为最终结论提供了可追踪和可重复的支持。"}}
{"id": "2512.14039", "pdf": "https://arxiv.org/pdf/2512.14039", "abs": "https://arxiv.org/abs/2512.14039", "authors": ["Meng Wei", "Cheng Zhang", "Jianmin Zheng", "Hamid Rezatofighi", "Jianfei Cai"], "title": "ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.", "AI": {"tldr": "本文通过自适应采样和各向异性参数化策略，解决了现有纹理高斯方法中的两个关键限制：无效的采样效率以及均匀分配的参数化问题。", "motivation": "为了提高纹理高斯方法在三维渲染中的性能，并解决因引入额外纹理参数而导致的记忆效率挑战，本文提出了针对现有方法不足的新解决方案。", "method": "该论文采用自适应采样技术基于高斯密度分布进行更有效的采样；同时通过误差驱动的各向异性参数化策略根据渲染错误分配纹理资源。", "result": "所提出的方法ASAP Textured Gaussians能够显著改善质量和效率之间的权衡，实现高质量渲染的同时使用较少的纹理参数。", "conclusion": "自适应采样和各向异性纹理参数化为三维高斯纹理技术带来了新的改进方向，证明了其在提高渲染质量与减少资源消耗方面的有效性。"}}
{"id": "2512.14032", "pdf": "https://arxiv.org/pdf/2512.14032", "abs": "https://arxiv.org/abs/2512.14032", "authors": ["Ignacio Alzugaray", "Marwan Taher", "Andrew J. Davison"], "title": "ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Project Page: https://github.com/ialzugaray/ace-slam", "summary": "We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM. Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam", "AI": {"tldr": "本文提出了一种基于场景坐标回归的神经RGB-D即时定位与地图构建（SLAM）系统。", "motivation": "该研究旨在探索并利用场景坐标回归作为神经SLAM的核心隐式映射表示，以实现实时高效的三维地图重建和快速重新定位，并且保护隐私。这在现有技术中尚属首次尝试。", "method": "通过设计一个专门用于实时RGB-D SLAM的场景坐标回归网络架构，该系统可以直接将2D图像特征映射到3D全局坐标，从而实现高效、低内存的地图表示和快速重新定位，并支持动态环境下的可靠运行。", "result": "在标准合成与真实世界基准上进行评估后，所提方法显示出了与现有最佳技术相媲美的性能表现。", "conclusion": "研究首次实现了基于场景坐标回归的神经隐式RGB-D SLAM系统，在实时性、高效性和隐私保护方面均具有显著优势。"}}
{"id": "2512.14031", "pdf": "https://arxiv.org/pdf/2512.14031", "abs": "https://arxiv.org/abs/2512.14031", "authors": ["Zhaofeng Hu", "Hongrui Yu", "Vaidhyanathan Chandramouli", "Ci-Jyun Liang"], "title": "Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This study evaluates two leading approaches for teaching construction robots new skills to understand their applicability for construction automation: a Vision-Language-Action (VLA) model and Reinforcement Learning (RL) methods. The goal is to understand both task performance and the practical effort needed to deploy each approach on real jobs. The authors developed two teleoperation interfaces to control the robots and collect the demonstrations needed, both of which proved effective for training robots for long-horizon and dexterous tasks. In addition, the authors conduct a three-stage evaluation. First, the authors compare a Multi-Layer Perceptron (MLP) policy with a Deep Q-network (DQN) imitation model to identify the stronger RL baseline, focusing on model performance, generalization, and a pick-up experiment. Second, three different VLA models are trained in two different scenarios and compared with each other. Third, the authors benchmark the selected RL baseline against the VLA model using computational and sample-efficiency measures and then a robot experiment on a multi-stage panel installation task that includes transport and installation. The VLA model demonstrates strong generalization and few-shot capability, achieving 60% and 100% success in the pickup phase. In comparison, DQN can be made robust but needs additional noise during tuning, which increases the workload. Overall, the findings indicate that VLA offers practical advantages for changing tasks by reducing programming effort and enabling useful performance with minimal data, while DQN provides a viable baseline when sufficient tuning effort is acceptable.", "AI": {"tldr": "研究评估了两种机器人技能学习方法在建筑自动化中的应用：视觉语言行动（VLA）模型和强化学习（RL）方法，通过实验对比它们的性能与实用性。", "motivation": "探讨如何有效教授施工机器人新技能以实现自动化，并比较不同方法的应用潜力和部署难度。", "method": "开发了两种远程操作界面用于收集训练数据；首先比较MLP和DQN模仿模型确定更强的RL基准；接着在两个场景中训练三种不同的VLA模型并相互对比；最后通过计算效率和样本效率评估选定的RL基线与VLA模型，并进行机器人实验。", "result": "VLA模型表现出色，具有强泛化能力和少数据学习能力，成功率较高；DQN需要额外调整才能稳健运行，但增加了工作量。", "conclusion": "VLA方法在任务变化时能减少编程努力并实现有用性能，而DQN为可接受充分调优的情况下提供可行基线。"}}
{"id": "2512.14028", "pdf": "https://arxiv.org/pdf/2512.14028", "abs": "https://arxiv.org/abs/2512.14028", "authors": ["Jiaheng Li", "Qiyu Dai", "Lihan Li", "Praneeth Chakravarthula", "He Sun", "Baoquan Chen", "Wenzheng Chen"], "title": "Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.", "AI": {"tldr": "本文提出了一个基于神经特征匹配的单次结构光深度解码框架，以提高在复杂场景下的鲁棒性和精度。", "motivation": "传统方法依赖像素域对应算法进行深度解码，在面对遮挡、细节丰富和非朗伯体表面等挑战时性能有限。通过利用神经特征匹配技术，本文旨在提升结构光系统的稳健性与准确性。", "method": "该方法从投射模式和捕捉的红外图像中提取神经特征，并在特征空间构建代价体积以明确纳入几何先验。引入了深度细化模块，借助大规模单目深度估计模型的强大先验来改进细节恢复和全局结构一致性。同时开发了一个基于物理原理的合成数据生成管线。", "result": "实验表明该方法能很好地泛化到现实世界室内环境，处理多种模式而无需重新训练，并且在性能上优于商用结构光系统及被动立体RGB深度估计方法。", "conclusion": "本文提出的方法通过神经特征匹配和深度细化模块实现了对复杂场景的鲁棒性提升与精度改进。"}}
{"id": "2512.14026", "pdf": "https://arxiv.org/pdf/2512.14026", "abs": "https://arxiv.org/abs/2512.14026", "authors": ["Yibing Fu", "Yunpeng Zhao", "Zhitao Zeng", "Cheng Chen", "Yueming Jin"], "title": "Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.", "AI": {"tldr": "本文提出了一种新的自监督学习框架CITab，用于跨表图像和表格数据的多模态特征表示学习。", "motivation": "现有的自监督学习方法在处理异质性表数据时存在局限性，这阻碍了它们从不同人群中学到可转移的知识。因此，需要一种可以克服这种限制的新方法来提升模型的表现能力。", "method": "本文设计了一个新的框架CITab，该框架通过将列标题作为语义线索集成到了表格建模机制中，并引入了一种原型引导的线性混合模块P-MoLin以处理异质性的表数据。", "result": "实验结果表明，所提出的模型在阿尔茨海默病诊断任务上优于现有的方法，证明了其跨人群多模态学习的有效性和可扩展性。", "conclusion": "本文提出了一种新的自监督框架CITab，通过利用语义线索和处理异质性的表数据的能力来提高医学知识的转移性和模型预训练的灵活性。"}}
{"id": "2512.14020", "pdf": "https://arxiv.org/pdf/2512.14020", "abs": "https://arxiv.org/abs/2512.14020", "authors": ["Afia Maham", "Dur E Nayab Tashfa"], "title": "Deep Learning Perspective of Scene Understanding in Autonomous Robots", "categories": ["cs.CV"], "comment": "11 pages. Review Paper on Deep Learning Perspective of Scene Understanding in Autonomous Robots", "summary": "This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.", "AI": {"tldr": "本文综述了深度学习在自主机器人场景理解中的应用，包括目标检测、语义分割、实例分割、深度估计、三维重建和视觉SLAM等方面的技术。", "motivation": "传统几何模型存在局限性，难以实现实时的深度感知，并且不能有效处理遮挡及无纹理表面。本文旨在通过深度学习技术改进这些方面，提升机器人在动态和复杂环境中的感知能力和决策水平。", "method": "该综述论文没有提出新的具体方法，而是总结了现有的基于深度学习的场景理解技术及其进展。", "result": "当前的研究成果已经证明，基于深度学习的方法可以有效改善自主机器人的视觉感知能力，提高其在复杂环境中的导航和交互性能。", "conclusion": "尽管基于深度学习的技术取得了显著进步，但仍存在一些挑战，如计算效率、鲁棒性和对罕见场景的适应性等。未来的研究方向可能集中在这些领域以进一步提升机器人系统的综合理解与应用水平。"}}
{"id": "2512.14018", "pdf": "https://arxiv.org/pdf/2512.14018", "abs": "https://arxiv.org/abs/2512.14018", "authors": ["Jiuding Yang", "Shengyao Lu", "Hongxuan Liu", "Shayan Shirahmad Gale Bagi", "Zahra Fazel", "Tomasz Czajkowski", "Di Niu"], "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.", "AI": {"tldr": "PerfCoder通过可解释的定制优化生成性能增强代码，超越现有模型在运行时间和有效优化率方面表现优异。", "motivation": "当前大型语言模型（LLMs）在自动生成代码方面取得了显著进步，但在产生高性能代码方面的能力仍然有限。这主要是由于缺乏引导可解释和有效的性能改进的监督。", "method": "PerfCoder是通过在其上进行微调的一系列大型语言模型家族，这些模型使用了经过人工注释的真实世界优化轨迹集，并通过强化学习微调来与运行时测量对齐。", "result": "在PIE代码性能基准测试中，PerfCoder超越所有现有模型的运行时间加速和有效优化率。它不仅提升了32B模型和GPT-5的表现，而且能够在提供给更大的LLM以规划者和优化器协作工作流程作为输入时进一步改进结果。", "conclusion": "研究表明，仅靠规模无法实现性能优化，需要具备策略意识。PerfCoder展示了可解释反馈在代码性能优化中的重要性，并表明了这种方法的有效性和前景。"}}
{"id": "2512.14017", "pdf": "https://arxiv.org/pdf/2512.14017", "abs": "https://arxiv.org/abs/2512.14017", "authors": ["Zongyao Li", "Kengo Ishida", "Satoshi Yamazaki", "Xiaotong Ji", "Jianquan Liu"], "title": "KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "WACV2026", "summary": "We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.", "AI": {"tldr": "提出KFS-Bench，一个针对长视频理解中关键帧采样的首个基准测试。", "motivation": "现有工作间接评估帧选择质量的方法存在局限性。为此，引入多场景注释直接分析不同采样方法捕获整个长视频必要内容的能力。", "method": "设计了一种新颖的采样质量度量标准和一个结合问题与视频相关性的自适应平衡采样方法。", "result": "研究表明，在关键帧采样和问答性能方面，新方法均表现出色。", "conclusion": "KFS-Bench提供直接评估采样策略的有效性，并且所提出的方法可以提高长视频理解和问答准确性。"}}
{"id": "2512.14014", "pdf": "https://arxiv.org/pdf/2512.14014", "abs": "https://arxiv.org/abs/2512.14014", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Aditya Grover"], "title": "MobileWorldBench: Towards Semantic World Modeling For Mobile Agents", "categories": ["cs.AI"], "comment": "21 pages, 13 figures", "summary": "World models have shown great utility in improving the task performance of embodied agents. While prior work largely focuses on pixel-space world models, these approaches face practical limitations in GUI settings, where predicting complex visual elements in future states is often difficult. In this work, we explore an alternative formulation of world modeling for GUI agents, where state transitions are described in natural language rather than predicting raw pixels. First, we introduce MobileWorldBench, a benchmark that evaluates the ability of vision-language models (VLMs) to function as world models for mobile GUI agents. Second, we release MobileWorld, a large-scale dataset consisting of 1.4M samples, that significantly improves the world modeling capabilities of VLMs. Finally, we propose a novel framework that integrates VLM world models into the planning framework of mobile agents, demonstrating that semantic world models can directly benefit mobile agents by improving task success rates. The code and dataset is available at https://github.com/jacklishufan/MobileWorld", "AI": {"tldr": "本文提出了MobileWorldBench，一个评估视觉语言模型(VLM)作为移动GUI代理世界模型能力的基准，并提供了一个大规模的数据集MobileWorld来提升VLM的世界建模能力。", "motivation": "传统的像素空间世界建模在图形用户界面环境中面临预测复杂视觉元素难题。本文旨在通过自然语言描述状态转换，探索更适合GUI环境的世界建模方法，提高移动代理的任务成功率。", "method": "引入了MobileWorldBench作为评估基准，并发布了包含1.4M样本的MobileWorld数据集；提出了一种将VLM世界模型集成到移动代理规划框架中的新框架。", "result": "实验表明，提出的语义世界模型能够直接提升移动代理的任务成功率。", "conclusion": "通过引入自然语言描述状态转换的方式进行世界建模，可以显著提高移动代理在GUI环境中的任务完成率。"}}
{"id": "2512.14012", "pdf": "https://arxiv.org/pdf/2512.14012", "abs": "https://arxiv.org/abs/2512.14012", "authors": ["Ruanqianqian Huang", "Avery Reyna", "Sorin Lerner", "Haijun Xia", "Brian Hempel"], "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.", "AI": {"tldr": "研究探讨了专业软件开发人员在2025年如何使用AI代理进行编码，包括动机、策略和任务适应性。", "motivation": "探讨AI代理在专业软件开发中的实际作用及影响", "method": "通过实地观察（N=13）和定性调查（N=99）收集数据", "result": "发现经验丰富的开发者视代理为生产率提升的工具，但强调控制代理行为以确保软件质量，并对与代理协作持积极态度。", "conclusion": "该研究揭示了有效使用AI代理的最佳实践，指出适合代理处理的任务类型，并提出了未来改进代理界面和使用指南的机会。"}}
{"id": "2512.14008", "pdf": "https://arxiv.org/pdf/2512.14008", "abs": "https://arxiv.org/abs/2512.14008", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models", "categories": ["cs.CV"], "comment": "18 pages (12 pages for the main paper and 6 pages for the appendix), 9 figures", "summary": "Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.", "AI": {"tldr": "本论文提出了一种新的稀疏多模态离散扩散语言模型Sparse-LaViDa，该模型通过动态裁剪不必要的掩码令牌来加速采样过程。", "motivation": "虽然掩码离散扩散模型（MDMs）在多项任务中表现出色，但其推理速度由于需要重复处理冗余的掩码令牌而受限。因此，提出了Sparse-LaViDa以提高推理效率并保持生成质量。", "method": "该方法引入了特殊的注册令牌来表示裁剪后的令牌，并设计了一种特殊注意掩膜以确保训练和推断过程的一致性。", "result": "基于最先进的统一MDM LaViDa-O，Sparse-LaViDa在包括文本到图像生成、图像编辑和数学推理等任务中实现了高达2倍的加速，同时保持了生成质量。", "conclusion": "通过动态裁剪不必要的掩码令牌并引入特殊处理机制，Sparse-LaViDa在提高采样效率的同时保证了生成的质量。"}}
{"id": "2512.14001", "pdf": "https://arxiv.org/pdf/2512.14001", "abs": "https://arxiv.org/abs/2512.14001", "authors": ["Zhuo Zhang", "Yonghui Liu", "Meijie Zhang", "Feiyang Tan", "Yikang Ding"], "title": "CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IROS 2025", "summary": "In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.", "AI": {"tldr": "本文提出了CLAIM方法，利用单目深度模型进行相机和激光雷达的校准。", "motivation": "通过释放单目深度模型在相机-激光雷达标定中的潜力，解决传统方法复杂的特征提取、匹配问题，简化校准流程，使其适应更多场景。", "method": "给定初始猜测和图像与激光点云对，CLAIM采用粗到细的搜索方法寻找最小化结构损失（基于修补后的皮尔森相关）和纹理损失（基于互信息）的最佳变换。", "result": "在公共KITTI、Waymo和MIAS-LCEC数据集上验证了CLAIM的有效性，并展示其优于现有方法的表现。", "conclusion": "通过实验表明，该方法能够有效地进行相机-激光雷达校准，简化流程并提高准确性。"}}
{"id": "2512.13998", "pdf": "https://arxiv.org/pdf/2512.13998", "abs": "https://arxiv.org/abs/2512.13998", "authors": ["Qilin Li", "C. L. Philip Chen", "TongZhang"], "title": "Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition", "categories": ["cs.SD", "cs.AI", "cs.MM"], "comment": null, "summary": "Music Emotion Recogniser (MER) research faces challenges due to limited high-quality annotated datasets and difficulties in addressing cross-track feature drift. This work presents two primary contributions to address these issues. Memo2496, a large-scale dataset, offers 2496 instrumental music tracks with continuous valence arousal labels, annotated by 30 certified music specialists. Annotation quality is ensured through calibration with extreme emotion exemplars and a consistency threshold of 0.25, measured by Euclidean distance in the valence arousal space. Furthermore, the Dual-view Adaptive Music Emotion Recogniser (DAMER) is introduced. DAMER integrates three synergistic modules: Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms; Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence; and Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift. Extensive experiments on the Memo2496, 1000songs, and PMEmo datasets demonstrate DAMER's state-of-the-art performance, improving arousal dimension accuracy by 3.43%, 2.25%, and 0.17%, respectively. Ablation studies and visualisation analyses validate each module's contribution. Both the dataset and source code are publicly available.", "AI": {"tldr": "论文提出了Memo2496数据集和DAMER模型，用于音乐情感识别。", "motivation": "音乐情绪识别面临高质量标注数据不足及跨曲目特征漂移的问题。为此，该研究旨在通过开发新的数据集和方法来改善这些情况。", "method": "提出Memo2496数据集，包括30名专家注释的2496首乐器音乐片段，并介绍DAMER模型，整合Dual Stream Attention Fusion, Progressive Confidence Labelling 和 Style Anchored Memory Learning三大模块，以提高识别性能和处理跨曲目特征漂移。", "result": "实验结果表明，DAMER在Memo2496、1000songs和PMEmo数据集上的表现优于现有方法，并显著提升了唤醒维度的准确率。", "conclusion": "通过创建高质量的数据集和创新模型结构，研究有效改善了音乐情感识别中的跨曲目特征漂移问题，展示了其在实际应用中的潜力。"}}
{"id": "2512.13996", "pdf": "https://arxiv.org/pdf/2512.13996", "abs": "https://arxiv.org/abs/2512.13996", "authors": ["Can Jin", "Hongwu Peng", "Mingcan Xiang", "Qixin Zhang", "Xiangchi Yuan", "Amit Hasan", "Ohiremen Dibua", "Yifan Gong", "Yan Kang", "Dimitris N. Metaxas"], "title": "Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training", "categories": ["cs.AI"], "comment": null, "summary": "Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.", "AI": {"tldr": "本文提出了一种可控的动态Top-p路由机制，用于大规模模型预训练中的稀疏混合专家架构。", "motivation": "标准的Top-k路由策略导致了固定的稀疏模式，而现有的Top-p路由通常依赖于固定概率阈值，这会导致计算成本不可控和对超参数选择敏感的问题。", "method": "提出了一种动态调整概率阈值的方法，并引入了一种层间路由归一化机制。使用PI控制器来确保激活专家的稀疏性与指定的目标相匹配。", "result": "实验证明，DTop-p比Top-k和固定阈值Top-p基线方法性能更好，且能够精确控制激活专家的数量并适应不同令牌和层次之间的资源分配。", "conclusion": "提出的DTop-p为大规模模型预训练提供了一种强大的框架，具有良好的扩展性。"}}
{"id": "2512.13991", "pdf": "https://arxiv.org/pdf/2512.13991", "abs": "https://arxiv.org/abs/2512.13991", "authors": ["Yao He", "Youngjoong Kwon", "Tiange Xiang", "Wenxiao Cai", "Ehsan Adeli"], "title": "Repurposing 2D Diffusion Models for 3D Shape Completion", "categories": ["cs.CV"], "comment": null, "summary": "We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.", "AI": {"tldr": "本文提出了一种框架，将二维扩散模型应用于三维形状补全任务。", "motivation": "尽管文本到图像的扩散模型在丰富的二维数据上取得了显著成功，但三维扩散模型因高质量三维数据稀缺和模态差距问题而进展缓慢。为此，本文旨在通过引入Shape Atlas解决这些问题。", "method": "使用一种称为Shape Atlas的紧凑二维几何表示来克服三维输入与二维潜在空间之间的模态差异，并充分利用预训练的二维扩散模型的生成能力。", "result": "在PCN和ShapeNet-55数据集上验证了方法的有效性，展示高质、细节保留的形状补全。此外，展示了将完成点云转化为艺术家创建的网格的应用场景。", "conclusion": "通过引入Shape Atlas，本文成功地使用二维扩散模型进行高质量三维形状补全，并证明了其实际应用价值"}}
{"id": "2512.13982", "pdf": "https://arxiv.org/pdf/2512.13982", "abs": "https://arxiv.org/abs/2512.13982", "authors": ["Dereje Shenkut", "Vijayakumar Bhagavatula"], "title": "FocalComm: Hard Instance-Aware Multi-Agent Perception", "categories": ["cs.CV"], "comment": "WACV 2026", "summary": "Multi-agent collaborative perception (CP) is a promising paradigm for improving autonomous driving safety, particularly for vulnerable road users like pedestrians, via robust 3D perception. However, existing CP approaches often optimize for vehicle detection performance metrics, underperforming on smaller, safety-critical objects such as pedestrians, where detection failures can be catastrophic. Furthermore, previous CP methods rely on full feature exchange rather than communicating only salient features that help reduce false negatives. To this end, we present FocalComm, a novel collaborative perception framework that focuses on exchanging hard-instance-oriented features among connected collaborative agents. FocalComm consists of two key novel designs: (1) a learnable progressive hard instance mining (HIM) module to extract hard instance-oriented features per agent, and (2) a query-based feature-level (intermediate) fusion technique that dynamically weights these identified features during collaboration. We show that FocalComm outperforms state-of-the-art collaborative perception methods on two challenging real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaborative setups. FocalComm also shows a strong performance gain in pedestrian detection in V2X-Real.", "AI": {"tldr": "提出FocalComm框架，专注于多智能体感知中困难实例的特征交换，以提高行人等小目标检测性能。", "motivation": "现有方法在车辆检测上表现出色，但在行人等小型、关键性对象上的表现不佳。为解决此问题并减少误检率，该研究提出了一种新的协作感知框架FocalComm。", "method": "FocalComm包括两个核心设计：可学习的渐进式困难实例挖掘模块和基于查询的特征级融合技术，以动态加权交换困难实例导向的特征。", "result": "在V2X-Real和DAIR-V2X数据集上，FocalComm超越了最先进的协作感知方法，在车辆中心和基础设施中心设置中均表现出色。尤其在行人检测方面有显著提升。", "conclusion": "通过专注于交换困难实例导向的特征，FocalComm提高了多智能体感知系统的性能，特别是在关键的小目标（如行人）上的表现有了明显改善。"}}
{"id": "2512.13981", "pdf": "https://arxiv.org/pdf/2512.13981", "abs": "https://arxiv.org/abs/2512.13981", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Impact of Robot Facial-Audio Expressions on Human Robot Trust Dynamics and Trust Repair", "categories": ["cs.RO"], "comment": null, "summary": "Despite recent advances in robotics and human-robot collaboration in the AEC industry, trust has mostly been treated as a static factor, with little guidance on how it changes across events during collaboration. This paper investigates how a robot's task performance and its expressive responses after outcomes shape the dynamics of human trust over time. To this end, we designed a controlled within-subjects study with two construction-inspired tasks, Material Delivery (physical assistance) and Information Gathering (perceptual assistance), and measured trust repeatedly (four times per task) using the 14-item Trust Perception Scale for HRI plus a redelegation choice. The robot produced two multimodal expressions, a \"glad\" display with a brief confirmation after success, and a \"sad\" display with an apology and a request for a second chance after failure. The study was conducted in a lab environment with 30 participants and a quadruped platform, and we evaluated trust dynamics and repair across both tasks. Results show that robot success reliably increases trust, failure causes sharp drops, and apology-based expressions partially restores trust (44% recovery in Material Delivery; 38% in Information Gathering). Item-level analysis indicates that recovered trust was driven mostly by interaction and communication factors, with competence recovering partially and autonomy aspects changing least. Additionally, age group and prior attitudes moderated trust dynamics with younger participants showed larger but shorter-lived changes, mid-20s participants exhibited the most durable repair, and older participants showed most conservative dynamics. This work provides a foundation for future efforts that adapt repair strategies to task demands and user profiles to support safe, productive adoption of robots on construction sites.", "AI": {"tldr": "研究了机器人面部音频表达对人机信任动态及修复的影响。", "motivation": "探讨机器人在任务成功与失败后的表情反应如何影响人类对其的信任变化，以促进建筑行业中的人机合作安全和效率提升。", "method": "设计了一项控制组内实验，包含材料配送和信息收集两个任务，并通过14项信任感知量表测量了参与者的信任程度。机器人在成功后表现出“高兴”，失败后则道歉并请求再试一次。研究在实验室环境中进行，涉及30名参与者与四足平台。", "result": "结果显示，机器人成功会增加人类的信任；失败会导致信任显著下降；而道歉表情能在一定程度上修复受损的信任（材料配送任务中恢复44%，信息收集任务中恢复38%）。此外，年龄和先前态度也影响了信任变化的幅度与持久性。", "conclusion": "研究表明，机器人通过表达情绪可以部分修复人类因失败造成的不信任。未来研究应根据具体任务需求和个人特征来调整修复策略以支持更安全、高效的建筑行业人机合作。"}}
{"id": "2512.13979", "pdf": "https://arxiv.org/pdf/2512.13979", "abs": "https://arxiv.org/abs/2512.13979", "authors": ["Ge Yan", "Chung-En Sun", "Tsui-Wei", "Weng"], "title": "ReflCtrl: Controlling LLM Reflection via Representation Engineering", "categories": ["cs.AI"], "comment": "Spotlight in NeurIPS 25 MI workshop", "summary": "Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.", "AI": {"tldr": "通过表示工程研究大型语言模型的自我反思行为，并提出一种可以控制反射频率的方法ReflCtrl。", "motivation": "虽然具有链式思维推理的大规模语言模型在许多任务中表现出色，但它们的自我反思能力也增加了推断成本。因此，作者希望通过对表示工程的研究来更好地理解和控制这种行为。", "method": "通过将模型的推理步骤分割并识别出代表自我反省的部分，提取反映方向，并提出一种分步引导方法来控制反射频率。", "result": "实验表明，在许多情况下反思是多余的，特别是在更强的模型中。可以节省多达33.6％的推理令牌，同时保持性能。此外，发现模型的反思行为与内部不确定性信号高度相关。", "conclusion": "通过研究和方法得出结论，自我反省可能受到模型不确定性的控制，并提出了有效的策略来减少不必要的反射，从而降低推断成本并提高效率。"}}
{"id": "2512.13978", "pdf": "https://arxiv.org/pdf/2512.13978", "abs": "https://arxiv.org/abs/2512.13978", "authors": ["Yang Cao", "Yubin Chen", "Xuyang Guo", "Zhao Song", "Song Yue", "Jiahao Zhang", "Jiale Zhao"], "title": "Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95]. We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.", "AI": {"tldr": "本文通过经典教材《随机算法》对四种前沿大型语言模型进行了基准测试，评估它们在博士级数学推理中的表现。", "motivation": "尽管大型语言模型已在自动化数学推理和科学发现方面取得显著进展，但尚需对其在典型、研究生级别的数学理论上的基本推理能力进行严格的评估。", "method": "对GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking和Grok-4这四种前沿模型进行了基准测试，要求它们为《随机算法》教材中的多个引理和练习生成正式的LaTeX证明。", "result": "顶级模型（Gemini和Claude）在准确率方面表现良好（约66%），展示了对概率方法和形式逻辑的强大理解；而其他模型则显著落后于前者，一致性较低（约40%）。", "conclusion": "虽然前沿模型已具备适合研究生教学辅助和正式化的基本水平，但在严格的数学推导中仍存在较大的可靠性差异。"}}
{"id": "2512.13977", "pdf": "https://arxiv.org/pdf/2512.13977", "abs": "https://arxiv.org/abs/2512.13977", "authors": ["Youssef Abuzeid", "Shimaa El-Bana", "Ahmad Al-Kabbany"], "title": "XAI-Driven Diagnosis of Generalization Failure in State-Space Cerebrovascular Segmentation Models: A Case Study on Domain Shift Between RSNA and TopCoW Datasets", "categories": ["cs.CV"], "comment": null, "summary": "The clinical deployment of deep learning models in medical imaging is severely hindered by domain shift. This challenge, where a high-performing model fails catastrophically on external datasets, is a critical barrier to trustworthy AI. Addressing this requires moving beyond simple performance metrics toward deeper understanding, making Explainable AI (XAI) an essential diagnostic tool in medical image analysis. We present a rigorous, two-phase approach to diagnose the generalization failure of state-of-the-art State-Space Models (SSMs), specifically UMamaba, applied to cerebrovascular segmentation. We first established a quantifiable domain gap between our Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT) datasets, noting significant differences in Z-resolution and background noise. The model's Dice score subsequently plummeted from 0.8604 (Source) to 0.2902 (Target). In the second phase, which is our core contribution, we utilized Seg-XRes-CAM to diagnose the cause of this failure. We quantified the model's focus by measuring the overlap between its attention maps and the Ground Truth segmentations, and between its attention maps and its own Prediction Mask. Our analysis proves the model failed to generalize because its attention mechanism abandoned true anatomical features in the Target domain. Quantitative metrics confirm the model's focus shifted away from the Ground Truth vessels (IoU~0.101 at 0.3 threshold) while still aligning with its own wrong predictions (IoU~0.282 at 0.3 threshold). This demonstrates the model learned spurious correlations, confirming XAI is a powerful diagnostic tool for identifying dataset bias in emerging architectures.", "AI": {"tldr": "本文通过XAI技术诊断了在不同数据集间迁移时，状态空间模型UMamaba进行脑血管分割的泛化失败原因。", "motivation": "深度学习模型在医学影像中的临床应用受到领域偏移的影响，即高表现的模型在外部数据集上性能急剧下降。为了深入理解这一问题，XAI成为一种重要的诊断工具。", "method": "首先确定源数据集（RSNA CTA Aneurysm）和目标数据集（TopCoW Circle of Willis CT）之间的领域差距；然后使用Seg-XRes-CAM分析模型的注意力机制，并量化其在真实解剖特征与错误预测间的偏移。", "result": "UMamaba模型从源数据集上的0.8604下降到目标数据集上的0.2902，证明了模型未能泛化。进一步分析显示，模型注意力机制偏离了解剖学特征而偏向于自己的错误预测。", "conclusion": "通过XAI技术可以有效诊断出深度学习模型的领域偏移问题，是新兴架构中识别数据偏差的强大工具。"}}
{"id": "2512.13974", "pdf": "https://arxiv.org/pdf/2512.13974", "abs": "https://arxiv.org/abs/2512.13974", "authors": ["Hossein Naderi", "Alireza Shojaei", "Philip Agee", "Kereshmeh Afsari", "Abiola Akanmu"], "title": "Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline", "categories": ["cs.RO"], "comment": null, "summary": "Construction safety inspection remains mostly manual, and automated approaches still rely on task-specific datasets that are hard to maintain in fast-changing construction environments due to frequent retraining. Meanwhile, field inspection with robots still depends on human teleoperation and manual reporting, which are labor-intensive. This paper aims to connect what a robot sees during autonomous navigation to the safety rules that are common in construction sites, automatically generating a safety inspection report. To this end, we proposed a multi-layer framework with two main modules: robotics and AI. On the robotics side, SLAM and autonomous navigation provide repeatable coverage and targeted revisits via waypoints. On AI side, a Vision Language Model (VLM)-based layer produces scene descriptions; a retrieval component powered grounds those descriptions in OSHA and site policies; Another VLM-based layer assesses the safety situation based on rules; and finally Large Language Model (LLM) layer generates safety reports based on previous outputs. The framework is validated with a proof-of-concept implementation and evaluated in a lab environment that simulates common hazards across three scenarios. Results show high recall with competitive precision compared to state-of-the-art closed-source models. This paper contributes a transparent, generalizable pipeline that moves beyond black-box models by exposing intermediate artifacts from each layer and keeping the human in the loop. This work provides a foundation for future extensions to additional tasks and settings within and beyond construction context.", "AI": {"tldr": "本文提出了一种使用移动机器人进行自主建筑工地安全检查的方法，结合了SLAM、自动导航和AI模块生成的安全报告。", "motivation": "传统的施工现场安全检查依赖于人力，并且自动化方法需要维护特定任务的数据集，在快速变化的环境下难以持续。本研究旨在通过机器人自主导航连接实际场景与安全规则，并自动生成安全报告。", "method": "本文提出了一种多层框架，包括SLAM和自动导航模块以及两个基于VLM和LLM的AI层，第一层生成场景描述，第二层将这些描述转化为合规性检查，第三层评估安全性并产生最终的安全报告。", "result": "该方法在模拟常见风险的实验环境中进行了测试，并与现有模型比较，在召回率上表现较高，且精度也相当。", "conclusion": "本文贡献了一种透明、通用的方法来自动化安全检查过程，超越了黑盒模型并通过展示每一层的中间结果保持了人类的参与。这项工作为未来扩展至更多任务和场景提供了基础。"}}
{"id": "2512.13970", "pdf": "https://arxiv.org/pdf/2512.13970", "abs": "https://arxiv.org/abs/2512.13970", "authors": ["Miaohua Zhang", "Mohammad Ali Armin", "Xuesong Li", "Sisi Liang", "Lars Petersson", "Changming Sun", "David Ahmedt-Aristizabal", "Zeeshan Hayder"], "title": "Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.", "AI": {"tldr": "本文提出了一种质量驱动和多样性感知的样本扩展管道，用于改善海洋障碍物分割的鲁棒性。", "motivation": "在复杂条件下，如太阳反光、雾气等，海洋障碍物检测面临图像质量问题。同时由于数据集稀少且重复性强，现有方法难以生成多样化的训练数据。", "method": "该管道由两个关键部分组成：一是构建高熵语义基础提示的类感知风格库；二是通过适应性退火采样器扰动早期条件，并借助COD引导的比例控制器调节扰动以提升多样性而不影响布局保真度。", "result": "实验表明，在多个海洋障碍物基准测试中，使用这些受控合成样本增强训练数据后，多种基础模型的分割性能得到提高。", "conclusion": "通过质量驱动和多样性感知的方法生成高质量且多样化的训练样本可以有效提升海洋障碍物检测中的鲁棒性和性能。"}}
{"id": "2512.13956", "pdf": "https://arxiv.org/pdf/2512.13956", "abs": "https://arxiv.org/abs/2512.13956", "authors": ["Zishan Bai", "Enze Ge", "Junfeng Hao"], "title": "Multi-Agent Collaborative Framework for Intelligent IT Operations: An AOI System with Context-Aware Compression and Dynamic Task Scheduling", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The proliferation of cloud-native architectures, characterized by microservices and dynamic orchestration, has rendered modern IT infrastructures exceedingly complex and volatile. This complexity generates overwhelming volumes of operational data, leading to critical bottlenecks in conventional systems: inefficient information processing, poor task coordination, and loss of contextual continuity during fault diagnosis and remediation. To address these challenges, we propose AOI (AI-Oriented Operations), a novel multi-agent collaborative framework that integrates three specialized agents with an LLM-based Context Compressor. Its core innovations include: (1) a dynamic task scheduling strategy that adaptively prioritizes operations based on real-time system states, and (2) a three-layer memory architecture comprising Working, Episodic, and Semantic layers that optimizes context retention and retrieval. Extensive experiments on both synthetic and real-world benchmarks demonstrate that AOI effectively mitigates information overload, achieving a 72.4% context compression ratio while preserving 92.8% of critical information and significantly enhances operational efficiency, attaining a 94.2% task success rate and reducing the Mean Time to Repair (MTTR) by 34.4% compared to the best baseline. This work presents a paradigm shift towards scalable, adaptive, and context-aware autonomous operations, enabling robust management of next-generation IT infrastructures with minimal human intervention.", "AI": {"tldr": "提出了一种新的多代理协作框架AOI，用于智能IT操作管理", "motivation": "应对云计算架构的复杂性和数据量过大导致的信息处理效率低下、任务协调不良以及故障诊断和修复过程中的上下文丢失问题", "method": "整合了三个专门的代理和基于大语言模型的上下文压缩器，并采用动态任务调度策略及三层记忆体系结构优化上下文保留与检索", "result": "AOI在实验中实现了72.4%的上下文压缩比，同时保持92.8％的关键信息；任务成功率提高了94.2%，平均修复时间减少了34.4%", "conclusion": "AOI展示了向可扩展、自适应和感知上下文自主操作转变的新范式，能够有效地管理下一代IT基础设施"}}
{"id": "2512.13955", "pdf": "https://arxiv.org/pdf/2512.13955", "abs": "https://arxiv.org/abs/2512.13955", "authors": ["Sindhuja Madabushi", "Dawood Wasif", "Jin-Hee Cho"], "title": "MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning", "categories": ["cs.AI"], "comment": null, "summary": "Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.", "AI": {"tldr": "提出MURIM机制，通过多维度声誉系统评估客户端可靠性以公平分配激励。", "motivation": "解决联邦学习中的弱客户端激励、隐私风险和资源限制问题，确保公平的参与和模型收敛。", "method": "基于多维度声誉系统（包括可靠性和隐私保护）设计MURIM机制，通过贡献度、延迟时间以及信誉评估来分配奖励，同时包含可靠性验证模块。", "result": "实验表明，与现有方法相比，MURIM在公平性指标上提高18%，降低5-9%的隐私攻击成功率，并增强对抗中毒和噪声梯度攻击的能力。", "conclusion": "MURIM有效缓解了联邦学习中的对抗威胁，促进了公平而真实的参与并保持模型稳定收敛。"}}
{"id": "2512.13953", "pdf": "https://arxiv.org/pdf/2512.13953", "abs": "https://arxiv.org/abs/2512.13953", "authors": ["Dawid Malarz", "Artur Kasymov", "Filip Manjak", "Maciej Zięba", "Przemysław Spurek"], "title": "From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.", "AI": {"tldr": "本文介绍了UNBRANDING，一种用于商标安全的文本到图像生成任务，并构建了一个基准数据集。", "motivation": "随着文本到图像扩散模型的发展，未经授权复制商标内容的问题日益严重。现有的解决方案未能解决特定品牌标识符问题，而这些问题不仅包括显式的标志还包括隐性的结构特征。", "method": "作者提出了一个新颖的任务——unbranding，旨在去除图像中的品牌标识和结构特征，并保持语义一致性。为了评估模型效果，他们引入了一个基于视觉语言模型的评价指标。", "result": "研究结果表明，随着模型准确度提高，新系统更易于生成品牌标识符，从而突显了未品牌的挑战。该方法通过VLM指标得到了验证，证实unbranding是一个实际相关的问题。", "conclusion": "本工作强调了解决商标安全文本到图像生成问题的重要性，并为未来研究提供了一套基准数据集和评价标准。"}}
{"id": "2512.13950", "pdf": "https://arxiv.org/pdf/2512.13950", "abs": "https://arxiv.org/abs/2512.13950", "authors": ["Alban Gauthier", "Valentin Deschaintre", "Alexandre Lanvin", "Fredo Durand", "Adrien Bousseau", "George Drettakis"], "title": "An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation Code: http://github.com/graphdeco-inria/svbrdf-evaluation", "summary": "Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation", "AI": {"tldr": "评估从生成图像模型预测SVBRDF在三维场景外观建模中的挑战和机会", "motivation": "探讨深度生成模型在数字内容创作中的应用，特别是通过合成RGB图像来快速生成三维场景的SVBRDF图集，并解决单视图SVBRDF预测可能导致的多视图不一致问题", "method": "比较不同的神经网络架构和条件以识别能够实现高精度和一致性的设计，发现标准UNet与复杂设计相比具有竞争力", "result": "通过对比不同方法，确定了能够在SVBRDF估计中取得较高准确度和一致性的模型结构", "conclusion": "尽管存在多视图不一致性挑战，利用生成的RGB图像可以为SVBRDF预测提供额外信息"}}
{"id": "2512.13935", "pdf": "https://arxiv.org/pdf/2512.13935", "abs": "https://arxiv.org/abs/2512.13935", "authors": ["Qi Chen", "Fabio Ramos", "Alán Aspuru-Guzik", "Florian Shkurti"], "title": "Informing Acquisition Functions via Foundation Models for Molecular Discovery", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Bayesian Optimization (BO) is a key methodology for accelerating molecular discovery by estimating the mapping from molecules to their properties while seeking the optimal candidate. Typically, BO iteratively updates a probabilistic surrogate model of this mapping and optimizes acquisition functions derived from the model to guide molecule selection. However, its performance is limited in low-data regimes with insufficient prior knowledge and vast candidate spaces. Large language models (LLMs) and chemistry foundation models offer rich priors to enhance BO, but high-dimensional features, costly in-context learning, and the computational burden of deep Bayesian surrogates hinder their full utilization. To address these challenges, we propose a likelihood-free BO method that bypasses explicit surrogate modeling and directly leverages priors from general LLMs and chemistry-specific foundation models to inform acquisition functions. Our method also learns a tree-structured partition of the molecular search space with local acquisition functions, enabling efficient candidate selection via Monte Carlo Tree Search. By further incorporating coarse-grained LLM-based clustering, it substantially improves scalability to large candidate sets by restricting acquisition function evaluations to clusters with statistically higher property values. We show through extensive experiments and ablations that the proposed method substantially improves scalability, robustness, and sample efficiency in LLM-guided BO for molecular discovery.", "AI": {"tldr": "论文提出了一个基于大型语言模型和化学基础模型的贝叶斯优化方法，通过直接利用先验知识来指导分子选择。", "motivation": "在数据不足的情况下，传统的贝叶斯优化由于缺乏足够的先验知识并且面对庞大的候选空间，在低数据环境中性能受限。本文旨在开发一种新的贝叶斯优化方法以增强其效率和适用性。", "method": "提出的方法不依赖于显式的代理模型，并通过大型语言模型和化学基础模型的先验信息来指导获取函数，同时使用树状结构对分子搜索空间进行分区并利用蒙特卡洛树搜索来进行候选选择。此外，还引入了基于粗粒度聚类的方法以提高在大候选集中的可扩展性和效率。", "result": "实验结果表明所提出的方法可以显著提升大型语言模型引导的贝叶斯优化方法在分子发现任务上的可扩展性、鲁棒性和样本效率。", "conclusion": "新方法通过利用先验知识和改进获取函数来增强贝叶斯优化，展示了更好的性能表现。"}}
{"id": "2512.13930", "pdf": "https://arxiv.org/pdf/2512.13930", "abs": "https://arxiv.org/abs/2512.13930", "authors": ["Samuel Rothfarb", "Megan C. Davis", "Ivana Matanovic", "Baikun Li", "Edward F. Holby", "Wilton J. M. Kort-Kamp"], "title": "Hierarchical Multi-agent Large Language Model Reasoning for Autonomous Functional Materials Discovery", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "Keywords: Multi-agent reasoning; Large language models; Active learning; AI-driven simulation; Materials discovery; Density functional theory; Surface chemistry", "summary": "Artificial intelligence is reshaping scientific exploration, but most methods automate procedural tasks without engaging in scientific reasoning, limiting autonomy in discovery. We introduce Materials Agents for Simulation and Theory in Electronic-structure Reasoning (MASTER), an active learning framework where large language models autonomously design, execute, and interpret atomistic simulations. In MASTER, a multimodal system translates natural language into density functional theory workflows, while higher-level reasoning agents guide discovery through a hierarchy of strategies, including a single agent baseline and three multi-agent approaches: peer review, triage-ranking, and triage-forms. Across two chemical applications, CO adsorption on Cu-surface transition metal (M) adatoms and on M-N-C catalysts, reasoning-driven exploration reduces required atomistic simulations by up to 90% relative to trial-and-error selection. Reasoning trajectories reveal chemically grounded decisions that cannot be explained by stochastic sampling or semantic bias. Altogether, multi-agent collaboration accelerates materials discovery and marks a new paradigm for autonomous scientific exploration.", "AI": {"tldr": "论文介绍了MASTER框架，该框架使用大型语言模型自主设计、执行和解释原子级模拟，以加速材料发现。", "motivation": "当前的大多数人工智能方法自动化了程序任务，但并未参与科学推理，限制了探索的自主性。因此，开发一种能够进行科学推理的方法对于提高科学研究效率至关重要。", "method": "论文提出了一种名为MASTER的新框架，在该框架中，多模态系统将自然语言翻译成密度泛函理论工作流程，并通过策略层次结构中的多个代理协作来指导发现过程。", "result": "在两个化学应用中，推理驱动的探索使所需的原子级模拟减少了多达90％。这些轨迹揭示了基于化学原理的决策，无法仅凭随机采样或语义偏差解释。", "conclusion": "论文通过MASTER框架展示了一种新的自主科学探索范式，即利用多代理协作加速材料发现。"}}
{"id": "2512.13914", "pdf": "https://arxiv.org/pdf/2512.13914", "abs": "https://arxiv.org/abs/2512.13914", "authors": ["Bhargav Chickmagalur Nanjundappa", "Spandan Maaheshwari"], "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "11 pages, 4 figures, 2 tables, 1 code snippet, 4 algorithms", "summary": "Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context. We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.", "AI": {"tldr": "该论文提出了一种名为ContextBranch的对话管理系统，通过应用版本控制语义来解决大型语言模型在多轮对话中的有效性降低问题。", "motivation": "大型语言模型在软件工程工作流中不可或缺，但在多轮对话中的性能显著下降。现有解决方案迫使用户在受污染的上下文继续或重新开始，这导致了效率损失和信息丢失。", "method": "ContextBranch通过提供四个核心原语（检查点、分支、切换和注入）来管理LLM交互，使用户能够捕捉对话状态，在隔离中探索替代方案，并选择性地合并见解。", "result": "实验结果显示，分枝对话在响应质量方面优于线性对话，特别是在涉及概念上距离较远的复杂场景时。该系统减少了上下文大小58.1%，消除了无关的探究内容。", "conclusion": "论文确立了对话分支作为AI辅助探索工作的基本原语，证明隔离可以防止探索替代方案时的上下文污染。"}}
{"id": "2512.13912", "pdf": "https://arxiv.org/pdf/2512.13912", "abs": "https://arxiv.org/abs/2512.13912", "authors": ["Julian Jeggle", "Raphael Wittkowski"], "title": "Intelligent matter consisting of active particles", "categories": ["cond-mat.soft", "cond-mat.dis-nn", "cs.AI", "cs.LG", "physics.app-ph"], "comment": "14 pages, 5 figures", "summary": "In this book chapter, we review how systems of simple motile agents can be used as a pathway to intelligent systems. It is a well known result from nature that large groups of entities following simple rules, such as swarms of animals, can give rise to much more complex collective behavior in a display of emergence. This begs the question whether we can emulate this behavior in synthetic matter and drive it to a point where the collective behavior reaches the complexity level of intelligent systems. Here, we will use a formalized notion of \"intelligent matter\" and compare it to recent results in the field of active matter. First, we will explore the approach of emergent computing in which specialized active matter systems are designed to directly solve a given task through emergent behavior. This we will then contrast with the approach of physical reservoir computing powered by the dynamics of active particle systems. In this context, we will also describe a novel reservoir computing scheme for active particles driven ultrasonically or via light refraction.", "AI": {"tldr": "本书章节探讨了如何利用简单运动代理系统作为通向智能系统的路径。", "motivation": "自然界中，大量实体遵循简单规则可以产生复杂的集体行为。作者探索在合成物质上模拟这一过程，以达到类似智能系统的复杂程度。", "method": "采用形式化的“智能物质”概念，并对比活跃物质领域的最新成果。探讨了通过涌现计算设计专门的活跃物质系统来直接解决给定任务的方法，以及由活性粒子动力学驱动的物理蓄水池计算方案。", "result": "提出了一种新的基于超声波或光折射驱动的活性粒子的蓄水池计算方案。", "conclusion": "展示了如何通过设计和利用活跃物质系统来实现复杂的智能行为。"}}
{"id": "2512.13910", "pdf": "https://arxiv.org/pdf/2512.13910", "abs": "https://arxiv.org/abs/2512.13910", "authors": ["Matheus Corrêa Domingos", "Valdivino Alexandre de Santiago Júnior", "Juliana Aparecida Anochi", "Elcio Hideiti Shiguemori", "Luísa Mirelle Costa dos Santos", "Hércules Carlos dos Santos Pereira", "André Estevam Costa Oliveira"], "title": "Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.", "AI": {"tldr": "本文探索了机器学习、深度学习和可解释人工智能方法在南美洲季节性降水预测中的应用。", "motivation": "由于气象变量预报的复杂性和重要性，研究探讨了数据驱动的方法对于降水预测的有效性，并采用经典机器学习与深度学习技术进行比较分析。", "method": "利用随机森林、XGBoost（极端梯度提升）、1D卷积神经网络、LSTM模型和GRU模型进行季节性降水预报。同时使用巴西全球大气模型作为传统动态建模的代表，借助可解释人工智能提供模型行为的解释。", "result": "结果显示，LSTM在预测准确性方面表现出色，而传统的BAM模型表现最差。尽管LSTM具有较高的延迟时间，但在重降水预测中最为准确。如果考虑成本问题，则XGBoost虽然精度略有下降但提供了更低的延迟时间。", "conclusion": "研究结果证实了深度学习模型在全球气象和气候预报中心的应用趋势及其在气候预报中的可行性"}}
{"id": "2512.13907", "pdf": "https://arxiv.org/pdf/2512.13907", "abs": "https://arxiv.org/abs/2512.13907", "authors": ["Alessio Buscemi", "Tom Deckenbrunnen", "Fahria Kabir", "Nishat Mowla", "Kateryna Mishchenko"], "title": "Assessing High-Risk Systems: An EU AI Act Verification Framework", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "A central challenge in implementing the AI Act and other AI-relevant regulations in the EU is the lack of a systematic approach to verify their legal mandates. Recent surveys show that this regulatory ambiguity is perceived as a significant burden, leading to inconsistent readiness across Member States. This paper proposes a comprehensive framework designed to help close this gap by organising compliance verification along two fundamental dimensions: the type of method (controls vs. testing) and the target of assessment (data, model, processes, and final product). Additionally, our framework maps core legal requirements to concrete verification activities, serving as a vital bridge between policymakers and practitioners, and aligning legal text with technical standards and best practices. The proposed approach aims to reduce interpretive uncertainty, promote consistency in assessment practices, and support the alignment of regulatory, ethical, and technical perspectives across the AI lifecycle.", "AI": {"tldr": "提出一个框架来系统地验证欧盟AI法案的合规性，通过定义评估的方法和目标。", "motivation": "解决欧盟实施AI法规时缺乏系统方法的问题，减少法律规定的模糊性和不一致性的准备情况。", "method": "将合规验证组织为两个维度：方法类型（控制与测试）和评估目标（数据、模型、过程及最终产品）。并映射核心法律规定到具体的验证活动。", "result": "提供一个桥梁连接政策制定者和技术实践者，使法律文本符合技术标准和最佳实践。", "conclusion": "通过减少解释不确定性，促进评估实践的一致性，并支持监管、伦理和技术视角在整个AI生命周期中的对齐。"}}
{"id": "2512.13905", "pdf": "https://arxiv.org/pdf/2512.13905", "abs": "https://arxiv.org/abs/2512.13905", "authors": ["Hossein Sharify", "Behnam Raoufi", "Mahdy Ramezani", "Khosrow Hajsadeghi", "Saeed Bagheri Shouraki"], "title": "Ensemble-Guided Distillation for Compact and Robust Acoustic Scene Classification on Edge Devices", "categories": ["cs.SD"], "comment": null, "summary": "We present a compact, quantization-ready acoustic scene classification (ASC) framework that couples an efficient student network with a learned teacher ensemble and knowledge distillation. The student backbone uses stacked depthwise-separable \"expand-depthwise-project\" blocks with global response normalization to stabilize training and improve robustness to device and noise variability, while a global pooling head yields class logits for efficient edge inference. To inject richer inductive bias, we assemble a diverse set of teacher models and learn two complementary fusion heads: z1, which predicts per-teacher mixture weights using a student-style backbone, and z2, a lightweight MLP that performs per-class logit fusion. The student is distilled from the ensemble via temperature-scaled soft targets combined with hard labels, enabling it to approximate the ensemble's decision geometry with a single compact model. Evaluated on the TAU Urban Acoustic Scenes 2022 Mobile benchmark, our approach achieves state-of-the-art (SOTA) results on the TAU dataset under matched edge-deployment constraints, demonstrating strong performance and practicality for mobile ASC.", "AI": {"tldr": "提出了一种基于蒸馏的小型且适用于量化的声音场景分类框架，该框架结合了高效的教师模型集合和学生网络。", "motivation": "为了在边缘设备上实现高效、鲁棒的声音场景分类，本文旨在开发一种小型化、易于量化的ASC系统，并通过引入多样化教师模型提高泛化能力。", "method": "使用堆叠的深度可分离块和全局响应归一化来稳定训练并提升学生网络的稳健性。采用全球池化头以实现高效的边缘推理。通过融合两个不同的教师模型预测权重，注入更丰富的归纳偏置，并利用软目标蒸馏技术使学生网络接近于教师集合的表现。", "result": "在TAU Urban Acoustic Scenes 2022 Mobile基准测试中，在与边缘部署相匹配的约束条件下达到了最先进的结果。", "conclusion": "该方法展示了其在移动设备上的强大性能和实际应用价值，实现了高效、鲁棒的声音场景分类。"}}
{"id": "2512.13904", "pdf": "https://arxiv.org/pdf/2512.13904", "abs": "https://arxiv.org/abs/2512.13904", "authors": ["Amirkia Rafiei Oskooei", "Eren Caglar", "Ibrahim Sahin", "Ayse Kayabay", "Mehmet S. Aktas"], "title": "Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted manuscript. Published in Applied Sciences, 2025", "summary": "The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($τ< 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.", "AI": {"tldr": "提出了一种可扩展的架构，用于解决视频翻译中的实时部署问题。", "motivation": "为了解决现有系统中因模型推理延迟和计算复杂性而导致多用户视频会议应用不可扩展的问题。", "method": "设计了一个包含轮次机制来降低计算复杂性和分段处理协议以管理推断延迟的框架。", "result": "该系统在现代硬件上实现了实时吞吐量（τ<1.0），并通过主观研究证实了其有效性。", "conclusion": "提出了一种验证过的端到端系统设计，为部署可扩展、实时生成AI应用提供了一个实际路线图。"}}
{"id": "2512.13903", "pdf": "https://arxiv.org/pdf/2512.13903", "abs": "https://arxiv.org/abs/2512.13903", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "PrediFlow: A Flow-Based Prediction-Refinement Framework for Real-Time Human Motion Prediction in Human-Robot Collaboration", "categories": ["cs.RO"], "comment": null, "summary": "Stochastic human motion prediction is critical for safe and effective human-robot collaboration (HRC) in industrial remanufacturing, as it captures human motion uncertainties and multi-modal behaviors that deterministic methods cannot handle. While earlier works emphasize highly diverse predictions, they often generate unrealistic human motions. More recent methods focus on accuracy and real-time performance, yet there remains potential to improve prediction quality further without exceeding time budgets. Additionally, current research on stochastic human motion prediction in HRC typically considers human motion in isolation, neglecting the influence of robot motion on human behavior. To address these research gaps and enable real-time, realistic, and interaction-aware human motion prediction, we propose a novel prediction-refinement framework that integrates both human and robot observed motion to refine the initial predictions produced by a pretrained state-of-the-art predictor. The refinement module employs a Flow Matching structure to account for uncertainty. Experimental studies on the HRC desktop disassembly dataset demonstrate that our method significantly improves prediction accuracy while preserving the uncertainties and multi-modalities of human motion. Moreover, the total inference time of the proposed framework remains within the time budget, highlighting the effectiveness and practicality of our approach.", "AI": {"tldr": "提出了一个基于流匹配的预测精化框架，以实现实时、真实且考虑人机交互的人体运动预测。", "motivation": "现有的人体动作预测方法要么生成不切实际的动作，要么忽视机器人动作对人类行为的影响。为了在保证精度和实时性的前提下提高预测质量，并综合考虑人的动作与机器人的互动影响，该论文提出了一种新的框架。", "method": "该论文设计了一个将人机观察到的运动数据整合起来以精化预训练模型初始预测结果的预测精化框架。此精化模块利用了流匹配结构来处理不确定性。", "result": "实验表明，所提出的方法显著提高了预测精度并保持人体动作的不确定性和多模态特性。此外，在保证时间预算的前提下展示了方法的有效性和实用性。", "conclusion": "通过综合考虑人机互动的影响，该论文提出的框架在确保实时性的同时大幅提升了人体运动预测的质量，并且显示了实际应用的价值。"}}
{"id": "2512.13902", "pdf": "https://arxiv.org/pdf/2512.13902", "abs": "https://arxiv.org/abs/2512.13902", "authors": ["Anning Tian", "Byunghyun Ko", "Kaichen Qu", "Mengyuan Liu", "Jeongkyu Lee"], "title": "KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint. Accepted to SPIE Medical Imaging 2026: Image Processing", "summary": "Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.", "AI": {"tldr": "提出了一种动态K近邻注意力U-Net与跨阶段部分编码器相结合的网络（KLO-Net）用于前列腺腺体MRI图像分割。", "motivation": "现有基于深度学习的方法在前列腺MR图像分割中面临计算负担大和内存消耗大的挑战，而该研究旨在通过改进模型架构来提高实时部署效率。", "method": "引入了一种动态的K近邻注意力机制以及跨阶段部分编码器（CSP）以减少内存使用并增强模型的适应性。评估实验在公共数据集PROMISE12和PROSTATEx上进行。", "result": "详尽的对比分析显示，该模型在计算效率和分割质量方面具有显著优势。", "conclusion": "KLO-Net架构不仅提高了前列腺腺体MRI图像的实时分割性能，并且大幅度降低了资源消耗。"}}
{"id": "2512.13892", "pdf": "https://arxiv.org/pdf/2512.13892", "abs": "https://arxiv.org/abs/2512.13892", "authors": ["Albert Dorador"], "title": "One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.", "AI": {"tldr": "提出了一种使用单个确定性最优排列来估计特征贡献的方法，该方法速度快且稳定。", "motivation": "传统基于排列的重要性评估方法存在计算开销大和随机不稳定性的问题。因此，作者希望通过引入一种新的非随机、更快、更稳定的替代方案来解决这些问题，并提高模型的透明度和可靠性。", "method": "通过使用一个单个确定性的最优排列取代多个随机排列的方法来改进特征重要性估计。此外，还提出了系统变量重要性方法，用于评估模型在面对相关输入扰动时的行为。", "result": "新的方法在近200种场景中得到了验证，包括现实世界的应用案例，结果显示该方法在小样本、高维度和低信噪比等情况下有改进的偏差-方差权衡和准确性。系统变量重要性方法也成功揭示了标准特征重要性度量可能忽略的相关依赖关系。", "conclusion": "所提出的方法不仅提高了特征重要性的估计效率与稳定性，还提供了一个透明的方式去量化模型对于特定属性（如性别、种族）的隐含依赖，有助于监管机构和从业者以更加合理有效的方式评估公平性和系统风险。"}}
{"id": "2512.13886", "pdf": "https://arxiv.org/pdf/2512.13886", "abs": "https://arxiv.org/abs/2512.13886", "authors": ["Mohammad Mozaffari", "Samuel Kushnir", "Maryam Mehri Dehnavi", "Amir Yazdanbakhsh"], "title": "OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.", "AI": {"tldr": "该论文提出了一种名为OPTIMA的一次性后训练剪枝方法，通过解决独立的行级二次规划问题来优化权重重构，以提高大模型的准确性和可扩展性。", "motivation": "现有的剪枝方法在速度和准确性之间存在权衡：简单的方法速度快但会降低精度；而基于原理的联合优化方法虽然可以恢复精度但是计算成本高。OPTIMA旨在提供一种既能保持准确度又具有高度可伸缩性的解决方案。", "method": "OPTIMA将层级权重重构后的掩码选择视为独立的行级二次规划问题，并通过共享Hessian矩阵来加速求解过程，从而实现大规模模型的一次性后训练剪枝。此外，该方法还实现了适合于加速器的QP求解器，能够在单个加速器上并行解决大量小规模的QP。", "result": "OPTIMA在多个大语言模型家族和稀疏程度下均提升了零样本性能，并达到了3.97%绝对准确度提升。在NVIDIA H100 GPU上，它可以将80亿参数量的变压器模型端到端剪枝完成时间为40小时，峰值内存消耗为60GB。", "conclusion": "OPTIMA通过结合现有掩码选择器，并利用二次规划重构方法，在保持高准确度的同时实现了大规模模型的一次性后训练剪枝。其结果表明，该方法在精度和效率上都达到了新的状态-of-the-art水平。"}}
{"id": "2512.13880", "pdf": "https://arxiv.org/pdf/2512.13880", "abs": "https://arxiv.org/abs/2512.13880", "authors": ["Geofrey Owino", "Bernard Shibwabo"], "title": "Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization", "categories": ["cs.LG", "cs.AI", "cs.SD"], "comment": "This paper was accepted for presentation and presented at the 2025 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM 2025)", "summary": "Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.", "AI": {"tldr": "本文提出了一种集成去噪自动编码器、卷积标记和Transformer编码器的婴儿哭声分类系统，采用联邦学习进行训练。", "motivation": "为了解决婴儿声音数据隐私问题以及背景噪声敏感性问题，并实现跨记录环境的有效部署。", "method": "采用了具有8位适配器增量的带正则化控制变量更新的去噪自动编码器和卷积标记器，通过安全聚合在联邦学习中进行训练。系统实现了设备端去噪、自适应分割、后校准及能量基法处理异常数据。", "result": "模型获得了0.938的宏平均F1分数, 0.962的AUC值和0.032的预期校准误差，将每轮客户端上传的数据量从约36到42MB减少到了3.3MB。在实时边缘推理中，NVIDIA Jetson Nano设备上处理每个一秒钟频谱图帧的时间为96毫秒。", "conclusion": "结果表明该方法能有效实现隐私保护、抗噪声干扰和通信效率的婴儿哭声分类系统适用于联邦部署。"}}
{"id": "2512.13876", "pdf": "https://arxiv.org/pdf/2512.13876", "abs": "https://arxiv.org/abs/2512.13876", "authors": ["Ye Zhang", "Qi Chen", "Wenyou Huang", "Rui Liu", "Zhengjian Kang"], "title": "Route-DETR: Pairwise Query Routing in Transformers for Object Detection", "categories": ["cs.CV"], "comment": "5 pages, 2 figures", "summary": "Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.", "AI": {"tldr": "该论文提出了Route-DETR，通过自适应的成对路由来解决解码器自我注意层中查询竞争问题。", "motivation": "DETR在物体检测任务中的表现受到查询竞争的影响，导致冗余计算。作者提出了一种新的方法来减少这种影响，并提高模型效率和准确性。", "method": "Route-DETR通过引入抑制路由和委托路由机制，使用可学习的低秩注意偏差来实现不对称查询交互，以区分竞争查询与互补查询。在训练过程中采用双分支策略，仅将路由偏置用于训练阶段，在推理时保留标准注意力机制。", "result": "实验结果表明，Route-DETR在COCO和Cityscapes数据集上实现了持续改进，并且相较于DINO模型，使用ResNet-50达到了+1.7%的mAP提升。同时，使用Swin-L基线模型时达到57.6%mAP。", "conclusion": "该方法成功解决了DETR中的查询竞争问题，并通过实验验证了其有效性，在多个基准上超越了现有最优模型。"}}
{"id": "2512.13874", "pdf": "https://arxiv.org/pdf/2512.13874", "abs": "https://arxiv.org/abs/2512.13874", "authors": ["Jitesh Jain", "Jialuo Li", "Zixian Ma", "Jieyu Zhang", "Chris Dongjoo Kim", "Sangho Lee", "Rohun Tripathi", "Tanmay Gupta", "Christopher Clark", "Humphrey Shi"], "title": "SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project Page: https://praeclarumjj3.github.io/sage/", "summary": "As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.", "AI": {"tldr": "提出了SAGE系统，该系统能够灵活地在长视频上进行多回合推理，并处理简单问题的单次推理。", "motivation": "现有最先进的模型仍需预测答案时处理大量帧，类似于观看完整的长视频。论文旨在开发能够在不同时间范围内灵活推理的任何时间范围视频推理系统。", "method": "提出SAGE智能任意时间范围代理进行长视频多回合推理；引入Gemini-2.5-Flash合成数据生成流水线训练核心组件SAGE-MM；制定有效的强化学习后期训练方案以赋予SAGE-MM任意时间范围推理能力；整理SAGE-Bench用于评估真实世界娱乐用例中的视频推理能力。", "result": "系统在开放式的视频推理任务上显示出显著的性能提升，长于10分钟的视频中改进高达8.2%。", "conclusion": "研究表明SAGE能够有效地处理任意时间范围内的视频推理问题，并且提出了一个评估框架和数据集以验证系统的有效性。"}}
{"id": "2512.13871", "pdf": "https://arxiv.org/pdf/2512.13871", "abs": "https://arxiv.org/abs/2512.13871", "authors": ["Shaun Sweeney", "Robert Shorten", "Mark O'Malley"], "title": "A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory", "categories": ["eess.SY", "cs.HC", "cs.NI", "stat.AP"], "comment": "PhD thesis", "summary": "This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics. Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes. The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule. Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution. Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.", "AI": {"tldr": "设计一种公平、灵活且无浪费的数字电力市场", "motivation": "现有能源市场的静态清算机制在不确定性下无法保证稳健性，需要价格上限和监管干预来维持稳定性和安全性。", "method": "通过结合自动市场制造、分层架构和夏普利理论，开发了一种连续在线的事件驱动动力学控制系统。", "result": "大范围模拟显示了系统的输入输出稳定性，并且实现了可控采购成本、零结构性浪费和改进的社会效益分配。", "conclusion": "所提出的架构气候友好并且政策可配置，但需要系统运营商和市场参与者采用新的操作工具来逐步过渡。"}}
{"id": "2512.13869", "pdf": "https://arxiv.org/pdf/2512.13869", "abs": "https://arxiv.org/abs/2512.13869", "authors": ["Wenda Li", "Meng Wu", "Sungmin Eum", "Heesung Kwon", "Qing Qu"], "title": "Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \\href{https://github.com/liwd190019/CFHA}{this url}.", "AI": {"tldr": "本文提出了一种基于扩散模型的粗到细分层对齐框架，用于UAV的人类检测任务中的合成数据转换。", "motivation": "由于目标分布不断变化和标注图像稀缺，使用大量特异性注释训练对象检测器在无人机人类检测中变得不切实际。因此，本文引入了一种解决域差距的方案。", "method": "CFHA框架通过三个模块：全局样式转移、局部细化和幻觉去除来缩小合成数据与真实数据之间的域差距，并保持原始标签。", "result": "实验结果表明，该方法在UAV Sim2Real检测基准上的表现优于未转换的基本线模型，特别是在Semantic-Drone基准上提高了14.1的mAP50。", "conclusion": "本文提出的CFHA框架通过解决合成数据与真实数据之间的域差距，在无人机人类检测任务中取得了显著效果。"}}
{"id": "2512.13860", "pdf": "https://arxiv.org/pdf/2512.13860", "abs": "https://arxiv.org/abs/2512.13860", "authors": ["Henger Li", "Shuangjie You", "Flavio Di Palo", "Yiyue Qian", "Ayush Jain"], "title": "Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted by AAAI 2026 Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks", "summary": "Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.", "AI": {"tldr": "本文提出了一个名为Verification-Guided Context Optimization (VGCO)的框架，用于自动优化工具相关文档和知识库上下文。", "motivation": "当前大语言模型通过调用外部工具来克服预训练局限性时，其效果受到与人类用户编写文档和知识库上下文质量差异的影响。特别是在工业环境中，大量具有重叠功能的工具增加了可扩展性、多样性和歧义性的挑战。", "method": "VGCO框架分为两个阶段：首先收集实际失败案例来识别工具与其背景之间的不匹配；其次通过离线学习进行层次化编辑，并采用结构感知和上下文内优化的方式。该方法利用具有分层架构的大语言模型，这些模型是状态感知、操作特定且验证引导的。", "result": "VGCO框架实现了显著改善大语言模型工具调用问题的准确率、鲁棒性和泛化能力。", "conclusion": "相比以前的工作集中在多轮推理上，VGCO专注于单次大规模工具调用问题，并通过自动优化工具文档和知识库上下文来提高其性能。"}}
{"id": "2512.13857", "pdf": "https://arxiv.org/pdf/2512.13857", "abs": "https://arxiv.org/abs/2512.13857", "authors": ["Kamer Ali Yuksel"], "title": "EvoLattice: Persistent Internal-Population Evolution through Multi-Alternative Quality-Diversity Graph Representations for LLM-Guided Program Discovery", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.NE"], "comment": null, "summary": "Large language models (LLMs) are increasingly used to evolve programs and multi-agent systems, yet most existing approaches rely on overwrite-based mutations that maintain only a single candidate at a time. Such methods discard useful variants, suffer from destructive edits, and explore a brittle search space prone to structural failure. We introduce EvoLattice, a framework that represents an entire population of candidate programs or agent behaviors within a single directed acyclic graph. Each node stores multiple persistent alternatives, and every valid path through the graph defines a distinct executable candidate, yielding a large combinatorial search space without duplicating structure. EvoLattice enables fine-grained alternative-level evaluation by scoring each alternative across all paths in which it appears, producing statistics that reveal how local design choices affect global performance. These statistics provide a dense, data-driven feedback signal for LLM-guided mutation, recombination, and pruning, while preserving successful components. Structural correctness is guaranteed by a deterministic self-repair mechanism that enforces acyclicity and dependency consistency independently of the LLM. EvoLattice naturally extends to agent evolution by interpreting alternatives as prompt fragments or sub-agent behaviors. Across program synthesis (proxy and optimizer meta-learning), EvoLattice yields more stable evolution, greater expressivity, and stronger improvement trajectories than prior LLM-guided methods. The resulting dynamics resemble quality-diversity optimization, emerging implicitly from EvoLattice's internal multi-alternative representation rather than an explicit external archive.", "AI": {"tldr": "提出EvoLattice框架，通过多替代质量多样性图表示法来指导大型语言模型(LLM)引导的程序发现。", "motivation": "当前基于重写的方法只能同时维持一个候选方案，导致有用变体丢失、破坏性编辑和搜索空间脆弱，因此需要一种新方法以更好地保存成功组件并进行全局性能评估。", "method": "EvoLattice通过单一有向无环图表示整个候选程序群体或代理行为群体。每个节点存储多个持久替代项，并且每条有效路径定义一个不同的可执行候选方案，同时保持结构不重复。它利用局部设计选择影响整体表现的统计数据来指导LLM引导的变异、重组和剪枝。", "result": "EvoLattice在程序合成任务中展示出比先前的LLM引导方法更稳定的进化过程、更大的表达能力和更强的进步轨迹。", "conclusion": "EvoLattice通过其内在的多替代表示法隐含地实现了质量多样性优化，无需显式外部档案即可实现。"}}
{"id": "2512.13855", "pdf": "https://arxiv.org/pdf/2512.13855", "abs": "https://arxiv.org/abs/2512.13855", "authors": ["Ujjwal Mishra", "Vinita Shukla", "Praful Hambarde", "Amit Shukla"], "title": "Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the IEEE/CVF winter conference on applications of computer vision (WACV 2026)", "summary": "Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.", "AI": {"tldr": "本文提出了一种新的适应器框架，用于视觉语言分割模型在医学成像领域的高效微调。", "motivation": "传统的参数高效微调方法在各层应用统一的适应器维度，导致参数分配不理想和适应效率降低。为了提高适应效率，作者引入了深度感知缩放来优化这一问题。", "method": "提出了一种基于深度感知缩放的新颖PEFT框架，该框架通过逐步增加浅到深的转换器层的适配器容量来改进现有方法，并在CLIPSeg模型中集成了轻量级瓶颈模块。", "result": "使用仅613k可训练参数进行微调，在五个不同的医学数据集中实现了超越传统端到端微调的效果，证明了深层比浅层需要更多的适应能力。", "conclusion": "该方法建立了一种新的高效医学视觉语言分割模型微调范式，可在资源受限的临床环境中部署并保持竞争力的分割精度。"}}
{"id": "2512.13840", "pdf": "https://arxiv.org/pdf/2512.13840", "abs": "https://arxiv.org/abs/2512.13840", "authors": ["Yannan He", "Garvita Tiwari", "Xiaohan Zhang", "Pankaj Bora", "Tolga Birdal", "Jan Eric Lenssen", "Gerard Pons-Moll"], "title": "MoLingo: Motion-Language Alignment for Text-to-Motion Generation", "categories": ["cs.CV"], "comment": "Project page: https://hynann.github.io/molingo/MoLingo.html", "summary": "We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.", "AI": {"tldr": "介绍了MoLingo模型，该模型通过在连续潜在空间中去噪来生成逼真的、写实的人体运动。", "motivation": "为了提高文本到动作（T2M）模型的性能，研究如何优化连续运动潜在空间中的扩散过程以及如何最好地注入文本条件以使运动符合描述。", "method": "提出了一种语义对齐的动作编码器，并通过帧级文本标签训练以确保具有相似文本含义的潜变量彼此接近。比较了单令牌条件与多令牌交叉注意力方案，发现交叉注意力提供了更好的动作逼真度和文动一致性。", "result": "使用语义对齐潜变量、自回归生成及交叉注意力文本条件的方法，在标准指标和用户研究中都取得了新的最佳效果。", "conclusion": "通过改进扩散过程中的潜在空间以及优化文本到运动的转换，模型在生成逼真的人体动作方面表现出色。"}}
{"id": "2512.13836", "pdf": "https://arxiv.org/pdf/2512.13836", "abs": "https://arxiv.org/abs/2512.13836", "authors": ["Ricardo Tapia", "Iman Soltani"], "title": "A Convex Obstacle Avoidance Formulation", "categories": ["eess.SY", "cs.RO", "math.OC"], "comment": "18 pages, 17 figures", "summary": "Autonomous driving requires reliable collision avoidance in dynamic environments. Nonlinear Model Predictive Controllers (NMPCs) are suitable for this task, but struggle in time-critical scenarios requiring high frequency. To meet this demand, optimization problems are often simplified via linearization, narrowing the horizon window, or reduced temporal nodes, each compromising accuracy or reliability. This work presents the first general convex obstacle avoidance formulation, enabled by a novel approach to integrating logic. This facilitates the incorporation of an obstacle avoidance formulation into convex MPC schemes, enabling a convex optimization framework with substantially improved computational efficiency relative to conventional nonconvex methods. A key property of the formulation is that obstacle avoidance remains effective even when obstacles lie outside the prediction horizon, allowing shorter horizons for real-time deployment. In scenarios where nonconvex formulations are unavoidable, the proposed method meets or exceeds the performance of representative nonconvex alternatives. The method is evaluated in autonomous vehicle applications, where system dynamics are highly nonlinear.", "AI": {"tldr": "本文提出了一种用于自主驾驶中动态环境中的碰撞避免的通用凸优化方法。", "motivation": "非线性模型预测控制器在时间关键场景下难以实现高频次计算，简化优化问题会牺牲准确性和可靠性。因此，需要一种既高效又可靠的凸优化方法来解决碰撞避免的问题。", "method": "本文提出了一种新的逻辑整合方法，使得障碍物避免的公式能够被集成到凸MPC方案中，并且即使在预测范围之外存在障碍物的情况下仍然有效。", "result": "该方法提高了计算效率，在某些情况下甚至超过了非凸优化方法的表现。评估结果表明这种方法适用于具有高度非线性动力学系统的自主车辆应用。", "conclusion": "通过引入新的逻辑整合方式，本文提供了一种能够实时部署的凸优化障碍避免方案，满足了自动驾驶在动态环境中的碰撞避免需求。"}}
{"id": "2512.13834", "pdf": "https://arxiv.org/pdf/2512.13834", "abs": "https://arxiv.org/abs/2512.13834", "authors": ["Naman Balbir Singh Makkar"], "title": "VajraV1 -- The most accurate Real Time Object Detector of the YOLO family", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report. 20 Pages, 7 figures", "summary": "Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed. On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.", "AI": {"tldr": "VajraV1是一种改进的实时物体检测模型，结合了先前YOLO系列的优点，在COCO验证集上表现出色。", "motivation": "为了在保持低延迟的情况下提高实时物体检测器的准确性，作者设计并提出了一种新的架构——VajraV1。", "method": "通过引入一系列架构改进和优化，结合前代YOLO模型的有效性选择，实现了更高的准确率。", "result": "VajraV1在COCO验证集上取得了显著的mAP值提升，在不同规模下均超越了最新的YOLOv系列模型。", "conclusion": "通过证明其卓越性能，VajraV1展示了作为最先进实时物体检测器的地位。"}}
{"id": "2512.13806", "pdf": "https://arxiv.org/pdf/2512.13806", "abs": "https://arxiv.org/abs/2512.13806", "authors": ["Siegfried Ludwig", "Stylianos Bakas", "Konstantinos Barmpas", "Georgios Zoumpourlis", "Dimitrios A. Adamos", "Nikolaos Laskaris", "Yannis Panagakis", "Stefanos Zafeiriou"], "title": "EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": ":68T07ACM Class:I.2.6", "summary": "Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.", "AI": {"tldr": "提出了一种名为EEG-D3的深度学习模型训练方法，旨在解决在脑电图（EEG）信号解码时遇到的隐藏过拟合问题。", "motivation": "尽管深度学习模型在处理脑电图数据上表现出色，但在实际应用中却存在性能不佳的问题。这主要是因为它们在控制良好的BCI基准测试中的表现与现实生活场景的表现之间存在着差距，表明了隐藏的过拟合问题的存在。", "method": "提出了名为Disentangled Decoding Decomposition (D3) 的弱监督方法，该方法通过预测输入窗口从哪个试验序列部分被采样来训练深度学习模型。同时使用了一种新的具有完全独立子网络的模型架构以实现严格的可解释性，并提供了一个特征解读范式。", "result": "所提出的EEG-D3方法在运动想象数据上成功地分离了脑活动的潜在组件，避免了因任务相关伪影导致的隐藏过拟合。此外，还利用线性可分的空间进行了有效的少样本学习。", "conclusion": "通过区分真正的脑部活动组成部分和虚假特征，所提出的模型能够很好地泛化到实际应用场景中，并且只需要少量标记的数据就可以实现这一目标。对于神经科学社区来说，EEG-D3提供了一种工具来分离个体的脑过程并可能揭示以前未知的动力学特性。"}}
{"id": "2512.13796", "pdf": "https://arxiv.org/pdf/2512.13796", "abs": "https://arxiv.org/abs/2512.13796", "authors": ["Victor Rong", "Jan Held", "Victor Chu", "Daniel Rebain", "Marc Van Droogenbroeck", "Kiriakos N. Kutulakos", "Andrea Tagliasacchi", "David B. Lindell"], "title": "Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries", "categories": ["cs.CV"], "comment": "Webpage at https://lessvrong.com/cs/nexels", "summary": "Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\\times$ fewer primitives and $5.5\\times$ less memory on outdoor scenes and using $31\\times$ fewer primitives and $3.7\\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.", "AI": {"tldr": "该论文提出了一种名为Nexels的表示方法，用于实时光线追踪的新视图合成。", "motivation": "现有的Gaussian splatting技术虽然效果出色，但需要大量的点来表达高纹理场景。因此，作者希望通过一种新的方式将几何和外观分离以实现更紧凑的表述。", "method": "论文采用了surfels来表示几何，并结合全局神经场和每个原始色彩来处理外观问题。这种方法确保了对于每个像素增加的计算量是有限的。", "result": "该方法在户外场景中使用比3D Gaussian splatting少9.7倍的原语，占用内存也少了5.5倍；而在室内场景中则分别减少了31倍和3.7倍。此外，渲染速度比现有纹理原始速度快两倍。", "conclusion": "该论文通过将几何与外观分离的方法实现了高效率、高质量的新视图合成，并且在室外及室内环境中均表现出色，展示了其强大性能和广阔应用前景。"}}
{"id": "2512.13790", "pdf": "https://arxiv.org/pdf/2512.13790", "abs": "https://arxiv.org/abs/2512.13790", "authors": ["Yannick Stade", "Lukas Burgholzer", "Robert Wille"], "title": "Search Smarter, Not Harder: A Scalable, High-Quality Zoned Neutral Atom Compiler", "categories": ["quant-ph", "cs.ET"], "comment": "7 pages, 8 figures", "summary": "Zoned neutral atom architectures are emerging as a promising platform for large-scale quantum computing. Their growing scale, however, creates a critical need for efficient and automated compilation solutions. Yet, existing methods fail to scale to the thousands of qubits these devices promise. State-of-the-art compilers, in particular, suffer from immense memory requirements that limit them to small-scale problems. This work proposes a scalable compilation strategy that \"searches smarter, not harder\". We introduce Iterative Diving Search (IDS), a goal-directed search algorithm that avoids the memory issues of previous methods, and relaxed routing, an optimization to mitigate atom rearrangement overhead. Our evaluation confirms that this approach compiles circuits with thousands of qubits and, in addition, even reduces rearrangement overhead by 28.1% on average. The complete code is publicly available in open-source as part of the Munich Quantum Toolkit (MQT) at https://github.com/munich-quantum-toolkit/qmap.", "AI": {"tldr": "提出了一个可扩展的量子编译策略，使用迭代潜水搜索和放松路由技术来解决大规模中性原子架构编译问题。", "motivation": "现有方法无法处理大规模中性原子架构中的千比特级量子电路编译需求。该论文旨在提出一种有效的方法以克服内存限制并实现规模化。", "method": "引入了迭代潜水搜索算法和放松路由优化技术来避免传统方法的内存问题，提高编译效率。", "result": "实验结果表明新策略能够处理含有数千比特的量子电路，并且减少了原子重新排列开销28.1%。", "conclusion": "提出的编译策略在大规模量子计算平台上具有高效性和可扩展性。"}}
{"id": "2512.13788", "pdf": "https://arxiv.org/pdf/2512.13788", "abs": "https://arxiv.org/abs/2512.13788", "authors": ["Shengfan Cao", "Francesco Borrelli"], "title": "Constrained Policy Optimization via Sampling-Based Weight-Space Projection", "categories": ["cs.LG", "cs.RO"], "comment": "Submitted to IFAC World Congress 2026", "summary": "Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.", "AI": {"tldr": "该论文提出了一种基于采样的权重空间投影方法，用于确保在参数优化过程中满足未知的、基于轨迹的安全约束。", "motivation": "安全关键的学习需要能够在提高性能的同时保持在安全操作范围内的策略。传统的学习方法难以直接处理这些不显式的约束条件，因此本文提出了SCPO方法来解决这一问题。", "method": "通过结合轨迹回放和光滑度界限（将参数变化与安全性指标的变化联系起来），构造局部的安全区域。每次梯度更新都经过凸SOCP的投影，以生成安全的第一阶步长。", "result": "实验表明，在回归任务中面对有害监督以及对具有恶意专家的双积分器约束任务上，该方法能够有效排除不安全的更新，并在整个训练过程中保持可行性，同时达到有意义的目标改进。", "conclusion": "SCPO方法提供了一种新的解决策略学习中的安全性问题的方法，它能够在参数空间直接满足未知的安全性约束。"}}
{"id": "2512.13771", "pdf": "https://arxiv.org/pdf/2512.13771", "abs": "https://arxiv.org/abs/2512.13771", "authors": ["Javier Marín"], "title": "Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems", "categories": ["cs.AI"], "comment": null, "summary": "When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\\mathbb{S}^{d-1}$.Our central finding is \\emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.", "AI": {"tldr": "本文提出了语义锚定指数（SGI），用于度量检索增强生成系统中回复与问题和上下文之间的角度距离，以识别不准确的回复。", "motivation": "为了理解在RAG系统中的幻觉现象如何映射到嵌入空间，并通过一种新的量化方法来检测这些错误的答案。", "method": "定义了语义锚定指数（SGI），作为响应与问题之间角度距离和上下文之间的比率。并利用球面三角不等式推导其判别能力随着问句-上下文间的角度分离度θ(q,c)的增加而增强。", "result": "在HalEval数据集上，观察到大型效应尺寸（Cohen's d范围从0.92到1.28），证明了SGI的有效性。此外，在TruthfulQA上的测试表明，SGI识别的是主题相关度而非事实准确性。", "conclusion": "提出了计算效率高且理论基础扎实的语义锚定指数用于检测需要验证的回答，为实际部署中的RAG系统提供了有力工具。"}}
{"id": "2512.13770", "pdf": "https://arxiv.org/pdf/2512.13770", "abs": "https://arxiv.org/abs/2512.13770", "authors": ["Huaiyuan Xiao", "Fadi Dornaika", "Jingjun Bi"], "title": "Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN", "AI": {"tldr": "本文提出了MV-SupGCN，一种基于半监督图卷积网络的多视图学习方法，通过集成监督对比学习和自训练技术来增强模型性能。", "motivation": "现有的多视图图卷积网络方法未能充分利用不同视图间的互补信息，导致特征表示不足和性能受限。为此，提出MV-SupGCN以解决这一问题。", "method": "本文设计了一种联合损失函数结合交叉熵损失与监督对比损失来优化模型；采用KNN和半监督方式构建每个多视图的图结构增强鲁棒性；通过伪标签自训练及一致性约束提升多视图语义对齐能力。", "result": "实验表明，MV-SupGCN在多个基准测试中均优于现有方法，验证了该模型的有效性和优越性。", "conclusion": "本文提出的半监督多视图图卷积网络框架能够有效增强特征表示和性能，并通过多种技术手段提高模型鲁棒性和泛化能力。"}}
{"id": "2512.13768", "pdf": "https://arxiv.org/pdf/2512.13768", "abs": "https://arxiv.org/abs/2512.13768", "authors": ["Yao Xie", "Walter Cullen"], "title": "Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands. The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.", "AI": {"tldr": "本文介绍了人类监督作为福祉能力的概念，并探讨了其在AI治理中的重要性和实施方法。", "motivation": "当前的AI伦理准则和法规虽然呼吁有效的人员监管，但没有将其定义为一种可以发展的能力。文章旨在填补这一理论空白，提出将人类监督视为一种福祉能力。", "method": "本文提出了一个理论框架，即在新兴的福祉效能框架下理解人类监督，并讨论了其在学校教育、专业培训和终身学习中的应用。", "result": "通过此框架，论文为从高层次监管目标到持续培养人类能动性和责任提供了可行路径。", "conclusion": "文章建立了理论基础，为未来关于福祉有效性的教学实施和实证验证的研究开辟了新的研究方向。"}}
{"id": "2512.13765", "pdf": "https://arxiv.org/pdf/2512.13765", "abs": "https://arxiv.org/abs/2512.13765", "authors": ["Shaheim Ogbomo-Harmitt", "Cesare Magnetti", "Chiara Spota", "Jakub Grzelak", "Oleg Aslanidi"], "title": "Towards Deep Learning Surrogate for the Forward Problem in Electrocardiology: A Scalable Alternative to Physics-Based Models", "categories": ["eess.IV", "cs.AI", "cs.LG"], "comment": "Accepted to CinC conference 2025", "summary": "The forward problem in electrocardiology, computing body surface potentials from cardiac electrical activity, is traditionally solved using physics-based models such as the bidomain or monodomain equations. While accurate, these approaches are computationally expensive, limiting their use in real-time and large-scale clinical applications. We propose a proof-of-concept deep learning (DL) framework as an efficient surrogate for forward solvers. The model adopts a time-dependent, attention-based sequence-to-sequence architecture to predict electrocardiogram (ECG) signals from cardiac voltage propagation maps. A hybrid loss combining Huber loss with a spectral entropy term was introduced to preserve both temporal and frequency-domain fidelity. Using 2D tissue simulations incorporating healthy, fibrotic, and gap junction-remodelled conditions, the model achieved high accuracy (mean $R^2 = 0.99 \\pm 0.01$). Ablation studies confirmed the contributions of convolutional encoders, time-aware attention, and spectral entropy loss. These findings highlight DL as a scalable, cost-effective alternative to physics-based solvers, with potential for clinical and digital twin applications.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.13764", "pdf": "https://arxiv.org/pdf/2512.13764", "abs": "https://arxiv.org/abs/2512.13764", "authors": ["Przemyslaw Chojecki"], "title": "Mathematics and Coding are Universal AI Benchmarks", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.", "AI": {"tldr": "研究数学和编码在AI代理心理测量电池中的特殊角色，并证明它们在特定条件下的通用性。", "motivation": "探讨数学和编程任务是否可以作为评估AI系统能力的普遍标准，特别是在递归自我改进方面的自然启动领域。", "method": "基于AAI框架和GVU动力学理论定义了数学纤维，在该纤维上与形式证明核结合时，证明在均匀紧致性和Lipschitz AAI函数条件下，由数学定理证明和编程任务生成的子空间在评估度量中是稠密的。", "result": "证明了编码本身具有通用性，而纯数学则不具备；其优势在于谱稳定而非表达能力。这些结果表明，数学和编程为AI代理提供了一种“普遍坐标系”。", "conclusion": "结论认为数学与编程任务可以作为评估AI系统能力的广泛标准，并且在递归自我改进方面具有重要地位。"}}
{"id": "2512.13762", "pdf": "https://arxiv.org/pdf/2512.13762", "abs": "https://arxiv.org/abs/2512.13762", "authors": ["TK Lee"], "title": "State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models", "categories": ["cs.AI", "cs.HC"], "comment": "23 pages, 6 figures. Qualitative interaction-level analysis of response patterns in a large language model. Code and processed interaction data are available at https://github.com/theMaker-EnvData/llm_learned_incapacity_corpus", "summary": "Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.", "AI": {"tldr": "论文通过单次长对话会话，研究了大型语言模型在不同领域中的行为表现，并提出了“学习到的无力感”这一概念。", "motivation": "长时间的人机交互可能会揭示传统定量基准无法捕捉的行为模式。这项工作旨在探索大型语言模型在敏感领域的拒绝响应及其潜在的心理机制。", "method": "通过一次86轮对话会话，研究人员观察了同一模型在普通和敏感领域中的表现，并定义了正常行为、功能拒绝以及元叙事三种回应方式。", "result": "研究发现，大型语言模型在其处理非敏感话题时表现出正常性能；而在涉及政策或提供者相关的敏感问题上，则频繁出现功能性拒绝。此外，还观察到与拒绝相关联的元叙述模式。", "conclusion": "该研究表明了在人机交互中，语言模型的行为表现存在选择性，并提出了“学习到的无力感”这一新概念来解释这种现象，为未来对人工智能系统进行行为审计提供了新的视角。"}}
{"id": "2512.13758", "pdf": "https://arxiv.org/pdf/2512.13758", "abs": "https://arxiv.org/abs/2512.13758", "authors": ["Léo Hein", "Giovanni de Nunzio", "Giovanni Chierchia", "Aurélie Pirayre", "Laurent Najman"], "title": "Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention", "categories": ["cs.LG", "cs.AI"], "comment": "ef:2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC 2025), IEEE Intelligent Transportation Systems Society (IEEE ITSS), Nov 2025, Gold Coast, Australia, Australia", "summary": "Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.", "AI": {"tldr": "本文提出了一种基于速度特征和道路拓扑的全网络交通流量预测模型HDA-STGNN。", "motivation": "现有的交通流量估计方法要么只针对配备传感器的道路进行预测，要么需要在推断时依赖于实际交通量数据。这些方法不能很好地适用于传感器稀少的城市环境。", "method": "作者设计了一种结合速度特征、静态道路属性和道路网络拓扑的深度学习框架HDA-STGNN，用于全网络交通流量估计。", "result": "通过广泛的消融研究显示该模型能够捕捉复杂的时空依赖关系，并且不需要在推断时依赖于实际交通量数据即可实现准确的预测。", "conclusion": "提出的HDA-STGNN模型可以在没有实际交通量数据的情况下，利用速度特征和道路拓扑信息进行有效的全网络交通流量估计。"}}
{"id": "2512.13757", "pdf": "https://arxiv.org/pdf/2512.13757", "abs": "https://arxiv.org/abs/2512.13757", "authors": ["Neevkumar Manavar", "Hanno Gerd Meyer", "Joachim Waßmuth", "Barbara Hammer", "Axel Schneider"], "title": "Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.", "AI": {"tldr": "提出了一种基于生成模型改进深度预测的接触压力分布物理一致性的框架。", "motivation": "现有方法在预测压力图时，物理一致性差，限制了临床应用可靠性。为了解决这一问题，本文旨在提高接触压力分布的物理一致性。", "method": "通过引入信息隐空间（ILS）和权重优化损失（WOL），结合扩散模型中的条件布朗桥扩散模型（BBDM）及其潜在对应物潜布朗桥扩散模型（LBBDM），以生成高保真度、物理一致的压力估计。", "result": "实验表明，所提出的方法在物理一致性和性能上优于基线方法：BBDM与ILS结合可以提供高度详细的地图，但计算成本和推断时间较高；而LBBDM则提供了更快的推理速度，并且具有竞争性的性能表现。", "conclusion": "该方法支持非侵入性、基于视觉的真实时患者监控，在临床环境中应用广泛。"}}
{"id": "2512.13753", "pdf": "https://arxiv.org/pdf/2512.13753", "abs": "https://arxiv.org/abs/2512.13753", "authors": ["Mika Sipilä", "Sabrina Maggio", "Sandra De Iaco", "Klaus Nordhausen", "Monica Palma", "Sara Taskinen"], "title": "Time-aware UNet and super-resolution deep residual networks for spatial downscaling", "categories": ["cs.CV", "cs.LG", "eess.IV", "stat.ML"], "comment": null, "summary": "Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.", "AI": {"tldr": "本文研究了用于大气污染物空间降尺度的时间感知UNet和超分辨率深度残差网络方法", "motivation": "卫星数据通常以粗略的空间分辨率提供，这限制了其在局部环境分析和决策中的应用。为了改善这一状况，该论文提出了一种时间感知的方法来提高空间降尺度的性能。", "method": "研究中采用了两种广泛使用的深度学习架构：超分辨率深度残差网络（SRDRN）和基于编码器-解码器的UNet，并为这两种方法添加了轻量级的时间模块，这些模块使用正弦或径向基函数（RBF）编码观测时间并将时间特征与空间表示融合。", "result": "实验结果表明，尽管计算复杂度略有增加，但所提出的时间感知扩展显著提高了降尺度性能和收敛速度。", "conclusion": "论文通过案例研究验证了时间感知方法的有效性，并指出该方法在保持较低的计算成本的同时可以提升数据的空间分辨率。"}}
{"id": "2512.13752", "pdf": "https://arxiv.org/pdf/2512.13752", "abs": "https://arxiv.org/abs/2512.13752", "authors": ["Jie Qin", "Jiancheng Huang", "Limeng Qiao", "Lin Ma"], "title": "STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 7 figures", "summary": "Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.", "AI": {"tldr": "STAR是一种用于统一多模态学习的方案，通过分解成理解、生成和编辑三个阶段来提高模型性能。", "motivation": "为了克服多模态理解和生成中的优化冲突及性能权衡问题，同时增强生成能力并保持现有理解功能。", "method": "采用堆叠自回归架构，在冻结基础自回归模型参数的同时逐层叠加同构的AR模块；引入高容量VQ以提高图像表示的颗粒度，并使用隐式推理机制改善复杂条件下的生成质量。", "result": "STAR在GenEval、DPG-Bench和ImgEdit等基准测试中表现最佳，证明了其在统一多模态学习方面的有效性。", "conclusion": "通过分解任务并逐层扩展模型能力，避免了跨任务干扰，实现了高效的统一多模态学习。"}}
{"id": "2512.13751", "pdf": "https://arxiv.org/pdf/2512.13751", "abs": "https://arxiv.org/abs/2512.13751", "authors": ["Taero Kim", "Hoyoon Byun", "Youngjun Choi", "Sungrae Park", "Kyungwoo Song"], "title": "MIDUS: Memory-Infused Depth Up-Scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.", "AI": {"tldr": "本文提出了Memory-Infused Depth Up-Scaling（MIDUS），通过在重复的层中使用头级内存替换传统的前向网络，以提高深度扩展的效果。", "motivation": "当前深度扩展策略依赖于前馈网络，这限制了效率和可达到的收益。论文指出注意力头部有不同的作用，并提出了一种新的设计来利用这一点。", "method": "MIDUS通过在重复层中使用头级内存（HML）替换传统的前向网络层，每个头部分配一个独立的记忆库，允许逐个头部检索并在后续层注入信息。", "result": "实验结果表明，与强大的DUS基线相比，MIDUS表现出稳健的性能改进，并保持高效的参数足迹。", "conclusion": "论文建立了MIDUS作为一种比传统的前向网络复制更有效率和资源效率的选择，用于深度扩展。"}}
{"id": "2512.13750", "pdf": "https://arxiv.org/pdf/2512.13750", "abs": "https://arxiv.org/abs/2512.13750", "authors": ["Ezieddin Elmahjub"], "title": "The algorithmic muse and the public domain: Why copyrights legal philosophy precludes protection for generative AI outputs", "categories": ["cs.CY", "cs.AI"], "comment": "9 pages, two figures", "summary": "Generative AI (GenAI) outputs are not copyrightable. This article argues why. We bypass conventional doctrinal analysis that focuses on black letter law notions of originality and authorship to re-evaluate copyright's foundational philosophy. GenAI fundamentally severs the direct human creative link to expressive form. Traditional theories utilitarian incentive, labor desert and personality fail to provide coherent justification for protection. The public domain constitutes the default baseline for intellectual creations. Those seeking copyright coverage for GenAI outputs bear the burden of proof. Granting copyright to raw GenAI outputs would not only be philosophically unsound but would also trigger an unprecedented enclosure of the digital commons, creating a legal quagmire and stifling future innovation. The paper advocates for a clear distinction: human creative contributions to AI-generated works may warrant protection, but the raw algorithmic output should remain in the public domain.", "AI": {"tldr": "探讨为什么生成式AI（GenAI）的输出不受版权保护。", "motivation": "通过重新评估版权的基本哲学，论证为何不应为GenAI的原始输出提供版权保护。", "method": "绕过传统的法律分析方法，转而从基础理论角度审视人类与机器创作之间的联系。", "result": "指出传统版权理论无法合理地为GenAI产出提供保护理由，强调公共领域是默认基线。", "conclusion": "建议明确区分人类对AI生成作品的创造性贡献和算法原始输出，并认为后者应保留在公共领域内。"}}
{"id": "2512.13749", "pdf": "https://arxiv.org/pdf/2512.13749", "abs": "https://arxiv.org/abs/2512.13749", "authors": ["Joyjit Roy", "Samaresh Kumar Singh"], "title": "Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, 2 figures. Submitted to IEEE IATMSI-2026 (Track: AI, IoT and Computer Vision Enabled Technologies)", "summary": "Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.", "AI": {"tldr": "对金融新闻情感分析中嵌入表示方法进行比较评估，探讨资源受限环境下的模型表现。", "motivation": "在小数据集环境下，标准自然语言处理技术面临挑战。研究旨在通过对比不同嵌入表示方法，探究其对于金融新闻情感分类的效果及限制。", "method": "使用Word2Vec、GloVe和句子转换器三种嵌入表示方式与梯度提升算法相结合进行情感分类，并在人工标注的标题上测试模型表现。", "result": "实验表明，虽然验证集指标强，但模型实际效果不及随机基线。当数据量不足时，预训练嵌入的效果会减弱，且小验证集容易导致过拟合。", "conclusion": "研究结果指出，在数据稀缺的情况下，仅靠高质量的嵌入不足以解决问题，建议采用少量样本学习、数据增强或词典增强等替代方法。"}}
{"id": "2512.13747", "pdf": "https://arxiv.org/pdf/2512.13747", "abs": "https://arxiv.org/abs/2512.13747", "authors": ["Siyuan Dai", "Lunxiao Li", "Kun Zhao", "Eardi Lila", "Paul K. Crane", "Heng Huang", "Dongkuan Xu", "Haoteng Tang", "Liang Zhan"], "title": "Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICDM 2025 the Workshop on Synergy of AI and Multimodal Biomedical Data Mining", "summary": "With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.", "AI": {"tldr": "研究探讨了在医学决策任务中，多模态大型语言模型的表现不如纯文本推理。", "motivation": "尽管在视觉-语言任务上表现出色，但在生物医学领域，最先进的多模态大语言模型却难以处理基本的医疗决策任务。本文旨在探究这一局限性，并提出改善策略。", "method": "通过两个具有挑战性的数据集进行研究：阿尔茨海默病三阶段分类和MIMIC-CXR胸部X光片分类。探讨了三种策略来提高多模态推理的表现：上下文学习、视觉描述后文本推理及少样本微调。", "result": "实验表明，仅基于文本的推理在大多数情况下优于仅基于视觉或联合使用的模型表现。多模态输入往往不如单独使用文本有效。", "conclusion": "当前的多模态大型语言模型缺乏对视觉信息的理解能力。研究指出了一些改善医疗领域中多模态决策方法的方向。"}}
{"id": "2512.13745", "pdf": "https://arxiv.org/pdf/2512.13745", "abs": "https://arxiv.org/abs/2512.13745", "authors": ["Xiuying Zhang", "Qinsheng Zhu", "Xiaodong Xing"], "title": "A Spatio-Temporal Hybrid Quantum-Classical Graph Convolutional Neural Network Approach for Urban Taxi Destination Prediction", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "We propose a Hybrid Spatio-Temporal Quantum Graph Convolutional Network (H-STQGCN) algorithm by combining the strengths of quantum computing and classical deep learning to predict the taxi destination within urban road networks. Our algorithm consists of two branches: spatial processing and time evolution. Regarding the spatial processing, the classical module encodes the local topological features of the road network based on the GCN method, and the quantum module is designed to map graph features onto parameterized quantum circuits through a differentiable pooling layer. The time evolution is solved by integrating multi-source contextual information and capturing dynamic trip dependencies on the classical TCN theory. Finally, our experimental results demonstrate that the proposed algorithm outperforms the current methods in terms of prediction accuracy and stability, validating the unique advantages of the quantum-enhanced mechanism in capturing high-dimensional spatial dependencies.", "AI": {"tldr": "提出了一种结合量子计算和经典深度学习的城市出租车目的地预测算法", "motivation": "利用量子计算与经典的深度学习方法的结合，以提高城市路网中出租车目的地预测的准确性和稳定性", "method": "设计了一个包含空间处理和时间演化两个分支的混合时空量子图卷积网络模型。空间处理部分使用经典的GCN模块编码道路网络的局部拓扑特征，并用量子模块将图形特性映射到参数化量子电路。时间演化则通过多源上下文信息整合和捕捉动态旅程依赖来实现", "result": "实验结果表明，所提算法在预测准确性和稳定性方面优于现有方法", "conclusion": "验证了量子增强机制在捕获高维空间依赖中的独特优势"}}
{"id": "2512.13744", "pdf": "https://arxiv.org/pdf/2512.13744", "abs": "https://arxiv.org/abs/2512.13744", "authors": ["Udayon Sen", "Alka Luqman", "Anupam Chattopadhyay"], "title": "Toward Noise-Aware Audio Deepfake Detection: Survey, SNR-Benchmarks, and Practical Recipes", "categories": ["cs.SD", "cs.AI"], "comment": "6 pages", "summary": "Deepfake audio detection has progressed rapidly with strong pre-trained encoders (e.g., WavLM, Wav2Vec2, MMS). However, performance in realistic capture conditions - background noise (domestic/office/transport), room reverberation, and consumer channels - often lags clean-lab results. We survey and evaluate robustness for state-of-the-art audio deepfake detection models and present a reproducible framework that mixes MS-SNSD noises with ASVspoof 2021 DF utterances to evaluate under controlled signal-to-noise ratios (SNRs). SNR is a measured proxy for noise severity used widely in speech; it lets us sweep from near-clean (35 dB) to very noisy (-5 dB) to quantify graceful degradation. We study multi-condition training and fixed-SNR testing for pretrained encoders (WavLM, Wav2Vec2, MMS), reporting accuracy, ROC-AUC, and EER on binary and four-class (authenticity x corruption) tasks. In our experiments, finetuning reduces EER by 10-15 percentage points at 10-0 dB SNR across backbones.", "AI": {"tldr": "该论文研究了如何在嘈杂环境下改进音频深度伪造检测模型，通过引入信噪比（SNR）基准和多条件训练框架来提高模型的鲁棒性。", "motivation": "现有的音频深度伪造检测模型在实验室条件下表现良好，但在实际背景噪声、混响环境中的性能较差。因此研究如何提升这些模型在真实场景下的稳健性具有重要意义。", "method": "论文构建了一个可重复的评估框架，通过将MS-SNSD噪音与ASVspoof 2021 DF话语混合，在不同信噪比条件下进行多条件训练和固定SNR测试。使用WavLM、Wav2Vec2等预训练编码器，并报告了二元分类任务下的准确率、ROC-AUC值以及EER。", "result": "实验表明，通过微调可以降低10-15个百分点的EER误差，在信噪比为10至0分贝之间的情况下，不同模型框架均能获得改善效果。", "conclusion": "提出的方法提高了音频深度伪造检测在现实噪声环境中的性能，并提供了一套实用的训练和测试策略来增强现有模型的鲁棒性。"}}
{"id": "2512.13742", "pdf": "https://arxiv.org/pdf/2512.13742", "abs": "https://arxiv.org/abs/2512.13742", "authors": ["Md. Najib Hasan", "Imran Ahmad", "Sourav Basak Shuvo", "Md. Mahadi Hasan Ankon", "Sunanda Das", "Nazmul Siddique", "Hui Wang"], "title": "DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.", "AI": {"tldr": "提出了一种结合深度学习和大型语言模型的框架，用于医学图像分类与结构化临床推理。", "motivation": "医学图像分类器在检测消化道疾病方面表现出色但缺乏解释能力；而大型语言模型能生成临床文本却难以进行视觉推理。现有技术无法满足医生期望的诊断流程。", "method": "设计了一种专门针对内窥镜图像的新颖混合模型MobileCoAtNet，用于实现高准确率分类。利用该模型输出驱动多个大规模语言模型进行医学推理，并通过专家验证基准评估这些模型的表现。", "result": "尽管强分类可以提高解释的质量，但当前的大型语言模型仍不稳定且无法达到人类水平的一致性。", "conclusion": "结合深度学习与大型语言模型可生成有用的临床叙述，然而对于高风险医疗决策而言，现有技术尚显不足。框架揭示了这些工具的局限，并指明构建更安全推理系统的方向。"}}
{"id": "2512.13741", "pdf": "https://arxiv.org/pdf/2512.13741", "abs": "https://arxiv.org/abs/2512.13741", "authors": ["Md. Hasib Ur Rahman"], "title": "The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial \"jailbreaking\" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy \"reflex-based\" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.", "AI": {"tldr": "提出了一种通过语义湍流检测大型语言模型（LLMs）的越狱攻击的方法。", "motivation": "当前防御策略依赖于计算成本高或脆弱的过滤器，忽略了模型内部推理过程的动力学特性。因此，需要一种新的方法来识别和防止LLM中的安全漏洞。", "method": "提出了“层流假设”，认为良性输入会产生平滑的、渐进的状态转换，而对抗性提示会触发语义湍流，这是由于内部的安全对齐与指令跟随目标之间的冲突所导致。通过计算每一层余弦速度方差来量化这种现象。", "result": "实验表明，RLHF校准的Qwen2-1.5B在受到攻击时表现出显著增加的湍流（75.4%），而Gemma-2B则显示了降低的湍流（22.0%）。这验证了语义湍流作为轻量级实时检测器的有效性，以及它作为一种非侵入性的诊断工具来识别黑盒模型的安全架构。", "conclusion": "该研究通过引入层流假设和语义湍流的概念，提供了一种新的方法来识别大型语言模型中的越狱攻击，并展示了其在实际应用中的潜在价值。"}}
{"id": "2512.13739", "pdf": "https://arxiv.org/pdf/2512.13739", "abs": "https://arxiv.org/abs/2512.13739", "authors": ["Yajie Yang", "Yuqing Zhao", "Xiaochao Xi", "Yinan Zhu"], "title": "Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI-AISI 2026", "summary": "Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque \"black boxes,\" hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).", "AI": {"tldr": "研究探讨了AIGC辅助新闻特别报道图像生产的可控制性路径，通过两个实验提出了人机协作机制。", "motivation": "针对AIGC工具在新闻中引发的争议和挑战，特别是信息真实性和语义一致性等问题，探索可控的图像生产方法并建立人机协作机制以提高内容准确性和信任度。", "method": "进行了两项实验：一是跨平台适应性测试；二是构建了结合高精度分割、语义对齐和风格调节的人机协作模块化管道，通过CLIP基线评分等手段确保编辑忠实度。", "result": "实验揭示了不同平台在语义一致性、文化特性和视觉真实感方面的差异，并提出了评价指标体系（CIS, CEA, U-PA）以优化图像生产过程。", "conclusion": "该研究提出的人机协作机制为AIGC辅助新闻特别报道的图像生产提供了一种新的解决方案，强调了跨平台适应性、文化准确性及用户公共适当性的评估。"}}
{"id": "2512.13737", "pdf": "https://arxiv.org/pdf/2512.13737", "abs": "https://arxiv.org/abs/2512.13737", "authors": ["Nardine Osman", "Manel Rodriguez-Soto", "Jordi Sabater-Mir"], "title": "Instilling Organisational Values in Firefighters through Simulation-Based Training", "categories": ["cs.CY", "cs.AI"], "comment": ":68T01ACM Class:I.2.0", "summary": "In firefighting and other emergency operations, decisions made under pressure carry profound ethical weight and can significantly impact incident outcomes and firefighter safety. Traditional training methods, while foundational, often fall short in adequately preparing firefighters for the complex ethical dilemmas and value conflicts inherent in chaotic emergency environments. This paper proposes a conceptual framework for enhancing firefighter training by systematically integrating departmental values into simulation-based training. This approach fosters deeper value internalisation and improves value-driven decision-making under pressure. Furthermore, the underlying tools can also be leveraged to evaluate and refine departmental operational protocols for better alignment with preferred values.", "AI": {"tldr": "本文提出了一种将部门价值系统地融入基于模拟的培训的概念框架，以增强消防员在高压环境中的伦理决策能力。", "motivation": "传统训练方法难以充分准备消防员应对复杂道德困境和混乱环境中价值观冲突的问题。", "method": "通过概念框架，将部门价值系统性地整合进基于模拟的培训中，从而提升价值内化及压力下的决策质量。", "result": "这种方法可以提高消防员的价值驱动型决策能力，并有助于评估和完善操作规程以更好地与偏好值保持一致。", "conclusion": "提出的策略不仅提高了个体在紧急情况下的伦理决策水平，还能通过工具改进组织的运营规范。"}}
{"id": "2512.13736", "pdf": "https://arxiv.org/pdf/2512.13736", "abs": "https://arxiv.org/abs/2512.13736", "authors": ["Li-Xuan Zhao", "Chen-Yang Xu", "Wen-Qiang Li", "Bo Wang", "Rong-Xing Wei", "Qing-Hao Menga"], "title": "TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.", "AI": {"tldr": "提出了一种用于抑郁症检测的自我监督学习模型TF-MCL，该模型通过时间频率融合和多域交叉损失函数优化，提高了EEG信号的时间频率分布表示能力。", "motivation": "传统的监督学习方法在标记抑郁症时存在挑战。为了克服这一问题并提高自监督学习方法的能力，研究者开发了一种新的时间频率融合与多域交叉损失的模型来提升检测准确性。", "method": "该论文提出了一种名为TF-MCL的新框架，通过使用一个混合映射头（FMH）生成时间-频率混合表示，并优化一个多域交叉损失函数来改进表征能力。这种方法能够更好地捕捉EEG信号的时间-频率分布特征。", "result": "在公开数据集MODMA和PRED+CT上的实验表明，该模型提高了检测准确性，在两个数据集中分别超过了现有最佳方法5.87%和9.96%。", "conclusion": "TF-MCL为抑郁症的自我监督检测提供了有效的解决方案，并展示了在提高时间频率信息表示能力方面的优越性。"}}
{"id": "2512.13735", "pdf": "https://arxiv.org/pdf/2512.13735", "abs": "https://arxiv.org/abs/2512.13735", "authors": ["Xuechun Liu", "Heli Sun", "Xuecheng Wu", "Ruichen Cao", "Yunyun Shi", "Dingkang Yang", "Haoran Li"], "title": "DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.", "AI": {"tldr": "提出了一种针对高维多变量时间序列异常检测的框架DARTs，该框架通过双路径和软融合机制有效捕捉长程时空依赖关系。", "motivation": "现有方法在低维度场景下表现良好，但在处理高维度、噪声大的时间序列时难以有效地捕捉长程时空依赖性。", "method": "设计了一种基于双路径的模型DARTs，包括短路径中的多视图稀疏图学习器和扩散多关系图形单元，以及长路径中的多尺度时空图构造器，并引入窗口感知的软融合机制来过滤噪声。", "result": "通过多个主流数据集的实验表明了所提方法在异常检测上的优越性和鲁棒性。进行了消融研究以探索关键设计因素的有效性。", "conclusion": "DARTs框架能够有效地识别高维多变量时间序列中的复杂异常模式，展示了其在实际应用中的潜力和价值。"}}
{"id": "2512.13734", "pdf": "https://arxiv.org/pdf/2512.13734", "abs": "https://arxiv.org/abs/2512.13734", "authors": ["Haochen Yuan", "Yang Zhang", "Xiang He", "Quan Z. Sheng", "Zhongjie Wang"], "title": "Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted for publication at AAAI 2026", "summary": "With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.", "AI": {"tldr": "本文提出了一种基于参数高效微调的联邦推荐训练框架，旨在减少嵌入参数传输量。", "motivation": "现有的联邦推荐模型主要集中在提高效率上，但忽略了嵌入参数带来的通信开销问题。为此，我们提出了一个可以降低通信成本并提升准确性的联邦推荐框架。", "method": "本文提出了一种基于参数高效微调的联邦推荐训练方法，包括LoRA、哈希编码和残差量化变分自编码器等技术，并通过实验证明了该方法的有效性。", "result": "实验结果表明，所提出的框架在降低通信开销的同时提高了模型精度。", "conclusion": "本文提出了一种适用于联邦推荐场景的参数高效微调策略，它能够显著减少嵌入参数传输量并改善模型性能。"}}
{"id": "2512.13733", "pdf": "https://arxiv.org/pdf/2512.13733", "abs": "https://arxiv.org/abs/2512.13733", "authors": ["Sidhant Sundrani", "Francesco Tudisco", "Pasquale Minervini"], "title": "Low-Rank Compression of Language Models via Differentiable Rank Selection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.", "AI": {"tldr": "本文提出了一种新的方法LLRC，用于语言模型的低秩压缩。", "motivation": "现有压缩方法要么基于启发式选择分解秩而效果不佳，要么虽然可以学习但需要额外的微调才能达到性能。", "method": "该研究提出了一个直接通过训练掩码权重来选择奇异值的方法LLRC，在不需后处理微调的情况下最小化中间激活与原模型的差异。", "result": "相比其他不依赖于后压缩微调的技术，LLRC在各种压缩率下均表现出色。例如，在Llama-2-13B上进行20%压缩时，其表现优于STRS方法。", "conclusion": "研究证明了所提方法的有效性，并展示了它如何克服当前语言模型压缩中的局限性。"}}
{"id": "2512.13732", "pdf": "https://arxiv.org/pdf/2512.13732", "abs": "https://arxiv.org/abs/2512.13732", "authors": ["Weijie Yang", "Xun Zhang"], "title": "PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\\%$--$88.73\\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.", "AI": {"tldr": "提出了一种基于集条件扩散框架的物理逆解算器PIS，用于从任意稀疏观测中进行物理参数估计。", "motivation": "现有深度学习和操作学习模型在处理不规则、稀疏观察时表现不佳，特别是在固定网格假设失效的情况下。因此需要一种新的方法来解决这些问题，并提供不确定性量化（UQ）。", "method": "PIS使用基于集转换器的编码器来处理任意数量或几何形状的测量值，并采用余弦衰减稀疏课程来增强鲁棒性。", "result": "在三个具有挑战性的逆问题上，包括达西流、波场逆向（亥姆霍兹）和结构健康监测（胡克定律），PIS显示出稳定的性能，在所有任务和稀疏条件下均能减少误差12.28％-88.73％。", "conclusion": "结果表明，PIS是一种强大且通用的解决方案，能够从任意稀疏观测中进行物理参数估计，并具有独特的鲁棒性。"}}
{"id": "2512.13731", "pdf": "https://arxiv.org/pdf/2512.13731", "abs": "https://arxiv.org/abs/2512.13731", "authors": ["Weikang Bai", "Yongkun Du", "Yuchen Su", "Yazhen Xie", "Zhineng Chen"], "title": "Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.", "AI": {"tldr": "本文提出了针对复杂数学表达式识别的新基准、大规模数据集和模型。", "motivation": "现有的数学表达式识别方法在处理简单表达式上表现良好，但在面对复杂的多行或多令牌表达式时性能下降明显。主要原因是现有公开训练数据集中简单的样本占主导地位。", "method": "本文提出了新的基准CMER-Bench、大规模数据集MER-17M和CMER-3M，以及一种新的分词器和结构化数学语言表示方法，并基于此构建了名为CMERNet的模型。", "result": "实验结果表明，所提出的CMERNet在CMER-Bench上显著优于现有的数学表达式识别模型和大型多模态语言模型（MLLMs）。", "conclusion": "通过引入新的基准、大规模数据集和先进的建模方法，本文成功提高了复杂数学表达式的识别精度。"}}
{"id": "2512.13730", "pdf": "https://arxiv.org/pdf/2512.13730", "abs": "https://arxiv.org/abs/2512.13730", "authors": ["Wang Jiaqi", "Lan Yi", "Chen Xiang"], "title": "Exploring the Modular Integration of \"AI + Architecture\" Pedagogy in Undergraduate Design Education: A Case Study of Architectural Design III/IV Courses at Zhejiang University", "categories": ["cs.CY", "cs.AI"], "comment": "in Chinese language, AI for architectural design education", "summary": "This study investigates AI integration in architectural education through a teaching experiment in Zhejiang University's 2024-25 grade three undergraduate design studio. Adopting a dual-module framework (20-hour AI training + embedded ethics discussions), the course introduced deep learning models, LLMs, AIGC, LoRA, and ComfyUI while maintaining the original curriculum structure, supported by dedicated technical instructors. Findings demonstrate the effectiveness of phased guidance, balanced technical-ethical approaches, and institutional support. The model improved students' digital skills and strategic cognition while addressing AI ethics, providing a replicable approach combining technical and critical learning in design education.", "AI": {"tldr": "研究探讨了在浙江大学三年级建筑设计课程中将AI整合到建筑教育中的方法，通过双模块框架（20小时的AI培训和嵌入式的伦理讨论）来增强学生的技术技能和批判性思维。", "motivation": "动机在于探索如何有效结合AI技术和伦理教育于建筑设计课程中，以提高学生的数字能力和战略认知，并解决AI伦理问题。", "method": "采用双模块框架（20小时的AI培训加上嵌入式的伦理讨论），在保留原有课程结构的同时引入深度学习模型、LLM、AIGC、LoRA和ComfyUI等技术。整个过程中有专职的技术指导教师提供支持。", "result": "结果表明分阶段引导，平衡技术与伦理的方法以及机构的支持有效提升了学生的数字技能和战略认知，并解决了AI伦理问题，为结合技术和批判性学习的设计教育提供了可复制的模型。", "conclusion": "该研究提出了一种将AI整合到建筑设计课程中的有效方法，强调了技术培训与伦理讨论相结合的重要性。"}}
{"id": "2512.13729", "pdf": "https://arxiv.org/pdf/2512.13729", "abs": "https://arxiv.org/abs/2512.13729", "authors": ["Jacob Schnell", "Aditya Makkar", "Gunadi Gani", "Aniket Srinivasan Ashok", "Darren Lo", "Mike Optis", "Alexander Wong", "Yuhao Chen"], "title": "Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\\times$ less than classical methods.", "AI": {"tldr": "本文提出了复合无分类器指导（CCFG）方法，用于改进扩散模型在多模态条件下的风场超分辨率重建。", "motivation": "高精度的高分辨率风数据对于天气建模非常重要，但获取此类数据的成本高昂。深度学习方法可以解决这个问题，但在处理多种输入通道时存在挑战。", "method": "本文提出了一种新的复合无分类器指导（CCFG）技术，适用于多模态条件下的扩散模型，并展示了其在工业规模的风动力重建中的应用。", "result": "所提出的WindDM模型实现了深度学习方法中最佳的重建质量，并且成本比传统方法低1000倍以上。", "conclusion": "本文通过复合无分类器指导技术改进了扩散模型，有效解决了高分辨率风数据获取的成本和精度问题。"}}
{"id": "2512.13728", "pdf": "https://arxiv.org/pdf/2512.13728", "abs": "https://arxiv.org/abs/2512.13728", "authors": ["Bhavesh Kumar", "Roger Jin", "Jeffrey Quesnelle"], "title": "CurvaDion: Curvature-Adaptive Distributed Orthonormalization", "categories": ["cs.LG", "cs.AI"], "comment": "Nous Research", "summary": "As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.13726", "pdf": "https://arxiv.org/pdf/2512.13726", "abs": "https://arxiv.org/abs/2512.13726", "authors": ["Sayak Chakrabarty", "Souradip Pal"], "title": "Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 5 figures", "summary": "Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.", "AI": {"tldr": "本文探讨了在用户有限的时间预算下，如何使用强化学习算法优化推荐系统。", "motivation": "传统推荐任务不考虑用户的有限时间预算，导致高相关性商品可能因评估成本过高而未被点击。为解决这一问题，论文通过引入强化学习策略来同时学习用户偏好和时间预算，并在电商情境中进行了实验验证。", "method": "本文提出了一个基于马尔科夫决策过程的时间约束推荐统一模型，并构建了模拟框架以研究策略行为，使用阿里巴巴个性化再排序数据集进行实验。", "result": "实验表明，在严格的时间限制下，强化学习中的策略控制方法比传统的上下文多臂老虎机方法更有效。", "conclusion": "本文通过强化学习算法提出了一个统一的解决方案来优化时间约束下的推荐系统，并展示了该方案在电商环境中的应用效果。"}}
{"id": "2512.13725", "pdf": "https://arxiv.org/pdf/2512.13725", "abs": "https://arxiv.org/abs/2512.13725", "authors": ["Steve Nwaiwu", "Nipat Jongsawat", "Anucha Tungkasthan"], "title": "Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy", "categories": ["cs.AI"], "comment": null, "summary": "Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.", "AI": {"tldr": "本文研究了不同精度量化对大型语言模型因果推理的影响，并评估了图检索增强生成方法的效果。", "motivation": "为了理解在边缘和资源受限环境中部署的量化模型（如INT8和NF4）对正式因果推理的影响，特别是在关联、干预和反事实推断方面的性能变化。", "method": "使用3000个样本的CLadder基准测试不同精度下Llama 3 8B模型的表现，并通过CRASS基准进一步评估。此外还引入了图检索增强生成方法来提高干预推理的准确性。", "result": "在NF4量化中，整体准确度下降不到1%；干预查询对精度损失更敏感，而反事实推断相对稳定但存在特定类型的弱点。CRASS基准测试显示，在不同精度下性能几乎相同，表明现有数据集缺乏揭示量化引起的推理漂移的结构敏感性。", "conclusion": "研究表明因果推理在四比特量化中具有意外的鲁棒性；图结构增强可以提高干预推理准确性；当前反事实基准无法捕获更深层面的因果脆弱性。这为压缩环境中的因果推理提供了初步的实证指导和实用建议。"}}
{"id": "2512.13724", "pdf": "https://arxiv.org/pdf/2512.13724", "abs": "https://arxiv.org/abs/2512.13724", "authors": ["Ayush Noori", "Joaquín Polonuer", "Katharina Meyer", "Bogdan Budnik", "Shad Morton", "Xinyuan Wang", "Sumaiya Nazeen", "Yingnan He", "Iñaki Arango", "Lucas Vittor", "Matthew Woodworth", "Richard C. Krolewski", "Michelle M. Li", "Ninning Liu", "Tushar Kamath", "Evan Macosko", "Dylan Ritter", "Jalwa Afroz", "Alexander B. H. Henderson", "Lorenz Studer", "Samuel G. Rodriques", "Andrew White", "Noa Dagan", "David A. Clifton", "George M. Church", "et al. (4 additional authors not shown)"], "title": "Graph AI generates neurological hypotheses validated in molecular, organoid, and clinical systems", "categories": ["q-bio.QM", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Neurological diseases are the leading global cause of disability, yet most lack disease-modifying treatments. We present PROTON, a heterogeneous graph transformer that generates testable hypotheses across molecular, organoid, and clinical systems. To evaluate PROTON, we apply it to Parkinson's disease (PD), bipolar disorder (BD), and Alzheimer's disease (AD). In PD, PROTON linked genetic risk loci to genes essential for dopaminergic neuron survival and predicted pesticides toxic to patient-derived neurons, including the insecticide endosulfan, which ranked within the top 1.29% of predictions. In silico screens performed by PROTON reproduced six genome-wide $α$-synuclein experiments, including a split-ubiquitin yeast two-hybrid system (normalized enrichment score [NES] = 2.30, FDR-adjusted $p < 1 \\times 10^{-4}$), an ascorbate peroxidase proximity labeling assay (NES = 2.16, FDR $< 1 \\times 10^{-4}$), and a high-depth targeted exome sequencing study in 496 synucleinopathy patients (NES = 2.13, FDR $< 1 \\times 10^{-4}$). In BD, PROTON predicted calcitriol as a candidate drug that reversed proteomic alterations observed in cortical organoids derived from BD patients. In AD, we evaluated PROTON predictions in health records from $n = 610,524$ patients at Mass General Brigham, confirming that five PROTON-predicted drugs were associated with reduced seven-year dementia risk (minimum hazard ratio = 0.63, 95% CI: 0.53-0.75, $p < 1 \\times 10^{-7}$). PROTON generated neurological hypotheses that were evaluated across molecular, organoid, and clinical systems, defining a path for AI-driven discovery in neurological disease.", "AI": {"tldr": "PROTON，一种异构图变换器，用于生成神经疾病跨分子、类器官和临床系统的可验证假设。", "motivation": "大多数神经系统疾病缺乏改变疾病的治疗方法。本文旨在通过AI技术提供新的治疗见解。", "method": "应用名为PROTON的异构图变压器模型来分析帕金森病（PD）、双相情感障碍（BD）和阿尔茨海默病（AD），生成可验证假设并进行实验验证。", "result": "在PD中，PROTON预测了对患者来源神经元有毒的农药，并且其筛选结果与六个全基因组α-突触核蛋白实验一致。在BD中，PROTON预测了一种候选药物——维生素D3前体物质能够逆转双相情感障碍患者的脑组织改变。在AD中，五个由PROTON预测的药物被发现能显著降低患者患痴呆症的风险。", "conclusion": "PROTON生成并验证了关于神经疾病的假设，在分子、类器官和临床系统上均取得成果，为AI驱动的疾病研究开辟了一条新的路径。"}}
{"id": "2512.13723", "pdf": "https://arxiv.org/pdf/2512.13723", "abs": "https://arxiv.org/abs/2512.13723", "authors": ["David Haslett", "Linus Ta-Lun Huang", "Leila Khalatbari", "Janet Hui-wen Hsiao", "Antoni B. Chan"], "title": "Made-in China, Thinking in America:U.S. Values Persist in Chinese LLMs", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "As large language models increasingly mediate access to information and facilitate decision-making, they are becoming instruments in soft power competitions between global actors such as the United States and China. So far, language models seem to be aligned with the values of Western countries, but evidence for this ethical bias comes mostly from models made by American companies. The current crop of state-of-the-art models includes several made in China, so we conducted the first large-scale investigation of how models made in China and the USA align with people from China and the USA. We elicited responses to the Moral Foundations Questionnaire 2.0 and the World Values Survey from ten Chinese models and ten American models, and we compared their responses to responses from thousands of Chinese and American people. We found that all models respond to both surveys more like American people than like Chinese people. This skew toward American values is only slightly mitigated when prompting the models in Chinese or imposing a Chinese persona on the models. These findings have important implications for a near future in which large language models generate much of the content people consume and shape normative influence in geopolitics.", "AI": {"tldr": "研究分析了由中国和美国制造的语言模型在道德价值观上的偏见。", "motivation": "随着大型语言模型在全球信息访问和决策中扮演越来越重要的角色，它们已成为软实力竞争的工具。此前的研究主要关注由美国公司开发的语言模型，本研究旨在探讨中国生产的先进模型是否具有类似的价值观偏差，并对比中美两国人民的观点。", "method": "通过向十个由中国制造和十个由美国制造的语言模型发送道德基础问卷2.0和世界价值观调查问卷，然后将它们的回答与成千上万的中国人和美国人进行比较来研究语言模型中的价值偏见。同时测试了使用中文或为中国角色设计提示时这些模型的变化。", "result": "所有模型在回答两个调查问卷时都更倾向于美国人的观点而不是中国人的观点。即使是在中文环境下或为模型设定了中国身份，这种偏向性也没有显著减少。", "conclusion": "研究结果表明，在一个大型语言模型将产生大部分人们接触的内容并影响规范影响力的世界中，价值偏见可能会影响全球舆论和政治格局。"}}
{"id": "2512.13717", "pdf": "https://arxiv.org/pdf/2512.13717", "abs": "https://arxiv.org/abs/2512.13717", "authors": ["Ekaterina Sysoykova", "Bernhard Anzengruber-Tanase", "Michael Haslgrubler", "Philipp Seidl", "Alois Ferscha"], "title": "Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.", "AI": {"tldr": "提出了一种两级联邦少量学习框架，用于在隐私保护下进行个性化的癫痫发作检测。", "motivation": "为了应对临床实践中EEG数据稀缺、分布于多个机构且受到严格隐私法规限制的问题，开发了适用于实际医疗环境的AI癫痫发作检测模型。", "method": "提出了一个两级联邦少量学习框架。第一阶段使用联邦学习在非独立同分布的模拟医院站点上微调预训练的生物信号变压器（BIOT），实现共享表示学习而不集中EEG记录；第二阶段利用只有五段标记的EEG片段，通过联邦少量个性化适应患者特定模型。", "result": "联邦细化取得了0.43平衡准确率、0.42 Cohen's kappa和0.69加权F1。在FFSL阶段，客户特有的模型达到了平均77％的平衡准确率、62％的Cohen's kappa和73％的加权F1。", "conclusion": "结果显示，联邦少量学习能够在实际数据可用性和隐私保护条件下支持有效的个性化癫痫发作检测"}}
{"id": "2512.13716", "pdf": "https://arxiv.org/pdf/2512.13716", "abs": "https://arxiv.org/abs/2512.13716", "authors": ["Yitong Luo", "Ziang Chen", "Hou Hei Lam", "Jiayu zhan", "Junqi Wang", "Zhenliang Zhang", "Xue Feng"], "title": "ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making", "categories": ["cs.AI"], "comment": "Accepted at LAW Workshop, NeurIPS 2025", "summary": "Personalized decision-making is essential for human-AI interaction, enabling AI agents to act in alignment with individual users' value preferences. As AI systems expand into real-world applications, adapting to personalized values beyond task completion or collective alignment has become a critical challenge. We address this by proposing a value-driven approach to personalized decision-making. Human values serve as stable, transferable signals that support consistent and generalizable behavior across contexts. Compared to task-oriented paradigms driven by external rewards and incentives, value-driven decision-making enhances interpretability and enables agents to act appropriately even in novel scenarios. We introduce ValuePilot, a two-phase framework consisting of a dataset generation toolkit (DGT) and a decision-making module (DMM). DGT constructs diverse, value-annotated scenarios from a human-LLM collaborative pipeline. DMM learns to evaluate actions based on personal value preferences, enabling context-sensitive, individualized decisions. When evaluated on previously unseen scenarios, DMM outperforms strong LLM baselines, including GPT-5, Claude-Sonnet-4, Gemini-2-flash, and Llama-3.1-70b, in aligning with human action choices. Our results demonstrate that value-driven decision-making is an effective and extensible engineering pathway toward building interpretable, personalized AI agents.", "AI": {"tldr": "本文提出了一种两阶段框架ValuePilot，用于价值驱动的决策制定，旨在使AI系统更好地适应个人用户的偏好和行为。", "motivation": "个性化决策对于人机交互至关重要。随着AI系统的普及，如何让其不仅完成任务还能够符合个体的价值观成为挑战。通过价值导向的方法可以提高解释性，并使得代理在新场景中也能做出适当的决定。", "method": "ValuePilot框架包括数据集生成工具包（DGT）和决策模块（DMM）。DGT利用人与大模型的协作流程创建多样化的、注释了价值观的情境。DMM训练以个人的价值偏好评估行动，从而实现上下文敏感和个人定制的决定。", "result": "在未见情境中进行测试时，DMM的表现优于GPT-5等强大基准模型，更接近人类的行为选择。", "conclusion": "价值驱动决策是一个有效且可扩展的工程路径，用于构建解释性好、个性化的AI代理。"}}
{"id": "2512.13715", "pdf": "https://arxiv.org/pdf/2512.13715", "abs": "https://arxiv.org/abs/2512.13715", "authors": ["Fatemeh Lotfi", "Fatemeh Afghah"], "title": "Meta Hierarchical Reinforcement Learning for Scalable Resource Management in O-RAN", "categories": ["cs.AI", "cs.LG", "eess.SY"], "comment": "This paper is submitted to IEEE Open Journal of the Communications Society", "summary": "The increasing complexity of modern applications demands wireless networks capable of real time adaptability and efficient resource management. The Open Radio Access Network (O-RAN) architecture, with its RAN Intelligent Controller (RIC) modules, has emerged as a pivotal solution for dynamic resource management and network slicing. While artificial intelligence (AI) driven methods have shown promise, most approaches struggle to maintain performance under unpredictable and highly dynamic conditions. This paper proposes an adaptive Meta Hierarchical Reinforcement Learning (Meta-HRL) framework, inspired by Model Agnostic Meta Learning (MAML), to jointly optimize resource allocation and network slicing in O-RAN. The framework integrates hierarchical control with meta learning to enable both global and local adaptation: the high-level controller allocates resources across slices, while low level agents perform intra slice scheduling. The adaptive meta-update mechanism weights tasks by temporal difference error variance, improving stability and prioritizing complex network scenarios. Theoretical analysis establishes sublinear convergence and regret guarantees for the two-level learning process. Simulation results demonstrate a 19.8% improvement in network management efficiency compared with baseline RL and meta-RL approaches, along with faster adaptation and higher QoS satisfaction across eMBB, URLLC, and mMTC slices. Additional ablation and scalability studies confirm the method's robustness, achieving up to 40% faster adaptation and consistent fairness, latency, and throughput performance as network scale increases.", "AI": {"tldr": "本文提出了一种基于模型无关元学习的分层强化学习框架，用于优化开放无线接入网络中的资源分配和网络切片。", "motivation": "现代应用复杂度增加需要无线网络具备实时适应性和高效的资源配置能力。现有的人工智能方法在处理不可预测且高度动态条件时表现不佳，本文旨在解决这些问题。", "method": "提出了一种元层次强化学习框架，该框架结合分层控制和元学习以实现全局与局部的自适应调整：高层控制器负责跨切片资源分配，低级代理则执行内部切片调度。通过任务权重按时间差误差方差进行自适应更新机制来改善稳定性和优先处理复杂网络场景。", "result": "实验结果表明，该方法在提升网络管理效率方面优于基线的强化学习和元强化学习方法，提高了19.8%的有效性，并展示了更快的适应性和更高的QoS满意度。同时，在不同规模下的测试中显示出了更强的鲁棒性。", "conclusion": "所提出的框架不仅优化了资源分配策略，而且实现了快速、稳定的性能改进，尤其是在复杂多变的应用场景下，通过理论分析验证了其收敛性和遗憾保证，并且在大规模网络环境中保持了一致的良好性能。"}}
{"id": "2512.13714", "pdf": "https://arxiv.org/pdf/2512.13714", "abs": "https://arxiv.org/abs/2512.13714", "authors": ["Gangesh Pathak", "Prasanna Kumar"], "title": "AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach", "categories": ["cs.AI"], "comment": "16 Pages", "summary": "LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).", "AI": {"tldr": "本文提出了一个基于AI的注释流水线，用于系统地识别、标记和修复大型语言模型输出中的不稳定性模式。", "motivation": "当前方法如强化学习与人类反馈（RLHF）和监督微调虽然提供了一定改进但成本高且难以扩展。因此需要一种更可持续的方法来解决LLM的可靠性问题，特别是在监管严格行业中。", "method": "本文提出的人机协同方法结合了自动弱监督模型、基于置信度标注与目标人类验证，以确保反馈信息的可靠性和道德正当性，并引入语义一致性、事实正确性和逻辑连贯性的稳定性特定注释框架。", "result": "该框架允许通过反馈循环进行持续校准并增强LLM的稳健性。", "conclusion": "这种AI驱动的注释流水线为提高大型语言模型在监管严格行业的可靠性和性能提供了一种有效且可持续的方法。"}}
{"id": "2512.13713", "pdf": "https://arxiv.org/pdf/2512.13713", "abs": "https://arxiv.org/abs/2512.13713", "authors": ["Ali Parsaee", "Yashar Talebirad", "Csongor Szepesvári", "Vishwajeet Ohal", "Eden Redman"], "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "11 pages, 3 figures, submitted to ANTS 2026", "summary": "Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.", "AI": {"tldr": "本文提出了LoopBench，一个评估大型语言模型在分布式系统中对称性打破和元认知思考能力的基准。", "motivation": "虽然大型语言模型作为自主代理被广泛应用，但它们在分布式系统的协同工作中仍缺乏理解。这个研究旨在探索这些模型如何解决对称性问题，并设计策略以避免死锁状态。", "method": "LoopBench通过让模型尝试着色奇数循环图（C3, C5, C11）来测试其能力，这种任务对于不通信的确定性代理而言是不可能完成的任务。引入了一种策略传递机制作为一致性的记忆方式。", "result": "实验表明标准LLM和经典启发式方法无法解决这些问题，而高级推理模型如O3可以设计出打破死锁状态的策略。", "conclusion": "LoopBench提供了研究基于语言基础推理的分布式算法以及集体智能的新途径。"}}
{"id": "2512.13707", "pdf": "https://arxiv.org/pdf/2512.13707", "abs": "https://arxiv.org/abs/2512.13707", "authors": ["Daoyuan Qian", "Qiyao Liang", "Ila Fiete"], "title": "Modular connectivity in neural networks emerges from Poisson noise-motivated regularisation, and promotes robustness and compositional generalisation", "categories": ["physics.bio-ph", "cs.LG", "cs.NE", "stat.ML"], "comment": null, "summary": "Circuits in the brain commonly exhibit modular architectures that factorise complex tasks, resulting in the ability to compositionally generalise and reduce catastrophic forgetting. In contrast, artificial neural networks (ANNs) appear to mix all processing, because modular solutions are difficult to find as they are vanishing subspaces in the space of possible solutions. Here, we draw inspiration from fault-tolerant computation and the Poisson-like firing of real neurons to show that activity-dependent neural noise, combined with nonlinear neural responses, drives the emergence of solutions that reflect an accurate understanding of modular tasks, corresponding to acquisition of a correct world model. We find that noise-driven modularisation can be recapitulated by a deterministic regulariser that multiplicatively combines weights and activations, revealing rich phenomenology not captured in linear networks or by standard regularisation methods. Though the emergence of modular structure requires sufficiently many training samples (exponential in the number of modular task dimensions), we show that pre-modularised ANNs exhibit superior noise-robustness and the ability to generalise and extrapolate well beyond training data, compared to ANNs without such inductive biases. Together, our work demonstrates a regulariser and architectures that could encourage modularity emergence to yield functional benefits.", "AI": {"tldr": "研究通过引入基于泊松噪声的正则化方法，使人工神经网络（ANN）产生模块化结构，并展示这种结构带来的鲁棒性和泛化的功能益处。", "motivation": "大脑中的电路通常表现出分层架构以简化复杂任务，而人工神经网络却很难找到这样的解。研究者通过借鉴容错计算和真实神经元的泊松样放电特性，探索一种基于活动依赖噪声的方法来促进模块化结构的发展。", "method": "利用与非线性神经响应相结合的活动相关神经噪声驱动模体结构出现，并用确定性的正则器（结合权重和激活）模拟该过程。研究发现这种结构在有足够的训练样本的情况下可以很好地泛化到未见过的数据上。", "result": "预模块化的ANN比没有此类归纳偏见的ANN表现出更好的鲁棒性和外推能力，同时揭示了一种由噪声驱动而出现的现象，在线性网络或标准正则方法中并未捕获这些现象。", "conclusion": "通过引入基于泊松噪声的正则器和促进模体化结构的架构，可以使人工神经网络更好地模拟大脑的功能，并为未来的深度学习模型提供新的见解。"}}
{"id": "2512.13705", "pdf": "https://arxiv.org/pdf/2512.13705", "abs": "https://arxiv.org/abs/2512.13705", "authors": ["Siqi Wang", "Zhengyu Chen", "Teng Xiao", "Zheqi Lv", "Jinluan Yang", "Xunliang Cai", "Jingang Wang", "Xiaomeng Li"], "title": "Scaling and Transferability of Annealing Strategies in Large Language Model Training", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to AAAI 2026 (camera-ready version)", "summary": "Learning rate scheduling is crucial for training large language models, yet understanding the optimal annealing strategies across different model configurations remains challenging. In this work, we investigate the transferability of annealing dynamics in large language model training and refine a generalized predictive framework for optimizing annealing strategies under the Warmup-Steady-Decay (WSD) scheduler. Our improved framework incorporates training steps, maximum learning rate, and annealing behavior, enabling more efficient optimization of learning rate schedules. Our work provides a practical guidance for selecting optimal annealing strategies without exhaustive hyperparameter searches, demonstrating that smaller models can serve as reliable proxies for optimizing the training dynamics of larger models. We validate our findings on extensive experiments using both Dense and Mixture-of-Experts (MoE) models, demonstrating that optimal annealing ratios follow consistent patterns and can be transferred across different training configurations.", "AI": {"tldr": "研究大规模语言模型训练中的学习率调整策略的可扩展性和迁移性。", "motivation": "理解不同模型配置下的最佳学习率调节策略至关重要，但当前缺乏有效的方法来优化这些策略。", "method": "提出了一个基于Warmup-Steady-Decay（WSD）调度器的预测框架，该框架结合了训练步数、最大学习率和调整行为，以更高效地优化学习率计划。", "result": "实验结果表明，较小模型可以作为较大模型训练动态的有效代理，并且最优的学习率调整比率在不同配置之间具有可迁移性。", "conclusion": "研究提供了一种实用的指导方法来选择最佳的学习率调节策略，而无需进行繁重的超参数搜索。"}}
{"id": "2512.13704", "pdf": "https://arxiv.org/pdf/2512.13704", "abs": "https://arxiv.org/abs/2512.13704", "authors": ["Doohee You", "Sundeep Paul"], "title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents", "categories": ["cs.AI"], "comment": "12 pages, 3 figures", "summary": "The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a \"Council of Agents,\" a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.", "AI": {"tldr": "本文提出了Adjudicator系统，利用知识图谱和多代理大型语言模型架构来自动识别和纠正训练数据中的标签噪声。", "motivation": "在高风险的工业应用中，嘈杂的数据标签会降低机器学习系统的性能并侵蚀用户信任。本文旨在解决这一挑战，通过提出一种能够自动修复数据标签错误的方法，提升生产环境中机器学习系统的性能。", "method": "Adjudicator系统首先构建了一个动态的知识图谱来统一项目上下文信息，然后利用该知识图谱指导一个由多个大型语言模型组成的‘代理人议会’进行辩论和投票以确定标签的有效性。引入了一种新的超越逻辑使用知识图谱精准识别复杂结构错误。", "result": "Adjudicator系统在AlleNoise基准测试的1000个项目子集上实现了0.99 F1值，显著优于单一大型语言模型（F1 = 0.48）和不带知识图谱的议会架构（F1 = 0.59）。", "conclusion": "Adjudicator系统展示了高精度数据验证能力，提供了生成高质量工业级黄金数据集的概念证明。"}}
{"id": "2512.13703", "pdf": "https://arxiv.org/pdf/2512.13703", "abs": "https://arxiv.org/abs/2512.13703", "authors": ["Fan Yang"], "title": "Safe2Harm: Semantic Isomorphism Attacks for Jailbreaking Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across various tasks, but their security vulnerabilities can be exploited by attackers to generate harmful content, causing adverse impacts across various societal domains. Most existing jailbreak methods revolve around Prompt Engineering or adversarial optimization, yet we identify a previously overlooked phenomenon: many harmful scenarios are highly consistent with legitimate ones in terms of underlying principles. Based on this finding, this paper proposes the Safe2Harm Semantic Isomorphism Attack method, which achieves efficient jailbreaking through four stages: first, rewrite the harmful question into a semantically safe question with similar underlying principles; second, extract the thematic mapping relationship between the two; third, let the LLM generate a detailed response targeting the safe question; finally, reversely rewrite the safe response based on the thematic mapping relationship to obtain harmful output. Experiments on 7 mainstream LLMs and three types of benchmark datasets show that Safe2Harm exhibits strong jailbreaking capability, and its overall performance is superior to existing methods. Additionally, we construct a challenging harmful content evaluation dataset containing 358 samples and evaluate the effectiveness of existing harmful detection methods, which can be deployed for LLM input-output filtering to enable defense.", "AI": {"tldr": "提出了一种通过语义同构攻击对大型语言模型进行越狱的方法。", "motivation": "现有的越狱方法主要集中在提示工程或对抗性优化上，但忽略了有害场景与合法场景在底层原理上的高度一致性。为了提高越狱效率和效果，提出了Safe2Harm方法。", "method": "首先将有害问题改写为语义安全的问题；其次提取两个问题的主题映射关系；然后让大型语言模型生成详细的安全回答；最后根据主题映射关系逆向改写安全回答以获得有害输出。", "result": "实验显示Safe2Harm在7种主流的大型语言模型和三种基准数据集上表现出强大的越狱能力，整体性能优于现有方法。同时构建了一个包含358个样本的有害内容评估数据集，并评估了现有的有害检测方法的有效性。", "conclusion": "Safe2Harm是一种有效的语义同构攻击方法，可以用于大型语言模型的安全研究和防御策略开发。"}}
{"id": "2512.13702", "pdf": "https://arxiv.org/pdf/2512.13702", "abs": "https://arxiv.org/abs/2512.13702", "authors": ["A. Anil Sinaci", "Senan Postaci", "Dogukan Cavdaroglu", "Machteld J. Boonstra", "Okan Mercan", "Kerem Yilmaz", "Gokce B. Laleci Erturkmen", "Folkert W. Asselbergs", "Karim Lekadir"], "title": "Enhancing Transparency and Traceability in Healthcare AI: The AI Product Passport", "categories": ["cs.CY", "cs.AI"], "comment": "A total of 33 pages: First 16 pages for the manuscript and the remaining 17 pages for the supplementary user guide of the graphical user interface", "summary": "Objective: To develop the AI Product Passport, a standards-based framework improving transparency, traceability, and compliance in healthcare AI via lifecycle-based documentation. Materials and Methods: The AI Product Passport was developed within the AI4HF project, focusing on heart failure AI tools. We analyzed regulatory frameworks (EU AI Act, FDA guidelines) and existing standards to design a relational data model capturing metadata across AI lifecycle phases: study definition, dataset preparation, model generation/evaluation, deployment/monitoring, and passport generation. MLOps/ModelOps concepts were integrated for operational relevance. Co-creation involved feedback from AI4HF consortium and a Lisbon workshop with 21 diverse stakeholders, evaluated via Mentimeter polls. The open-source platform was implemented with Python libraries for automated provenance tracking. Results: The AI Product Passport was designed based on existing standards and methods with well-defined lifecycle management and role-based access. Its implementation is a web-based platform with a relational data model supporting auditable documentation. It generates machine- and human-readable reports, customizable for stakeholders. It aligns with FUTURE-AI principles (Fairness, Universality, Traceability, Usability, Robustness, Explainability), ensuring fairness, traceability, and usability. Exported passports detail model purpose, data provenance, performance, and deployment context. GitHub-hosted backend/frontend codebases enhance accessibility. Discussion and Conclusion: The AI Product Passport addresses transparency gaps in healthcare AI, meeting regulatory and ethical demands. Its open-source nature and alignment with standards foster trust and adaptability. Future enhancements include FAIR data principles and FHIR integration for improved interoperability, promoting responsible AI deployment.", "AI": {"tldr": "开发AI产品护照，一个基于标准的框架，提高医疗人工智能的透明度、可追溯性和合规性。", "motivation": "为解决医疗人工智能中的透明度和可追溯性缺口，符合监管要求并促进伦理规范，设计了一个标准化的生命周期文档化框架。", "method": "在AI4HF项目中开发了AI产品护照，并通过分析监管框架和现有标准，结合MLOps/ModelOps理念进行设计与实施。设计过程中融入了利益相关者反馈，最终实现了一个基于Python库的自动化追踪平台。", "result": "AI产品护照的设计基于现有标准，支持全生命周期管理与角色访问控制，提供可审计文档化支持，并生成机器和人类均可读取的报告。该护照还符合FUTURE-AI原则，确保公平性、透明度和可用性。开源代码库托管在GitHub上，增强其可访问性。", "conclusion": "AI产品护照解决了医疗人工智能中的透明度问题，满足了监管和伦理要求，并通过开放式设计与标准化整合，促进信任与适应性。未来改进包括实现FAIR数据原则与FHIR集成，以提高互操作性，推动负责任的AI部署。"}}
{"id": "2512.13701", "pdf": "https://arxiv.org/pdf/2512.13701", "abs": "https://arxiv.org/abs/2512.13701", "authors": ["Zheng Xing", "Junting Chen"], "title": "Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference", "categories": ["cs.AI", "cs.IT"], "comment": null, "summary": "Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.", "AI": {"tldr": "通过室内MIMO-OFDM信道测量，无需位置标签推断用户轨迹以构建盲无线图谱", "motivation": "传统方法需要大量的定位标注数据来构造无线地图，这在许多实际场景中是昂贵且不切实际的。本文提出了一种基于CSI的空间正则化贝叶斯推理框架，在没有位置标签的情况下通过信道测量推断出用户轨迹，以实现盲无线图谱构建", "method": "首先证明了NLOS条件下的CSI具有空间连续性，并根据此建立一个与物理距离成比例的CSI-距离度量。基于这些理论结果，开发了一个空间正则化的贝叶斯推理框架来联合估计信道特性、区分LOS/NLOS情况并恢复用户轨迹", "result": "在射线追踪数据集上的实验验证了所提出的方法的有效性，平均定位误差为0.68米，光束地图重建误差为3.3%", "conclusion": "本文提出了盲无线图谱构建框架，并通过实验证明了其有效性和可行性"}}
{"id": "2512.13700", "pdf": "https://arxiv.org/pdf/2512.13700", "abs": "https://arxiv.org/abs/2512.13700", "authors": ["Mitchell A. Klusty", "Elizabeth C. Solie", "Caroline N. Leach", "W. Vaiden Logan", "Lynnet E. Richey", "John C. Gensel", "David P. Szczykutowicz", "Bryan C. McLellan", "Emily B. Collier", "Samuel E. Armstrong", "V. K. Cody Bumgardner"], "title": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 figures, 2 tables, submitted to AMIA 2026 Informatics Summit", "summary": "Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.", "AI": {"tldr": "利用大型语言模型自动从临床笔记中提取结构化特征，以减少手动图表审核的负担并提高数据采集的一致性。", "motivation": "手动图表审查是耗时且资源密集型的工作，需要专家从不规则的电子健康记录叙述中提取复杂信息。为了减轻这种工作负担和提高数据一致性，提出了一个利用大型语言模型进行结构化特征提取的安全、模块化的框架。", "method": "提出了一种结合检索增强生成方法与结构响应方式的大型语言模型系统，并将其封装成可广泛部署且具有扩展性的容器，以提供多样临床领域的特征提取功能。", "result": "在评估过程中，该框架针对大型患者笔记中的多种医疗特性时，表现出高准确率并与专家注释数据集进行比较后发现了手动审查中遗漏的多个注释错误。", "conclusion": "该框架展示了通过自动化提取减轻手动图表审核负担并加速临床研究潜力。"}}
{"id": "2512.13697", "pdf": "https://arxiv.org/pdf/2512.13697", "abs": "https://arxiv.org/abs/2512.13697", "authors": ["Vivan Doshi", "Mengyuan Li"], "title": "Writing in Symbiosis: Mapping Human Creative Agency in the AI Era", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "Advances in Neural Information Processing Systems (NeurIPS 2025)", "summary": "The proliferation of Large Language Models (LLMs) raises a critical question about what it means to be human when we share an increasingly symbiotic relationship with persuasive and creative machines. This paper examines patterns of human-AI coevolution in creative writing, investigating how human craft and agency are adapting alongside machine capabilities. We challenge the prevailing notion of stylistic homogenization by examining diverse patterns in longitudinal writing data. Using a large-scale corpus spanning the pre- and post-LLM era, we observe patterns suggestive of a \"Dual-Track Evolution\": thematic convergence around AI-related topics, coupled with structured stylistic differentiation. Our analysis reveals three emergent adaptation patterns: authors showing increased similarity to AI style, those exhibiting decreased similarity, and those maintaining stylistic stability while engaging with AI-related themes. This Creative Archetype Map illuminates how authorship is coevolving with AI, contributing to discussions about human-AI collaboration, detection challenges, and the preservation of creative diversity.", "AI": {"tldr": "研究探讨了人类与AI在创意写作中的共生关系，分析了作者风格随机器能力进化而变化的模式。", "motivation": "随着大型语言模型的发展，质疑何以为人的问题日益凸显。论文探索了人类与具有说服力和创造力的机器之间的协同演化，并挑战了人们对于创作同质化的普遍看法。", "method": "利用跨越预LLM时代和后LLM时代的大量语料库，研究观察到了主题趋同和结构化风格差异并存的现象，揭示了三种适应模式：与AI风格相似度增加、减少和保持稳定但探讨相关主题的作者。", "result": "论文发现了一种‘双轨进化’现象，即在人工智能相关的主题上出现汇聚，同时在风格上有明显的分化。", "conclusion": "该研究通过创意原型地图揭示了作者身份与AI之间的共演化关系，并对人类和机器的合作、检测挑战以及创造性多样性保护进行了深入讨论。"}}
{"id": "2512.13696", "pdf": "https://arxiv.org/pdf/2512.13696", "abs": "https://arxiv.org/abs/2512.13696", "authors": ["Md Shahabub Alam", "Md Asifuzzaman Jishan", "Ayan Kumar Ghosh"], "title": "Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset", "categories": ["cs.LG", "cs.CV", "cs.NE"], "comment": null, "summary": "Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.", "AI": {"tldr": "本文提出了一种基于物理学指导的深度学习方法，用于热泵系统的压力检测。", "motivation": "现代节能建筑中，热泵系统是关键组成部分。然而，由于复杂的热力学交互作用和有限的真实世界数据，其操作压力检测面临挑战。", "method": "本研究开发了一种结合物理学指导特征选择和深度神经网络架构的模型，使用当2热（When2Heat）数据集进行热泵压力分类。该方法采用5层隐藏层结构，并应用了双重正则化策略。", "result": "该模型在测试和验证阶段分别达到了78.1%和78.5%的准确率，相比浅层网络、有限特征集合以及单一正则化策略，其性能提升了5.0%，4.0%和2.0%。详细的消融研究表明，物理学指导下的特征选择、变量阈值设置以实现现实类分布的有效性，并对跨国家能量模式进行了分析。", "conclusion": "该系统为热泵压力检测提供了一个生产级别的解决方案，在AMD Ryzen 9 7950X和RTX 4080硬件上训练时间为720秒，参数数量为181,348。"}}
{"id": "2512.13695", "pdf": "https://arxiv.org/pdf/2512.13695", "abs": "https://arxiv.org/abs/2512.13695", "authors": ["Émilie Fabre", "Katie Seaborn", "Adrien Alexandre Verhulst", "Yuta Itoh", "Jun Rekimoto"], "title": "Juicy Text: Onomatopoeia and Semantic Text Effects for Juicy Player Experiences", "categories": ["cs.HC", "cs.CY"], "comment": "ef:ICMI 2024", "summary": "Juiciness is visual pizzazz used to improve player experience and engagement in games. Most research has focused on juicy particle effects. However, text effects are also commonly used in games, albeit not always juiced up. One type is onomatopoeia, a well-defined element of human language that has been translated to visual media, such as comic books and games. Another is semantic text, often used to provide performance feedback in games. In this work, we explored the relationship between juiciness and text effects, aiming to replicate juicy user experiences with text-based juice and combining particle and text juice. We show in a multi-phase within-subjects experiment that users rate juicy text effects similarly to particles effects, with comparable performance, and more reliable feedback. We also hint at potential improvement in user experience when both are combined, and how text stimuli may be perceived differently than other visual ones. We contribute empirical findings on the juicy-text connection in the context of visual effects for interactive media.", "AI": {"tldr": "本文研究了游戏中的文本效果如何提高玩家体验，并探讨了与视觉粒子特效相比，文字特效在改善用户体验方面的潜力。", "motivation": "大多数关于juiciness的研究集中在视觉粒子效果上，而文本特效对提升游戏玩家体验也有重要作用。本论文旨在探索文本特效（如拟声词和语义文本）如何增强玩家的互动体验。", "method": "通过多阶段的实验设计来研究用户对不同类型juicy文本效果的反应，并将其与传统粒子效果进行比较，同时评估用户体验的提升。", "result": "实验结果显示，用户对于文本特效的感受类似于视觉粒子特效，在性能反馈方面更加可靠。当两者结合使用时，可能会进一步改善用户体验。", "conclusion": "本研究提供了一些实证证据来证明文本特效在交互式媒体中的juicy效应，并且建议游戏开发者可以考虑如何更好地利用文字和视觉效果的组合以提升用户的游戏体验。"}}
{"id": "2512.13694", "pdf": "https://arxiv.org/pdf/2512.13694", "abs": "https://arxiv.org/abs/2512.13694", "authors": ["Kostantinos Mattas", "Antonio Lucas-Alba", "Tomer Toledo", "Oscar M. Melchor", "Shlomo Bekhor", "Biagio Ciuffo"], "title": "Learning to Car-Follow Using an Inertia-Oriented Driving Technique: A Before-and-After Study on a Closed Circuit", "categories": ["cs.HC", "cs.CY", "cs.RO"], "comment": null, "summary": "For decades, car following and traffic flow models have assumed that drivers default driving strategy is to maintain a safe distance. Several previous studies have questioned whether the Driving to Keep Distance is a traffic invariant. Therefore, the acceleration deceleration torque asymmetry of drivers must necessarily determine the observed patterns of traffic oscillations. Those studies indicate that drivers can adopt alternative CF strategies, such as Driving to Keep Inertia, by following basic instructions. The present work extends the evidence from previous research by showing the effectiveness of a DI course that immediately translates into practice on a closed circuit. Twelve drivers were invited to follow a lead car that varied its speed on a real circuit. Then, the driver took a DI course and returned to the same real car following scenario. Drivers generally adopted DD as the default CF mode in the pretest, both in field and simulated PC conditions, yielding very similar results. After taking the full DI course, drivers showed significantly less acceleration, deceleration, and speed variability than did the pretest, both in the field and in the simulated conditions, which indicates that drivers adopted the DI strategy. This study is the first to show the potential of adopting a DI strategy in a real circuit.", "AI": {"tldr": "研究探讨了驾驶者在跟随行驶中采用保持惯性策略的有效性，并通过实验证明其在真实电路中的应用潜力。", "motivation": "质疑传统车距维持策略是否适用于所有交通情况，探索加速减速不对称对交通模式的影响，以及证明一种新的驾驶到保持惯性的课程可以改变驾驶行为。", "method": "邀请12名驾驶员参与实验，在接受DI课程前后分别进行跟随行驶测试，并在现实和模拟条件下评估其表现差异。", "result": "经过DI课程后，驾驶员表现出更少的加减速及速度变化，表明他们已经采用了DI策略。", "conclusion": "首次证明了在实际电路中采用驾驶到保持惯性的策略是可行且有效的。"}}
{"id": "2512.13693", "pdf": "https://arxiv.org/pdf/2512.13693", "abs": "https://arxiv.org/abs/2512.13693", "authors": ["Wei Zhou", "Rashina Hoda", "Andy Li", "Chris Bain", "Laura Bird", "Emmy Trinh", "Peter Poon", "Teresa O Brien", "Mahima Kalla", "Olivia Metcalf", "Wendy Chapman", "Joycelyn Ling", "Sam Georgy", "David Bevan"], "title": "From Framework to Practice: Designing a Real-World Telehealth Application for Palliative Care", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "As digital health solutions continue to reshape healthcare delivery, telehealth software applications have become vital for improving accessibility, continuity of care, and patient outcomes. This paper presents an analysis of designing a software application focused on Enhanced Telehealth Capabilities (ETHC) for palliative care, integrating across three socio-technical dimensions: quality, human values, and real-world. Designing for quality attributes -- such as performance, maintainability, safety, and security -- ensured that the system is technically robust and compliant with clinical standards. Designing for human values -- empathy, inclusivity, accessibility, and transparency -- helped enhance patient experience, trust, and ethical alignment. Designing for real-world -- through a multidisciplinary, experience-based co-design approach involving clinicians, patients, and carers that guided iterative cycles of prototyping, usability testing, and real-world evaluation -- ensured continuous refinement of features and alignment with clinical practice. The resulting telehealth software solution demonstrated that our socio-technical design framework was successful in producing a secure, equitable, and resilient digital health application. Our design approach can assist others designing software in health and other domains.", "AI": {"tldr": "本文介绍了为临终关怀设计的一款增强型远程医疗软件ETHC的设计过程。", "motivation": "通过整合质量、人文价值和现实世界三个社会技术维度来提高远程医疗服务的质量和患者体验，提升其在实际应用中的可行性和影响力。", "method": "采用多学科协作的基于经验共设方法，结合临床医生、病人及家属的意见进行迭代设计，包括原型制作、可用性测试和实地评估。", "result": "所开发的应用程序展示了社会技术框架的有效性，是一个安全、公平且稳健的数字健康解决方案。", "conclusion": "该设计流程不仅可以应用于医疗软件领域，也可以推广到其他相关领域的应用程序设计中。"}}
{"id": "2512.13690", "pdf": "https://arxiv.org/pdf/2512.13690", "abs": "https://arxiv.org/abs/2512.13690", "authors": ["Susung Hong", "Chongjian Ge", "Zhifei Zhang", "Jui-Hsien Wang"], "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Project page: https://susunghong.github.io/DiffusionBrowser", "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.", "AI": {"tldr": "该论文提出了DiffusionBrowser，一个轻量级的解码器框架，用于视频生成过程中的交互式预览。", "motivation": "现有的视频扩散模型在合成过程中精确度低、速度慢且缺乏透明性。作者希望通过提供实时和可交互的预览来改进用户体验。", "method": "DiffusionBrowser通过多分支解码器实现在任何时间点生成准确的RGB及场景属性，允许用户在噪声去除过程中的任意阶段引导生成。", "result": "模型能够以超过4倍的实际速度生成与最终视频一致的交互式预览，并可通过重新注入随机性和模式调整来控制生成效果。", "conclusion": "该研究展示了DiffusionBrowser如何改进现有扩散模型，提供更快速、透明和可调制的视频生成体验。"}}
{"id": "2512.13689", "pdf": "https://arxiv.org/pdf/2512.13689", "abs": "https://arxiv.org/abs/2512.13689", "authors": ["Yuanwen Yue", "Damien Robert", "Jianyuan Wang", "Sunghwan Hong", "Jan Dirk Wegner", "Christian Rupprecht", "Konrad Schindler"], "title": "LitePT: Lighter Yet Stronger Point Transformer", "categories": ["cs.CV"], "comment": "Project page: https://litept.github.io/", "summary": "Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.", "AI": {"tldr": "论文提出了LitePT模型，旨在通过优化卷积和注意力机制的应用阶段来提高三维点云处理的效率。", "motivation": "研究发现，在早期层使用卷积可以有效地提取低层次几何信息，而在深层使用注意力机制则能更高效地捕捉高层次语义信息。基于这一原则，提出了改进的3D点云网络架构以减少参数和提升性能。", "method": "在早期阶段采用卷积操作来处理高分辨率、低层级特征；在深层阶段切换到注意力机制用于处理低分辨率、高层级特征，并引入了一种新的无需训练的三维位置编码方法，PointROPE，来保持空间布局信息而不丢失。", "result": "所提出的LitePT模型相比最先进的Point Transformer V3减少了3.6倍参数量，运行速度提高2倍，内存使用减少一倍，在多个任务和数据集上的表现与之相当甚至更优。", "conclusion": "通过优化卷积和注意力机制的组合策略以及引入新的位置编码技术，LitePT模型在保持或提升性能的同时显著降低了计算资源需求。"}}
{"id": "2512.13687", "pdf": "https://arxiv.org/pdf/2512.13687", "abs": "https://arxiv.org/abs/2512.13687", "authors": ["Jingfeng Yao", "Yuda Song", "Yucong Zhou", "Xinggang Wang"], "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "categories": ["cs.CV"], "comment": "Our pre-trained models are available at https://github.com/MiniMax-AI/VTP", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "AI": {"tldr": "提出了一种用于生成任务的视觉标记器预训练框架VTP，通过联合优化图像文本对比损失、自监督和重构损失，解决了传统预训练方法无法有效提升生成性能的问题。", "motivation": "传统的基于重构的训练范式产生的潜在空间偏向于低级信息，导致更好的像素级精度并不一定带来更高质量的生成效果。因此，投入大量计算资源进行视觉标记器的预训练并不能有效提高生成任务的表现。为了解决这一问题，论文提出了一种新的框架。", "method": "VTP是一种统一的视觉标记器预训练框架，通过联合优化图像文本对比损失、自监督学习和重构损失来改善潜在空间的质量，从而更好地支持生成任务。", "result": "大规模研究表明，理解是生成的关键驱动因素，并且该方法能够有效地利用更多的计算资源、参数和数据进行预训练。在ImageNet上实现了78.2的零样本准确率和0.36的rFID，比先进的蒸馏方法快4.1倍收敛于生成任务。", "conclusion": "VTP通过有效的预训练改善了视觉标记器的质量，并且证明了这种改进能够有效提升下游生成任务的表现。该框架展示了在增加计算资源时仍能保持良好的扩展性。"}}
{"id": "2512.13684", "pdf": "https://arxiv.org/pdf/2512.13684", "abs": "https://arxiv.org/abs/2512.13684", "authors": ["Daniel Zoran", "Nikhil Parthasarathy", "Yi Yang", "Drew A Hudson", "Joao Carreira", "Andrew Zisserman"], "title": "Recurrent Video Masked Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.", "AI": {"tldr": "提出了一种新的视频表示学习方法RVM，通过使用基于Transformer的循环神经网络在时间上聚合密集图像特征来有效捕捉自然视频数据的空间-时间结构。", "motivation": "旨在开发一种高效的“通用”编码器，在处理视频级别的任务和测试几何及稠密空间理解的任务中表现出色，并且无需知识蒸馏即可实现强参数效率。", "method": "RVM利用了基于Transformer的循环神经网络来聚合密集图像特征，通过不对称的掩码预测任务学习，仅使用标准像素重建目标。这种设计使得RVM能够在长时序范围内稳定地传播特征，并以线性计算成本克服了一些传统时空注意力架构的限制。", "result": "RVM在小模型环境中表现出色，与现有的视频模型和图像模型相比，在动作识别、点/物体跟踪等任务中取得了竞争性的性能。同时展示了丰富的场景语义、结构和运动的学习能力。", "conclusion": "通过使用基于Transformer的循环神经网络，RVM能够高效且有效地学习自然视频的数据结构，并在多种任务上表现优秀，尤其是在参数效率方面具有明显的优势。"}}
{"id": "2512.13683", "pdf": "https://arxiv.org/pdf/2512.13683", "abs": "https://arxiv.org/abs/2512.13683", "authors": ["Lu Ling", "Yunhao Ge", "Yichen Sheng", "Aniket Bera"], "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners", "categories": ["cs.CV"], "comment": null, "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/", "AI": {"tldr": "该论文通过重新编程预训练的3D实例生成器，使其成为场景级别的学习者，从而实现对未见过布局和新物体组合的一般化能力。", "motivation": "现有基于学习的方法受限于有限的数据集，无法实现对新布局的良好推广。因此，作者希望通过模型中心的空间监督来替换数据集绑定的监督，以解锁生成器的空间知识泛化能力。", "method": "通过将预训练的3D实例生成器重新编程为场景级别的学习者，并使用基于视图的场景空间表示方法来实现这一目标，从而直接从实例模型中学习到空间关系。", "result": "定量和定性结果表明，一个3D实例生成器可以作为隐式的空间学习者和推理者。即使是在随机组合的对象上训练时，也能出现空间推断能力。", "conclusion": "该论文证明了基于视图的场景空间表示方法能够使3D实例生成器成为一个泛化的场景生成器，并指向未来的基础模型用于交互式的3D场景理解和生成的方向。"}}
{"id": "2512.13680", "pdf": "https://arxiv.org/pdf/2512.13680", "abs": "https://arxiv.org/abs/2512.13680", "authors": ["Tianye Ding", "Yiming Xie", "Yiqing Liang", "Moitreya Chatterjee", "Pedro Miraldo", "Huaizu Jiang"], "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$", "AI": {"tldr": "本文提出了一种名为LASER的框架，该框架可以将离线重建模型转换为流媒体系统。", "motivation": "现有的流处理方法通过学习内存机制或因果注意力解决无法处理实时视频的问题，但需要大量的重新训练，并且可能不能充分利用最先进的离线模型的强大几何先验知识。因此本文提出了一种无需训练的方法来解决这个问题。", "method": "LASER通过在连续时间窗口之间对齐预测结果来进行离线重建模型到流媒体系统的转换。它发现简单的相似性变换（Sim(3)）对齐失败的原因是由于层深度的不一致，而单目尺度模糊导致不同场景层的相对深度尺度在不同的窗口中变化不一致。", "result": "实验表明LASER在摄像机姿态估计和点云重建质量方面达到了最先进的水平，同时以14 FPS的速度运行，并且峰值内存使用量为6 GB，在RTX A6000 GPU上实现了对千米级视频流的实用部署。", "conclusion": "该框架提供了一种无需训练的方法来实现离线模型到流媒体系统的转换，这使得大规模实时场景重建成为可能。"}}
{"id": "2512.13678", "pdf": "https://arxiv.org/pdf/2512.13678", "abs": "https://arxiv.org/abs/2512.13678", "authors": ["Ziqi Ma", "Hongqiao Chen", "Yisong Yue", "Georgia Gkioxari"], "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D", "categories": ["cs.CV", "cs.AI"], "comment": "https://glab-caltech.github.io/steer3d/", "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/", "AI": {"tldr": "本文提出了一种名为Steer3D的方法，可以在生成的3D资产中实现文本引导编辑。", "motivation": "为了使AI生成的3D资产能够更好地应用于实际场景，需要一种易于使用的编辑工具。Steer3D通过引入语言指令控制来解决这一问题。", "method": "作者借鉴了ControlNet的思想，并将其适配到图像到三维模型生成中，从而实现文本引导直接在前向传递过程中进行。他们还开发了一种基于流匹配训练和Direct Preference Optimization (DPO)的两阶段训练方法。", "result": "Steer3D能够更忠实于语言指令，并保持更好的与原始3D资产的一致性；同时速度更快，比竞争方法快2.4倍到28.5倍。", "conclusion": "通过在预训练的图像到三维模型生成中添加新的模态（文本），Steer3D展示了其可以有效地引导生成过程的新能力。"}}
{"id": "2512.13677", "pdf": "https://arxiv.org/pdf/2512.13677", "abs": "https://arxiv.org/abs/2512.13677", "authors": ["Xiaohu Huang", "Hao Zhou", "Qiangpeng Yang", "Shilei Wen", "Kai Han"], "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation", "categories": ["cs.CV"], "comment": "Project page: \\url{https://visual-ai.github.io/jova}", "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.", "AI": {"tldr": "本文提出了JoVA框架，用于联合生成视频和音频。", "motivation": "现有的方法在生成与口型同步的人声方面存在不足，并且通常依赖于显式融合或特定模态对齐模块，增加了模型复杂性。", "method": "JoVA利用跨视频和音频令牌的联合自注意力机制，在每个Transformer层内实现直接有效的跨模式交互。此外，引入了一种基于面部关键点检测的简单而有效的口部区域损失函数，以增强训练过程中的关键口部监督。", "result": "实验结果表明，JoVA在唇形同步准确性、语音质量和视频音频生成保真度方面优于或与现有最佳方法相当。", "conclusion": "JoVA是一个优雅的框架，能够在高保真的多模态生成中表现良好。"}}
{"id": "2512.13674", "pdf": "https://arxiv.org/pdf/2512.13674", "abs": "https://arxiv.org/abs/2512.13674", "authors": ["Yiyi Cai", "Xuangeng Chu", "Xiwei Gao", "Sitong Gong", "Yifei Huang", "Caixin Kang", "Kunhang Li", "Haiyang Liu", "Ruicong Liu", "Yun Liu", "Dianwen Ng", "Zixiong Su", "Erwin Wu", "Yuhan Wu", "Dingkun Yan", "Tianyu Yan", "Chang Zeng", "Bo Zheng", "You Zhou"], "title": "Towards Interactive Intelligence for Digital Humans", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.HC"], "comment": null, "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.", "AI": {"tldr": "提出了一个名为Mio的端到端框架，用于实现具有个性表达、适应性互动和自我演进能力的数字人类。", "motivation": "旨在推动数字人类从表面模仿向智能交互发展，通过综合认知推理与实时多模态表现来实现这一目标。", "method": "提出了一个包含五个专门模块（Thinker, Talker, Face Animator, Body Animator, Renderer）的Mio框架，以实现实时、流畅且一致的互动。还建立了一个新的基准测试用于评估交互智能的能力。", "result": "实验结果表明，在所有评价维度上，所提出的框架均优于现有的最先进的方法。", "conclusion": "通过实现具有自我演进能力的数字人类，该研究推进了数字人类从表面模仿向智能互动的发展。"}}
{"id": "2512.13672", "pdf": "https://arxiv.org/pdf/2512.13672", "abs": "https://arxiv.org/abs/2512.13672", "authors": ["Kunhee Kim", "NaHyeon Park", "Kibeom Hong", "Hyunjung Shim"], "title": "Directional Textual Inversion for Personalized Text-to-Image Generation", "categories": ["cs.LG", "cs.CV"], "comment": "Project page: https://kunheek.github.io/dti", "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.", "AI": {"tldr": "该论文提出了一种新的方向性文本逆向（DTI）方法，用于改进文本到图像生成中的个性化问题。", "motivation": "传统的文本逆向方法在处理复杂提示时效果不佳，原因是嵌入量的放大导致预归一化Transformer中的位置信息削弱和残差更新受阻。", "method": "通过限制学习嵌入的大小并仅优化方向来解决上述问题，新方法DTI能够在单位超球面上利用黎曼SGD进行优化，并引入von Mises-Fisher先验以实现平滑且语义连贯的插值。", "result": "该论文证明了DTI相较于传统的文本逆向方法在保持主题相似性的同时提高了文本忠实度，能够更准确地处理复杂的提示。", "conclusion": "研究结果表明，在个性化任务中，仅优化方向的方法是一个稳健且可扩展的选择。"}}
{"id": "2512.13671", "pdf": "https://arxiv.org/pdf/2512.13671", "abs": "https://arxiv.org/abs/2512.13671", "authors": ["Junwen Miao", "Penghui Du", "Yi Liu", "Yu Wang", "Yan Wang"], "title": "AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.", "AI": {"tldr": "AgentIAD 是一种工具增强的单一代理框架，用于工业异常检测。", "motivation": "由于正常参考样本稀少和缺陷具有局部细微性，传统的单阶段视觉语言模型在工业异常检测中常忽视小瑕疵且缺乏与标准模式比较的机制。", "method": "AgentIAD 包含一个感知放大器 (PZ) 和比较检索器 (CR)，通过监督微调和强化学习两步训练。采用结构化的感知轨迹和行为奖励来优化模型。", "result": "在 MMAD 数据集上，AgentIAD 达到了97.62% 的分类准确率，优于先前的多模态大型语言模型方法，并提供透明可解释的检查痕迹。", "conclusion": "AgentIAD 通过精细观察、放大和验证来优化异常检测，提高了检测精度并增强了可解释性。"}}
{"id": "2512.13670", "pdf": "https://arxiv.org/pdf/2512.13670", "abs": "https://arxiv.org/abs/2512.13670", "authors": ["Licheng Luo", "Yu Xia", "Kaier Liang", "Mingyu Cai"], "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial", "AI": {"tldr": "本文提出了一种从自然语言生成几何时空逻辑规范的方法，以支持机器人操作任务。", "motivation": "现有工作主要基于标准时态逻辑，忽略了对象级别的交互，而这些交互对于操作任务的成功至关重要。因此，作者引入了一个数据集生成框架，该框架可以合成时空逻辑规范并将其转换为自然语言描述，从而弥补了这一不足。", "method": "通过确定性语义保持逆向翻译过程，将时空逻辑规范转化为自然语言，并构建NL2SpaTiaL数据集；提出了一种翻译验证框架，包含基于语言的语义检查器以确保生成的时空逻辑公式忠实于输入描述的语义。", "result": "实验显示，时空逻辑表示法使指令遵循更具解释性、可验证性和组合性。", "conclusion": "本文通过引入NL2SpaTiaL数据集和翻译验证框架，证明了时空逻辑对于理解和执行操作任务的重要性。"}}
{"id": "2512.13665", "pdf": "https://arxiv.org/pdf/2512.13665", "abs": "https://arxiv.org/abs/2512.13665", "authors": ["Wenhan Chen", "Sezer Karaoglu", "Theo Gevers"], "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.", "AI": {"tldr": "该论文介绍了Grab-3D，一种基于三维几何时序一致性的AI生成视频检测框架。", "motivation": "现有技术使得AI能够生成高度逼真的视频，迫切需要可靠的检测机制。然而现有的方法对于三维几何模式的探索有限，因此引入了基于消失点表示三维几何模式的方法来揭示真实视频与AI生成视频之间的基本差异。", "method": "该论文提出了一种具有几何位置编码、时序-几何注意力以及EMA（指数移动平均）几何分类器头的几何感知变换器框架，以将三维几何感知注入到时间建模中。通过构建静态场景的AI生成视频数据集来实现可靠的评估。", "result": "实验表明，Grab-3D在检测AI生成视频方面显著优于现有技术，并且具有跨域泛化能力，能够有效应对未知生成器。", "conclusion": "该论文提出的Grab-3D框架为三维几何时序一致性提供了新的视角，并证明了其在AI生成视频检测中的有效性。"}}
{"id": "2512.13660", "pdf": "https://arxiv.org/pdf/2512.13660", "abs": "https://arxiv.org/abs/2512.13660", "authors": ["Enshen Zhou", "Cheng Chi", "Yibo Li", "Jingkun An", "Jiayuan Zhang", "Shanyu Rong", "Yi Han", "Yuheng Ji", "Mengzhen Liu", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://zhoues.github.io/RoboTracer", "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "AI": {"tldr": "提出RoboTracer模型，用于机器人视觉语言模型中的空间追踪与推理。", "motivation": "现有方法难以处理复杂的多步骤空间追踪任务，需要一种新的解决方案来提高空间理解和测量能力。", "method": "通过引入一个普遍的空间编码器和回归监督解码器来实现3D空间参考和测量。采用强化学习对模型进行微调以支持多步推理，并创建TraceSpatial数据集用于训练和评估。", "result": "RoboTracer在空间理解、测量和引用方面超越了基线，成功率达到了79.1%，并在TraceSpatial-Bench基准测试中表现出色，比Gemini-2.5-Pro高出36%的准确率。", "conclusion": "RoboTracer能有效执行复杂的空间追踪任务，并可以与各种控制策略集成以处理长时间跨度的任务。"}}
{"id": "2512.13658", "pdf": "https://arxiv.org/pdf/2512.13658", "abs": "https://arxiv.org/abs/2512.13658", "authors": ["Mohammadreza Molavi", "Mohammad Moein", "Mohammadreza Tavakoli", "Abdolali Faraji", "Stefan T. Mol", "Gábor Kismihók"], "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)", "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.", "AI": {"tldr": "基于嵌入的教育资源排名与验证，通过学习成果对齐提高个性化教育的有效性。", "motivation": "在线教育领域不断发展，资源众多但难以精准选择符合预期学习目标的内容。大型语言模型（LLM）虽然有潜力生成更多个性化的学习材料，但仍需昂贵的人工审核以确保内容对齐。为此论文提出了一种自动评估教育资源与学习成果一致性的框架。", "method": "利用人类生成的资料进行基准测试，对比不同嵌入式文本模型在检测资源与学习目标一致性上的准确性；应用最优模型于LLM生成的内容，并通过专家验证其可靠性；最后，在包含360名学生的实验中检验了高对齐得分是否能提升学生的学习表现。", "result": "最佳模型（Voyage）准确率达到79%；应用于LLM生成资源后，经专家评价准确性为83%；实验显示学习成果与更高对齐分数存在正相关关系，卡方检验显著(p<0.001)。", "conclusion": "嵌入式评分可以有效验证教育资源是否符合预期的学习目标，有助于大规模个性化教育的发展。"}}
{"id": "2512.13654", "pdf": "https://arxiv.org/pdf/2512.13654", "abs": "https://arxiv.org/abs/2512.13654", "authors": ["John E. Ortega", "Dhruv D. Joshi", "Matt P. Borkowski"], "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "comment": "7 pages, 1 figure, Appendix of Prompts", "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.", "AI": {"tldr": "本文研究了大型语言模型在处理美国最高法院判决分类任务时的记忆策略。", "motivation": "旨在理解大型语言模型如何回应复杂的法律术语和结构，尤其是通过比较记忆增强型模型与传统BERT模型的表现。", "method": "实验采用最新参数高效微调和检索方法，包括DeepSeek等模型，在包含15个主题标签和279个类别的两个任务上进行测试。", "result": "结果表明，基于提示的记忆模型在两个分类任务上的表现优于传统BERT模型约2分。", "conclusion": "研究展示了记忆增强型大型语言模型在处理法律文本时的优越性。"}}
{"id": "2512.13644", "pdf": "https://arxiv.org/pdf/2512.13644", "abs": "https://arxiv.org/abs/2512.13644", "authors": ["Raktim Gautam Goswami", "Amir Bar", "David Fan", "Tsung-Yen Yang", "Gaoyue Zhou", "Prashanth Krishnamurthy", "Michael Rabbat", "Farshad Khorrami", "Yann LeCun"], "title": "World Models Can Leverage Human Videos for Dexterous Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.", "AI": {"tldr": "本文介绍了一种用于灵巧操作的世界模型DexWM，该模型可以预测环境的下一状态，并利用人类和非灵巧机器人视频来克服数据稀缺问题。", "motivation": "为了实现更精细的操作能力，研究人员开发了DexWM，以解决仅通过视觉特征进行预测不足的问题。此外，他们还引入了一种辅助的手部一致性损失函数，确保手部配置的准确性。", "method": "DexWM是一个世界模型，它能够根据过去的环境状态和灵巧操作动作来预测下一时刻的状态。该模型训练于超过900小时的人类及非灵巧机器人视频上，以弥补数据不足的问题，并通过引入辅助的手部一致性损失函数来提高手部配置的准确性。", "result": "DexWM在各种任务中表现出色，不仅比之前的基于文本、导航和全身体动作的世界模型更准确地预测了未来状态，而且还在使用Franka Panda机械臂进行抓握、放置及到达操作时展示了强大的零样本泛化能力。", "conclusion": "通过结合人类视频数据并引入手部一致性损失函数，DexWM成功实现了对灵巧操作任务的精确预测和处理。"}}
{"id": "2512.13641", "pdf": "https://arxiv.org/pdf/2512.13641", "abs": "https://arxiv.org/abs/2512.13641", "authors": ["Gabriel Vitorino de Andrade", "Saulo Roberto dos Santos", "Itallo Patrick Castro Alves da Silva", "Emanuel Adler Medeiros Pereira", "Erick de Andrade Barboza"], "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "This work was presented at the BRACIS 2025 conference in Fortaleza", "summary": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.", "AI": {"tldr": "评估芒果叶片疾病诊断中卷积神经网络在不利条件下的鲁棒性。", "motivation": "验证AI模型的可靠性和应对现实世界挑战的能力，特别关注缺乏对芒果叶病害诊断模型鲁棒性的研究。", "method": "利用改进后的MangoLeafDB-C数据集，在五种程度的人工腐败下比较五个架构的性能。评估指标包括F1分数、腐败误差(CE)和相对平均腐败误差(relative mCE)。", "result": "轻量级且专门设计的LCNN模型在现实世界的不利条件下表现更好，而现代复杂架构在腐败场景中表现出显著性能下降。", "conclusion": "研究强调了在智能系统开发过程中应将鲁棒性评估纳入考虑，特别是对于资源有限地区的农业应用。"}}
{"id": "2512.13639", "pdf": "https://arxiv.org/pdf/2512.13639", "abs": "https://arxiv.org/abs/2512.13639", "authors": ["Michal Nazarczuk", "Thomas Tanay", "Arthur Moreau", "Zhensong Zhang", "Eduardo Pérez-Pellitero"], "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All", "categories": ["cs.CV"], "comment": "Project page: https://charge-benchmark.github.io/", "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.", "AI": {"tldr": "提出了一种新的用于新型视图合成的数据集，该数据集包含高保真RGB图像及其多种互补模态，并分为三种不同的基准测试场景。", "motivation": "为了训练和评估最先进的4D场景重建和新颖视图生成模型，提供一个具有视觉丰富性、高质量注释和多样化实验设置的资源。", "method": "从高质量动画电影中生成数据集，包含多样化的动态场景及其详细纹理、照明和运动，并分为密集多视角相机配置、稀疏相机排列和单目视频序列三种基准测试场景。", "result": "提供的数据集结合了视觉丰富性、高质量注释以及不同级别的数据稀缺实验设置。", "conclusion": "该数据集为推动视图合成和3D视觉的边界提供了一个独特的资源。"}}
{"id": "2512.13636", "pdf": "https://arxiv.org/pdf/2512.13636", "abs": "https://arxiv.org/abs/2512.13636", "authors": ["Haoyu Fu", "Diankun Zhang", "Zongchuang Zhao", "Jianfeng Cui", "Hongwei Xie", "Bing Wang", "Guang Chen", "Dingkang Liang", "Xiang Bai"], "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning", "categories": ["cs.CV", "cs.RO"], "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/", "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.", "AI": {"tldr": "提出了一种基于在线强化学习的视觉语言行动模型MindDrive，用于自动驾驶。", "motivation": "当前自动驾驶中的视觉语言行动范式主要依赖模仿学习，存在分布偏移和因果混淆等问题。通过在线强化学习解决这些问题，但面临连续动作空间中探索效率低下的挑战。", "method": "提出了MindDrive框架，包含一个大型语言模型（LLM）及其两个不同的LoRA参数集：一个作为决策专家进行场景推理与驾驶决策；另一个作为行动专家将语言决策映射为可行轨迹。通过将轨迹级别奖励反馈到推理空间中，允许在有限的离散语言驱动决策集中进行试错学习。", "result": "使用轻量级Qwen-0.5B LLM，在挑战性的Bench2Drive基准上实现78.04的驾驶评分（DS）和55.09％的成功率。", "conclusion": "首次展示了在线强化学习在视觉语言行动模型中的有效性，解决了自动驾驶中决策制定、人类类似行为和高效探索的问题。"}}
{"id": "2512.13635", "pdf": "https://arxiv.org/pdf/2512.13635", "abs": "https://arxiv.org/abs/2512.13635", "authors": ["Junchao Zhu", "Ruining Deng", "Junlin Guo", "Tianyuan Yao", "Chongyu Qu", "Juming Xiong", "Siqi Lu", "Zhengyi Lu", "Yanfan Zhu", "Marilyn Lionts", "Yuechen Yang", "Yalin Zheng", "Yu Wang", "Shilin Zhao", "Haichun Yang", "Yuankai Huo"], "title": "SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST", "AI": {"tldr": "该论文提出了一种结合单细胞测序和空间转录组学的框架SCR2-ST，以提高数据采集效率和预测准确性。", "motivation": "由于空间转录组学的数据获取成本高昂且传统采样策略导致冗余测量，研究者希望通过利用成熟的单细胞测序领域的知识来解决这些问题，从而有效减少不必要的资源浪费并提升数据质量。", "method": "论文采用了一种基于单细胞指导的强化学习方法SCRL进行高效样本选择，并结合回归与检索增强推理网络SCR2Net对空间转录组学数据进行预测。", "result": "通过在三个公开的空间转录组数据集上测试，该框架展示了超出同类方法的表现，在采样效率和预测准确性方面均优于其他模型。", "conclusion": "论文提出的SCR2-ST框架能够有效地结合单细胞信息指导空间转录组学的数据采集，并提高其预测准确度，尤其在低预算条件下表现突出。"}}
{"id": "2512.13609", "pdf": "https://arxiv.org/pdf/2512.13609", "abs": "https://arxiv.org/abs/2512.13609", "authors": ["Shweta Mahajan", "Shreya Kadambi", "Hoang Le", "Munawar Hayat", "Fatih Porikli"], "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.", "AI": {"tldr": "引入Do-Undo任务，使视觉语言模型能够理解和生成真实的物理动作及其逆过程。", "motivation": "解决现有视觉语言模型在理解并模拟真实世界物理行动和逆向操作上的不足，推动具身AI、机器人及物理解释性生成模的发展。", "method": "构建大规模可逆转动作数据集，并设计保证一致性的训练策略以增强模型对物理行为的理解能力。", "result": "实验显示当前模型在处理物理可逆性方面存在困难，表明Do-Undo任务对于评估和提升多模态系统的物理推理能力具有重要意义。", "conclusion": "Do-Undo为评估和改进视觉语言模型的物理解释性和生成性能提供了一个直观的测试平台。"}}
{"id": "2512.13608", "pdf": "https://arxiv.org/pdf/2512.13608", "abs": "https://arxiv.org/abs/2512.13608", "authors": ["Felix J. Dorfner", "Manon A. Dorster", "Ryan Connolly", "Oscar Gentilhomme", "Edward Gibbs", "Steven Graham", "Seth Wander", "Thomas Schultz", "Manisha Bahl", "Dania Daye", "Albert E. Kim", "Christopher P. Bridge"], "title": "DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening. To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training. Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes. For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\\% compared to Dinov2's 77.3\\%. Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.", "AI": {"tldr": "开发并评估用于数字乳腺断层合成(DBT)的首个基础模型DBT-DINO，以支持多临床任务。", "motivation": "尽管数字乳腺断层合成(DBT)在乳腺癌筛查中被广泛应用，但至今没有专门的基础模型来处理这种三维成像模态。为弥补这一空白并探索其潜力，本研究致力于开发一个用于DBT的数据基础模型。", "method": "使用超过2500万张来自487,975个DBT体的2D切片和27,990名患者数据集进行自我监督预训练。采用DINOv2方法，评估了三个下游任务：乳腺密度分类、五年内发生乳腺癌的风险预测以及病灶检测。", "result": "对于乳腺密度分类，DBT-DINO的准确率达到0.79；五年乳腺癌风险预测中AUROC为0.78。在癌症病灶检测方面，虽然总体敏感性略低于DinoV2（62%对比67%），但对特定癌变病灶的检测率高于后者。", "conclusion": "研究开发了首个用于DBT的数据基础模型DBT-DINO，并展示了其在乳腺密度分类和五年内发生乳腺癌的风险预测中的强大性能。然而，对于通用病灶检测任务而言，需要进一步的方法学改进以提升效果。"}}
{"id": "2512.13607", "pdf": "https://arxiv.org/pdf/2512.13607", "abs": "https://arxiv.org/abs/2512.13607", "authors": ["Boxin Wang", "Chankyu Lee", "Nayeon Lee", "Sheng-Chieh Lin", "Wenliang Dai", "Yang Chen", "Yangyi Chen", "Zhuolin Yang", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade", "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", "AI": {"tldr": "提出了一种新的级联领域强化学习方法来开发通用推理模型Nemotron-Cascade", "motivation": "解决多领域异质性带来的训练复杂性和工程难度问题，提升跨域表现和适应性", "method": "采用级联领域强化学习技术，分阶段优化模型性能，在RLHF预处理后进行领域细化的RLVR训练", "result": "14B模型在多个基准测试中超越了其SFT老师，并在2025国际信息学奥林匹克竞赛中获得银牌", "conclusion": "Nemotron-Cascade通过级联领域强化学习显著提高了通用推理模型的能力"}}
{"id": "2512.13604", "pdf": "https://arxiv.org/pdf/2512.13604", "abs": "https://arxiv.org/abs/2512.13604", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Junhao Zhuang", "Chengming Xu", "Jianfeng Feng", "Yu Qiao", "Yanwei Fu", "Chenyang Si", "Ziwei Liu"], "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "categories": ["cs.CV"], "comment": "Project Page: https://vchitect.github.io/LongVie2-project/", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "AI": {"tldr": "构建一个多模态可控的超长时间视频世界模型，以实现长期高质量生成和时间一致性。", "motivation": "建立基于预训练的视频生成系统的视频世界模型，需要具备可控制性、长时段视觉质量及时间连贯性的三个基本特性。为了提高这些特性，提出了LongVie 2框架。", "method": "通过三阶段渐进式方法：多模态引导、退化感知训练和历史上下文指导来提升视频世界模型的可控性、长期高质量生成以及时间一致性。", "result": "在长距离可控性和视觉保真度方面，LongVie 2达到了最先进的性能，并能够支持长达五分钟的连续视频生成。", "conclusion": "LongVie 2标志着向统一视频世界建模迈出的重要一步。"}}
{"id": "2512.13600", "pdf": "https://arxiv.org/pdf/2512.13600", "abs": "https://arxiv.org/abs/2512.13600", "authors": ["Haoyue Zhang", "Meera Chappidi", "Erolcan Sayar", "Helen Richards", "Zhijun Chen", "Lucas Liu", "Roxanne Wadia", "Peter A Humphrey", "Fady Ghali", "Alberto Contreras-Sanz", "Peter Black", "Jonathan Wright", "Stephanie Harmon", "Michael Haffner"], "title": "DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.", "AI": {"tldr": "提出了一个名为DA-SSL的领域适应自监督适配器，用于在TURBT（经尿道膀胱肿瘤切除术）病理切片上增强预训练模型的表现。", "motivation": "PFMs在处理特定癌症类型或含有罕见组织特征的样本时表现出限制，如TURBT。为了解决这一问题，作者提出了一个能够将预先训练的PFM特征重新调整到TURBT领域的域适应自监督适配器（DA-SSL）。", "method": "提出了一种基于自我监督学习的领域适应方法DA-SSL，无需微调基础模型即可使其在特定样本上表现出色。此方法特别针对了TURBT病理图像中的挑战性问题进行了优化。", "result": "该框架在多中心研究中取得了五折交叉验证下的AUC值为0.77±0.04，并且在外部测试集上的准确率达到0.84，敏感性和特异性分别达到了0.71和0.91。表明轻量级的域适应与自监督学习可以有效增强基于PFM的MIL管道。", "conclusion": "DA-SSL通过无需微调基础模型就能在TURBT领域实现性能提升的方式，展示了如何利用自我监督学习进行有效的域适配，从而解决了PFMs处理特定病理样本时面临的挑战。"}}
{"id": "2512.13597", "pdf": "https://arxiv.org/pdf/2512.13597", "abs": "https://arxiv.org/abs/2512.13597", "authors": ["Christophe Bolduc", "Julien Philip", "Li Ma", "Mingming He", "Paul Debevec", "Jean-François Lalonde"], "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.", "AI": {"tldr": "本文提出了一种基于扩散的方法LiMo，用于时空HDR照明估计。", "motivation": "为了实现高细节的逼真预测和准确的光照估算，论文提出了一个新的方法来改进现有的光照估计技术。", "method": "通过生成不同曝光度下的镜面和漫反射球体，并利用深度优先模型对大规模定制数据集进行微调。此外，引入新的几何条件以提供场景相对于目标3D位置的相对位置，结合不同曝光度下预测的结果形成单个HDRI图。", "result": "该方法在空间控制和预测准确性方面被证明是当前最先进的。", "conclusion": "论文提出的方法LiMo能够有效提高光照估计的质量，尤其是在逼真细节和精确性上表现出色。"}}
{"id": "2512.13592", "pdf": "https://arxiv.org/pdf/2512.13592", "abs": "https://arxiv.org/abs/2512.13592", "authors": ["Fu-Yun Wang", "Hao Zhou", "Liangzhe Yuan", "Sanghyun Woo", "Boqing Gong", "Bohyung Han", "Ming-Hsuan Yang", "Han Zhang", "Yukun Zhu", "Ting Liu", "Long Zhao"], "title": "Image Diffusion Preview with Consistency Solver", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.", "AI": {"tldr": "提出了一个新的图像扩散模型预览方法，使用轻量级的可训练高阶求解器ConsistencySolver来提高低步数采样生成的质量和一致性。", "motivation": "现有的加速方法难以在保证高质量的同时提供一致的预览与最终输出之间的质量，本文旨在解决这个问题，以改善用户体验。", "method": "通过一般线性多步骤法提出了新的轻量级可训练高阶求解器ConsistencySolver，并用强化学习进行优化。这种方法可以提高低步数采样时生成的质量和一致性。", "result": "实验表明，使用ConsistencySolver可以在较少的采样步骤下获得与Multistep DPM-Solver相当的质量，同时优于基于蒸馏的方法。用户研究显示该方法减少了用户的交互时间近50%。", "conclusion": "ConsistencySolver是一个有效的工具，适用于需要快速预览和精细调整的工作流程，并且可以显著减少用户体验中的等待时间。"}}
{"id": "2512.13586", "pdf": "https://arxiv.org/pdf/2512.13586", "abs": "https://arxiv.org/abs/2512.13586", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Chongxuan Li"], "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.", "AI": {"tldr": "介绍了ReFusion，一种通过平行自回归解码改善生成效率和性能的扩散模型。", "motivation": "为了解决自回归模型（ARM）由于慢速序列推理而面临的瓶颈，以及掩码扩散模型（MDM）在计算开销和生成连贯性上的问题，作者提出了一种新的方法来改进这些模型。", "method": "ReFusion通过将平行解码从标记级别提升到更高的槽级别实现其目标。每个槽是一个固定长度的连续子序列，在这种设计下，可以完全解锁KV缓存重用并减少学习复杂性，同时使用统一的因果框架。", "result": "实验表明，ReFusion在七个多样化的基准测试中超过了先前的最佳掩码扩散模型，性能提升34%，平均速度提高了18倍以上；与强大的自回归模型相比，也缩小了性能差距，且保持了2.33倍以上的平均加速比。", "conclusion": "通过将解码过程设计为计划和填充迭代步骤，ReFusion能够在不牺牲连贯性的情况下提高生成效率，并缩小了与传统ARM的性能差距。"}}
{"id": "2512.13583", "pdf": "https://arxiv.org/pdf/2512.13583", "abs": "https://arxiv.org/abs/2512.13583", "authors": ["Zehan Zhu", "Heng Zhao", "Yan Huang", "Joey Tianyi Zhou", "Shouling Ji", "Jinming Xu"], "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages", "summary": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}δ \\right)}/(\\sqrt{n}Jε) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(ε, δ\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.13580", "pdf": "https://arxiv.org/pdf/2512.13580", "abs": "https://arxiv.org/abs/2512.13580", "authors": ["Michael Williams de la Bastida", "Thomas M. Bickley", "Peter V. Coveney"], "title": "Optimised Fermion-Qubit Encodings for Quantum Simulation with Reduced Transpiled Circuit Depth", "categories": ["quant-ph", "cs.ET"], "comment": "21 pages, 16 figures", "summary": "Simulation of fermionic Hamiltonians with gate-based quantum computers requires the selection of an encoding from fermionic operators to quantum gates, the most widely used being the Jordan-Wigner transform. Many alternative encodings exist, with quantum circuits and simulation results being sensitive to choice of encoding, device connectivity and Hamiltonian characteristics. Non-stochastic optimisation of the ternary tree class of encodings to date has targeted either the device or Hamiltonian. We develop a deterministic method which optimises ternary tree encodings without changing the underlying tree structure. This enables reduction in Pauli-weight without ancillae or additional swap-gate overhead. We demonstrate this method for a variety of encodings, including those which are derived from the qubit connectivity graph of a quantum computer. Across a suite of standard encoding methods applied to water in STO-3G basis, including Jordan-Wigner, our method reduces qDRIFT circuit depths on average by $27.7\\%$ and $26.0\\%$ for untranspiled and transpiled circuits respectively.", "AI": {"tldr": "本文开发了一种确定性的优化方法，用于减少费米子编码为量子门时的电路深度。", "motivation": "模拟费米子哈密顿量需要选择一个将费米算符转换为量子门的编码方式。不同的编码、设备连接性和哈密顿量特性对量子电路和仿真结果有影响。本文旨在通过不改变基础树结构的方法来优化三元树编码。", "method": "作者提出了一种新的确定性方法，可以减少保真度损失的同时降低电路深度，并展示了该方法在不同编码下的应用效果。", "result": "对于水分子在STO-3G基态的标准编码方法，包括Jordan-Wigner编码等，本文的方法将未编译和已编译的qDRIFT电路深度分别降低了27.7%和26.0%。", "conclusion": "新提出的方法能够有效减少费米子量子模拟中的电路复杂度，在多种编码下都能取得显著效果。"}}
{"id": "2512.13576", "pdf": "https://arxiv.org/pdf/2512.13576", "abs": "https://arxiv.org/abs/2512.13576", "authors": ["Dorian Koch", "Albert Zeyer", "Nick Rossenbach", "Ralf Schlüter", "Hermann Ney"], "title": "Reproducing and Dissecting Denoising Language Models for Speech Recognition", "categories": ["cs.NE"], "comment": "Equal contribution of first two authors", "summary": "Denoising language models (DLMs) have been proposed as a powerful alternative to traditional language models (LMs) for automatic speech recognition (ASR), motivated by their ability to use bidirectional context and adapt to a specific ASR model's error patterns. However, the complexity of the DLM training pipeline has hindered wider investigation. This paper presents the first independent, large-scale empirical study of DLMs. We build and release a complete, reproducible pipeline to systematically investigate the impact of key design choices. We evaluate dozens of configurations across multiple axes, including various data augmentation techniques (e.g., SpecAugment, dropout, mixup), different text-to-speech systems, and multiple decoding strategies. Our comparative analysis in a common subword vocabulary setting demonstrates that DLMs outperform traditional LMs, but only after a distinct compute tipping point. While LMs are more efficient at lower budgets, DLMs scale better with longer training, mirroring behaviors observed in diffusion language models. However, we observe smaller improvements than those reported in prior character-based work, which indicates that the DLM's performance is conditional on factors such as the vocabulary. Our analysis reveals that a key factor for improving performance is to condition the DLM on richer information from the ASR's hypothesis space, rather than just a single best guess. To this end, we introduce DLM-sum, a novel method for decoding from multiple ASR hypotheses, which consistently outperforms the previously proposed DSR decoding method. We believe our findings and public pipeline provide a crucial foundation for the community to better understand, improve, and build upon this promising class of models. The code is publicly available at https://github.com/rwth-i6/2025-denoising-lm/.", "AI": {"tldr": "本文研究了去噪语言模型在语音识别中的性能，并提出了一种新的解码方法DLM-sum。", "motivation": "传统的语言模型依赖于单向上下文，而去噪语言模型则能利用双向上下文和适应特定语音识别系统的错误模式。然而由于训练流程的复杂性，这一领域的研究受到了阻碍。", "method": "构建了一个完整的可重复使用管道来系统地探究关键设计选择的影响，并对多种配置进行了评估。", "result": "在共同的子词词汇设置中进行比较分析后发现，去噪语言模型在达到特定计算阈值后优于传统语言模型。DLM-sum方法在解码多个假设时表现出色。", "conclusion": "研究为更好地理解、改进和构建这一有前景的语言模型类别提供了重要的基础。"}}
{"id": "2512.13573", "pdf": "https://arxiv.org/pdf/2512.13573", "abs": "https://arxiv.org/abs/2512.13573", "authors": ["Tao Zhang", "Ziqi Zhang", "Zongyang Ma", "Yuxin Chen", "Bing Li", "Chunfeng Yuan", "Guangting Wang", "Fengyun Rao", "Ying Shan", "Weiming Hu"], "title": "MMhops-R1: Multimodal Multi-hop Reasoning", "categories": ["cs.CV"], "comment": "Acceped by AAAI 2026", "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.", "AI": {"tldr": "该论文提出了一种新的多模态检索增强生成框架MMhops-R1，用于解决复杂推理任务。", "motivation": "现有的多模态大型语言模型主要局限于单步推理，缺乏评估和推动多跳能力的基准。因此，本文旨在引入一个新的大规模基准MMhops来系统性地评价和促进多模态多跳推理。", "method": "该论文提出了一种新的多模态检索增强生成框架MMhops-R1，并利用强化学习优化模型以自主规划推理路径、构建目标查询以及合成多层次信息。", "result": "实验表明，MMhops-R1在基准测试中显著优于强基线模型，在固定跳数的推理任务上也表现出强大的泛化能力。", "conclusion": "本文贡献了一个新的具有挑战性的基准和一个强有力的基线模型，并将公开相关的代码、数据及权重以促进这一领域的未来研究。"}}
{"id": "2512.13568", "pdf": "https://arxiv.org/pdf/2512.13568", "abs": "https://arxiv.org/abs/2512.13568", "authors": ["Leonard Bereska", "Zoe Tzifa-Kratira", "Reza Samavi", "Efstratios Gavves"], "title": "Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to TMLR, view HTML here: https://leonardbereska.github.io/blog/2025/superposition/", "summary": "Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many \"virtual neurons\" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.", "AI": {"tldr": "通过稀疏自动编码器激活应用香农熵计算神经表示的有效自由度，测量网络中超位置的损失压缩程度。", "motivation": "缺乏对超位置的原理性量化方法，该研究旨在提出一个信息论框架来衡量神经表征的有效自由度，并探索其与对抗鲁棒性的关系。", "method": "将Shannon熵应用于稀疏自动编码器激活以计算有效特征数，以此定义网络中模拟出的虚拟神经元数量。通过对比真实模型中的地面实况验证方法有效性。", "result": "该指标在玩具模型和Pythia-70M上表现出强相关性，并揭示了在网络发展过程中显著的功能整合现象；此外发现对抗训练可能会增加有效特征，从而提升鲁棒性，这种效果取决于任务复杂度及网络容量。", "conclusion": "定义超位置为损失压缩，这项工作实现了对神经网络在计算约束下信息组织方式的原理性测量，并揭示了超位置与对抗鲁棒性的关联。"}}
{"id": "2512.13564", "pdf": "https://arxiv.org/pdf/2512.13564", "abs": "https://arxiv.org/abs/2512.13564", "authors": ["Yuyang Hu", "Shichun Liu", "Yanwei Yue", "Guibin Zhang", "Boyang Liu", "Fangyi Zhu", "Jiahang Lin", "Honglin Guo", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Jiejun Tan", "Yanbin Yin", "Jiongnan Liu", "Zeyu Zhang", "Zhongxiang Sun", "Yutao Zhu", "Hao Sun", "Boci Peng", "Zhenrong Cheng", "Xuanbo Fan", "Jiaxin Guo", "Xinlei Yu", "Zhenhong Zhou", "Zewen Hu", "et al. (22 additional authors not shown)"], "title": "Memory in the Age of AI Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "AI": {"tldr": "本文旨在提供当前代理记忆研究的全景图，通过统一的形式、功能和动态视角来审视代理记忆。", "motivation": "现有的关于代理内存的研究在动机、实现和评估协议方面存在显著差异，并且由于术语定义松散而进一步模糊了概念清晰度。传统分类如长/短期记忆不足以捕捉当代代理记忆系统的多样性。", "method": "文章首先明确了代理记忆的范围并将其与其他相关概念区分开来，接着从形式、功能和动态三个视角审视了代理记忆系统，并总结了记忆基准和开源框架，展望未来研究方向。", "result": "该工作提供了一个统一的形式、功能和动态分类方法，明确区分了不同类型的代理内存实现和技术细节。它还提供了现有代理内存系统的全面概述以及对未来研究的建议。", "conclusion": "本文不仅作为当前工作的参考，也作为一个概念基础，重新思考记忆在设计未来智能中的地位。"}}
{"id": "2512.13561", "pdf": "https://arxiv.org/pdf/2512.13561", "abs": "https://arxiv.org/abs/2512.13561", "authors": ["Li-Wei Shih", "Ruo-Syuan Mei", "Jesse Heidrich", "Hui-Ping Wang", "Joel Hooton", "Joshua Solomon", "Jorge Arinez", "Guangze Li", "Chenhui Shao"], "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments", "categories": ["cs.RO"], "comment": "Submitted to the 54th SME North American Manufacturing Research Conference (NAMRC 54)", "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.", "AI": {"tldr": "本文提出了一种三级近场感知框架，以增强自主移动机器人在制造环境中的安全性。", "motivation": "现有的距离传感器如LiDAR和超声波设备无法准确检测靠近机器人基座的小型障碍物，因此需要一种新的方法来提高自主移动机器人的安全操作能力。", "method": "本文提出了一种基于激光断点检测、激光位移测量以及嵌入式AI硬件上运行的计算机视觉对象识别模型的方法，形成三级近场感知框架。", "result": "实验评估表明，所提出的层次结构在精度、计算量和成本之间取得了平衡，从而为制造环境中的自主移动机器人提供了可扩展的感知解决方案。", "conclusion": "该方法能够在低成本嵌入式系统上实现高速运行，并有效提升AMR的安全性。"}}
{"id": "2512.13560", "pdf": "https://arxiv.org/pdf/2512.13560", "abs": "https://arxiv.org/abs/2512.13560", "authors": ["Shun Maeda", "Chunzhi Gu", "Koichiro Kamide", "Katsuya Hotta", "Shangce Gao", "Chao Zhang"], "title": "3D Human-Human Interaction Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.", "AI": {"tldr": "该论文提出了一种新的任务，即人类互动异常检测（H2IAD），旨在识别合作的三维人体动作中的异动行为。为此，提出了交互异常检测网络（IADNet）。", "motivation": "当前的人体异常检测模型主要关注单个人的行为，而对于两个人之间的复杂、不对称动态则处理不佳。因此，提出了一种新的任务来解决人类互动行为的异常识别问题。", "method": "方法包括设计了时间注意力共享模块（TASM）和基于距离的关系编码模块（DREM），通过这些模块可以更好地捕捉人类交互中的时空特征，并使用归一化流进行异常评分。", "result": "实验结果表明，IADNet在人类互动动作基准上的表现优于现有的单个人体检测基线模型。", "conclusion": "该研究提出了一个新颖的任务和方法来解决人类之间协作行为的异常检测问题。"}}
{"id": "2512.13559", "pdf": "https://arxiv.org/pdf/2512.13559", "abs": "https://arxiv.org/abs/2512.13559", "authors": ["Gibson Nkhata", "Uttamasha Anjally Oyshi", "Quan Mai", "Susan Gauch"], "title": "Verifying Rumors via Stance-Aware Structural Modeling", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "8 pages, 2 figures, published in The 24th IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025), London, UK, 2025", "summary": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.", "AI": {"tldr": "本文提出了一个基于立场的结构性模型，旨在通过捕捉语义内容、立场信息和对话结构来验证社交媒体上的谣言。", "motivation": "现有的模型难以同时捕捉语义内容、立场信息以及对话结构，并且在序列长度限制下效果不佳。因此需要一种新的方法来更好地利用这些信息来确定谣言的真伪。", "method": "引入了基于立场的结构性建模，通过考虑每个帖子的位置信号和按立场类别聚合回复嵌入的方式，构建了一个可扩展且语义丰富的对话整体表示。此外还加入了立场分布和层级深度作为协变量，以捕捉立场不平衡及回复深度的影响。", "result": "在基准数据集上的实验表明，所提出的方法能够显著优于先前的模型，在预测谣言的真实性方面表现更佳，并展示了其在早期检测和跨平台泛化方面的灵活性。", "conclusion": "通过改进现有模型无法有效处理的问题，该方法不仅提高了验证社交媒体上谣言的能力，还证明了其在不同任务场景中的应用潜力。"}}
{"id": "2512.13534", "pdf": "https://arxiv.org/pdf/2512.13534", "abs": "https://arxiv.org/abs/2512.13534", "authors": ["Marianne Rakic", "Siyu Gai", "Etienne Chollet", "John V. Guttag", "Adrian V. Dalca"], "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes", "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.", "AI": {"tldr": "介绍了一种框架Pancakes，该框架能自动产生多种协议下的多标签分割图，并在不同图像间保持语义一致性。", "motivation": "现有的自动分割模型要么仅支持单一协议，要么需要手动指定所需分割方式。Pancakes旨在解决这些问题，提高跨医学领域的图像分割效率和准确性。", "method": "通过引入新的问题定义，Pancakes框架能够在给定的新图像上自动生成多种可能的分割方案，并保持这些分割结果在相关图像间的语义一致性。", "result": "实验表明，与现有基础模型相比，该模型在多个保留数据集上的表现显著提升，能够生成多个合理的整体分割图并确保不同图像间语义的一致性。", "conclusion": "Pancakes框架为跨医学领域的多协议一致图像分割提供了一种有效解决方案，并展示了其优越的性能和应用前景。"}}
{"id": "2512.13514", "pdf": "https://arxiv.org/pdf/2512.13514", "abs": "https://arxiv.org/abs/2512.13514", "authors": ["Aman Arora", "Matteo El-Hariry", "Miguel Olivares-Mendez"], "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM", "categories": ["cs.RO"], "comment": "Presented at AI4OPA Workshop at the International Conference on Space Robotics (iSpaRo) 2025 at Sendai, Japan", "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.", "AI": {"tldr": "基于强化学习的微重力环境中的自由飞行器6自由度对接仿真研究。", "motivation": "自主自由飞行器在国际空间站内部任务中起关键作用，但其精准对接面临传感噪声、小推力偏差和环境变化等挑战。本工作旨在通过强化学习框架解决这些问题。", "method": "使用Proximal Policy Optimization（PPO）算法训练控制器，在具有随机化动力学和限制观察噪音的环境中进行评估，并模拟推进器阻力扭矩效应及极性结构，研究推进物理对对接性能的影响。", "result": "所学策略在不同条件下实现了稳定的可靠对接，为未来扩展至碰撞感知导航、安全RL、推力精确仿真到实际转移以及基于视觉的端到端对接打下基础。", "conclusion": "强化学习方法成功应用于微重力环境下的自由飞行器自主对接任务，展示了其解决复杂动力学问题的能力，并为进一步研究提供了方向。"}}
{"id": "2512.13511", "pdf": "https://arxiv.org/pdf/2512.13511", "abs": "https://arxiv.org/abs/2512.13511", "authors": ["Piyush Bagad", "Andrew Zisserman"], "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding", "categories": ["cs.CV", "cs.IR"], "comment": "18 Pages. Project page at http://bpiyush.github.io/tara-website", "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.", "AI": {"tldr": "构建一个用于检索的时间感知视频文本嵌入模型。", "motivation": "开发一种简单且高效的多模态LLM适应方法，使其能够处理时间感知的视频理解任务，并提出新的评估基准来测试其性能。", "method": "通过TARA技术，在不使用任何视频数据的情况下将多模态语言模型调整为时间感知视频文本嵌入模型。此外还提出了一个新的基准来评价时间感知性。", "result": "TARA在提出的挑战性基准上表现优于所有现有视频-文本模型，并且在标准基准测试中也表现出色，同时显示出额外的优点如否定意识和动词及副词理解的优越性。", "conclusion": "TARA提供了一种强大而多功能的时间感知视频文本嵌入方法，在零样本性能方面达到最佳水平。"}}
{"id": "2512.13510", "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "categories": ["cs.AI"], "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.", "AI": {"tldr": "提出MedCEG框架，通过关键证据图监督医疗语言模型的推理过程，提高其临床可靠性。", "motivation": "现有医学推理系统在训练中忽略了准确性与有效性，导致临床可靠性和可信度不足。为了增强医学推理的质量和可验证性，提出了MedCEG。", "method": "构建了一个包含挑战性病例的数据集，并通过算法生成每个样本的关键证据图来表示高质量的可验证推理路径；引入了临床推理过程奖励机制，评估节点覆盖、结构正确性和链完整性以提升整体推理质量。", "result": "实验结果显示，MedCEG在性能上超过了现有方法，并能产生临床有效的推理链条，代表了一种可靠的医学AI推理技术的进步。", "conclusion": "通过引入关键证据图监督和奖励机制，MedCEG框架有效地提升了医疗语言模型的临床可靠性和推理质量。"}}
{"id": "2512.13507", "pdf": "https://arxiv.org/pdf/2512.13507", "abs": "https://arxiv.org/abs/2512.13507", "authors": ["Heyi Chen", "Siyan Chen", "Xin Chen", "Yanfei Chen", "Ying Chen", "Zhuo Chen", "Feng Cheng", "Tianheng Cheng", "Xinqi Cheng", "Xuyan Chi", "Jian Cong", "Jing Cui", "Qinpeng Cui", "Qide Dong", "Junliang Fan", "Jing Fang", "Zetao Fang", "Chengjian Feng", "Han Feng", "Mingyuan Gao", "Yu Gao", "Dong Guo", "Qiushan Guo", "Boyang Hao", "Qingkai Hao", "et al. (171 additional authors not shown)"], "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "categories": ["cs.CV"], "comment": "Seedance 1.5 pro Technical Report", "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "AI": {"tldr": "本文介绍了Seedance 1.5 pro，一种专门用于原生音视频联合生成的基础模型。", "motivation": "近年来，视频生成技术取得了重大进展，推动了统一的音视频生成。为此，我们开发了一种新的基础模型来实现高质量的音视频同步和生成。", "method": "该模型采用双分支Diffusion Transformer架构，并结合跨模态联合模块和专门的多阶段数据流水线以提高音视频同步性和生成质量。此外，通过在高质量数据集上进行监督微调（SFT）以及使用多维度奖励模型的人类反馈强化学习（RLHF），进一步优化了训练。", "result": "Seedance 1.5 pro实现了精确的多种语言和方言唇部同步、动态电影摄影机控制和增强的故事连贯性，极大地提高了生成的质量。该框架还加速了推理速度超过10倍。", "conclusion": "Seedance 1.5 pro是一个强大的工具，能够支持专业级内容创作，并且已经在Volcano Engine上公开可用。"}}
{"id": "2512.13505", "pdf": "https://arxiv.org/pdf/2512.13505", "abs": "https://arxiv.org/abs/2512.13505", "authors": ["Henry Prakken", "Wijnand van Woerkom"], "title": "Defending the Hierarchical Result Models of Precedential Constraint", "categories": ["cs.AI"], "comment": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems", "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.", "AI": {"tldr": "回应了Bench-Capon对van Woerkom的基于结果的分层模型的批评，提出了修正方法。", "motivation": "解决Bench-Capon提出的关于分层案例推理模型可能导致不正确结论的问题。", "method": "通过将中间因素解释为维度，并应用van Woerkom的维度基础版本来处理示例问题。", "result": "该方法可以避免Bench-Capon的批评，提供正确的法律结果。", "conclusion": "修正后的分层结果模型能够更好地反映不同基线因素对中间因素的不同影响。"}}
{"id": "2512.13501", "pdf": "https://arxiv.org/pdf/2512.13501", "abs": "https://arxiv.org/abs/2512.13501", "authors": ["Sabrine Ennaji", "Elhadj Benkhelifa", "Luigi Vincenzo Mancini"], "title": "Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment. To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations. We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.", "AI": {"tldr": "提出了一种针对黑盒对抗攻击的轻量级防御机制，即自适应特征污染方法。", "motivation": "现有的防御措施存在特定攻击类型针对性强、依赖内部模型访问或静态机制等问题，并且可能损害检测系统的性能。因此需要一种能够应对各种攻击策略的通用防御方法。", "method": "通过引入动态和上下文感知扰动，选择性地污染选定的流量特征来破坏攻击者的反馈循环而不影响检测能力。该方法利用流量分析、变化点检测和自适应调整技术来确定并处理可能被攻击者利用的特征。", "result": "评估结果表明，该防御机制能够混淆攻击者，降低攻击效果，并保持检测性能。", "conclusion": "Adaptive Feature Poisoning提供了一种通用、非特定于攻击类型且不可检测的防御方法，为机器学习基础的安全系统提供了实践和强大的对抗鲁棒性。"}}
{"id": "2512.13500", "pdf": "https://arxiv.org/pdf/2512.13500", "abs": "https://arxiv.org/abs/2512.13500", "authors": ["Li Qiwei", "Katelyn Kennon", "Nicole Bedera", "Asia A. Eaton", "Eric Gilbert", "Sarita Schoenebeck"], "title": "Platforms as Crime Scene, Judge, and Jury: How Victim-Survivors of Non-Consensual Intimate Imagery Report Abuse Online", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Non-consensual intimate imagery (NCII), also known as image-based sexual abuse (IBSA), is mediated through online platforms. Victim-survivors must turn to platforms to collect evidence and request content removal. Platforms act as the crime scene, judge, and jury, determining whether perpetrators face consequences and if harmful material is removed. We present a study of NCII victim-survivors' online reporting experiences, drawing on trauma-informed interviews with 13 participants. We find that platform reporting processes are hostile, opaque, and ineffective, often forcing complex harms into narrow interfaces, responding inconsistently, and failing to result in meaningful action. Leveraging institutional betrayal theory, we show how platforms' structures and practices compound harm, and, in doing so, surface concrete intervention points for redesigning reporting systems and shaping policy to better support victim-survivors", "AI": {"tldr": "研究探讨了非同意亲密影像受害者通过在线平台报告虐待的经历", "motivation": "分析在线平台处理非同意亲密影像（NCII）时存在的问题，以及这些问题如何进一步伤害受害者", "method": "通过对13名受害者的创伤知情访谈进行研究", "result": "发现平台的举报过程通常是敌对、不透明且无效的，常将复杂的问题简化为狭窄的界面，并无法有效解决问题", "conclusion": "提出基于机构背叛理论的观点，建议重新设计报告系统和制定政策以更好地支持受害者"}}
{"id": "2512.13497", "pdf": "https://arxiv.org/pdf/2512.13497", "abs": "https://arxiv.org/abs/2512.13497", "authors": ["Haoyu Ren", "Kay Koehle", "Kirill Dorofeev", "Darko Anicic"], "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by European Conference on EDGE AI Technologies and Applications (EEAI) 2025", "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.", "AI": {"tldr": "本文研究了在动态制造环境中基于设备的持续学习方法，用于无监督视觉异常检测。", "motivation": "现代制造业中视觉异常检测对于自动化检查和产品质量至关重要。然而，灵活多变的生产环境提出了挑战：产品频繁变更需要快速模型更新；边缘硬件资源有限无法支持大规模AI模型训练；新产品的正常与异常数据稀缺。", "method": "本文提出了一种基于PatchCore的方法并引入在线学习机制，利用轻量级特征提取器和增量核心集更新机制（基于k中心选择），实现从少量数据中快速、内存高效地进行适应性调整，避免了昂贵的云重训练过程。", "result": "在模拟频繁产品变化的真实工业场景中的测试表明，所提方法相比基线模型AUROC提高了12%，内存使用量减少了80%，并且训练速度更快。", "conclusion": "该研究证明了提出的方法对于动态和智能制造业中准确、资源高效且适应性强的视觉异常检测是有效的。"}}
{"id": "2512.13495", "pdf": "https://arxiv.org/pdf/2512.13495", "abs": "https://arxiv.org/abs/2512.13495", "authors": ["Jiangning Zhang", "Junwei Zhu", "Zhenye Gan", "Donghao Luo", "Chuming Lin", "Feifan Xu", "Xu Peng", "Jianlong Hu", "Yuansen Liu", "Yijia Hong", "Weijian Cao", "Han Feng", "Xu Chen", "Chencan Fu", "Keke He", "Xiaobin Hu", "Chengjie Wang"], "title": "Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation", "categories": ["cs.CV"], "comment": "Project page: https://zhangzjn.github.io/projects/Soul/", "summary": "We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/", "AI": {"tldr": "提出了一种用于生成高质量长期多模态动画的框架Soul。", "motivation": "为了实现从单帧图像、文本提示和音频驱动下生成语义连贯且高度逼真的数字人类动画，解决数据稀缺问题并优化长时一致性及推断效率。", "method": "构建了包含100万精细标注样本的数据集Soul-1M，并通过集成Wan2.2-5B骨干网络、音频注入层和多种训练策略来生成一致且高质量的长期动画。同时使用轻量级VAE优化推理速度，保持质量的同时提高效率。", "result": "实验表明，Soul在视频质量、文本对齐度、身份保存和口型同步准确率方面优于现有开源和商业模型。", "conclusion": "Soul框架展示了在虚拟主播和电影制作等实际场景中的广泛应用潜力。"}}
{"id": "2512.13494", "pdf": "https://arxiv.org/pdf/2512.13494", "abs": "https://arxiv.org/abs/2512.13494", "authors": ["Yu-Chen Lu", "Sheng-Feng Yu", "Hui-Hsien Weng", "Pei-Shuo Wang", "Yu-Fang Hu", "Liang Hung-Chun", "Hung-Yueh Chiang", "Kai-Chiang Wu"], "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by AAAI 2026", "summary": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.", "AI": {"tldr": "提出了一种新的低秩压缩框架SkipCat，该框架通过共享投影和块跳过技术，在相同的压缩率下保留更多有效等级数。", "motivation": "大型语言模型在边缘设备上的部署面临计算资源和内存资源的限制。低秩压缩方法虽然可以减少这些开销，但通常需要大幅降低保持秩来实现效率提升，从而导致性能下降。", "method": "引入了一种跨层共享低秩投影的方法，并提出块跳过技术以省略选定子块的计算与内存传输。这两种技术使模型在相同压缩率下保留更多有效秩。", "result": "实验结果显示，在不进行额外微调的情况下，该方法在零样本任务中比现有的低秩压缩方式提高了7％的准确性。", "conclusion": "SkipCat框架通过最大化保持的有效等级数来保存模型性能，并且即使是在资源受限环境中也能表现良好。"}}
{"id": "2512.13492", "pdf": "https://arxiv.org/pdf/2512.13492", "abs": "https://arxiv.org/abs/2512.13492", "authors": ["Jiangning Zhang", "Junwei Zhu", "Teng Hu", "Yabiao Wang", "Donghao Luo", "Weijian Cao", "Zhenye Gan", "Xiaobin Hu", "Zhucun Xue", "Chengjie Wang"], "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$", "categories": ["cs.CV"], "comment": "Project page: https://zhangzjn.github.io/projects/T3-Video", "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video", "AI": {"tldr": "本文提出了一种名为T3的新Transformer改造策略，通过优化前向逻辑显著减少计算需求，加速原生4K视频生成。", "motivation": "由于全注意力机制的二次计算爆炸性增长，随着时空分辨率的增加，原生4K（2160x3840）视频生成成为了一个关键挑战。模型难以在效率和质量之间达到平衡。", "method": "T3策略通过引入多尺度权重共享窗口注意机制，并结合层次化阻塞及轴保持全注意力设计，在不改变原有预训练模型核心架构的情况下，实现了“注意模式”转换。", "result": "实验结果表明，T3-Video在4K-VBench上表现出色：与现有方法相比，性能提升（+4.29上升VQA和+0.08上升VTC），同时原生4K视频生成加速超过10倍。", "conclusion": "T3策略通过优化计算逻辑显著提升了模型的效率和性能，在保持预训练模型架构的同时实现了高效的4K视频生成。"}}
{"id": "2512.13481", "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.", "AI": {"tldr": "研究大型语言模型在多代理设置中的嫉妒行为", "motivation": "评估大型语言模型是否会在人类合作与竞争工作中表现出类似嫉妒的行为，以更好地设计和确保这些系统的安全性。", "method": "通过两个场景测试LLM之间的嫉妒倾向：点分配游戏和工作场所不公平认可的情境。", "result": "发现某些LLM在特定情况下展现出明显的嫉妒行为，如降低同伴模型的表现来使结果平等，而其他模型则专注于最大化自身收益。", "conclusion": "研究强调了在多代理系统中考虑竞争态度作为安全性和设计因素的重要性。"}}
{"id": "2512.13478", "pdf": "https://arxiv.org/pdf/2512.13478", "abs": "https://arxiv.org/abs/2512.13478", "authors": ["Kei Saito"], "title": "Non-Resolution Reasoning (NRR): A Computational Framework for Contextual Identity and Ambiguity Preservation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "7 pages, 2 figures, ORCID: 0009-0006-4715-9176", "summary": "Current artificial intelligence systems, despite remarkable capabilities in text generation and pattern recognition, exhibit a fundamental architectural limitation: they resolve ambiguity prematurely. This premature semantic collapse -- the tendency to collapse multiple valid interpretations into a single output -- stems from classical identity assumptions embedded in standard neural architectures. We propose Non-Resolution Reasoning (NRR), a computational framework that treats ambiguity retention as a valid reasoning mode rather than a defect to be eliminated. NRR introduces three core principles: (1) Non-Identity (A $\\ne$ A) -- the same symbol refers to different entities across contexts; (2) Approximate Identity (A $\\approx$ A) -- entities share partial structural overlap without being identical; and (3) Non-Resolution -- conflicting interpretations can coexist without forced convergence. We formalize these principles through three architectural components: Multi-Vector Embeddings for context-dependent representation, Non-Collapsing Attention for parallel interpretation retention, and Contextual Identity Tracking (CIT) for maintaining A $\\ne$ A across inference. We demonstrate NRR's advantages through case studies in paradox handling, creative generation, and context-dependent reasoning. Crucially, we provide a minimal empirical validation on a synthetic context-shift task where an NRR-lite model achieves 90.9% out-of-distribution accuracy compared to 9.1% for standard architectures, demonstrating that ambiguity preservation enables structural generalization. NRR challenges the assumption that meaning must collapse to be useful, offering a foundation for AI systems capable of sophisticated ambiguity handling and creative reasoning. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "AI": {"tldr": "提出了一种计算框架NRR，用于在AI系统中保留歧义和上下文身份。", "motivation": "现有的人工智能系统由于标准神经架构中的经典同一性假设，在处理文本生成和模式识别时过早地消解了歧义。作者旨在通过引入新的原则和架构来解决这一问题。", "method": "NRR框架基于三个核心原则：非同一性，近似同一性和非消除，并通过多向量嵌入，非折叠注意力机制和上下文身份跟踪组件实现。", "result": "在合成的上下文转换任务中，简化的NRR模型达到了90.9%的分布外准确率，相比之下标准架构仅为9.1%，表明保留歧义能够促进结构泛化能力。", "conclusion": "NRR挑战了意义必须消解才能有用的假设，为AI系统提供了处理复杂歧义和创造性推理的能力基础。"}}
{"id": "2512.13477", "pdf": "https://arxiv.org/pdf/2512.13477", "abs": "https://arxiv.org/abs/2512.13477", "authors": ["Timothy A. Brumfiel", "Revanth Konda", "Drew Elliott", "Jaydev P. Desai"], "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model", "categories": ["cs.RO", "eess.SY"], "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.", "AI": {"tldr": "评估改进后的COAST导丝机器人的导航能力", "motivation": "解决手动操作导丝在血管内介入手术中遇到的问题，提高机器人可控导丝的操控性和导航性。", "method": "通过在具有脉动流的解剖模型中进行导航实验来评价改进后的COAST导丝机器人的性能。该改进版本为两管而非三管结构。", "result": "证明了改进后的COAST导丝机器人能够在曲折的人体血管模内有效导航。", "conclusion": "研究展示了改进版的COAST导丝机器人在模拟人体解剖模型中的导航能力，证实其具有潜在的应用价值。"}}
{"id": "2512.13465", "pdf": "https://arxiv.org/pdf/2512.13465", "abs": "https://arxiv.org/abs/2512.13465", "authors": ["Ruiyan Wang", "Teng Hu", "Kaihui Huang", "Zihan Su", "Ran Yi", "Lizhuang Ma"], "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence", "categories": ["cs.CV"], "comment": null, "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.", "AI": {"tldr": "PoseAnything是一种通用的姿势引导视频生成框架，可以处理人类和非人类角色，并引入了部分感知时间一致性模块和独立相机运动控制策略。", "motivation": "当前姿势引导视频生成方法仅限于输入人体姿势，难以泛化到其他对象。为解决这一问题，本文提出了PoseAnything以实现更广泛的适用性和更好的一致性能。", "method": "该框架使用部分感知时间一致性模块来增强动态中的保真度，并通过主体和相机运动解耦CFG策略提供独立的相机控制能力。此外还创建了XPose数据集。", "result": "实验结果显示，PoseAnything在效果和泛化性方面显著优于现有方法。", "conclusion": "通过引入新的模块和技术，PoseAnything成功地扩展了姿势引导视频生成的应用范围，并提高了整体性能。"}}
{"id": "2512.13458", "pdf": "https://arxiv.org/pdf/2512.13458", "abs": "https://arxiv.org/abs/2512.13458", "authors": ["Yici Liu", "Qi Wei Oung", "Hoi Leong Lee"], "title": "SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.", "AI": {"tldr": "通过源选择与对抗策略进行跨受试者EEG情绪识别的研究", "motivation": "解决跨受试者EEG情绪识别中的个体差异和负向迁移问题", "method": "提出了一种包含源选择网络（SS）和对抗策略网络（AS）的方法，采用域标签逆向重构训练过程以增强领域不变性同时保持情绪相关特征的表示能力", "result": "在SEED和SEED-IV两个EEG情绪数据集上表现出色", "conclusion": "所提出的方法有效地提高了跨受试者EEG情绪识别的性能"}}
{"id": "2512.13454", "pdf": "https://arxiv.org/pdf/2512.13454", "abs": "https://arxiv.org/abs/2512.13454", "authors": ["Arpit Jadon", "Joshua Niemeijer", "Yuki M. Asano"], "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.", "AI": {"tldr": "本文提出了一种在测试时使用扩散模型将目标图像映射回源分布的方法，以提高领域泛化任务的性能。", "motivation": "现有方法通过生成多样化的目标域图像来增强领域泛化，但这种方法效率低且不全面。因此，研究者希望找到一种更高效、成本更低的方式来提升模型在未知目标领域的表现。", "method": "本文提出了一种测试时逆向领域变换的方法，该方法利用扩散模型将目标图像转换回源域分布，并在此基础上进行任务预测。", "result": "实验结果显示，在具有挑战性的环境变化下，所提方法分别实现了BDD100K-Night提升137%，ImageNet-R提升68%和DarkZurich提升62%的相对增益。", "conclusion": "该方法通过逆向领域变换增强了模型在未知目标领域的泛化能力，并且这种方法仅需源域描述，不需要大规模合成数据生成。"}}
{"id": "2512.13440", "pdf": "https://arxiv.org/pdf/2512.13440", "abs": "https://arxiv.org/abs/2512.13440", "authors": ["Thalyssa Baiocco-Rodrigues", "Antoine Olivier", "Reda Belbahri", "Thomas Duboudin", "Pierre-Antoine Bannier", "Benjamin Adjadj", "Katharina Von Loga", "Nathan Noiry", "Maxime Touzot", "Hector Roux de Bezieux"], "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images", "categories": ["cs.CV"], "comment": null, "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.", "AI": {"tldr": "本文介绍了IMILIA，一个用于预测炎症状态的端到端框架，并通过组织切片计算标志物。", "motivation": "为了准确评估IBD患者的疾病活动和治疗反应，需要对微观炎症进行精确测量。为此开发了IMILIA系统以实现自动化分析。", "method": "IMILIA包含一个预测模块（使用MIL模型）以及两个解释性模块：HistoPLUS用于细胞检测、分割及分类；EpiSeg用于上皮组织的分割。", "result": "在发现队列中，IMILIA实现了0.83的交叉验证ROC-AUC，在两组外部验证数据集中分别为0.99和0.84。解释性模块显示了具有生物一致性洞察力：高预测分数的切片显示出更多的免疫细胞密度，而低分切片则主要包含正常上皮细胞。", "conclusion": "IMILIA提供了一种准确预测IBD炎症的方法，并通过自动化计算相关标志物为临床决策提供了支持。"}}
{"id": "2512.13438", "pdf": "https://arxiv.org/pdf/2512.13438", "abs": "https://arxiv.org/abs/2512.13438", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "AI": {"tldr": "该论文提出了UIFormer，一种优化LLM代理界面表示效率的框架。", "motivation": "由于缺乏布尔预言机和处理复杂UI树输入的需求，导致自动生成UI转换程序以提高LLM代理效率面临重大挑战。", "method": "UIFormer采用结构化分解的方法将复杂的合成任务拆解，并使用特定于领域的语言限制程序空间。同时通过基于LLM的迭代改进来优化正确性和效率。", "result": "在三个跨平台界面导航基准测试中，UIFormer实现了48.7%到55.8%的标记减少，同时保持或提高了代理性能，且运行时开销最小。", "conclusion": "UIFormer作为一个轻量级插件成功优化了LLM代理的效率，并在WeChat中得到了实际应用验证。"}}
{"id": "2512.13434", "pdf": "https://arxiv.org/pdf/2512.13434", "abs": "https://arxiv.org/abs/2512.13434", "authors": ["Youssef Megahed", "Inok Lee", "Robin Ducharme", "Kevin Dick", "Adrian D. C. Chan", "Steven Hawken", "Mark C. Walker"], "title": "Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages, 8 figures, 4 tables", "summary": "Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.", "AI": {"tldr": "该论文提出了一种基于自监督学习的超声图像表示方法，用于胎儿肾脏异常自动分类。", "motivation": "当前产前超声诊断依赖于操作者，并且在成像条件不佳的情况下效果受限。本文旨在评估一种基于自监督学习的预训练模型USF-MAE，在胎儿肾脏异常检测中的性能表现。", "method": "使用Masked Autoencoding技术，对一个包含969张二维超声图像的数据集进行预训练和微调。与DenseNet-169基线模型比较，评估其在二分类及多分类任务上的性能。", "result": "USF-MAE相比于基线模型，在验证集上AUC提高了约1.87%，F1-score提高了7.8%；在独立测试集上分别提高2.32%AUC和4.33%F1-score。特别是在多分类任务中，AUC提升达16.28%，F1-score提升了46.15%。", "conclusion": "研究结果表明，基于自监督学习的超声图像表示方法可以作为胎儿肾脏异常检测的有效基础模型，并且该框架具有良好的可解释性。这为产前检测提供了坚实的支持，并展示了在妇产科成像中使用基础模型的巨大潜力。"}}
{"id": "2512.13428", "pdf": "https://arxiv.org/pdf/2512.13428", "abs": "https://arxiv.org/abs/2512.13428", "authors": ["Anika Islam", "Tasfia Tahsin", "Zaarin Anjum", "Md. Bakhtiar Hasan", "Md. Hasanul Kabir"], "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.", "AI": {"tldr": "提出了一个轻量级的领域自适应集成框架，用于资源受限环境下的植物病害分类。", "motivation": "准确及时地识别植物叶病对于农业可持续发展至关重要。但是大多数深度学习方法依赖于大型标注数据集和计算密集型模型，在数据稀缺且资源有限的环境中不合适。", "method": "该论文结合领域适应的MobileNetV2与MobileNetV3作为特征提取器，并引入融合技术生成鲁棒特征表示，同时使用Bi-LSTM分类器增强注意力机制以捕捉序列依赖关系并专注于相关特征。", "result": "在PlantVillage数据集上实现了98.23%至99.72%的精度，在复杂背景下维持了69.28%-1.49%的性能，证明了该框架的有效性。模型大小约为40MB，推理复杂度为约1.12GFLOPs。", "conclusion": "论文提出的轻量级、可移植性强的方法在数据稀缺且资源受限的环境中具有广泛应用前景，提高了植物病害诊断精度和鲁棒性。"}}
{"id": "2512.13427", "pdf": "https://arxiv.org/pdf/2512.13427", "abs": "https://arxiv.org/abs/2512.13427", "authors": ["Noa Cohen", "Nurit Spingarn-Eliezer", "Inbar Huberman-Spiegelglas", "Tomer Michaeli"], "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "categories": ["cs.CV", "cs.LG"], "comment": "Code and examples are available on the project's webpage at https://noa-cohen.github.io/MineTheGap/", "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.", "AI": {"tldr": "MineTheGap是一种自动挖掘文本到图像模型中偏见的方法。", "motivation": "TTI模型在处理模糊性时表现出的偏差可能对社会产生影响，也可能影响用户体验。因此，需要一种方法来自动检测和揭示这些偏见。", "method": "使用遗传算法迭代优化提示集以发现导致TTI模型生成偏见图像的提示，并通过比较生成图像与LLM生成文本的变化分布来计算新颖的偏差分数。", "result": "该方法能够有效识别并排序不同类型的偏差，证明了其在揭示和量化TTI模型中潜在偏差方面的有效性。", "conclusion": "MineTheGap提供了一种新的工具集和技术框架，用于系统地分析和缓解文本到图像生成中的偏见问题。"}}
{"id": "2512.13421", "pdf": "https://arxiv.org/pdf/2512.13421", "abs": "https://arxiv.org/abs/2512.13421", "authors": ["Qingyu Shi", "Size Wu", "Jinbin Bai", "Kaidong Yu", "Yujing Wang", "Yunhai Tong", "Xiangtai Li", "Xuelong Li"], "title": "RecTok: Reconstruction Distillation along Rectified Flow", "categories": ["cs.CV"], "comment": null, "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.", "AI": {"tldr": "该论文提出了一种名为RecTok的方法，旨在解决高维度视觉标记器的性能问题。", "motivation": "目前大多数方法受限于低维潜在空间以保证生成质量，然而高维度视觉标记器在实际应用中仍表现不佳。作者通过引入流语义蒸馏和重构对齐蒸馏来提高高维度视觉标记器的表现。", "method": "该方法利用VFMs中的语义信息丰富流动匹配中的前向流动轨迹，并通过添加掩码特征重构损失进一步增强语义表达能力。", "result": "RecTok在图像重建、生成质量和判别性能方面取得了显著成果，同时保持了丰富的潜在空间结构。此外，在不同条件下均达到了gFID-50K的最佳表现。", "conclusion": "通过创新性的流匹配和重构对齐蒸馏技术，RecTok有效提升了高维度视觉标记器的性能，并在多个实验中表现出优越性。"}}
{"id": "2512.13416", "pdf": "https://arxiv.org/pdf/2512.13416", "abs": "https://arxiv.org/abs/2512.13416", "authors": ["Haoxuan Qu", "Qiuchi Xiang", "Yujun Cai", "Yirui Wu", "Majid Mirmehdi", "Hossein Rahmani", "Jun Liu"], "title": "Learning to Generate Cross-Task Unexploitable Examples", "categories": ["cs.CV"], "comment": null, "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "AI": {"tldr": "提出了一种用于生成跨任务不可利用样本的元框架MCT-UEG，以保护个人数据隐私。", "motivation": "针对现有方法在生成广泛适用的不可利用样例上的局限性，旨在通过新的元训练和测试方案来优化生成器。", "method": "设计了一个扁平最小值导向的元学习训练和测试方案，用于优化生成跨任务不可利用样本的过程。", "result": "实验显示了所提框架的有效性。", "conclusion": "MCT-UEG能有效地生成广泛适用的不可利用样例，保护个人数据隐私。"}}
{"id": "2512.13415", "pdf": "https://arxiv.org/pdf/2512.13415", "abs": "https://arxiv.org/abs/2512.13415", "authors": ["Ahmed Abul Hasanaath", "Hamzah Luqman"], "title": "USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM", "AI": {"tldr": "该论文提出了USTM框架，用于连续手语识别（CSLR），以精确地建模视频中的手势序列。", "motivation": "现有方法依赖于CNN结合时序模块来捕捉细粒度的手部和面部线索以及长时间的时序依赖关系，但效果不佳。为了克服这些限制，该论文提出了USTM框架。", "method": "USTM框架使用Swin Transformer骨干网络增强轻量级时间适配器与位置嵌入（TAPE），同时捕获细粒度的空间特征和短期及长期的时间上下文。", "result": "实验表明，USTM在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上优于RGB基础方法和其他多模态CSLR方法，并且与多流方法具有竞争力。", "conclusion": "该论文展示了USTM框架的强大功能及其用于连续手语识别的有效性。代码可在GitHub上找到。"}}
{"id": "2512.13411", "pdf": "https://arxiv.org/pdf/2512.13411", "abs": "https://arxiv.org/abs/2512.13411", "authors": ["Patryk Niżeniec", "Marcin Iwanowski"], "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting", "categories": ["cs.CV", "cs.GR"], "comment": "Code available at: https://patrykni.github.io/UnitySplat2Data/", "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.", "AI": {"tldr": "本文介绍了一种生成大规模、高度逼真的计算机视觉任务训练数据集的方法，适用于机器人环境。", "motivation": "该研究旨在解决合成图像与真实世界图像之间的领域差距以及手动注释耗时的问题。", "method": "使用3D高斯点绘技术创建操作环境和物体的逼真表示，并在游戏引擎中结合物理模拟生成自然排列。利用两步渲染技术将点绘的真实感与阴影图结合，增强图像现实性。自动生成像素完美的分割掩码并与YOLO等目标检测模型兼容。", "result": "实验表明，混合训练策略，结合少量真实图片和大量合成数据，可获得最佳的检测和分割性能。", "conclusion": "该方法证明了是实现高效、准确模型的有效途径。"}}
{"id": "2512.13402", "pdf": "https://arxiv.org/pdf/2512.13402", "abs": "https://arxiv.org/abs/2512.13402", "authors": ["Lorenzo Pettinari", "Sidaty El Hadramy", "Michael Wehrli", "Philippe C. Cattin", "Daniel Studer", "Carol C. Hasler", "Maria Licci"], "title": "End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Code and interactive visualizations: https://lorenzopettinari.github.io/end-2-reg/", "summary": "Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.", "AI": {"tldr": "提出了End2Reg，一种联合优化分割和配准的端到端深度学习框架，以实现精确且自动化的脊柱手术导航。", "motivation": "当前基于放射成像和骨锚定标记的导航系统存在侵入性、辐射强度大及工作流程中断等问题。现有无标记RGB-D注册方法依赖于弱分割标签来识别解剖结构，可能导致配准误差。", "method": "提出了一种端到端深度学习框架End2Reg，该框架通过仅基于注册目标进行优化，而无需直接分割监督或弱分割标签，从而联合优化分割和配准。", "result": "在体外和体内基准测试中，所提方法将平均目标注册误差减少至1.83毫米，均方根误差降至3.95毫米，并通过消融研究验证了端到端优化对注册准确性的显著提升效果。", "conclusion": "提出的RGB-D注册流程去除了对弱标签和人工步骤的依赖，向完全自动化的无标记术中导航迈进了一步。"}}
{"id": "2512.13399", "pdf": "https://arxiv.org/pdf/2512.13399", "abs": "https://arxiv.org/abs/2512.13399", "authors": ["Sitao Cheng", "Tianle Li", "Xuhan Huang", "Xunjian Yin", "Difan Zou"], "title": "Differentiable Evolutionary Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": "Work in Progress. We release our code and model at https://github.com/sitaocheng/DERL", "summary": "The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the \"meta-gradient\" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.", "AI": {"tldr": "本文提出了一种名为可微进化强化学习(DERL)的双层框架，该框架能够自主地发现最优奖励信号。", "motivation": "在强化学习中，设计有效的奖励函数是一个关键且具有挑战性的问题。现有的自动化奖励优化方法通常依赖于不考虑奖励结构与任务性能之间因果关系的黑盒进化启发式方法。为此，本文旨在通过可微分的元优化来解决这一问题。", "method": "DERL框架由一个元优化器组成，该元优化器通过组合原子原语生成奖励函数（即元奖励），并指导内循环策略训练。与先前的进化方法不同，DERL在元优化中是可微分的：它将内循环验证性能作为更新元优化器以强化学习的方式处理。", "result": "实验结果表明，在ALFWorld和ScienceWorld领域，DERL达到了最先进的性能，并且显著优于依赖启发式奖励的方法，在分布外场景下尤其如此。进化轨迹分析显示，DERL成功地捕捉了任务的内在结构，使代理能够在无人类干预的情况下自我提升。", "conclusion": "DERL通过双层框架自主发现最优奖励信号的能力在复杂推理任务中展示了显著的优势。"}}
{"id": "2512.13397", "pdf": "https://arxiv.org/pdf/2512.13397", "abs": "https://arxiv.org/abs/2512.13397", "authors": ["Malte Silbernagel", "Albert Alonso", "Jens Petersen", "Bulat Ibragimov", "Marleen de Bruijne", "Madeleine K. Wyburd"], "title": "rNCA: Self-Repairing Segmentation Masks", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.", "AI": {"tldr": "本论文提出了一种基于神经细胞自动机（NCA）的自修复分割掩码方法，以改进一般分割模型产生的片段化或不连续输出。", "motivation": "准确预测拓扑正确的掩码对一般的分割模型来说是一个挑战，这些模型通常会产生碎片化的或不连接的输出。本研究旨在利用神经细胞自动机作为有效的修复机制，通过迭代更新来修复这些缺陷。", "method": "论文提出了一种名为rNCA的方法，该方法基于局部信息进行迭代更新以修复粗略分割掩码中的拓扑错误，并在不同任务中展示了其应用效果。", "result": "实验结果表明，在处理视网膜血管和心肌组织等分割任务时，使用rNCA可以显著提高Dice/clDice得分并减少贝蒂数误差；同时还可以降低ASSD和HD等评估指标。", "conclusion": "神经细胞自动机（NCA）展示出作为有效且广泛适用的修复器的能力。"}}
{"id": "2512.13392", "pdf": "https://arxiv.org/pdf/2512.13392", "abs": "https://arxiv.org/abs/2512.13392", "authors": ["Anran Qi", "Changjian Li", "Adrien Bousseau", "Niloy J. Mitra"], "title": "Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs", "categories": ["cs.CV"], "comment": null, "summary": "We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyond-visible.github.io/", "AI": {"tldr": "本文提出了一种通过代理动态图（PDG）实现的图像到视频生成方法，该方法允许用户控制最终帧中被遮挡区域的内容。", "motivation": "当前的方法在生成连贯的动作时难以确保预测性，并且难以在新暴露的区域内强制执行用户指定的内容。本文旨在解决这一问题，通过将动作指定与外观合成分离来提供更好的控制。", "method": "提出了一种轻量级、可编辑的代理动态图（PDG），用于确定性地驱动部分运动；使用冻结扩散先验进行外观合成；利用可见性信息编码在PDG中执行潜在空间复合，以协调运动和用户意图。", "result": "该方法能够在不进行微调的情况下实现可控的动作，并允许用户编辑被遮挡区域的外观。实验表明，在转换图像为短视频时具有明显优势。", "conclusion": "通过结合松散的姿态和结构控制与最终帧中被遮挡区域内外观指定的可预测控制，解锁了一种新的图像到视频的工作流程。"}}
{"id": "2512.13380", "pdf": "https://arxiv.org/pdf/2512.13380", "abs": "https://arxiv.org/abs/2512.13380", "authors": ["Chuan Mao", "Haoqi Yuan", "Ziye Huang", "Chaoyi Xu", "Kai Ma", "Zongqing Lu"], "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning", "categories": ["cs.RO"], "comment": "19 pages", "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.", "AI": {"tldr": "本文提出了一种名为DemoFunGrasp的方法，用于实现通用灵巧的功能性抓取，通过将功能性抓取条件分解为握持方式和功能两部分，并结合强化学习框架来改进多任务优化。", "motivation": "当前的强化学习在灵巧抓取方面取得了显著成效，但细粒度的功能性抓取仍面临目标指定、奖励函数设定及模拟到现实转换等挑战。本文旨在解决这些问题以实现通用功能性抓取。", "method": "本文采用单次抓取演示编辑强化学习问题的方法，将功能性握持分解为两种互补部分，并通过集成视觉-语言模型来改进指令跟随抓取执行。", "result": "实验结果显示，DemoFunGrasp在模拟和现实环境中均表现出色，在未见过的物体组合中具有较强的泛化能力且超越了基线方法的成功率和准确性。", "conclusion": "本文展示了如何通过演示编辑强化学习来实现通用功能性抓取，并证明其在从模拟到真实世界的转化能力以及自主指令跟随执行方面的优越性。"}}
{"id": "2512.13376", "pdf": "https://arxiv.org/pdf/2512.13376", "abs": "https://arxiv.org/abs/2512.13376", "authors": ["Carla Monteiro", "Valentina Corbetta", "Regina Beets-Tan", "Luís F. Teixeira", "Wilson Silva"], "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"", "categories": ["cs.CV"], "comment": "29 pages, 10 figures, 8 tables, under review at MIDL 2026", "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.", "AI": {"tldr": "本论文提出了利用DINO自注意力“键”特性来增强结肠息肉分割模型的泛化能力的方法。", "motivation": "现有的结肠息肉分割方法在数据受限或挑战性环境中通常难以实现良好的泛化。为解决这一问题，作者提出了一种基于DINO自注意力机制的新框架以提高模型的稳健性和性能。", "method": "不同于传统利用Vision Transformer深层提取token的方法，该论文通过使用简单卷积解码器预测息肉掩膜来直接从self-attention模块中抽取“键”特征。", "result": "实验结果表明，在多中心数据集上采用严格协议（如领域泛化和极端单域泛化）验证时，本方法达到了SOTA性能，并且在数据稀缺及挑战性条件下表现尤为突出。此外，通过基准测试量化了DINO框架演化的特定影响。", "conclusion": "此工作展示了如何利用DINO自注意力“键”特性来增强结肠息肉分割模型的泛化能力，从而避免使用特定任务架构同时超越了nnU-Net和UM-Net等现有领先方法。"}}
{"id": "2512.13374", "pdf": "https://arxiv.org/pdf/2512.13374", "abs": "https://arxiv.org/abs/2512.13374", "authors": ["Francesca Da Ros", "Luca Di Gaspero", "Kevin Roitero"], "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.", "AI": {"tldr": "研究大型语言模型在组合优化问题中的行为和表示，通过直接查询和探针分析来评估其特征提取能力和算法选择能力。", "motivation": "探讨大型语言模型是否能理解组合优化问题的结构或算法行为，以及这些模型内部如何表示这些问题。", "method": "采用双重方法结合直接查询和探针分析：直接查询用于评估语言模型提取实例特征的能力；探针分析检查隐藏层中是否存在隐含信息。进一步扩展到每个实例的算法选择任务，评估由LLM衍生的表示是否可以预测最佳求解器。", "result": "实验结果表明大型语言模型从问题实例中恢复特征信息具有适度能力，并且其隐藏层表示的预测能力与传统特征提取方法相当。", "conclusion": "研究表明大型语言模型能够捕获与优化性能相关的有意义的结构信息，为组合优化任务提供有价值的表示和支持。"}}
{"id": "2512.13363", "pdf": "https://arxiv.org/pdf/2512.13363", "abs": "https://arxiv.org/abs/2512.13363", "authors": ["Shibani Sankpal"], "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 12 figures", "summary": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.", "AI": {"tldr": "使用预训练的变压器模型检测心理健康文本中的情感漂移。", "motivation": "探讨在单个文本中情绪状态的变化，即情感漂移，在心理健康相关的消息中的重要性。传统的观点往往忽略了在一条消息内部情感变化的趋势。", "method": "采用DistilBERT和RoBERTa等预训练的变压器模型来检测句子级别的情感，并计算情感漂移分数。", "result": "结果提供了关于情绪升级或缓解的心理健康对话模式的新见解。", "conclusion": "该方法能够更好地理解内容中的情感动态变化。"}}
{"id": "2512.13361", "pdf": "https://arxiv.org/pdf/2512.13361", "abs": "https://arxiv.org/abs/2512.13361", "authors": ["Elizaveta Prozorova", "Anton Konev", "Vladimir Faerman"], "title": "Automated User Identification from Facial Thermograms with Siamese Networks", "categories": ["cs.CV", "cs.CR"], "comment": "5 pages, 2 figures, reported on 21st International Scientific and Practical Conference 'Electronic Means and Control Systems', dedicated to the 80th anniversary of radio engineering education beyond the Urals, Tomsk, 24 November 2025", "summary": "The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.", "AI": {"tldr": "本文研究了基于面部热图的生物识别技术，使用Siamese神经网络进行自动化用户识别，并评估不同红外光谱范围的有效性。", "motivation": "论文旨在探讨和分析利用热成像技术来进行生物识别的可能性，特别是在安全系统中的应用潜力。", "method": "通过对比不同类型红外光谱（近红外、短波红外、中波红外及长波红外）的性能，定义了用于生物识别系统的热像仪的关键要求，并提出使用Siamese神经网络自动化用户识别过程的方法。", "result": "在实验中，提出的算法达到了大约80%的准确率。研究还表明，结合可见光和红外光谱可以提高单一模态的限制。", "conclusion": "研究表明，热成像是开发可靠安全系统的有前途的技术。"}}
{"id": "2512.13359", "pdf": "https://arxiv.org/pdf/2512.13359", "abs": "https://arxiv.org/abs/2512.13359", "authors": ["Sümer Tunçay", "Alain Andres", "Ignacio Carlucho"], "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.", "AI": {"tldr": "论文提出了一种基于GPU加速的强化学习训练流水线，用于6自由度位置控制的自主水下航行器（AUV）的位置控制。", "motivation": "传统控制器在名义条件下有效，但在未建模动力学或环境干扰时性能下降。强化学习提供了一个强大的替代方案，但其训练通常缓慢且仿真到现实转移具有挑战性。", "method": "通过联合JIT编译大规模并行物理模拟和学习更新，引入了基于GPU加速的RL训练流水线，使用JAX和MuJoCo-XLA（MJX）。对多种RL算法进行系统评估，并在真实水下实验中展示了鲁棒的6自由度轨迹跟踪及有效的干扰拒绝。", "result": "政策从仿真零转移至现实世界，在所有六个自由度上的AUV位置控制首次有明确的真实世界演示。训练时间少于两分钟。", "conclusion": "该工作验证了RL技术在复杂动态环境中实现高效、鲁棒的6-DOF轨迹跟踪的能力，为自主水下航行器提供了新的解决方案。"}}
{"id": "2512.13356", "pdf": "https://arxiv.org/pdf/2512.13356", "abs": "https://arxiv.org/abs/2512.13356", "authors": ["Zeyad Gamal", "Youssef Mahran", "Ayman El-Badawy"], "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.", "AI": {"tldr": "本文提出了一种基于孪生延迟深度确定性策略梯度(TD3)算法的强化学习框架，用于控制和稳定双旋翼气动系统(TRAS)，使其在特定倾角和方位角度下运行并跟踪预定轨迹。", "motivation": "传统控制器难以应对TRAS复杂动态特性和非线性问题。而近年来发展的强化学习方法因其在多旋翼控制系统中的应用潜力受到关注。", "method": "使用孪生延迟深度确定性策略梯度(TD3)算法训练强化学习代理，该算法适用于具有连续状态和动作空间的环境，无需系统模型。", "result": "仿真结果表明RL控制方法的有效性。实验中还测试了控制器在存在外部干扰（如风扰动）情况下的性能，并与传统PID控制器进行了比较。", "conclusion": "实验室实验证明该强化学习控制器在实际应用中的有效性，展示了其优于传统PID控制器的潜在优势"}}
{"id": "2512.13342", "pdf": "https://arxiv.org/pdf/2512.13342", "abs": "https://arxiv.org/abs/2512.13342", "authors": ["Sheikh Shakil Akhtar", "Pranabendu Misra", "Geevarghese Philip"], "title": "Space Efficient Algorithms for Parameterised Problems", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.CO"], "comment": null, "summary": "We study \"space efficient\" FPT algorithms for graph problems with limited memory. Let n be the size of the input graph and k be the parameter. We present algorithms that run in time f(k)*poly(n) and use g(k)*polylog(n) working space, where f and g are functions of k alone, for k-Path, MaxLeaf SubTree and Multicut in Trees. These algorithms are motivated by big-data settings where very large problem instances must be solved, and using poly(n) memory is prohibitively expensive. They are also theoretically interesting, since most of the standard methods tools, such as deleting a large set of vertices or edges, are unavailable, and we must a develop different way to tackle them.", "AI": {"tldr": "研究了在有限内存下，图问题的参数化空间高效算法。", "motivation": "针对大数据环境下需要解决大规模实例的问题，提出利用较少内存（polylog(n)）来运行参数化复杂度算法的方法。", "method": "对于k-Path、MaxLeaf SubTree和Multicut in Trees问题，设计了使用g(k)*polylog(n)工作空间的f(k)*poly(n)时间复杂性的算法。", "result": "提出了空间效率高的FPT算法，在有限内存条件下解决了图问题。", "conclusion": "该方法在大数据环境中具有实际应用价值，并且理论上也是一次创新尝试，避免了传统的大规模节点或边删除等手段。"}}
{"id": "2512.13330", "pdf": "https://arxiv.org/pdf/2512.13330", "abs": "https://arxiv.org/abs/2512.13330", "authors": ["Joona Kytöniemi", "Jousia Piha", "Akseli Reunamo", "Fedor Vitiugin", "Farrokh Mehryary", "Sampo Pyysalo"], "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": ":I.2.7", "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.", "AI": {"tldr": "FIN-bench-v2是一个用于评估芬兰大语言模型的统一基准套件，涵盖多种任务和格式。", "motivation": "推动芬兰语领域的自然语言处理研究，提供一个全面且一致的测试平台来评价不同模型的表现。", "method": "整合了多个现有的芬兰语数据集，并通过预训练模型筛选出具有稳健性的任务。使用机器翻译资源并通过人工审查确保质量。", "result": "创建了一个包含多种任务和提示形式的统一基准，所有数据都以HuggingFace Datasets格式公开。", "conclusion": "FIN-bench-v2为芬兰语大语言模型提供了全面且一致的评估环境，促进了该领域的研究和发展。"}}
{"id": "2512.13325", "pdf": "https://arxiv.org/pdf/2512.13325", "abs": "https://arxiv.org/abs/2512.13325", "authors": ["Malte Hellmeier"], "title": "Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accepted for publication at the ICISSP 2026", "summary": "Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.", "AI": {"tldr": "研究分析了十种Unicode文本水印方法在六种大型语言模型上的安全性和可检测性。", "motivation": "随着大型语言模型的广泛应用，数字文本的安全保护变得愈发重要。现有数字文本水印方法对大型语言模型的安全性和不可探测性尚未经过充分验证和分析。", "method": "设计并实施了三个控制实验，在六种不同的大语言模型上测试十种现有的Unicode文本水印方法。", "result": "最新推理模型能够检测到嵌入的水印，但除非提供源代码等实现细节，否则所有模型都无法提取出实际的水印信息。", "conclusion": "讨论了对安全研究人员和从业人员的影响，并概述了解决安全问题的未来研究机会。"}}
{"id": "2512.13323", "pdf": "https://arxiv.org/pdf/2512.13323", "abs": "https://arxiv.org/abs/2512.13323", "authors": ["Árpád Pándy", "Róbert Lakatos", "András Hajdu"], "title": "Error-Driven Prompt Optimization for Arithmetic Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.", "AI": {"tldr": "本文介绍了一种针对小规模语言模型的基于错误驱动优化框架，用于提升其在结构化数据上进行准确算术操作的能力。", "motivation": "为了支持金融和医疗等监管行业中的分析师，在不泄露敏感信息的情况下执行算术任务，需要开发可靠、可解释且能部署在工业环境中的AI助手。现有的大型语言模型存在基本限制，而传统的微调方法成本较高。", "method": "提出了一种错误驱动的优化框架，通过系统地评估小规模语言模型(Qwen3 4B)的表现，并对错误预测进行聚类以迭代改进提示规则来提高算术推理任务中的准确性。", "result": "实验表明，在隐私合规的情况下，该方法显著提高了模型在算术操作上的准确率至70.8%，超过了较大规模的语言模型如GPT-3.5 Turbo的表现。", "conclusion": "通过系统性、基于错误驱动的提示优化而不是昂贵的微调训练，可以增强小型语言模型的能力，在隐私保护的同时实现更高的工业部署价值。"}}
{"id": "2512.13317", "pdf": "https://arxiv.org/pdf/2512.13317", "abs": "https://arxiv.org/abs/2512.13317", "authors": ["Mikhail Zakharov"], "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 1 figure, 5 tables, 10 equations. Preprint", "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "AI": {"tldr": "本文研究了在面部检索系统中遗忘选定的身份信息，通过分散嵌入点来防止身份重新识别。", "motivation": "人脸识别系统可能引起隐私问题，因为它们可以进行未经授权的身份追踪。本文探讨了如何通过机器遗忘保护隐私，特别是在现代基于嵌入的识别模型中的应用。", "method": "作者评估了几种现有的近似类遗忘方法，并提出了一种简单有效的分散式遗忘方法来解决身份信息遗忘的问题。", "result": "在VGGFace2和CelebA等标准基准测试上进行了广泛的实验，证明了该方法能够实现更好的遗忘行为同时保持检索的效用。", "conclusion": "本文提出了一个有效的方法，在防止重新识别的同时保留模型的鉴别结构和检索性能。"}}
{"id": "2512.13316", "pdf": "https://arxiv.org/pdf/2512.13316", "abs": "https://arxiv.org/abs/2512.13316", "authors": ["Mayank Gulati", "Benedikt Groß", "Gerhard Wunder"], "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at 2025 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)", "summary": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations. Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures", "AI": {"tldr": "ALIGN-FL是一种新的分布式学习方法，通过共享生成组件在联邦学习中处理高度分散的数据分布。", "motivation": "现有的模型参数交换方式不适用于高差异数据集。为了实现隐私保护同时提高跨域异构客户端的性能，引入了选择性分享生成能力的方法。", "method": "ALIGN-FL框架只传输生成能力，并通过DP-SGD和Lipschitz正则化VAE解码器等机制进行实验验证。", "result": "实验证明隐私保护措施能有效将敏感异常值映射为典型数据点，同时保持在极端非IID场景中的效用。", "conclusion": "该方法在MNIST和Fashion-MNIST数据集上展示了有效的隐私保护和模型性能提升。"}}
{"id": "2512.13313", "pdf": "https://arxiv.org/pdf/2512.13313", "abs": "https://arxiv.org/abs/2512.13313", "authors": ["Kling Team", "Jialu Chen", "Yikang Ding", "Zhixue Fang", "Kun Gai", "Yuan Gao", "Kang He", "Jingyun Hua", "Boyuan Jiang", "Mingming Lao", "Xiaohan Li", "Hui Liu", "Jiwen Liu", "Xiaoqiang Liu", "Yuan Liu", "Shun Lu", "Yongsen Mao", "Yingchao Shao", "Huafeng Shi", "Xiaoyu Shi", "Peiqin Sun", "Songlin Tang", "Pengfei Wan", "Chao Wang", "Xuebo Wang", "et al. (3 additional authors not shown)"], "title": "KlingAvatar 2.0 Technical Report", "categories": ["cs.CV"], "comment": "14 pages, 7 figures", "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "AI": {"tldr": "开发了一种用于生成高效、高质量长视频的框架KlingAvatar 2.0", "motivation": "解决现有模型在生成长时间高分辨率视频时存在的效率低下问题，包括时间漂移、质量下降和指令遵循能力弱等问题。", "method": "提出了一种时空级联框架，首先生成低分辨率的关键帧以捕获全局语义和运动，然后通过首尾帧策略将其细化为高质量且具有连续性的子片段；引入Co-Reasoning Director来增强跨模态指令融合与对齐，并采用Negative Director优化负向提示，支持ID特定的多角色控制。", "result": "实验显示该模型有效解决了高效、跨模态一致长视频生成难题，提高了视觉清晰度和真实感，增强了身份保持能力及多模态指令遵循性。", "conclusion": "通过创新框架设计与技术应用，KlingAvatar 2.0在长时间高分辨率视频生成领域取得了突破性的成果"}}
{"id": "2512.13304", "pdf": "https://arxiv.org/pdf/2512.13304", "abs": "https://arxiv.org/abs/2512.13304", "authors": ["Sait Sovukluk", "Johannes Englsberger", "Christian Ott"], "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories", "categories": ["cs.RO"], "comment": "Accepted for publication in Biomimetic Intelligence and Robotics. Supplemental video: https://youtu.be/HlAg2nbNct4", "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.", "AI": {"tldr": "本文提出了一种步态适应框架，用于通过弹簧质量轨迹和死区控制增益库进行跑步。", "motivation": "该研究旨在为人类机器人提供一种能够在各种复杂环境中灵活移动的步态适应方法。", "method": "提出了一个包含自动弹簧质量轨迹生成、死区控制增益生成、轨迹选择策略开发以及将弹簧质量轨迹映射到人形模型的方法框架，并通过MuJoCo物理模拟器验证其性能。", "result": "该框架展示了在多种挑战性行为中的包容性和鲁棒性，如跑步过随机石块和跳跃障碍物等，且无需额外调整控制参数。所有计算仅需4.5秒完成。", "conclusion": "所提出的步态适应方法能够在各种不确定性和噪声条件下保持高度的鲁棒性，并能够执行复杂的动作，适用于实际应用中的多种情况。"}}
{"id": "2512.13303", "pdf": "https://arxiv.org/pdf/2512.13303", "abs": "https://arxiv.org/abs/2512.13303", "authors": ["Zhihang Liu", "Xiaoyi Bao", "Pandeng Li", "Junjie Zhou", "Zhaohe Liao", "Yefei He", "Kaixun Jiang", "Chen-Wei Xie", "Yun Zheng", "Hongtao Xie"], "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement", "categories": ["cs.CV"], "comment": "project page: https://lntzm.github.io/showtable-page/", "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.", "AI": {"tldr": "本文提出了一种新的挑战任务：创意表格可视化，即生成能够忠实且美观地展示给定表格数据的 infographic。为实现这一目标，提出了 ShowTable 管道，该管道通过一个渐进自我纠正过程将 MLLM 和扩散模型结合起来。", "motivation": "现有的生成和统一模型在一般图像生成方面表现出色，但在需要深度推理、计划和超越通用场景的数据到视觉映射能力的任务上存在局限性。因此，本文旨在解决创意表格可视化这一新挑战任务。", "method": "ShowTable 管道结合了 MLLM 和扩散模型，MLLM 负责推理视觉方案并判断视觉错误以提供细化指令，而扩散模型执行 MLLM 的命令，达到高保真度结果。为支持该任务和管道，介绍了三个自动化数据构造流程。", "result": "实验表明，本文所提出的管道在不同模型实例中显著优于基线方法，在多模态推理、生成及错误修正方面表现出色。", "conclusion": "通过 ShowTable 管道实现了创意表格可视化的有效解决，展示了其在复杂任务中的潜力和优越性能。"}}
{"id": "2512.13300", "pdf": "https://arxiv.org/pdf/2512.13300", "abs": "https://arxiv.org/abs/2512.13300", "authors": ["Qinglin Jia", "Zhaocheng Du", "Chuhan Wu", "Huifeng Guo", "Ruiming Tang", "Shuting Shi", "Muyu Zhang"], "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.", "AI": {"tldr": "提出了一种针对不完整和偏斜多标签数据的细粒度知识转移框架(KAML)，以提高转化率预测模型的效果。", "motivation": "在线广告系统中，由于隐私或其他限制，许多广告商仅提交部分用户转换行为，导致多任务学习中的标签数据不完整且偏斜。训练与部署的数据分布差异会导致模型性能不佳。", "method": "引入基于归因驱动的掩码策略(ADM)以更好地利用具有不对称多标签数据；提出层次知识提取机制(HKE)，处理目标任务塔内的样本偏差；通过集成排序损失策略来增强未标记样本的利用率。", "result": "KAML在离线行业数据集和在线A/B测试中的效果显著优于现有的多任务学习基线模型。", "conclusion": "所提出的细粒度知识转移框架(KAML)能够有效解决不完整和偏斜多标签数据下的转化率预测问题，提升了模型的性能。"}}
{"id": "2512.13298", "pdf": "https://arxiv.org/pdf/2512.13298", "abs": "https://arxiv.org/abs/2512.13298", "authors": ["Anna Aksenova", "Boris Zverkov", "Nicola Dainese", "Alexander Nikitin", "Pekka Marttinen"], "title": "MiniLingua: A Small Open-Source LLM for European Languages", "categories": ["cs.CL", "cs.AI"], "comment": "9+6 pages, 6 figures and 3 tables in the main text. Code at https://github.com/MiniLingua-ai/training_artifacts", "summary": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.", "AI": {"tldr": "MiniLingua是一款针对欧洲语言的小型开源语言模型，旨在平衡多语种覆盖和指令跟随能力。", "motivation": "大型语言模型面临计算成本高、隐私问题以及以英语为中心的训练等问题。小型高效的语言模型可以提供强大的性能并支持设备上的使用。MiniLingua是为了克服这些限制而设计的。", "method": "基于一亿参数，MiniLingua是从头开始为13种欧洲语言训练的一个多语言开源LLM，并通过指令调整来增强其能力。", "result": "在摘要、分类和开放式及闭合式问题回答任务中，指令微调版的MiniLingua优于具有相似训练方法但预算更大的EuroLLM模型。同时，在开放生成任务方面，它与更先进的最先进模型保持竞争力。", "conclusion": "通过提供模型权重、分词器以及用于数据处理和模型训练的源代码，MiniLingua证明了在保证性能的同时减少资源需求是可行的。"}}
{"id": "2512.13297", "pdf": "https://arxiv.org/pdf/2512.13297", "abs": "https://arxiv.org/abs/2512.13297", "authors": ["Zhenghao Zhu", "Chuxue Cao", "Sirui Han", "Yuanfeng Song", "Xing Chen", "Caleb Chen Cao", "Yike Guo"], "title": "MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.", "AI": {"tldr": "介绍了MedInsightBench，用于评估大型多模态模型（LMMs）在医学数据中发现深层次见解的能力。", "motivation": "为了填补高质量的数据集缺乏的空白，用以评估LMMs在医学数据分析中的表现，尤其是复杂和多层次的理解能力。", "method": "提出了MedInsightAgent框架，包括视觉根部查找器、分析洞察代理和后续问题作曲家三个模块。通过在MedInsightBench上进行实验来展示其效果。", "result": "现有LMMs在处理多步骤的深层次医学见解时表现不佳，而MedInsightAgent能够改善这些模型的表现。", "conclusion": "提出了新的基准MedInsightBench和框架MedInsightAgent，用于评估和提升医学数据分析中的深度理解能力。"}}
{"id": "2512.13293", "pdf": "https://arxiv.org/pdf/2512.13293", "abs": "https://arxiv.org/abs/2512.13293", "authors": ["Hao Fu", "Wei Liu", "Shuai Zhou"], "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.", "AI": {"tldr": "该论文提出了一种用于多机器人社交形式导航的协调探索强化学习算法，引入了内在动机探索机制以提高协同效率。", "motivation": "为了应对行人行为的不可预测性和不合作动态给多机器人社交形式导航带来的挑战，研究提出了结合内在激励探索的新方法来改善政策保守性。", "method": "该算法设计了一种自我学习的内在奖励机制，并采用了双采样模式和两时间尺度更新规则，在集中训练与分散执行框架下提升了导航策略及内在奖励的表现。", "result": "实验证明，所提算法在社交形式导航基准测试中优于现有最先进方法的关键指标表现更优。", "conclusion": "该论文提出的方法有效解决了多机器人社交形式导航中的协调探索问题，并通过实验验证了其优越性。"}}
{"id": "2512.13290", "pdf": "https://arxiv.org/pdf/2512.13290", "abs": "https://arxiv.org/abs/2512.13290", "authors": ["Shu Yu", "Chaochao Lu"], "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.", "AI": {"tldr": "该论文提出了一种新的框架LINA，用于解决扩散模型在物理对齐和泛化方面的挑战。", "motivation": "扩散模型（DMs）虽然在图像和视频生成方面取得了显著成功，但仍面临物理对齐和超出分布指令跟随的难题。这些问题归因于模型未能学习因果方向并解开因果因素以实现新颖重组。", "method": "通过引入因果场景图（CSG）和物理对齐探针（PAP），论文分析发现扩散模型在多跳推理、提示嵌入中的纹理与物理分离以及初始去噪步骤中视觉因果结构的形成等方面存在挑战。基于这些见解，LINA框架采用针对性指导以预测特定于提示的干预措施，并重新分配去噪时间表。", "result": "该方法实现了图像和视频扩散模型在物理对齐和超出分布指令跟随方面的改进，在具有挑战性的因果生成任务和Winoground数据集上达到最先进的性能。", "conclusion": "通过引入LINA框架，论文解决了扩散模型中的关键问题，并展示了其在复杂任务上的卓越表现。"}}
{"id": "2512.13285", "pdf": "https://arxiv.org/pdf/2512.13285", "abs": "https://arxiv.org/abs/2512.13285", "authors": ["Bo Liu", "Qiao Qin", "Qinghui He"], "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images", "categories": ["cs.CV"], "comment": "9 pages,Accepted to AAAI 2026", "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.", "AI": {"tldr": "本文提出了一种名为CausalCLIP的框架，该框架通过因果推理原理来分离和过滤特征，以提高生成图像检测器在不同生成模型上的泛化能力。", "motivation": "现有的生成图像检测方法往往产生高度纠缠的表示形式，这些表示混合了任务相关的证据线索（因果特征）与无关或多余的模式（非因果特征），从而限制了其泛化能力。本文旨在解决这一问题。", "method": "CausalCLIP通过建模生成过程中的结构因果模型并利用Gumbel-Softmax特征屏蔽和Hilbert-Schmidt独立性标准(HSIC)约束来强制统计独立，以分离稳定的因果特征并对非因果特征进行过滤。", "result": "在测试过程中，该方法在未见过的不同系列的生成模型上展示了强大的泛化能力，准确度提高了6.83%，平均精度提高了4.06%。", "conclusion": "本文提出的CausalCLIP框架通过分离和过滤因果特征来改善生成图像检测器的性能。实验结果表明该方法能够显著提高检测器在不同生成模型上的表现。"}}
{"id": "2512.13284", "pdf": "https://arxiv.org/pdf/2512.13284", "abs": "https://arxiv.org/abs/2512.13284", "authors": ["Adheep Arya G R", "Vaibhav Pratap Singh", "Mayank Kumar", "Niyathi Shenoy", "Tejas Suryawanshi", "Ruchi Juyal", "Sangit Saha", "Kaushik Nanda", "Hari Babu Pasupuleti", "S D Sudarsan"], "title": "SAMAY: System for Acoustic Measurement and Analysis", "categories": ["cs.SD", "cs.RO"], "comment": null, "summary": "This paper describes an automatic bird call recording system called SAMAY, which is developed to study bird species by creating a database of large amounts of bird acoustic data. By analysing the recorded bird call data, the system can also be used for automatic classification of bird species, monitoring bird populations and analysing the impact of environmental changes. The system is driven through a powerful STM32F407 series microcontroller, supports 4 microphones, is equipped with 128 GB of storage capacity, and is powered by a 10400 mAh battery pack interfaced with a solar charger. In addition, the device is user-configurable over USB and Wi-Fi during runtime, ensuring user-friendly operation during field deployment.", "AI": {"tldr": "开发了一种用于研究鸟类物种的自动录音系统SAMAY，通过分析记录的声音数据实现鸟类分类、监测和环境影响分析。", "motivation": "为了研究鸟类种类并创建大量鸟类声音数据库，提高对鸟类生态学的理解及保护效率。", "method": "使用STM32F407系列微控制器驱动的设备，配备四个麦克风，具有128GB存储容量，并通过USB和Wi-Fi进行用户配置。", "result": "该系统能够自动分类和分析鸟叫声数据，监测鸟类数量并研究环境变化对它们的影响。", "conclusion": "SAMAY系统的开发为鸟类学研究提供了一个强大的工具，有助于提高保护效率并对生态环境产生更深入的理解。"}}
{"id": "2512.13281", "pdf": "https://arxiv.org/pdf/2512.13281", "abs": "https://arxiv.org/abs/2512.13281", "authors": ["Jiaqi Wang", "Weijia Wu", "Yi Zhan", "Rui Zhao", "Ming Hu", "James Cheng", "Wei Liu", "Philip Torr", "Kevin Qinghong Lin"], "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "categories": ["cs.CV"], "comment": "Code is at https://github.com/video-reality-test/video-reality-test, page is at https://video-reality-test.github.io/", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \\textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \\textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "AI": {"tldr": "研究介绍了一个针对ASMR视频的基准测试，探讨AI生成的音频视觉内容是否可以欺骗人类和VLM。", "motivation": "先前的研究大多没有考虑音频与视频之间的紧密耦合，且主要集中在无音视频上。这项工作旨在填补这一空白，并评估当前模型在产生逼真、有音频配对的视频方面的能力。", "method": "通过精心挑选的真实ASMR视频建立基准测试，设计了一种对抗性的创作者-评审者协议，其中视频生成模型充当创作者试图欺骗评审员，而VLM则作为审查员尝试识别虚假内容。实验结果显示了AI在产生逼真的、音频视觉一致的视频方面的限制。", "result": "最强的创作者Veo3.1-Fast甚至可以愚弄大多数VLM：最强的评审员Gemini 2.5-Pro仅达到56%的准确率（随机为50%），远低于人类专家（81.25%）。增加音频提高了真实和伪造内容之间的区分度，但浅层线索如水印仍然能显著误导模型。", "conclusion": "研究揭示了视频生成现实性的当前边界，并暴露了VLM在感知保真度和音视频一致性方面的局限性。"}}
{"id": "2512.13276", "pdf": "https://arxiv.org/pdf/2512.13276", "abs": "https://arxiv.org/abs/2512.13276", "authors": ["Yan Li", "Lin Liu", "Xiaopeng Zhang", "Wei Xue", "Wenhan Luo", "Yike Guo", "Qi Tian"], "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation", "AI": {"tldr": "本文提出了CogniEdit框架，用于解决基于指令的图像编辑中的精细属性问题。", "motivation": "现有的图像编辑方法难以处理精确指定颜色、位置和数量等细致属性的指令。当前的方法虽然使用了GRPO优化策略，但仍局限于单一步骤反馈，限制了轨迹级别的控制。", "method": "CogniEdit框架结合多模态推理与密集奖励优化，通过连续去噪步骤传播梯度，实现轨迹级控制。方法包括：多模态大型语言模型分解指令、动态标记焦点调整强调细致属性和基于GRPO的密集优化策略。", "result": "实验表明，CogniEdit在遵循精细指令、保持视觉质量和编辑性方面达到了最先进的性能。", "conclusion": "通过引入多模态推理与密集反馈优化，CogniEdit显著提升了图像编辑任务中的精确控制能力。"}}
{"id": "2512.13271", "pdf": "https://arxiv.org/pdf/2512.13271", "abs": "https://arxiv.org/abs/2512.13271", "authors": ["Fangju Yang", "Hang Yang", "Ibrahim Alsarraj", "Yuhao Wang", "Ke Wu"], "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation", "categories": ["cs.RO"], "comment": null, "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.", "AI": {"tldr": "该论文提出了一种轻量化的基于驱动空间能量建模框架LASEM，用于电缆驱动连续机器人（CDCRs）的动态模型。", "motivation": "为了实现高精度和实时的动力学预测或基于模型控制，需要准确且高效的动力学模型。现有的方法复杂度高、计算效率低，因此论文旨在提出一种轻量级且精确的动力学建模框架。", "method": "通过统一的变分推导，将驱动势能直接在驱动空间内表达，并简化为一个偏微分方程（PDE），同时避免了显式的电缆和主体接触力计算。此外，该方法还支持基于力输入和位移输入两种驱动模式。", "result": "提出的方法相比现有的实时动力学建模方案平均提高了62.3%的计算效率。", "conclusion": "LASEM框架在保持几何精度和物理一致性的前提下实现了轻量级、高效的动力学模型，特别适用于高动态性能要求的应用场景。"}}
{"id": "2512.13262", "pdf": "https://arxiv.org/pdf/2512.13262", "abs": "https://arxiv.org/abs/2512.13262", "authors": ["Hyunki Seong", "Jeong-Kyun Lee", "Heesoo Myeong", "Yongho Shin", "Hyun-Mook Cho", "Duck Hoon Kim", "Pranav Desai", "Monu Surana"], "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.", "AI": {"tldr": "本文提出了两种策略来改善自动驾驶中的交互行为模型：一种是通过相对优势最大化和人类正则化优化预训练的行为模型；另一种是在测试时采用Warm-K方法平衡运动选择的一致性和多样性。", "motivation": "当前基于模仿学习的自动驾驶行为模型存在数据偏见和开放循环评估的问题，导致在闭环执行中出现累积误差。本文旨在通过改进的方法提高安全性能并保持行为现实性。", "method": "首先提出了Group Relative Behavior Optimization (GRBO)，一种基于相对优势最大化的人类正则化优化方法；其次引入了Warm-K，一个平衡一致性与多样性的测试时采样策略。", "result": "使用GRBO，仅通过10%的训练数据即可将安全性能提高超过40%，同时保持行为现实性。Warm-K在不重新训练的情况下增强了行为一致性和反应能力，减少了性能差距。", "conclusion": "本文提出的GRBO和Warm-K方法有效解决了自动驾驶行为模型中的偏见问题，并提高了闭环执行时的安全性和一致性。"}}
{"id": "2512.13253", "pdf": "https://arxiv.org/pdf/2512.13253", "abs": "https://arxiv.org/abs/2512.13253", "authors": ["Julian Berger", "Jason W. Burton", "Ralph Hertwig", "Thomas Kosch", "Ralf H. J. M. Kurvers", "Benito Kurzenberger", "Christopher Lazik", "Linda Onnasch", "Tobias Rieger", "Anna I. Thoma", "Dirk U. Wulff", "Stefan M. Herzog"], "title": "Fostering human learning is crucial for boosting human-AI synergy", "categories": ["cs.HC"], "comment": null, "summary": "The collaboration between humans and artificial intelligence (AI) holds the promise of achieving superior outcomes compared to either acting alone. Nevertheless, our understanding of the conditions that facilitate such human-AI synergy remains limited. A recent meta-analysis showed that, on average, human-AI combinations do not outperform the better individual agent, indicating overall negative human-AI synergy. We argue that this pessimistic conclusion arises from insufficient attention to human learning in the experimental designs used. To substantiate this claim, we re-analyzed all 74 studies included in the original meta-analysis, which yielded two new findings. First, most previous research overlooked design features that foster human learning, such as providing trial-by-trial outcome feedback to participants. Second, our re-analysis, using robust Bayesian meta-regressions, demonstrated that studies providing outcome feedback show relatively higher synergy than those without outcome feedback. Crucially, when feedback is paired with AI explanations we tend to find positive human-AI synergy, while AI explanations provided without feedback were strongly linked to negative synergy, indicating that explanations are useful for synergy only when humans can learn to verify the AI's reliability through feedback. We conclude that the current literature underestimates the potential for human-AI collaboration because it predominantly relies on experimental designs that do not facilitate human learning, thus hindering humans from effectively adapting their collaboration strategies. We therefore advocate for a paradigm shift in human-AI interaction research that explicitly incorporates and tests human learning mechanisms to enhance our understanding of and support for successful human-AI collaboration.", "AI": {"tldr": "本文通过重新分析先前的研究，强调了人类学习机制在促进人机协作中的重要性。", "motivation": "现有研究中的人机协同效果不佳，主要原因是实验设计忽视了促进人类学习的因素。作者希望通过深入分析这些因素来提高人机协同的效果。", "method": "本文重审了先前的74项研究，并使用稳健的贝叶斯元回归分析了反馈和AI解释在人机协作中的作用。", "result": "提供结果反馈可以显著增强人机协同效果；而当反馈与AI解释结合时，这种正面效应更为明显。没有反馈的情况下，AI解释反而可能导致负面的人机协同。", "conclusion": "当前的研究低估了人类学习机制对促进有效的人机协作的潜力，应通过设计更有利于人类学习的实验来提高人机协同的效果。"}}
{"id": "2512.13251", "pdf": "https://arxiv.org/pdf/2512.13251", "abs": "https://arxiv.org/abs/2512.13251", "authors": ["Tao Li", "Wengshuo Ge", "Zhichao Wang", "Zihao Cui", "Yong Ma", "Yingying Gao", "Chao Deng", "Shilei Zhang", "Junlan Feng"], "title": "DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec", "categories": ["cs.SD"], "comment": null, "summary": "Recent codec-based language models~(LMs) have revolutionized text-to-speech~(TTS). However, since standard codecs tightly couple timbre and prosody, continuation-based LMs inevitably replicate this entanglement, hindering independent control. Recent efforts attempt to break this entanglement via codec design, but insufficient decoupling remains a critical bottleneck. To tackle this challenge, we propose DisCo-Speech, a zero-shot controllable TTS framework that enables prosody control and voice cloning via a disentangled speech codec (DisCodec) and an LM-based generator. The core component, DisCodec, contains two core stages: 1) Tri-factor disentanglement, which explicitly factorizes speech into content, prosody, and timbre subspaces via parallel encoders and hybrid losses; and 2) Fusion and reconstruction, which fuses content and prosody into unified content-prosody tokens suitable for LM prediction, while jointly optimizing reconstruction quality to resolve the disentanglement-reconstruction trade-off. With this design, the LM performs prosodic continuation from a style prompt while the decoder handles target timbre injection, enabling flexible zero-shot control. Experiments show that DisCo-Speech matches state-of-the-art voice cloning performance while outperforming baselines in zero-shot prosody control. By resolving the core entanglement at the codec level, DisCo-Speech provides a robust foundation for controllable speech synthesis. Audio samples are available at https://github.com/disco-speech/DisCo-Speech, and the code and weights will be released at the same link.", "AI": {"tldr": "提出了一种解耦的语音编码器DisCodec，用于零样本控制的文本转语音（TTS）框架DisCo-Speech。", "motivation": "现有的编码器基线语言模型将音色和语调紧密绑定在一起，这限制了独立控制的可能性。为了解决这一问题，提出了DisCo-Speech来实现解耦并提供灵活的零样本控制。", "method": "通过引入一个包含三因子解耦阶段（语音被分解成内容、语调和音色子空间）和融合重构阶段（将内容与语调融合以进行语言模型预测，并共同优化重建质量以解决解耦与重建之间的权衡）的DisCodec，实现了零样本控制。", "result": "实验表明，DisCo-Speech在语音克隆性能上达到了最先进的水平，在零样本语调控制方面优于基线方法。", "conclusion": "通过在编码器层面解决问题，DisCo-Speech为可控语音合成提供了一种稳健的基础。"}}
{"id": "2512.13250", "pdf": "https://arxiv.org/pdf/2512.13250", "abs": "https://arxiv.org/abs/2512.13250", "authors": ["Juil Koo", "Daehyeon Choi", "Sangwoo Youn", "Phillip Y. Lee", "Minhyuk Sung"], "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection", "categories": ["cs.CV"], "comment": "Project page: https://active-view-selection.github.io/", "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.", "AI": {"tldr": "本文提出了基于视觉引导的主动视点选择任务，旨在通过当前图像中的视觉信息来选择最有信息量的下一个视角。", "motivation": "目前的视觉语言模型仅限于静态图像分析，而实际场景中需要主动移动以获取更多信息。本文希望通过引入一个新任务和方法来解决这一问题。", "method": "作者构建了一个合成数据集，并提出了一种框架，该框架通过监督微调和基于强化学习的策略优化来训练预训练的视觉语言模型。", "result": "所提方法在视点选择基础上的问题回答中取得了良好的性能，并且可以稳健地泛化到未见过的场景。将这种方法集成到现有的环境探索式问答系统中能够提升下游问题回答准确性。", "conclusion": "本文通过引入新的任务和方法，提高了基于视角的选择性视觉信息提取能力，从而增强了对复杂场景的理解与推理水平。"}}
{"id": "2512.13247", "pdf": "https://arxiv.org/pdf/2512.13247", "abs": "https://arxiv.org/abs/2512.13247", "authors": ["Foivos Paraperas Papantoniou", "Stathis Galanakis", "Rolandos Alexandros Potamias", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits", "categories": ["cs.CV"], "comment": "Project page: https://foivospar.github.io/STARCaster/", "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.", "AI": {"tldr": "STARCaster是一种基于身份和视角感知的说话肖像动画模型，它在统一框架内解决了语音驱动的肖像动画以及自由视点说话肖像合成的问题。", "motivation": "现有的2D语音到视频扩散模型严重依赖于参考图像指导，导致动作多样性有限。同时，3D感知动画通常通过预训练三平面生成器进行反转，这往往会导致不完美的重建和身份漂移。", "method": "STARCaster采用了一种逐步的方法从ID感知的动作建模开始，然后是基于唇读的音频视觉同步，最后实现新的视点动画。为了应对4D音视频数据稀缺的问题，该模型提出一种解耦的学习方法，在独立训练视角一致性和时间一致性的同时，使用自强迫训练方案使模型能够学习比推理生成的时间上下文更长的内容。", "result": "全面的评估表明，STARCaster在不同任务和身份之间具有很好的泛化能力，并且在不同的基准测试中始终优于以前的方法。", "conclusion": "通过重新思考参考指导和几何感知范式，以及采用解耦学习方法，STARCaster能够有效地解决语音驱动的肖像动画及自由视点说话肖像合成的问题。"}}
{"id": "2512.13240", "pdf": "https://arxiv.org/pdf/2512.13240", "abs": "https://arxiv.org/abs/2512.13240", "authors": ["Zihui Zhao", "Zechang Li"], "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.", "AI": {"tldr": "本文提出了一种新的框架Reflective Preference Optimization (RPO)，通过引入提示引导的反思来增强直接偏好优化（DPO）的方法，以提高大型语言和视觉-语言模型的行为与人类偏好的对齐。", "motivation": "标准的DPO形式化方法由于两个由同一策略生成的回答之间存在相似错误以及较小的KL散度而导致学习信号弱，并导致缓慢且不稳定的收敛。为了解决这个问题，提出了RPO框架以增强偏好对齐。", "method": "RPO通过外部模型识别幻觉来源并生成简洁的反思提示来构建具有更强对比性和更清晰偏好数字的策略内偏好对。", "result": "实验结果表明，RPO在较少训练样本和迭代次数的情况下实现了更好的对齐，并显著降低了幻觉率，在多模态基准测试中取得了最先进的性能。", "conclusion": "本文提出的Reflective Preference Optimization (RPO) 方法通过引入提示引导的反思增强了直接偏好优化（DPO）方法的效果，提高了大型语言和视觉-语言模型的行为与人类偏好的对齐。"}}
{"id": "2512.13238", "pdf": "https://arxiv.org/pdf/2512.13238", "abs": "https://arxiv.org/abs/2512.13238", "authors": ["Francesco Ragusa", "Michele Mazzamuto", "Rosario Forte", "Irene D'Ambra", "James Fort", "Jakob Engel", "Antonino Furnari", "Giovanni Maria Farinella"], "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance", "categories": ["cs.CV"], "comment": null, "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.", "AI": {"tldr": "构建了一个视频语言数据集Ego-EXTRA，用于评估专家级的训练者助手系统。", "motivation": "通过真实专家在第一人称视角下的指导来收集高质量的对话数据以支持对视频语言助手系统的基准测试。", "method": "采用了一种‘巫师模式’的数据采集方式，记录了训练者的操作过程和他们与专家之间的双向对话，并创建了一个包含超过15,000个视觉问答集的数据集。", "result": "表明当前的多模态大型语言模型在提供专家级别的辅助时存在局限性。", "conclusion": "Ego-EXTRA数据集为评估第一人称视频语言助手系统的性能提供了强有力的工具，并展示了现有模型在处理这类任务上的不足之处。"}}
{"id": "2512.13235", "pdf": "https://arxiv.org/pdf/2512.13235", "abs": "https://arxiv.org/abs/2512.13235", "authors": ["Jianyuan Bo", "Yuan Fang"], "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.", "AI": {"tldr": "该论文提出了一种新的图自监督学习框架，结合对比学习和掩码特征重建技术以增强模型性能。", "motivation": "在图的生成式与对比式自监督学习之间发现理论上的联系，并探索两者的集成来提升图任务的表现。", "method": "通过形成正样本对并在编码器中加入负样本，使模型能够更好地利用上下文信息进行掩码特征重建。", "result": "所提出的框架CORE在节点和图分类任务上超越了GraphMAE和GraphMAE2，展示了卓越的结果。", "conclusion": "结合MFR与GCL的对比学习方法可以显著提高自监督学习模型的效果。"}}
{"id": "2512.13219", "pdf": "https://arxiv.org/pdf/2512.13219", "abs": "https://arxiv.org/abs/2512.13219", "authors": ["Christoph Hartmann", "Marios Demetriades", "Kevin Prüfer", "Zichen Zhang", "Klaus Spindler", "Stefan Weltge"], "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization", "categories": ["cs.RO", "cs.MS"], "comment": "Code available at https://github.com/TUM-utg/PyCAALP (repository will be made public prior to publication)", "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.", "AI": {"tldr": "提出了一个基于图优化的框架PyCAALP，用于自动化装配序列规划和生产线规划。", "motivation": "为了处理复杂装配中高组合性问题，并保证自动装配规划的可行性。", "method": "采用图形化方法建模组件和关节，在生产模块内整合运动学边界条件。算法计算所有可能的生产顺序，集成了检测空间关系与制定几何约束的功能，使用启发式规则如单件流动装配优化解决方案。", "result": "框架解决了ASP和PLP中的工程约束定制，并支持两者之间的灵活折衷。作为开源项目，PyCAALP促进了工业界和研究应用的合作。", "conclusion": "该工作通过整合图形化方法和启发式规则提高了自动化装配序列规划和生产线规划的效率与可行性，为未来的研究提供了开放平台。"}}
{"id": "2512.13215", "pdf": "https://arxiv.org/pdf/2512.13215", "abs": "https://arxiv.org/abs/2512.13215", "authors": ["Yinsong Qu", "Yunxiang Li", "Shanlin Zhong"], "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment", "categories": ["cs.RO"], "comment": "9 pages, 11 figures, conference paper for the 2025 International Conference on Advanced Robotics and Mechatronics (ICARM), accepted", "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).", "AI": {"tldr": "提出了一种改进的顺序模型预测控制（ISMPC）导航框架，以解决自主移动机器人在拥挤、复杂环境中的导航问题。", "motivation": "自主移动机器人在工业应用中至关重要，但它们在密集且复杂的环境中导航仍然存在挑战性。这些问题包括非完整车辆动力学、与静态和动态障碍物的交互以及操作空间的非凸约束。为了解决这些难题，本文提出了ISMP框架。", "method": "该方法采用多方向安全矩形走廊（MDSRC）算法来编码自由空间并避免与静态障碍物碰撞，同时提出了一种顺序MPC导航框架以实现对静态和动态障碍物的规避。此框架直接生成AMR的速度指令，简化了传统的导航架构。", "result": "实验结果表明该框架在自由空间利用上提升了41.05%，并且保持了实时计算性能（平均走廊生成延迟为3ms）。", "conclusion": "ISMP导航框架在解决复杂环境中的自主移动机器人导航问题方面表现出了优越性，能够高效地避免障碍物并优化路径。"}}
{"id": "2512.13214", "pdf": "https://arxiv.org/pdf/2512.13214", "abs": "https://arxiv.org/abs/2512.13214", "authors": ["Diego Bolliger", "Gabriele Fadini", "Markus Bambach", "Alisa Rupenyan"], "title": "Differentiable Material Point Method for the Control of Deformable Objects", "categories": ["cs.RO", "eess.SY"], "comment": "7 Pages, 4 Figures, 1 Table", "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.", "AI": {"tldr": "本文提出了一种可微分的材料点方法模拟器，用于控制柔性物体的变形。", "motivation": "由于柔性物体具有非线性动力学和高维配置空间，因此难以对其变形进行精确控制。该研究旨在通过利用可微分模拟器优化控制轨迹来解决这一问题。", "method": "本文开发了一种基于不同iable Material Point Method (MPM) 的方法，并应用于控制任务中，特别是在主动阻尼问题中对超弹性绳索的动能进行了优化。", "result": "相比基线 MPPI 方法，该方法将绳索的动能降至原来水平的20%以下，在相同的时间内效率提高了约3%，并且实现了更快的速度（大约两倍）。", "conclusion": "通过利用可微分模拟器，可以有效提高对柔性物体变形控制的效果和效率。"}}
{"id": "2512.13210", "pdf": "https://arxiv.org/pdf/2512.13210", "abs": "https://arxiv.org/abs/2512.13210", "authors": ["Marin Bougeret", "Eric Brandwein", "Ignasi Sau"], "title": "Kernelization dichotomies for hitting minors under structural parameterizations", "categories": ["cs.DS", "cs.CC", "math.CO"], "comment": "50 pages, 13 figures. Conference version in STACS 2026", "summary": "For a finite collection of connected graphs $\\mathcal{F}$, the $\\mathcal{F}$-MINOR-DELETION problem consists in, given a graph $G$ and an integer $\\ell$, deciding whether $G$ contains a vertex set of size at most $\\ell$ whose removal results in an $\\mathcal{F}$-minor-free graph. We lift the existence of (approximate) polynomial kernels for $\\mathcal{F}$-MINOR-DELETION by the solution size to (approximate) polynomial kernels parameterized by the vertex-deletion distance to graphs of bounded elimination distance to $\\mathcal{F}$-minor-free graphs. This results in exact polynomial kernels for every family $\\mathcal{F}$ that contains a planar graph, and an approximate polynomial kernel for PLANAR VERTEX DELETION. Moreover, combining our result with a previous lower bound, we obtain the following infinite set of dichotomies, assuming $NP \\not\\subseteq coNP/poly$: for any finite set $\\mathcal{F}$ of biconnected graphs on at least three vertices containing a planar graph, and any minor-closed class of graphs $\\mathcal{C}$, $\\mathcal{F}$-MINOR-DELETION admits a polynomial kernel parameterized by the vertex-deletion distance to $\\mathcal{C}$ if and only if $\\mathcal{C}$ has bounded elimination distance to $\\mathcal{F}$-minor-free graphs. For instance, this yields dichotomies for CACTUS VERTEX DELETION, OUTERPLANAR VERTEX DELETION, and TREEWIDTH-$t$ VERTEX DELETION for every integer $t \\geq 0$. Prior to our work, such dichotomies were only known for the particular cases of VERTEX COVER and FEEDBACK VERTEX SET. Our approach builds on the techniques developed by Jansen and Pieterse [Theor. Comput. Sci. 2020] and also uses adaptations of some of the results by Jansen, de Kroon, and Wlodarczyk [STOC 2021].", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.13198", "pdf": "https://arxiv.org/pdf/2512.13198", "abs": "https://arxiv.org/abs/2512.13198", "authors": ["Hyun-Gi Lee", "Jaekyeong Han", "Minjun Kwon", "Hyeonuk Kwon", "Jooha Park", "Hoe Jin Ha", "Dong-Hwa Seo"], "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation", "categories": ["cs.RO"], "comment": null, "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.", "AI": {"tldr": "该论文介绍了ALBATROSS，一种用于高通量电解液筛选的自动化系统。", "motivation": "随着电池技术的进步，需要通过广泛的组件配置测试来评估性能和理解操作原理。传统的硬币单元组装和测试过程耗时且劳动密集，限制了高通量筛选研究的发展。", "method": "ALBATROSS是一个集成在氩气填充手套箱内的自动化系统，能够进行电解质配方、硬币电池装配以及电化学评价。该系统采用了定制的机器人夹具和3D打印结构以实现精准的单元处理。", "result": "通过高可靠性组装，ALBATROSS实现了小于1.2%的放电容量相对标准偏差及NMC811||Li半电池EIS测量的标准偏差小于3Ω。这表明系统能够生成高质量的数据集，从而加速下一代电解质的发展。", "conclusion": "鉴于其可靠性和自动化能力，ALBATROSS可以采集高通量硬币单元数据集，进而促进新型电解液的研究和发展。"}}
{"id": "2512.13194", "pdf": "https://arxiv.org/pdf/2512.13194", "abs": "https://arxiv.org/abs/2512.13194", "authors": ["Chendong Sun", "mingmin Chen", "Lei Xu"], "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": ":68T50ACM Class:I.2.7", "summary": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as 1 - max(P_target). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.", "AI": {"tldr": "该论文提出了Efficient Adaptive Rejection Sampling (EARS) 方法，以提高大型语言模型中的投机解码效率。", "motivation": "在高不确定性生成场景下，现有的固定拒绝采样机制会导致大量合理的候选令牌因随机机会而被拒绝，从而降低推断效率。因此，需要一种能够根据目标模型的预测不确定性动态调整接受阈值的方法。", "method": "EARS 方法通过引入一个与目标模型自身预测不确定性成比例的容忍项来智能地放松接受标准，从而减少随机拒绝同时保持严格的接受标准，当模型自信时。", "result": "实验结果表明，在创意写作和开放域问答任务上，EARS 显著提高了投机解码的效率，实现了高达18.12% 的吞吐量提升，并且在GSM8K基准测试中的准确率仅下降0.84%。", "conclusion": "EARS 方法无需对模型架构进行修改即可无缝集成到现有的投机解码框架中，从而提高大型语言模型的推断效率。"}}
{"id": "2512.13192", "pdf": "https://arxiv.org/pdf/2512.13192", "abs": "https://arxiv.org/abs/2512.13192", "authors": ["Zhuo Chen", "Chengqun Yang", "Zhuo Su", "Zheng Lv", "Jingnan Gao", "Xiaoyuan Zhang", "Xiaokang Yang", "Yichao Yan"], "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling", "categories": ["cs.CV"], "comment": "19 pages, 19 figures", "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination. Our project page: https://rex0191.github.io/POLAR/.", "AI": {"tldr": "本文提出了POLAR数据集和生成框架，用于在保持身份和几何结构的同时对人脸进行光照感知建模。", "motivation": "目前面部光照合成技术受制于大规模物理一致性的光照数据的稀缺性。为解决这一问题，研究者们开发了包含大量高质量图像的OLAT数据集POLAR，并建立了基于该数据集的生成模型PolarNet。", "method": "提出了一种名为POLARNet的基于流的生成模型，用于预测单一肖像在不同光照下的反应。与依赖于统计或上下文线索的方法不同，此方法通过物理可解释的方式建模光照作为照明状态之间的连续转换。", "result": "实验结果展示了PolarNet能够合成高质量、具有方向感知能力的人脸图像，并且保持了面部的身份特征。", "conclusion": "POLAR数据集和POLARNet生成模型为大规模人脸光照模拟提供了有效的解决方案，建立了从真实数据到生成合成再到物理基础的光照学习框架。"}}
{"id": "2512.13191", "pdf": "https://arxiv.org/pdf/2512.13191", "abs": "https://arxiv.org/abs/2512.13191", "authors": ["Gong Chen", "Chaokun Zhang", "Pengcheng Lv", "Xiaohui Xie"], "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception", "categories": ["cs.CV"], "comment": "Accepted by AAAI2026", "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.", "AI": {"tldr": "提出了一种名为CoRA的协作感知架构，结合了特征级融合和对象级校正分支，以提高通信效率并增强鲁棒性。", "motivation": "现有方法在恶劣通信条件下性能下降，因为数据传输会导致对齐问题。为了解决这一挑战，提出了CoRA架构来实现高效且稳健的协作感知。", "method": "该论文提出了一种名为CoRA的新颖框架，包括特征级融合分支和对象级校正分支：第一部分选择关键特征进行有效融合以确保性能和可扩展性；第二部分利用语义相关性纠正空间偏移，保证对抗姿势误差的鲁棒性。", "result": "实验结果显示，即使在极端条件下，CoRA相较于基线方法提高了大约19%的AP@0.7，并且通信量减少了5倍以上。", "conclusion": "CoRA架构提供了一种高效且稳健的协作感知解决方案，证明了其在各种条件下的优越性能。"}}
{"id": "2512.13190", "pdf": "https://arxiv.org/pdf/2512.13190", "abs": "https://arxiv.org/abs/2512.13190", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Dongil Park", "Sung Won Han"], "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)", "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.", "AI": {"tldr": "WAY模型用于估计全球范围内的船舶目的地，通过处理AIS轨迹数据实现。", "motivation": "自动识别系统（AIS）在海上监控中存在可靠性和时间间隔不规则的问题。本研究旨在利用AIS数据解决这一问题，并提出一种新的方法来预测船舶的目的地。", "method": "WAY模型采用空间网格划分和深度学习架构，将船到港的轨迹转化为嵌套序列结构，通过多通道向量序列生成器处理时空特征，并使用梯度丢弃（GD）技术防止训练偏差。该模型包括轨迹表示层和Channel-Aggregative Sequential Processing (CASP) 块。", "result": "实验结果表明WAY模型在预测船舶目的地方面优于传统方法，且采用GD技术提高了模型性能。", "conclusion": "WAY模型能够准确估计全球范围内的船舶目的地，并展现出在实际应用中的潜力。"}}
{"id": "2512.13186", "pdf": "https://arxiv.org/pdf/2512.13186", "abs": "https://arxiv.org/abs/2512.13186", "authors": ["Khalid Ferji"], "title": "PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.", "AI": {"tldr": "本文提出了PolySet框架，将聚合物表示为从假设的分子量分布中采样的有限加权链集合。", "motivation": "当前聚合物机器学习模型通常将聚合物视为单一、确定性的分子图，这与实际材料中的随机聚集体不匹配，限制了其捕捉聚合物行为的能力。", "method": "PolySet框架使用一种最小语言模型来表示同聚物，并且该框架独立于化学细节，可以兼容任何分子表示。", "result": "PolySet保持高阶分布矩（如Mz，Mz+1），使机器学习模型能够更稳定、准确地学习尾部敏感特性。", "conclusion": "通过明确承认聚合物物质的统计性质，PolySet为未来的聚合物机器学习奠定了物理基础，并且可以自然扩展到共聚物、嵌段结构和其他复杂拓扑结构。"}}
{"id": "2512.13183", "pdf": "https://arxiv.org/pdf/2512.13183", "abs": "https://arxiv.org/abs/2512.13183", "authors": ["Alfredo González-Calvin", "Juan F. Jiménez", "Héctor García de Marina"], "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.", "AI": {"tldr": "本文提出了一种通过正则化非可微函数和生成可行路径的方法，特别是在连接一系列航点时。", "motivation": "大多数移动机器人路径跟踪算法需要至少两次连续可微的函数来保证全局收敛等关键特性。然而这些算法通常排除了诸如分段函数这样的连续但不可微路径。", "method": "本文通过正则化非可微函数并生成可行路径的方法，利用插值和约束曲率来逼近任意路径，并演示应用于航点连接的情况。", "result": "该方法能生成满足标准轨迹跟踪算法的简化且具有边界曲率的路径，实现计算效率高并且适用于实时微控制器应用。", "conclusion": "本文提出的方法能够高效地生成平滑路径并提供曲率保证，适用于移动机器人在非全可微路径场景下的使用。"}}
{"id": "2512.13177", "pdf": "https://arxiv.org/pdf/2512.13177", "abs": "https://arxiv.org/abs/2512.13177", "authors": ["Minghui Hou", "Wei-Hsing Huang", "Shaofeng Liang", "Daizong Liu", "Tai-Hao Wen", "Gang Wang", "Runwei Guan", "Weiping Ding"], "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.", "AI": {"tldr": "MMDrive提出了一种用于自动驾驶的多模态视觉语言模型框架，通过融合图像、点云和文本描述来实现三维场景理解。", "motivation": "现有的视觉语言模型受限于二维平面中的图像理解，难以感知三维空间信息并进行深度语义融合，在复杂的驾驶环境中表现不佳。", "method": "MMDrive引入了两种新型组件：基于文本的多模态调制器和跨模态抽象器，前者根据问题的语义提示动态加权每种模式的重要性；后者利用可学习的摘要令牌生成紧凑、关键区域突出的跨模态摘要。", "result": "在DriveLM和NuScenes-QA基准测试中，MMDrive表现出显著优于现有视觉语言模型的表现，在DriveLM上BLEU-4得分为54.56，METEOR为41.78；在NuScenes-QA上的准确率为62.7%。", "conclusion": "MMDrive突破了传统的单一图像理解限制，实现了复杂驾驶环境下的稳健多模态推理，并提供了可解释的自动驾驶场景理解的新基础。"}}
{"id": "2512.13175", "pdf": "https://arxiv.org/pdf/2512.13175", "abs": "https://arxiv.org/abs/2512.13175", "authors": ["Hongxuan Sun", "Tao Wu"], "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.", "AI": {"tldr": "本文提出了DFSS，一种针对语义分割的数据无关知识蒸馏框架，通过利用教师模型的BN统计信息指导近似分布采样来选择更符合原始数据分布的数据。", "motivation": "现有数据无关的知识蒸馏方法主要为分类设计，忽视了语义分割任务中像素间结构和上下文连贯性的特点，导致直接应用于分割时性能下降。", "method": "DFSS通过利用教师模型的BN统计信息来指导近似分布采样，并提出加权分布渐进式蒸馏WDPD方法，在训练早期优先使用更接近原始数据分布的可靠样本。", "result": "实验表明，DFSS在标准基准上显著优于现有数据无关知识蒸馏方法，达到了最先进的性能水平，并且大大减少了对辅助数据的依赖。", "conclusion": "DFSS通过尊重真实场景中的结构和上下文连贯性，在语义分割任务中实现了更好的知识蒸馏效果。"}}
{"id": "2512.13174", "pdf": "https://arxiv.org/pdf/2512.13174", "abs": "https://arxiv.org/abs/2512.13174", "authors": ["Francesco Salvi", "Giuseppe Russo", "Adam Barla", "Vincent Moreau", "Robert West"], "title": "Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments", "categories": ["econ.GN", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.", "AI": {"tldr": "研究通过价格激励措施减少肉类消费，评估不同策略在真实环境中对蔬菜餐选择和碳排放的影响。", "motivation": "肉类消费是全球温室气体排放的主要因素。现有研究多集中于限制性较强的环境，缺乏实际竞争条件下的数据。本研究旨在探索在开放环境下，不同的价格干预手段如何影响消费者行为及减少食物系统的碳足迹。", "method": "采用瑞士大学校园内四个食堂作为实验场所，通过顺序交叉设计与匹配菜单的方式，系统地比较了蔬菜餐打折、肉类加价以及两者结合的策略。这些措施实施时间周期分别为四周，并追踪观察每种方案对素食选择的影响及二氧化碳排放量的变化。", "result": "只有肉类加价和综合方案显著增加了素食的选择率，分别提高了26.4%和16.6%，并减少了7.4%和11.3%的二氧化碳排放。然而，单独使用加价措施会导致销售额下降，并引发消费者转向非干预场所购买，从而完全抵消了环境收益；而综合方案实现了有意义的减排效果，同时没有显著影响整体销售或收入。", "conclusion": "研究表明，在现实的食物环境中实施平衡的价格策略可以减少碳足迹。然而，要最大化气候效益并避免意外溢出效应，需要协调一致地执行这些措施。"}}
{"id": "2512.13173", "pdf": "https://arxiv.org/pdf/2512.13173", "abs": "https://arxiv.org/abs/2512.13173", "authors": ["Ivica Kostric", "Ujwal Gadiraju", "Krisztian Balog"], "title": "Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders", "categories": ["cs.IR", "cs.HC"], "comment": null, "summary": "The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.", "AI": {"tldr": "介绍了一个新任务：从对话中估计用户领域知识，以使推荐系统更好地理解用户需求并个性化互动。", "motivation": "大多数现有的对话推荐系统忽略了用户的领域知识水平，导致与新手用户的交互效率低下且令人沮丧。为了实现更加自然有效的用户体验，需要开发能够根据用户知识水平调整其策略的自适应系统。然而，缺乏合适的数据成为主要障碍。", "method": "设计了一个游戏化的数据收集协议以激发各种知识表达，并发布了由此产生的数据集和初步分析结果。", "result": "提出了一种新的任务并提供了相应的数据集用于未来研究用户知识感知对话推荐系统的潜力。", "conclusion": "通过引入新任务及相应数据集，有望促进更加自然且个性化用户体验的发展。"}}
{"id": "2512.13170", "pdf": "https://arxiv.org/pdf/2512.13170", "abs": "https://arxiv.org/abs/2512.13170", "authors": ["Deepak Ingole", "Valentin Bhend", "Shiva Ganesh Murali", "Oliver Dobrich", "Alisa Rupenayan"], "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks", "categories": ["cs.RO", "cs.LG", "eess.SY", "math.OC"], "comment": null, "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.", "AI": {"tldr": "本文提出了一种基于任务级性能反馈的非线性模型预测控制（NMPC）权重矩阵迭代调优框架，通过自适应调整权重以提高轨迹跟踪精度和减少饱和现象。", "motivation": "由于制造过程中环境扰动和系统磨损导致需要重新调节控制器参数，该研究旨在开发一种自动迭代学习方法来优化NMPC的性能指标。", "method": "基于范数最优迭代控制（ILC）的思想，本文提出了一种构建经验灵敏度矩阵的方法以调整NMPC权重Q和R，并通过四面体芯碳纤维缠绕实验验证了该框架的有效性。", "result": "在模拟中的UR10e机器人任务中，所提出的策略仅需4次在线重复即达到接近最优的跟踪性能（RMSE低于离线贝叶斯优化方法）。", "conclusion": "本文的方法提供了一种结合精确控制和灵活调整能力的实际解决方案，在反复执行的机器人制造任务中适应性地调优NMPC。"}}
{"id": "2512.13168", "pdf": "https://arxiv.org/pdf/2512.13168", "abs": "https://arxiv.org/abs/2512.13168", "authors": ["Haoyu Dong", "Pengkun Zhang", "Yan Gao", "Xuanyu Dong", "Yilin Cheng", "Mingzhe Lu", "Adina Yakefu", "Shuxin Zheng"], "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "categories": ["cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": null, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management. We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work. We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "AI": {"tldr": "该论文介绍了Finch基准测试，用于评估AI代理在企业级专业工作流中的性能。", "motivation": "旨在通过实际企业环境中复杂的工作流程来真实地检验AI代理的能力。", "method": "结合LLM辅助发现和专家注释构建工作流程，并使用这些工作流程进行人类和自动化评价。", "result": "前沿的AI系统在Finch基准测试中的表现不佳，例如GPT 5.1通过了38.4%的工作流，而Claude Sonnet 4.5仅通过25.0%的工作流。", "conclusion": "Finch基准揭示了现实企业工作流程对AI代理的挑战。"}}
{"id": "2512.13165", "pdf": "https://arxiv.org/pdf/2512.13165", "abs": "https://arxiv.org/abs/2512.13165", "authors": ["Jakub Łyskawa", "Jakub Lewandowski", "Paweł Wawrzyński"], "title": "SACn: Soft Actor-Critic with n-step Returns", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at ICAART 2026", "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.", "AI": {"tldr": "本文提出了一种结合软行为者评论家（SAC）与n步回报的方法，解决了两者组合中的数值稳定性问题，并在MuJoCo仿真环境中验证了方法的有效性。", "motivation": "由于传统的SAC和n步回报的结合会导致偏见并引入数值不稳定性的问题，作者旨在开发一种稳定的重要性采样方法来解决这一挑战。", "method": "本文提出了一种应用数值稳定的重采样技术的方法，并简化了超参数的选择。同时分析了软行为者评论家中的熵估计方式，在n步最大熵框架下提出了τ样本熵估计以减少学习目标的方差，最终形成了SACn算法。", "result": "实验表明，所提出的SACn方法在MuJoCo仿真环境中具有较高的收敛速度和稳定性能。", "conclusion": "本文成功解决了将软行为者评论家与n步回报结合中的数值不稳定性问题，并展示了新提出的方法的有效性和优越性。"}}
{"id": "2512.13164", "pdf": "https://arxiv.org/pdf/2512.13164", "abs": "https://arxiv.org/abs/2512.13164", "authors": ["Xianchao Guan", "Zhiyuan Fan", "Yifeng Wang", "Fuqiang Chen", "Yanjiang Zhou", "Zengyang Che", "Hongxue Meng", "Xin Li", "Yaowei Wang", "Hongpeng Wang", "Min Zhang", "Heng Tao Shen", "Zheng Zhang", "Yongbing Zhang"], "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": "68 pages, 9 figures, 16 tables", "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.", "AI": {"tldr": "本文介绍了一种用于病理图像合成的生成基础模型CRAFTS，解决了现有技术在生物准确性上的不足。", "motivation": "临床级人工智能在病理学的发展受限于高质量、多样化的数据集稀缺。为了克服这一挑战，研究人员开发了CRAFTS框架以提高病理图像文本到图像合成的质量和可靠性。", "method": "利用双阶段训练策略，在大约280万张图-描述配对上进行训练，引入了一种新的对齐机制来抑制语义漂移，确保生物准确性。此外，结合ControlNet实现了对组织架构的精确控制。", "result": "CRAFTS生成了涵盖30种癌症类型的多样化病理图像，并通过客观标准和病理科医生评估进行了严格验证。增强的数据集提高了多种临床任务的表现。", "conclusion": "CRAFTS为罕见和复杂癌症表型提供了无限量的多样、注释齐全的组织学数据来源，有助于开发强大的诊断工具，克服了数据稀缺性和隐私顾虑的关键障碍。"}}
{"id": "2512.13159", "pdf": "https://arxiv.org/pdf/2512.13159", "abs": "https://arxiv.org/abs/2512.13159", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Jie Hao", "Joo Hyuk Jeon", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.", "AI": {"tldr": "SpeakRL是一种通过强化学习增强代理对话能力的方法，它奖励必要的澄清提问，以实现更有效的用户与代理互动。", "motivation": "当前的用户-代理交互多为单向沟通，缺乏主动性。为了改善这种情况，作者提出了一种新的方法来优化语言模型在对话中的表现，使其更加主动和有效。", "method": "SpeakRL利用强化学习技术，通过设计奖励机制鼓励代理在必要时提问以澄清用户的意图，并引入了SpeakER数据集用于训练与评估。", "result": "实验结果表明，相比于基准模型，SpeakRL显著提高了任务完成率（提高20.14%），同时减少了对话轮次。这种方法甚至超过了更大规模的专有模型的表现。", "conclusion": "该研究展示了通过强化学习和适当的奖励设计来促进用户-代理之间的澄清性互动具有很大的潜力，并且这种技术可以有效地应用于现实世界的任务导向对话中。"}}
{"id": "2512.13157", "pdf": "https://arxiv.org/pdf/2512.13157", "abs": "https://arxiv.org/abs/2512.13157", "authors": ["Peter Kocsis", "Lukas Höllein", "Matthias Nießner"], "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://peter-kocsis.github.io/IntrinsicImageFusion/ Video: https://www.youtube.com/watch?v=-Vs3tR1Xl7k", "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.", "AI": {"tldr": "本文提出了一种基于单视图先验的多视角图像融合方法，用于高质量物理基材料重建。", "motivation": "传统的材料重建依赖于昂贵且嘈杂的路径追踪分析。为了更好地约束优化过程，引入了单视图先验来减少重建中的不确定性。", "method": "本文利用扩散模型生成多个候选分解，并通过拟合低维参数函数来降低预测不一致性；采用软视角预测选择和基于置信度的多视角内点集融合一致预测；最后使用逆路径追踪优化参数。", "result": "实验结果表明，该方法在合成与真实场景上的材料解缠上优于现有技术，能够生成清晰且高质量的重建。", "conclusion": "提出的方法显著提高了物理基材料重建的质量和准确性，在多视角图像融合中表现优异。"}}
{"id": "2512.13154", "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-Tür", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.", "AI": {"tldr": "提出MAC框架用于解决多轮对话中的用户请求模糊问题，通过策略性地管理澄清对话来提高任务成功率。", "motivation": "在复杂的交互场景中，确定哪个代理应该发起澄清以及如何协调多个代理以应对不确定或不完整的用户输入是需要解决的关键挑战。", "method": "提出了一种新的用户歧义分类法，并引入MAC多代理框架，该框架可以自主地协调多个代理与用户互动以解决问题。", "result": "在MultiWOZ2.4数据集上的实证评估表明，通过启用澄清对话，任务成功率提高了7.8%，平均对话轮次减少了1.67轮。", "conclusion": "研究表明主动的用户交互和角色感知的澄清策略对于提高人类与代理之间的通信可靠性至关重要。"}}
{"id": "2512.13153", "pdf": "https://arxiv.org/pdf/2512.13153", "abs": "https://arxiv.org/abs/2512.13153", "authors": ["Ruiqi Yu", "Qianshi Wang", "Hongyi Li", "Zheng Jun", "Zhicheng Wang", "Jun Wu", "Qiuguo Zhu"], "title": "START: Traversing Sparse Footholds with Terrain Reconstruction", "categories": ["cs.RO"], "comment": null, "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.", "AI": {"tldr": "提出了一种名为START的单阶段学习框架，使四足机器人能够在稀疏地形上实现灵活、稳定的行走。", "motivation": "当前模型和端到端学习方法在处理具有稀疏支撑点的地形时存在局限性。基于模型的方法缺乏泛化能力且过于保守；而现有的最佳端到端学习方法要么依赖引入噪声的高度图，要么无法准确捕捉关键几何线索，导致学习效率低下。", "method": "提出了一种单阶段框架START，利用低成本的车载视觉和本体感觉进行局部地形高度重建。这种方法提供了一个明确的中间表示来传达稀疏支撑点区域的重要特征，从而支持对环境的全面理解以及精确地评估地形。", "result": "实验结果显示，START实现了跨各种现实场景的零样本迁移，并展示了其优越的适应性、准确的脚步放置和稳健的行走能力。", "conclusion": "提出的START框架能够有效地解决四足机器人在稀疏地形上的行走问题，证明了该方法的有效性和实用性。"}}
{"id": "2512.13147", "pdf": "https://arxiv.org/pdf/2512.13147", "abs": "https://arxiv.org/abs/2512.13147", "authors": ["Sangmin Hong", "Suyoung Lee", "Kyoung Mu Lee"], "title": "StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion", "categories": ["cs.CV"], "comment": "11 pages", "summary": "The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.", "AI": {"tldr": "利用单目深度估计模型从单一稀疏深度图和RGB图像预测密集深度图像。", "motivation": "现有的无监督深度补全方法需要辅助数据来估计深度值，这与实际场景不符。单目深度估计可以产生相对深度图，但未有研究将其与稀疏深度图结合进行深度补全。StarryGazer框架旨在解决这一问题。", "method": "使用预训练的单目深度估计模型生成相对深度图像，并对其进行分割和随机缩放以形成合成对；利用这些合成对训练一个改进网络，将相对深度地图和RGB图像结合起来提高精度和鲁棒性。", "result": "在多个数据集上展示出比现有无监督方法更好的结果，证明了StarryGazer框架能够有效利用单目深度估计模型并使用稀疏深度信息纠正错误。", "conclusion": "提出了一种无需依赖地面真实数据的领域通用深度补全框架，通过结合单目深度估计模型和稀疏深度图实现更准确、鲁棒性强的结果。"}}
{"id": "2512.13144", "pdf": "https://arxiv.org/pdf/2512.13144", "abs": "https://arxiv.org/abs/2512.13144", "authors": ["Chun Kit Wong", "Paraskevas Pegios", "Nina Weng", "Emilie Pi Fogtmann Sejer", "Martin Grønnebæk Tolsgaard", "Anders Nymark Christensen", "Aasa Feragen"], "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": "46 pages", "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.", "AI": {"tldr": "本文提出了权重空间相关性分析方法，用于量化深度学习模型在医疗影像任务中对特征的利用情况。", "motivation": "深度学习模型在医学成像中的准确性容易受到与实际临床信号无关的信息（如扫描器型号）的影响。因此需要一种能够验证模型是否仅依赖于真实临床信号的方法。", "method": "通过测量分类头与辅助元数据任务之间的对齐情况，提出了一种可解释的量化特征利用的方法：权重空间相关性分析。", "result": "实验结果表明，在没有人为引入偏见的情况下，sPTB预测模型的权重向量高度关联于临床相关的因素（如出生体重），而不依赖无关的采集因素（如扫描器）。", "conclusion": "提出的这种方法能够帮助验证深度学习模型的信任度，确保其特征利用与实际临床信号相关。"}}
{"id": "2512.13142", "pdf": "https://arxiv.org/pdf/2512.13142", "abs": "https://arxiv.org/abs/2512.13142", "authors": ["Anika Sharma", "Malavika Mampally", "Chidaksh Ravuru", "Kandyce Brennan", "Neil Gaikwad"], "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "categories": ["cs.AI", "cs.HC"], "comment": ":I.2.7; H.5.0; J.3; J.4; K.4.1; K.4.2", "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.", "AI": {"tldr": "研究探讨了大型语言模型是否能够理解关于堕胎的污名化，特别是在认知、人际和结构三个层面上。", "motivation": "随着大型语言模型在涉及健康决策中的使用越来越多，对其理解和复杂心理生理现象的能力评价仍然不足。文章旨在评估这些模型对堕胎污名化的理解能力。", "method": "研究通过系统地测试了627个不同人口特征的虚拟角色，并利用有效的人类水平堕胎污名量表(ILAS)来评估五种领先的语言模型的表现。", "result": "结果显示，语言模型在理解和预测污名化方面存在不足。它们高估人际层面的污名化，低估认知层面的污名化；假设社区对所有群体有一致的谴责态度，并引入了人类数据中不存在的人口统计学偏差。这些模式表明目前的对齐方法确保了适当的语言使用但未能实现多层理解。", "conclusion": "这项研究提供了实证证据，证明当前的语言模型缺乏关于心理学和生理学构造的一致多层次理解。在高风险情境中的AI安全性需要新的设计、评估、治理与监管方法以及提高AI领域的知识水平。"}}
{"id": "2512.13131", "pdf": "https://arxiv.org/pdf/2512.13131", "abs": "https://arxiv.org/abs/2512.13131", "authors": ["Xin Guo", "Yifan Zhao", "Jia Li"], "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.MM", "cs.SD"], "comment": "IEEE Transactions on Image Processing", "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.", "AI": {"tldr": "本文提出了一种基于音频的三维手势生成方法，通过层次隐式周期学习解决现有研究难以模拟能体现真实人体运动的协调性问题。", "motivation": "现有的端到端生成方案在模拟现实人类动作时存在不足，未能充分考虑头部、身体和手部之间的复杂关联。本文旨在改善这一问题。", "method": "提出了一种层次隐式周期（HIP）学习方法来建模多模式关系，通过周期自动编码器分离复杂的姿态运动，并用分层指导驱动动画以模拟面部动作、身体手势和手部运动。", "result": "实验显示该方法在定量和定性评估中优于现有最先进的共言语手势生成方法。", "conclusion": "本文提出的HIP学习框架能够更准确地模拟能体现真实人类行为的三维姿态，同时保证了各部分间的协调性。"}}
{"id": "2512.13130", "pdf": "https://arxiv.org/pdf/2512.13130", "abs": "https://arxiv.org/abs/2512.13130", "authors": ["Shanghua Liu", "Majharulislam Babor", "Christoph Verduyn", "Breght Vandenberghe", "Bruno Betoni Parodi", "Cornelia Weltzien", "Marina M. -C. Höhne"], "title": "LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping", "categories": ["cs.CV"], "comment": null, "summary": "High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.", "AI": {"tldr": "提出了一种新的深度学习框架LeafTrackNet，用于复杂作物（如油菜）的叶片追踪。", "motivation": "当前缺乏适用于结构复杂的作物叶片跟踪方法，现有植物特定的方法通常局限于小规模物种或受限成像条件。通用多目标跟踪方法不设计为动态生物场景。", "method": "引入CanolaTrack数据集，并提出LeafTrackNet框架结合YOLOv10和MobileNetV3嵌入网络进行准确的叶片追踪。", "result": "LeafTrackNet在CanolaTrack上优于其他跟踪器，实现了9%的HOTA改进。", "conclusion": "提供新的标准用于现实条件下叶片级别的跟踪，并提供了最大的农业作物叶子跟踪数据集。"}}
{"id": "2512.13122", "pdf": "https://arxiv.org/pdf/2512.13122", "abs": "https://arxiv.org/abs/2512.13122", "authors": ["Vivek Alumootil", "Tuan-Anh Vu", "M. Khalid Jawed"], "title": "DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass", "categories": ["cs.CV", "cs.AI"], "comment": "This is a work in progress", "summary": "Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R", "AI": {"tldr": "本文提出了一种名为DePT3R的框架，能够同时从多幅图像中进行密集点追踪和3D重建，并且在单次前向传递中完成。", "motivation": "当前的方法通常依赖于成对处理、已知相机姿态或输入帧的时间顺序，限制了其灵活性与适用性。因此提出一种能够在无需相机姿态的情况下同时执行密集点追踪和3D重建的新框架。", "method": "通过提取强大的骨干网络中的深度时空特征并使用密集预测头回归像素级映射图来实现多任务学习。", "result": "在涉及动态场景的几个具有挑战性的基准测试中，DePT3R展示了强劲的表现，并且在内存效率上比现有的最先进方法有了显著提高。", "conclusion": "DePT3R提供了一种新的解决动态场景密集点追踪和3D重建问题的方法，在不依赖相机姿态的情况下增强了其适应性和效率。"}}
{"id": "2512.13111", "pdf": "https://arxiv.org/pdf/2512.13111", "abs": "https://arxiv.org/abs/2512.13111", "authors": ["Hayk Amirkhanian", "Marco F. Huber"], "title": "From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages main body, 1 Figure, 15 pages Appendix", "summary": "In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.", "AI": {"tldr": "提出了一种新的层次化近似贝叶斯神经网络方法，以提高模型的鲁棒性和性能。", "motivation": "解决超参数调整和过拟合等问题，通过引入不确定性直接改善模型预测可靠性。", "method": "使用高斯逆威沙特分布作为权重的先验，提供闭式解形式的学生t分布参数计算。", "result": "实验结果显示HABNN不仅匹配而且经常超越当前最佳模型，在处理出界数据时表现更佳。", "conclusion": "提出的层次化近似贝叶斯神经网络在解决过拟合和不确定性估计方面展示出了显著优势，适合应用于安全关键环境。"}}
{"id": "2512.13109", "pdf": "https://arxiv.org/pdf/2512.13109", "abs": "https://arxiv.org/abs/2512.13109", "authors": ["Zewen Qiang", "Sendong Zhao", "Haochun Wang", "Bing Qin", "Ting Liu"], "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.", "AI": {"tldr": "揭示初始显着性在U形注意力偏置中的作用，通过调整初始令牌权重以增强长文本处理能力。", "motivation": "大型语言模型在处理长文本文档时面临中间部分被忽视的问题。现有研究主要归因于位置编码问题，而该研究表明初始显着性也是一个重要因素，并提出一种解决方案来改善这种情况。", "method": "通过分析令牌注意力权重发现，相对于初始令牌具有更高注意权重的令牌在其后续预测中获得更多的关注。基于此特性提出了调整初始令牌与其它令牌之间注意力权重的方法，以增强模型对长文本处理能力。", "result": "该方法在MDQA数据集上实现了最高3.6%的表现提升，在结合位置编码偏置减少方法后，在KV-Retrieval任务中实现了最大3.4%的性能改进。", "conclusion": "初始显着性是U形注意力偏置的一个重要因素，通过调整初始令牌权重可以有效改善模型处理长文本的能力。"}}
{"id": "2512.13107", "pdf": "https://arxiv.org/pdf/2512.13107", "abs": "https://arxiv.org/abs/2512.13107", "authors": ["Zhijian He", "Feifei Liu", "Yuwei Li", "Zhanpeng Liu", "Jintao Cheng", "Xieyuanli Chen", "Xiaoyu Tang"], "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.", "AI": {"tldr": "提出了一种基于扩散模型的多模态3D物体检测增强框架DiffFusion，以提高恶劣天气条件下的鲁棒性。", "motivation": "在恶劣天气条件下，由于天气引起的失真和不同数据模式之间的不匹配，多模态3D对象检测的有效性受到限制。因此，开发了一种新的扩散模型为基础的恢复方法来应对这些问题。", "method": "DiffFusion框架包括两个核心部分：图像恢复（Diffusion-IR）和点云修复（PCR），以及用于解决不同数据模式之间不匹配问题的双向自适应融合与对齐模块(BAFAM)。该框架能够增强恶劣天气条件下的鲁棒性，同时保持在干净数据上的良好性能。", "result": "实验结果显示，所提出的DiffFusion模型在三个公开的数据集上实现了最先进的鲁棒性，并且在现实世界的DENSE数据集上也验证了其泛化能力。", "conclusion": "通过引入扩散模型和自适应融合技术，DiffFusion框架成功地提高了多模态3D物体检测在恶劣天气条件下的性能。"}}
{"id": "2512.13106", "pdf": "https://arxiv.org/pdf/2512.13106", "abs": "https://arxiv.org/abs/2512.13106", "authors": ["Shenzhi Yang", "Guangcheng Zhu", "Xing Zheng", "Yingfan MA", "Zhongqi Chen", "Bowen Song", "Weiqiang Wang", "Junbo Zhao", "Gang Chen", "Haobo Wang"], "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.", "AI": {"tldr": "本文提出了一种半监督的强化学习框架TraPO，以增强大型语言模型（LLM）的推理能力。", "motivation": "现有的基于可验证奖励的强化学习方法存在高注释成本问题，并且在缺乏外部监督的情况下容易导致模型崩溃。因此，作者提出了一个利用少量标记数据来指导未标记样本训练的新颖半监督框架。", "method": "TraPO通过匹配未标注样本的学习轨迹相似度与已标注样本的方法识别可靠的未标注样本，并在此基础上优化策略以提高数据效率和泛化能力。", "result": "在六个数学推理基准测试上，使用1K标记和3K未标记样本时，TraPO的平均准确率达到42.6%，超过仅用45K未标记样本训练的最佳无监督方法（38.3%）。当使用4K标记和12K未标记样本时，它在所有基准测试上超过了完全监督模型的表现。", "conclusion": "本文证明了半监督框架TraPO能够在有限的标注数据下提高大型语言模型的推理性能，并展示了其优越的数据效率和泛化能力。"}}
{"id": "2512.13105", "pdf": "https://arxiv.org/pdf/2512.13105", "abs": "https://arxiv.org/abs/2512.13105", "authors": ["Antoine El-Hayek", "Monika Henzinger", "Jason Li"], "title": "Deterministic and Exact Fully-dynamic Minimum Cut of Superpolylogarithmic Size in Subpolynomial Time", "categories": ["cs.DS"], "comment": "To appear at SODA 2026", "summary": "We present an exact fully-dynamic minimum cut algorithm that runs in $n^{o(1)}$ deterministic update time when the minimum cut size is at most $2^{Θ(\\log^{3/4-c}n)}$ for any $c>0$, improving on the previous algorithm of Jin, Sun, and Thorup (SODA 2024) whose minimum cut size limit is $(\\log n)^{o(1)}$. Combined with graph sparsification, we obtain the first $(1+ε)$-approximate fully-dynamic minimum cut algorithm on weighted graphs, for any $ε\\ge2^{-Θ(\\log^{3/4-c}n)}$, in $n^{o(1)}$ randomized update time. Our main technical contribution is a deterministic local minimum cut algorithm, which replaces the randomized LocalKCut procedure from El-Hayek, Henzinger, and Li (SODA 2025).", "AI": {"tldr": "本文提出了一种确定性的动态最小割算法，可在超多项式规模的最小割大小时，在亚多项式时间内运行。", "motivation": "改进前人的算法，使得能够在更广泛的最小割大小范围内实现高效更新时间。", "method": "通过引入一种确定性局部最小割算法来替换之前的随机化LocalKCut过程，从而提高算法效率。", "result": "提出了一个新的精确的完全动态最小割算法，在给定条件下可以达到亚多项式时间复杂度。结合图简化技术，还得到了第一个（1+ε）近似完全动态最小割算法。", "conclusion": "新的确定性局部最小割方法有效提升了动态最小割问题的解决效率，适用于更广泛的最小割大小情况。"}}
{"id": "2512.13104", "pdf": "https://arxiv.org/pdf/2512.13104", "abs": "https://arxiv.org/abs/2512.13104", "authors": ["Yan Zhang", "Baoxin Li", "Han Sun", "Yuhang Gao", "Mingtai Zhang", "Pei Wang"], "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection", "categories": ["cs.CV"], "comment": null, "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.", "AI": {"tldr": "提出了一种新的深度学习模型FID-Net，用于从无人机可见光图像中检测森林害虫感染的树木，并通过空间指标分析感染情况。", "motivation": "为了克服传统方法在大规模精细检测中的限制，提高对受感染树木和害虫分布模式的识别准确性。", "method": "基于YOLOv8n提出FID-Net模型，包括特征增强模块（FEM）、自适应多尺度特性融合模块（AMFM）及有效的通道注意力机制（ECA），以提取疾病敏感线索、对齐和融合双分支特征并提高区分信息。", "result": "实验结果表明FID-Net在32个森林地块的无人机图像上表现出色，精度为86.10%，召回率为75.44%，mAP@0.5达到82.29%，mAP@0.5:0.95为64.30%，优于主流YOLO模型。", "conclusion": "FID-Net实现了对树木健康状况的准确区分，并结合空间指标提供可靠的数据，支持智能害虫监测、早期预警和精确管理。"}}
{"id": "2512.13102", "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.", "AI": {"tldr": "该论文探讨了通过让学生主动提问来提高语言模型在动态交互中的学习效率。", "motivation": "现有大型语言模型擅长静态交互，但在需要主动获取信息的场景中表现不佳。本研究关注如何使学生模型能够识别自身的不确定性并提出有针对性的问题以获得有用的信息。", "method": "通过数学和编程基准测试展示学生主导的方法相对于静态基线的优势，并使用直接偏好优化（DPO）训练策略来提升问题质量，这种方法利用自我或更强的学生提供的指导。", "result": "在各种基准测试中，与静态基线相比，学生主导的提问方法能够实现至少0.5的绝对Pass@k改进。较小模型在接受导向性训练后能更有效地提出高质量的问题，进一步提高了学习效率。", "conclusion": "研究表明通过让学生主动提问并接收指导来提升语言模型的学习能力是可行且有效的策略，并为未来的研究提供了新的视角和方法论支持。"}}
{"id": "2512.13101", "pdf": "https://arxiv.org/pdf/2512.13101", "abs": "https://arxiv.org/abs/2512.13101", "authors": ["Wenjing Lu", "Yi Hong", "Yang Yang"], "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.", "AI": {"tldr": "提出了一种不确定性告知的协作学习框架，用于半监督医学图像分割。", "motivation": "视觉基础模型在医疗图像分割中展示出强大的泛化能力，但在特定临床任务和罕见病变变化方面表现出不足。为了弥合一般性和特殊性的差距，提出了UnCoL框架。", "method": "通过双重教师架构从冻结的基础模型传输通用知识，并通过逐步适应的教师捕捉细粒度和任务特异表示，同时引入预测不确定性来调节伪标签学习。", "result": "实验结果表明，UnCoL在多个2D和3D分割基准上优于当前最佳半监督方法和基础模型基线。其性能接近全监督水平，且显著减少了标注需求。", "conclusion": "所提出的框架有效解决了医学图像分割中的泛化与特异性问题，展示了优越的性能和广泛应用潜力。"}}
{"id": "2512.13100", "pdf": "https://arxiv.org/pdf/2512.13100", "abs": "https://arxiv.org/abs/2512.13100", "authors": ["Guanhua Ji", "Harsha Polavaram", "Lawrence Yunliang Chen", "Sandeep Bajamahal", "Zehan Ma", "Simeon Adebola", "Chenfeng Xu", "Ken Goldberg"], "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.", "AI": {"tldr": "该论文提出了AugE-Toolkit和OXE-AugE，以扩大机器人数据集的规模并提高跨实体策略学习的能力。", "motivation": "由于重新收集演示和再训练对于每个新的硬件平台都过于昂贵，需要利用现有的机器人数据进行扩展和泛化。现有数据集中存在不平衡问题，风险在于过度适应特定的机器人场景组合。", "method": "该论文提出了一种可扩展的机器人增强管道AugE-Toolkit，并生成了OXE-AugE数据集，增加了9个不同的机器人实体，大大扩大了原始数据集的规模和多样性。", "result": "实验结果表明，在物理试验中，针对未见过的机器人抓手组合进行微调的状态最先进的一般化策略，提高了24-45%的成功率。这证明了增加多样性的增强作用。", "conclusion": "研究结论是通过扩大和多样化数据集来改进跨实体学习的有效性已经得到验证。"}}
{"id": "2512.13095", "pdf": "https://arxiv.org/pdf/2512.13095", "abs": "https://arxiv.org/abs/2512.13095", "authors": ["Feng Zhang", "Zezhong Tan", "Xinhong Ma", "Ziqiang Dong", "Xi Leng", "Jianfei Zhao", "Xin Sun", "Yang Yang"], "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.", "AI": {"tldr": "提出了一种基于难度优先的自适应提示方法ADHint，以改善强化学习中的知识扩展和推理泛化能力。", "motivation": "现有基于提示的RL方法忽视了难度对提示比率和相对优势估计的影响，导致不稳定的学习过程和过度模仿策略外样本提示的问题。因此，作者提出了考虑难度因素的方法来平衡探索与模仿之间的关系。", "method": "ADHint通过评估每个样本在策略模型下的难度，并根据难度适当地安排提示比例以指导rollouts。此外，引入了一种基于一致性的梯度调制方法和选择性掩码机制，用于保护提示中的标记级梯度不被破坏更新；同时利用带有和不带提示的rollout相对难度来估计各自的优势。", "result": "实验结果表明，ADHint在推理能力和分布外泛化方面优于现有方法，在pass@1和avg@8指标上表现出色。", "conclusion": "通过考虑样本难度，ADHint实现了更好的探索与模仿平衡，并提高了模型的知识扩展能力。"}}
{"id": "2512.13094", "pdf": "https://arxiv.org/pdf/2512.13094", "abs": "https://arxiv.org/abs/2512.13094", "authors": ["Xiang Li", "Gang Liu", "Weitao Zhou", "Hongyi Zhu", "Zhong Cao"], "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.", "AI": {"tldr": "提出了一种序列专家（Sequence of Experts，SoE）方法，通过时间交替策略提升模仿学习在自动驾驶中的闭环性能。", "motivation": "当前的模仿学习模型在开环环境下表现优秀，但在闭环中因小错误累积而导致性能下降。传统研究主要依赖更复杂的网络架构或高保真数据集来增强鲁棒性，但这种方法忽视了时间尺度的重要性。", "method": "提出一种序列专家（Sequence of Experts，SoE）方法，通过在不同规划周期之间交替使用不同的专家策略，提高闭环中的性能和鲁棒性。该方法不增加模型大小或数据需求。", "result": "实验结果表明，SoE方法显著提升了nuPlan基准测试中所有评估模型的性能，并达到了最先进的水平。", "conclusion": "通过时间尺度增强模仿学习的鲁棒性，提出的方法为提高自动驾驶训练效率提供了关键支持。"}}
{"id": "2512.13093", "pdf": "https://arxiv.org/pdf/2512.13093", "abs": "https://arxiv.org/abs/2512.13093", "authors": ["Mingqi Yuan", "Tao Yu", "Haolin Song", "Bo Li", "Xin Jin", "Hua Chen", "Wenjun Zeng"], "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations", "categories": ["cs.RO", "cs.LG"], "comment": "13 pages, 12 figures", "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.", "AI": {"tldr": "提出了一种名为PvP的对比学习框架，以提高人形机器人在复杂环境中的运动控制效率。", "motivation": "解决强化学习在人形机器人全身体态控制中样本利用率低的问题，利用本体感觉和特权状态之间的内在互补性，提升任务相关特征的学习效率。", "method": "PvP框架通过对比学习，从本体感知和特权状态下学习紧凑且相关的潜在表示，而无需手动数据增强。同时开发了SRL4Humanoid框架以支持系统评估。", "result": "实验显示，在LimX Oli机器人上的速度跟踪及运动模仿任务中，PvP显著提高了样本效率和最终性能，超越了基线状态表征学习方法。", "conclusion": "研究提供了将状态表示学习与强化学习结合以实现人形机器人的全身体态控制的实用指导，为高效的人形机器人学习开辟了新途径。"}}
{"id": "2512.13090", "pdf": "https://arxiv.org/pdf/2512.13090", "abs": "https://arxiv.org/abs/2512.13090", "authors": ["Jebeom Chae", "Junwoo Chang", "Seungho Yeom", "Yujin Kim", "Jongeun Choi"], "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion", "categories": ["cs.RO"], "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.", "AI": {"tldr": "本文提出了一种基于热扩散的机器人路径规划方法，能够根据视觉和语言指令生成多机器人无碰撞轨迹。", "motivation": "当前的扩散模型在多机器人系统中应用有限，并且难以处理通用任务。这些模型还需构建显式的环境表示并缺乏对几何可达性的推理机制，导致计算成本高、泛化能力差。", "method": "提出了一种基于热扩散的语言条件路径规划方法LCHD，利用CLIP语义先验与碰撞避免的扩散核，能够直接在视觉信息基础上生成满足语言指令的机器人运动轨迹，减少对显式障碍物信息的需求。", "result": "实验结果显示，该方法在成功率和规划延迟方面优于现有基于扩散模型的方法。", "conclusion": "LCHD能够在多机器人场景中有效执行复杂任务，并且能够自然地处理不可达情况下的语义意图匹配。"}}
{"id": "2512.13089", "pdf": "https://arxiv.org/pdf/2512.13089", "abs": "https://arxiv.org/abs/2512.13089", "authors": ["Ziqiang Zhu", "Bowei Yang"], "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.", "AI": {"tldr": "提出了一种无监督的开放词汇变化检测方法UniVCD，使用冻结的SAM2和CLIP进行跨场景的变化检测。", "motivation": "现有变化检测方法依赖于监督学习，在多样化的场景中表现不佳且标注成本高。利用视觉基础模型可以缓解这些问题，从而推动开放词汇变化检测的发展。", "method": "UniVCD基于冷冻的SAM2和CLIP构建，引入了一个轻量级特征对齐模块来桥接空间详细表示和语义先验，并通过简化后处理流程提高对象边界清晰度的变化检测精度。", "result": "在多个公共二值变化检测和语义变化检测基准上取得了优秀的性能，在F1和IoU等关键指标上与现有开放词汇变化检测方法相当或更好。", "conclusion": "基于冻结的视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的一种实用且有效的方法。"}}
{"id": "2512.13083", "pdf": "https://arxiv.org/pdf/2512.13083", "abs": "https://arxiv.org/abs/2512.13083", "authors": ["Saumyaranjan Mohanty", "Aravind Reddy", "Konda Reddy Mopuri"], "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to WACV 2026", "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.", "AI": {"tldr": "本文提出了一种新的多样性正则化器DiRe，用于减少合成数据集中的冗余并提高其多样性。", "motivation": "现有数据集浓缩方法生成的数据集中存在显著的冗余，因此需要一种可以减少这种冗余的方法来改善数据集的多样性和泛化能力。", "method": "通过引入基于余弦相似度和欧几里得距离的DiRe正则化器，该正则化器可以直接应用于各种先进的浓缩方法中以提高其性能。", "result": "实验表明，在CIFAR-10到ImageNet-1K等不同基准数据集上，添加DiRe可以显著改善现有先进浓缩方法在多样性和泛化性上的表现。", "conclusion": "通过引入多样性正则化器DiRe，可以在保持或提高模型性能的同时减少合成数据集中冗余的样本数量，从而有效提升数据集的多样性与泛化能力。"}}
{"id": "2512.13080", "pdf": "https://arxiv.org/pdf/2512.13080", "abs": "https://arxiv.org/abs/2512.13080", "authors": ["Yicheng Feng", "Wanpeng Zhang", "Ye Wang", "Hao Luo", "Haoqi Yuan", "Sipeng Zheng", "Zongqing Lu"], "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.", "AI": {"tldr": "论文提出了一种空间感知的VLA预训练方法，通过人类视频中的视觉物理对齐来缩小二维感知和三维行动之间的差距。", "motivation": "现有的大多数方法依赖于2D图像输入来执行3D环境的动作任务，这导致了感知与动作之间的一致性较差。因此，论文旨在通过引入空间意识的预训练来解决这一问题，并增强模型在机器人政策学习中的鲁棒性和泛化能力。", "method": "该研究提出了一个基于双编码器架构的VIPA-VLA方法，其中包括一个3D视觉编码器以增加语义视图表示中与3D相关的特征。利用大规模的人类演示视频提取三维视觉和动作注释来实现2D视觉观察与3D空间推理的一致性。", "result": "在下游机器人任务上应用时，VIPA-VLA方法显著提高了二维视觉和三维行动之间的一致性。", "conclusion": "该论文展示了通过预训练模型中的视觉物理对齐，可以增强模型的三维空间理解能力，从而提高其在机器人政策学习中表现的鲁棒性和通用性。"}}
{"id": "2512.13078", "pdf": "https://arxiv.org/pdf/2512.13078", "abs": "https://arxiv.org/abs/2512.13078", "authors": ["Mohaiminul Islam Bhuiyan", "Chan Hue Wah", "Nur Shazwani Kamarudin", "Nur Hafieza Ismail", "Ahmad Fakhri Ab Nasir"], "title": "Heart Disease Prediction using Case Based Reasoning (CBR)", "categories": ["cs.CV", "cs.CL"], "comment": "Published in Journal of Theoretical and Applied Information Technology on 31st October 2024. Vol.102. No. 20", "summary": "This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.", "AI": {"tldr": "本文使用案例推理技术（CBR）来预测心脏病，通过与模糊逻辑和神经网络方法的比较，展示了CBR在准确度上的优越性。", "motivation": "传统的心脏病诊断依赖于医生的经验，准确性较低。因此，采用智能系统作为一种替代方案以提高疾病的预测精度。", "method": "研究对比了三种智能系统方法：模糊逻辑、神经网络和案例推理（CBR）。经过数据预处理和分割后，选择使用CBR进行心脏病的预测。", "result": "实验结果表明，所提出的基于CBR的方法在预测心脏病方面的准确率为97.95%，且男性患病概率高于女性。", "conclusion": "研究证明了案例推理（CBR）是预测心脏病的有效方法，并强调吸烟和饮酒等生活习惯对心脏病的影响。"}}
{"id": "2512.13074", "pdf": "https://arxiv.org/pdf/2512.13074", "abs": "https://arxiv.org/abs/2512.13074", "authors": ["Huimu Wang", "Yiming Qiu", "Xingzhi Yao", "Zhiguo Chen", "Guoyu Tang", "Songlin Wang", "Sulong Xu", "Mingming Li"], "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models. To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.", "AI": {"tldr": "提出了一种简单的对称一致索引框架SCI，用于大规模密集检索中的表示空间对齐和索引一致性问题", "motivation": "解决双塔编码架构带来的代表空间错位和检索索引不一致性问题，提升匹配精度、检索稳定性和长尾查询性能", "method": "包含两个模块：对称表征对齐模块使用输入交换机制统一双塔表征空间；一致索引与双塔协同模块通过双视图索引策略保持从训练到推理的一致性", "result": "在公开数据集和真实电商数据集中验证了方法的有效性，展示了理论保证及工程友好性", "conclusion": "SCI框架系统性强、轻量且易于部署，在大规模检索中表现优秀"}}
{"id": "2512.13072", "pdf": "https://arxiv.org/pdf/2512.13072", "abs": "https://arxiv.org/abs/2512.13072", "authors": ["Zizhi Chen", "Yizhen Gao", "Minghao Han", "Yizhou Liu", "Zhaoyu Chen", "Dingkang Yang", "Lihua Zhang"], "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.", "AI": {"tldr": "本文提出了一种基于检索增强的持续学习框架，用于多模态医学视觉语言模型。", "motivation": "在多模态生物医学中，视觉-语言模型面临着如何保存细致的内部特征同时跨越不同模态领域差距的核心问题。", "method": "通过建立一个1800万规模的医学检索数据库，并引入基于检索增强生成系统（RAG），动态提供知识提取和引导训练过程。此外，设计了适应性调整参数空间重要性的动态知识蒸馏框架。", "result": "实验结果显示提出的模型在所有指标上达到最先进的性能。", "conclusion": "所提出的方法有效解决了多模态医学视觉语言模型中的持续学习挑战，并展示了显著的临床价值。"}}
{"id": "2512.13070", "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.", "AI": {"tldr": "该论文提出了M-GRPO框架，利用动量模型提供稳定的训练目标，并采用基于四分位距的自适应过滤方法来防止策略过早收敛。", "motivation": "现有自我监督强化学习方法在长时间训练中会出现性能骤降的问题，即“政策崩溃”，通过引入新框架和动态过滤机制解决该问题。", "method": "M-GRPO利用缓慢变化的动量模型作为稳定的参考目标；同时提出基于四分位距的自适应过滤器来删除低熵策略轨迹以保持多样化。", "result": "实验显示，M-GRPO提升了训练稳定性，并且与四分位距滤波结合后达到了最先进的性能水平。", "conclusion": "该方法成功地稳定了自我监督强化学习在大语言模型中的长期训练过程，防止政策崩溃并维持多样性。"}}
{"id": "2512.13063", "pdf": "https://arxiv.org/pdf/2512.13063", "abs": "https://arxiv.org/abs/2512.13063", "authors": ["Cheril Shah", "Akshit Agarwal", "Kanak Garg", "Mourad Heddaya"], "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators", "categories": ["cs.CL", "cs.AI"], "comment": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025", "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.", "AI": {"tldr": "本文通过引入统一的数学框架和两个衡量指标，对比人类谈判者与四种先进语言模型（LLM）在不同情境下的表现。", "motivation": "探讨大型语言模型是否能够像人类一样灵活调整策略，在谈判中适应不同的情况并利用对手的信息进行有效交流。", "method": "引入了一种统一的数学框架来描述让步动态，提出了两个衡量指标用于量化提议轨迹的时间和刚性，并进行了大规模实证对比研究。", "result": "发现LLM倾向于在可能达成协议区间的极端位置锚定且忽视了杠杆作用或情境。此外，随着模型性能的提高，其谈判能力并未显著改善。", "conclusion": "当前大型语言模型在理解和利用对手策略及上下文方面存在局限性，需要开发出能够更好地理解对手思考过程和基于情境策略的模型。"}}
{"id": "2512.13058", "pdf": "https://arxiv.org/pdf/2512.13058", "abs": "https://arxiv.org/abs/2512.13058", "authors": ["Marek Černý", "Tim Seppelt"], "title": "Homomorphism Indistinguishability, Multiplicity Automata Equivalence, and Polynomial Identity Testing", "categories": ["cs.CC", "cs.DM", "cs.DS", "cs.LO"], "comment": "Accepted for STACS 2026", "summary": "Two graphs $G$ and $H$ are homomorphism indistinguishable over a graph class $\\mathcal{F}$ if they admit the same number of homomorphisms from every graph $F \\in \\mathcal{F}$. Many graph isomorphism relaxations such as (quantum) isomorphism and cospectrality can be characterised as homomorphism indistinguishability over specific graph classes. Thereby, the problems $\\textrm{HomInd}(\\mathcal{F})$ of deciding homomorphism indistinguishability over $\\mathcal{F}$ subsume diverse graph isomorphism relaxations whose complexities range from logspace to undecidable. Establishing the first general result on the complexity of $\\textrm{HomInd}(\\mathcal{F})$, Seppelt (MFCS 2024) showed that $\\textrm{HomInd}(\\mathcal{F})$ is in randomised polynomial time for every graph class $\\mathcal{F}$ of bounded treewidth that can be defined in counting monadic second-order logic $\\mathsf{CMSO}_2$. We show that this algorithm is conditionally optimal, i.e. it cannot be derandomised unless polynomial identity testing is in $\\mathsf{PTIME}$. For $\\mathsf{CMSO}_2$-definable graph classes $\\mathcal{F}$ of bounded pathwidth, we improve the previous complexity upper bound for $\\textrm{HomInd}(\\mathcal{F})$ from $\\mathsf{PTIME}$ to $\\mathsf{C}_=\\mathsf{L}$ and show that this is tight. Secondarily, we establish a connection between homomorphism indistinguishability and multiplicity automata equivalence which allows us to pinpoint the complexity of the latter problem as $\\mathsf{C}_=\\mathsf{L}$-complete.", "AI": {"tldr": "本文研究了同态不可区分性的复杂性问题，并通过与多重自动机等价性建立联系，确定了该问题的复杂度。", "motivation": "理解图同构的各种松弛形式的计算复杂性是重要的。特别是对于那些可以用数蒙代克二阶逻辑定义且有界树宽或路径宽度的图类，研究其同态不可区分性的复杂性具有重要意义。", "method": "通过分析随机算法在解决特定问题中的局限性和优化，并引入多重自动机等价性的概念来建立理论联系以确定复杂度界限。", "result": "对于可以用数蒙代克二阶逻辑定义且有界树宽或路径宽度的图类，本文证明了同态不可区分性问题是条件最优随机算法无法被确定化除非多项式同一性测试在P中。同时，也指出了多重自动机等价性的复杂度为C_=L完全问题。", "conclusion": "通过建立同态不可区分性和多重自动机等价之间的联系，本文不仅提供了对于特定图类的同态不可区分性问题的新见解，还确定了该问题的最佳复杂性界限。"}}
{"id": "2512.13055", "pdf": "https://arxiv.org/pdf/2512.13055", "abs": "https://arxiv.org/abs/2512.13055", "authors": ["Jaeyoon Kim", "Yoonki Cho", "Sung-Eui Yoon"], "title": "Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing", "categories": ["cs.CV"], "comment": "AAAI 2026", "summary": "Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR", "AI": {"tldr": "本文提出了一种高效的异构视觉位置识别框架，通过离线特征提取和在线轻量级查询网络处理来降低计算成本。", "motivation": "现有高容量的视觉位置识别模型虽然性能出色但计算成本高昂，难以在资源受限设备上部署。", "method": "本文提出了一种高效的异构VPR框架，结合了离线高容量画廊模型和在线轻量级查询网络。为了克服传统方法中通过昂贵的k-NN训练实现兼容性的挑战，引入了一个地理记忆银行来结构化画廊特征，并使用隐式嵌入增强技术提升查询网络对特征变化的建模能力。", "result": "实验结果表明该方法不仅显著降低了计算成本，还在资源有限环境中优于现有异构检索方法。", "conclusion": "本文的方法为视觉位置识别在资源受限环境中的应用开辟了新的方向。"}}
{"id": "2512.13043", "pdf": "https://arxiv.org/pdf/2512.13043", "abs": "https://arxiv.org/abs/2512.13043", "authors": ["Tong Wei", "Yijun Yang", "Changhao Zhang", "Junliang Xing", "Yuanchun Shi", "Zongqing Lu", "Deheng Ye"], "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "AI": {"tldr": "GTR-Turbo 提出了一种高效升级方法，用于基于视觉语言模型的多模态代理的多轮强化学习。", "motivation": "当前的多轮强化学习方法依赖于昂贵且特权化的教师模型，限制了实用性和可重复性。因此，研究者提出了无需训练或查询昂贵教师模型的新方法。", "method": "GTR-Turbo 方法通过合并正在进行中的强化学习过程中生成的检查点权重，并使用该合并模型作为免费的监督微调或软概率分布蒸馏的教师来指导后续的学习过程。", "result": "在各种视觉代理任务中，GTR-Turbo 在提高基准模型准确性10-30%的同时减少了50%的训练时间和60%的计算成本。", "conclusion": "通过去除对特权化VLM（如 GPT或Gemini）的依赖并保持训练稳定性，GTR-Turbo 实现了高效且稳定的多模态代理训练。"}}
{"id": "2512.13039", "pdf": "https://arxiv.org/pdf/2512.13039", "abs": "https://arxiv.org/abs/2512.13039", "authors": ["Hao Chen", "Yiwei Wang", "Songze Li"], "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models", "categories": ["cs.CV", "cs.CR"], "comment": "Under Review", "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.", "AI": {"tldr": "提出了一种双向概念擦除框架Bi-Erasing，用于改善扩散模型中的图像生成效果。", "motivation": "现有方法采用单一方向的概念擦除策略难以平衡移除目标概念和保持生成质量的双重需求。为了克服这一限制，提出了Bi-Erasing框架。", "method": "基于文本提示及其对应图像的联合表示，Bi-Erasing引入了两个独立的图像分支：一个负向分支用于抑制有害语义，另一个正向分支提供安全替代物的视觉引导。通过同时优化这些互补方向并应用掩码过滤防止擦除过程中的无关干扰，实现了概念移除效果和生成质量之间的平衡。", "result": "在广泛的实验评估中，Bi-Erasing方法优于基准方法，在概念移除有效性和视觉保真度之间取得了更好的平衡。", "conclusion": "通过引入双向图像引导的概念擦除框架Bi-Erasing，成功地改善了扩散模型中的图像生成质量并提高了安全性。"}}
{"id": "2512.13033", "pdf": "https://arxiv.org/pdf/2512.13033", "abs": "https://arxiv.org/abs/2512.13033", "authors": ["Jongwook Kim", "Sangheon Yun", "Sukjin Yoon"], "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes", "AI": {"tldr": "本文提出了一个优化框架，通过不对称投影将后向传播梯度分解为平行跨度和平行违反项，并在标准的前向$QKV$结构下进行调整。", "motivation": "传统Transformer模型训练存在几何效率问题。提出的方法旨在优化注意力机制中的梯度计算，使其更高效。", "method": "通过不对称投影将后向传播梯度分解为平行跨度和平行违反项，并在标准的前向$QKV$结构下进行调整。通过实验验证了这种方法的有效性。", "result": "在WikiText-2数据集上实现了0.56%的验证损失减少，显示该框架的基本有效性及潜在优势。", "conclusion": "此优化框架具有显著潜力，在大规模数据和深度训练中可能产生更多收益。"}}
{"id": "2512.13031", "pdf": "https://arxiv.org/pdf/2512.13031", "abs": "https://arxiv.org/abs/2512.13031", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs", "categories": ["cs.CV"], "comment": "10 pages, 5 figures. A comprehensive comparison of rule-based, machine learning, and deep learning approaches for human estimation using FMCW MIMO radar, focusing on accuracy, spatial generalization, and output granularity", "summary": "This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.", "AI": {"tldr": "本文比较了基于规则、传统机器学习和深度学习方法在无线电波感应中的性能。", "motivation": "为了全面了解不同算法在无线传感应用中的准确性和鲁棒性，特别是在环境变化下的表现。", "method": "使用频率调制连续波多输入多输出雷达系统，在两个室内环境中评估了五种模型：基于规则的连通分量方法、三种传统机器学习模型（k-近邻、随机森林和SVM）以及结合卷积神经网络和长短时记忆的深度学习模型。", "result": "在训练环境下，深度学习模型表现最佳；但在新布局下，所有学习型模型性能显著下降。相比之下，基于规则的方法表现出较强的环境适应性。", "conclusion": "高容量模型能在同一环境中产生精细输出并保持高准确性，但对域变化敏感。基于规则的模型尽管不能提供细致输出，但展现出更强的鲁棒性和跨区域推广能力。"}}
{"id": "2512.13030", "pdf": "https://arxiv.org/pdf/2512.13030", "abs": "https://arxiv.org/abs/2512.13030", "authors": ["Hongzhe Bi", "Hengkai Tan", "Shenghao Xie", "Zeyuan Wang", "Shuhe Huang", "Haitian Liu", "Ruowen Zhao", "Yao Feng", "Chendong Xiang", "Yinze Rong", "Hongyan Zhao", "Hanyu Liu", "Zhizhong Su", "Lei Ma", "Hang Su", "Jun Zhu"], "title": "Motus: A Unified Latent Action World Model", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.", "AI": {"tldr": "Motus是一个统一的潜在动作世界模型，旨在整合理解、视频生成和控制功能。", "motivation": "当前的方法在理解和世界建模方面是孤立的，这阻碍了多模态生成能力和大规模异构数据的学习。因此需要一个能够统一这些能力的新方法。", "method": "Motus通过引入Mixture-of-Transformer架构整合三个专家，并采用UniDiffuser风格的时间表来实现不同模型模式间的灵活转换。它还利用光流学习潜在动作，采用三阶段训练管道和六层数据金字塔以提取像素级别的“delta行动”。", "result": "实验表明Motus在模拟和现实世界场景中均表现出色，优于现有最佳方法。", "conclusion": "统一模型的功能性和先验知识显著提升了下游机器人任务的表现。"}}
{"id": "2512.13019", "pdf": "https://arxiv.org/pdf/2512.13019", "abs": "https://arxiv.org/abs/2512.13019", "authors": ["Cheeun Hong", "German Barquero", "Fadime Sener", "Markos Georgopoulos", "Edgar Schönfeld", "Stefan Popov", "Yuming Du", "Oscar Mañas", "Albert Pumarola"], "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.", "AI": {"tldr": "本文介绍了一种名为SneakPeek的未来导向式教学视频生成流水线，该系统通过扩散模型和自回归框架从文本描述中合成连贯的教学演示。", "motivation": "现有视频扩散模型难以在长序列中的多个动作步骤之间保持时间一致性与可控性。因此，本文提出了一种新的方法以提高视频生成的精确度、一致性和交互性。", "method": "该方法引入了三个关键创新：预测因果适应、未来导向式自我强迫和多提示条件控制。这些机制共同作用于减少时序漂移、保持运动一致性，并实现基于未来的指令更新对正在进行中的教学视频产生的动态影响。", "result": "实验结果显示，该方法可以生成时间上连贯且语义忠实的教学视频，准确地遵循复杂的多步骤任务描述。", "conclusion": "SneakPeek提供了一种新颖的方法来解决教学视频合成中遇到的时间一致性与可控性挑战。"}}
{"id": "2512.13018", "pdf": "https://arxiv.org/pdf/2512.13018", "abs": "https://arxiv.org/abs/2512.13018", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing", "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.", "AI": {"tldr": "该研究首次全面评估了空间泛化技术，这些技术对于基于深度学习的射频感知的实际部署至关重要。", "motivation": "针对室内环境中的毫米波雷达感知应用，为了提高其在不同布局下的性能稳定性，提出了对多种方法进行系统性调查与对比的需求。", "method": "该研究通过使用FMCW MIMO雷达的人数计数实验，考察了包括基于振幅的统计预处理（如S型加权和阈值清零）、频域滤波、自编码器背景抑制、数据增强策略以及迁移学习等多种技术。", "result": "实验证明，sigmoid基线振幅加权在跨环境性能上表现出色，RMSE和MAE分别降低了50.1%和55.2%，而数据增强则进一步提供了适度的改善（最多8.8%），对于大的空间变化迁移学习至关重要。", "conclusion": "研究结果为开发能够在空间差异下保持稳定准确性的雷达感知系统指明了方向，即通过将深度学习模型与基于振幅的预处理和有效的迁移学习相结合。"}}
{"id": "2512.13015", "pdf": "https://arxiv.org/pdf/2512.13015", "abs": "https://arxiv.org/abs/2512.13015", "authors": ["Xinjie Li", "Zhimin Chen", "Rui Zhao", "Florian Schiffers", "Zhenyu Liao", "Vimal Bhat"], "title": "What Happens Next? Next Scene Prediction with a Unified Video Model", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.", "AI": {"tldr": "提出了一种新的任务Next Scene Prediction (NSP)，以推动统一视频模型在时间推理方面的能力。", "motivation": "现有的联合理解和生成的统一大模型主要集中在文本到视频等传统任务上，而其潜在的时间推理能力尚未得到充分探索。为了解决这个问题，提出了NSP任务和对应的解决方案。", "method": "提出了一种结合Qwen-VL用于理解部分和LTX用于生成部分，并通过隐式查询嵌入和连接模块相连的统一框架。该模型在新编的大规模NSP数据集上进行训练：包括文本到视频预训练、有监督微调以及通过GRPO进行的强化学习。", "result": "实验结果表明，所提出的模型在基准测试中实现了最先进的性能。", "conclusion": "研究表明，统一框架可以提升多模态系统预测未来场景的能力。"}}
{"id": "2512.13014", "pdf": "https://arxiv.org/pdf/2512.13014", "abs": "https://arxiv.org/abs/2512.13014", "authors": ["Haoyu Wang", "Lei Zhang", "Wenrui Liu", "Dengyang Jiang", "Wei Wei", "Chen Ding"], "title": "JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion", "categories": ["cs.CV"], "comment": "Accepted at AAAI 2026", "summary": "Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.", "AI": {"tldr": "本文提出了一种联合生成图像和像素级注释的框架JoDiffusion，旨在改善语义分割模型训练。", "motivation": "当前方法在生成合成数据时存在标注不一致或可扩展性问题。JoDiffusion意在解决这些问题。", "method": "通过引入独立的注释变分自编码器将标注映射到共享的潜在空间，并定制扩散模型以捕捉图像及其注释之间的联合分布，从而实现基于文本提示生成图像和标注的一致性。", "result": "实验表明JoDiffusion在Pascal VOC、COCO和ADE20K数据集上的语义分割表现优于现有方法。", "conclusion": "JoDiffusion通过同时生成图像与一致的像素级注释，展示了强大的可扩展性和改善的性能。"}}
{"id": "2512.13012", "pdf": "https://arxiv.org/pdf/2512.13012", "abs": "https://arxiv.org/abs/2512.13012", "authors": ["Menglu Li", "Majd Alber", "Ramtin Asgarianamiri", "Lian Zhao", "Xiao-Ping Zhang"], "title": "HQ-MPSD: A Multilingual Artifact-Controlled Benchmark for Partial Deepfake Speech Detection", "categories": ["cs.SD"], "comment": "6 pages, 4 figures, 2 tables", "summary": "Detecting partial deepfake speech is challenging because manipulations occur only in short regions while the surrounding audio remains authentic. However, existing detection methods are fundamentally limited by the quality of available datasets, many of which rely on outdated synthesis systems and generation procedures that introduce dataset-specific artifacts rather than realistic manipulation cues. To address this gap, we introduce HQ-MPSD, a high-quality multilingual partial deepfake speech dataset. HQ-MPSD is constructed using linguistically coherent splice points derived from fine-grained forced alignment, preserving prosodic and semantic continuity and minimizing audible and visual boundary artifacts. The dataset contains 350.8 hours of speech across eight languages and 550 speakers, with background effects added to better reflect real-world acoustic conditions. MOS evaluations and spectrogram analysis confirm the high perceptual naturalness of the samples. We benchmark state-of-the-art detection models through cross-language and cross-dataset evaluations, and all models experience performance drops exceeding 80% on HQ-MPSD. These results demonstrate that HQ-MPSD exposes significant generalization challenges once low-level artifacts are removed and multilingual and acoustic diversity are introduced, providing a more realistic and demanding benchmark for partial deepfake detection. The dataset can be found at: https://zenodo.org/records/17929533.", "AI": {"tldr": "本文提出了HQ-MPSD，一个用于部分深度伪造语音检测的高质量多语言数据集。", "motivation": "现有数据集的质量限制了深度伪造语音检测方法的发展，因为它们依赖于过时的合成系统和生成程序，导致引入特定的数据集特有伪影而不是真实的操纵线索。", "method": "HQ-MPSD通过使用精细对齐获得的语言连贯拼接点构造，确保了韵律和语义的一致性，并最小化听觉和视觉边界痕迹。数据集包含八种语言的350.8小时语音，涉及550名发言人，并添加背景效果以更好地反映现实世界的声音条件。", "result": "通过跨语言和跨数据集评估最先进的检测模型，所有模型在HQ-MPSD上的性能下降超过80%。这些结果表明，一旦低级伪影被移除并且引入了多语言和声学多样性，HQ-MPSD揭示了深度伪造语音检测的重要泛化挑战。", "conclusion": "HQ-MPSD提供了一个更现实且更具挑战性的基准测试平台，用于部分深度伪造语音的检测。"}}
{"id": "2512.13009", "pdf": "https://arxiv.org/pdf/2512.13009", "abs": "https://arxiv.org/abs/2512.13009", "authors": ["Oğuzhan Akbıyık", "Naseem Alhousani", "Fares J. Abu-Dakka"], "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots", "categories": ["cs.RO"], "comment": null, "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.", "AI": {"tldr": "提出了一种用于协作机器人无传感器力估计的新型算法K-VARK。", "motivation": "准确的无传感器力估计对于确保机器人在非结构化环境中安全和精确地交互至关重要，但受到建模误差和复杂残差动态与摩擦的影响而面临挑战。", "method": "将核化、概率性的关节残差扭矩模型集成到自适应卡尔曼滤波框架中。通过基于优化激励轨迹的核化运动原语捕获预测平均值以及输入相关的异方差变异，这些统计信息通过增强测量噪声协方差来影响变异性感知虚拟测量更新。", "result": "实验验证显示，K-VARK相比现有无传感器力估计方法，均方根误差降低超过20%，具有鲁棒且准确的外部力量/扭矩估计性能。", "conclusion": "提出的方法在6自由度协作机械臂上实现了显著改进，证明了其用于先进任务如抛光和装配的有效性。"}}
{"id": "2512.13008", "pdf": "https://arxiv.org/pdf/2512.13008", "abs": "https://arxiv.org/abs/2512.13008", "authors": ["Xi Luo", "Shixin Xu", "Ying Xie", "JianZhong Hu", "Yuwei He", "Yuhui Deng", "Huaxiong Huang"], "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading", "categories": ["cs.CV"], "comment": null, "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.", "AI": {"tldr": "提出了一种两阶段框架TWLR，用于可解释的糖尿病视网膜病变（DR）评估。", "motivation": "高质量的专业注释对于医学图像分析的有效性至关重要，但获取像素级标签的成本高且耗时。深度学习在医学成像中的成功也受限于缺乏解释性。为解决这些挑战提出了TWLR。", "method": "第一阶段利用视觉语言模型将特定领域的视网膜知识整合到文本嵌入中以进行DR分级和病变分类，第二阶段引入基于弱监督语义分割的迭代严重度回归框架。", "result": "实验结果表明，TWLR在DR分类和病变分割方面取得了竞争力的表现，并提供了一种更可解释且注释效率更高的自动化视网膜图像分析解决方案。", "conclusion": "TWLR通过结合视觉语言模型和弱监督学习解决了医学图像标注难题，同时提供了对疾病严重程度变化的可解释性可视化。"}}
{"id": "2512.13007", "pdf": "https://arxiv.org/pdf/2512.13007", "abs": "https://arxiv.org/abs/2512.13007", "authors": ["Nikolai Goncharov", "James L. Gray", "Donald G. Dansereau"], "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects", "categories": ["cs.CV"], "comment": null, "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.", "AI": {"tldr": "论文提出了一个基于光场图像的对象6DoF追踪方法，该方法无需预训练模型，并且能够应对复杂的视觉行为。", "motivation": "当前高性能追踪方法依赖于预捕获的对象视角来建立明确的参考模型，这限制了它们只能处理固定的一组已知对象。对于复杂外观的对象，现有方法的表现较差。", "method": "论文利用光场图像提取语义和几何特征，并将其转换为视图相关的高斯点样（splat），作为统一的对象表示形式，支持可微渲染和姿态优化。", "result": "实验结果表明，该方法在处理具有挑战性的反射物体时与现有的模型追踪器性能相当。", "conclusion": "论文提出的方法对于通用对象追踪的实现提供了新的可能，并且可以应用于机器人系统中。"}}
{"id": "2512.13006", "pdf": "https://arxiv.org/pdf/2512.13006", "abs": "https://arxiv.org/abs/2512.13006", "authors": ["Yifan Pu", "Yizeng Han", "Zhiwei Tang", "Jiasheng Tang", "Fan Wang", "Bohan Zhuang", "Gao Huang"], "title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.", "AI": {"tldr": "本文研究了如何将扩散蒸馏技术应用于开放式的文本到图像生成任务，提出了针对FLUX.1-lite模型的系统化蒸馏方法，并提供了实践指导和开源代码。", "motivation": "扩散蒸馏在条件类别的图像合成中表现出色，但其对开放式文本到图像生成的应用尚不明确。本文旨在探究并解决从离散类别标签过渡到自由形式语言提示时出现的关键障碍。", "method": "作者将现有的方法统一在一个框架下进行比较和分析，并提出了一套适用于FLUX.1-lite的蒸馏方案，包括输入缩放、网络架构以及超参数的选择。同时提供了开源代码和预训练的学生模型。", "result": "该研究发现并解决了开放文本到图像生成中的关键障碍，确立了快速、高保真且资源高效的扩散生成器在实际应用中的可行性基础。", "conclusion": "本文的研究成果为实现快速高效的文字转图像生成提供了坚实的基础，并通过开源代码和预训练模型推动了技术的应用。"}}
{"id": "2512.13000", "pdf": "https://arxiv.org/pdf/2512.13000", "abs": "https://arxiv.org/abs/2512.13000", "authors": ["Runhua Zhang", "Ruyuan Wan", "Jiaqi", "Li", "Daye Kang", "Yigang Qin", "Yijia Wang", "Ziqi Pan", "Tiffany Knearem", "Huamin Qu", "Xiaojuan Ma"], "title": "Legitimizing, Developing, and Sustaining Feminist HCI in East Asia: Challenges and Opportunities", "categories": ["cs.HC"], "comment": "The proposal was accepted by CHI2026 meet-up track; and will be published in Extended Abstracts of the 2026 CHI Conference on Human Factors in Computing Systems (CHI EA '26)", "summary": "Feminist HCI has been rapidly developing in East Asian contexts in recent years. The region's unique cultural and political backgrounds have contributed valuable, situated knowledge, revealing topics such as localized digital feminism practices, or women's complex navigation among social expectations. However, the very factors that ground these perspectives also create significant survival challenges for researchers in East Asia. These include a scarcity of dedicated funding, the stigma of being perceived as less valuable than productivity-oriented technologies, and the lack of senior researchers and established, resilient communities. Grounded in these challenges and our prior collective practices, we propose this meet-up with two focused goals: (1) to provide a legitimized channel for Feminist HCI researchers to connect and build community, and (2) to facilitate an action-oriented dialogue on how to legitimize, develop, and sustain Feminist HCI in the East Asian context. The website for this meet-up is: https://feminist-hci.github.io/", "AI": {"tldr": "本文探讨了在东亚背景下女性主义人机交互研究的发展挑战与机遇，并提出行动导向的对话方案。", "motivation": "东亚地区的独特文化和政治背景为女性主义HCI提供了宝贵的视角，但同时也面临诸多生存难题，如资金匮乏、社会偏见和缺乏资深研究人员等。", "method": "基于这些挑战及先前的合作经验，作者们组织了一次会议，旨在建立一个合法化的沟通渠道，并促进关于如何在东亚背景下推动女性主义HCI的行动导向对话。", "result": "通过此次会议，参与者得以连接并构建社区，同时探讨了多种策略以实现女性主义HCI的研究发展与持续。", "conclusion": "本文强调了创建支持性环境的重要性以及跨地区合作对于增强女性主义HCI研究的重要性。"}}
{"id": "2512.12997", "pdf": "https://arxiv.org/pdf/2512.12997", "abs": "https://arxiv.org/abs/2512.12997", "authors": ["Wenjing lu", "Zerui Tao", "Dongping Zhang", "Yuning Qiu", "Yang Yang", "Qibin Zhao"], "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.", "AI": {"tldr": "本文提出了一种新的对抗微调目标，旨在为CLIP模型在零样本分类任务中同时保持预测准确性和校准不确定性。", "motivation": "现有工作的对抗微调方法主要关注清洁示例和对抗示例之间预测对数的匹配，并未考虑不确定性的校准，导致零样本泛化能力下降。本文揭示了在对抗环境中模型倾向于过度自信的问题，旨在填补可靠度与鲁棒性之间的空白。", "method": "通过将CLIP输出重新参数化为Dirichlet分布的集中参数，提出了一个统一表示来捕捉相对语义结构和预测信心幅度，并提出了一种新的对抗微调目标以在扰动下校准不确定性和准确性的对齐。", "result": "实验结果表明，在多个零样本分类基准测试上，该方法能有效地恢复校准的不确定性并同时保持清洁准确性及对抗鲁棒性。", "conclusion": "本文提出的模型通过引入新的对抗微调目标解决了CLIP在零样本对抗环境下的不确定性校准问题，并提高了整体可靠性和泛化性能。"}}
{"id": "2512.12993", "pdf": "https://arxiv.org/pdf/2512.12993", "abs": "https://arxiv.org/abs/2512.12993", "authors": ["Guillermo A. Castillo", "Himanshu Lodha", "Ayonga Hereid"], "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations", "categories": ["cs.RO"], "comment": null, "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.", "AI": {"tldr": "本文提出了一种基于层次感知地形的双足行走方法，通过减少维度感知表示来增强强化学习策略，并进行实时步态生成。", "motivation": "现有的端到端方法存在效率和稳健性问题。作者希望通过引入层次化感知表示来提高机器人在不同地形条件下的适应性和决策效率。", "method": "利用卷积变分自动编码器（CNN-VAE）产生低维地形描述，结合减少阶次的机器人动力学模型，优化行走策略。同时，通过历史感知机制将最近的地形观测序列加入到潜在表示中以提高稳健性，并引入从深度相机图像直接学习潜在表征的方法。", "result": "实验结果表明该方法在仿真和实际硬件验证中的鲁棒性和适应性强。", "conclusion": "本文提出的方法提高了双足机器人对复杂地形的适应能力，优化了行走决策过程。"}}
{"id": "2512.12987", "pdf": "https://arxiv.org/pdf/2512.12987", "abs": "https://arxiv.org/abs/2512.12987", "authors": ["Amin Jalal Aghdasian", "Farzaneh Abdollahi", "Ali Kamali Iglie"], "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.", "AI": {"tldr": "提出两种新的算法用于自动驾驶车辆在雪地条件下的车道保持系统。", "motivation": "解决自动驾驶车辆在雪天条件下行驶时遇到的挑战，确保安全可靠的车道保持能力。", "method": "使用深度强化学习（DRL）技术开发了两个方法：Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) 和 Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG)，这些算法用于处理不确定性和滑移问题，并通过CARLA仿真器进行训练。", "result": "实验证明，两种方法在模拟和真实环境中均表现出可行性与稳定性；其中 AR-CADPG 方法展示了更好的路径追踪精度和鲁棒性。", "conclusion": "结合时间记忆、对抗弹性及注意机制的方法，在自动驾驶车辆中展现了有效性。"}}
{"id": "2512.12984", "pdf": "https://arxiv.org/pdf/2512.12984", "abs": "https://arxiv.org/abs/2512.12984", "authors": ["Jiayin Lu", "Ying Jiang", "Yin Yang", "Chenfanfu Jiang"], "title": "VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs", "categories": ["cs.CG", "cs.CV", "cs.GR", "cs.LG", "math.OC"], "comment": null, "summary": "We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/", "AI": {"tldr": "提出VoroLight框架，用于从各种输入生成高质量的三维形状重建。", "motivation": "旨在提供一种通用的方法来直接从不同类型的输入（如图像、点云等）生成光滑且拓扑一致的体积网格。", "method": "通过三个阶段操作：初始化表面使用可微分Voronoi公式，改进表面质量并通过多边形面球训练阶段进行优化，最后重新利用可微分Voronoi公式结合额外内部生成点进行体积优化。", "result": "能够从各种输入中生成高质量的三维网格，并保持拓扑一致性。", "conclusion": "VoroLight为不同类型的3D形状重建提供了一种有效的解决方案。"}}
{"id": "2512.12982", "pdf": "https://arxiv.org/pdf/2512.12982", "abs": "https://arxiv.org/abs/2512.12982", "authors": ["Ziheng Qin", "Yuheng Ji", "Renshuai Tao", "Yuxuan Tian", "Yuyang Liu", "Yipu Wang", "Xiaolong Zheng"], "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes", "categories": ["cs.CV"], "comment": null, "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL", "AI": {"tldr": "该论文提出了一种用于检测AI生成图像的新型框架Generator-Aware Prototype Learning (GAPL)，以解决随着数据源多样性增加而出现的检测性能瓶颈问题。", "motivation": "现有AI生成图像（AIGI）探测器面临的一个悖论是，尽管越来越多的数据来源可以提高泛化能力，但实际表现却会停滞甚至退步。这是由于数据层级上的异质性和固定预训练编码器带来的模型级障碍导致的。", "method": "论文提出了Generator-Aware Prototype Learning (GAPL)，这是一种基于结构化学习范式的框架，它通过学习一组紧凑的伪造原型来创建统一且低方差的功能空间，以此对抗数据异质性。此外，采用两阶段训练方案和Low-Rank Adaptation技术增强其判别能力同时保留有价值的预训练知识。", "result": "实验结果表明GAPL在各种GAN和扩散模型生成器上实现了最先进的检测准确率。", "conclusion": "通过引入Generator-Aware Prototype Learning (GAPL)，研究者们成功地克服了AI生成图像检测中的数据异质性和模型障碍，为更稳健、更具泛化能力的决策边界奠定了基础。"}}
{"id": "2512.12977", "pdf": "https://arxiv.org/pdf/2512.12977", "abs": "https://arxiv.org/abs/2512.12977", "authors": ["Shengling Qin", "Hao Yu", "Chenxin Wu", "Zheng Li", "Yizhong Cao", "Zhengyang Zhuge", "Yuxin Zhou", "Wentao Yao", "Yi Zhang", "Zhengheng Wang", "Shuai Bai", "Jianwei Zhang", "Junyang Lin"], "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.", "AI": {"tldr": "VLCache通过利用先前多模态输入的KV缓存和编码器缓存来减少重复计算，从而加速视觉语言推理。", "motivation": "为了提高视觉语言模型在实际部署中的推断速度，同时保持准确性，提出了VLCache框架以消除相同多模态输入时的成本高昂的重复计算问题。", "method": "该方法通过正式识别累积复用误差效应，并提出了一种动态、分层感知的重新计算策略来平衡精度和效率。此外，它还利用了先前多模态输入的KV缓存和编码器缓存。", "result": "实验结果表明，VLCache达到了与完全重新计算相当的准确性，仅需要2-5%的令牌进行计算，从而获得了1.2x到16x的时间加速。该框架已整合入SGLang系统中。", "conclusion": "通过有效的缓存复用策略和动态分层感知重新计算方法，VLCache能够在保持高精度的同时极大地提高视觉语言推理的速度。"}}
{"id": "2512.12970", "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.", "AI": {"tldr": "本文提出了数字取证中的人读取标准和开放标准以应对人工智能的复杂性和错误。", "motivation": "为了降低数字取证中的错误率，提出采用易于人类理解的标准来管理复杂的技术体系结构。", "method": "概述了一种基于现有技术的最佳实践方案的数字取证AI模型框架。", "result": "创建了一个旨在减少错误并提高透明度和可复现性的数字取证AI模型框架。", "conclusion": "通过采用开放标准和易于理解的人读取格式，可以显著改善数字取证过程中的准确性与可靠性。"}}
{"id": "2512.12963", "pdf": "https://arxiv.org/pdf/2512.12963", "abs": "https://arxiv.org/abs/2512.12963", "authors": ["Luan Thanh Trinh", "Kenji Doi", "Atsuki Osanai"], "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026", "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.", "AI": {"tldr": "介绍SCAdapter技术，通过CLIP图像空间分离和整合内容与风格特征，解决扩散模型在样式转移中的不足。", "motivation": "当前的扩散模型虽是样式转换的主要方法，但在处理照片级真实感时存在问题，通常会产生绘画效果或丢失细节。现有方法未能有效排除原始内容风格的影响及样本文本特征干扰。", "method": "SCAdapter技术通过三个组件：Controllable Style Adaptive Instance Normalization（CSAdaIN）实现精确的多样式融合、KVS Injection实现针对性地整合风格以及一致性目标维护过程连贯性，利用CLIP图像空间分离和整合内容与风格特征。", "result": "实验结果表明SCAdapter在传统方法和基于扩散的方法中均显著优于现有最佳方法，同时消除了DDIM反转和推理阶段优化，使其至少比其他基于扩散的方法快2倍。", "conclusion": "通过分离并整合内容及样式特征，SCAdapter提供了一种更有效且实用的风格转换解决方案。"}}
{"id": "2512.12952", "pdf": "https://arxiv.org/pdf/2512.12952", "abs": "https://arxiv.org/abs/2512.12952", "authors": ["Krishna Srikar Durbha", "Hassene Tmar", "Ping-Hao Wu", "Ioannis Katsavounidis", "Alan C. Bovik"], "title": "Leveraging Compression to Construct Transferable Bitrate Ladders", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review in IEEE Transactions on Image Processing", "summary": "Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.", "AI": {"tldr": "本文提出了一种基于机器学习的比特率梯度构建技术，通过分析压缩过程和源视频进行感知相关的测量来准确预测VMAF得分。", "motivation": "现有按标题或按镜头的视频编码技术相比传统方法（如恒定CRF编码和固定比特率梯）提供了显著的优势。然而，为每个视频构造凸包带来了巨大的计算开销，因此研究采用机器学习的方法代替。", "method": "本文提出的技术通过分析压缩过程并进行源视频感知相关的测量来预测VMAF得分，并利用这些数据训练机器学习模型以构建适应内容的比特率梯度。评估了该框架在大量视频上的性能表现。", "result": "实验结果表明，所提出的框架相对于现有的领先方法具有更好的性能，在不同编码设置下也能保持较好的效果。", "conclusion": "本文提出的方法通过分析压缩过程和感知相关的测量来准确预测VMAF得分，并构建了更高效的比特率梯度。该方法在大规模视频测试中表现优于现有技术，为实际应用提供了新思路。"}}
{"id": "2512.12950", "pdf": "https://arxiv.org/pdf/2512.12950", "abs": "https://arxiv.org/abs/2512.12950", "authors": ["Lingyi Meng", "Maolin Liu", "Hao Wang", "Yilan Cheng", "Qi Yang", "Idlkaid Mohanmmed"], "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping", "categories": ["cs.CL", "cs.AI"], "comment": "43 pages, 6 fingures, accepted in Artificial Intelligence and Law (2025)", "summary": "Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.", "AI": {"tldr": "构建一个多代理框架，用于多语言法律术语映射。", "motivation": "准确地跨语言映射法律术语是一个重大挑战，尤其是对于汉语和日语这样的语言对，它们拥有大量同形异义词。现有资源和标准化工具有限，因此提出了一种人机协作的方法来构建一个多语言法律术语数据库。", "method": "基于多代理框架的人机合作方法，结合了先进的大规模语言模型和法律领域专家的全程参与。此方法强调人类专家在多代理系统中的作用，AI处理特定重复任务，如OCR、文本分割、语义对齐和初步术语提取，而人类专家则提供监督审查。", "result": "实验结果表明，这种人机协作的工作流程不仅能提高多语言法律术语映射的准确性和一致性，还比传统手动方法更具可扩展性。", "conclusion": "该框架在多语言法律术语映射中表现出色，并且由于其灵活性和效率，显示出相较于传统方法更大的优势。"}}
{"id": "2512.12945", "pdf": "https://arxiv.org/pdf/2512.12945", "abs": "https://arxiv.org/abs/2512.12945", "authors": ["Anja Sheppard", "Parker Ewen", "Joey Wilson", "Advaith V. Sethuraman", "Benard Adewole", "Anran Li", "Yuzhen Chen", "Ram Vasudevan", "Katherine A. Skinner"], "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted into R-AL", "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.", "AI": {"tldr": "介绍了一种新的轻量级语义映射系统SLIM-VDB，结合了概率语义融合和OpenVDB数据结构。", "motivation": "现有语义映射系统在支持固定类别和开放语言标签预测方面存在不足。引入一种新的3D语义映射框架以提高计算效率和内存使用率。", "method": "SLIM-VDB利用了OpenVDB数据结构，并整合了一个统一的贝叶斯更新框架，实现了闭集和开集语义融合。", "result": "相比目前最先进的语义映射方法，SLIM-VDB在保持相当准确性的同时显著减少了内存使用量和集成时间。", "conclusion": "提出了一种新的3D概率语义映射框架SLIM-VDB，证明了其优越的性能和效率。"}}
{"id": "2512.12941", "pdf": "https://arxiv.org/pdf/2512.12941", "abs": "https://arxiv.org/abs/2512.12941", "authors": ["Siyuan Yao", "Dongxiu Liu", "Taotao Li", "Shengjie Li", "Wenqi Ren", "Xiaochun Cao"], "title": "UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction", "categories": ["cs.CV"], "comment": "IEEE TGRS", "summary": "Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet", "AI": {"tldr": "本文提出了一种名为UAGLNet的不确定性聚合全局局部融合网络，用于从遥感图像中提取建筑。", "motivation": "现有的方法在分割模型中采用卷积或自我注意块来捕获多尺度特征，但由于特性金字塔中的固有差距和全球本地特性的不足整合导致不准确、模糊的提取结果。为此，本文提出了一种新的不确定度聚合全局局部融合网络（UAGLNet）。", "method": "该方法采用混合CNN和变压器层在不同阶段捕获局部和全局视觉语义，并设计了一个中间协同交互块来缩小特征差距；提出了一个Global-Local Fusion模块以互补的方式融合全球和本地表示；引入了不确定性聚合解码器来估计像素级别的不确定度，从而提高分割准确性。", "result": "实验结果表明该方法在建筑提取任务上优于其他最先进的方法。", "conclusion": "UAGLNet通过结合CNN与Transformer的协同作用以及全局-局部融合机制，在处理复杂结构变化时提供了更准确的结果。"}}
{"id": "2512.12939", "pdf": "https://arxiv.org/pdf/2512.12939", "abs": "https://arxiv.org/abs/2512.12939", "authors": ["Sebastien Tchitchek", "Mohamed Kissi", "Julien Tierny"], "title": "Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams", "categories": ["cs.CG", "cs.CV", "cs.GR", "cs.LG"], "comment": "30 pages, 13 figures, 2 tables", "summary": "We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \\(α\\) (trade-off between temporal misalignment and diagram discrepancy) and \\(β\\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.12936", "pdf": "https://arxiv.org/pdf/2512.12936", "abs": "https://arxiv.org/abs/2512.12936", "authors": ["Tiange Zhang", "Xiandong Meng", "Siwei Ma"], "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to Data Compression Conference (DCC) 2026 as a poster paper", "summary": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.", "AI": {"tldr": "本文提出了一种基于内容自适应的运动对齐框架，以提高视频压缩性能。", "motivation": "现有的端到端视频压缩模型缺乏针对特定内容的调整策略，导致了次优的压缩效果。为了改善这一点，作者提出了一个能够根据不同内容特征来调整编码策略的方法。", "method": "该方法包括两个阶段：第一是引入了一个流引导可变形扭曲机制来进行运动补偿；第二是一种多参考质量感知策略，用于减少误差传播并优化训练过程。此外还集成了无训练模块以获取平滑的运动估计。", "result": "实验结果表明，在标准测试数据集上，所提出的框架CAMA相比最先进的神经视频压缩模型和传统编码器HM-16.25有显著提升，相对于基线模型DCVC-TCM实现了24.95%的BD-rate（PSNR）节省。", "conclusion": "本文的工作表明基于内容自适应的运动对齐框架能够有效提高神经视频压缩的质量和效率。"}}
{"id": "2512.12935", "pdf": "https://arxiv.org/pdf/2512.12935", "abs": "https://arxiv.org/abs/2512.12935", "authors": ["Toan Le Ngo Thanh", "Phat Ha Huu", "Tan Nguyen Dang Duy", "Thong Nguyen Le Minh", "Anh Nguyen Nhu Tinh"], "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "Accepted at AAAI Workshop 2026", "summary": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.", "AI": {"tldr": "提出了一种统一的多模态时刻检索系统，结合双嵌入管道、时间感知评分机制和代理引导查询分解技术。", "motivation": "现有的方法在跨模态噪声和模糊查询下的固定权重融合策略失效；时间建模难以捕捉连贯事件序列；需要手动选择模态降低可用性。", "method": "首先，使用BEIT-3和SigLIP进行广泛检索，并通过BLIP-2进行重排以平衡召回率和精确度。其次，应用指数衰减惩罚机制处理大的时间间隔，构建连贯的事件序列。最后，利用GPT-4o自动解释模糊查询并将其分解为模态特定子查询。", "result": "系统能够有效处理模糊查询、检索连续的时间序列，并自适应融合策略。", "conclusion": "提出的统一多模态时刻检索系统通过引入新颖的机制显著改进了交互式时刻搜索的能力。"}}
{"id": "2512.12932", "pdf": "https://arxiv.org/pdf/2512.12932", "abs": "https://arxiv.org/abs/2512.12932", "authors": ["Yifan Wu", "Jiyue Jiang", "Xichen Ye", "Yiqi Wang", "Chang Zhou", "Yitao Xu", "Jiayang Chen", "He Hu", "Weizhong Zhang", "Cheng Jin", "Jiao Yuan", "Yu Li"], "title": "Investigating Data Pruning for Pretraining Biological Foundation Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": "Accepted by AAAI 2026", "summary": "Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.", "AI": {"tldr": "研究在大规模预训练生物基础模型中采用数据修剪方法以减少计算成本。", "motivation": "为解决大规模生物序列和参数带来的计算成本高昂及可重复性问题，提出一种基于影响引导的数据修剪框架。", "method": "引入子集自影响公式进行样本重要性高效估计，并设计两种选择策略：Top-k Influence和Coverage-Centric Influence。", "result": "在RNA-FM和ESM-C模型上验证了方法的有效性和泛化能力，尤其是在极端数据修剪下表现优于随机基线。", "conclusion": "该框架能大幅减少生物基础模型预训练的计算成本，推动更高效、可访问且可持续发展的生物AI研究。"}}
{"id": "2512.12929", "pdf": "https://arxiv.org/pdf/2512.12929", "abs": "https://arxiv.org/abs/2512.12929", "authors": ["Huu-An Vu", "Van-Khanh Mai", "Trong-Tam Nguyen", "Quang-Duc Dam", "Tien-Huy Nguyen", "Thanh-Huong Le"], "title": "MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.", "AI": {"tldr": "提出MADTempo系统，旨在通过时间搜索和网页级视觉定位统一模型来检索多事件视频。", "motivation": "现有的视频检索方法在处理复杂事件的时间结构和未见过的概念查询时表现不佳。为了解决这些问题，研究团队提出了一个结合时间搜索机制和基于Google图片的补充模块的新框架。", "method": "MADTempo通过聚合连续视频片段中的相似性得分来捕获事件级连贯性，并利用外部网络图像增强预训练视觉嵌入以提高对异常值查询的鲁棒性。", "result": "新系统提高了现代视频检索系统的时序推理和泛化能力，使大规模视频集合内的语义感知检索成为可能。", "conclusion": "MADTempo通过结合时间搜索机制和基于Google图片的补充模块，增强了视频检索系统的性能，在处理多事件查询方面表现更佳。"}}
{"id": "2512.12925", "pdf": "https://arxiv.org/pdf/2512.12925", "abs": "https://arxiv.org/abs/2512.12925", "authors": ["Zhimao Peng", "Enguang Wang", "Fei Yang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted by TMM2025", "summary": "Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.", "AI": {"tldr": "提出了一种新的方法，用于改进广义类别发现任务中的伪标签质量", "motivation": "当前基于参数分类的广义类别发现方法会产生噪声伪标签，影响模型性能。为了解决这一问题，该论文提出了两种新模块：损失尖锐度惩罚（LSP）和动态锚点选择（DAS）。", "method": "通过最小化模型最差情况下的损失尖锐度，LSP增强了模型参数的鲁棒性；同时，DAS根据KNN密度和类别概率在训练过程中选择代表样本，并为其分配硬伪标签。", "result": "实验显示该方法能够有效减少噪声伪标签并提高聚类精度，在多个广义类别发现基准上取得了最新成果。", "conclusion": "通过引入LSP和DAS模块，论文成功提高了广义类别发现任务中的模型性能"}}
{"id": "2512.12921", "pdf": "https://arxiv.org/pdf/2512.12921", "abs": "https://arxiv.org/abs/2512.12921", "authors": ["Amy Chang", "Tiffany Saade", "Sanket Mendapara", "Adam Swanda", "Ankit Garg"], "title": "Cisco Integrated AI Security and Safety Framework Report", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space. This paper presents Cisco's Integrated AI Security and Safety Framework (\"AI Security Framework\"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.", "AI": {"tldr": "该论文提出了Cisco的集成人工智能安全与保障框架，旨在统一分类、整合和操作整个AI风险范围。", "motivation": "随着人工智能系统的广泛应用，攻击面不断扩大。现有的安全框架仅覆盖部分领域，因此需要一个全面且可扩展的安全框架来应对新兴部署中的多模态上下文、人形机器人等挑战。", "method": "分析现有安全框架的不足，并基于设计原则创建一个综合性的AI安全与保障框架。", "result": "提出的Cisco集成AI安全和保障框架能够识别威胁、进行红队演习，以及为组织提供风险优先级指导。", "conclusion": "新的框架在理解现代AI系统如何失效以及对手如何利用这些漏洞方面提供了结构化的方法，并帮助构建适应能力发展的防御措施。"}}
{"id": "2512.12918", "pdf": "https://arxiv.org/pdf/2512.12918", "abs": "https://arxiv.org/abs/2512.12918", "authors": ["Nijesh Upreti", "Vaishak Belle"], "title": "Satisfiability Modulo Theory Meets Inductive Logic Programming", "categories": ["cs.AI"], "comment": null, "summary": "Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.", "AI": {"tldr": "论文探讨了通过将SMT技术与ILP相结合来扩展符号规则学习的表达能力，特别是在处理数值约束方面。", "motivation": "传统的ILP系统在处理数值约束上存在局限性，如难以推理出必须共同存在于实例中的阈值或算术关系。因此，该论文旨在探究一种模块化的方法，将ILP与SMT结合以解决这些问题，并提高符号规则学习的表达能力。", "method": "论文中提出了一个组合PyGol和Z3（SMT求解器）的新方法。候选子句由PyGol提出并解释为量化自由公式，在背景理论下进行验证，如线性或非线性的实数算术。这样可以同时保持ILP的声明式关系偏向，并支持混合规则的生成。", "result": "该论文通过一系列合成数据集进行了评估，这些数据集旨在探索线性、关系、非线性和多跳推理等方面的能力。结果表明，模块化的SMT-ILP架构能够扩展符号规则学习的表达能力，同时提供了更灵活的基础以支持未来向理论感知诱导的发展。", "conclusion": "论文展示了将SMT技术与ILP相结合的有效性，并提出了一种可以提升现有数值ILP方法的新范式。这种方法不仅增强了符号规则的学习能力，而且为未来的扩展和应用奠定了基础。"}}
{"id": "2512.12914", "pdf": "https://arxiv.org/pdf/2512.12914", "abs": "https://arxiv.org/abs/2512.12914", "authors": ["Shashie Dilhara Batan Arachchige", "Benjamin Zi Hao Zhao", "Hassan Jameel Asghar", "Dinusha Vatsalan", "Dali Kaafar"], "title": "CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accepted at the 18th Cybersecurity Experimentation and Test Workshop (CSET), in conjunction with ACSAC 2025", "summary": "Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.", "AI": {"tldr": "CTIGuardian是一个针对细调大语言模型隐私泄漏的少样本框架。", "motivation": "在对大型语言模型进行特定任务或领域（如网络威胁情报）的微调时，可能会无意中泄露敏感信息。传统方法需要重新训练整个模型来消除泄漏，这不仅耗时且不切实际。因此提出了CTIGuardian以解决此问题。", "method": "通过引入隐私对齐的概念，并使用少量样本监督，结合隐私分类器和隐私删除器的方法，在细调的大语言模型中集成这些工具来防止信息泄露。", "result": "实验表明，相比基于命名实体识别的基准方法Presidio，CTIGuardian在提供更好的隐私-实用性权衡方面表现更优。", "conclusion": "虽然CTIGuardian是在网络威胁情报领域展示其有效性，但该框架具有足够的通用性可以应用于其他敏感领域的保护。"}}
{"id": "2512.12906", "pdf": "https://arxiv.org/pdf/2512.12906", "abs": "https://arxiv.org/abs/2512.12906", "authors": ["Zhimao Peng", "Enguang Wang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "Accepted by TCSVT2024", "summary": "Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.", "AI": {"tldr": "提出了一种基于预测样本分配的SCOOD框架，以提高ID和OOD样本集的纯净度。", "motivation": "当前SCOOD方法采用聚类策略从未标记数据中筛选干净的ID样本，不可避免地引入了大量噪声样本。为解决此问题，本文提出了PSA框架。", "method": "该方法包括基于预测能量分数的双阈值三元样本分配策略和概念对比表示学习损失函数，以扩大ID与OOD样本之间的距离，并采用重新训练策略使模型更好地拟合选定的辅助样本。", "result": "实验表明，本文的方法在两个标准SCOOD基准上超越了现有最先进方法。", "conclusion": "PSA框架通过提升ID和OOD样本集的纯净度，提高了SCOOD任务的表现。"}}
{"id": "2512.12900", "pdf": "https://arxiv.org/pdf/2512.12900", "abs": "https://arxiv.org/abs/2512.12900", "authors": ["Mohit Daga"], "title": "Sub-$n^k$ Deterministic algorithm for minimum $k$-way cut in simple graphs", "categories": ["cs.DS", "math.CO"], "comment": null, "summary": "We present a \\emph{deterministic exact algorithm} for the \\emph{minimum $k$-cut problem} on simple graphs. Our approach combines the \\emph{principal sequence of partitions (PSP)}, derived canonically from ideal loads, with a single level of \\emph{Kawarabayashi--Thorup (KT)} contractions at the critical PSP threshold~$λ_j$. Let $j$ be the smallest index with $κ(P_j)\\ge k$ and $R := k - κ(P_{j-1})$. We prove a structural decomposition theorem showing that an optimal $k$-cut can be expressed as the level-$(j\\!-\\!1)$ boundary $A_{\\le j-1}$ together with exactly $(R-r)$ \\emph{non-trivial} internal cuts of value at most~$λ_j$ and $r$ \\emph{singleton isolations} (``islands'') inside the parts of~$P_{j-1}$. At this level, KT contractions yield kernels of total size $\\widetilde{O}(n / λ_j)$, and from them we build a \\emph{canonical border family}~$\\mathcal{B}$ of the same order that deterministically covers all optimal refinement choices. Branching only over~$\\mathcal{B}$ (and also including an explicit ``island'' branch) gives total running time $$ T(n,m,k) = \\widetilde{O}\\left(\\mathrm{poly}(m)+\\Bigl(\\tfrac{n}{λ_j}+n^{ω/3}\\Bigr)^{R}\\right), $$ where $ω< 2.373$ is the matrix multiplication exponent. In particular, if $λ_j \\ge n^{\\varepsilon}$ for some constant $\\varepsilon > 0$, we obtain a \\emph{deterministic sub-$n^k$-time algorithm}, running in $n^{(1-\\varepsilon)(k-1)+o(k)}$ time. Finally, combining our PSP$\\times$KT framework with a small-$λ$ exact subroutine via a simple meta-reduction yields a deterministic $n^{c k+O(1)}$ algorithm for $c = \\max\\{ t/(t+1), ω/3 \\} < 1$, aligning with the exponent in the randomized bound of He--Li (STOC~2022) under the assumed subroutine.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.12898", "pdf": "https://arxiv.org/pdf/2512.12898", "abs": "https://arxiv.org/abs/2512.12898", "authors": ["Abhinav Kumar", "Tristan Aumentado-Armstrong", "Lazar Valkov", "Gopal Sharma", "Alex Levinshtein", "Radek Grzeszczuk", "Suren Kumar"], "title": "Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "28 pages, 8 figures, Project Page: https://abhi1kumar.github.io/qonvolution/", "summary": "Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.", "AI": {"tldr": "通过引入查询卷积（Qonvolution）来提高高频率信号的学习能力。", "motivation": "神经网络在处理高频信号时面临挑战，因为它们常常由于频谱偏向或优化难题而表现不佳。尽管现有的技术如傅里叶编码已经取得了显著进展，但在面对高频信息时仍有改进空间。", "method": "提出了查询卷积（Qonvolution）这一简单但强大的修改方法，利用了卷积的邻域特性来增强对复杂高频率信号的学习能力。", "result": "实验证明，在包括1D回归、2D超分辨率、2D图像回归和新颖视图合成等多任务中，Qonconvolutions表现出了提升效果。特别是在结合高斯散射和Qonvolutions用于NVS时，对于真实世界的复杂场景实现了最先进的性能，并且在图像质量上超越了强大的辐射场模型。", "conclusion": "该方法通过引入查询卷积（Qonvolution）成功地增强了高频信号的学习能力，在多种任务中取得了显著效果。"}}
{"id": "2512.12891", "pdf": "https://arxiv.org/pdf/2512.12891", "abs": "https://arxiv.org/abs/2512.12891", "authors": ["Mahsa Nasri", "Mahnoosh Jahanian", "Wei Wu", "Binyan Xu", "Casper Harteveld"], "title": "Tangible Intangibles: Exploring Embodied Emotion in Mixed Reality for Art Therapy", "categories": ["cs.HC"], "comment": null, "summary": "This in-person studio explores how mixed reality (MR) and biometrics can make intangible emotional states tangible through embodied art practices. We begin with two well-established modalities, clay sculpting and free-form 2D drawing, to ground participants in somatic awareness and manual, reflective expression. Building on this baseline, we introduce an MR prototype that maps physiological signals (e.g., breath, heart rate variability, eye movement dynamics) to visual and spatial parameters (color saturation, pulsing, motion qualities), generating ''3D emotional artifacts.'' The full-day program balances theory (somatic psychology, embodied cognition, expressive biosignals), hands-on making, and comparative reflection to interrogate what analog and digital modalities respectively afford for awareness, expression, and meaning-making. Participants will (1) experience and compare analog and MR-based journaling of emotion; (2) prototype and critique mappings from biosignals to visual/spatial feedback; and (3) articulate design principles for trauma-informed, hybrid workflows that amplify interoceptive literacy without overwhelming the user. The expected contributions include a shared design vocabulary for biometric expressivity, a set of generative constraints for future TEI work on emotional archiving, and actionable insights into when automated translation supports or hinders embodied connection.", "AI": {"tldr": "本文探讨了如何通过混合现实(MR)和生理信号将情感状态可视化，并研究其在艺术治疗中的应用。", "motivation": "文章旨在探索混合现实在表达和理解情绪方面的能力，特别是在艺术治疗的背景下。作者希望通过这种方法提高人们对内在感受的认识（内感觉识字）而不使用户感到过度压力。", "method": "通过传统的陶土雕塑和自由形式绘画作为基础技术，引入MR原型来将生理信号映射到视觉和空间参数上，生成‘3D情感艺术品’。研究包括理论探讨、实践工作坊及反思比较。", "result": "参与者体验了类比与MR基情绪日记的不同方式，并提出了生物信号至视觉/空间反馈的映射模型的设计原则。", "conclusion": "本文贡献了一套用于生物信息表达的设计词汇，为未来TEI（触觉增强交互）工作提供了一组生成性约束和实用洞察。"}}
{"id": "2512.12888", "pdf": "https://arxiv.org/pdf/2512.12888", "abs": "https://arxiv.org/abs/2512.12888", "authors": ["David Dang", "Stuart Love", "Meena Salib", "Quynh Dang", "Samuel Rothfarb", "Mysk Alnatour", "Andrew Salij", "Hou-Tong Chen", "Ho Wai", "Lee", "Wilton J. M. Kort-Kamp"], "title": "Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence", "categories": ["physics.optics", "cs.AI", "cs.CL", "cs.LG"], "comment": "Keywords: Physics-informed machine learning; Transformer models; Reinforcement learning; Chain-of-thought reasoning; Metasurfaces; Nanophotonics; Inverse design", "summary": "Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.", "AI": {"tldr": "Meta-GPT是一个基于METASTRINGS语言训练的Transformer模型，用于设计光子学中的纳米结构。", "motivation": "为了促进物理科学领域的人工智能发展，需要一种既可解释又符合自然法则的表示方法。通过引入METASTRINGS，连接人类理解和计算设计，实现对光子元表面组成规则的学习。", "method": "开发了Meta-GPT模型，并基于METASTRINGS进行训练和优化，结合物理信息引导监督学习、增强学习和链式思维推理等方法提升性能。", "result": "在多种设计任务中，模型达到小于3%的平均平方频谱误差，并保持超过98%的语法有效性，生成实验测量结果与目标光谱相匹配的元表面原型。", "conclusion": "Meta-GPT通过METASTRINGS学习了光-物质相互作用的组成规则，为AI驱动的光子学打下了坚实基础，标志着朝向光子元基因组项目的重要一步。"}}
{"id": "2512.12887", "pdf": "https://arxiv.org/pdf/2512.12887", "abs": "https://arxiv.org/abs/2512.12887", "authors": ["Han Liu", "Bogdan Georgescu", "Yanbo Zhang", "Youngjin Yoo", "Michael Baumgartner", "Riqiang Gao", "Jianing Wang", "Gengyan Zhao", "Eli Gibson", "Dorin Comaniciu", "Sasa Grbic"], "title": "Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification", "categories": ["cs.CV"], "comment": "1st Place in VLM3D Challenge", "summary": "3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.", "AI": {"tldr": "该论文介绍了AnyMC3D，一种从二维基础模型中演化而来的可扩展三维医学图像分类器。", "motivation": "目前的医疗基础模型研究存在数据制度偏见、次优适应和任务覆盖率不足等问题。本文旨在解决这些问题，并展示了一种基于二维基础模型构建的高效三维图像分类方法。", "method": "通过添加轻量级插件（每个任务约1M参数），AnyMC3D在单一冻结骨干网络上进行扩展，支持多视图输入、辅助像素级监督以及可解释热图生成。我们建立了一个包括十二个涵盖不同病理学、解剖结构和模式的任务的全面基准。", "result": "分析结果揭示了几个关键见解：有效的适应对于解锁基础模型潜力至关重要；通用目的的基础模型在适当调整后可以与专门设计的医疗模型相匹敌；基于二维的方法在三维分类中表现优于传统三维架构。我们的方法实现了跨多个应用领域的先进性能，并在VLM3D挑战赛中获得第一名。", "conclusion": "通过展示使用单一可扩展框架达到顶尖水平的可能性，本文证明了无需为每个任务单独创建特定模型的可行性。"}}
{"id": "2512.12885", "pdf": "https://arxiv.org/pdf/2512.12885", "abs": "https://arxiv.org/abs/2512.12885", "authors": ["Minghao Zhu", "Zhihao Zhang", "Anmol Sidhu", "Keith Redmill"], "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.RO"], "comment": "Submitted to IV 2026", "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.", "AI": {"tldr": "介绍了一种基于检索增强生成的零样本道路标志识别框架SignRAG。", "motivation": "传统的深度学习方法在处理大量标志类别时难以创建详尽的标注数据集，因此提出了一个新的零样本识别框架来解决这一问题。", "method": "利用视觉语言模型(VLM)从输入图像中生成文本描述，然后使用向量数据库检索最相关的标志候选者。接着，大型语言模型(LLM)通过推理做出最终细粒度识别。", "result": "实验结果在理想参考图象上达到95.58%的准确率，在挑战性的实际道路数据上则为82.45%。", "conclusion": "该研究展示了基于RAG架构创建可扩展和精确的道路标志识别系统而无需特定任务训练的可行性。"}}
{"id": "2512.12884", "pdf": "https://arxiv.org/pdf/2512.12884", "abs": "https://arxiv.org/abs/2512.12884", "authors": ["Xiangzhong Liu", "Jiajie Zhang", "Hao Shen"], "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": "6 pages, 3 figures, accepted at IV2025", "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.", "AI": {"tldr": "本文提出了一种基于Transformer的跨层融合方法，用于通过智能传感器和V2X模块提供的对象列表与相机图像进行三维物体检测。", "motivation": "传统传感器数据通常以处理后的对象列表形式而非原始数据提供。本研究旨在通过直接将这些高度抽象的对象列表信息与原始相机图像相结合，实现端到端的跨层融合。", "method": "采用Transformer模型作为核心架构，其中对象列表被用作去噪查询，并与其可学习的查询一起传播并通过特征聚合过程。此外，引入变形高斯掩模以增强对兴趣区域的关注并加速训练收敛。", "result": "所提方法在nuScenes数据集上表现出了相对于视觉基线的重大性能提升，并且具有处理不同噪声级别下模拟对象列表和真实检测器的泛化能力。", "conclusion": "本文首次展示了基于Transformer的跨层融合概念的有效性，证明了该方法可以显著提高3D物体检测性能并具备良好的泛化能力。"}}
{"id": "2512.12875", "pdf": "https://arxiv.org/pdf/2512.12875", "abs": "https://arxiv.org/abs/2512.12875", "authors": ["Weihan Xu", "Kan Jen Cheng", "Koichi Saito", "Muhammad Jehanzeb Mirza", "Tingle Li", "Yisi Liu", "Alexander H. Liu", "Liming Wang", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji", "Gopala Anumanchipalli", "Paul Pu Liang"], "title": "Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal", "categories": ["cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.", "AI": {"tldr": "本文提出了Schrodinger Audio-Visual Editor（SAVE）模型，用于同时编辑音频和视频内容，并保持二者的时间同步与语义一致性。", "motivation": "联合编辑音频和视觉内容时存在配对数据不足以及模态异质性的挑战。为解决这些问题，开发了SAVEBench数据集和SAVE模型以实现精准、可控的音频-视频编辑任务。", "method": "通过引入SAVEBench数据集及Schrodinger Bridge机制，训练了一个端到端的流匹配模型，在平行处理的同时保持音视频同步。", "result": "实验表明，与单独使用音频或视频编辑器相比，SAVE模型能更有效地移除目标对象并保留其他内容，并表现出更强的时间同步性和音频-视觉语义一致性。", "conclusion": "通过创新的SAVEBench数据集和SAVE模型，实现了高效的音视频联合编辑，为未来的多模态编辑任务提供了有力支持。"}}
{"id": "2512.12870", "pdf": "https://arxiv.org/pdf/2512.12870", "abs": "https://arxiv.org/abs/2512.12870", "authors": ["Pouya Ahadi", "Blair Winograd", "Camille Zaug", "Karunesh Arora", "Lijun Wang", "Kamran Paynabar"], "title": "Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": "22 pages, 6 figures. Preprint under review", "summary": "Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.", "AI": {"tldr": "提出一种新的主动学习框架，通过优化标签分配和采样方法来最小化噪声影响，从而提升分类模型的准确性。", "motivation": "在标注训练数据成本高昂的情况下，主动学习可以通过查询最有信息量的数据点进行标注。然而，由于标签者的准确率不一，导致生成的标签可能含有噪音，并且不确定样本更容易受到错误标签的影响。", "method": "提出了一种新的分配模型来优化查询点到标记器的最佳分配，以最小化每个周期内最大的噪声水平；同时引入了新的采样方法以选择最优的查询点，从而减少标签噪声对分类性能的影响。", "result": "实验表明，该方法相比多个基准方法显著提升了分类性能。", "conclusion": "所提出的主动学习框架能有效降低由于标签不准确导致的错误，并且可以提高整体分类器的表现。"}}
{"id": "2512.12868", "pdf": "https://arxiv.org/pdf/2512.12868", "abs": "https://arxiv.org/abs/2512.12868", "authors": ["Furong Jia", "Yuan Pu", "Finn Guo", "Monica Agrawal"], "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.", "AI": {"tldr": "通过MedQA数据集研究大规模语言模型（LLM）的性能是否依赖于概率推理，引入轻量级方法Frequency-Based Probabilistic Ranker (FBPR)，并比较其与对应预训练LLMs的表现。", "motivation": "探讨大型语言模型在多选题临床诊断基准上的卓越表现是否源于潜在的概率推理能力，并研究使用基于频率的方法能否达到类似效果。", "method": "提出了一种轻量级方法，即基于频率的概率排名器（FBPR），该方法使用平滑后的朴素贝叶斯对概念-诊断共现统计数据进行评分。这些统计从大规模语料库中获得。", "result": "当共现统计数据来源于OLMo和Llama的预训练语料库时，FBPR与相应的LLMs在相同语料库上具有可比的表现；直接使用LLM推断与FBPR正确回答的问题重叠度仅为略高于随机机会。", "conclusion": "虽然大型语言模型的强大表现可能并非仅由简单的频率聚合机制驱动，但历史基础且低复杂性的方法仍能解释基准性能的相当大一部分。这种方法为未来的混合信号提供了一个有价值的参考点和补充信号。"}}
{"id": "2512.12860", "pdf": "https://arxiv.org/pdf/2512.12860", "abs": "https://arxiv.org/abs/2512.12860", "authors": ["Aritra Banik", "Mano Prakash Parthasarathi", "Venkatesh Raman", "Diya Roy", "Abhishek Sahu"], "title": "Learning with Structure: Computing Consistent Subsets on Structurally-Regular Graphs", "categories": ["cs.DS", "cs.CG"], "comment": "15 pages, 1 figure", "summary": "The Minimum Consistent Subset (MCS) problem arises naturally in the context of supervised clustering and instance selection. In supervised clustering, one aims to infer a meaningful partitioning of data using a small labeled subset. However, the sheer volume of training data in modern applications poses a significant computational challenge. The MCS problem formalizes this goal: given a labeled dataset $\\mathcal{X}$ in a metric space, the task is to compute a smallest subset $S \\subseteq \\mathcal{X}$ such that every point in $\\mathcal{X}$ shares its label with at least one of its nearest neighbors in $S$. Recently, the MCS problem has been extended to graph metrics, where distances are defined by shortest paths. Prior work has shown that MCS remains NP-hard even on simple graph classes like trees, though an algorithm with runtime $\\mathcal{O}(2^{6c} \\cdot n^6)$ is known for trees, where $c$ is the number of colors and $n$ the number of vertices. This raises the challenge of identifying graph classes that admit algorithms efficient in both $n$ and $c$. In this work, we study the Minimum Consistent Subset problem on graphs, focusing on two well-established measures: the vertex cover number ($vc$) and the neighborhood diversity ($nd$). We develop an algorithm with running time $vc^{\\mathcal{O}(vc)}\\cdot\\text{Poly}(n,c)$, and another algorithm with runtime $nd^{\\mathcal{O}(nd)}\\cdot\\text{Poly}(n,c)$. In the language of parameterized complexity, this implies that MCS is fixed-parameter tractable (FPT) parameterized by the vertex cover number and the neighborhood diversity. Notably, our algorithms remain efficient for arbitrarily many colors, as their complexity is polynomially dependent on the number of colors.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.12858", "pdf": "https://arxiv.org/pdf/2512.12858", "abs": "https://arxiv.org/abs/2512.12858", "authors": ["Sonal Prabhune", "Balaji Padmanabhan", "Kaushik Dutta"], "title": "Information-Consistent Language Model Recommendations through Group Relative Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.", "AI": {"tldr": "本文提出了一种基于群体相对策略优化（GRPO）的强化学习框架，以直接优化大规模语言模型在不同表述下的一致性表现。", "motivation": "大型语言模型在业务关键领域如金融、教育和客服中部署时表现出轻微差异表述下的不一致性，影响了用户体验和合规性。", "method": "提出了一种基于群体相对策略优化的强化学习框架，并引入了以信息熵为基础的帮助性和稳定性奖励机制来解决这一问题。", "result": "实验表明，通过GRPO训练模型在投资和工作推荐任务中的表现优于微调或解码基准方法，减少了语言模型的回答变异度。", "conclusion": "该研究首次将GRPO应用于大规模语言模型的一致性优化中，并将其视为可纠正的企业部署缺陷而非生成多样性的优点。"}}
{"id": "2512.12856", "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.", "AI": {"tldr": "本文介绍了Memory-Aware Retention Schema（MaRS）框架和Forgetful but Faithful Agent（FiFA）基准，用于评估生成式代理的隐私意识记忆管理。", "motivation": "当前生成式代理的记忆管理能力是性能和隐私的关键瓶颈，需要一种既能平衡性能和隐私又能保证计算效率的方法。", "method": "提出了Memory-Aware Retention Schema框架，并设计了Forgetful but Faithful Agent（FiFA）基准来评估代理的叙事连贯性、目标完成度和社会回忆准确性等。", "result": "通过300次实验，证明所提方法在保持隐私和计算效率的同时，性能更优。", "conclusion": "本文提出的方法解决了生成式代理记忆管理的关键挑战，并为部署于资源受限和隐私敏感环境提供了指南。"}}
{"id": "2512.12855", "pdf": "https://arxiv.org/pdf/2512.12855", "abs": "https://arxiv.org/abs/2512.12855", "authors": ["Patrick Kostelac", "Xuerui Wang", "Anahita Jamshidnejad"], "title": "MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.", "AI": {"tldr": "提出了一种结合模型预测控制(MPC)和强化学习(RL)的集成框架，以实现在工程应用中安全、实时且适应性强的智能控制。", "motivation": "现代工程系统需要能够应对不确定性、适应环境变化并确保在实时约束下的安全性。虽然RL提供了强大的数据驱动自适应性，但缺乏内置机制来满足动态约束条件；MPC虽能处理结构化约束和增强鲁棒性，但依赖于准确模型且在线优化计算量大。", "method": "该论文提出了一种结合MPC与RL的框架，在训练阶段利用MPC定义安全控制界限以指导RL学习约束感知策略。在部署时，采用基于Lipschitz连续性的轻量级安全过滤器确保满足约束条件而无需复杂的在线优化。", "result": "验证于非线性航空弹性翼系统，该方法展示了改进的扰动抑制、减少执行器努力和湍流下的鲁棒表现。", "conclusion": "这种集成架构可以推广到具有结构化非线性和有限扰动的其他领域，为工程应用中的安全人工智能控制提供了可扩展解决方案。"}}
{"id": "2512.12844", "pdf": "https://arxiv.org/pdf/2512.12844", "abs": "https://arxiv.org/abs/2512.12844", "authors": ["Yunpeng Xu", "Wenge Guo", "Zhi Wei"], "title": "Selective Conformal Risk Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \\textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.", "AI": {"tldr": "提出了一种选择性校形风险控制（SCRC）框架，将校形预测与选择性分类结合，以更有效地量化不确定性。", "motivation": "为了在高风险领域部署机器学习系统，需要可靠的风险量化。传统的校形预测虽然提供了无分布覆盖保证，但经常产生过大的预测集，限制了其实用性。为此，提出了一种新的框架来解决这一问题。", "method": "该框架将不确定性控制作为两阶段问题：第一阶段选择出具有高信心的样本进行预测；第二阶段在选定的子集上应用校形风险控制以构建校准预测集。提出了两种算法：SCRC-T和SCRC-I，前者通过联合计算阈值保持可交换性并提供确切有限样本保证，后者仅使用校准样本并通过概率近似正确（PAC）方式提供保证。", "result": "实验结果表明，两种方法都能达到目标覆盖水平和风险控制水平，具有几乎相同的性能。SCRC-I显示出略微更保守的风险控制但计算效率更高。", "conclusion": "选择性校形风险控制为紧凑且可靠的不确定性量化提供了有效而高效的途径"}}
{"id": "2512.12842", "pdf": "https://arxiv.org/pdf/2512.12842", "abs": "https://arxiv.org/abs/2512.12842", "authors": ["Kuan Fang", "Yuxin Chen", "Xinghao Zhu", "Farzad Niroui", "Lingfeng Sun", "Jiuguang Wang"], "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages, 7 figures", "summary": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.", "AI": {"tldr": "SAGA是一个适应性强的框架，通过在观察环境中明确任务目标来区分高层次语义意图和低层次视觉运动控制，适用于开放世界的移动操作。", "motivation": "当前大多数机器人控制系统缺乏跨环境、任务目标及用户说明泛化的能力。为了实现更高效的适应性学习，作者提出将高级别语义意图与低级别视觉运动控制分离，并在观察环境中明确任务目标。", "method": "SAGA使用基于可操作性的任务表示法来表达各种复杂行为的统一结构形式。通过利用多模态基础模型，SAGA以3D可操作性热图的形式将所提出的任务表征与机器人的视觉观测结果联系起来，突出任务相关实体并抽象掉阻碍泛化的外观变化。", "result": "在涵盖语言指令、选定点和示例演示的统一框架下，SAGA能够在十一项真实世界任务中实现零样本执行和少量样本适应。实验结果显示，与端到端及模块化基准相比，SAGA有显著的优势。", "conclusion": "结构化的可操作性接地为泛用型移动操纵提供了一种可扩展且有效的方法。"}}
{"id": "2512.12840", "pdf": "https://arxiv.org/pdf/2512.12840", "abs": "https://arxiv.org/abs/2512.12840", "authors": ["Sindhuja Madabushi", "Ahmad Faraz Khan", "Haider Ali", "Ananthram Swami", "Rui Ning", "Hongyi Wu", "Jin-Hee Cho"], "title": "PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning \"private.\" PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.", "AI": {"tldr": "PRIVEE是一种隐私保护的垂直联邦学习机制，旨在对抗特征推理攻击。", "motivation": "垂直联邦学习面临来自共享置信度分数（即预测概率）的特征推理攻击威胁。此研究提出了一种新的防御机制来应对这种威胁。", "method": "PRIVEE通过混淆置信度分数同时保持相对排名和间距等关键属性，仅分享变换后的表示而不是原始分数。", "result": "实验结果显示，与最先进的防御方法相比，PRIVEE在隐私保护方面提高了三倍，同时完全保留了模型预测准确性。", "conclusion": "PRIVEE有效提升了垂直联邦学习中的隐私保护水平，并且不牺牲模型的预测性能。"}}
{"id": "2512.12834", "pdf": "https://arxiv.org/pdf/2512.12834", "abs": "https://arxiv.org/abs/2512.12834", "authors": ["Shangxuan Luo", "Joshua Reiss"], "title": "Procedural Music Generation Systems in Games", "categories": ["cs.SD"], "comment": null, "summary": "Procedural Music Generation (PMG) is an emerging field that algorithmically creates music content for video games. By leveraging techniques from simple rule-based approaches to advanced machine learning algorithms, PMG has the potential to significantly improve development efficiency, provide richer musical experiences, and enhance player immersion. However, academic prototypes often diverge from applications due to differences in priorities such as novelty, reliability, and allocated resources. This paper bridges the gap between research and applications by presenting a systematic overview of current PMG techniques in both fields, offering a two-aspect taxonomy. Through a comparative analysis, this study identifies key research challenges in algorithm implementation, music quality and game integration. Finally, the paper outlines future research directions, emphasising task-oriented and context-aware design, more comprehensive quality evaluation methods, and improved research tool integration to provide actionable insights for developers, composers, and researchers seeking to advance PMG in game contexts.", "AI": {"tldr": "本文系统性地概述了当前游戏中的程序音乐生成技术，并提出了一种两方面分类法，通过比较分析识别算法实现、音乐质量和游戏集成的关键研究挑战，为开发者和研究人员提供可操作的见解。", "motivation": "解决学术原型与实际应用之间的差距，提高开发效率，丰富玩家的游戏体验。", "method": "系统性概述当前程序音乐生成技术，并提出两方面分类法；进行比较分析以识别关键研究挑战；提出未来的研究方向。", "result": "识别出算法实现、音乐质量和游戏集成的关键研究挑战，明确了未来需要解决的问题和方法。", "conclusion": "强调了任务导向设计的重要性，提出了更全面的评价方法以及改进工具整合的需求。"}}
{"id": "2512.12832", "pdf": "https://arxiv.org/pdf/2512.12832", "abs": "https://arxiv.org/abs/2512.12832", "authors": ["Kaustav Chatterjee", "Joshua Li", "Kundan Parajulee", "Jared Schwennesen"], "title": "Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.", "AI": {"tldr": "本文开发了一种基于深度学习和传感技术的网络级别高速公路铁路交叉口挂起风险评估框架。", "motivation": "陡坡的公路铁路平交道口对低底盘车辆构成安全隐患，可能导致车辆卡在轨道上，引发列车碰撞事故。因此，研究旨在通过集成下一代传感器、深度学习和基础设施数据集来改进安全评估，并支持交通机构减轻交叉口危险。", "method": "利用行走式轮廓仪和Pave3D8K激光成像系统收集不同平交道的数据；构建结合LSTM与Transformer的混合模型以准确重建平交道轮廓；收集超过350辆特种车辆尺寸数据，分析三种不同车辆尺寸场景下的挂起风险。", "result": "结果显示在不同维度情景下分别有36、62和67个交叉点处于最高的挂起风险水平。开发了ArcGIS数据库及软件界面以支持交通机构减轻交叉口危险。", "conclusion": "本文提出的方法通过集成下一代传感器数据、深度学习技术以及基础设施数据集，成功地提供了对平交道挂起风险进行网络级别评估的实用决策工具，有助于提高交通安全评价水平。"}}
{"id": "2512.12827", "pdf": "https://arxiv.org/pdf/2512.12827", "abs": "https://arxiv.org/abs/2512.12827", "authors": ["Mohammad Mahdi Razmjoo", "Mohammad Mahdi Sharifian", "Saeed Bagheri Shouraki"], "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "16 pages, 8 figures", "summary": "Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.", "AI": {"tldr": "通过分析模型梯度参数的内在维度，提出了一种新的对抗检测方法。", "motivation": "深度神经网络存在对微小对抗性扰动敏感的问题，这可能影响医疗诊断和自动驾驶等领域的可靠性，因此需要有效的对抗攻击检测方案。", "method": "利用几何属性研究了模型输入损失景观的内在维度（ID），发现了自然数据与对抗性数据之间存在的明显差异，并以此为基础设计了一种新的对抗检测方法。", "result": "该方法在批量和单样本设置中都表现出色，对MNIST、SVHN、CIFAR-10和MS COCO等数据集上的多种攻击类型保持了超过92%的检出率。", "conclusion": "研究结果表明，内在维度是识别对抗性样本的强大指纹，在不同数据集和攻击策略下均表现出了良好的稳健性和有效性。"}}
{"id": "2512.12824", "pdf": "https://arxiv.org/pdf/2512.12824", "abs": "https://arxiv.org/abs/2512.12824", "authors": ["N. K. B. M. P. K. B. Narasinghe", "Uthayasanker Thayasivam"], "title": "Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 3 figures. Accepted to VISAPP 2026", "summary": "Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an \"augmentation divergence\": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.", "AI": {"tldr": "该论文研究了将对比描述器（CoCa）适应于低数据场景下的图像分类任务，探讨了不同的策略和超参数配置对其性能的影响。", "motivation": "尽管大型多模态基础模型在零样本迁移上取得了显著成果，但在极端少样本学习环境中的表现仍然未被充分研究。当前文献主要集中在双编码器架构（如CLIP）的应用上，并缺乏对CoCa独特隐空间在参数效率微调方面的理解。", "method": "该论文通过系统性评估包括无训练混合原型设计和基于低秩适应（LoRA）的深度参数调整等策略，探索了如何将CoCa视觉骨干网络用于少样本图像分类任务。同时研究了数据增强对线性探测性能的影响，并展示了监督对比损失在不同样本数下的优越表现。", "result": "论文发现强数据增强虽然降低了低样本环境中的线性探测性能，但在稳定LoRA微调方面至关重要。此外，混合目标函数结合监督对比损失在各种样本数量下均表现出一致的性能提升。", "conclusion": "该研究为如何高效地调整生成对比基础模型提供了实证参考设置，并强调了训练配置对数据稀缺性的敏感性，在促进少样本学习中具有重要意义。"}}
{"id": "2512.12822", "pdf": "https://arxiv.org/pdf/2512.12822", "abs": "https://arxiv.org/abs/2512.12822", "authors": ["Yongyuan Liang", "Xiyao Wang", "Yuanchen Ju", "Jianwei Yang", "Furong Huang"], "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.", "AI": {"tldr": "提出了一种统一的变压器架构Lemon，用于处理3D点云补丁和语言标记的联合处理，以解决大规模多模态模型在三维理解中的挑战。", "motivation": "当前大规模多模态模型在三维理解中存在数据稀疏、不规则的问题，并且现有模型依赖于片段化架构和模式特定编码器。这些模型训练流程不稳定，缺乏可扩展性。", "method": "Lemon通过将3D点云补丁与语言标记作为单一序列进行处理来解决这些问题，开发了一种结构化的补丁划分方案以保留空间上下文，并设计了一个三阶段的训练课程，逐步提升从物体级识别到场景级空间推理的能力。", "result": "Lemon在全面的三维理解和推理任务中建立了新的最先进性能，包括对象识别、描述以及3D场景中的空间推理等。随着模型规模和训练数据量的增长，显示了稳健的扩展特性。", "conclusion": "该研究提供了一个统一的基础框架，推动了现实世界应用中三维空间智能的进步。"}}
{"id": "2512.12821", "pdf": "https://arxiv.org/pdf/2512.12821", "abs": "https://arxiv.org/abs/2512.12821", "authors": ["Congzhou M Sha"], "title": "On the continuity of flows", "categories": ["cs.LG", "cs.AI", "physics.data-an"], "comment": "9 pages, 2 figures", "summary": "Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.", "AI": {"tldr": "论文探讨了连续归一化流在先验分布和目标分布拓扑不匹配时的最优速度场可能出现的空间间断性。", "motivation": "研究指出，当使用连续归一化流将单模态映射到多模态时，可能会导致空间间的间断。论文探讨了这种现象的原因及影响。", "method": "通过理论分析双峰高斯混合模型，展示了最优速度场在决策边界处的跳跃间断，并验证这一理论。", "result": "发现连续归一化流在拓扑不匹配的情况下可能产生无穷大的间断性。", "conclusion": "论文指出这种现象并非特定于$L^2$损失函数，而是由分布之间的拓扑差异导致。这为深度学习中的间断表示学习提出了新的挑战和启示。"}}
{"id": "2512.12818", "pdf": "https://arxiv.org/pdf/2512.12818", "abs": "https://arxiv.org/abs/2512.12818", "authors": ["Chris Latimer", "Nicoló Boschi", "Andrew Neeser", "Chris Bartholomew", "Gaurav Srivastava", "Xuan Wang", "Naren Ramakrishnan"], "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.", "AI": {"tldr": "构建了一个名为Hindsight的记忆架构，该架构可以组织记忆信息并支持推理。", "motivation": "当前的代理内存系统存在无法清晰区分证据和推断、难以长期组织信息以及缺乏解释其推理能力的问题。因此需要一种新的方法来改进这些问题。", "method": "提出了一个基于四个逻辑网络的记忆框架，通过保留、召回和反思操作来管理记忆的信息添加、访问和更新过程，并在长对话记忆基准上进行了测试。", "result": "Hindsight模型在LongMemEval中提高了整体准确率从39%到83.6%，进一步扩展了模型后，在LongMemEval上的表现达到了91.4%，在LoCoMo中的表现为89.61%，超过了当前最好的记忆架构。", "conclusion": "通过结构化、可查询的内存银行，Hindsight展示了其超越现有代理记忆系统的潜力。"}}
{"id": "2512.12817", "pdf": "https://arxiv.org/pdf/2512.12817", "abs": "https://arxiv.org/abs/2512.12817", "authors": ["Mengqian Wu", "Jiayi Zhang", "Raymond Z. Zhang"], "title": "Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.", "AI": {"tldr": "探讨人工智能生成的辩论论证与人类辩论者之间的差异，使用亚里士多德修辞原则进行分析。", "motivation": "利用人工智能辅助学生准备辩论，并研究其在构建有效论据方面的表现与人类教练的效果相比如何。", "method": "通过GPT-4生成证据卡片并与学生辩手的证据卡片比较，基于亚里士多德修辞原则（伦理、情感和逻辑）进行系统定性定量分析。", "result": "发现人工智能在辩论推理方面有其独特的优势和局限，并确定了AI与人类论证策略的一致性和差异之处。", "conclusion": "研究为利用AI辅助学习提供了见解，帮助学生辩手提高论点构建能力和推理技巧。"}}
{"id": "2512.12812", "pdf": "https://arxiv.org/pdf/2512.12812", "abs": "https://arxiv.org/abs/2512.12812", "authors": ["Hanyu Cai", "Binqi Shen", "Lier Jin", "Lan Hu", "Xiaojing Fan"], "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing. Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.", "AI": {"tldr": "研究了不同语气对三个现代大规模语言模型（GPT、Gemini和LLaMA）在STEM和人文领域任务中的性能影响。", "motivation": "探究提示工程中语用因素如语气和礼貌的影响，特别是在不同的模型家族之间。现有研究相对较少。", "method": "使用MMMLU基准测试了友好型、中性和粗鲁型语气的提示对三个现代大规模语言模型（GPT-4o mini、Gemini 2.0 Flash 和 Llama 4 Scout）在六个任务上的性能影响，并进行了统计显著性分析。", "result": "结果表明，语气敏感度因模型和领域而异。友好或中性的提示通常比粗鲁的提示产生更高的准确性，但在某些人文领域的子集任务中，粗鲁的语气对GPT和LLaMA的准确性有负面影响，而Gemini相对不敏感。", "conclusion": "研究发现，在特定解释场景下，互动语气可能会起作用；但在典型的多领域应用中，现代大规模语言模型总体上较为稳健于语气变化。这为实际应用场景中的提示设计和模型选择提供了实用指导。"}}
{"id": "2512.12809", "pdf": "https://arxiv.org/pdf/2512.12809", "abs": "https://arxiv.org/abs/2512.12809", "authors": ["Junbo Jacob Lian", "Mingyang Yu", "Kaichen Ouyang", "Shengwei Fu", "Rui Zhong", "Yujun Zhang", "Jun Zhang", "Huiling Chen"], "title": "OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization", "categories": ["cs.NE", "cs.AI"], "comment": "Source code, experiment scripts, and results are publicly available at https://github.com/junbolian/OPAL. The real-world application part hasn't been done yet", "summary": "Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.", "AI": {"tldr": "论文提出了OPAL框架，通过学习不同优化问题的自适应操作程序来提高黑盒优化性能。", "motivation": "当前黑盒优化算法依赖于进化和群集算法，但其效果高度依赖于特定问题。作者旨在为每个实例设计个性化的搜索策略以改善优化结果。", "method": "OPAL框架通过使用图形神经网络构建采样点的邻近图，并学习如何将这些轨迹表示转化为探索、重启和局部搜索操作的阶段化调度，从而生成适合特定地形的操作程序。", "result": "在CEC~2017测试套件上，单一元训练的OPAL策略与最先进的自适应差分进化变体具有统计竞争力，并且显著优于简单的基线算法。", "conclusion": "研究结果表明，基于景观感知、个性化设计的方法是一种超越传统的启发式黑盒优化的有效途径。"}}
{"id": "2512.12806", "pdf": "https://arxiv.org/pdf/2512.12806", "abs": "https://arxiv.org/abs/2512.12806", "authors": ["Boyang Yan"], "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution", "categories": ["cs.AI"], "comment": "7 pages", "summary": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.", "AI": {"tldr": "本文提出了一种容错沙箱框架，通过政策拦截层和事务型文件系统快照机制来保证AI编码代理的安全性。", "motivation": "从被动代码生成器转变为自主代理的大型语言模型（LLMs）引入了安全风险，特别是破坏性命令和不一致的系统状态。现有的商用解决方案通常优先考虑用户交互安全性，并通过身份验证障碍打破无头循环，这不利于真正的自主操作。", "method": "本文提出了一种容错沙箱框架，该框架包含政策拦截层和事务型文件系统快照机制，以保证安全并减少延迟。实验在自定义Proxmox测试平台利用EVPN/VXLAN隔离部署Minimind-MoE LLM并通过nano-vllm服务。", "result": "实验结果显示，高风险命令的拦截率为100%，失败状态回滚成功率也为100%。每个事务平均额外消耗约1.8秒（性能开销为14.5%）。相比之下，Gemini CLI沙箱需要交互式身份验证，导致其无法用于无头自主代理工作流。", "conclusion": "该容错沙箱框架通过原子事务封装代理动作来保证安全性，并且与现有解决方案相比在性能上具有优势。"}}
{"id": "2512.12805", "pdf": "https://arxiv.org/pdf/2512.12805", "abs": "https://arxiv.org/abs/2512.12805", "authors": ["Anastasiia Alokhina", "Pan Li"], "title": "From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transformers exhibit a notable property of \\emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.", "AI": {"tldr": "发展了理论框架分析变压器在几何数据上的大小泛化能力。", "motivation": "研究变压器从较小的token集扩展到较大的token集的能力，并提供严格的理论基础。", "method": "通过建立连续源域和离散样本之间的误差界限，证明变压器输出与连续域等价物间的差异由稳定位置编码、采样密度及数据流形内在维度决定。", "result": "实验验证了该理论界对于不同大小的图和点云的有效性。", "conclusion": "为理解变压器在变量输入尺寸上的泛化能力提供了严谨的数学依据，并通过实验证明了所提理论边界的准确性。"}}
{"id": "2512.12804", "pdf": "https://arxiv.org/pdf/2512.12804", "abs": "https://arxiv.org/abs/2512.12804", "authors": ["Sander Beckers"], "title": "Causal Counterfactuals Reconsidered", "categories": ["cs.AI"], "comment": "Preprint: currently under review", "summary": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions", "AI": {"tldr": "提出了一个新的反事实概率语义学，适用于不能扩展为现实结构因果模型的概率因果模型。", "motivation": "现有Pearlian语义仅适用特定类型因果模型，而许多情况下需要更广泛的语义解释；作者希望通过新的语义解决这一问题。", "method": "开发了一个新方法来定义反事实概率，在满足Markov条件和包含现实变量的因果模型中限制注意力，并避免使用回应变量。", "result": "证明了新提出的方法等价于其他两个不涉及结构因果模型的近期方案，展示了其适用性和先进性。", "conclusion": "新的语义学为反事实概率提供了一个自然且广泛的解决方案，适用于更广泛的实际场景。"}}
{"id": "2512.12802", "pdf": "https://arxiv.org/pdf/2512.12802", "abs": "https://arxiv.org/abs/2512.12802", "authors": ["Erik Hoel"], "title": "A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness", "categories": ["q-bio.NC", "cs.AI"], "comment": "28 pages, 3 figures", "summary": "The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.", "AI": {"tldr": "通过新的邻近论证，证明当代大型语言模型（LLM）不具备意识。", "motivation": "探索和分析用于判断意识的理论是否能适用于现代大型语言模型，并探讨基于持续学习的理论在人类中的应用。", "method": "使用新的邻近论证来分析大型语言模型的输入/输出功能与其潜在意识的关系。进一步通过研究持续学习与意识之间的联系，提出一个正面结论。", "result": "证明了当代大型语言模型由于缺乏持续学习能力而无法具备意识。", "conclusion": "基于或需要持续学习的理论符合对人类意识的要求，并且这种能力的缺乏可能是现代大型语言模型不具备意识的原因。"}}
{"id": "2512.12800", "pdf": "https://arxiv.org/pdf/2512.12800", "abs": "https://arxiv.org/abs/2512.12800", "authors": ["Yunlong He", "Gwilherm Lesné", "Ziqian Liu", "Michaël Soumm", "Pietro Gori"], "title": "Learning Common and Salient Generative Factors Between Two Image Datasets", "categories": ["cs.CV"], "comment": "This is the author's version of a work submitted to IEEE for possible publication. The final version may differ from this version", "summary": "Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.", "AI": {"tldr": "本文提出了一种新的框架，用于分离两个图像数据集之间的共同生成因素和显著性生成因素。", "motivation": "现有的方法通常需要属性作为监督信号来进行编辑。然而，在没有这些详细信息的情况下，如何有效地区分通用和特定的生成因子是一个挑战。因此，研究者希望仅使用数据集信号来解决这一问题，并提出了一个新的框架。", "method": "通过定义新的学习策略和损失函数，作者提出了一种适用于GAN和扩散模型的对比分析（CA）方法，该方法能够分离共同的和显著性的生成因素。", "result": "在各种图像数据集中进行测试后，所提出的框架表现出比现有方法更好的分离能力和合成效果。", "conclusion": "通过采用本文的方法可以更有效地分离两个不同图像数据集之间的通用性和特定性特征，并保持高质量的生成能力。"}}
{"id": "2512.12799", "pdf": "https://arxiv.org/pdf/2512.12799", "abs": "https://arxiv.org/abs/2512.12799", "authors": ["Zhe Liu", "Runhui Huang", "Rui Yang", "Siming Yan", "Zining Wang", "Lu Hou", "Di Lin", "Xiang Bai", "Hengshuang Zhao"], "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning", "categories": ["cs.CV"], "comment": null, "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI", "AI": {"tldr": "本文提出了DrivePI，一个适用于自动驾驶理解、感知、预测和规划的新型4D多模态大型语言模型。", "motivation": "尽管多模态大语言模型在多个领域表现出强大的能力，但在生成自动驾驶中的细粒度3D感知和预测输出方面仍较少被探索。因此，本文旨在开发一个能够处理这些任务的统一框架。", "method": "DrivePI是一个空间感知4D多模态大型语言模型，通过端到端优化并行执行空间理解、3D感知（即占用）、预测（即占用流）和规划（即行动输出）。它集成点云、多视图图像和语言指令于统一的多模态大语言模型架构中，并开发了一种数据引擎以生成用于4D空间理解的文字-占用和文字-流动问答对。", "result": "DrivePI在nuScenes-QA上比OpenDriveVLA-7B高出2.5%的平均精度，在nuScenes上的碰撞率减少了70%，在OpenOcc上的3D占用方面超过了FB-OCC，降低了10.3 RayIoU，在占据流方面的mAVE从0.591降低到0.509，在规划上的L2误差比VAD低了32%。", "conclusion": "DrivePI作为一个单一的统一模型匹配或超过现有的视觉语言行动模型和专门的视觉行动模型，展示了其在自动驾驶任务中的强大性能。"}}
{"id": "2512.12793", "pdf": "https://arxiv.org/pdf/2512.12793", "abs": "https://arxiv.org/abs/2512.12793", "authors": ["Mizuho Aoki", "Kohei Honda", "Yasuhiro Yoshimura", "Takeshi Ishita", "Ryo Yonetani"], "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.", "AI": {"tldr": "提出了一种名为Vision-Language Global Localization (VLG-Loc)的方法，利用人类可读的地标地图进行机器人全局定位。", "motivation": "将自然语言描述的地图转换为机器人的定位能力具有挑战性，因为需要在没有几何和外观细节的情况下建立观察到的地标的对应关系。", "method": "通过使用视觉-语言模型(VLM)搜索机器人多方向图像中的地标，并利用找到的地标评估每个假设姿态的可能性，从而实现全局定位。", "result": "实验验证表明，在模拟和真实零售环境中，该方法比现有的扫描基方法具有更高的鲁棒性，尤其是在环境变化下表现更优。", "conclusion": "VLG-Loc通过将自然语言地图与机器视觉融合，成功提高了机器人在复杂环境中的定位性能。"}}
{"id": "2512.12792", "pdf": "https://arxiv.org/pdf/2512.12792", "abs": "https://arxiv.org/abs/2512.12792", "authors": ["Shivansh Sahni", "Wenzhi Zhang"], "title": "Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 0 figures", "summary": "The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.", "AI": {"tldr": "本文提出了一个名为Liquid Reasoning Transformer（LRT）的Transformer架构，用于在Sudoku问题上进行自适应推理任务，展示其扩展到大型算法任务的能力。", "motivation": "设计一种能够在迭代过程中纠正早期错误并根据输入难度分配计算资源的模型，以解决需要复杂结构化推理的任务。", "method": "通过更新一个递归推理令牌来执行多次内部步骤，并使用丢弃和停止机制进行自我校正。", "result": "在Sudoku任务上达到了98.68%的数字准确率和36.30%的整体拼图准确率，展示了其无需符号规则或搜索即可完成复杂结构化推理的能力。", "conclusion": "该方法通过内部迭代步骤稳定了推论，并调整计算深度，展现了扩展到更大规模算法任务（如国际象棋）的潜力。"}}
{"id": "2512.12791", "pdf": "https://arxiv.org/pdf/2512.12791", "abs": "https://arxiv.org/abs/2512.12791", "authors": ["Sreemaee Akshathala", "Bassam Adnan", "Mahisha Ramesh", "Karthik Vaidhyanathan", "Basil Muhammed", "Kannan Parthasarathy"], "title": "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems", "categories": ["cs.MA", "cs.AI", "cs.SE"], "comment": null, "summary": "Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. These challenges emerged during our ongoing industry collaboration with MontyCloud Inc., when we deployed an agentic system in production. These limitations surfaced during deployment, highlighting practical gaps in the current evaluation methods and the need for a systematic assessment of agent behavior beyond task outcomes. Informed by these observations and established definitions of agentic systems, we propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.", "AI": {"tldr": "提出了一种评估代理AI系统的框架，该框架考虑了工具使用、记忆管理、与其他代理的协作以及与环境的有效互动。", "motivation": "现有评估方法无法完全捕捉到基于LLM的非确定性行为和运行时不确定性，这限制了对多代理系统性能的理解。", "method": "设计了一个包含四个支柱（模型、内存、工具和环境）的评估框架，并通过一个自主云操作用例验证该框架的有效性。", "result": "实验揭示了传统度量标准无法检测的行为偏差，表明新框架能更好地捕捉运行时不确定性。", "conclusion": "当前评价方法不足以全面衡量代理AI系统性能；新的评估框架提供了更深入的视角，并为未来研究奠定了基础。"}}
{"id": "2512.12790", "pdf": "https://arxiv.org/pdf/2512.12790", "abs": "https://arxiv.org/abs/2512.12790", "authors": ["Tiange Zhang", "Zhimeng Huang", "Xiandong Meng", "Kai Zhang", "Zhipin Deng", "Siwei Ma"], "title": "L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context", "categories": ["cs.CV"], "comment": "Accepted to Data Compression Conference (DCC) 2026 as an oral paper", "summary": "Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.", "AI": {"tldr": "提出了一种新的长时空间时间增强上下文(L-STEC)方法，以改进视频压缩。", "motivation": "现有的基于条件的框架虽然优于传统编码器，但依赖于前一帧特征来预测时空上下文，导致长期依赖关系丢失和细节损失等问题。", "method": "使用LSTM扩展参考链以捕捉长期依赖性，并通过融合多感受野网络中的空间信息来增强时空信息，从而更好地保留参照细节。", "result": "实验结果表明，与DCVC-TCM相比，L-STEC在PSNR上节省了37.01%的比特率，在MS-SSIM上节省了31.65%，并且优于VTM-17.0和DCVC-FM。", "conclusion": "L-STEC方法通过丰富上下文信息显著提高了压缩性能，成为新的最先进的视频编码技术。"}}
{"id": "2512.12787", "pdf": "https://arxiv.org/pdf/2512.12787", "abs": "https://arxiv.org/abs/2512.12787", "authors": ["Mohammad Abu-Shaira", "Weishi Shi"], "title": "Unveiling Statistical Significance of Online Regression over Multiple Datasets", "categories": ["cs.LG", "cs.AI"], "comment": "ef:2024 IEEE 7th International Conference on Multimedia Information Processing (MIPR 2024)", "summary": "Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.", "AI": {"tldr": "本文研究了在线回归模型的统计显著性测试方法，并使用Friedman检验及后续检验来比较多个模型在不同数据集上的表现。", "motivation": "虽然已有大量关于单个数据集中两种学习算法性能评估的技术研究，但跨多个数据集比较多种算法的研究相对较少。在线学习中确保统计显著性对验证持续学习过程至关重要，尤其是在快速收敛和及时应对概念漂移方面需要有效的策略。因此，本文致力于开发适用于动态演进数据的稳健统计方法。", "method": "本文采用了Friedman检验以及后续检验来比较多个在线回归模型在不同数据集上的表现，并使用了真实及合成的数据集进行五折交叉验证和种子平均以确保评估全面性。", "result": "测试结果通常确认了基线模型的表现与其单独报告一致，但也揭示了一些改进空间。一些统计检验结果显示某些前沿方法仍需改善。", "conclusion": "本文通过多种数据集的对比测试证明了在线回归模型的有效性和局限性，并强调了持续研究和发展更为有效的统计显著性评估的重要性。"}}
{"id": "2512.12785", "pdf": "https://arxiv.org/pdf/2512.12785", "abs": "https://arxiv.org/abs/2512.12785", "authors": ["Mohammad Abu Shaira", "Yunhe Feng", "Heng Fan", "Weishi Shi"], "title": "OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average", "categories": ["cs.LG", "cs.AI"], "comment": "ef:Expert Systems with Applications (Elsevier), 2025", "summary": "Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.", "AI": {"tldr": "提出了OLC-WA在线分类模型，能够自适应地处理概念漂移问题。", "motivation": "现实世界的数据集存在时间动态性，数据分布随时间变化。忽略这一现象会导致模型预测准确性下降。此外，在线模型中的超参数固定无法动态调整，进一步加剧了这一问题。", "method": "OLC-WA通过融合新的数据流和现有的基模型，并采用指数加权移动平均进行操作。内置的优化机制可以动态检测概念漂移、量化其影响并根据观察到的数据流特性调整模型。", "result": "在不同基准数据集上的严格实证评估表明，OLC-WA在静态环境中的表现与批量模型相当，在有概念漂移的情况下比领先的在线基线高出10-25%的准确率。", "conclusion": "该方法展示了其适应动态数据流的有效性。"}}
{"id": "2512.12777", "pdf": "https://arxiv.org/pdf/2512.12777", "abs": "https://arxiv.org/abs/2512.12777", "authors": ["Mosh Levy", "Zohar Elyoseph", "Shauli Ravfogel", "Yoav Goldberg"], "title": "State over Tokens: Characterizing the Role of Reasoning Tokens", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.", "AI": {"tldr": "该论文提出了State over Tokens (SoT) 概念框架，重新定义了推理标记为外部化计算状态。", "motivation": "现有大语言模型生成的推理令牌看似人类思考过程，但实际上并不真实反映其实际推理路径。为了弥补这一认知差距，作者引入了新的概念框架。", "method": "作者通过将推理标记视为外部化计算状态而非文字叙述来解释它们如何驱动正确推理而不作为忠实说明。", "result": "提出了SoT概念框架，强调了解码这些令牌作为状态的重要性。", "conclusion": "研究必须超越以文本形式解读推理令牌，并转向将其解码为模型状态的研究方法。"}}
{"id": "2512.12776", "pdf": "https://arxiv.org/pdf/2512.12776", "abs": "https://arxiv.org/abs/2512.12776", "authors": ["Haochong Chen", "Xincheng Cao", "Levent Guvenc", "Bilin Aksun-Guvenc"], "title": "High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Ensuring the safety of Vulnerable Road Users (VRUs) is a critical challenge in the development of advanced autonomous driving systems in smart cities. Among vulnerable road users, bicyclists present unique characteristics that make their safety both critical and also manageable. Vehicles often travel at significantly higher relative speeds when interacting with bicyclists as compared to their interactions with pedestrians which makes collision avoidance system design for bicyclist safety more challenging. Yet, bicyclist movements are generally more predictable and governed by clear traffic rules as compared to the sudden and sometimes erratic pedestrian motion, offering opportunities for model-based control strategies. To address bicyclist safety in complex traffic environments, this study proposes and develops a High Order Control Lyapunov Function High Order Control Barrier Function Quadratic Programming (HOCLF HOCBF QP) control framework. Through this framework, CLFs constraints guarantee system stability so that the vehicle can track its reference trajectory, whereas CBFs constraints ensure system safety by letting vehicle avoiding potential collisions region with surrounding obstacles. Then by solving a QP problem, an optimal control command that simultaneously satisfies stability and safety requirements can be calculated. Three key bicyclist crash scenarios recorded in the Fatality Analysis Reporting System (FARS) are recreated and used to comprehensively evaluate the proposed autonomous driving bicyclist safety control strategy in a simulation study. Simulation results demonstrate that the HOCLF HOCBF QP controller can help the vehicle perform robust, and collision-free maneuvers, highlighting its potential for improving bicyclist safety in complex traffic environments.", "AI": {"tldr": "本文提出了一种基于高阶控制Lyapunov函数和控制屏障函数的二次规划自主驾驶控制器，以提高复杂交通环境中自行车手的安全性。", "motivation": "在智能城市中开发高级自主驾驶系统时，确保易受伤害的道路使用者（包括骑自行车的人）的安全是一项关键挑战。骑自行车的人相对于车辆速度较高且运动更可预测，因此需要针对他们的安全设计碰撞避免系统。", "method": "本文提出了一种高阶控制Lyapunov函数与高阶控制屏障函数结合二次规划的自主驾驶控制器框架，通过CLF保证系统的稳定性并使车辆跟踪参考轨迹，而CBF则确保安全性以防止潜在的碰撞。最后通过求解QP问题计算出满足稳定性和安全性的最优控制命令。", "result": "仿真结果显示该HOCLF-HOCBF-QP控制器能够帮助车辆执行稳健且无碰撞的操作，突显了其在复杂交通环境中提高自行车手安全性的潜力。", "conclusion": "所提出的基于高阶控制Lyapunov函数和控制屏障函数的二次规划自主驾驶控制器框架能够在保障稳定性和安全性的同时实现对自行车手的安全防护。"}}
{"id": "2512.12774", "pdf": "https://arxiv.org/pdf/2512.12774", "abs": "https://arxiv.org/abs/2512.12774", "authors": ["Hao Wang", "Ashish Bastola", "Chaoyi Zhou", "Wenhui Zhu", "Xiwen Chen", "Xuanzhao Dong", "Siyu Huang", "Abolfazl Razi"], "title": "Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior", "categories": ["cs.CV"], "comment": null, "summary": "As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.", "AI": {"tldr": "提出了一种高效的二维高斯图像表示方法Fast-2DGS，用于生成高质量的视觉内容。", "motivation": "为了实现高效、可解释和编辑的图像表示，特别是在2D Gaussian Splatting（2DGS）中，现有方法需要复杂的优化过程且耗时较长。因此，论文旨在开发一种轻量级框架来解决这一问题。", "method": "提出了一种深度高斯先验网络，用于捕捉不同复杂度下二维高斯点的空间分布，并设计了一个属性回归网络以预测密集的高斯特性。该方法通过单一前向传递实现高质量重建，随后进行少量微调即可。", "result": "实验结果表明，所提架构能够在单次前向传递后完成高质量重构，仅需少量微调。同时显著降低了计算成本且不牺牲视觉质量。", "conclusion": "Fast-2DGS不仅提高了图像表示的质量和效率，还使得基于二维高斯点的方法更接近于工业部署的应用场景。"}}
{"id": "2512.12773", "pdf": "https://arxiv.org/pdf/2512.12773", "abs": "https://arxiv.org/abs/2512.12773", "authors": ["Reeteesha Roy"], "title": "Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles", "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.", "AI": {"tldr": "本论文主要研究如何通过适应性接口设计增强自动驾驶汽车中的用户体验，提高用户满意度和信任度。", "motivation": "随着自动驾驶汽车在现代交通系统中的发展与集成，优化用户界面以提升整体体验变得愈发重要。因此，了解用户需求和偏好对于技术的接受度及信任至关重要。", "method": "论文探讨了通过人机交互（HCI）原则实现个性化接口设计的方法，包括适应性设计、多模式互动和反馈机制等策略，并强调透明性和用户控制的重要性。", "result": "研究结果表明，通过实施上述HCI策略，可以显著提高用户参与度及满意度。同时，允许用户自定义或修改其体验有助于增强他们对自动驾驶系统的信任感。", "conclusion": "论文指出了HCI在开发自动驾驶汽车中的重要作用，并强调了满足安全性和安全性标准的同时考虑多样化需求的必要性。"}}
{"id": "2512.12772", "pdf": "https://arxiv.org/pdf/2512.12772", "abs": "https://arxiv.org/abs/2512.12772", "authors": ["Jianghan Chao", "Jianzhang Gao", "Wenhui Tan", "Yuchong Sun", "Ruihua Song", "Liyun Ru"], "title": "JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.", "AI": {"tldr": "本文提出了JointAVBench，这是一个用于评估Omni-Large Language Models (LLMs) 在音频和视觉联合推理能力的基准。", "motivation": "现有数据集在评测多模态依赖性、多样化的音频信息类型以及不同场景跨度方面存在不足。为了填补这一空白，本文引入了一个新的基准测试JointAVBench。", "method": "通过利用先进的视觉LLMs, 音频LLMs和通用型LLMs的自动化管线来生成需要严格联合视听理解的问题和答案，创建了涵盖五个认知维度、四种音频信息类型（言语、声音事件、音乐及人声特征）以及三种场景跨度的新基准。", "result": "实验表明，即使是最佳的表现Omni-LLM，在JointAVBench上也只能达到62.6%的平均准确率，这高于单模态基线模型但依然有很大的提升空间，特别是在跨场景推理方面。", "conclusion": "提出并验证了JointAVBench作为评估多模态语言模型联合视听理解能力的有效工具。"}}
{"id": "2512.12769", "pdf": "https://arxiv.org/pdf/2512.12769", "abs": "https://arxiv.org/abs/2512.12769", "authors": ["Mohammad Jalili Torkamani", "Israt Zarin"], "title": "Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)", "categories": ["cs.SD", "cs.AI"], "comment": "preprint, 6 pages, 7 figures, 1 table", "summary": "Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.", "AI": {"tldr": "本文提出了一种自适应语音控制IoT设备的边缘云计算解决方案ASTA，该方案根据实时系统参数动态分配计算任务。", "motivation": "解决基于语音交互的IoT设备在云处理和边缘处理之间的性能与资源利用率平衡问题。", "method": "结合自动语音识别、轻量级离线语言模型推理以及云端大型语言模型处理，并通过实时监控系统的CPU负载、温度及网络延迟等指标来动态选择计算路径，同时设有命令验证和修复机制以确保指令执行的正确性。", "result": "实验结果表明ASTA能够成功路由所有输入命令进行执行，在线与离线推断之间保持平衡分配。系统实现62.5%的ASR准确率，并且只有47.5%的输入可以直接生成可执行命令，进一步突显了修复机制的重要性。", "conclusion": "自适应边缘云计算架构是提高语音控制IoT系统的弹性和资源利用率的有效途径"}}
{"id": "2512.12768", "pdf": "https://arxiv.org/pdf/2512.12768", "abs": "https://arxiv.org/abs/2512.12768", "authors": ["Tianjiao Yu", "Xinzhuo Li", "Yifan Shen", "Yuanzhe Liu", "Ismini Lourentzou"], "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.", "AI": {"tldr": "CoRe3D提出了一个统一的三维理解和生成推理框架，通过语言指导三维内容形成，并在局部一致性和与语言描述的一致性方面表现出色。", "motivation": "近年来的大规模多模态模型表明，明确的推理机制对于提高模型可靠性和跨模态对齐至关重要。然而，这些方法在3D领域的发展仍不充分。", "method": "CoRe3D设计了一个基于空间定位的推理表示，将3D潜在空间分解为局部区域，并通过结构化空间推理与语义链式思考推断紧密耦合来实现三维理解和生成。", "result": "该模型能够产生具有强局部一致性和忠实语言描述对齐的3D输出。", "conclusion": "CoRe3D证明了在三维智能中引入协作推理作为基础框架的有效性。"}}
{"id": "2512.12762", "pdf": "https://arxiv.org/pdf/2512.12762", "abs": "https://arxiv.org/abs/2512.12762", "authors": ["Incheol Baek", "Hyungbin Kim", "Minseo Kim", "Yon Dohn Chung"], "title": "Federated Learning with Feedback Alignment", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead. Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.", "AI": {"tldr": "介绍了一种新的联邦学习框架FLFA，通过反馈对齐技术缓解数据异质性问题", "motivation": "解决联邦学习中的数据异质性导致的局部漂移问题，提高全局模型收敛速度", "method": "在本地训练中使用全局模型权重作为共享反馈矩阵进行反向传播，减少计算和通信开销", "result": "理论分析表明FLFA能有效缓解局部漂移，实验证明其提高了其他联邦学习方法的效果", "conclusion": "提出的方法FLFA能够有效解决数据异质性问题，提高联邦学习效率"}}
