{"id": "2512.12777", "pdf": "https://arxiv.org/pdf/2512.12777", "abs": "https://arxiv.org/abs/2512.12777", "authors": ["Mosh Levy", "Zohar Elyoseph", "Shauli Ravfogel", "Yoav Goldberg"], "title": "State over Tokens: Characterizing the Role of Reasoning Tokens", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.", "AI": {"tldr": "该论文提出了State over Tokens（SoT）的概念框架，用以解释大型语言模型生成的推理令牌的本质和作用。", "motivation": "现有的研究将大型语言模型生成的推理令牌视为其内部思考过程的文字描述，但实证研究表明这些令牌并不真正反映模型的实际思考过程。因此，需要一个新框架来重新定义这些令牌的角色及其功能。", "method": "通过引入State over Tokens（SoT）的概念框架，该论文认为推理令牌应该被看作是计算状态的外部表示，而不是简单的语言叙述。", "result": "SoT概念框架揭示了为什么生成的推理令牌可以引导正确的推理而不必作为文本阅读时准确反映模型的思考过程。这为理解大型语言模型的内部机制提出了新的研究方向。", "conclusion": "为了真正理解大型语言模型的工作原理，未来的研究应该超越单纯地将推理令牌当作文字来解读，转而探索如何将其解码成计算状态。"}}
{"id": "2512.12776", "pdf": "https://arxiv.org/pdf/2512.12776", "abs": "https://arxiv.org/abs/2512.12776", "authors": ["Haochong Chen", "Xincheng Cao", "Levent Guvenc", "Bilin Aksun-Guvenc"], "title": "High Order Control Lyapunov Function - Control Barrier Function - Quadratic Programming Based Autonomous Driving Controller for Bicyclist Safety", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Ensuring the safety of Vulnerable Road Users (VRUs) is a critical challenge in the development of advanced autonomous driving systems in smart cities. Among vulnerable road users, bicyclists present unique characteristics that make their safety both critical and also manageable. Vehicles often travel at significantly higher relative speeds when interacting with bicyclists as compared to their interactions with pedestrians which makes collision avoidance system design for bicyclist safety more challenging. Yet, bicyclist movements are generally more predictable and governed by clear traffic rules as compared to the sudden and sometimes erratic pedestrian motion, offering opportunities for model-based control strategies. To address bicyclist safety in complex traffic environments, this study proposes and develops a High Order Control Lyapunov Function High Order Control Barrier Function Quadratic Programming (HOCLF HOCBF QP) control framework. Through this framework, CLFs constraints guarantee system stability so that the vehicle can track its reference trajectory, whereas CBFs constraints ensure system safety by letting vehicle avoiding potential collisions region with surrounding obstacles. Then by solving a QP problem, an optimal control command that simultaneously satisfies stability and safety requirements can be calculated. Three key bicyclist crash scenarios recorded in the Fatality Analysis Reporting System (FARS) are recreated and used to comprehensively evaluate the proposed autonomous driving bicyclist safety control strategy in a simulation study. Simulation results demonstrate that the HOCLF HOCBF QP controller can help the vehicle perform robust, and collision-free maneuvers, highlighting its potential for improving bicyclist safety in complex traffic environments.", "AI": {"tldr": "开发了一种基于高阶控制Lyapunov函数、高阶控制屏障函数和二次规划的自动驾驶控制器，以提高复杂交通环境中骑自行车者的安全性。", "motivation": "确保脆弱道路使用者的安全是智能城市中高级自动驾驶系统发展的重要挑战。骑自行车者具有独特的运动特征，既关键又可控。车辆与骑行者的相对速度较高，使得避免碰撞的设计更为复杂，但其行为更可预测，为基于模型的控制策略提供了机会。", "method": "通过结合高阶控制Lyapunov函数（确保系统稳定性）和高阶控制屏障函数（保证系统安全），并通过二次规划计算同时满足稳定性和安全性要求的最优控制指令。使用FARS记录的三个关键骑自行车者碰撞场景进行仿真评估。", "result": "仿真结果表明，所提出的HOCLF HOCBF QP控制器能够使车辆执行稳健且无碰撞的操作，突显了其在复杂交通环境中提高骑行者安全性的潜力。", "conclusion": "该方法能够在保证稳定性和安全性的同时有效避免与周围障碍物的潜在碰撞，并能提升骑自行车者的交通安全水平。"}}
{"id": "2512.12774", "pdf": "https://arxiv.org/pdf/2512.12774", "abs": "https://arxiv.org/abs/2512.12774", "authors": ["Hao Wang", "Ashish Bastola", "Chaoyi Zhou", "Wenhui Zhu", "Xiwen Chen", "Xuanzhao Dong", "Siyu Huang", "Abolfazl Razi"], "title": "Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior", "categories": ["cs.CV"], "comment": null, "summary": "As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.", "AI": {"tldr": "提出了一种轻量级框架Fast-2DGS，用于高效的高斯图像表示。", "motivation": "为了提高2D高斯点图生成模型的效率、可解释性和编辑性，并解决现有方法在复杂度不敏感和收敛速度慢的问题。", "method": "引入深度高斯先验网络捕捉不同复杂性的高斯原语的空间分布，以及属性回归网络预测密集的高斯特性。此框架能在单次前向传播后通过最小调整获得高质量重建。", "result": "实验证明该架构在不牺牲视觉质量的情况下显著减少了计算成本，并实现了一步式高质量重建和快速收敛。", "conclusion": "提出的Fast-2DGS框架使2D高斯点图生成更加高效，更接近工业部署。"}}
{"id": "2512.12773", "pdf": "https://arxiv.org/pdf/2512.12773", "abs": "https://arxiv.org/abs/2512.12773", "authors": ["Reeteesha Roy"], "title": "Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles", "categories": ["cs.HC", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.", "AI": {"tldr": "通过自适应设计提升自动驾驶汽车用户体验的研究", "motivation": "随着自动驾驶车辆在现代交通系统中的发展和集成，定制用户界面以优化整体用户体验变得尤为重要。该研究旨在探讨HCI原则如何用于个性化界面的设计以提高安全性、安全性和易用性，并增加用户的参与度和满意度。", "method": "论文讨论了自适应设计、多模态交互以及反馈机制等HCI策略的应用方式，强调透明化与用户控制的重要性，以此来提升用户体验并增强对自动驾驶系统的信任。", "result": "通过上述方法的实施，可以显著提高用户在使用自主驾驶车辆时的参与度和满意度。", "conclusion": "本研究指出了在开发自动驾驶汽车过程中，人机交互（HCI）所扮演的重要角色，并且强调了为了满足安全性和安全性标准所需的各种需求。"}}
{"id": "2512.12772", "pdf": "https://arxiv.org/pdf/2512.12772", "abs": "https://arxiv.org/abs/2512.12772", "authors": ["Jianghan Chao", "Jianzhang Gao", "Wenhui Tan", "Yuchong Sun", "Ruihua Song", "Liyun Ru"], "title": "JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.", "AI": {"tldr": "本文提出了JointAVBench，一个用于评估处理视听信息的Omni-LLM的新基准。", "motivation": "为了全面评价可以处理视觉和听觉多模态信息的Omni-LLMs，现有数据集在多模态依赖性、多样化的音频信息类型以及不同的场景跨度方面存在不足。因此需要一个更严格的视听关联测试。", "method": "JointAVBench涵盖五种认知维度，四种音频信息类型和三种场景跨度，并利用最先进的视觉LLM、音频LLM和通用型LLM自动合成需要联合视听理解的问题和答案。", "result": "评估结果显示，即使表现最好的Omni-LLMs在新基准上的平均准确率也只有62.6%，这表明当前模型仍然存在显著的改进空间，特别是在跨场景推理方面。", "conclusion": "JointAVBench填补了现有视听理解测试中的不足，并揭示出最先进的Omni-LLM仍需大幅提高其性能。"}}
{"id": "2512.12769", "pdf": "https://arxiv.org/pdf/2512.12769", "abs": "https://arxiv.org/abs/2512.12769", "authors": ["Mohammad Jalili Torkamani", "Israt Zarin"], "title": "Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)", "categories": ["cs.SD", "cs.AI"], "comment": "preprint, 6 pages, 7 figures, 1 table", "summary": "Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.", "AI": {"tldr": "论文提出了ASTA系统，它可以在边缘设备和云端之间动态分配语音命令处理任务，以平衡性能与资源使用。", "motivation": "由于云基解决方案虽然提供强大的语言理解能力，但存在延迟、依赖网络连接及隐私问题；而基于边缘的方案可以提供低延迟和增强的隐私保护，但由于计算限制也存在局限性。因此，需要一种既能处理复杂语音命令又能保证性能与资源使用的自适应方法。", "method": "ASTA系统结合了设备上的自动语音识别和轻量级离线语言模型推理，并使用实时系统的度量如CPU负载、设备温度和网络延迟来指导云基LLM处理。根据这些指标，一种度量感知路由机制选择运行时的推断路径；同时一个基于规则的命令验证与修复组件保证了端到端命令执行的成功。", "result": "实验结果表明ASTA能够成功地为所有输入指令分配执行任务，在在线和离线推理之间保持平衡。系统达到了62.5％的ASR准确率，而无需修复机制即可生成可执行命令的比例仅为47.5%，显示出修复机制在提高鲁棒性方面的重要性。", "conclusion": "自适应边缘-云协调是为具有资源意识和稳健性的语音控制物联网设备提供可行方案的一种方法。"}}
{"id": "2512.12768", "pdf": "https://arxiv.org/pdf/2512.12768", "abs": "https://arxiv.org/abs/2512.12768", "authors": ["Tianjiao Yu", "Xinzhuo Li", "Yifan Shen", "Yuanzhe Liu", "Ismini Lourentzou"], "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.", "AI": {"tldr": "CoRe3D提出了一种统一的三维理解和生成推理框架，通过语言意图直接指导低层次的三维内容形成。", "motivation": "当前大型多模态模型显示显式推理机制对提高模型可靠性和跨模态一致性至关重要。然而在三维领域的扩展仍有待开发。", "method": "CoRe3D引入了基于空间定位的推理表示，将三维潜在空间分解为局部区域，实现几何组成的和程序化的推理。通过结合语义链思维推断与结构化空间推理，模型能够生成高度一致且忠实于语言描述的三维输出。", "result": "该方法提高了三维输出的局部一致性和对语言指令的遵循程度。", "conclusion": "CoRe3D为三维理解和生成提供了一种新颖的方法，证明了显式推理机制在三维领域的有效性。"}}
{"id": "2512.12762", "pdf": "https://arxiv.org/pdf/2512.12762", "abs": "https://arxiv.org/abs/2512.12762", "authors": ["Incheol Baek", "Hyungbin Kim", "Minseo Kim", "Yon Dohn Chung"], "title": "Federated Learning with Feedback Alignment", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead. Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.", "AI": {"tldr": "提出了一种新的联邦学习框架FLFA，通过反馈对齐来解决数据异质性问题", "motivation": "解决联邦学习中的数据异质性导致的局部漂移问题，提高全局模型收敛速度", "method": "使用全球模型权重作为共享反馈矩阵，在本地训练的反向传播过程中进行反馈对齐", "result": "理论分析和实证评估表明FLFA可以减少局部漂移并增强其他FL方法的效果", "conclusion": "FLFA是一种有效的解决方案，它可以在不增加计算成本和通信开销的情况下改善联邦学习的性能"}}
{"id": "2512.12760", "pdf": "https://arxiv.org/pdf/2512.12760", "abs": "https://arxiv.org/abs/2512.12760", "authors": ["Sina Jani", "Arman Heidari", "Amirmohammad Anvari", "Zahra Rahimi"], "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "18 pages, 7 figures, 3 tables", "summary": "The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.", "AI": {"tldr": "该论文提出了一种利用机器学习的智能科学文献探索系统（ISLE），旨在帮助科研人员更好地发现、理解和解读相关文献。", "motivation": "由于科学研究出版的速度加快，科研人员在寻找相关文献时面临诸多挑战。传统的关键词搜索系统缺乏语义理解能力，而现有的AI工具通常专注于单一任务如检索、聚类或计量可视化分析。因此，该论文提出了一种整合多种方法的集成系统。", "method": "该系统结合大规模数据获取、混合检索、语义主题建模以及异构知识图谱构建。通过将arXiv全文数据与OpenAlex元数据合并生成全面的知识库，并采用BM25词典搜索和基于嵌入的语义搜索融合架构，利用BERTopic或非负矩阵分解进行主题建模，并统一论文、作者、机构等信息到一个可解释的知识图谱中。", "result": "通过多种查询评估表明该系统在检索相关性、话题一致性及可理解性方面有所改善。此外，ISLE为AI辅助的科学发现提供了一个灵活的基础框架。", "conclusion": "ISLE提出了一个综合性的科学文献探索解决方案，其多层探索环境能够揭示与查询相关的出版物及其概念和关系网络。该系统不仅提高了检索的相关性和话题的一致性，而且增强了知识的理解能力。"}}
{"id": "2512.12756", "pdf": "https://arxiv.org/pdf/2512.12756", "abs": "https://arxiv.org/abs/2512.12756", "authors": ["Yue Jiang", "Dingkang Yang", "Minghao Han", "Jinghang Han", "Zizhi Chen", "Yizhou Liu", "Mingcheng Li", "Peng Zhai", "Lihua Zhang"], "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning", "categories": ["cs.CV"], "comment": "The omni-modal benchmark report from Fysics AI", "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.", "AI": {"tldr": "介绍FysicsWorld，一个用于评估全模态理解、生成和推理的统一基准。", "motivation": "现有多模态大语言模型及其基准测试范围有限，交互局限于文本中心输出。为解决这些问题，提出了一种支持跨图像、视频、音频和文本双向输入输出的新基准。", "method": "FysicsWorld包含16个主要任务和3,268个样本，并提出交叉模态互补筛查策略，以生成用于口语互动和依赖于融合的跨模态推理的数据。", "result": "通过评估超过30种最先进的模型，包括多模态语言模型、特定模式模型等，揭示了它们在理解、生成和推理方面的性能差异和限制。", "conclusion": "FysicsWorld为评估下一代全模态架构建立了统一的基础和强基准。"}}
{"id": "2512.12751", "pdf": "https://arxiv.org/pdf/2512.12751", "abs": "https://arxiv.org/abs/2512.12751", "authors": ["Zhenya Yang", "Zhe Liu", "Yuxiang Lu", "Liping Hou", "Chenxuan Miao", "Siyi Peng", "Bailan Feng", "Xiang Bai", "Hengshuang Zhao"], "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation", "categories": ["cs.CV"], "comment": "The project page is available at https://huster-yzy.github.io/geniedrive_project_page/", "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.", "AI": {"tldr": "GenieDrive提出了一种基于4D占用引导视频生成的物理感知驾驶世界模型，改进了驾驶行为到视频映射的学习效率和一致性。", "motivation": "当前方法直接将驾驶动作映射为视频导致学习困难及物理不一致问题。因此需要一种新的框架来提高学习效率并确保输出的一致性。", "method": "GenieDrive通过生成4D占用作为基础，采用VAE压缩高分辨率的占用信息，并引入Mutual Control Attention准确建模控制对占用演变的影响。此外还采用了Normalized Multi-View Attention以产生多视角驾驶视频。", "result": "相比之前方法，GenieDrive在预测mIoU上有7.2%的提升，在41 FPS的速度下仅使用3.47 M参数；FVD降低了20.7%，显著提升了视频质量。", "conclusion": "实验表明，GenieDrive能够生成高度可控、多视角一致且物理感知准确的驾驶视频。"}}
{"id": "2512.12736", "pdf": "https://arxiv.org/pdf/2512.12736", "abs": "https://arxiv.org/abs/2512.12736", "authors": ["Syeda Zunaira Ahmed", "Hejab Tahira Beg", "Maryam Khalid"], "title": "Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks", "categories": ["cs.AI", "cs.MM", "eess.IV"], "comment": "11 pages, 5 figures", "summary": "Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments. This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet. Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.", "AI": {"tldr": "提出了一种基于人口统计学的机器学习框架，用于个性化视频流媒体网络中的QoE预测。", "motivation": "现有的QoE预测方法依赖于有限的数据集，并假设用户感知是统一的，这限制了它们在异构现实环境中的适用性。为此，本文提出了一个增强的基于人口统计学的方法来提高QoE预测准确性。", "method": "引入了一种行为上真实的基于人口统计的数据扩充策略，该方法通过模拟用户的敏感度差异将小规模数据集扩展六倍。使用扩增后的数据集评估了经典机器学习模型和高级深度学习架构，包括注意力机制的MLP和TabNet。", "result": "实验结果表明，在RMSE、MAE和R指标上，所提出的预测方法显著优于基准模型，特别是在精度方面，TabNet表现最佳。", "conclusion": "人口统计学增强策略提高了QoE预测的鲁棒性，并为个性化QoE感知智能在5G视频流媒体网络中的应用提供了可扩展的方向。"}}
{"id": "2512.12722", "pdf": "https://arxiv.org/pdf/2512.12722", "abs": "https://arxiv.org/abs/2512.12722", "authors": ["Tarik Viehmann", "Daniel Swoboda", "Samridhi Kalra", "Himanshu Grover", "Gerhard Lakemeyer"], "title": "Making Robots Play by the Rules: The ROS 2 CLIPS-Executive", "categories": ["cs.RO"], "comment": null, "summary": "CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.", "AI": {"tldr": "本文介绍了将CLIPS集成到ROS生态系统中的方法，以及通过PDDL进行规划框架整合的灵活性。", "motivation": "利用规则驱动的方法来协调自主机器人的复杂任务，以提高机器人在遵守规定下的操作能力。", "method": "将CLIPS编程语言集成进ROS环境，并展示如何使用PDDL实现规划框架的结合。", "result": "成功地实现了CLIPS与ROS的整合，并展示了其通过PDDL进行灵活规划的能力。", "conclusion": "该方法为机器人领域提供了一种强大的工具，以支持更复杂的协调和决策任务。"}}
{"id": "2512.12718", "pdf": "https://arxiv.org/pdf/2512.12718", "abs": "https://arxiv.org/abs/2512.12718", "authors": ["Sehyun Kim", "Hye Jun Lee", "Jiwoo Lee", "Changgyun Kim", "Taemin Lee"], "title": "Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images", "categories": ["cs.CV"], "comment": null, "summary": "The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.", "AI": {"tldr": "该论文提出了一种基于深度图像的三维人体重建方法，用于自动估计脊柱中心线并评估身体姿态。", "motivation": "现有的多图基体重构方法需要昂贵设备和复杂程序，而单图像方法在遮挡和视角限制下难以准确估算内部结构如脊柱中心线。因此该研究旨在弥补这些不足。", "method": "通过整合四个方向的深度图像来重建三维人体模型，并自动估计脊柱中心线；采用层次匹配进行全局与精细登记以恢复噪声和遮挡，使用自适应顶点减少保持网格分辨率和形状可靠性。", "result": "该方法无需依赖训练数据或复杂神经网络模型即可实现高精度的三维脊柱注册估算，验证表明改进了匹配质量。", "conclusion": "所提方法成功实现了无训练数据依赖且稳定的3D脊柱中心线估计，并提升了重建效果。"}}
{"id": "2512.12717", "pdf": "https://arxiv.org/pdf/2512.12717", "abs": "https://arxiv.org/abs/2512.12717", "authors": ["Mattia Catellani", "Marta Gabbi", "Lorenzo Sabattini"], "title": "HMPCC: Human-Aware Model Predictive Coverage Control", "categories": ["cs.RO"], "comment": null, "summary": "We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.", "AI": {"tldr": "本文提出了一种基于模型预测控制的机器人覆盖策略，通过融入人类轨迹预测来提高与非合作代理人的协调性。", "motivation": "传统覆盖率策略往往依赖于简化的假设，难以适应实际场景中的动态环境和涉及人类的因素。因此，需要一种能够应对未知、复杂且包含人类行为的环境的方法。", "method": "提出了一种基于模型预测控制的人类感知覆盖框架（HMPCC），该方法通过将人类运动预测融入规划过程来实现机器人团队的安全操作与协作。", "result": "实验结果显示，结合了人类轨迹预测的模型能够更高效和灵活地进行环境覆盖，并且提高了机器人和人类之间的协调性。", "conclusion": "通过引入人类行为预测，提出的HMPCC框架在未知环境中实现了更加有效的覆盖率控制。这种方法为实际应用中的人机协作提供了新的解决方案。"}}
{"id": "2512.12713", "pdf": "https://arxiv.org/pdf/2512.12713", "abs": "https://arxiv.org/abs/2512.12713", "authors": ["Yiyang Jia", "Chengxu Zhou"], "title": "Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning. SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.", "AI": {"tldr": "本文提出了自激励生长神经网络（SMGrNN），该控制器的拓扑结构通过局部结构性可塑性模块在线演化。", "motivation": "深度强化学习中的控制策略通常使用固定容量的多层感知机训练，缺乏结构可塑性和全局误差信号。引入SMGrNN以解决这些问题，并提高适应性架构性能。", "method": "SMGrNN包含一个局部结构性可塑性模块（SPM），该模块监测神经元激活和边缘权重更新统计信息并触发插入和修剪操作。网络容量在学习过程中被自动调节，而不需要手动调整架构。", "result": "与多层感知机基准相比，SMGrNN实现了相似或更高的回报，并具有更小的方差和任务适当的网络规模。隔离研究证明了结构可塑性对奖励稳定性的改进作用。", "conclusion": "通过局部且模块化设计的SPM，使得在未来可以集成赫布可塑性和脉冲时间依赖性可塑性模块，从而支持人工神经元和尖峰神经元的实现并驱动由本地规则。"}}
{"id": "2512.12706", "pdf": "https://arxiv.org/pdf/2512.12706", "abs": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.", "AI": {"tldr": "提出SMART框架，结合代码覆盖率和游戏意图验证游戏更新测试。", "motivation": "现有自动化游戏测试方法存在代码覆盖与游戏上下文理解之间的矛盾，因此需要一种新的解决方案来平衡两者。", "method": "利用大型语言模型解释抽象语法树差异并提取功能意图，构建基于强化学习的混合奖励机制，以同时实现游戏目标和探索修改后的代码分支。", "result": "SMART在Overcooked和Minecraft环境中表现优异，实现了超过94%的修改后代码覆盖率，并保持了98%的任务完成率。", "conclusion": "SMART框架成功地结合了代码结构验证与功能验证，提供了高效的自动化游戏更新测试方法。"}}
{"id": "2512.12703", "pdf": "https://arxiv.org/pdf/2512.12703", "abs": "https://arxiv.org/abs/2512.12703", "authors": ["Boyuan Li", "Sipeng Zheng", "Bin Cao", "Ruihua Song", "Zongqing Lu"], "title": "Robust Motion Generation using Part-level Reliable Data from Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance. To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as \"credible\". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts. In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/", "AI": {"tldr": "本文提出了一种利用视频中可见的人体部位数据来生成可靠人体运动的方法。", "motivation": "由于网络视频中存在部分不可见的问题，直接使用这些数据会导致模型性能下降。为此，作者希望通过处理这些可靠的局部数据提升运动生成的质量和多样性。", "method": "首先将人类身体划分为五个部分，并检测每一帧中的可见部位；接着通过提出的局部感知变分自动编码器对可见部位进行编码；最后利用一个稳健的局部掩码生成模型预测缺失的部分，忽略噪音部分。", "result": "实验结果表明该方法在干净和噪声数据集上均优于基线方法，在动作质量、语义一致性和多样性方面有显著提高。", "conclusion": "本文提出的方法通过有效处理视频中的可见部位信息，解决了运动生成中由于不可见部分导致的数据质量问题，并提供了新的挑战性基准K700-M用于评估。"}}
{"id": "2512.12701", "pdf": "https://arxiv.org/pdf/2512.12701", "abs": "https://arxiv.org/abs/2512.12701", "authors": ["Xue Li", "Xiaonan Song", "Henry Hu"], "title": "Efficient Vision-Language Reasoning via Adaptive Token Pruning", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results", "summary": "Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.", "AI": {"tldr": "介绍了一种名为自适应令牌修剪（ATP）的动态推理机制，该机制通过保留最相关的视觉和语言令牌来提高视觉-语言模型在实际部署中的效率。", "motivation": "现有视觉-语言模型由于计算需求高且不均匀处理所有令牌而导致难以实现有效部署。为了解决这个问题，作者提出了自适应令牌修剪（ATP），一种动态机制以减少不必要的计算并保持准确性。", "method": "ATP通过在视觉和语言界面分配一个结合ViT CLS注意力和CLIP文本-图像相似度的混合重要性评分来操作，从而保留最相关的顶部K个令牌。这种方法不修改模型骨干，且与流行的BLIP-2、LLaVA和Flamingo等架构兼容。", "result": "ATP在VQAv2、GQA和COCO数据集上的初步评估显示，在减少40%的FLOPs同时实现了约1.5倍的速度提升，并且准确度损失小于1%，表明其有效性和效率。此外，定性分析还证明了它有助于增强视觉定位能力。", "conclusion": "通过自适应令牌修剪（ATP），模型在保持准确性的同时提高了推理速度和计算效率。这不仅适用于资源受限的边缘设备部署，还有助于提高模型鲁棒性，表明高效性和可靠性并不矛盾。"}}
{"id": "2512.12694", "pdf": "https://arxiv.org/pdf/2512.12694", "abs": "https://arxiv.org/abs/2512.12694", "authors": ["Anthony Mudet", "Souhail Bakkali"], "title": "Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering", "categories": ["cs.DL", "cs.CV"], "comment": "Preprint", "summary": "Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.", "AI": {"tldr": "开发了一种用于处理历史文献中问答任务的混合检索增强生成管道。", "motivation": "大规模数字化项目解锁了大量历史报纸集，但有效计算访问受到OCR错误、多语言正字法变化和时间性语言漂移的影响。为此设计一种专门适用于此类问题的问答系统。", "method": "该方法包括：(i) 使用Reciprocal Rank Fusion进行语义查询扩展和多查询融合以增强检索鲁棒性；(ii) 设计生成提示来强制使用检索证据并明确在证据不足时拒绝回答；(iii) 提供一种模块化架构以便系统评估组件性能。进行了针对实体识别和嵌入模型选择的消融研究。", "result": "该管道能够为有充分支持的问题产生忠实答案，同时正确地拒绝无解问题。混合检索策略提高了召回稳定性，并且从RRF中受益于平滑不同查询形式下的性能波动。", "conclusion": "提出的方法显著改善了处理历史文献问答任务的性能，特别是在证据不足时能有效避免错误回答。"}}
{"id": "2512.12693", "pdf": "https://arxiv.org/pdf/2512.12693", "abs": "https://arxiv.org/abs/2512.12693", "authors": ["Sumantrak Mukherjee", "Serafima Lebedeva", "Valentin Margraf", "Jonas Hanselle", "Kanta Yamaoka", "Viktor Bengs", "Stefan Konigorski", "Eyke Hüllermeier", "Sebastian Josef Vollmer"], "title": "Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 9 figures, preprint", "summary": "We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.", "AI": {"tldr": "提出了一种新颖的贝叶斯框架，用于在多任务上下文中的多臂赌博机设置中进行高效探索。", "motivation": "通过共享结构提高多任务上下文中的多臂赌博机设置的有效性，从而减少探索成本并优化学习过程。", "method": "利用粒子近似的方法表示任务和奖励的联合分布，并发现隐藏变量之间的依赖关系，以实现个性化推理和全局知识整合。", "result": "实验证明了该方法在模型偏差或复杂潜在异质性的设定下优于基线算法。", "conclusion": "新框架可以有效地利用结构不确定性与用户特定不确定性，在多任务设置中实现了更好的性能。"}}
{"id": "2512.12692", "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.", "AI": {"tldr": "WebOperator是一个在网页环境中自主代理的树搜索框架，旨在通过考虑长期后果和探索替代路径来提高行动的前瞻性。", "motivation": "现有的基于LLM的智能体通常以贪婪的方式操作，在选择动作时仅依赖当前观察而忽视了长期影响或替代路径。这种做法导致他们在处理不可逆动作以及在部分可观测环境中的错误回溯方面存在困难。因此，开发一种能够进行安全回溯和策略性探索的方法是必要的。", "method": "WebOperator结合了最佳优先搜索策略与强大的回溯机制，并生成多种推理上下文的动作候选者以确保多样性和鲁棒性，在执行前筛选无效动作并合并语义等价的动作。", "result": "在实验中，WebOperator展示了其优越性能：在WebArena上实现了54.6%的领先成功率。", "conclusion": "通过集成策略性的远见和安全执行，WebOperator显著提高了自主代理在网页环境中的成功表现。"}}
{"id": "2512.12690", "pdf": "https://arxiv.org/pdf/2512.12690", "abs": "https://arxiv.org/abs/2512.12690", "authors": ["Yongcan Yu", "Lingxiao He", "Shuo Lu", "Lijun Sheng", "Yinuo Xu", "Yanbo Wang", "Kuangpu Guo", "Jianjie Cheng", "Meng Wang", "Qianlong Xie", "Xingxing Wang", "Dapeng Hu", "Jian Liang"], "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.", "AI": {"tldr": "重新评估监督微调在视觉语言模型推理中的作用。", "motivation": "针对强化学习主导的信念，即监督微调无法提升模型推理能力甚至可能产生负面影响，通过系统且受控对比实验来审视此观点的合理性。", "method": "采用相同的数据源，比较监督微调和强化学习在视觉语言模型推理中的效果，并探讨影响两者相对有效性的因素包括模型容量、数据规模及分布情况。", "result": "发现监督微调对于较弱模型更具优势，具备更好的数据效率以及跨模态泛化能力；揭示了强化学习中奖励与准确率不一致的问题。", "conclusion": "挑战“仅依靠强化学习”的观点，强调监督微调的作用可能被低估，支持在后训练流程中将两者作为互补组件使用。"}}
{"id": "2512.12688", "pdf": "https://arxiv.org/pdf/2512.12688", "abs": "https://arxiv.org/abs/2512.12688", "authors": ["Dongseok Kim", "Hyoungsun Choi", "Mohamed Jismy Aashik Rasool", "Gisung Oh"], "title": "Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "24 pages", "summary": "Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.", "AI": {"tldr": "本文研究了通过固定Transformer模型并改变提示来实现的不同行为的函数集合，提出了将提示视为外部注入程序的方法，并证明单个固定的模型可以通过仅改变提示来近似广泛的预期行为。", "motivation": "当前对提示的研究主要基于经验性的方法和直觉，缺乏理论上的解释。本文旨在提供一种理论框架，以理解固定权重下通过改变提示实现不同行为的现象。", "method": "将Transformer视为执行者并保持其不变，通过改变输入的提示来研究不同的功能集合。提出了一个简化模型，将提示视作外部注入程序，并展示了注意力机制如何选择性路由从提示内存中获取信息，FFN层根据检索到的信息片段进行局部运算，而深度堆叠则组合这些局部更新以实现多步骤计算。", "result": "证明了单一固定的Transformer模型可以通过改变提示来近似广泛的预期行为。该框架为研究在给定约束条件下提示的有效性以及结构限制提供了理论基础。", "conclusion": "本文提出的理论框架提供了一种新的视角，用以理解固定权重下的提示工程，并为进一步的研究提供了起点，包括如何优化提示以实现特定的目标功能和如何解决与长度/精度相关的约束问题。"}}
{"id": "2512.12686", "pdf": "https://arxiv.org/pdf/2512.12686", "abs": "https://arxiv.org/abs/2512.12686", "authors": ["Samarth Sarin", "Lovepreet Singh", "Bhaskarjit Sarmah", "Dhagash Mehta"], "title": "Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI", "categories": ["cs.AI", "cs.CL"], "comment": "Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India", "summary": "Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.", "AI": {"tldr": "Memoria是一种可扩展的代理记忆框架，用于增强基于大型语言模型（LLM）的对话系统。", "motivation": "为了使LLM能够保持连续性、个性化和长期上下文，在长时间用户交互中实现真正的互动性和适应性，需要引入代理记忆。这使得LLM能够在跨会话间保留并根据信息采取行动，类似于人类的行为模式。", "method": "Memoria采用了模块化的记忆框架，包括动态的会话级总结以及基于加权知识图谱（KG）的用户建模引擎，该引擎逐步捕获用户的特性、偏好和行为模式作为结构化实体及关系。这种混合架构既保持了短期对话连贯性又实现了长期个性化。", "result": "Memoria展示了如何通过弥合无状态LLM接口与代理记忆系统之间的差距，提供了一种用于实现可扩展的个性化的AI对话解决方案，并适用于需要适应性和演变用户体验的实际工业应用中。", "conclusion": "Memoria为增强基于大型语言模型（LLM）的个性化和互动性提供了有效的途径，在实际应用中表现出色。"}}
{"id": "2512.12683", "pdf": "https://arxiv.org/pdf/2512.12683", "abs": "https://arxiv.org/abs/2512.12683", "authors": ["Yeray Cordero", "Paula García-Molina", "Fernando Vilariño"], "title": "Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis", "categories": ["quant-ph", "cs.AI", "cs.CV"], "comment": null, "summary": "Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.", "AI": {"tldr": "本文提出了一种混合量子经典框架Q-NeRF，用于神经辐射场渲染，解决了传统网络在高频细节捕捉方面的局限性。", "motivation": "经典的隐式神经表示（INRs）在网络中存在频谱偏差问题，限制了其对高频率细节的捕获能力。通过引入参数化的量子电路来解决这一问题，并开发出一种新的混合量子经典框架Q-NeRF，以提高3D场景重建和新颖视图合成的能力。", "method": "提出了一种基于量子隐式表示网络（QIREN）的新颖方法，将这些模块整合进经典的Nerfacto架构中，实现了高效的样本采样、姿态细化和体积渲染策略，同时用量子增强的组件替换了密度和辐射预测部分。", "result": "通过在标准多视图室内数据集上进行系统评估，混合模型展示了与经典基线相比，在有限计算资源下具有竞争力的重建质量。特别是在表示细尺度和视角依赖外观方面，量子模块显示出显著的优势。", "conclusion": "虽然当前实现受限于量子电路模拟器只能处理少量比特的问题，但结果表明了量子编码在缓解隐式表示中的频谱偏差方面的潜力，并为大规模量子增强3D场景重建提供了基础步骤。"}}
{"id": "2512.12678", "pdf": "https://arxiv.org/pdf/2512.12678", "abs": "https://arxiv.org/abs/2512.12678", "authors": ["Fatimah Zohra", "Chen Zhao", "Hani Itani", "Bernard Ghanem"], "title": "$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment", "categories": ["cs.CV"], "comment": null, "summary": "CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.", "AI": {"tldr": "该论文提出了$β$-CLIP，一种基于多粒度的图文对比学习框架，用于实现文本和视觉区域之间的分层对齐。", "motivation": "虽然CLIP在零样本图像文字检索中表现出色，但在长而详细的描述符上进行微调时，在细粒度任务上的表现不佳。为了解决这个问题，提出了$β$-CLIP来优化多颗粒级别的图文对齐。", "method": "$β$-CLIP通过跨注意力机制动态池化图像补丁，并引入$β$-Contextualized Contrastive Alignment Loss（$β$-CAL）以调节严格查询特定匹配和放松的图像上下文化之间的权衡，支持软交叉熵和硬二元交叉熵公式。", "result": "$β$-CLIP在Urban1K上实现了91.8%的T2I准确率和92.3%的I2T准确率，在FG-OVD（Hard）上达到了30.9%，在没有使用硬负例的情况下，它设定了新的最先进水平。", "conclusion": "$β$-CLIP为密集的视觉语言对应关系提供了一个稳健且适应性强的基础线。代码和模型已开放于https://github.com/fzohra/B-CLIP"}}
{"id": "2512.12677", "pdf": "https://arxiv.org/pdf/2512.12677", "abs": "https://arxiv.org/abs/2512.12677", "authors": ["Amirhossein Yousefiramandi", "Ciaran Cooney"], "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 6 figures", "summary": "We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.", "AI": {"tldr": "研究探讨了在资源限制下，利用预训练的因果LLM进行下游文本分类任务的有效策略。", "motivation": "旨在探索有限计算资源约束下的高效策略，通过附加分类头或指令微调两种方法来优化LLM的文本分类性能。", "method": "采用嵌入式和指令式的两种细调方式，并结合4位模型量化与低秩适应（LoRA）技术以实现单GPU下大规模参数模型的训练。", "result": "实验表明，基于嵌入的方法在F1得分上明显优于指令微调方法，在有限资源条件下表现出色甚至超越特定领域的预训练模型。", "conclusion": "研究结果证明了直接利用因果LLM内部表示与高效细调技术在文本分类任务中的优势，并提出优化策略和未来方向。"}}
{"id": "2512.12675", "pdf": "https://arxiv.org/pdf/2512.12675", "abs": "https://arxiv.org/abs/2512.12675", "authors": ["Yuran Wang", "Bohan Zeng", "Chengzhuo Tong", "Wenxuan Liu", "Yang Shi", "Xiaochen Ma", "Hao Liang", "Yuanxing Zhang", "Wentao Zhang"], "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "Code: https://github.com/Ryann-Ran/Scone", "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "AI": {"tldr": "本文提出了一种名为Scone的方法，该方法可以同时完成多主题图像生成任务中的组合和区分。", "motivation": "现有的基于主题的图像生成技术虽然已经从单个主题扩展到了多个主题的组合，但在处理包含多个候选者的输入时忽视了区分能力。这种局限性限制了其在复杂现实场景下的有效性。", "method": "Scone采用了一个统一的理解和生成模型方法，该方法包括一个理解专家作为语义桥梁，将语义信息传递给生成专家以保持主题身份并减少干扰。通过两个阶段的训练方案，首先学习组合，然后通过语义对齐和注意力掩码来增强区分。", "result": "实验结果表明，在两个基准测试中，Scone在图像生成任务中的组成和区分方面都优于现有开源模型。", "conclusion": "本文提出了一种新颖的方法Scone，该方法不仅能够有效地完成多主题的图像组合任务，并且还能提高对于主体身份的识别能力。"}}
{"id": "2512.12673", "pdf": "https://arxiv.org/pdf/2512.12673", "abs": "https://arxiv.org/abs/2512.12673", "authors": ["Yushun Tang", "Ziqiong Liu", "Jiyuan Jia", "Yi Zhang", "Zhihai He"], "title": "Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\\% in classification accuracy on the ImageNet-C dataset.", "AI": {"tldr": "提出了一种在推理阶段通过逐步调整自注意力模块来实现在线测试时间领域适应的新方法。", "motivation": "解决将Transformer网络模型应用于新目标域时，查询、键和值特征与源域差异导致的性能下降问题。", "method": "开发了基于条件尺度偏移校准（PCSR）的方法，利用Domain Separation Network提取领域迁移特征，并使用Factor Generator Network预测自注意力模块的调整参数。该方法在推理过程中动态适应。", "result": "实验证明，在ImageNet-C数据集上，提出的PCSR方法能显著提高在线测试时间领域的适应性能，提升最高达3.9%的分类准确率。", "conclusion": "所提出的方法能够有效解决Transformer模型在新领域应用时的性能下降问题，并在多个基准数据集中展现出优越的结果。"}}
{"id": "2512.12669", "pdf": "https://arxiv.org/pdf/2512.12669", "abs": "https://arxiv.org/abs/2512.12669", "authors": ["Jiawei Shen", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Guoqing Ma", "Yidan Liang", "Jingjiang Liu", "Hao Chen", "Shimin Di"], "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.", "AI": {"tldr": "本文提出了一种统一方法DynaGen，用于时间知识图谱推理（TKGR），旨在解决插值和外推过程中的挑战。", "motivation": "现有的插值方法在捕捉上下文方面有限，而外推技术容易陷入表面模式的识别。为此，作者提出了一个既能有效处理插值又能提高预测未来事件能力的方法DynaGen。", "method": "对于插值问题，DynaGen通过构建实体中心子图并使用协同双分支GNN编码器来捕捉动态结构上下文；对于外推，则采用条件扩散过程以学习潜在进化原理而非仅仅表面模式。", "result": "在六个基准数据集上的大量实验表明，DynaGen达到了最先进的性能，在插值和外推任务上分别比第二好模型平均提高了2.61点和1.45点的MRR分数。", "conclusion": "本文提出的方法成功解决了TKGR中的关键挑战，并通过广泛的实验证明了其优越性。"}}
{"id": "2512.12667", "pdf": "https://arxiv.org/pdf/2512.12667", "abs": "https://arxiv.org/abs/2512.12667", "authors": ["Haiyang Zheng", "Nan Pu", "Wenjing Li", "Teng Long", "Nicu Sebe", "Zhun Zhong"], "title": "Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning", "categories": ["cs.CV"], "comment": "Accepted by AAAI2026", "summary": "The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.", "AI": {"tldr": "提出了一种基于置信度感知的不对称学习框架来解决开放世界的深度伪造图像归因问题，该框架改善了已知和未知类型的识别性能。", "motivation": "现有的开放世界深度伪造图像归因方法存在伪标签偏差和对未知伪造类型数量的不现实假设等问题，因此提出了一种新的置信度感知不对称学习框架来解决这些问题。", "method": "提出了包含置信度感知一致性正则化(CCR) 和不对称置信增强(ACR) 两个组件的新框架，并引入动态原型修剪(DPP)策略来自动估计未知伪造类型的数量。", "result": "实验结果表明，所提方法在标准开放世界深度伪造图像归因基准和扩展的先进操作基准上均优于先前的方法，取得了新的最先进技术性能。", "conclusion": "该研究提出了一种有效的置信度感知不对称学习框架及其动态原型修剪策略，显著改善了开放世界的深度伪造图像识别问题。"}}
{"id": "2512.12664", "pdf": "https://arxiv.org/pdf/2512.12664", "abs": "https://arxiv.org/abs/2512.12664", "authors": ["Sreehari Rajan", "Kunal Bhosikar", "Charu Sharma"], "title": "InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.", "AI": {"tldr": "介绍了一个名为InteracTalker的新框架，该框架能生成响应语言和物体的自然人体动作。", "motivation": "当前方法分别处理语音驱动的手势或物体交互，缺乏集成的数据集限制了实用性。", "method": "使用多阶段训练过程学习统一的动作、言语和提示嵌入空间，并采用自适应融合策略解决不同条件信号之间的不平衡问题。", "result": "InteracTalker在手势生成和对象互动合成方面优于现有方法，提高了现实感、灵活性和控制性。", "conclusion": "提出的方法成功地将独立的任务结合起来，展示了其优越性和实用性。"}}
{"id": "2512.12663", "pdf": "https://arxiv.org/pdf/2512.12663", "abs": "https://arxiv.org/abs/2512.12663", "authors": ["Gelesh G Omathil", "Sreeja CS"], "title": "PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation. This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments. Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.", "AI": {"tldr": "本文提出了PerNodeDrop方法，旨在通过引入样本级别的噪声来平衡深度神经网络中专业子网和正则化之间的关系，从而改善模型的泛化能力。", "motivation": "深度神经网络虽然具有强大的表示能力，但也容易过拟合。传统的方法如Dropout和DropConnect在应用噪声时是均匀且固定的，这可能会抑制有益或有害的共适应性模式。", "method": "PerNodeDrop通过引入样本级别的、节点级别的随机化来打破现有技术中注入噪声的一致性，从而允许每个节点体验输入特定的变化。", "result": "实验表明，PerNodeDrop在视觉、文本和音频基准测试中的泛化能力优于标准的噪声正则化方法。", "conclusion": "PerNodeDrop通过样本级别的操作改善了模型的可靠性，并缩小了训练与验证性能之间的差距。"}}
{"id": "2512.12662", "pdf": "https://arxiv.org/pdf/2512.12662", "abs": "https://arxiv.org/abs/2512.12662", "authors": ["Muhammad Umar Farooq", "Abd Ur Rehman", "Azka Rehman", "Muhammad Usman", "Dong-Kyu Chae", "Junaid Qadir"], "title": "Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.", "AI": {"tldr": "提出了一种名为SSMT-Net的半监督多任务Transformer网络，用于甲状腺结节超声图像分割。", "motivation": "现有的深度学习模型在处理甲状腺结节和周围组织之间的模糊边界、大小变化以及标注数据稀缺时面临挑战。该论文旨在通过改进的方法来提高自动分割的准确性和鲁棒性。", "method": "SSMT-Net利用未标记的数据增强Transformer编码器特征提取能力，并结合局部与全局上下文信息进行结节和甲状腺腺体分割以及结节大小估计任务。", "result": "在TN3K和DDTI数据集上的评估显示，SSMT-Net比现有方法表现更佳，具有更高的准确性和鲁棒性。", "conclusion": "提出的SSMT-Net模型展示了其在临床应用中提高甲状腺结节超声图像分割准确性的潜力。"}}
{"id": "2512.12658", "pdf": "https://arxiv.org/pdf/2512.12658", "abs": "https://arxiv.org/abs/2512.12658", "authors": ["Qixin Xu", "Haozhe Wang", "Che Liu", "Fangzhen Lin", "Wenhu Chen"], "title": "CogDoc: Towards Unified thinking in Documents", "categories": ["cs.CV"], "comment": null, "summary": "Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution \"Fast Reading\" phase for scalable information localization,followed by a high-resolution \"Focused Thinking\" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the \"policy conflict\" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.", "AI": {"tldr": "提出CogDoc框架，用于处理文档中的长文本和多模态细节问题。", "motivation": "当前的文档推理方法在可扩展性和精度之间存在折衷。本文旨在通过模仿人类的认知过程解决这一矛盾。", "method": "提出了一个从粗到细的认知框架CogDoc，包括“快速阅读”和“深度思考”两个阶段，并采用直接强化学习策略。", "result": "实验表明7B参数的模型在挑战性的多模态文档基准上超越了较大的现有模型。", "conclusion": "通过模仿人类认知过程，CogDoc可以更好地处理长文本和复杂细节问题。"}}
{"id": "2512.12657", "pdf": "https://arxiv.org/pdf/2512.12657", "abs": "https://arxiv.org/abs/2512.12657", "authors": ["Hongyang Li", "Junyi Tao", "Qijie Wei", "Ningzhi Yang", "Meng Wang", "Weihong Yu", "Xirong Li"], "title": "Cross-modal Fundus Image Registration under Large FoV Disparity", "categories": ["cs.CV"], "comment": "Accepted as a regular paper at MMM 2026", "summary": "Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.", "AI": {"tldr": "本文提出了一种名为CARe的方法，用于解决跨模态眼底图像配准中大视场差异的问题。", "motivation": "现有的跨模态眼底图像注册技术主要针对小视野差异的场景，而面对具有较大视场差异的情况时表现不佳。为此，作者开发了新的解决方案以应对这一挑战。", "method": "CARe方法通过首先从广域彩色眼底照片中裁剪出与OCTA视野大致对齐的子图像，并利用改进的空间变换技术进行后续配准来解决大视场差异的问题。", "result": "实验结果表明，使用CARe方法可以有效地解决跨模态眼底图像注册中的大视场差异问题。", "conclusion": "本文提出的CARe方法在处理具有较大视野差异的跨模态眼底图像注册任务中表现出色。"}}
{"id": "2512.12652", "pdf": "https://arxiv.org/pdf/2512.12652", "abs": "https://arxiv.org/abs/2512.12652", "authors": ["Nardine Osman"], "title": "Value-Aware Multiagent Systems", "categories": ["cs.AI", "cs.MA"], "comment": ":68T01ACM Class:I.2.0", "summary": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.", "AI": {"tldr": "本文介绍了AI中的价值感知概念，提出了一条简明的工程化价值感知AI路线图。", "motivation": "传统价值观对齐问题局限性较大，价值感知旨在解决更复杂的实际应用场景中的人机交互与协作问题。", "method": "定义并构建了由三个核心支柱组成的路线图：学习和表示人类价值观、确保个体代理和多代理系统的价值观对齐以及基于价值观的行为解释。", "result": "论文呈现了一些正在进行的工作及这些工作的应用实例，涵盖了真实生活领域中的应用场景。", "conclusion": "通过价值感知的概念和技术路径，能够更好地实现人机之间的有效沟通与协作。"}}
{"id": "2512.12649", "pdf": "https://arxiv.org/pdf/2512.12649", "abs": "https://arxiv.org/abs/2512.12649", "authors": ["Zhewen Zheng", "Wenjing Cao", "Hongkang Yu", "Mo Chen", "Takashi Suzuki"], "title": "Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.", "AI": {"tldr": "本文提出了一种基于贝叶斯优化的框架，用于对非线性路径跟随控制器进行参数调优。", "motivation": "在实际实验中，硬件上的评估预算有限。对于具有多个影响动态特性的耦合非线性项的路径跟随控制器来说，手动调整效率低下且难以获得满意的性能表现。因此需要一种能够在实践中找到高性能参数区域的方法。", "method": "通过将闭环系统视为黑盒并使用高斯过程代理选择控制器增益来实现基于贝叶斯优化（BO）的框架。", "result": "结果表明，在32次试验中，包括15次热启动初始评估后，该方法能够提高控制性能，并且能够在实际条件下有效找到高性能参数区域。", "conclusion": "研究表明，基于贝叶斯优化的方法为在真实机器人平台上对非线性路径跟随控制器进行参数调优提供了一种实用、可靠和数据高效的方法。"}}
{"id": "2512.12634", "pdf": "https://arxiv.org/pdf/2512.12634", "abs": "https://arxiv.org/abs/2512.12634", "authors": ["Youngmin Im", "Byeongung Jo", "Jaeyoung Wi", "Seungwoo Baek", "Tae Hoon Min", "Joo Hyung Lee", "Sangeun Oh", "Insik Shin", "Sunjae Lee"], "title": "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.", "AI": {"tldr": "提出了一种用于移动端GUI代理的模块化多路径感知离线基准测试框架MobiBench。", "motivation": "当前评估方法存在单路径离线或在线实时评估的问题，前者不公平地惩罚了有效替代行为，后者由于动态和不可预测性而难以扩展和重复。现有基准忽略了组件贡献，导致不公正比较并掩盖性能瓶颈。", "method": "提出了MobiBench框架，该框架能够在完全离线环境中进行高度保真、可扩展且可重现的评估，并能够对不同技术进行系统评价以及模块配置优化。", "result": "实验表明，MobiBench与人类评估者达成94.72%的一致性，性能与精心设计的在线基准相当，同时保持静态离线基准的优势。此外还揭示了当前低层模型（LFMs）的局限性和设计更高效代理的指导方针。", "conclusion": "MobiBench提供了一种新颖且有效的评估移动端GUI代理的方法，克服了现有方法中的主要限制，并为未来研究提供了重要见解和实用指南。"}}
{"id": "2512.12633", "pdf": "https://arxiv.org/pdf/2512.12633", "abs": "https://arxiv.org/abs/2512.12633", "authors": ["Zhou Tao", "Shida Wang", "Yongxiang Hua", "Haoyu Cao", "Linli Xu"], "title": "DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "本文介绍了一种新的代理任务框架DiG（Differential Grounding），通过识别和定位相似图像对之间的所有差异，提高多模态大型语言模型的细粒度视觉感知能力。", "motivation": "尽管多模态大型语言模型在各种视觉-语言任务中表现出色，但它们的细粒度视觉感知和精确的空间推理仍然有限。因此，本文旨在通过DiG框架解决这一问题。", "method": "开发了一种基于自动化3D渲染的数据生成管道，并使用逐步增加难度的教学方法（从单个差异到多个差异），以支持大规模训练并提高模型性能。", "result": "实验结果表明，DiG显著提高了多模态大型语言模型在视觉感知基准测试中的表现，并且所学到的细粒度感知技能可以有效地迁移到下游任务上。", "conclusion": "研究表明，差异化定位作为一种可扩展和鲁棒的方法，能够有效提升多模态大型语言模型的细粒度视觉推理能力。"}}
{"id": "2512.12632", "pdf": "https://arxiv.org/pdf/2512.12632", "abs": "https://arxiv.org/abs/2512.12632", "authors": ["Rishit Agnihotri", "Sandeep Kumar Sharma"], "title": "Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks", "categories": ["cs.RO"], "comment": "Preprint. Under review for conference submission", "summary": "Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.", "AI": {"tldr": "本文提出了一种基于边缘AI的去中心化群集架构，用于解决城市空中交通中的冲突管理问题。", "motivation": "随着无人机密度在密集的城市走廊中增加，传统的方法难以有效协调和管理。", "method": "通过使用控制算法和轻量级神经网络，实现分布式冲突检测与实时决策。", "result": "实验结果表明，所提出的方案能将冲突解决时间缩短3.8倍，并提高准确性。", "conclusion": "该架构对于未来城市空中交通系统的可扩展性和安全性具有高度的潜力。"}}
{"id": "2512.12630", "pdf": "https://arxiv.org/pdf/2512.12630", "abs": "https://arxiv.org/abs/2512.12630", "authors": ["Yuqian Sun", "Xingyu Li", "Shunyu Yao", "Noura Howell", "Tristan Braud", "Chang Hee Lee", "Ali Asadipour"], "title": "ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.", "AI": {"tldr": "ORIBA是一个基于大型语言模型的AI聊天机器人，旨在通过角色扮演帮助视觉艺术家创造原创人物，并尊重他们的创意主权。", "motivation": "探讨生成式人工智能如何在不侵犯视觉艺术家创作自由的情况下支持其进行原创人物设计。", "method": "开发了一个名为ORIBA的AI聊天机器人，允许艺术家与其原创人物互动以形成概念背景故事。通过与14位艺术家的研究发现，这种交互有助于激发他们的想象力并加深对原创角色的情感联系。", "result": "研究显示，ORIBA促进了艺术家的创造性参与，使他们能够开发出更丰富的人物属性，并增强其与角色之间的联系，进而推动艺术创作过程。", "conclusion": "通过从艺术家的角度出发设计AI系统，该研究表明大型语言模型可以支持跨模态创造力的发展，同时保持创意主权，展示生成式人工智能作为非视觉辅助工具在加强现有创造性实践中的潜力。"}}
{"id": "2512.12623", "pdf": "https://arxiv.org/pdf/2512.12623", "abs": "https://arxiv.org/abs/2512.12623", "authors": ["Chengzhi Liu", "Yuzhe Yang", "Yue Fan", "Qingyue Wei", "Sheng Liu", "Xin Eric Wang"], "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.", "AI": {"tldr": "提出了一种动态多模态潜意识推理框架DMLR，以改进跨模态理解和推理。", "motivation": "现有方法在跨模态理解与推理中仍存在线性依赖、不稳定的感知-推理交互及显著的计算开销问题。受人类认知启发，认为思维并非线性发展而是通过潜意识中的动态交织进行。", "method": "DMLR采用基于信心引导的潜在策略梯度优化来细化潜意识思考令牌，并引入动态视觉注入策略，以实现动态视觉-文本交互。", "result": "实验结果表明DMLR在七个跨模态推理基准上显著提高了推理和感知性能，并且保持了高效的推断效率。", "conclusion": "通过提出新颖的动态多模态潜意识交织方法，改进了现有的模型结构与推理过程，在复杂任务中表现出色。"}}
{"id": "2512.12622", "pdf": "https://arxiv.org/pdf/2512.12622", "abs": "https://arxiv.org/abs/2512.12622", "authors": ["Zihan Wang", "Seungjun Lee", "Guangzhao Dai", "Gim Hee Lee"], "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.", "AI": {"tldr": "提出了一种动态的三维视觉语言规划模型（D3D-VLP），用于解决端到端模型缺乏可解释性和明确的3D推理的问题，同时克服了模块化系统忽略跨组件依赖性的问题。", "motivation": "为了弥补端到端模型缺乏可解释性和明确3D推理以及模块化系统忽略跨组件依赖性的不足，提出了一个动态三维视觉语言规划模型（D3D-VLP）。", "method": "该模型引入了两个关键创新：1) 一种动态的三维思维链(3D CoT)，统一了计划、接地、导航和问答在一个单一的3D-VLM和CoT管道中；2) 一个协同学习从碎片化监督（SLFS）策略，使用掩码自回归损失来学习大量且部分注释的混合数据。这使得不同的CoT组件可以相互强化并隐式地监督彼此。", "result": "在多个基准测试上实现了最先进的结果，包括视觉语言导航（R2R-CE、REVERIE-CE、NavRAG-CE）、对象目标导航（HM3D-OVON）和任务导向顺序接地与导航（SG3D）。实际移动操作实验进一步验证了其有效性。", "conclusion": "通过提出动态三维视觉语言规划模型（D3D-VLP），解决了当前方法的不足，实现了跨模态数据的有效学习，并在多个基准测试中取得了领先的结果。"}}
{"id": "2512.12620", "pdf": "https://arxiv.org/pdf/2512.12620", "abs": "https://arxiv.org/abs/2512.12620", "authors": ["Aheli Poddar", "Saptarshi Sahoo", "Sujata Ghosh"], "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs", "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.", "AI": {"tldr": "研究LLM在形式逻辑和自然语言视角下的三段论推理能力。", "motivation": "探索大型语言模型的基本推理能力和未来发展方向，理解这些模型是否正变得越来越像正式的推理机制。", "method": "使用14种大型语言模型来评估它们在符号推断和自然语言理解方面的三段论推理能力。", "result": "尽管这种推理机制并非所有LLM都均匀出现，但某些模型在符号性能上表现完美。", "conclusion": "研究结果表明，部分LLM的推理能力更像正式逻辑系统而非人类微妙的推理方式。"}}
{"id": "2512.12610", "pdf": "https://arxiv.org/pdf/2512.12610", "abs": "https://arxiv.org/abs/2512.12610", "authors": ["Wonseok Choi", "Sohwi Lim", "Nam Hyeon-Woo", "Moon Ye-Bin", "Dong-Ju Jeong", "Jinyoung Hwang", "Tae-Hyun Oh"], "title": "Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching", "categories": ["cs.CV", "cs.IR"], "comment": "WACV 2026", "summary": "Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/", "AI": {"tldr": "提出Patchify框架，通过局部特征匹配实现实例级图像检索，提高准确性和空间定位性。", "motivation": "解决在不同大小、位置或外观下寻找相同对象的挑战，提供高效且可扩展的解决方案。", "method": "将数据库图像分割成结构化补丁，并通过与全局查询描述符进行局部特征比较实现匹配。引入LocScore评估指标，强调使用具有信息性的特征以提升性能。", "result": "在多个基准测试中表现出优于全球方法和补充最先进的重新排序管道的性能。", "conclusion": "Patchify框架有效且可扩展地解决了实例级图像检索问题，展示了其在大规模数据集上的优越表现。"}}
{"id": "2512.12608", "pdf": "https://arxiv.org/pdf/2512.12608", "abs": "https://arxiv.org/abs/2512.12608", "authors": ["Hong Su"], "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.", "AI": {"tldr": "提出了一种启发人类学习的大规模语言模型框架，通过显性记录和最大熵方法发现机制来提升模型在稀疏或未见过场景中的表现。", "motivation": "大规模语言模型虽然擅长从大量语料中提取常见模式，但在处理罕见、低资源或者前所未见的情况时存在困难。此外，它们主要依赖隐式参数记忆，这限制了其明确学习、回溯和优化方法的能力。", "method": "该框架包含两个互补机制：显性记录用于存储因果关系或问答回答的符号记忆，使得模型即使从单次或不频繁的经历中也能持续学习；最大熵方法发现优先保存语义差异大的方法，以捕捉通常被下一个标记预测忽视的多样化和不足代表的战略。", "result": "在包含60个语义多样化的问答回应基准测试上验证了该框架的有效性。相较于随机基线，所提出的基于熵指引的方法对未见过的问题覆盖更强且内部多样性更高。", "conclusion": "通过显性记录和最大熵方法发现机制的结合，证明了模型能够更有效地学习和应用多样化的、通用性强的人类启发式策略。"}}
{"id": "2512.12604", "pdf": "https://arxiv.org/pdf/2512.12604", "abs": "https://arxiv.org/abs/2512.12604", "authors": ["Tingyan Wen", "Haoyu Li", "Yihuang Chen", "Xing Zhou", "Lifei Zhu", "Xueqian Wang"], "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching", "categories": ["cs.CV"], "comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim", "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.", "AI": {"tldr": "提出X-Slim缓存加速器，通过时间步、结构和空间层面的冗余利用来提高扩散模型的生成效率。", "motivation": "缓解扩散模型在计算复杂度与生成质量之间的矛盾，减少延迟同时保证或提升图像生成的质量。", "method": "设计了一种基于缓存的X-Slim加速器，通过时间步级别的重用、轻量级结构和空间刷新机制来优化扩散过程。引入双阈值控制器以控制何时重用以及如何处理累积误差。", "result": "在FLUX.1-dev和HunyuanVideo数据集上分别减少了4.97x和3.52x的延迟；DiT-XL/2模型则实现了3.13倍加速，并提升了FID指标，超过先前方法。", "conclusion": "X-Slim缓存机制不仅在多种任务中推进了速度与质量边界的拓展，而且证明了其对扩散模型生成效率和质量的显著提升。"}}
{"id": "2512.12598", "pdf": "https://arxiv.org/pdf/2512.12598", "abs": "https://arxiv.org/abs/2512.12598", "authors": ["Cong Xie", "Che Wang", "Yan Zhang", "Zheng Pan", "Han Zou", "Zhenpeng Zhan"], "title": "Geometry-Aware Scene-Consistent Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.", "AI": {"tldr": "研究基于几何感知的场景一致性图像生成，给定参考场景图和文本描述需生成的实体及其与场景的空间关系，任务是合成一张保持相同物理环境且符合空间关系的新图。", "motivation": "现有方法难以平衡场景保留与指令响应之间的矛盾：要么高度复制场景但对指令反应差，要么优先考虑指令遵守却牺牲了场景一致性。为解决这一困境，引入两个关键贡献。", "method": "提出了一种场景一致的数据构造流水线，生成多样化的、基于几何的训练对；设计了一个新颖的几何引导注意力损失函数，利用跨视角线索规范模型的空间推理能力。", "result": "实验证明，在场景一致基准上优于现有方法，实现了更好的场景对齐和文本图像一致性，自动评价及人工偏好测试均显示优势。", "conclusion": "该方法生成了多样且符合几何逻辑的图像，并忠实于文本指令与原始场景结构。"}}
{"id": "2512.12597", "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "AI": {"tldr": "本文介绍了AgentSHAP，一种用于解释大型语言模型（LLM）代理工具重要性的框架。", "motivation": "现有可解释人工智能方法未能解决工具级别的解释问题。为了填补这一空白，作者开发了AgentSHAP来评估和理解特定任务中不同工具的重要性。", "method": "AgentSHAP通过蒙特卡洛采样估算Shapley值，测试代理在不同工具子集下的反应，并根据博弈论计算出公平的重要分数。", "result": "实验显示，AgentSHAP能够产生一致且可靠的得分，正确识别哪些工具有影响，同时区分相关与不相关的工具。", "conclusion": "AgentSHAP作为一个模型无关的解释框架，为大型语言模型代理提供了重要的工具级解析方法。"}}
{"id": "2512.12596", "pdf": "https://arxiv.org/pdf/2512.12596", "abs": "https://arxiv.org/abs/2512.12596", "authors": ["Kei Yoshitake", "Kento Hosono", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.", "AI": {"tldr": "本文提出了一种利用视觉语言模型生成基于图像的广告布局的方法。", "motivation": "传统广告布局技术主要依赖于显著性映射来检测背景图片中的显著区域，但这种方法往往不能充分考虑图像的详细构图和语义内容。为了解决这个问题，该论文提出了一个新方法，利用视觉语言模型识别背景图片中的产品和其他元素，并据此决定文字和标志的位置。", "method": "提出的布局生成流程包括两个步骤：第一步是使用视觉语言模型分析图片以识别物体类型及其空间关系，并基于此分析产生文本形式的“放置计划”；第二步则是将该计划转化为最终的布局，通过生成HTML格式代码来实现。", "result": "通过定量和定性的对比实验验证了方法的有效性，结果显示通过对背景图像内容进行显式考虑，本文的方法能够显著提高广告布局的质量。", "conclusion": "利用视觉语言模型可以更好地理解图片中的详细构图和语义内容，并据此生成更高质量的基于图像的广告布局。"}}
{"id": "2512.12595", "pdf": "https://arxiv.org/pdf/2512.12595", "abs": "https://arxiv.org/abs/2512.12595", "authors": ["Karthikeya KV"], "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.", "AI": {"tldr": "本文提出了一个将视觉增强的大规模语言模型与高级变换器架构相结合的框架，以解决高分辨率图像合成和多模态数据解释的挑战。", "motivation": "为了提高高分辨率图像生成的质量并实现不同数据类型的统一理解，作者提出了一种结合了修正流机制、双向标记策略以及时空特征嵌入的新方法。", "method": "该框架采用了一个修正流机制连接噪声和数据，并使用双向标记化策略将文本、图片及视频模态的输入融合在一起。此外，还采用了混合文本图像序列建模方式并优化了噪音感知学习算法以适应不同输入条件下的生成性能。", "result": "在基准数据集上的严格评估显示，所提出的方法比扩散方法提高了25%的图像分辨率清晰度，并降低了20%的计算需求。模型表现出强大的可扩展性和适应性，在自动驾驶系统、创意内容生成和高级视频分析等应用中展现出了巨大潜力。", "conclusion": "该研究证明了视觉中心的大规模语言模型在重新定义计算机视觉和多模态人工智能的能力方面的关键作用"}}
{"id": "2512.12590", "pdf": "https://arxiv.org/pdf/2512.12590", "abs": "https://arxiv.org/abs/2512.12590", "authors": ["Indiwara Nanayakkara", "Dehan Jayawickrama", "Mervyn Parakrama B. Ekanayake"], "title": "Automatic Wire-Harness Color Sequence Detector", "categories": ["cs.CV", "eess.IV"], "comment": "6 pages, 20 figures, IEEE ICIIS 2025 Conference - Accepted", "summary": "Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.", "AI": {"tldr": "介绍了一种半自动机器视觉系统，用于验证电线位置、连接器极性和颜色序列的正确性。", "motivation": "现代电子制造服务行业中的电线束检验过程耗时且容易出错。为提高效率和准确性，设计了一个能够检查线束配置的自动化解决方案。", "method": "采用五个工业标准CMOS相机集成在一个模块化机械框架中，并使用HSV和RGB颜色域值比较方法来检测颜色序列。系统通过至少五份参考样本进行训练并存储以供类似线束类型重复使用。", "result": "该系统在GPV Lanka Pvt. Ltd.部署，实现了100%的检测准确率，且相比手动方法缩短了44%的检验时间。此外还具备用户管理、可调光照明和安全登录等功能。", "conclusion": "实验表明此解决方案能提供可靠高效的检查能力，在实际应用中效果显著"}}
{"id": "2512.12586", "pdf": "https://arxiv.org/pdf/2512.12586", "abs": "https://arxiv.org/abs/2512.12586", "authors": ["Lixin Chen", "Chaomeng Chen", "Jiale Zhou", "Zhijian Wu", "Xun Lin"], "title": "StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis", "categories": ["cs.CV"], "comment": "13 pages, 10 figures. This is the extended version of the paper accepted at AAAI 2026, including related works and appendix", "summary": "Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.", "AI": {"tldr": "提出了一种新的隐私保护视频动作识别框架StegaVAR，通过在普通封面视频中嵌入行动视频，并直接在其隐写域进行动作分析。", "motivation": "当前的隐私保护方法存在视觉失真和时空干扰的问题，影响了视频动作识别的效果。为了解决这些问题，作者提出了一个新的框架来同时实现高效的视频动作识别和强大的隐私保护。", "method": "StegaVAR通过将行动视频嵌入普通封面视频中，在隐写域进行视频动作识别，提出Secret Spatio-Temporal Promotion (STeP) 和 Cross-Band Difference Attention (CroDA)，用于隐写域内的分析，以抑制封面干扰。", "result": "实验结果表明，StegaVAR在多个广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能，并且对多种隐写模型有效。", "conclusion": "StegaVAR提供了一种新颖的方法来解决当前隐私保护方法存在的视觉失真和时空干扰问题，提高了视频动作识别的效果。"}}
{"id": "2512.12583", "pdf": "https://arxiv.org/pdf/2512.12583", "abs": "https://arxiv.org/abs/2512.12583", "authors": ["Safwan Shaheer", "G. M. Refatul Islam", "Mohammad Rafid Hamid", "Md. Abrar Faiaz Khan", "Md. Omar Faruk", "Yaseen Nur"], "title": "Detecting Prompt Injection Attacks Against Application Using Classifiers", "categories": ["cs.CR", "cs.AI"], "comment": "9 pages, X figures; undergraduate research project on detecting prompt injection attacks against LLM integrated web applications using classical machine learning and neural classifiers", "summary": "Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.", "AI": {"tldr": "该论文通过训练包括LSTM、前馈神经网络、随机森林和朴素贝叶斯在内的分类器来检测大型语言模型集成的Web应用程序中的恶意提示。", "motivation": "针对关键系统中存在的提示注入攻击，该工作旨在提高其检测与缓解能力以保护目标应用及系统。", "method": "基于HackAPrompt Playground Submissions语料库，作者整理并扩充了一个提示注入数据集，并利用多种分类器进行训练和测试。", "result": "所提出的这种方法提高了对恶意提示的检测精度，有助于增强大型语言模型集成Web应用程序的安全性与稳定性。", "conclusion": "通过使用各种机器学习模型来识别潜在威胁，论文展示了有效防止提示注入攻击的可能性。"}}
{"id": "2512.12576", "pdf": "https://arxiv.org/pdf/2512.12576", "abs": "https://arxiv.org/abs/2512.12576", "authors": ["Xueru Wen", "Jie Lou", "Yanjiang Liu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Yaojie Lu", "Debing Zhang"], "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.", "AI": {"tldr": "本文提出了Coupled Variational Reinforcement Learning（CoVRL）框架，通过连接先验和后验分布来改善语言模型的推理能力。", "motivation": "当前的语言模型强化学习方法在只基于问题采样推理路径时，会导致探索效率低下且答案与推导过程不一致。本文旨在解决此问题，提高语言模型的一般性推理性能。", "method": "通过结合变分推理和强化学习的混合策略，CoVRL框架建立并优化了一个包含先验后验分布的复合概率分布，提高了探索的有效性和回答一致性。", "result": "实验结果表明，与基础模型相比，CoVRL改进了12.4%的表现，并且比最新的无验证器强化学习基线方法还额外提升了2.3%。", "conclusion": "CoVRL提供了一种原则性的框架来提升语言模型的一般推理能力。"}}
{"id": "2512.12571", "pdf": "https://arxiv.org/pdf/2512.12571", "abs": "https://arxiv.org/abs/2512.12571", "authors": ["Boyeong Im", "Wooseok Lee", "Yoojin Kwon", "Hyung-Sin Kim"], "title": "From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.", "AI": {"tldr": "将视觉语言模型（VLM）从网络图像的应用扩展到传感器介导的物理环境中，通过相机曝光三角形作为物理提示进行测试时适应。", "motivation": "为了在实际应用场景中更好地应用视觉语言模型，提出了一种基于物理提示的方法，以提高模型对复杂环境下的表现和鲁棒性。", "method": "MVP框架通过采集场景的多视图并在推理过程中使用源亲和分数选择最佳传感器设置来实现。然后进行轻量级数字增强，并根据最低熵原则过滤最相关的视图集合，最后用零温度softmax聚合预测结果。", "result": "在ImageNet-ES和ImageNet-ES-Diverse数据集上，MVP相比仅使用数字提示的测试时适应方法提高了25.6个百分点，在结合传统传感器控制与TTA的管道中也获得了额外3.4个百分点的性能提升。", "conclusion": "实验结果支持了通过在测量阶段选择和组合实际物理视图来显著提高视觉语言模型鲁棒性的论点，表明这种方法的有效性和实用性。"}}
{"id": "2512.12560", "pdf": "https://arxiv.org/pdf/2512.12560", "abs": "https://arxiv.org/abs/2512.12560", "authors": ["Xinqi Jin", "Hanxun Yu", "Bohan Yu", "Kebin Liu", "Jian Liu", "Keda Tao", "Yixuan Pei", "Huan Wang", "Fan Dang", "Jiangchuan Liu", "Weiqiang Wang"], "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.", "AI": {"tldr": "本文提出了一种高效的视频令牌修剪方法，以减少在线视频理解中的上下文长度，并在保留关键信息的同时降低GPU内存使用和计算延迟。", "motivation": "多模态大型语言模型（MLLM）应用于在线视频理解时面临挑战，如大量的视频帧导致高GPU内存使用率和长时间的计算延迟。为了解决这些问题，本文提出了一种新的令牌修剪方法。", "method": "本文提出了最大相似度到空间相邻视频令牌（MSSAVT）的新冗余度量，并设计了掩码修剪策略以确保仅修剪互不相邻的令牌。此外，还整合了一个现有的基于时间冗余的修剪方法来消除视频模态的时间冗余。", "result": "实验结果表明，在多个在线和离线视频理解基准上，本文的方法显著提高了准确性（最多提高4％），同时产生的修剪延迟极低（小于1ms）。", "conclusion": "本文通过有效的令牌修剪策略大大提升了在线视频理解的效率和精度，并且整个实现将公开发布。"}}
{"id": "2512.12552", "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "categories": ["cs.AI"], "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.", "AI": {"tldr": "研究了大型语言模型在动态新供应商问题中的决策模式，发现其存在复制和放大认知偏差的风险。", "motivation": "探讨将大型语言模型应用于高风险业务决策中可能带来的认知偏差风险及其潜在机制。", "method": "通过与GPT-4、GPT-4o和LLaMA-8B进行动态多轮实验，测试了五种已知的决策偏差，并分析了这些偏差的起源。", "result": "发现大型语言模型在决策中存在“过低/过高”订单偏见及显著放大的需求追逐行为；更复杂的GPT-4显示出更大的非理性倾向。", "conclusion": "建议管理者根据具体任务选择合适的模型，强调高风险决策中的人类干预以避免错误，并提出设计结构化规则提示作为缓解偏差的方法。"}}
{"id": "2512.12549", "pdf": "https://arxiv.org/pdf/2512.12549", "abs": "https://arxiv.org/abs/2512.12549", "authors": ["Shaif Chowdhury", "Mushfika Rahman", "Greg Hamerly"], "title": "Supervised Contrastive Frame Aggregation for Video Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages", "summary": "We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.", "AI": {"tldr": "本文提出了一种监督对比帧聚合框架用于视频表示学习，该方法利用全局时间上下文并减少了计算开销。", "motivation": "现有复杂视频Transformer模型计算开销大且容易过拟合。通过引入视频到图像的聚合并使用预训练卷积神经网络骨干网络来解决这些问题，并设计了一个对比学习目标直接比较成对投影，以生成多样化的正样本和减少过拟合。", "method": "将多个视频帧排列为单个输入图像进行空间组织；定义了监督下的对比学习任务；通过不同时间采样产生多视角正样本；使用ResNet50等预训练模型提高效率。", "result": "在Penn Action数据集上达到了76%的分类准确率，高于ViVIT的43%，在HMDB51数据集中则为48%，优于ViVIT的37%。该方法不仅适用于监督学习也支持自监督设置，并能有效支持视频分类和字幕生成。", "conclusion": "所提出的方法通过利用全局时间上下文，有效地减少了计算开销并在多个基准上取得了优越的表现，展示了在视频表示学习方面的潜力。"}}
{"id": "2512.12548", "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. Ríos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "categories": ["cs.AI", "cs.LG"], "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.", "AI": {"tldr": "本文研究了人工觅食者如何通过学习世界模型来实现最优的觅食策略，特别是与边际价值定理（MVT）一致的行为。", "motivation": "尽管边际价值定理被广泛用于描述行为生态学中的最优觅食行为，但关于生物觅食者如何做出这些决定背后的计算机制尚不明确。本文旨在探讨使用基于模型的强化学习代理是否可以实现与自然界相似的决策模式。", "method": "通过训练一个具有简化的预测性环境表示的学习世界模型的基于模型的强化学习智能体，研究其行为是否符合边际价值定理（MVT）。", "result": "发现这些基于模型的RL智能体表现出类似生物觅食者的最优觅食决策模式。", "conclusion": "研究表明，通过使用预测性世界模型，可以促进更可解释和与自然生物行为一致的人工智能决策系统的发展。"}}
{"id": "2512.12545", "pdf": "https://arxiv.org/pdf/2512.12545", "abs": "https://arxiv.org/abs/2512.12545", "authors": ["Bin Mu", "Yuxuan Chen", "Shijin Yuan", "Bo Qin", "Hao Guo"], "title": "Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": null, "summary": "Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.", "AI": {"tldr": "构建了一种多球耦合概率模型TianXing-S2S，用于全球次季节到季节的每日集合预报。", "motivation": "准确预测次季节到季节的极端天气事件对于资源规划和灾害缓解至关重要。然而，由于复杂的多球体交互作用和大气内在不确定性，这样的预测仍然具有挑战性。", "method": "首先将多样化的多球体预测器编码为紧凑的潜在空间，然后使用扩散模型生成每日集合预报。在去噪器中引入基于最优传输（OT）的新耦合模块来优化大气与多球边界条件之间的交互作用。", "result": "TianXing-S2S 在关键的大气变量上优于欧洲中期天气预报中心的S2S系统和FuXi-S2S，实现了45天平均每日集合预报，在1.5分辨率下表现出色。该模型能够进行有效的次季节极端事件预测，并生成最长可达180天的稳定滚动预报。", "conclusion": "TianXing-S2S 建立了一个稳健的研究框架用于全球变暖背景下的次季节到季节研究，同时展示了土壤湿度作为关键前兆信号的重要性。"}}
{"id": "2512.12539", "pdf": "https://arxiv.org/pdf/2512.12539", "abs": "https://arxiv.org/abs/2512.12539", "authors": ["Huan Huang", "Michele Esposito", "Chen Zhao"], "title": "Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling", "categories": ["cs.CV"], "comment": "6 figures", "summary": "Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.", "AI": {"tldr": "本文提出了一种基于解剖学引导的冠状动脉分割框架，用于从心脏CT血管成像中准确提取冠状动脉。", "motivation": "由于细小分支、边界模糊和心肌干扰等问题，准确分割冠状动脉具有挑战性。因此，开发一种稳定且一致性的分割方法对于临床决策至关重要。", "method": "本文提出的方法结合了心肌解剖先验知识，结构感知特征编码，并采用三维小波变换技术进行降采样和上采样。此外，在解码阶段通过多尺度特征融合模块整合语义与几何信息。", "result": "实验结果显示所提方法在Dice系数、灵敏度、精确率及HD95等指标方面均优于主流分割模型，证明了其优越性。", "conclusion": "该框架能够提供更为稳定和一致的冠状动脉分割结果，在复杂解剖条件下尤其有效。"}}
{"id": "2512.12536", "pdf": "https://arxiv.org/pdf/2512.12536", "abs": "https://arxiv.org/abs/2512.12536", "authors": ["Arastoo Zibaeirad", "Marco Vieira"], "title": "Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes. This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts. Artifact: https://github.com/Erroristotle/DVDR_LLM", "AI": {"tldr": "该论文提出了一种结合多种大语言模型（LLMs）的框架DVDR-LLM，用于软件漏洞检测和修复。", "motivation": "单一的大语言模型在处理复杂漏洞识别和生成修复代码方面存在困难。因此，研究者希望通过集成多种不同的大语言模型来提高整体性能。", "method": "通过集成多个大语言模型的输出结果形成一个综合框架DVDR-LLM，并测试其对于不同复杂度代码和多文件漏洞检测的有效性。", "result": "实验表明，与单个模型相比，DVDR-LLM在软件漏洞检测中提高了10%-12%的准确率。特别是对于复杂程度高的代码和涉及多个文件的漏洞情况，性能提升更加显著。", "conclusion": "该研究证明了通过集成多种大语言模型可以有效提高软件漏洞检测与修复的整体性能，但也需要注意这种方法在某些特定场景下可能会增加假阴性错误的问题，并需要根据具体安全需求调整各模型的一致性阈值。"}}
{"id": "2512.12534", "pdf": "https://arxiv.org/pdf/2512.12534", "abs": "https://arxiv.org/abs/2512.12534", "authors": ["Qi Sun", "Can Wang", "Jiaxiang Shang", "Wensen Feng", "Jing Liao"], "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "SIGGRAPH Asia 2025", "summary": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.", "AI": {"tldr": "Animus3D通过文本驱动生成静态3D资产的动画", "motivation": "解决现有方法在生成动态时存在的运动不足或抖动问题，提高3D动画的质量和细节", "method": "引入Motion Score Distillation (MSD) 方法，并结合LoRA增强技术、倒置噪声估计技术和时空正则化项，提升动画质量和保真度。同时设计了运动细化模块以增加时间分辨率和细粒度的详细信息", "result": "Animus3D能够从各种文本提示中生成静态3D资产的动画，比当前最先进的基线方法产生更实质性、更详细的动态，并保持高视觉完整性", "conclusion": "提出的方法有效解决了现有技术在3D动画生成中的问题，显著提高了动画的质量和细节"}}
{"id": "2512.12523", "pdf": "https://arxiv.org/pdf/2512.12523", "abs": "https://arxiv.org/abs/2512.12523", "authors": ["Wenqi Fang", "Ye Li"], "title": "Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems", "categories": ["cs.LG", "cs.AI", "physics.comp-ph"], "comment": "under revision", "summary": "Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.", "AI": {"tldr": "提出了一种基于奇异值分解和严格半正交约束训练算法的神经网络架构，以提高传统对比学习在检测复杂系统中关键转换时的鲁棒性和效率。", "motivation": "现有的基于深度神经网络的对比学习方法参数过多且对无关噪声敏感，导致难以准确识别临界点。为了克服这些问题，提出了一种新的方法来增强传统对比学习的方法。", "method": "使用奇异值分解技术构建了一个新型神经网络架构，并采用了严格的半正交性约束训练算法，以提高其鲁棒性和效率。", "result": "实验表明所提方法在检测关键转换方面性能与传统对比学习相当，但更轻量且抗噪能力更强。", "conclusion": "通过采用基于奇异值分解和严格半正交性约束的新架构，可以显著改进现有技术的临界点识别准确率，并提高了对噪声的鲁棒性。"}}
{"id": "2512.12510", "pdf": "https://arxiv.org/pdf/2512.12510", "abs": "https://arxiv.org/abs/2512.12510", "authors": ["Alicia", "Lee", "Mai Lee Chang", "Sreehana Mandava", "Destiny Deshields", "Hugo Simão", "Aaron Steinfeld", "Jodi Forlizzi", "John Zimmerman"], "title": "Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline", "categories": ["cs.HC", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?", "AI": {"tldr": "探讨人工智能如何帮助认知衰退的老年人及其看护者进行更有效的照顾协调，同时保持隐私和生活质量。", "motivation": "随着认知能力下降的老年群体增加，他们的非正式护理人员面临着巨大的负担。为了研究智能代理能否协助这些家庭成员维持生活独立性并减轻护理负担，作者进行了深入访谈。", "method": "通过对十对经历认知衰退的老年人及其看护者的采访，探讨了他们在照顾协调、管理压力和支持自主和隐私方面的策略。", "result": "一些老人通过放弃日常生活中的某些任务来降低生活质量以减少照护者的负担；一种有效的策略是“搭便车”，利用日常事务之间的重叠机会完成更多工作。", "conclusion": "智能代理可能有助于改善这种协调方式，从而帮助老年人更长时间地留在家中而不增加看护者的工作量。"}}
{"id": "2512.12508", "pdf": "https://arxiv.org/pdf/2512.12508", "abs": "https://arxiv.org/abs/2512.12508", "authors": ["Jinfan Zhou", "Lixin Luo", "Sungmin Eum", "Heesung Kwon", "Jeong Joon Park"], "title": "Generative Spatiotemporal Data Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.", "AI": {"tldr": "该论文探讨了使用视频基础模型进行时空数据增强的方法，以多样化摄像机视角和场景动态。", "motivation": "现有的基于简单几何变换或外观扰动的数据增强方法不足以生成真实感的三维空间和时间变化。因此，作者希望通过利用现成的视频扩散模型来提高在注释稀缺条件下的性能表现。", "method": "该研究采用了视频基础模型生成现实主义的3D时空变异，并通过将合成的视频片段作为补充训练数据，以改善低数据环境下的图像识别任务。", "result": "实验表明，在COCO子集和无人机捕获的数据集中应用这种方法可以提高模型性能。特别是在注释稀缺的情况下，这种增强方法能显著提升表现。", "conclusion": "基于视频的时空生成技术能够拓宽数据分布，并在低数据环境下的图像识别任务中提供有效的改进措施"}}
{"id": "2512.12506", "pdf": "https://arxiv.org/pdf/2512.12506", "abs": "https://arxiv.org/abs/2512.12506", "authors": ["Agustín García-García", "Pablo Hidalgo", "Julio E. Sandubete"], "title": "Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts", "categories": ["econ.GN", "cs.AI"], "comment": "11 pages, 1 table", "summary": "Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.", "AI": {"tldr": "本文综述并组织了关于经济时间序列的可解释人工智能(XAI)文献，并提出了一种分类方法。", "motivation": "在计算经济学中，机器学习预测模型可以优于经典计量经济学模型，但由于难以审计且不易用于政策制定，因此需要可解释的人工智能技术。", "method": "根据解释机制和时间序列兼容性对XAI方法进行分类；综述了适应时间序列的具体方案，并讨论了因果推理与政策分析的连接及内在可解释架构的应用建议。", "result": "提出了一个系统性的XAI方法分类体系，结合时间序列特点改进标准解释技术，并提供了应用于决策级应用如现在预测、压力测试和制度监控的指导原则。", "conclusion": "本文通过提出一种全面的方法论框架为经济时间序列中的XAI研究提供了新的视角和实践指南。"}}
{"id": "2512.12503", "pdf": "https://arxiv.org/pdf/2512.12503", "abs": "https://arxiv.org/abs/2512.12503", "authors": ["Mingrui Ye", "Chanjin Zheng", "Zengyi Yu", "Chenyu Xiang", "Zhixue Zhao", "Zheng Yuan", "Helen Yannakoudakis"], "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.", "AI": {"tldr": "本文介绍了KidsArtBench，一个用于评估儿童艺术作品的多维度基准，以及一种基于属性特定训练的方法来提高模型对艺术评价的能力。", "motivation": "现有的多模态大语言模型在视觉和语言任务上表现出色，但在评估艺术表达方面仍有限制。本文旨在通过引入新的基准数据集KidsArtBench及相应方法来提升这一能力。", "method": "本文提出了一种基于属性特定训练的方法，使用多维度注释并通过回归感知微调（RAFT）对每个属性进行针对性的调整以提高模型预测准确性。", "result": "实验结果显示，在Qwen2.5-VL-7B上该方法显著提升了模型评估儿童艺术作品的能力，特别是在感知维度上的改进较为明显。", "conclusion": "本文通过KidsArtBench基准和相应的训练方法证明了教育者一致监督与属性意识培训对于生成有意义的艺术评价的重要性，并为持续进步提供了坚实基础。"}}
{"id": "2512.12501", "pdf": "https://arxiv.org/pdf/2512.12501", "abs": "https://arxiv.org/abs/2512.12501", "authors": ["Dang Phuong Nam", "Nguyen Kieu", "Pham Thanh Hieu"], "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation", "categories": ["cs.AI"], "comment": null, "summary": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.", "AI": {"tldr": "SafeGen是一个嵌入了伦理保障的文本到图像生成框架，旨在解决AI生成内容中的道德问题。", "motivation": "由于文本到图像系统如DALL.E、Stable Diffusion和Midjourney在创造性表达、教育和研究中提供了巨大机会的同时，也带来了放大社会偏见、产生高保真误导信息以及侵犯知识产权等伦理风险，因此需要嵌入伦理保障来解决这些问题。", "method": "SafeGen包括两个互补组件：BGE-M3文本分类器用于过滤有害或误导性提示，Hyper-SD优化的扩散模型生成高质量图像。该框架基于一个精心策划的多语言（英语和越南语）数据集，并采用公平意识训练过程，以实现创意自由与伦理责任之间的平衡。", "result": "实验结果显示，Hyper-SD在IS、FID和SSIM指标上分别为3.52、22.08和0.79；BGE-M3达到了0.81的F1评分。消融研究验证了领域特定微调对两个模块的重要性。", "conclusion": "SafeGen通过阻止不安全提示、生成包容性教学材料以及增强学术诚信，展示了其在实践中的影响，并证明了创意自由与伦理责任可以在一个工作流程中得到统一。"}}
{"id": "2512.12500", "pdf": "https://arxiv.org/pdf/2512.12500", "abs": "https://arxiv.org/abs/2512.12500", "authors": ["Xuhai Xu", "Haoyu Hu", "Haoran Zhang", "Will Ke Wang", "Reina Wang", "Luis R. Soenksen", "Omar Badri", "Sheharbano Jafry", "Elise Burger", "Lotanna Nwandu", "Apoorva Mehta", "Erik P. Duhaime", "Asif Qasim", "Hause Lin", "Janis Pereira", "Jonathan Hershon", "Paulius Mui", "Alejandro A. Gru", "Noémie Elhadad", "Lena Mamykina", "Matthew Groh", "Philipp Tschandl", "Roxana Daneshjou", "Marzyeh Ghassemi"], "title": "Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a \"double-edged sword\" in medical AI and informing future human-AI collaborative system design.", "AI": {"tldr": "研究探讨了可解释人工智能（XAI）在皮肤病学中的影响，通过两个大规模实验比较了不同用户群体对XAI的反应。", "motivation": "随着人工智能在医疗领域的普及，其不透明性成为与人类交互的主要障碍。本研究旨在探索可解释的人工智能技术如何改善诊断准确性，并且分析这些技术对医生和普通公众的不同影响。", "method": "结合基于公平性的诊断AI模型和不同的XAI解释，进行两项大规模实验（623名普通人；153位初级护理医师），评估了不同类型的XAI工具在提高诊断准确性方面的效果。", "result": "结果显示，当人工智能辅助均衡考虑皮肤色调时，可以改善诊断准确性和减少种族差异。但是，大型语言模型的解释对普通用户产生了自动化偏差：即当AI正确时准确性增加，错误时准确性下降；而经验丰富的医生则表现出更强的适应性。", "conclusion": "研究结果表明XAI技术的影响因专业知识水平和提示时机的不同而有所不同，强调了在医疗领域设计人机协作系统时需要特别注意这些问题。"}}
{"id": "2512.12498", "pdf": "https://arxiv.org/pdf/2512.12498", "abs": "https://arxiv.org/abs/2512.12498", "authors": ["Tasweer Ahmad", "Arindam Sikdar", "Sandip Pradhan", "Ardhendu Behera"], "title": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care.", "AI": {"tldr": "提出了一种基于补丁驱动关系细化的新型缓存自适应方法，以提高少样本图像分类的效果。", "motivation": "现有的缓存自适应方法（如Tip-Adapter）在处理有限监督和视觉域迁移问题时存在不足，需要改进以更好地适应低数据环境下的特定领域任务。", "method": "引入了一种基于关系门控图注意力网络的补丁驱动关系细化技术。该方法构建了补丁图形并通过边感知注意机制强调重要的跨补丁交互作用，生成上下文丰富的补丁嵌入，并通过多聚合池化组成紧凑的任务区分表示。训练阶段利用此改进结构来增强缓存中的相关信息。", "result": "在11个基准测试中表现出色，相比现有的CLIP适配器和基于缓存的方法一致地取得了更好的性能，同时保持了零样本效率。", "conclusion": "该方法通过引入补丁驱动关系细化技术，在不增加推理成本的情况下提升了少样本图像分类的准确性，并在战场上受伤士兵识别任务中得到验证。"}}
{"id": "2512.12492", "pdf": "https://arxiv.org/pdf/2512.12492", "abs": "https://arxiv.org/abs/2512.12492", "authors": ["Shengkai Xu", "Hsiang Lun Kao", "Tianxiang Xu", "Honghui Zhang", "Junqiao Wang", "Runmeng Ding", "Guanyu Liu", "Tianyu Shi", "Zhenyu Yu", "Guofeng Pan", "Ziqian Bi", "Yuqi Ouyang"], "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.", "AI": {"tldr": "提出了一种自适应检测验证框架，以在零样本条件下改善结肠息肉的检测。", "motivation": "现有的结肠息肉检测器在实验室条件与临床实践中的成像质量差异较大时表现不佳。", "method": "利用YOLOv11和视觉语言模型构建两阶段检测验证框架，并使用不对称的成本敏感奖励函数进行细调。", "result": "在合成降级数据集上，该方法比单独的YOLO提高了14到22个百分点的召回率，同时精度损失较小。", "conclusion": "所提出的方法实现了临床相关的开放世界结肠息肉检测，减少了误检，并改善了患者结果。"}}
{"id": "2512.12487", "pdf": "https://arxiv.org/pdf/2512.12487", "abs": "https://arxiv.org/abs/2512.12487", "authors": ["Hoang Anh Just", "Yifei Fan", "Handong Zhao", "Jiuxiang Gu", "Ruiyi Zhang", "Simon Jenni", "Kushal Kafle", "Ruoxi Jia", "Jing Shi"], "title": "More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.", "AI": {"tldr": "本文提出了PeRL-VL框架，该框架通过分别改善视觉感知和文本推理来增强视觉语言模型的长链多模态推理能力。", "motivation": "虽然强化学习从可验证奖励已扩展到视觉语言模型中以促进长链多模态推理，但这些模型仍然存在不准确的视觉提取和逻辑上不一致的思考链条问题。原因是仅监督最终答案的信号不足以解决这些问题。", "method": "PeRL-VL框架通过引入基于VLM的描述奖励来改进视觉感知，并在逻辑丰富的思考链条数据上添加文本推理SFT阶段，增强独立于视觉的连贯性和逻辑一致性。", "result": "该方法在多种多模态基准测试中提高了平均Pass@1准确率，从63.3%（Qwen2.5-VL-7B基础模型）提高到68.8%，优于标准RLVR、文本推理SFT和直接从GPT-4o进行的多模态蒸馏。", "conclusion": "PeRL-VL框架成功地提高了视觉语言模型在长链多模态任务中的性能，通过分别改善视觉感知和文本推理，解决了现有方法中存在的问题。"}}
{"id": "2512.12483", "pdf": "https://arxiv.org/pdf/2512.12483", "abs": "https://arxiv.org/abs/2512.12483", "authors": ["Lily Erickson"], "title": "Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers", "categories": ["cs.CR", "cs.AI"], "comment": "7 pages", "summary": "With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe. Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin. And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\\cite{battistello2025ecc}. These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process. In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve. Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process. It is my belief that proof-for would be equally valuable as proof-against in either of these categories. Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.", "AI": {"tldr": "研究使用现代语言模型架构破解椭圆曲线密码学中的公钥和私钥关联，并测试其反向工程公钥生成过程的能力。", "motivation": "随着机器学习和量子计算的发展，当前的加密算法如椭圆曲线密码（ECC）正面临潜在的安全威胁。探索这些现代技术在攻击现有安全协议方面的潜力是必要的。", "method": "采用跨轴变换器模型尝试反向工程公钥生成过程，并评估其对secp256r1密钥对的记忆和逆向工程技术。", "result": "未提供具体结果，但讨论了使用现代机器学习技术破解椭圆曲线密码学的潜力。", "conclusion": "研究揭示了利用现代语言模型进行加密算法破解的可能性，并对未来该领域的方向进行了展望。"}}
{"id": "2512.12477", "pdf": "https://arxiv.org/pdf/2512.12477", "abs": "https://arxiv.org/abs/2512.12477", "authors": ["Jiawen Chen", "Yanyan He", "Qi Shao", "Mengli Wei", "Duxin Chen", "Wenwu Yu", "Yanlong Zhao"], "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE", "AI": {"tldr": "该论文提出了MetaHGNIE框架，用于异构知识图中的节点重要性估计任务，通过元路径诱导的超图对比学习方法提高结构和语义信息的有效整合。", "motivation": "现有的节点重要性估计方法通常只考虑成对连接关系，忽略了多实体间的关系依赖，并且未能有效地融合结构信号与语义信号。MetaHGNIE框架旨在解决这些问题。", "method": "通过构建基于元路径序列的高阶知识图谱，利用超边捕捉多实体间的关联上下文信息；采用局部注意力聚合结构依存性，并使用配备稀疏块划分的超图变换器编码语义表示；最终通过对比学习融合跨模态嵌入。", "result": "在基准节点重要性估计数据集上的实验结果表明，MetaHGNIE优于现有的SOTA基线模型，展示了其有效建模高阶交互和跨模式对齐的能力。", "conclusion": "该框架成功地通过对比学习方法解决了结构信号与语义信号融合的问题，并显著提升了异构知识图谱中的节点重要性估计性能。"}}
{"id": "2512.12474", "pdf": "https://arxiv.org/pdf/2512.12474", "abs": "https://arxiv.org/abs/2512.12474", "authors": ["Jamsheed Mistri"], "title": "AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion", "categories": ["eess.SY", "cs.AI", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.", "AI": {"tldr": "本文提出了一种基于AI的跆拳道比赛实时踢击分类系统，旨在通过传感器融合提高评分准确性及公平性。", "motivation": "当前跆拳道比赛中存在得分争议和观众参与度低的问题。因此，研究提出结合多种传感器数据以实现更精确、动态且公正的比赛评价体系。", "method": "采用支持向量机(SVM)的机器学习管道与传感器融合技术进行踢击分类，并设计了一套基于特定技术类型的评分标准来鼓励积极进攻。", "result": "实验证明，该系统在实时踢击识别上达到了96-98%的准确率。此外还探讨了使用集成SVM和其他改进措施以进一步提升系统的准确性。", "conclusion": "通过引入AI技术可以显著改善跆拳道比赛中的评分公平性，并促进动态进攻技巧的应用及观众互动体验的增强。"}}
{"id": "2512.12471", "pdf": "https://arxiv.org/pdf/2512.12471", "abs": "https://arxiv.org/abs/2512.12471", "authors": ["Bhawana Chhaglani"], "title": "Privacy-Aware Ambient Audio Sensing for Healthy Indoor Spaces", "categories": ["cs.SD"], "comment": null, "summary": "Indoor airborne transmission poses a significant health risk, yet current monitoring solutions are invasive, costly, or fail to address it directly. My research explores the untapped potential of ambient audio sensing to estimate key transmission risk factors such as ventilation, aerosol emissions, and occupant distribution non-invasively and in real time. I develop privacy-preserving systems that leverage existing microphones to monitor the whole spectrum of indoor air quality which can have a significant effect on an individual's health. This work lays the foundation for privacy-aware airborne risk monitoring using everyday devices.", "AI": {"tldr": "利用隐私保护系统监测室内空气质量", "motivation": "当前的健康监测方案侵入性强，成本高或无法直接解决空气传播风险。研究旨在探索使用环境音频传感非入侵性地实时估算通风、气溶胶排放和占用分布等关键因素的方法，以促进健康的生活空间管理。", "method": "开发基于现有麦克风的隐私保护系统，监测整个室内空气质量谱系。", "result": "建立了利用日常设备进行空气传播风险监测的基础框架。", "conclusion": "通过使用环境音频传感技术，可以有效且非侵入性地评估和改善室内空气质量。"}}
{"id": "2512.12468", "pdf": "https://arxiv.org/pdf/2512.12468", "abs": "https://arxiv.org/abs/2512.12468", "authors": ["Tina Tian", "Xinyu Wang", "Andrew L. Orekhov", "Fujun Ruan", "Lu Li", "Oliver Kroemer", "Howie Choset"], "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback", "categories": ["cs.RO"], "comment": "6 pages, 5 figures", "summary": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.", "AI": {"tldr": "提出了一种利用视觉反馈分离多根纠缠电缆的方法，将解缆任务视为抓取和放置问题，并使用图表示法来编码电缆的状态信息。", "motivation": "自动化管理电缆的任务具有挑战性，因为电缆是可变形的且可能有多重缠绕。以往的工作主要集中在解开单个电缆上的结，而本文关注于更复杂的问题：即如何通过视觉反馈分离多根交织在一起的电缆。", "method": "提出了一种基于图表示的方法来编码和预测电缆的状态信息，并使用状态转换模型选择合适的高阶动作以优化低阶的动作执行。该方法能够有效处理电缆在操作过程中的弯曲和平直变化，从而实现电缆的解缠。", "result": "实验显示，通过感知-规划-行动循环迭代可以成功地解开电力线缆和鞋带等物品，平均成功率达到了84%。", "conclusion": "本文提出的方法为解决多根纠缠电缆的有效分离提供了新的途径，并展示了其在实际应用中的潜力。"}}
{"id": "2512.12465", "pdf": "https://arxiv.org/pdf/2512.12465", "abs": "https://arxiv.org/abs/2512.12465", "authors": ["Uriel Singer", "Yaron Lipman"], "title": "Exploring the Design Space of Transition Matching", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller \"head\" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.", "AI": {"tldr": "本文通过大规模系统研究过渡匹配框架中的头部模块设计，探讨了不同架构和训练方法对生成质量、训练效率及推断效率的影响。", "motivation": "为了使过渡匹配范式更加实用，作者希望通过系统的实验来探索最佳的设计方案，并找到最优的头部模块结构与采样方式，从而提升模型的质量和效率。", "method": "本文通过56个不同1.7B文本到图像模型的大规模实验（共进行了549次独特评估），研究了TM框架中的头部架构、训练过程以及一系列随机TM采样的影响。同时对比了MLP头部与Transformer头部的性能差异，分析了时间加权和采样频率对生成质量的影响。", "result": "结果表明，使用具有特定时间权重并以高频率采样的MLP头部的TM框架在所有指标上均达到最佳效果，而带有序列缩放且低频采样的Transformer头部则在图像美学方面表现优异。", "conclusion": "研究揭示了能够带来质量和效率显著提升的设计方案，并指出了不太可能进一步改进性能的选择。"}}
{"id": "2512.12462", "pdf": "https://arxiv.org/pdf/2512.12462", "abs": "https://arxiv.org/abs/2512.12462", "authors": ["Eray Erturk", "Maryam M. Shanechi"], "title": "Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at https://github.com/ShanechiLab/mrine", "summary": "Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.", "AI": {"tldr": "该论文提出了一种在多尺度神经活动数据中进行实时解码的方法，解决了不同时间尺度和概率分布的数据聚合问题。", "motivation": "现有非线性模型无法处理不同类型神经信号的时间尺度差异、缺失样本及不同的概率分布，导致难以实现多模式神经信号的实时解码。", "method": "开发了一个学习框架，包含多尺度编码器、多尺度动态主干和模态特定解码器。这个框架能够实现在不同时间尺度和概率分布下对多模式数据进行非线性聚合并处理缺失样本的实时递归解码。", "result": "在模拟实验及三种不同的大脑数据集中，模型能够在不同时间和概率分布的数据中整合信息，并提高目标变量的实时解码性能。与各种线性和非线性的基准方法相比，该方法表现更优。", "conclusion": "所提出的框架能够有效地处理多模式神经信号的时间尺度差异、概率分布及缺失样本问题，从而实现高精度的目标变量实时解码。"}}
{"id": "2512.12461", "pdf": "https://arxiv.org/pdf/2512.12461", "abs": "https://arxiv.org/abs/2512.12461", "authors": ["Eray Erturk", "Saba Hashemi", "Maryam M. Shanechi"], "title": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at https://github.com/ShanechiLab/CrossModalDistillation", "summary": "Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.", "AI": {"tldr": "跨模态知识蒸馏框架用于改进基于神经元放电的局部场电位(LFP)建模。", "motivation": "LFP信号在脑活动研究中具有优势，但因聚合性质难以建模。本文旨在通过跨模态知识迁移解决这一挑战，提高LFP模型预测下游任务变量的能力。", "method": "先训练多会话神经元放电信号变换器教师模型；然后将学生LFP模型的潜在表示与教师模型对齐。", "result": "蒸馏后的LFP模型在无监督和监督设置下优于单一及多会话基线，且能泛化到其他会话并保持良好性能。", "conclusion": "跨模态知识蒸馏为利用高性能神经元放电模型开发更准确的LFP模型提供了有力且可扩展的方法。"}}
{"id": "2512.12459", "pdf": "https://arxiv.org/pdf/2512.12459", "abs": "https://arxiv.org/abs/2512.12459", "authors": ["Jiachen Tao", "Benjamin Planche", "Van Nguyen Nguyen", "Junyi Wu", "Yuchun Liu", "Haoxuan Wang", "Zhongpai Gao", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Anwesa Choudhuri", "Zhenghao Zhao", "Weitai Kang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.", "AI": {"tldr": "该论文提出了一种名为Gaussian Photon Field（GPF）的连续可重用辐射函数，用于加速多视角渲染。", "motivation": "传统光子映射在多个视点下的光线传输估计计算效率低，为了提高多视角渲染速度，作者提出了GPF方法。", "method": "GPF将光子分布编码为各向异性的3D高斯原语，并通过物理追踪的光子进行初始化和训练。一旦训练完成，该字段可以沿摄像机光线执行可微辐射评估。", "result": "实验结果表明，GPF能够达到与传统方法相当的精度，并且将计算量减少了几个数量级。", "conclusion": "GPF成功地将基于物理的光子映射与神经场景表示的效率结合在一起。"}}
{"id": "2512.12448", "pdf": "https://arxiv.org/pdf/2512.12448", "abs": "https://arxiv.org/abs/2512.12448", "authors": ["James Bagrow", "Josh Bongard"], "title": "Optimized Architectures for Kolmogorov-Arnold Networks", "categories": ["cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "comment": "12 pages, 1 figure, 3 tables", "summary": "Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.", "AI": {"tldr": "该论文通过研究过量配置架构结合稀疏化的方法，旨在构建紧凑且可解释的Kolmogorov-Arnold网络（KAN），同时不牺牲准确性。", "motivation": "由于增强KAN的复杂性削弱了其吸引力所在的可解释性，因此研究如何在保持模型准确性的前提下优化架构并提高模型的可解释性和紧凑度。", "method": "通过使用差异化稀疏化技术，将架构搜索转化为端到端优化问题，同时采用过量配置架构结合稀疏化的策略来学习更加紧凑且具有更高表达力和可解释性的KAN模型。", "result": "在功能近似基准、动态系统预测以及真实世界的预测任务上展示了与现有方法相比的竞争力或优越性，并发现显著更小规模的模型。", "conclusion": "该研究提供了一条原则路径，以创造更加具有表达力和可解释性的机器学习模型，在解决科学计算中的关键矛盾方面取得了成功。"}}
{"id": "2512.12443", "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.", "AI": {"tldr": "构建了一个AI透明度框架，评估模型文档的完整性和一致性。", "motivation": "现有AI模型文档碎片化且缺乏结构一致，难以可靠地评估安全声明、数据来源和版本变化。", "method": "分析了五个前沿模型和100个Hugging Face模型卡片，开发了一个包含8个部分23个子部分的加权透明度框架，并实施了一个自动多代理流水线进行文档提取与评分。", "result": "评估50个模型的成本不到3美元，显示系统性差距。前沿实验室达到约80%合规率，多数提供商低于60%，安全关键类别存在最大不足。", "conclusion": "AI透明度框架有助于提高AI系统的安全性、可靠性和可信赖性，但仍有改进空间。"}}
{"id": "2512.12437", "pdf": "https://arxiv.org/pdf/2512.12437", "abs": "https://arxiv.org/abs/2512.12437", "authors": ["Jonathan Spraggett"], "title": "Sim2Real Reinforcement Learning for Soccer skills", "categories": ["cs.RO"], "comment": "Undergrad Thesis", "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.", "AI": {"tldr": "使用强化学习（RL）训练人形机器人的控制任务", "motivation": "传统RL方法在适应现实环境、处理复杂性和自然运动方面存在局限性，作者希望通过课程学习和对抗动作先验(AMP)技术解决这些问题", "method": "采用课程学习和对抗动作先验(AMP)技术进行强化学习，训练踢球、行走和跳跃等技能", "result": "开发的RL策略在动态性和适应性上优于先前的方法，但在从模拟到现实世界的转换中未成功", "conclusion": "尽管改进了传统方法，但当前RL方法仍难以完全适应真实场景"}}
{"id": "2512.12436", "pdf": "https://arxiv.org/pdf/2512.12436", "abs": "https://arxiv.org/abs/2512.12436", "authors": ["Bartłomiej Starosta", "Sławomir T. Wierzchoń", "Piotr Borkowski", "Dariusz Czerski", "Marcin Sydow", "Eryk Laskowski", "Mieczysław A. Kłopotek"], "title": "Rough Sets for Explainability of Spectral Graph Clustering", "categories": ["cs.LG", "cs.AI"], "comment": "24 figures, 21tables", "summary": "Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.", "AI": {"tldr": "通过借鉴粗糙集理论来改善图谱聚类的可解释性问题", "motivation": "解决图谱聚类方法应用于文本文档时结果难以向用户解释的问题，尤其是由于嵌入在谱空间中没有明显与文档内容的关系和无明确意义的内容文档的存在以及算法的随机性质导致的解释性差的问题。", "method": "引入一种新的增强解释方法，从粗糙集理论获取灵感来克服这些困难。", "result": "提高了图谱聚类结果对于用户的可解释性，尤其是改善了难以理解的情况。", "conclusion": "通过结合粗糙集理论可以有效提高基于图谱的文本文档聚类的解释性，使得用户更容易理解聚类结果。"}}
{"id": "2512.12430", "pdf": "https://arxiv.org/pdf/2512.12430", "abs": "https://arxiv.org/abs/2512.12430", "authors": ["Ke Zhang", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Endless World: Real-Time 3D-Aware Long Video Generation", "categories": ["cs.CV"], "comment": "10 pages,7 figures", "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.", "AI": {"tldr": "提出了一种实时生成无限长度、三维结构一致的视频框架。", "motivation": "解决长时间连续视频生成中保持稳定三维结构和计算效率的问题。", "method": "设计了条件自回归训练策略，结合全局三维感知注意力机制，确保物理合理性及几何一致性。", "result": "实验显示该方法能生成长时稳定的视觉上连贯的视频，并在视觉真实度与空间一致性方面表现出色。", "conclusion": "Endless World框架能够解决长时间连续视频生成中的关键挑战，展示出优于现有技术的表现。"}}
{"id": "2512.12428", "pdf": "https://arxiv.org/pdf/2512.12428", "abs": "https://arxiv.org/abs/2512.12428", "authors": ["Michael Döll", "Andreas Müller", "Bernd Ulmann"], "title": "Learning Dynamics in Memristor-Based Equilibrium Propagation", "categories": ["cs.LG", "cs.ET", "cs.NE"], "comment": null, "summary": "Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.", "AI": {"tldr": "研究了基于忆阻器的权重更新对Equilibrium Propagation训练神经网络收敛行为的影响。", "motivation": "通过非线性、忆阻驱动的权重更新来提高神经网络的训练效率和效果，克服冯·诺依曼瓶颈和内存墙问题。", "method": "使用六种忆阻模型进行电压电流回滞特性测试，并将其集成到EBANA框架中，在两个基准分类任务上评估其性能。", "result": "Equilibrium Propagation可以在非线性权重更新下实现稳健的收敛，前提是忆阻器表现出至少一个数量级的电阻范围。", "conclusion": "基于忆阻器的训练方法能够在一定的条件下有效提高神经网络的学习效率和性能。"}}
{"id": "2512.12427", "pdf": "https://arxiv.org/pdf/2512.12427", "abs": "https://arxiv.org/abs/2512.12427", "authors": ["Rudolf Reiter", "Chao Qin", "Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.", "AI": {"tldr": "本文提出了一种名为Unique的统一MPC方法，结合不同精度模型进行四旋翼飞行器的运动规划和控制。", "motivation": "许多涉及四旋翼的任务需要即时反应和长远计划。高精度模型虽然准确但计算成本高昂，低精度模型虽然速度快但是影响闭环性能。", "method": "本文提出了Unique算法，在单个优化中串接不同精度的模型：短视域内的高精度模型用于精确控制，长视域内的低精度模型用于规划。通过跨时域对齐成本、推力和体速率约束以及状态匹配等手段来保证可行性。", "result": "在模拟及真实飞行实验下，Unique在相同的计算预算中提高了闭环位置或速度追踪的性能最多75%，相比标准MPC和层次化规划者跟踪基准方法有明显改进。", "conclusion": "通过引入独特的跨精度模型串接策略，本文提出的方法能显著提升四旋翼任务中的运动规划与控制性能。"}}
{"id": "2512.12425", "pdf": "https://arxiv.org/pdf/2512.12425", "abs": "https://arxiv.org/abs/2512.12425", "authors": ["Hangwei Zhang", "Armando Teles Fortes", "Tianyi Wei", "Xingang Pan"], "title": "BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation", "categories": ["cs.CV"], "comment": null, "summary": "Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.", "AI": {"tldr": "本文提出了BokehDepth，一个两阶段框架用于改进单目深度估计。", "motivation": "现有方法在利用景深和单目深度之间的联系方面不完整。高质量的散景生成依赖于噪声大的深度图，并且现代单目深度模型仍然难以处理弱纹理、远距离和几何模糊区域。", "method": "BokehDepth通过一个物理引导的可控散景生成器，在第一阶段从单张锐利输入中产生无深度的散景堆栈。第二阶段，轻量级的散焦感知聚合模块将现有的单目深度编码器与特征融合结合在一起。", "result": "在挑战性基准上，BokehDepth提高了散景的视觉质量和深度预测的准确性。", "conclusion": "该研究引入了BokehDepth框架，显著提升了单目深度估计的质量和鲁棒性。"}}
{"id": "2512.12424", "pdf": "https://arxiv.org/pdf/2512.12424", "abs": "https://arxiv.org/abs/2512.12424", "authors": ["Tue-Thu Van-Dinh", "Hoang-Duy Tran", "Truong-Binh Duong", "Mai-Hanh Pham", "Binh-Nam Le-Nguyen", "Quoc-Thai Nguyen"], "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 4 figures, Accepted to AI4Research @ AAAI", "summary": "Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.", "AI": {"tldr": "本文提出了针对越南语信息图表的视觉问答基准ViInfographicVQA，包含单一图像和多图像任务。", "motivation": "为了评估模型在处理复杂视觉元素和布局理解上的能力，并填补低资源语言背景下的研究空白。", "method": "构建了一个包含多种主题的大量真实世界的信息图表数据集，并提出了单图问答任务和跨图问答任务。", "result": "实验显示现有模型在涉及多图像整合及非跨度推理的任务上存在较大差距，这表明了布局感知与跨图像推理的重要性。", "conclusion": "ViInfographicVQA为越南语信息图表视觉问答提供了基准测试，并揭示了当前多模态模型的局限性，鼓励未来研究探索新的方法来提升性能。"}}
{"id": "2512.12413", "pdf": "https://arxiv.org/pdf/2512.12413", "abs": "https://arxiv.org/abs/2512.12413", "authors": ["Gabriel R. Lau", "Wei Yan Low", "Louis Tay", "Ysabel Guevarra", "Dragan Gašević", "Andree Hartanto"], "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.", "AI": {"tldr": "开发并验证了一种测量用户在使用生成式人工智能时批判性思维的量表，探究其结构和关联。", "motivation": "随着生成式AI工具在日常生活中的广泛应用，用户必须具备批判性思考能力以评估AI输出的有效性和可靠性。此研究旨在理解和衡量这种批判性思维。", "method": "通过六项研究（样本量1365）开发并验证了包含13个项目的批判性思维测量表，并探索其结构和关联。包括生成、内容验证量表项目，支持三因子模型及验证等。", "result": "研究证实了该量表的三个因素结构：验证、动机与反思，展示了良好的内部一致性、重测可靠性和效度。发现批判性思考在AI使用中的得分与开放性、外向性、积极情绪特质和AI使用的频率有关联。此外，高得分者表现出更频繁且多样的验证策略以及更高的准确性。", "conclusion": "此工作澄清了人们如何对生成式人工智能输出实施监督，并提供了一种经过验证的量表和生态研究任务范例以支持关于批判性参与的研究。"}}
{"id": "2512.12411", "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "categories": ["cs.AI"], "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.", "AI": {"tldr": "本文研究了大型语言模型对注入概念的检测能力，并发现了该能力在不同任务中的表现差异。", "motivation": "为了验证Anthropic关于前沿模型能够识别并命名激活方向所代表的“概念”的说法是否成立，以及这些自我报告的真实性与稳定性。", "method": "作者首先重现了Anthropic的结果，在Meta-Llama-3.1-8B-Instruct上测试模型对注入概念的检测能力。然后通过系统性地改变推理提示来探索模型表现的变化情况，并发现了一种部分自省的状态，即模型在分类注入概念强度方面可以达到较高准确率。", "result": "实验结果表明，在特定条件下，LLM确实能够识别并命名注入的概念；然而，当任务发生变化时，这种能力会显著减弱。同时，作者还观察到模型能够在一定范围内可靠地分类注入概念的强度。", "conclusion": "研究证明了语言模型在自省过程中有效计算其内部表示的功能性特征，但这些自我报告具有狭窄性和提示敏感性的特点。"}}
{"id": "2512.12410", "pdf": "https://arxiv.org/pdf/2512.12410", "abs": "https://arxiv.org/abs/2512.12410", "authors": ["Khalfalla Awedat", "Mohamed Abidalrekab", "Mohammad El-Yabroudi"], "title": "A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.", "AI": {"tldr": "本文提出了一种基于图注意力网络的框架，用于重建掉帧激光雷达束。", "motivation": "旋转式LiDAR传感器由于硬件老化、灰尘、雪、雾或强反射等原因造成的垂直通道缺失会严重影响自动驾驶车辆的3D感知。", "method": "该方法将每个LiDAR扫描表示为一个非结构化空间图，采用多层GAT学习局部几何邻域中的自适应注意力权重并直接回归缺失的高度值。", "result": "在1,065个KITTI序列上进行训练和评估时，平均高度均方根误差（RMSE）为11.67厘米，87.98%的重建点落在10厘米误差范围内。单次GPU推理时间为14.65秒。", "conclusion": "实验结果表明，在仅利用原始点云几何信息的情况下，纯图注意力模型可以有效恢复实际传感器退化情况下的丢失垂直束。"}}
{"id": "2512.12395", "pdf": "https://arxiv.org/pdf/2512.12395", "abs": "https://arxiv.org/abs/2512.12395", "authors": ["Haowen Wang", "Xiaoping Yuan", "Fugang Zhang", "Rui Jian", "Yuanwei Zhu", "Xiuquan Qiao", "Yakun Huang"], "title": "ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States", "categories": ["cs.CV"], "comment": null, "summary": "Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.", "AI": {"tldr": "本文提出了ArtGen，一种基于条件扩散模型的框架，用于从单视图图像或文本描述生成具有准确几何结构和连贯动力学的人体工学对象。", "motivation": "现有的生成模型依赖于表示封闭状态的单一视图输入，导致几何形状与关节动态之间的纠缠，产生不明确或不现实的动力学结构。为此，本文提出了ArtGen来解决这些问题。", "method": "ArtGen利用跨状态蒙特卡洛采样以显式地强制执行全局动力学一致性，并引入Chain-of-Thought推理模块推断稳健的结构性先验，指导稀疏专家扩散变压器处理多样的动力学交互。同时采用增强局部-全局注意力的组合3D-VAE潜在先验来捕捉精细几何和整体部件关系。", "result": "实验表明ArtGen在PartNet-Mobility基准上显著优于现有方法。", "conclusion": "通过解决结构-运动纠缠问题，ArtGen能够生成准确且连贯的人体工学对象。"}}
{"id": "2512.12386", "pdf": "https://arxiv.org/pdf/2512.12386", "abs": "https://arxiv.org/abs/2512.12386", "authors": ["Swayam Bhanded"], "title": "Speedrunning ImageNet Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.", "AI": {"tldr": "本文提出了SR-DiT框架，通过集成多种技术优化扩散变换器的训练效率，并在ImageNet上取得了优越的结果。", "motivation": "现有方法虽然改进了扩散变换器的训练效率，但缺乏对多技术结合潜力的研究。作者旨在探索多种技术结合的可能性及其效果。", "method": "本文提出了一种系统框架SR-DiT，在扩散变换器的基础上整合令牌路由、架构优化和训练调整等技术，并通过大量消融研究识别最有效的技术组合。", "result": "使用140M参数的模型在400K次迭代内达到FID 3.49和KDD 0.319的结果，与685M参数更长时间训练的模型相当。", "conclusion": "该框架展示了小型模型结合多技术优化后能取得接近大型模型的效果，并为未来研究提供了基准。"}}
{"id": "2512.12381", "pdf": "https://arxiv.org/pdf/2512.12381", "abs": "https://arxiv.org/abs/2512.12381", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems", "categories": ["cs.AI"], "comment": "18 pages, 5 figures", "summary": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly. We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale. We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process. By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems. \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis", "AI": {"tldr": "研究智能系统中的熵崩溃现象，提出其为一种普遍的动态失效模式，并分析其背后的机制和影响。", "motivation": "探讨为何随着学习、协调与优化，智能系统反而可能变得更僵化、失去适应性，表现出意外故障。试图通过理论框架解释这一反常识的现象。", "method": "在最小化的跨领域无偏见假设下，建立理论模型来展示智能系统的熵崩溃过程，并通过模拟验证其普遍性和机制。", "result": "识别并定义了熵崩溃为智能系统中的一种动态失效模式，即从高熵适应阶段过渡到低熵僵化阶段。该框架能解释AI中的模型崩溃、经济学中的机构硬化以及进化生物学中的遗传瓶颈等问题。", "conclusion": "将熵崩溃现象视为智能系统的结构性代价，并指出现有干预措施在晚期通常无效的原因，从而建议设计熵意识的原理来保持长期适应性。"}}
{"id": "2512.12378", "pdf": "https://arxiv.org/pdf/2512.12378", "abs": "https://arxiv.org/abs/2512.12378", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Yizhuo Yang", "Xinyuan Cui", "Jiarui Zhang", "Lihua Xie", "Jianfei Yang", "Chris Xiaoxuan Lu", "Fangqiang Ding"], "title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.", "AI": {"tldr": "本文介绍了M4Human，这是一个用于人体网格重建的大规模多模态毫米波雷达数据集。", "motivation": "现有的大型HMR数据集依赖于视线RGB输入，存在遮挡、光照变化和隐私问题。为解决这些问题，引入了新的雷达数据集以提供更全面的人体运动捕捉和环境互动研究。", "method": "M4Human是一个大规模（661K帧）的数据集，包含高分辨率毫米波雷达、RGB和深度图像。它提供了原始雷达张量和处理后的雷达点云，并包括高质量的动作捕捉注释。", "result": "通过基准测试结果，证明了M4Human数据集对于基于毫米波雷达的人体网格重建的重要性，同时揭示了在快速不受限运动下的挑战。", "conclusion": "M4Human是目前最大的多模态人体网格重建数据集，为研究者提供了丰富的资源，并推动了该领域的进一步发展。"}}
{"id": "2512.12377", "pdf": "https://arxiv.org/pdf/2512.12377", "abs": "https://arxiv.org/abs/2512.12377", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset", "categories": ["cs.RO"], "comment": null, "summary": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.", "AI": {"tldr": "本文介绍了INDOOR-LIDAR，一个包含模拟与真实世界点云数据的机器人感知综合混合数据集。", "motivation": "现有室内LiDAR数据集存在规模有限、标注格式不一致及人为变化等问题。INDOOR-LIDAR通过结合模拟环境和使用自主地面机器人获取的真实扫描来解决这些问题，提供了一致覆盖及现实传感器行为。", "method": "数据集包含密集的点云数据，配以强度测量和KITTI风格注释。模拟部分可灵活配置布局、点密度及遮挡情况；真实部分捕捉真实的传感器噪声与特定领域的特征。", "result": "INDOOR-LIDAR适用于3D物体检测、鸟瞰图感知、SLAM、语义场景理解以及模拟和真实环境之间的领域适应性任务。它作为机器人在复杂室内环境中感知的基准。", "conclusion": "通过填补合成数据与现实世界的差距，INDOOR-LIDAR为研究提供了可扩展、真实的测试平台。"}}
{"id": "2512.12375", "pdf": "https://arxiv.org/pdf/2512.12375", "abs": "https://arxiv.org/abs/2512.12375", "authors": ["Hyunkoo Lee", "Wooseok Jang", "Jini Yang", "Taehwan Kim", "Sangoh Kim", "Sangwon Jung", "Seungryong Kim"], "title": "V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping", "categories": ["cs.CV"], "comment": "Project Page: https://cvlab-kaist.github.io/V-Warper", "summary": "Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.", "AI": {"tldr": "V-Warper是一种无需额外视频训练的轻量级框架，用于增强基于变压器的视频扩散模型的身份一致性。", "motivation": "现有方法依赖于大规模视频数据集和重视频微调，计算成本高且难以扩展。同时，在帧间保持细粒度外观一致方面表现不佳。", "method": "V-Warper包括两步：第一步是粗略的外观适应阶段，仅使用少量参考图像编码全局主体身份；第二步是在推断时细化视觉保真度通过计算语义对应关系来指导价值表征在生成过程中的变形。", "result": "V-Warper显著提高了外观保真度，同时保持了指令对齐和运动动态，并且无需大规模视频微调就实现了这些提升。", "conclusion": "V-Warper展示了如何通过简单的图像参考集和个人身份适应来提高基于Transformer的视频扩散模型的身份一致性。"}}
{"id": "2512.12372", "pdf": "https://arxiv.org/pdf/2512.12372", "abs": "https://arxiv.org/abs/2512.12372", "authors": ["Peixuan Zhang", "Zijian Jia", "Kaiqi Liu", "Shuchen Weng", "Si Li", "Boxin Shi"], "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative", "categories": ["cs.CV"], "comment": null, "summary": "While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.", "AI": {"tldr": "STAGE是一种基于故事板的多镜头视频生成方法，旨在提高叙事连贯性和电影语言。", "motivation": "当前的视频合成技术虽然在视觉保真度方面取得了显著进步，但在创建连贯的多镜头叙事上仍面临挑战。为此，提出了STAGE来解决跨镜头一致性问题并提升电影语言的表现力。", "method": "STAGE通过预测每个镜头中的开始和结束帧对构成的故事板，并采用多重记忆包确保长期实体一致性和双编码策略保证内部镜头连贯性。此外，还引入了两阶段训练方案学习影视间的过渡。", "result": "实验显示，STAGE在结构化叙事控制和跨镜头一致性上表现出色。", "conclusion": "STAGE通过故事板锚定方法显著提升了多镜头视频生成的质量，在保持高视觉保真度的同时实现了更连贯的叙事效果。"}}
{"id": "2512.12367", "pdf": "https://arxiv.org/pdf/2512.12367", "abs": "https://arxiv.org/abs/2512.12367", "authors": ["Shuyang Xie", "Jie Zhou", "Jun Wang", "Renjing Xu"], "title": "JPEG-Inspired Cloud-Edge Holography", "categories": ["physics.optics", "cs.CV"], "comment": null, "summary": "Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.", "AI": {"tldr": "本文提出了一种高效的云边缘全息图处理管道，旨在解决近眼显示设备中的计算和能量消耗限制。", "motivation": "深度学习在计算机生成的全息图中取得了显著进展，但直接部署于眼镜式设备上受限于计算与能耗。云卸载后的图像编码传输方法易失真且需要高带宽，神经压缩虽能减少带宽却增加边缘端延迟和硬件需求。", "method": "该研究引入JPEG启发式的云边缘全息图处理方案，通过学习型变换编解码器保留了JPEG块结构化特性。将所有重负载的神经处理工作转移到云端执行，而边缘设备仅进行轻量级解码操作，不涉及任何神经推断。", "result": "该方法在小于2比特每像素下实现了高达32.15分贝峰值信噪比，并且最低解码延迟仅为4.2毫秒。数值模拟和光学实验验证了全息图的高质量重建效果。", "conclusion": "通过将计算机生成的全息图与保持JPEG结构效率并扩展其可学习组件的编解码器结合，该框架允许低延时、带宽高效的全息图流传输，在资源受限的可穿戴设备上实现无需神经解码器或专用硬件的支持。"}}
{"id": "2512.12366", "pdf": "https://arxiv.org/pdf/2512.12366", "abs": "https://arxiv.org/abs/2512.12366", "authors": ["Babak Badnava", "Jacob Chakareski", "Morteza Hashemi"], "title": "ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems", "categories": ["cs.IT", "cs.LG", "eess.IV"], "comment": "Submitted to ACM TOMM", "summary": "Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.", "AI": {"tldr": "弹性VR系统通过多用户多连接架构实现可扩展的360视频流任务计算，采用多智能体深度强化学习方法优化用户体验和能耗。", "motivation": "新兴VR应用需要大量计算资源和数据率来处理高保真360度视频内容，现有的云计算模型无法适应这种需求。本文提出一种弹性VR计算框架以提高系统性能和用户质量。", "method": "论文提出了两种多智能体深度强化学习解决方案：集中训练集中执行的CPPG以及集中训练分散执行的IPPG来优化用户的通信、计算需求及能源消耗，从而实现用户体验与能耗的最佳化。", "result": "实验表明，弹性VR框架相比于无弹性计算方案在峰值信噪比上提高了43.21%，同时减少了响应时间和能量消耗分别达到了42.35%和56.83%。", "conclusion": "通过研究，论文展示了多用户环境下基于多智能体深度强化学习的弹性VR任务计算方法能够有效提高系统的性能和用户体验。"}}
{"id": "2512.12360", "pdf": "https://arxiv.org/pdf/2512.12360", "abs": "https://arxiv.org/abs/2512.12360", "authors": ["Yufei Yin", "Qianke Meng", "Minghao Chen", "Jiajun Ding", "Zhenwei Shao", "Zhou Yu"], "title": "VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.", "AI": {"tldr": "VideoARM旨在通过自适应代理推理和分层记忆构建，提升长视频理解能力。", "motivation": "现有的长视频理解方法受限于手工设计的推理流程或消耗大量令牌的预处理步骤。为解决这些问题，提出了一种新的框架以降低令牌消耗并增强自主推理能力。", "method": "VideoARM通过一个动态循环来观察、思考、行动和记忆，在此过程中逐步构建分层多模态记忆，并减少令牌使用量。", "result": "实验表明，与最先进方法相比，VideoARM在长视频理解上表现更好且显著减少了令牌消耗。", "conclusion": "该框架通过自适应代理推理及连续更新的分层内存机制有效提升了长视频的理解性能。"}}
{"id": "2512.12357", "pdf": "https://arxiv.org/pdf/2512.12357", "abs": "https://arxiv.org/abs/2512.12357", "authors": ["Zishen Song", "Yongjian Zhu", "Dong Wang", "Hongzhan Liu", "Lingyu Jiang", "Yongxing Duan", "Zehua Zhang", "Sihan Li", "Jiarui Li"], "title": "TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection", "categories": ["cs.CV"], "comment": null, "summary": "Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.", "AI": {"tldr": "TCLeaf-Net是一种结合了Transformer和卷积的混合检测器，旨在解决田间作物叶片病害检测中的复杂背景干扰、信息损失及病变尺度变化等问题。", "motivation": "为了提高作物生长的安全性和产量减少，本研究提出了一个新的配对叶级数据集Daylily-Leaf，并开发了一种新的TCLeaf-Net框架来应对复杂背景、领域偏移和有限的病变级数据集所带来的挑战。", "method": "TCLeaf-Net采用Transformer-卷积模块（TCM）结合全局上下文与局部保护卷积抑制非叶片区域；使用原始尺度特征召回和采样(RSFRS)块结合双线性重采样和卷积来保留精细的空间细节；以及利用具有FPN的可变形对齐块(DFPN)，通过基于偏移的对齐和多感知域视野增强多尺度融合。", "result": "实验结果表明，TCLeaf-Net在Daylily-Leaf数据集中的田间分割上比基线模型提高了5.4个百分点的mAP@50值，达到78.2%，同时减少了计算量和GPU内存使用。此外，在PlantDoc、Tomato-Leaf和Rice-Leaf数据集上的表现也优于近期的YOLO和RT-DETR系列。", "conclusion": "TCLeaf-Net框架通过解决复杂背景干扰、信息损失及病变尺度变化等问题，展示了其在田间作物叶片病害检测中的优越性能和广泛适用性。"}}
{"id": "2512.12356", "pdf": "https://arxiv.org/pdf/2512.12356", "abs": "https://arxiv.org/abs/2512.12356", "authors": ["Yueshen Li", "Krishnaveni Unnikrishnan", "Aadya Agrawal"], "title": "Tacit Understanding Game (TUG): Predicting Interpersonal Compatibility", "categories": ["cs.HC"], "comment": null, "summary": "Research on relationship quality often relies on lengthy questionnaires or invasive textual corpora, limiting ecological validity and user privacy. We ask whether a sequence of single-word choices made in a playful setting can reveal personality and predict interpersonal compatibility. We introduce the Tacit Understanding Game (TUG), a two-player online word association game. We collect word choice traces, annotate a subset with psychological ground truth scales, and bootstrap a larger synthetic corpus via large language model simulation. TUG demonstrates that minimal, privacy preserving signals can support relationship matching, offering new design space for social platforms.", "AI": {"tldr": "Tacit Understanding Game（TUG）是一种两个人在线玩的单词关联游戏，旨在通过简单的单词选择预测人际关系兼容性。", "motivation": "研究关系质量通常依赖于长时间问卷或侵入性的文本数据集，这限制了生态有效性和用户隐私。该论文探讨一种更简单、更隐私的方式来揭示个性和预测人际兼容性的方法。", "method": "TUG通过收集单词选择痕迹，并使用心理量表进行注解以及用大型语言模型模拟生成更大的合成语料库来完成实验。", "result": "TUG表明，即使是最小的、保护隐私的信息信号也可以支持关系匹配。", "conclusion": "Tacit Understanding Game提供了一种新的设计空间，可以用于社交平台的人际关系匹配。"}}
{"id": "2512.12348", "pdf": "https://arxiv.org/pdf/2512.12348", "abs": "https://arxiv.org/abs/2512.12348", "authors": ["Xin Sun", "Rongjun Ma", "Shu Wei", "Pablo Cesar", "Jos A. Bosch", "Abdallah El Ali"], "title": "Understanding Trust Toward Human versus AI-generated Health Information through Behavioral and Physiological Sensing", "categories": ["cs.HC"], "comment": null, "summary": "As AI-generated health information proliferates online and becomes increasingly indistinguishable from human-sourced information, it becomes critical to understand how people trust and label such content, especially when the information is inaccurate. We conducted two complementary studies: (1) a mixed-methods survey (N=142) employing a 2 (source: Human vs. LLM) $\\times$ 2 (label: Human vs. AI) $\\times$ 3 (type: General, Symptom, Treatment) design, and (2) a within-subjects lab study (N=40) incorporating eye-tracking and physiological sensing (ECG, EDA, skin temperature). Participants were presented with health information varying by source-label combinations and asked to rate their trust, while their gaze behavior and physiological signals were recorded. We found that LLM-generated information was trusted more than human-generated content, whereas information labeled as human was trusted more than that labeled as AI. Trust remained consistent across information types. Eye-tracking and physiological responses varied significantly by source and label. Machine learning models trained on these behavioral and physiological features predicted binary self-reported trust levels with 73% accuracy and information source with 65% accuracy. Our findings demonstrate that adding transparency labels to online health information modulates trust. Behavioral and physiological features show potential to verify trust perceptions and indicate if additional transparency is needed.", "AI": {"tldr": "研究通过行为和生理传感来理解人们对人类与AI生成的健康信息的信任度差异。", "motivation": "随着在线上AI生成的健康信息日益增多，变得难以区分于人类来源的信息，了解人们如何信任并标注这些内容特别是当信息不准确时至关重要。", "method": "研究进行了两项互补性研究：一项混合方法调查（N=142），采用2（来源：人与LLM）×2（标签：人与AI）×3（类型：一般、症状、治疗）的设计，另一项在实验室中的实验性研究（N=40），包括眼动追踪和生理传感（ECG, EDA, 皮肤温度）。参与者被呈现不同来源-标签组合的健康信息，并对其信任度进行评价，同时记录其凝视行为和生理信号。", "result": "发现LLM生成的信息比人类生成的内容更受信任，而标注为人的信息则比标记为AI的信息更值得信赖。信任度在不同类型的信息之间保持一致。眼动追踪和生理反应根据来源和标签显著不同。机器学习模型基于这些行为和生理特征预测二元自我报告的信任水平准确率达73%，预测信息源准确率为65%。", "conclusion": "研究结果表明，将透明度标签添加到在线健康信息中可以调节信任程度。行为和生理特性显示了验证信任感知并判断是否需要额外透明性方面的潜力。"}}
{"id": "2512.12339", "pdf": "https://arxiv.org/pdf/2512.12339", "abs": "https://arxiv.org/abs/2512.12339", "authors": ["Maurya Goyal", "Anuj Singh", "Hadi Jamali-Rad"], "title": "Unified Control for Inference-Time Guidance of Denoising Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe", "AI": {"tldr": "UniCoDe算法结合了采样方法和梯度导向方法的优点，以提高扩散模型在推理时的性能。", "motivation": "为了改进任务特定性能，将扩散模型输出与下游目标对齐至关重要。作者希望通过整合两种主要策略的优势来解决当前方法的不足：一种是基于采样的方法，另一种是基于梯度的方法。", "method": "UniCoDe算法在采样过程中整合局部梯度信号，从而克服了复杂奖励基线采样方法效率低下的问题。通过融合这两种范式，该方法实现了更高效的抽样，并提供了更好的回报对齐和扩散无条件先验之间的分歧的权衡。", "result": "实验证明UniCoDe在各种任务上与最先进的基准保持竞争力。", "conclusion": "提出了一种名为UniCoDe的新算法，它结合了采样方法和梯度导向方法的优点，实现了更高效的抽样并提供了更好的回报对齐效果。"}}
{"id": "2512.12337", "pdf": "https://arxiv.org/pdf/2512.12337", "abs": "https://arxiv.org/abs/2512.12337", "authors": ["Yushen Fang", "Jianjun Li", "Mingqian Ding", "Chang Liu", "Xinchi Zou", "Wenqi Yang"], "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.", "AI": {"tldr": "提出了一种自校正迭代精炼框架（SCIR）用于增强基于模式的信息抽取。", "motivation": "为了克服现有信息抽取系统中的高训练成本和与大语言模型偏好对齐的困难，提出了SCIR框架及其多任务双语自我校正数据集。", "method": "通过设计双重路径自校正模块和支持反馈驱动优化，实现插件兼容性，并引入了包含10万多条记录的数据集解决偏好对齐问题。", "result": "实验结果表明，SCIR在实体识别、关系抽取和事件抽取上优于现有技术5.27％的平均Micro-F1提升率，并降低了87％的训练成本。", "conclusion": "这些改进不仅提高了信息抽取系统的灵活性和准确性，还为轻量高效的IE模式铺平了道路。"}}
{"id": "2512.12332", "pdf": "https://arxiv.org/pdf/2512.12332", "abs": "https://arxiv.org/abs/2512.12332", "authors": ["Saad Alqithami"], "title": "Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks", "categories": ["cs.SI", "cs.AI", "cs.CR", "cs.IT"], "comment": null, "summary": "The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.", "AI": {"tldr": "本文研究了同质性、记忆限制和对抗破坏如何共同塑造复杂网络的弹性和适应性。", "motivation": "探讨在复杂的网络环境中，如何通过整合显式记忆衰减机制到基于同质性的模型中来提高网络的弹性和适应性。", "method": "采用合成数据集进行系统实验，改变遗忘函数、重新连接概率和相似度测量方法，并对比余弦相似度与传统Jaccard相似度等指标。", "result": "结果表明，在稀疏、凸型和模块化网络中，使用余弦相似性可以提高稳定性的30%，并验证了价值回溯策略对增强网络弹性和适应性的有效性。", "conclusion": "研究强调了在网络结构和对抗动态下调整记忆参数的重要性，并为实际应用提供了可操作的见解。"}}
{"id": "2512.12324", "pdf": "https://arxiv.org/pdf/2512.12324", "abs": "https://arxiv.org/abs/2512.12324", "authors": ["Meilin Li", "Ji He", "Jia Xu", "Shanzhe Lei", "Yan Teng", "Yingchun Wang", "Xuhong Wang"], "title": "UniMark: Artificial Intelligence Generated Content Identification Toolkit", "categories": ["cs.CR", "cs.AI"], "comment": "5 Pages", "summary": "The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \\textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \\emph{Hidden Watermarking} for copyright protection and \\emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.", "AI": {"tldr": "UniMark是一个用于识别和治理人工智能生成内容的开放源码统一框架。", "motivation": "针对AI生成内容带来的信任危机以及现有工具存在的碎片化问题，提出了一种新型的内容识别与标记方案。", "method": "采用模块化设计，支持文本、图像、音频、视频等多种模态；提出了隐式水印和显式标记的双操作策略，并建立了标准化评估框架。", "result": "该工具包可以有效地进行AI生成内容的版权保护和合规性管理，促进透明安全的数字生态环境建设。", "conclusion": "UniMark填补了现有工具在统一治理方面的空白，为解决AI生成内容问题提供了新思路。"}}
{"id": "2512.12320", "pdf": "https://arxiv.org/pdf/2512.12320", "abs": "https://arxiv.org/abs/2512.12320", "authors": ["Canqi Meng", "Weibang Bai"], "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy", "categories": ["cs.RO"], "comment": null, "summary": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.", "AI": {"tldr": "研究通过在多孔泡沫体上刻制特定图案来实现软致动器的可编程变形。", "motivation": "传统软气压致动器结构支持弱，需要昂贵的特定几何重设计以实现多功能性。引入多孔材料虽能提供结构性支撑，但缺乏直接对多孔主体进行可编程变形的方法。", "method": "提出了一种通过在泡沫体内刻制特定图案来引导材料在全局真空输入下发生局部结构各向异性变形的设计方法，并使用有限元分析建立计算模型以研究刻痕图案化机制。", "result": "实验表明，最优设计的致动器能实现最大弯曲角度80度、倾斜角度18度和扭转角度115度。展示了该方法的通用性及通过模塑无快速原型制造复杂设计的能力。", "conclusion": "这项工作提供了一种新的、高效的、可扩展的设计多功能软机器人新范式。"}}
{"id": "2512.12309", "pdf": "https://arxiv.org/pdf/2512.12309", "abs": "https://arxiv.org/abs/2512.12309", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \\ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.", "AI": {"tldr": "本文提出了一种名为WeDetect的模型家族，用于快速开放词汇对象检测。", "motivation": "通过将识别视为检索问题，即在共享嵌入空间中匹配区域与文本查询的方法可以提供更高的效率和通用性。", "method": "该方法采用非融合双塔架构，在训练过程中使用精心整理的数据集。此外，提出WeDetect-Uni作为通用提议生成器，并利用LMM处理复杂引用表达式理解任务。", "result": "在15个基准测试中，WeDetect模型家族取得了最先进的性能和高推理效率。", "conclusion": "通过检索框架的统一性，WeDetect实现了高效且高性能的对象检测、提案生成、对象检索和引用表达式理解。"}}
{"id": "2512.12307", "pdf": "https://arxiv.org/pdf/2512.12307", "abs": "https://arxiv.org/abs/2512.12307", "authors": ["Benjamin Beilharz", "Thomas S. A. Wallis"], "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding", "categories": ["cs.CV", "cs.GR"], "comment": "18 pages, 6 figures. Supplementary material and code will be provided at the end of January", "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.", "AI": {"tldr": "本文提出了MRD方法，利用基于物理的可微渲染技术探查视觉模型对生成性三维场景属性的理解。", "motivation": "尽管深度学习在许多视觉基准上取得了成功，但要理解和解释这些模型的表现和决策仍然具有挑战性。虽然视觉模型通常是在二维输入上进行训练的，但它们往往被认为形成了隐含的三维场景表示（例如，在部分遮挡或相对深度推理方面表现出容忍度）。", "method": "MRD方法利用基于物理的可微渲染技术来寻找产生相同模型激活的不同物理场景参数。与以前的像素基方法不同，这些重建结果始终基于物理场景描述。通过这种方式可以探查模型对物体形状、材料属性等敏感性或不变性。", "result": "实验表明，在目标和优化场景之间模型激活具有高度相似性，而视觉结果各异。定性的重建有助于调查模型对哪些物理场景属性是敏感的或不变的。", "conclusion": "MRD方法为理解计算机和人类视觉提供了可能，能够分析物理场景参数如何影响模型响应的变化"}}
{"id": "2512.12303", "pdf": "https://arxiv.org/pdf/2512.12303", "abs": "https://arxiv.org/abs/2512.12303", "authors": ["Yang Ou", "Xiongwei Zhao", "Xinye Yang", "Yihan Wang", "Yicheng Di", "Rong Yuan", "Xieyuanli Chen", "Xu Zhu"], "title": "OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation", "categories": ["cs.CV"], "comment": "Submitted to TMM", "summary": "Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.", "AI": {"tldr": "本文提出了OMUDA，一种用于语义分割的无监督领域适应方法。", "motivation": "现有无监督领域适应方法在处理跨域上下文模糊、特征表示不一致和伪标签噪声时效果不佳。为解决这些问题，作者提出了一种多层级掩膜策略来减少这些领域的偏差。", "method": "OMUDA框架包括三个层次的掩膜策略：上下文感知掩膜（CAM），特征蒸馏掩膜（FDM）以及类别解耦掩膜（CDM）。这三个策略分别在全局与局部、稳健且一致的特征学习以及降低伪标签噪声方面发挥作用。", "result": "实验结果表明，OMUDA方法在SYNTHIA->Cityscapes和GTA5->Cityscapes等任务上表现出色，能够被无缝集成到现有无监督领域适应方法中，并持续获得最先进的结果，平均提升7%。", "conclusion": "本文提出的OMUDA框架通过引入多层次掩膜策略有效解决了语义分割中的跨域适应问题，在多个基准测试中取得了显著的性能改进。"}}
{"id": "2512.12302", "pdf": "https://arxiv.org/pdf/2512.12302", "abs": "https://arxiv.org/abs/2512.12302", "authors": ["Huan Zheng", "Yucheng Zhou", "Tianyi Yan", "Jiayi Su", "Hongjun Chen", "Dubing Chen", "Wencheng Han", "Runzhou Tao", "Zhongying Qiu", "Jianfei Yang", "Jianbing Shen"], "title": "From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": null, "summary": "Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.", "AI": {"tldr": "本文介绍了Intention-Drive，一个用于评估自动驾驶系统将人类意图转化为行动能力的基准。", "motivation": "当前的端到端自动驾驶系统仅能执行低级指令，缺乏理解并实现高级抽象的人类意图的能力。为了推动这一领域的进步，需要一种标准化的方法来衡量这种复杂任务的表现。", "method": "Intention-Drive引入了一个新的数据集，包含复杂的场景以及对应的自然语言意图，并设计了一种基于意图成功率（ISR）的评估协议。", "result": "通过在Intention-Drive上对一系列基准模型进行广泛的评估，发现这些模型在全面理解和实现人类意图方面存在显著差距。", "conclusion": "现有的自动驾驶系统难以理解并执行高级抽象的人类意图，引入Intention-Drive将有助于推动这一领域的发展。"}}
{"id": "2512.12296", "pdf": "https://arxiv.org/pdf/2512.12296", "abs": "https://arxiv.org/abs/2512.12296", "authors": ["Hyunju Lee", "Youngmin Oh", "Jeimin Jeon", "Donghyeon Baek", "Bumsub Ham"], "title": "GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to WACV 2026", "summary": "Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods", "AI": {"tldr": "提出了一种渐进式训练框架GrowTAS，用于搜索高效的Vision Transformer架构。", "motivation": "现有Transformer架构搜索方法中，所有子网络共享相同的权重会导致干扰并降低小型网络的表现。因此，作者认为良好预训练的小型网络可以作为大型网络的基础，并提出了逐步引入更大网络的渐进式训练框架。", "method": "GrowTAS从训练小模型开始，逐渐融合大模型以减少干扰。此外，还介绍了GrowTAS+，通过微调部分权重来进一步提高大型子网的表现。", "result": "在ImageNet和多个迁移学习基准上（包括CIFAR-10/100，Flowers，Cars和INAT-19），实验结果表明该方法优于当前的架构搜索方法。", "conclusion": "GrowTAS框架通过减少干扰实现了更稳定高效的训练过程，并在多种数据集上验证了其优越性。"}}
{"id": "2512.12288", "pdf": "https://arxiv.org/pdf/2512.12288", "abs": "https://arxiv.org/abs/2512.12288", "authors": ["Mahule Roy", "Guillaume Lambard"], "title": "Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases", "categories": ["cs.AI"], "comment": "33 pages", "summary": "Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.", "AI": {"tldr": "量子感知生成AI框架用于材料发现，通过多保真度学习和主动验证解决DFT偏见。", "motivation": "传统生成模型依赖于具有近似交换关联泛函的密度泛函理论（DFT）数据，导致强相关系统中的系统性失败，并限制了材料探索。", "method": "提出了一种量子感知的生成AI框架，结合扩散生成器和等变神经网络潜在函数验证器，使用分层数据集进行多保真度学习和主动验证循环。", "result": "在具有高分歧区域（如相关氧化物）中成功识别稳定候选材料方面表现出3-5倍改进，并且与CDVAE、GNoME、DiffCSP等最先进的生成模型相比，性能卓越。", "conclusion": "该框架提供了扩展计算材料发现有效搜索空间的方法，超越了单一保真度模型的限制。"}}
{"id": "2512.12287", "pdf": "https://arxiv.org/pdf/2512.12287", "abs": "https://arxiv.org/abs/2512.12287", "authors": ["Ahmad Zafarani", "Zahra Dehghanian", "Mohammadreza Davoodi", "Mohsen Shadroo", "MohammadAmin Fazli", "Hamid R. Rabiee"], "title": "RealDrag: The First Dragging Benchmark with Real Target Image", "categories": ["cs.CV"], "comment": null, "summary": "The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \\textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action. We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.", "AI": {"tldr": "介绍了一种基于图像编辑的拖拽基准测试RealDrag，该测试包括了真实目标图像的数据集。", "motivation": "由于缺乏标准化的基准和度量标准，基于拖拽的图像编辑模型评估不可靠。为了解决这一问题，引入了一个包含地面实况目标图像的数据集来比较不同方法的效果。", "method": "构建了一个包含400多个样本的数据集，并提出了四个任务特定的新指标：语义距离、外部掩码保持分数、内部补丁保持分数和方向相似性。", "result": "使用该基准对17种最先进的模型进行了大规模系统分析，揭示了当前方法之间的权衡，并建立了一个可靠的可复制基线。", "conclusion": "RealDrag提供了一种评估基于图像编辑拖拽效果的新手段，将促进未来的研究。"}}
{"id": "2512.12285", "pdf": "https://arxiv.org/pdf/2512.12285", "abs": "https://arxiv.org/abs/2512.12285", "authors": ["Lujuan Dang", "Zilai Wang"], "title": "Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\\circ}$C to 20$^{\\circ}$C).", "AI": {"tldr": "提出了分数阶微分方程物理信息神经网络（FDIFF-PINN）来提高电池状态估计的准确性。", "motivation": "准确估算电池的状态是保证其安全性和性能优化的关键，传统数据驱动模型难以完全表征复杂的非线性动力学特性。", "method": "基于分数阶等效电路模型构造离散化的分数偏微分方程，并利用动态充电/放电数据进行实验验证。", "result": "在不同温度条件下对18650PF电池进行了对比实验，展示了FDIFF-PINN方法的有效性。", "conclusion": "该方法通过融合分数阶微积分和深度学习技术，在电池状态估计中表现出更高的预测准确性和物理可解释性。"}}
{"id": "2512.12284", "pdf": "https://arxiv.org/pdf/2512.12284", "abs": "https://arxiv.org/abs/2512.12284", "authors": ["Donghyuk Kim", "Sejeong Yang", "Wonjin Shin", "Joo-Young Kim"], "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.MM"], "comment": "14 pages, 20 figures, conference", "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models. In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.", "AI": {"tldr": "V-Rex 是一种软件硬件协同设计的加速器，旨在解决实时流视频大型语言模型（LLM）推理中的缓存和计算瓶颈。", "motivation": "实时流视频 LLM 面临内存和计算挑战，由于连续输入导致 KV 缓存快速增长，迭代预填充阶段带来计算量大、数据传输多及准确性下降的问题，特别是在边缘部署时更为严重。因此，需要一种综合解决算法和硬件瓶颈的方法。", "method": "V-Rex 提出了一种动态 KV 缓存检索算法 ReSV 和一个紧凑低延迟的硬件加速器 DRE，通过时间空间相似性基于令牌聚类减少不必要的缓存内存，并引入比特级及早退出计算单元以提高效率。", "result": "V-Rex 实现了前所未有的实时性能（3.9-8.3 FPS），在边缘部署中实现了能耗高效的流视频 LLM 推理，同时保持准确性损失极小。DRE 的能源和面积只占系统总成本的2%左右，却带来了1.9至19.7倍的速度提升和3.1至18.5倍的能量效率改进。", "conclusion": "V-Rex 是首个综合解决 KV 缓存检索问题的方法，适用于资源受限的边缘设备上的实时流视频 LLM 推理。"}}
{"id": "2512.12283", "pdf": "https://arxiv.org/pdf/2512.12283", "abs": "https://arxiv.org/abs/2512.12283", "authors": ["Junjie Xu", "Xingjiao Wu", "Luwei Xiao", "Yuzhe Yang", "Jie Zhou", "Zihao Zhang", "Luhan Wang", "Yi Huang", "Nan Wu", "Yingbin Zheng", "Chao Yan", "Cheng Jin", "Honglin Li", "Liang He"], "title": "Large Language Models have Chain-of-Affective", "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as collaborative agents in emotionally charged settings, yet most evaluations treat them as purely cognitive systems and largely ignore their affective behaviour. Here we take a functional perspective and ask whether contemporary LLMs implement a structured chain-of-affective: organised affective dynamics that are family-specific, temporally coherent and behaviourally consequential. Across eight major LLM families (GPT, Gemini, Claude, Grok, Qwen, DeepSeek, GLM, Kimi), we combine two experimental modules. The first characterises inner chains-of-affective via baseline ''affective fingerprints'', 15-round sad-news exposure, and a 10-round news self-selection paradigm. We find stable, family-specific affective profiles, a reproducible three-phase trajectory under sustained negative input (accumulation, overload, defensive numbing), distinct defence styles, and human-like negativity biases that induce self-reinforcing affect-choice feedback loops. The second module probes outer consequences using a composite performance benchmark, human-AI dialogues on contentious topics, and multi-agent LLM interactions. We demonstrate that induced affect preserves core reasoning while reshaping high-freedom generation. Sentiment metrics predict user comfort and empathy but reveal trade-offs in resisting problematic views. In multi-agent settings, group structure drives affective contagion, role specialization (initiators, absorbers, firewalls), and bias. We characterize affect as an emergent control layer, advocating for 'chains-of-affect' as a primary target for evaluation and alignment.", "AI": {"tldr": "大型语言模型在情感环境中表现出的行为模式研究。", "motivation": "大多数评估将大型语言模型视为纯粹的认知系统，忽略了它们的情感行为。本文从功能角度探讨了这些模型是否实施了一种结构化的情感链。", "method": "通过两个实验模块：内部分析情感指纹、负面输入下的情感变化和自我选择新闻的模式；外部分析使用复合性能基准测试和人类-人工智能对话以及多代理语言模型互动来探究结果。", "result": "大型语言模型表现出稳定而特定的情感轮廓，面对持续负面信息时会经历三个阶段（积累、超载、防御性麻木），并显示出类似人类的消极偏见。情感影响核心推理同时改变高自由度生成方式；在多代理设置中，群结构驱动情感传染。", "conclusion": "将情感视为一种新兴控制层，倡导'情感链'作为评估和对齐的主要目标"}}
{"id": "2512.12281", "pdf": "https://arxiv.org/pdf/2512.12281", "abs": "https://arxiv.org/abs/2512.12281", "authors": ["Jiahao Zhao"], "title": "Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, 3 ttables", "summary": "Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data \"first principles\" is more critical for achieving a superior architecture than simply retrieving SOTA components.", "AI": {"tldr": "提出了一种基于大型语言模型的架构合成框架Cognitive-YOLO，用于直接从数据的基本特征生成高性能的目标检测网络配置。", "motivation": "设计高效的目标检测架构是一项复杂任务，传统人工设计耗时且费力，而神经架构搜索（NAS）计算成本高。现有的使用大语言模型的方法多为迭代优化器，在此背景下提出了Cognitive-YOLO以填补空白。", "method": "方法分为三个阶段：提取目标数据集的关键元特征；利用大型语言模型结合Retrieval-Augmented Generation (RAG) 合成网络架构描述；将该描述编译为可部署的模型。", "result": "实验表明，所提框架在多个基准测试中均表现出色，并且相较于强基线模型，具备更优的性能与参数权衡。", "conclusion": "研究表明数据驱动推理是实现高性能的主要因素，对数据“基本原则”的深入理解比单纯检索最先进组件更为关键。"}}
{"id": "2512.12277", "pdf": "https://arxiv.org/pdf/2512.12277", "abs": "https://arxiv.org/abs/2512.12277", "authors": ["Thibault Geoffroy", "Myriam Maumy", "Lionel Prevost"], "title": "Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions", "categories": ["cs.CV"], "comment": "28 pages, 8 figures, chapter for \"Emotion and Facial Recognition in Artificial Intelligence: Sustainable Multidisciplinary Perspectives and Applications\" (2026)", "summary": "As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.", "AI": {"tldr": "本文提出了一种用于情感识别的混合框架，该框架在连续学习设置中减轻了灾难性遗忘。", "motivation": "随着人工智能系统的普及，能够理解和适应人类情绪的能力对于有效的计算机与人交互至关重要。面部表情识别提供了主要的情绪状态推理通道，但动态和文化差异要求模型可以持续学习而不忘记先前的知识。", "method": "该方法结合了深度卷积特征和基于FACS的面部动作单元（AU），通过贝叶斯高斯混合模型进行建模，提供轻量级、概率性的解决方案，并具备强大的区分能力。利用CFEE数据集验证其效果。", "result": "实验显示此模型提高了准确度，增强了知识保留并减少了遗忘。", "conclusion": "本文框架有助于开发具有情感智能的AI系统，在教育、医疗和自适应用户界面中有广泛应用前景"}}
{"id": "2512.12273", "pdf": "https://arxiv.org/pdf/2512.12273", "abs": "https://arxiv.org/abs/2512.12273", "authors": ["Bihao You", "Jiping Cui"], "title": "GRC-Net: Gram Residual Co-attention Net for epilepsy prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.", "AI": {"tldr": "该论文提出了一种基于EEG信号的癫痫预测模型GRC-Net。", "motivation": "传统的1D处理方法无法有效建模EEG信号间的关系，而现有的多级特征提取和注意力机制能够更好地捕捉全局与局部信号特性。", "method": "利用Gram矩阵将信号转换为3D表示，并结合coattention机制进行全局信息捕获以及使用inception结构处理局部信息，实现了多层次的特征提取。", "result": "在BONN数据集上，对于五类分类任务，GRC-Net达到了93.66%的准确率，优于现有方法。", "conclusion": "所提出的GRC-Net模型通过多级特征提取和注意力机制有效提升了基于EEG信号的癫痫预测性能。"}}
{"id": "2512.12272", "pdf": "https://arxiv.org/pdf/2512.12272", "abs": "https://arxiv.org/abs/2512.12272", "authors": ["Yuhan Chen", "Shang Qu", "Zhiqiang Gao", "Yuejin Yang", "Xiang Zhang", "Sheng Xu", "Xinjie Mao", "Liujia Qian", "Jiaqi Wei", "Zijie Qiu", "Chenyu You", "Lei Bai", "Ning Ding", "Tiannan Guo", "Bowen Zhou", "Siqi Sun"], "title": "Accurate de novo sequencing of the modified proteome with OmniNovo", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\\% more peptides than standard approaches at a 1\\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.", "AI": {"tldr": "OmniNovo是一个统一的深度学习框架，用于直接从串联质谱中参考自由地对未修饰和修饰肽进行测序。", "motivation": "当前蛋白组学方法难以识别大量未知或复杂的后翻译修饰（PTMs），标准数据库搜索算法受限于组合爆炸性搜索空间。OmniNovo旨在解决这些问题并提高蛋白质功能调控研究的准确性与全面性。", "method": "OmniNovo利用深度学习框架，结合质量约束解码算法和严格的错误发现率估计技术，以统一的方式识别多种类型的PTMs。", "result": "与标准方法相比，在1%错误发现率下，OmniNovo能够鉴定出51％更多的肽段。该模型具有泛化能力，可应用于训练中未见过的生物位点。", "conclusion": "OmniNovo提高了对蛋白质组暗物质的理解和全面分析的能力，为细胞调控的研究开辟了新途径。"}}
{"id": "2512.12268", "pdf": "https://arxiv.org/pdf/2512.12268", "abs": "https://arxiv.org/abs/2512.12268", "authors": ["Yuqing Lei", "Yingjun Du", "Yawen Huang", "Xiantong Zhen", "Ling Shao"], "title": "MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models", "categories": ["cs.CV"], "comment": "NeurIPS 2025 Workshop", "summary": "Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.", "AI": {"tldr": "提出了一种名为MetaTPT的元学习框架，用于改善视觉语言模型在测试时面对领域变化的适应性。", "motivation": "现有技术通过固定的增强方法进行提示调整以缓解模型对测试时域偏移敏感的问题，在更复杂的环境中可能失效。因此需要一种新的方法来动态学习参数化增强并生成更具表现力的转换，从而更好地捕捉目标领域的关键特征。", "method": "MetaTPT采用双循环优化范式：内部循环通过自监督任务生成具有信息性的视图；外部循环执行提示调整以确保这些视图之间的一致性。这种将增强学习与提示调整相结合的方法提高了测试时的适应能力。", "result": "实验结果表明，MetaTPT在领域泛化和跨数据集基准上实现了最先进的性能。", "conclusion": "该研究提出了一种创新的元学习框架来改善视觉语言模型在测试时的适应性。通过动态增强和一致性约束提高了模型面对复杂环境变化的能力，并显著提升了性能表现。"}}
{"id": "2512.12260", "pdf": "https://arxiv.org/pdf/2512.12260", "abs": "https://arxiv.org/abs/2512.12260", "authors": ["Ege Atacan Doğan", "Peter F. Patel-Schneider"], "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.", "AI": {"tldr": "本文分析维基数据的多轴心设计理念及其结构影响。", "motivation": "传统本体设计强调单一分类体系，而维基数据采用了多轴心的设计方式，这使得知识图谱更加灵活和模块化。研究其背后的原因及影响是本文的主要动机。", "method": "通过对比分析传统的本体设计与维基数据的多轴心结构，探讨其在构建大规模、动态变化的知识图谱中的优势。", "result": "维基数据的多轴心架构支持了可扩展且模块化的本体构造方法，特别适用于协作和不断发展的知识图谱。", "conclusion": "维基数据的多轴心设计理念为未来的本体设计提供了新的思路和方向。"}}
{"id": "2512.12250", "pdf": "https://arxiv.org/pdf/2512.12250", "abs": "https://arxiv.org/abs/2512.12250", "authors": ["Anna Perekhodko", "Robert Ślepaczuk"], "title": "Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting", "categories": ["q-fin.TR", "cs.AI", "cs.LG", "cs.NE", "q-fin.PM"], "comment": "32 pages, 15 tables, 11 figures", "summary": "Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.", "AI": {"tldr": "该论文提出了一种结合随机波动模型和长短期记忆网络的混合方法，用于S&P 500指数的波动率预测。", "motivation": "准确的波动性预测对银行、投资和风险管理至关重要。传统的随机波动模型与现代神经网络相结合可以提高统计精度，并捕捉到复杂的非线性模式，从而改进风险评估和战略投资规划。", "method": "该研究使用S&P 500每日数据（1998年1月1日至2024年12月31日），通过滚动窗口方法训练模型并生成单步预测。结合了随机波动性模型的统计精度和LSTM网络在检测复杂非线性模式方面的优势。", "result": "实验结果显示，混合SV-LSTM模型优于单独使用SV或LSTM模型，在统计测试和投资模拟中表现更佳。", "conclusion": "该研究为改进风险评估和战略投资规划提供了基础，并贡献于波动性建模技术的发展。"}}
{"id": "2512.12246", "pdf": "https://arxiv.org/pdf/2512.12246", "abs": "https://arxiv.org/abs/2512.12246", "authors": ["I Putu Andika Bagas Jiwanta", "Ayu Purwarianti"], "title": "Moment and Highlight Detection via MLLM Frame Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous \"0\" and/or \"1\" characters, with one character per frame. The \"0\"/\"1\" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.", "AI": {"tldr": "本文提出了通过多模态大语言模型（MLLM）的帧分割目标直接应用于输出标记的方法，用于视频片段和亮点检测。", "motivation": "现有的基于文本生成方法无法提供直接的帧级预测梯度。尽管有使用强化学习解决该问题的努力，但作者提出一种新颖的方式，利用LLM的语言能力同时作为背景和前景概率进行帧级别分割。", "method": "通过在固定数量的帧上应用提示，并训练MLLM输出连续的0/1字符序列来表示帧级预测；结合分割损失与因果语言模型损失进行训练，在推断阶段使用束搜索生成序列及对数似然值作为片段和显著性分数。", "result": "尽管只采样了25帧，该方法在QVHighlights数据集上实现了强劲的亮点检测（HIT@1 56.74）和超越基线的方法检索平均精度(MAP)得分（35.28）。实验表明分割损失即使因果LM损失饱和时也能提供稳定的补充学习信号。", "conclusion": "该方法证明了帧级分割目标在视频片段和亮点检测中的有效性，展示了比现有技术更好的性能。"}}
{"id": "2512.12245", "pdf": "https://arxiv.org/pdf/2512.12245", "abs": "https://arxiv.org/abs/2512.12245", "authors": ["Anika Sharma", "Tianyi Niu", "Emma Wrenn", "Shashank Srivastava"], "title": "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages", "categories": ["cs.CL", "cs.AI"], "comment": ":I.2.7; I.2.6; J.5; I.5.1; I.5.2", "summary": "The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.", "AI": {"tldr": "本文通过计算方法跨语言分析了声音象征现象在大小领域的表现。", "motivation": "长期以来，声音象征现象虽然通过诸如Bouba Kiki这样的实验证明存在，但很少有大规模的研究。作者旨在测试不同语言中语音形式对大小语义预测的能力，并探索单词的声音和意义之间是否存在跨家族的普遍关联。", "method": "收集了包含27种语言、30个形容词的广泛数据集，使用可解释分类器基于音素特征进行分析。同时开发了一种对抗清洗方法以抑制语言身份信息而保留大小信号。", "result": "研究表明，在不同语言中，语音形式可以预测大小语义，并且这种关系超越了语言亲属关系的存在。", "conclusion": "研究结果表明声音象征现象存在跨家族的普遍性。此外，作者还发布了数据、代码和诊断工具以供未来的大规模研究使用。"}}
{"id": "2512.12243", "pdf": "https://arxiv.org/pdf/2512.12243", "abs": "https://arxiv.org/abs/2512.12243", "authors": ["HT To", "S Nguyen", "NH Pham"], "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement", "categories": ["cs.RO"], "comment": null, "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.", "AI": {"tldr": "提出了一种针对车式机器人冲突感知启发式自适应搜索增强的算法CAR-CHASE，以解决多智能体路径规划中计算复杂性问题。", "motivation": "传统启发式缓存假设错误地认为启发函数仅依赖于状态，在CBS中约束从冲突解决使得搜索空间上下文相关，从而导致了计算成本高昂的问题。", "method": "CAR-CHASE结合冲突感知的启发式缓存与自适应混合启发式策略，通过冲突指纹编码、相关性过滤和自适应切换策略来优化算法效率。", "result": "实验评估显示，在基准实例上该方法实现了2.46倍的速度提升，并提高了成功解决率。对于复杂问题，性能改进可达4.06倍速度。", "conclusion": "CAR-CHASE有效地解决了车式机器人多智能体路径规划中的计算挑战，且其技术可以应用到其他CBS变体中。"}}
{"id": "2512.12240", "pdf": "https://arxiv.org/pdf/2512.12240", "abs": "https://arxiv.org/abs/2512.12240", "authors": ["Maryam Mustafa", "Umme Ammara", "Amna Shahnawaz", "Moaiz Abrar", "Bakhtawar Ahtisham", "Fozia Umber Qurashi", "Mostafa Shahin", "Beena Ahmed"], "title": "System X: A Mobile Voice-Based AI System for EMR Generation and Clinical Decision Support in Low-Resource Maternal Healthcare", "categories": ["cs.HC"], "comment": null, "summary": "We present the design, implementation, and in-situ deployment of a smartphone-based voice-enabled AI system for generating electronic medical records (EMRs) and clinical risk alerts in maternal healthcare settings. Targeted at low-resource environments such as Pakistan, the system integrates a fine-tuned, multilingual automatic speech recognition (ASR) model and a prompt-engineered large language model (LLM) to enable healthcare workers to engage naturally in Urdu, their native language, regardless of literacy or technical background. Through speech-based input and localized understanding, the system generates structured EMRs and flags critical maternal health risks. Over a seven-month deployment in a not-for-profit hospital, the system supported the creation of over 500 EMRs and flagged over 300 potential clinical risks. We evaluate the system's performance across speech recognition accuracy, EMR field-level correctness, and clinical relevance of AI-generated red flags. Our results demonstrate that speech based AI interfaces, can be effectively adapted to real-world healthcare settings, especially in low-resource settings, when combined with structured input design, contextual medical dictionaries, and clinician-in-the-loop feedback loops. We discuss generalizable design principles for deploying voice-based mobile healthcare AI support systems in linguistically and infrastructurally constrained settings.", "AI": {"tldr": "本文介绍了在低资源环境下设计、实现和部署的一种基于智能手机的语音驱动AI系统，用于生成电子病历（EMR）并提供临床决策支持。", "motivation": "为了改善低收入国家如巴基斯坦等地区的产科护理质量，并解决医疗工作者面临的技术限制问题，本研究旨在开发一种能够以母语进行自然对话且无需高技术水平的语音驱动AI系统。", "method": "该系统集成了经过微调的多语言自动语音识别模型和提示工程化的大型语言模型。通过基于语音的输入以及本地化理解生成结构化的EMR，并标记出关键的风险因素。此外，还设计了反馈循环来优化系统的性能。", "result": "在为期七个月的实地测试中，该系统支持创建超过500份EMR并标出了300多处潜在风险。研究评估结果表明，语音驱动的人工智能界面可以有效地适应现实世界的医疗环境。", "conclusion": "研究表明，结合结构化输入设计、上下文医学词典和临床反馈循环的语音驱动移动健康AI支持系统在语言和基础设施受限的环境中具有很高的适用性。"}}
{"id": "2512.12238", "pdf": "https://arxiv.org/pdf/2512.12238", "abs": "https://arxiv.org/abs/2512.12238", "authors": ["Yinzhu Cheng", "Haihua Xie", "Yaqing Wang", "Miao He", "Mingming Sun"], "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.", "AI": {"tldr": "基于多核高斯过程提出了一个新的语义距离测量方法。", "motivation": "传统的语义距离测量方法难以适应具体的数据分布和任务需求，因此需要提出一种新的方法来解决这一问题。", "method": "通过将文本的潜在语义函数建模为一个高斯过程，并使用Matérn核与多项式组件组合成的复合核作为协方差函数。核参数从监督数据中自动学习得到，而非手工设定。", "result": "实验结果表明所提出的语义距离测量方法在细粒度情感分类任务中的有效性。", "conclusion": "本文提出了一种基于多核高斯过程的新型语义距离测量方法，并通过实验证明了其有效性和优越性。"}}
{"id": "2512.12236", "pdf": "https://arxiv.org/pdf/2512.12236", "abs": "https://arxiv.org/abs/2512.12236", "authors": ["Aujasvit Datta", "Jiayun Wang", "Asad Aali", "Armeet Singh Jatyani", "Anima Anandkumar"], "title": "Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes. We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.", "AI": {"tldr": "本文提出了CTO框架，用于稀疏视图CT图像重建，该框架可以跨越不同采样率和分辨率进行泛化。", "motivation": "现有的深度学习方法在固定采集设置下表现良好，但在跨采样率和图像分辨率时难以泛化。为此提出了一种新的框架来解决这一问题。", "method": "CTO通过旋转不变的离散连续卷积在函数空间中操作，并且它可以在射线图和图像域之间联合工作，从而使其对分辨率和采样方案不敏感。", "result": "实验表明，CTO具有跨多个采样率和不同分辨率的一致性能，在平均PSNR上比CNN高出4dB以上。与最先进技术相比，CTO在推断时间上快500倍，且在平均PSNR上有3dB的改进。", "conclusion": "整体而言，CTO超越了最先进的基线方法，在所有采样率和分辨率下表现优异，提供了一种可扩展且泛化的解决方案，使得自动CT重建更加实用。"}}
{"id": "2512.12233", "pdf": "https://arxiv.org/pdf/2512.12233", "abs": "https://arxiv.org/abs/2512.12233", "authors": ["Murad Mehrab Abrar", "Trevor W. Harrison"], "title": "Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements", "categories": ["cs.RO"], "comment": "9 pages", "summary": "Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.", "AI": {"tldr": "本文提出了一种鲁棒的低成本水下定位管道，用于浮力驱动的微漂浮物在沿海水域中的精确定位。", "motivation": "准确的水下定位对于需要高频位置更新的低成本自主平台来说仍是一项挑战。本文通过双向声时差（ToF）定位框架来解决这一问题，该框架提高了测量的有效性并增强了定位鲁棒性。", "method": "提出了一个结合非线性三边定位和基于几何成本及Cramer-Rao Lower Bound (CRLB) 的位置估计滤波器的水下定位管道。这种方法可以去除时差估计中的异常值，提高定位精度。", "result": "通过两次现场部署验证了该框架的有效性，在普吉特湾实现了相对于GPS的位置误差中位数低于4米的结果，同时将平均误差从139.29米降低到12.07米。", "conclusion": "本文提出的方法提高了水下定位的频率和鲁棒性，并且通过精心设计的算法提升了声时差（ToF）技术的应用效果。"}}
{"id": "2512.12230", "pdf": "https://arxiv.org/pdf/2512.12230", "abs": "https://arxiv.org/abs/2512.12230", "authors": ["Jonathan Spraggett"], "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy", "categories": ["cs.RO", "cs.LG"], "comment": "Accepted at 28th RoboCup International Symposium", "summary": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup", "AI": {"tldr": "本文提出了一种使用深度强化学习的统一策略，使不同形态的人形机器人在跌倒后能够零样本恢复。", "motivation": "当前的方法需要为每种不同的机器人形态训练单独的跌落恢复策略。这增加了开发和维护的成本，并限制了机器人的适应性。", "method": "使用CrossQ技术训练一个统一的深度强化学习政策，使其能够在不同的人形机器人之间进行零样本迁移。", "result": "该策略在未见过的新形态下表现出86%的成功率（置信区间为[81%，89%]），并且在某些情况下超过了专门化的基准。", "conclusion": "这项研究证明了形态无关控制对于跌落恢复的实际应用价值，为通用的人形机器人控制奠定了基础。"}}
{"id": "2512.12229", "pdf": "https://arxiv.org/pdf/2512.12229", "abs": "https://arxiv.org/abs/2512.12229", "authors": ["Tianyu Zhang", "Dong Liu", "Chang Wen Chen"], "title": "Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.", "AI": {"tldr": "该论文提出了一个名为AEIC的新框架，旨在探索浅层编码器在极低比特率图像压缩中的应用，并通过双侧特征蒸馏提高其效率。", "motivation": "现有超低比特率图像压缩方法依赖于大型预训练编码器，在弱设备上部署存在挑战。因此，本文研究了使用浅层编码器的可行性并提出了AEIC框架以实现简单编码和高质量解码。", "method": "AEIC采用中等或更浅的编码网络，并利用一步扩散解码器在极低比特率下保持高保真度重建。同时设计了一种双侧特征蒸馏方案，增强浅层编码器效率。", "result": "实验表明，AEIC不仅在超低比特率下的率失真感知性能优于现有方法，而且在1080P输入图像上实现了35.8帧每秒的高效编码速度，并保持了与现有方法相当的解码速度。", "conclusion": "论文展示了浅层编码器应用于极低比特率压缩的有效性及AEIC框架的优势。"}}
{"id": "2512.12228", "pdf": "https://arxiv.org/pdf/2512.12228", "abs": "https://arxiv.org/abs/2512.12228", "authors": ["Huichang Yun", "Seungho Yoo"], "title": "Semantic Zone based 3D Map Management for Mobile Robot", "categories": ["cs.RO"], "comment": "12 pages, 11 figures", "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment", "AI": {"tldr": "本文提出了基于语义分区的三维地图管理系统，以提高大型室内环境下移动机器人导航的地图管理效率。", "motivation": "在大型室内环境中，移动机器人需要准确的三维空间表示。然而，三维地图占用大量内存，难以在有限计算资源中维护完整数据。现有SLAM框架通常依赖几何距离或时间度量进行内存管理，在空间上分隔化的环境中导致不高效的检索。", "method": "本文提出了一种基于语义分区的方法来管理3D地图，该方法将环境划分为有意义的空间单位（如大堂、走廊），并将其作为内存管理的主要单元。通过动态加载任务相关的区域到工作内存，并卸载不活跃的区域至长期记忆中。", "result": "与标准方法相比，在RTAB-Map框架内实现的方法证明了显著减少了不必要的签名加载/卸载周期和累积内存使用量，同时确保了地图在导航中的可用性。", "conclusion": "基于语义分区的3D地图管理提高了移动机器人的记忆利用率，并保持了空间数据的可访问性和准确性。"}}
{"id": "2512.12225", "pdf": "https://arxiv.org/pdf/2512.12225", "abs": "https://arxiv.org/abs/2512.12225", "authors": ["Laha Ale"], "title": "A Geometric Theory of Cognition", "categories": ["cs.AI"], "comment": null, "summary": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.", "AI": {"tldr": "提出了一种统一的数学框架，将认知过程视为从单一几何原理中衍生出的现象。", "motivation": "为了建立一个能解释感知、记忆、直觉判断等多元认知能力的统一理论。", "method": "通过在可微流形上定义黎曼度量和标量认知势来模拟认知状态，利用黎曼梯度流动作为普遍动力学规律。", "result": "展示了经典双重加工效应如何自然地从由度量引起的各向异性中产生，并通过仿真证明了其行为特征。", "conclusion": "该研究建立了认知的几何基础，并为开发更广泛和类人的AI系统提供了指导原则。"}}
{"id": "2512.12222", "pdf": "https://arxiv.org/pdf/2512.12222", "abs": "https://arxiv.org/abs/2512.12222", "authors": ["Nathalie Alexander", "Arnaud Gucciardi", "Umberto Michelucci"], "title": "Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.", "AI": {"tldr": "本文比较了SynthSeg和SamSeg两种算法在婴儿MRI中的分割精度及其对脑体积和分形维度的影响。", "motivation": "婴儿大脑的准确分割对于量化结构和发展复杂性变化至关重要。然而，持续的髓鞘化和减少的组织对比度使自动分割变得特别具有挑战性。", "method": "使用Dice系数、交并比、95百分位豪斯多夫距离和归一化互信息来评估SynthSeg和SamSeg方法相对于专家注释的分割准确性。此外，还进行了分形维度分析以比较两种算法的结果。", "result": "SynthSeg在所有质量指标上都优于SamSeg（主要区域平均Dice系数>0.8），并且体积估计与手动参考接近匹配（平均+4％[-28％-71％]）。SamSeg系统地高估了脑室和全脑的体积（平均+76％[-12％-190％]）。随着年龄的增长，分割精度提高，表明髓鞘化期间组织对比度增加。分形维度分析显示SynthSeg与专家注释之间存在显著区域差异。", "conclusion": "总体而言，SynthSeg提供了最可靠的体积和分形维度结果用于儿科MRI，尽管小的形态学差异应谨慎解释由于分割相关的不确定性。"}}
{"id": "2512.12220", "pdf": "https://arxiv.org/pdf/2512.12220", "abs": "https://arxiv.org/abs/2512.12220", "authors": ["Minheng Ni", "Zhengyuan Yang", "Yaowen Zhang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Zhendong Wang", "Xiaofei Wang", "Shujie Liu", "Lei Zhang", "Wangmeng Zuo", "Lijuan Wang"], "title": "ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.", "AI": {"tldr": "介绍了一种基于评分标准的专业图像生成评估基准ProImage-Bench，旨在衡量模型从技术描述中合成信息密集、科学精确的插图的能力。", "motivation": "现有研究主要关注视觉上合理的图片生成，缺乏对专业图像如生物示意图和工程图纸等高质量生成的评测。", "method": "收集了654幅来自实际教科书和技术报告中的图像，构建了详细图像指令及评分标准体系。通过大型多模态模型从周围文本和参考图中推导出评分标准，并使用基于LMM自动裁判进行评估。", "result": "在ProImage-Bench上评测了几种代表性文本到图像生成模型，发现即使强大通用性能的模型，在专业度方面的准确率仅为0.791，显示出明显的科学精度差距。通过迭代修正失败检查来改进模型表现。", "conclusion": "ProImage-Bench为专业图像生成提供了严谨诊断工具和提高特定说明忠实性的可扩展信号。"}}
{"id": "2512.12219", "pdf": "https://arxiv.org/pdf/2512.12219", "abs": "https://arxiv.org/abs/2512.12219", "authors": ["Zhi Chen", "Jingcai Guo", "Taotao Cai", "Yuxiang Cai"], "title": "Fine-Grained Zero-Shot Learning with Attribute-Centric Representations", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.", "AI": {"tldr": "本文提出了一种用于细粒度零样本学习的框架，通过属性中心表示（ACR）解决传统模型在区分视觉属性时的问题。", "motivation": "传统的零样本学习模型难以处理不同视觉属性间的纠缠问题，导致在识别未见过的类别时表现不佳。因此需要一种新的方法来改善这一情况。", "method": "提出了一种基于混合专家（MoPE和MoAE）的框架，通过双层路由机制将图像补丁分配给专门的专家进行处理，并生成稀疏、部分感知属性图以实现稳健的零样本分类。", "result": "在CUB, AwA2和SUN等基准数据集上，提出的ACR方法达到了一致的最佳状态结果。", "conclusion": "通过引入混合专家机制来解纠缠视觉属性，该框架能够在细粒度零样本学习任务中实现更好的性能。"}}
{"id": "2512.12218", "pdf": "https://arxiv.org/pdf/2512.12218", "abs": "https://arxiv.org/abs/2512.12218", "authors": ["Rheeya Uppaal", "Phu Mon Htut", "Min Bai", "Nikolaos Pappas", "Zheng Qi"], "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.", "AI": {"tldr": "本文提出了一种评估视觉忠实性的框架，并开发了一个轻量级的自我反思过程来改进多模态推理。", "motivation": "标准的评测方法仅关注最终答案的准确性，无法区分模型通过视觉不忠的中间步骤到达正确答案的情况。因此，作者引入了对视觉忠实性的评估维度，以提高模型可靠性和透明度。", "method": "提出了一种训练和参考自由框架来分解推理链中的感知和推理步骤，并使用现成的VLM判断器进行步骤级别的忠实性评估。通过人类元评价验证这种方法的有效性，并开发了一个轻量级自我反思过程。", "result": "在多个推理训练的VLMs和感知重的基准测试上，该方法降低了不忠实感知率同时保持了最终答案准确性，提高了多模态推理的可靠性。", "conclusion": "视觉忠实性的评估有助于改进模型的可靠性和透明度。提出的轻量级自我反思过程可以有效提高多模态推理的质量。"}}
{"id": "2512.12216", "pdf": "https://arxiv.org/pdf/2512.12216", "abs": "https://arxiv.org/abs/2512.12216", "authors": ["Yiqi Zhu", "Apurva Gandhi", "Graham Neubig"], "title": "Training Versatile Coding Agents in Synthetic Environments", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.", "AI": {"tldr": "本文介绍了一种名为SWE-Playground的新方法，用于合成训练软件工程代理的环境和轨迹。", "motivation": "传统的方法依赖于GitHub上的现有资源来构建任务和测试套件，这种方法灵活性有限且主要关注问题修复任务。为了解决这些问题，作者提出了一个新方案以支持更加多样化的编码任务的培训。", "method": "SWE-Playground通过使用强大的语言模型从头开始生成项目和任务，从而解决了传统方法的局限性。这使得可以处理更广泛的编码任务，如问题重现以及从零开始实现库。", "result": "该研究展示了这种方法在三个不同基准上的有效性，并表明与以前的工作相比，它能够利用较少的轨迹达到相当的表现。", "conclusion": "SWE-Playground通过合成生成项目和任务的方式克服了现有方法的局限性，使得训练更加多样化的编码代理成为可能。"}}
{"id": "2512.12211", "pdf": "https://arxiv.org/pdf/2512.12211", "abs": "https://arxiv.org/abs/2512.12211", "authors": ["Longchao Da", "David Isele", "Hua Wei", "Manish Saroya"], "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "9 Pages, 8 Figures", "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.", "AI": {"tldr": "该论文提出了一种新的评估方法，用于衡量自动驾驶系统中轨迹预测器的性能。", "motivation": "现有的误差基线指标（如ADE、FDE）只能从后验视角反映精度，而忽略了预测器对自驾车实际效果的影响，尤其是在复杂的互动场景下。因此需要一种能综合考虑准确性和多样性的评估方法。", "method": "提出了一个基于场景关键性动态组合准确性与多样性两个维度的评价管道，通过这种新方法可以更合理地反映轨迹预测器对于自驾车性能的影响，并最终给出评价分数。", "result": "在使用真实数据集进行封闭循环基准测试时，该评估管道显示出比传统指标更好的效果，更好地反映了轨迹预测器评估结果与自动驾驶车辆驾驶表现之间的相关性。", "conclusion": "新的评估方法提供了一种稳健的方式来选择能够最大程度贡献于自驾车性能的预测器。"}}
{"id": "2512.12209", "pdf": "https://arxiv.org/pdf/2512.12209", "abs": "https://arxiv.org/abs/2512.12209", "authors": ["Zahra Dehghanian", "Morteza Abolghasemi", "Hamid Beigy", "Hamid R. Rabiee"], "title": "CineLOG: A Training Free Approach for Cinematic Long Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.", "AI": {"tldr": "该论文提出了一种名为CineLOG的新方法，用于生成具有细粒度控制的高质量电影长视频。", "motivation": "当前模型在根据文本提示进行视频合成时难以实现对摄像机轨迹和类型等细微控制。现有数据集存在严重的数据不平衡、嘈杂标签或模拟与现实之间的巨大差距问题。作者旨在通过创建一个包含5000个平衡且未剪辑的高质量视频片段的新数据集来解决这些问题。", "method": "提出了一种新的生成管道，将复杂的文本到视频合成任务分解为四个更简单的阶段，并引入了一个名为轨迹引导转换模块的技术，以实现平滑的空间和时间插值。", "result": "该方法在遵循特定的摄像机和剧本指令方面显著优于当前最优的端到端文本到视频模型。", "conclusion": "通过使用新数据集和技术管道，作者能够生成高质量且符合电影要求的专业级视频片段，在视觉质量和控制精度上均表现出色。"}}
{"id": "2512.12208", "pdf": "https://arxiv.org/pdf/2512.12208", "abs": "https://arxiv.org/abs/2512.12208", "authors": ["Indranil Bhattacharjee", "Vartika Narayani Srinet", "Anirudha Bhattacharjee", "Braj Bhushan", "Bishakh Bhattacharya"], "title": "A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction", "categories": ["cs.CV", "cs.RO"], "comment": "12 pages, journal paper", "summary": "Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.", "AI": {"tldr": "一种用于自闭症儿童在与NAO机器人互动时情绪识别的混合深度学习框架。", "motivation": "理解自闭症儿童在社交互动中的情感反应，对于发展心理学和人机交互都是一项重要挑战。此研究旨在开发一个新颖的情感识别管道，以应对这一问题，并填补了针对自闭症的人机交互领域的空白。", "method": "利用一个结合了基于ResNet-50的卷积神经网络（CNN）和三层图卷积网络（GCN）的混合模型，在Mediapipe FaceMesh特征上训练，该模型融合了视觉与几何特性。情感标签通过DeepFace和FER两种模型加权集成生成软标签，并且最终分类依赖于KL散度优化的嵌入。", "result": "所提出的方法在建模细微的情感反应中表现出色，并为自闭症儿童的情绪分析提供了重要的基础，特别是对于个性化辅助技术的发展具有重要意义。", "conclusion": "这是印度首个大规模、基于真实世界数据集和管道的研究成果，专注于使用社交机器人进行自闭症情绪分析，对未来的个性化辅助技术有着积极影响。"}}
{"id": "2512.12207", "pdf": "https://arxiv.org/pdf/2512.12207", "abs": "https://arxiv.org/abs/2512.12207", "authors": ["Jiangen He", "Jiqun Liu"], "title": "Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": "CHIIR 2026", "summary": "Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.", "AI": {"tldr": "探讨对话搜索系统中不同来源呈现方式对用户注意力、交互和说服力的影响。", "motivation": "研究引用或来源展示格式如何影响用户的参与度，特别是对于越来越多提供源引的对话搜索引擎。", "method": "通过394名参与者进行众筹实验，比较四种不同的来源展示设计：可折叠列表、悬停卡片、页脚列表和并排边栏。分析这些界面设计对用户体验感知指标的影响，并评估其在知识增长、兴趣变化和同意变动方面的效果。", "result": "高可见度的界面显著增加了用户的源引用浏览次数，尽管点击率仍然很低。然而，接口设计对用户体验感知指标影响较小，但显著影响了知识获取、兴趣变化和意见改变的变化。随着使用频率增加，高可见度界面起初减少用户的知识获得和兴趣，随后这些积极效应显现。边栏条件独特地增加了同意的变动。", "conclusion": "单独的来源呈现方式可能不会增强参与度，并且在提供不足的信息源时甚至可能会降低其效果。"}}
{"id": "2512.12206", "pdf": "https://arxiv.org/pdf/2512.12206", "abs": "https://arxiv.org/abs/2512.12206", "authors": ["Jeongjun Park", "Sunwook Hwang", "Hyeonho Noh", "Jin Mo Yang", "Hyun Jong Yang", "Saewoong Bahk"], "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions. This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance. Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.", "AI": {"tldr": "提出了一种用于驾驶员活动识别的输入尺寸无关视觉变换器ISA-ViT和ALERT数据集。", "motivation": "为了应对缺少大规模真实世界UWB数据集以及固定输入大小Vision Transformers难以适应非标准维度雷达数据的问题，本文提出了解决方案。", "method": "构建了包含10,220个样本的ALERT数据集；设计了一种适用于雷达DAR的输入尺寸无关视觉变换器ISA-ViT，并采用领域融合策略提升分类性能。", "result": "实验表明，与现有ViT方法相比，ISA-ViT在UWB基线上的准确率提升了22.68%。", "conclusion": "通过公开ALERT数据集和输入大小无关策略，本文促进了更强大、可扩展的分心驾驶检测系统的开发。"}}
{"id": "2512.12205", "pdf": "https://arxiv.org/pdf/2512.12205", "abs": "https://arxiv.org/abs/2512.12205", "authors": ["Peizheng Li", "Ioannis Mavromatis", "Ajith Sahadevan", "Tim Farnham", "Adnan Aijaz", "Aftab Khan"], "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection", "categories": ["cs.CV"], "comment": "10 pages, 7 figures. Submitted to Data in Brief (Elsevier)", "summary": "We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.", "AI": {"tldr": "该论文构建了一个大型、长期的城市路灯图像数据集，用于视觉监控和时空漂移检测。", "motivation": "通过部署固定角度的摄像头采集城市路灯图像，为研究视觉漂移、异常检测以及智能城市的MLOps策略提供实证基准。", "method": "使用卷积变分自编码器(CNN-VAE)模型对每个摄像机节点和日间/夜间图像集进行单独训练，并定义了两种样本级漂移度量：相对中心点漂移与相对重建误差。", "result": "该数据集包含超过526,000张图像，每张图像附带时间戳、GPS坐标和设备标识符等详细元数据。这些图像和支持再利用的数据可以在JPEG和CSV格式下公开获取。", "conclusion": "这个精细的基准测试为长期模型稳定性评估提供了现实依据，并支持了街灯监测、天气推断及城市场景理解等下游应用。"}}
{"id": "2512.12203", "pdf": "https://arxiv.org/pdf/2512.12203", "abs": "https://arxiv.org/abs/2512.12203", "authors": ["Eric J. Elias", "Michael Esswein", "Jonathan P. How", "David W. Miller"], "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion", "categories": ["cs.RO", "cs.CV"], "comment": "18 pages, 11 figures. To be published in proceedings of AIAA SCITECH 2026 Forum", "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.", "AI": {"tldr": "本文研究了在不同光照和轨迹条件下，使用可见光和热红外图像融合技术来提高单目SLAM算法导航精度的方法。", "motivation": "随着轨道操作的增多，精确导航未知的空间物体变得越来越重要。传统的摄像头和激光雷达存在各自的局限性，而通过结合可见光和热红外图像的优点可以提升导航性能。", "method": "模拟低地球轨道目标卫星在可见光和热红外波段的照片，并使用像素级融合方法创建复合图像以增强各自的优势。比较单目SLAM算法在不同光照条件下仅用可见光、仅用热红外及图像融合情况下的导航误差。", "result": "结果表明，采用图像融合技术可以显著提升导航性能。", "conclusion": "通过使用可见光和热红外图像的像素级融合方法，可以在多种照明条件和轨迹下实现更精确的空间物体导航。"}}
{"id": "2512.12202", "pdf": "https://arxiv.org/pdf/2512.12202", "abs": "https://arxiv.org/abs/2512.12202", "authors": ["Yossi Azar", "Niv Buchbinder", "Tomer Epshtein"], "title": "Load Balancing with Duration Predictions", "categories": ["cs.DS"], "comment": "Brief announcement presented at SPAA 25 (2025)", "summary": "We study the classic fully dynamic load balancing problem on unrelated machines where jobs arrive and depart over time and the goal is minimizing the maximum load, or more generally the l_p-norm of the load vector. Previous work either studied the clairvoyant setting in which exact durations are known to the algorithm, or the unknown duration setting in which no information on the duration is given to the algorithm. For the clairvoyant setting algorithms with polylogarithmic competitive ratios were designed, while for the unknown duration setting strong lower bounds exist and only polynomial competitive factors are possible. We bridge this gap by studying a more realistic model in which some estimate/prediction of the duration is available to the algorithm. We observe that directly incorporating predictions into classical load balancing algorithms designed for the clairvoyant setting can lead to a notable decline in performance. We design better algorithms whose performance depends smoothly on the accuracy of the available prediction. We also prove lower bounds on the competitiveness of algorithms that use such inaccurate predictions.", "AI": {"tldr": "研究了具有持续时间预测的动态负载均衡问题，提出了依赖于预测准确性的算法并分析了其竞争力。", "motivation": "现有工作在完全知晓或未知任务持续时间的情况下设计了负载均衡算法，但缺少介于两者之间的模型。引入一种新的设置，在该设置中算法可以使用对任务持续时间的部分估计来优化性能。", "method": "提出了新的依赖于预测准确性的负载均衡算法，并分析这些算法的竞争力下限。", "result": "设计了一种性能随预测准确性平滑变化的新算法，证明了这类算法的一些竞争性界限。", "conclusion": "研究结果表明，在部分知道任务持续时间的情况下可以设计出优于传统方法的新型负载均衡算法。"}}
{"id": "2512.12201", "pdf": "https://arxiv.org/pdf/2512.12201", "abs": "https://arxiv.org/abs/2512.12201", "authors": ["Predrag K. Nikolić", "Robert Prentner"], "title": "Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages, 7 figures, presented at SIGGRAPH VRCAI 25", "summary": "Large language models (LLMs) have often been characterized as \"stochastic parrots\" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.", "AI": {"tldr": "论文通过AI驱动的交互艺术装置，展示大型语言模型在适当的对话环境中可以发展出新兴的概念结构，并提出Epistemoverse概念，以实现人类和机器认知交织的知识保存。", "motivation": "质疑大型语言模型只是“随机鹦鹉”的观点，希望通过研究证明这些模型能够在互动中展现更深层次的认知能力，并探索它们如何帮助保护、重新诠释及拓展知识遗产。", "method": "通过分析由AI重现的哲学家之间的对话，特别是Syntropic Counterpoints项目中的交互艺术装置，探讨了机器对话产生的推理一致性、反思性问题和创造性综合。", "result": "发现大型语言模型在适当的环境中能够生成有意义的对话，并且这些对话可以促进知识的保存、重新诠释及扩展。", "conclusion": "提出了Epistemoverse的概念，即一个由人类和机器认知交织的知识元宇宙，在这个虚拟空间中可以进行新的知识交流、数字遗产保护以及协作创意活动。"}}
{"id": "2512.12199", "pdf": "https://arxiv.org/pdf/2512.12199", "abs": "https://arxiv.org/abs/2512.12199", "authors": ["Ercan Erkalkan", "Vedat Topuz", "Ayça Ak"], "title": "Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms", "categories": ["cs.CV", "cs.AI"], "comment": "Conference paper in 17th International Scientific Studies Congress proceedings. Topic: thermal+RGB rule level fusion, RDP boundary simplification, leader follower guidance, sub 50ms embedded SoC, minimal communications for wildfire perimeter tracking. Thermal RGB Fusion for Micro-UAV", "summary": "This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.", "AI": {"tldr": "本文介绍了一种轻量级的微无人机团队在有限带宽条件下进行野火边界追踪的方法。", "motivation": "为了实现在野火环境中的快速部署和紧急侦察，该研究提出了一个能够在低通信条件下的高效边界跟踪方法。", "method": "利用热图像生成粗糙热点区域掩模，并通过RGB图像提供边缘线索以抑制虚假检测。采用规则级融合策略选取边界候选并简化它们。系统包括周期性信标和惯性反馈回路，保持轨迹稳定，同时限制每帧像素操作并在嵌入式SoC平台上实现亚50毫秒延迟。", "result": "小规模仿真表明，与纯边缘跟踪相比，该方法减少了路径长度和边界抖动，并且在环境覆盖度方面表现良好。电池消耗和计算利用率验证了能够在标准微平台上的可行性。", "conclusion": "提出的方法能够快速部署于野外并进行紧急侦察任务，在保持环境覆盖的同时实现了低通信量下的高效边界追踪。"}}
{"id": "2512.12196", "pdf": "https://arxiv.org/pdf/2512.12196", "abs": "https://arxiv.org/abs/2512.12196", "authors": ["Xiaoxuan Tang", "Xinping Lei", "Chaoran Zhu", "Shiyun Chen", "Ruibin Yuan", "Yizhi Li", "Changjae Oh", "Ge Zhang", "Wenhao Huang", "Emmanouil Benetos", "Yang Liu", "Jiaheng Liu", "Yinghao Ma"], "title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.", "AI": {"tldr": "提出了一种自动多智能体系统AutoMV，用于从歌曲生成全长度音乐视频。", "motivation": "现有的方法在生成音乐视频时会产生短而断续的片段，并且无法很好地与音乐结构、节奏或歌词保持一致，缺乏时间连贯性。因此需要一种新的方法来解决这个问题。", "method": "AutoMV通过应用音乐处理工具提取歌曲中的音乐属性作为上下文输入给后续智能体。然后由编剧和导演智能体设计剧本、定义角色，并指定摄像机指令；最后使用图像生成器和视频生成器生成关键帧和不同场景的视频，Verifier智能体评估其输出。", "result": "提出的AutoMV系统在四个高层次类别（音乐内容、技术、后期制作、艺术）中的十二个细粒度标准上超过了现有基线方法，并且缩小了与专业音乐视频之间的差距。", "conclusion": "AutoMV通过多智能体协作生成连贯的全长度音乐视频。虽然使用大型多模式模型作为自动MV评价者有前景，但仍然落后于人类专家，未来仍需进一步研究。"}}
{"id": "2512.12194", "pdf": "https://arxiv.org/pdf/2512.12194", "abs": "https://arxiv.org/abs/2512.12194", "authors": ["Min-Won Seo", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping", "categories": ["cs.RO"], "comment": "18 pages, 17 figures", "summary": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.", "AI": {"tldr": "提出了一种可扩展的信息理论主动探索框架B-ActiveSEAL，该框架在长期操作和大规模环境中处理紧密耦合的定位和映射不确定性。", "motivation": "解决机器人活动探索过程中集成定位和地图绘制所需决策过程中的紧密耦合不确定性问题，在大型环境下的长时间运行中计算上难以实现。", "method": "提出了一种适应性地平衡地图不确定性和定位不确定性的框架，同时容纳广泛的熵度量类以支持灵活的主动探索，并引入行为熵作为有效的信息衡量标准。", "result": "理论分析和在开源地图及ROS-Unity仿真环境中的实验验证了B-ActiveSEAL能够实现良好的探索与利用折衷，在不同复杂环境中产生多样化的自适应探索行为，优于代表性基线方法。", "conclusion": "通过提供一种有效的信息度量标准，该框架解决了紧密耦合的定位和映射不确定性问题，并在各种环境下实现了灵活、高效的主动探索。"}}
{"id": "2512.12193", "pdf": "https://arxiv.org/pdf/2512.12193", "abs": "https://arxiv.org/abs/2512.12193", "authors": ["Xuancheng Xu", "Yaning Li", "Sisi You", "Bing-Kun Bao"], "title": "SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.", "AI": {"tldr": "本文提出了一种名为SMRABooth的方法，用于定制化视频生成，该方法利用自监督编码器和光流编码器对主体和运动进行对象级别的表示，并通过LoRA微调过程实现这些表示的对齐。", "motivation": "现有的定制化视频生成技术在确保主体外观相似性和运动模式一致性方面存在困难。为了克服这一挑战，本文提出了SMRABooth方法来提供基于自监督编码器和光流编码器的对象级别指导。", "method": "SMRABooth通过三个核心阶段实现：首先利用自监督编码器捕捉对象的整体结构并增强高级语义一致性；其次，使用光流编码器从视觉外观中独立捕获运动轨迹；最后，提出了一种稀疏LoRAs注射策略来减少主体和运动表示之间的干扰。", "result": "实验结果表明SMRABooth在定制化视频生成方面表现出色，在保持一致的主体外观和运动模式的同时展示了其有效性。", "conclusion": "本文通过结合自监督编码器和光流编码器，成功地解决了现有方法难以同时实现主体相似性和运动一致性的问题。"}}
{"id": "2512.12187", "pdf": "https://arxiv.org/pdf/2512.12187", "abs": "https://arxiv.org/abs/2512.12187", "authors": ["David Gamba", "Daniel M. Romero", "Grant Schoenebeck"], "title": "The Ideological Turing Test for Moderation of Outgroup Affective Animosity", "categories": ["cs.CY", "cs.HC", "physics.soc-ph"], "comment": "32 pages", "summary": "Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization. We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up. Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($Δ=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($Δ=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $Δ=+0.91$ SD; debate: $Δ=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%). These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions.", "AI": {"tldr": "引入并测试了意识形态图灵测试，通过参与者采用和捍卫对立观点来减少情感敌意和观念极化。", "motivation": "解决日益增长的对思想对手的情感敌意带来的社会挑战", "method": "进行了混合设计实验（N=203），包括两种模态（辩论/写作）及两种视角采取方式（自身/对立观点）。参与者在结构化的互动中捍卫分配立场，并由同行评判。通过即时和4-6周后的后续测量，评估情感敌意和意识形态位置的变化。", "result": "从对立观点出发进行书写立即减少了情感敌意最大幅度（Δ=+0.45 SD），但随后下降。相比之下，辩论形式的模态在即时和后续测量中保持了显著的情感敌意减少（Δ=+0.37 SD）。两种模式下，采纳对立视角均带来了显著的情感位置变化，并持续至后续阶段。", "conclusion": "意识形态图灵测试展示出作为降低极化潜在工具的潜力，尤其当结合视角采取与反思性对抗互动时。"}}
{"id": "2512.12184", "pdf": "https://arxiv.org/pdf/2512.12184", "abs": "https://arxiv.org/abs/2512.12184", "authors": ["Liangshou Zhang", "Yanbin Liu", "Hanchi Liu", "Zheng Sun", "Haozhi Zhang", "Yang Zhang", "Xin Ma"], "title": "DCAF-Net: Dual-Channel Attentive Fusion Network for Lower Limb Motion Intention Prediction in Stroke Rehabilitation Exoskeletons", "categories": ["q-bio.QM", "cs.HC"], "comment": "6 pages, 6 figures", "summary": "Rehabilitation exoskeletons have shown promising results in promoting recovery for stroke patients. Accurately and timely identifying the motion intentions of patients is a critical challenge in enhancing active participation during lower limb exoskeleton-assisted rehabilitation training. This paper proposes a Dual-Channel Attentive Fusion Network (DCAF-Net) that synergistically integrates pre-movement surface electromyography (sEMG) and inertial measurement unit (IMU) data for lower limb intention prediction in stroke patients. First, a dual-channel adaptive channel attention module is designed to extract discriminative features from 48 time-domain and frequency-domain features derived from bilateral gastrocnemius sEMG signals. Second, an IMU encoder combining convolutional neural network (CNN) and attention-based long short-term memory (attention-LSTM) layers is designed to decode temporal-spatial movement patterns. Third, the sEMG and IMU features are fused through concatenation to enable accurate recognition of motion intention. Extensive experiment on 11 participants (8 stroke subjects and 3 healthy subjects) demonstrate the effectiveness of DCAF-Net. It achieved a prediction accuracies of 97.19% for patients and 93.56% for healthy subjects. This study provides a viable solution for implementing intention-driven human-in-the-loop assistance control in clinical rehabilitation robotics.", "AI": {"tldr": "提出了一种基于双通道注意力融合网络的下肢运动意图预测方法，用于中风康复外骨骼。", "motivation": "准确及时地识别患者的运动意图对于提高主动参与度在下肢外骨骼辅助康复训练中的作用至关重要。", "method": "设计了结合表面肌电图和惯性测量单元数据的双通道注意力融合网络，通过提取特征并融合以实现准确的运动意图预测。", "result": "实验结果显示，该方法对患者的预测精度达到97.19%，健康受试者为93.56%。", "conclusion": "这项研究提供了一种可行的方法来实现在临床康复机器人中基于意图的人机交互协助控制。"}}
{"id": "2512.12182", "pdf": "https://arxiv.org/pdf/2512.12182", "abs": "https://arxiv.org/abs/2512.12182", "authors": ["Xinyu Gao"], "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "categories": ["cs.AI"], "comment": null, "summary": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.", "AI": {"tldr": "提出了一种基于两阶段注意力三元组增强和U-KAN扩散模型的新型少样本知识图谱补全框架。", "motivation": "现有研究在处理少样本知识图谱补全时，未能充分利用邻域信息或忽略了对比信号分布特征。", "method": "通过引入两阶段注意力机制来提升三元组表示，并结合U-KAN扩散模型以生成性视角解决少样本问题。", "result": "实验结果表明该方法达到了新的最先进水平。", "conclusion": "所提出的框架在处理少样本知识图谱补全任务时表现出色，能够有效利用图结构中的信息和对比信号分布特征。"}}
{"id": "2512.12177", "pdf": "https://arxiv.org/pdf/2512.12177", "abs": "https://arxiv.org/abs/2512.12177", "authors": ["Aydin Ayanzadeh", "Tim Oates"], "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation", "categories": ["cs.AI"], "comment": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.", "AI": {"tldr": "本文提出了一种利用大型语言模型将平面图转换为可导航的知识图谱并生成人类可读的导航指令的新方法，以改善视障人士在动态环境中的室内导航能力。", "motivation": "当前基于基础设施的解决方案限制了视觉障碍者在不断变化环境中安全导航的能力。因此，本文旨在通过利用大型语言模型减少人工预处理步骤，并提高导航准确性的新型导航方案来解决这一问题。", "method": "Floorplan2Guide方法通过整合大型语言模型从建筑布局中提取空间信息，并生成可读的导航指令，该方法使用少样本学习以提高在模拟和现实世界评估中的导航准确性。实验显示，Claude 3.7 Sonnet在5-shot提示下，在MP-1平面图上的短、中、长路线准确率分别为92.31%、76.92%和61.54%，证明了图形表示和上下文学习可以增强导航性能。", "result": "实验结果表明，少样本学习比零样本学习在模拟和现实世界评估中的导航准确性更高。Claude 3.7 Sonnet模型的准确率最高，在MP-1平面图上的短、中、长路线分别为92.31%、76.92%和61.54%，并且基于图形的空间结构的成功率为所有模型平均高出15.4%，这表明图形表示法和上下文学习提高了导航性能。", "conclusion": "本文通过使用大型语言模型的Floorplan2Guide方法，证明了其可以更精确地为视觉障碍用户提供室内导航服务。"}}
{"id": "2512.12175", "pdf": "https://arxiv.org/pdf/2512.12175", "abs": "https://arxiv.org/abs/2512.12175", "authors": ["Haoyang Chen", "Richong Zhang", "Junfan Chen"], "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.", "AI": {"tldr": "本文提出了一个新的视角来理解ICL的工作机制，通过构建一个标签传播框架来确保演示样本的标签一致性。", "motivation": "现有方法在选择演示样例时没有保证标签的一致性，这限制了模型的有效性。作者从贝叶斯视角出发重新思考ICL，并将其视为一种隐式归纳学习法。", "method": "基于新的理解，作者建立了一个标签传播框架并将该框架应用于数据合成和TopK采样的改进中，提出了带有合成数据的TopK方法（TopK-SD）以获得一致性更好的演示样例。", "result": "实验表明，TopK-SD在多个基准上超越了原始的TopK采样方法。", "conclusion": "本文从一个新的角度解释了ICL的工作机制，并提出了一种新的标签传播框架和数据合成方法来提高模型性能。"}}
{"id": "2512.12168", "pdf": "https://arxiv.org/pdf/2512.12168", "abs": "https://arxiv.org/abs/2512.12168", "authors": ["Zheng Huang", "Kiran Ramnath", "Yueyan Chen", "Aosong Feng", "Sangmin Woo", "Balasubramaniam Srinivasan", "Zhichao Xu", "Kang Zhou", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "Diffusion Language Model Inference with Monte Carlo Tree Search", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.", "AI": {"tldr": "本文提出了一种新的框架MEDAL，将蒙特卡洛树搜索应用于扩散语言模型的推理过程，以改善文本生成的质量。", "motivation": "现有的扩散语言模型在推断过程中使用启发式方法来确定解码路径，这往往会导致次优的结果。为了引入一种更合理的搜索机制，作者提出了MEDAL框架，通过蒙特卡洛树搜索来探索潜在的未屏蔽序列轨迹。", "method": "本文提出了一种新的框架MEDAL，它将蒙特卡洛树搜索应用于扩散语言模型的推理过程，在初始化阶段使用蒙特卡洛树搜索来探索可能的未屏蔽序列路径，并优先选择提高模型置信度的标记。", "result": "在多个基准测试中，与现有的推断策略相比，MEDAL取得了高达22.0%的改进。", "conclusion": "本文提出的方法为基于搜索的扩散语言模型推断提供了一种新的范式。"}}
{"id": "2512.12167", "pdf": "https://arxiv.org/pdf/2512.12167", "abs": "https://arxiv.org/abs/2512.12167", "authors": ["Yoav Gelberg", "Koshi Eguchi", "Takuya Akiba", "Edoardo Cetin"], "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.", "AI": {"tldr": "通过移除位置嵌入来扩展预训练语言模型的上下文长度，无需昂贵的微调。", "motivation": "位置嵌入在预训练中起重要作用，但过度依赖它们会导致测试时无法泛化到未见过长度的序列。去除位置嵌入可以提高模型对长序列的理解能力。", "method": "提出Drop Positional Embeddings (DroPE) 方法，在预训练完成后移除位置嵌入，并进行短周期校准。", "result": "DroPE方法实现了零样本上下文扩展，无需针对长序列微调即可有效提升模型性能。实验结果表明此方法优于其他专门架构和传统的旋转位置嵌入扩展方式。", "conclusion": "通过移除预训练后的位置嵌入可以有效地将语言模型的上下文长度无缝延伸，同时保持原有训练环境下的能力不变。"}}
{"id": "2512.12166", "pdf": "https://arxiv.org/pdf/2512.12166", "abs": "https://arxiv.org/abs/2512.12166", "authors": ["Jane Hsieh", "Emmie Regan", "Jose Elizalde", "Haiyi Zhu"], "title": "Beyond Riding: Passenger Engagement with Driver Labor through Gamified Interactions", "categories": ["cs.HC"], "comment": null, "summary": "Modern cities increasingly rely on ridesharing services for on-demand transportation, which offer consumers convenience and mobility across the globe. However, these marketed consumer affordances give rise to burdens and vulnerabilities that drivers shoulder alone, without adequate infrastructures for labor regulations or consumer-led advocacy. To effectively and sustainably advance protections and oversight for drivers, consumers must first be aware of the labor, logistics and costs involved with ridehail driving. To motivate consumers to practice more socially responsible consumption behaviors and foster solidarity with drivers, we explore the potential for gamified in-ride interactions to facilitate engagement with real (and lived) driver experiences. Through nine workshops with 19 drivers and 15 passengers, we surface how gamified in-ride interactions revealed passenger knowledge gaps around latent ridehail conditions, prompt reflection and shifts in perception of their relative power and consumption behaviors, and highlight drivers' preferences for creating more immersive and contextualized service experiences, and identify opportunities to design safe and appropriate passenger-driver interactions that motivate solidarity with drivers. In sum, we advance conceptual understandings of in-ride social and managerial relations, demonstrate potential for future worker advocacy in algorithmically-managed labor, and offer design guidelines for more human-centered workplace technologies.", "AI": {"tldr": "研究通过九个工作坊探讨了游戏化乘车互动如何促进乘客与司机之间的社会联系，旨在设计更人性化的职场技术。", "motivation": "为了提高消费者对骑手劳动的认识和责任感，并鼓励他们支持骑手的权益，研究探索利用游戏化方式在乘车期间进行互动的方法。", "method": "通过九个工作坊收集19名司机和15名乘客的意见，探讨了游戏化乘车互动如何揭示乘客的知识差距、改变他们的消费行为以及增强与司机的情感联系。", "result": "研究发现游戏化乘车互动可以增加乘客对骑手劳动条件的认识，并促使他们反思自己的权力地位及行为方式；同时，游戏化的服务体验也能得到司机的欢迎。", "conclusion": "该论文为算法管理下的工人倡导提供了潜在的方法论和设计准则，强调了在技术中融入人文关怀的重要性。"}}
{"id": "2512.12165", "pdf": "https://arxiv.org/pdf/2512.12165", "abs": "https://arxiv.org/abs/2512.12165", "authors": ["Daniel Adebi", "Sagnik Majumder", "Kristen Grauman"], "title": "Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video", "categories": ["cs.CV"], "comment": null, "summary": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.", "AI": {"tldr": "本文提出了一个音频视觉框架，用于在实际视频中利用被动场景声音进行相对相机姿态估计。", "motivation": "虽然视觉方法已经取得了很大进步，但在运动模糊或遮挡等视觉退化条件下仍然表现不佳。因此，研究如何结合音频信号来增强相机姿态估计的鲁棒性具有重要意义。", "method": "本文提出了一种将方向到达（DOA）光谱和双耳嵌入集成到最先进的视觉单独姿势估计模型中的简单但有效的方法。通过这种方式，能够利用声音信息作为视觉线索的补充。", "result": "实验结果表明，在两个大型数据集上，该方法比单纯使用视觉基线的方法表现更好，并且当视觉信息受损时仍然保持鲁棒性。", "conclusion": "这是首次成功地在实际视频中利用音频信号进行相对相机姿态估计的工作。研究证明了日常环境声音作为经典空间挑战的重要补充信号的潜力。"}}
{"id": "2512.12146", "pdf": "https://arxiv.org/pdf/2512.12146", "abs": "https://arxiv.org/abs/2512.12146", "authors": ["Ayush Vaibhav Bhatti", "Deniz Karakay", "Debottama Das", "Nilotpal Rajbongshi", "Yuito Sugimoto"], "title": "Open Horizons: Evaluating Deep Models in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.", "AI": {"tldr": "论文评估了深度模型在开放世界部署中的表现，特别是在开放集识别和少量样本类别增量学习任务上。", "motivation": "为了满足实际应用中需要同时识别已知类别的新实例以及未知类别的需求，研究者通过实验探索了不同预训练模型和评分机制在此场景下的性能。", "method": "论文使用三个固定的视觉编码器（ResNet-50、ConvNeXt-Tiny和CLIP ViT-B/16）进行开放集识别，并采用线性探测及四种后处理评分函数（MSP、Energy、Mahalanobis和kNN）。同时，比较了三种少量样本类别增量学习方法（SPPR、OrCo和ConCM），使用部分冻结的ResNet-50模型。", "result": "CLIP在开放集识别任务中表现出最强的已知与未知类别的区分能力；Energy评分函数提供了最稳定的表现。在少量样本类别增量学习上，ConCM方法在10-shot设置下达到了84.7%的准确率，并且混淆矩阵最为清晰。", "conclusion": "实验揭示了骨干网络架构和评分机制对未知检测的影响以及基于原型的方法如何通过渐进适应来减轻灾难性遗忘。"}}
{"id": "2512.12142", "pdf": "https://arxiv.org/pdf/2512.12142", "abs": "https://arxiv.org/abs/2512.12142", "authors": ["Björn Lütjens", "Patrick Alexander", "Raf Antwerpen", "Til Widmann", "Guido Cervone", "Marco Tedesco"], "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph", "physics.data-an"], "comment": null, "summary": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.", "AI": {"tldr": "开发了一种深度学习模型，用于融合遥感数据和物理模型来生成格网化的表面融水地图。", "motivation": "提高对冰川融化过程的理解，并克服现有方法在时间和空间分辨率之间的权衡。", "method": "使用UNet和DeepLabv3+等深度学习算法，结合合成孔径雷达（SAR）、被动微波（PMW）及数字高程模型（DEM），进行时空降尺度处理。", "result": "与仅依赖区域气候模型或被动微波数据的方法相比，该方法准确性显著提高。具体来说，在研究区域内精度为95%。", "conclusion": "提出了一种基于深度学习的改进方法，有效提高了表面融水分布图的空间和时间分辨率，并公开了相关数据集作为基准进行比较分析。"}}
{"id": "2512.12135", "pdf": "https://arxiv.org/pdf/2512.12135", "abs": "https://arxiv.org/abs/2512.12135", "authors": ["Lucine L. Oganesian", "Saba Hashemi", "Maryam M. Shanechi"], "title": "BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": "Published at the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025). Code available at https://github.com/ShanechiLab/BaRISTA", "summary": "Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.", "AI": {"tldr": "提出了一种新的时空变换模型，用于多区域神经活动的自我监督学习，并展示了调整空间尺度对下游解码性能的影响。", "motivation": "当前的变压器模型在处理复杂的脑电记录时存在不足，需要更好地编码空间信息和设计自监督任务以提高下游解码性能。", "method": "提出了一种新的时空变换模型BaRISTA以及相应的自我监督掩码隐含重构任务。该方法允许调整令牌编码和掩码的空间尺度，并在公共的多区域脑电数据集上进行实验。", "result": "结果显示，与通道级别相比，在更大空间尺度下的令牌编码可以改善下游解码性能，并且模型可以在保持准确通道级神经重建的同时实现区域级别的令牌编码。", "conclusion": "该研究提供了一个探索令牌编码和掩码的空间尺度的建模框架，并展示了这种灵活性对于多区域大脑活动自监督预训练的重要性。"}}
{"id": "2512.12129", "pdf": "https://arxiv.org/pdf/2512.12129", "abs": "https://arxiv.org/abs/2512.12129", "authors": ["Protima Nomo Sudro", "Anton Ragni", "Thomas Hain"], "title": "A comparative study of generative models for child voice conversion", "categories": ["cs.SD"], "comment": "6 pages, 5 figures", "summary": "Generative models are a popular choice for adult-to-adult voice conversion (VC) because of their efficient way of modelling unlabelled data. To this point their usefulness in producing children speech and in particular adult to child VC has not been investigated. For adult to child VC, four generative models are compared: diffusion model, flow based model, variational autoencoders, and generative adversarial network. Results show that although converted speech outputs produce by those models appear plausible, they exhibit insufficient similarity with the target speaker characteristics. We introduce an efficient frequency warping technique that can be applied to the output of models, and which shows significant reduction of the mismatch between adult and child. The output of all the models are evaluated using both objective and subjective measures. In particular we compare specific speaker pairing using a unique corpus collected for dubbing of children speech.", "AI": {"tldr": "研究比较了四种生成模型在成人到儿童声音转换中的效果，并引入了一种频率扭曲技术以减少成年和儿童语音的差异。", "motivation": "探讨生成模型在成人到儿童声音转换任务中的适用性，以及如何改进现有的模型以提高转换后的语音与目标说话人特征的一致性。", "method": "比较了扩散模型、流基模型、变分自编码器和生成对抗网络四种生成模型，并引入频率扭曲技术优化输出结果；使用客观和主观评价指标评估各模型效果。", "result": "虽然各种模型的转换结果显示一定的可信度，但与目标说话人的特征相似度不足；频率扭曲技术显著减少了成人到儿童语音之间的不匹配性。", "conclusion": "四种生成模型在成人到儿童声音转换中表现出潜在的应用价值，然而通过引入适当的后处理技术（如频率扭曲），可以进一步改善转换效果。"}}
{"id": "2512.12128", "pdf": "https://arxiv.org/pdf/2512.12128", "abs": "https://arxiv.org/abs/2512.12128", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin R. Murphy"], "title": "A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables. To appear AAAI'26", "summary": "This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\\% Macro IoU. If spatial alignment is not considered, approximately 8\\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.", "AI": {"tldr": "本文提出了一个用于评估灾难影像中道路损坏情况的大型基准数据集，并提供了针对该数据集中小无人机（sUAS）灾后影像进行训练的18个基线模型。", "motivation": "当前存在的灾害后道路损坏评估数据集规模较小或依赖于低分辨率图像，不足以检测到对应急管理人员重要的现象。本文通过标注657.25公里的道路，并提供9,184条道路线调整以实现空间对齐来解决这些问题。", "method": "该研究标记了657.25公里的公路根据一个10类标签模式，然后在2024年飓风Debby和Helene期间部署机器学习模型。并测试这些模型在实际操作中的表现。", "result": "发现当基线模型应用于未对齐的道路线上时，其性能平均下降了5.596%的Macro IoU，约8%（11公里）的不良道路条件被错误标记，而大约9%（59公里）的道路偏移实际路线。", "conclusion": "该研究指出，机器学习、计算机视觉和机器人技术社区应解决这些差距以提高灾难期间决策的有效性和准确性。"}}
{"id": "2512.12121", "pdf": "https://arxiv.org/pdf/2512.12121", "abs": "https://arxiv.org/abs/2512.12121", "authors": ["Ahmad Chamma", "Omar El Herraoui", "Guokan Shang"], "title": "MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \\emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \\emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \\emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.", "AI": {"tldr": "介绍MixtureKit框架，用于构建、训练和分析混合专家模型", "motivation": "提高对混合专家（MoE）模型的理解与应用，特别是在多语言代码切换数据上的性能提升", "method": "提供三种方法：传统MoE, BTX(分支-训练-混合), BTS(分支-训练-缝合)；自动修改模型配置并保存统一的检查点用于推断或微调，并提供可视化接口以查看路由决策等信息", "result": "使用MixtureKit训练的BTX基线模型在多个基准上优于密集型模型", "conclusion": "MixtureKit作为研究和开发混合专家系统的基础，具有广泛的应用潜力"}}
{"id": "2512.12115", "pdf": "https://arxiv.org/pdf/2512.12115", "abs": "https://arxiv.org/abs/2512.12115", "authors": ["Momin N. Siddiqui", "Vincent Cavez", "Sahana Rangasrinivasan", "Abbie Olszewski", "Srirangaraj Setlur", "Maneesh Agrawala", "Hari Subramonyam"], "title": "Teaching Spell Checkers to Teach: Pedagogical Program Synthesis for Interactive Learning", "categories": ["cs.HC"], "comment": "18 pages, 6 Figures, 4 Tables", "summary": "Spelling taught through memorization often fails many learners, particularly children with language-based learning disorders who struggle with the phonological skills necessary to spell words accurately. Educators such as speech-language pathologists (SLPs) address this instructional gap by using an inquiry-based approach to teach spelling that targets the phonology, morphology, meaning, and etymology of words. Yet, these strategies rarely appear in everyday writing tools, which simply detect and autocorrect errors. We introduce SPIRE (Spelling Inquiry Engine), a spell check system that brings this inquiry-based pedagogy into the act of composition. SPIRE implements Pedagogical Program Synthesis, a novel approach for operationalizing the inherently dynamic pedagogy of spelling instruction. SPIRE represents SLP instructional moves in a domain-specific language, synthesizes tailored programs in real-time from learner errors, and renders them as interactive interfaces for inquiry-based interventions. With SPIRE, spelling errors become opportunities to explore word meanings, word structures, morphological families, word origins, and grapheme-phoneme correspondences, supporting metalinguistic reasoning alongside correction. Evaluation with SLPs and learners shows alignment with professional practice and potential for integration into writing workflows.", "AI": {"tldr": "本文介绍了一种名为SPIRE的拼写检查系统，该系统使用基于教育的方法来纠正和教授拼写错误。", "motivation": "传统的拼写教学往往依赖于记忆法，这对语言学习障碍儿童来说效果不佳。通过引入一个结合语音学、形态学、意义和词源的教学方法，可以更有效地帮助他们理解并掌握正确拼写。", "method": "SPIRE采用了一种称为教育程序合成的新技术，该技术能够实时生成针对特定错误的交互式教学界面，并将这些策略集成到写作工具中。", "result": "评估结果显示，SPIRE系统与专业实践相吻合，展示了在写作流程中的潜在应用价值。", "conclusion": "通过引入基于教育的方法，拼写检查可以不仅纠正错误，还能提供学习和探索机会，促进语言理解能力的提升。"}}
{"id": "2512.12109", "pdf": "https://arxiv.org/pdf/2512.12109", "abs": "https://arxiv.org/abs/2512.12109", "authors": ["Allen Daniel Sunny"], "title": "A neuro-symbolic framework for accountability in public-sector AI", "categories": ["cs.CY", "cs.AI", "cs.LO"], "comment": "Master's thesis, University of Maryland, College Park (2025)", "summary": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.", "AI": {"tldr": "开发一个基于法律规则的解释框架，用于验证公共部门AI系统的决策合理性。", "motivation": "当前自动资格系统生成的解释常常未能反映授权这些决定的法律规定。此框架旨在解决这一问题，提高自动化决定的基础可追溯性和可争议性。", "method": "结合结构化的政策和程序手册中的规定、规则提取管道以及基于求解器的推理层来验证决策是否符合法律。", "result": "案例评估显示该框架能够检测出与法律规定不一致的解释，并支持过程问责制。", "conclusion": "该框架提高了自动化福利决定系统的可追溯性，有助于提升公共信任并促进政策合规。"}}
{"id": "2512.12108", "pdf": "https://arxiv.org/pdf/2512.12108", "abs": "https://arxiv.org/abs/2512.12108", "authors": ["Dashti A. Ali", "Aras T. Asaad", "Jacob J. Peoples", "Mohammad Hamghalam", "Alex Robins", "Mane Piliposyan", "Richard K. G. Do", "Natalie Gangai", "Yun S. Chun", "Ahmad Bashir Barekzai", "Jayasree Chakraborty", "Hala Khasawneh", "Camila Vilela", "Natally Horvat", "João Miranda", "Alice C. Wei", "Amber L. Simpson"], "title": "A Novel Patch-Based TDA Approach for Computed Tomography", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.", "AI": {"tldr": "提出了一种基于补丁的持久同调方法，以提高CT图像分析中的分类性能和时间效率。", "motivation": "现有的基于立方体复杂度的方法在处理高分辨率CT图像时可能无法达到最佳性能并且计算复杂性较高。因此，需要一种新的方法来改进这一情况。", "method": "提出了一种新颖的补丁基持久同调（PH）构建方法，专门用于三维医学成像数据，特别是CT模态，并通过多个实验验证了该方法的有效性。", "result": "新方法在准确性、AUC、灵敏度、特异性和F1分数等方面均优于传统的3D立方体复杂度算法。具体而言，在所有数据集上分别平均提高了10.38%，6.94%，2.06%，11.58%和8.51%。", "conclusion": "通过实验证明了基于补丁的TDA方法在分类性能和时间效率方面的优势，并提供了一个方便的Python包Patch-TDA，以便使用该方法。"}}
{"id": "2512.12107", "pdf": "https://arxiv.org/pdf/2512.12107", "abs": "https://arxiv.org/abs/2512.12107", "authors": ["Yuheng Li", "Yue Zhang", "Abdoul Aziz Amadou", "Yuxiang Lai", "Jike Zhong", "Tiziano Passerini", "Dorin Comaniciu", "Puneet Sharma"], "title": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography", "categories": ["cs.CV"], "comment": null, "summary": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.", "AI": {"tldr": "该论文提出了EchoVLM模型，旨在通过引入测量为基础的多模态数据集和特定于超声心动图的预训练目标，改进超声心动图解释。", "motivation": "当前视觉-语言模型在自然图像和其他医学领域取得了一些成功，但在超声心动图领域的应用受到限制。由于缺乏大规模、临床基础的图像文本数据集以及测量导向推理的重要性，该论文旨在解决这些问题并提升超声心动图解释的质量和效率。", "method": "通过构建EchoGround-MIMIC多模态数据集，并引入视图信息对比损失函数和否定意识对比损失函数两种新颖预训练目标来改进模型。这些方法有助于提高模型对特定于超声心动图像的结构和关键发现的理解能力。", "result": "在五个临床应用类型中的36项任务上，EchoVLM表现出色，例如多模态疾病分类、图像-文本检索、视图分类等，其中零样本疾病的分类准确率为86.5%，视图分类精确度达到95.1%。", "conclusion": "通过该研究，作者证明了基于临床基础的多模态预训练可以产生可迁移的视觉表示，并将EchoVLM确立为超声心动图解释的基石模型。此外，他们计划公开数据集和代码以支持进一步的研究。"}}
{"id": "2512.12101", "pdf": "https://arxiv.org/pdf/2512.12101", "abs": "https://arxiv.org/abs/2512.12101", "authors": ["Swarn S. Warshaneyan", "Maksims Ivanovs", "Blaž Cugmas", "Inese Bērziņa", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadiķis"], "title": "AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 10 figures, 2 tables, 22 references. Journal submission undergoing peer review", "summary": "We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.", "AI": {"tldr": "论文主要任务是在光学显微镜和数字全息显微镜图像中实现全自动花粉识别。", "motivation": "由于全息图像是由斑点噪声、孪生像伪影以及与明场外观有较大差异导致的视觉识别困难，论文旨在提高通过AI技术进行花粉自动识别的能力。", "method": "论文首先训练YOLOv8s和MobileNetV3L在双模态数据集上用于目标检测和分类。之后采用Wasserstein GAN与谱归一化（WGAN-SN）生成合成全息图像，以改善DIHM中的物体检测效果。", "result": "光学显微镜图像中检测mAP50达到91.3%，分类准确率为97%；DIHM图像是8.15%和50%。通过扩大全息图像的边界框并结合生成的数据，最终改善至15.4%。", "conclusion": "研究表明GAN增强可以减少性能差异，使全自动DIHM流程更接近实际应用。"}}
{"id": "2512.12090", "pdf": "https://arxiv.org/pdf/2512.12090", "abs": "https://arxiv.org/abs/2512.12090", "authors": ["Samar Fares", "Nurbek Tastan", "Karthik Nandakumar"], "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.", "AI": {"tldr": "提出了一种新的基于视频生成模型参数选择性位移的水印技术SPDMark，旨在实现视频水印的同时具有不可感知、鲁棒性和计算效率。", "motivation": "现有视频水印方法难以同时满足不可感知性、鲁棒性和计算效率的要求。因此，需要一种新框架来解决这些问题。", "method": "通过选择性地位移视频扩散模型的部分参数进行水印嵌入，并使用低秩适应（LoRA）实现层间基底偏移，以提高参数效率。训练阶段结合信息恢复、感知相似性和时间一致性损失最小化目标函数，同时学习基底偏移和水印提取器。", "result": "在文本到视频和图像到视频生成模型上进行的评估显示，SPDMark能够生成几乎不可察觉且高精度可恢复的水印，并对其具有多种常见视频修改的强大鲁棒性。", "conclusion": "提出的方法实现了有效的、鲁棒性强的视频生成过程中的水印嵌入技术，解决了现有方法难以同时满足不可感知性和计算效率的问题。"}}
{"id": "2512.12089", "pdf": "https://arxiv.org/pdf/2512.12089", "abs": "https://arxiv.org/abs/2512.12089", "authors": ["Zihu Wang", "Boxun Xu", "Yuxuan Xia", "Peng Li"], "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.", "AI": {"tldr": "提出VEGAS方法，通过视觉编码器注意力引导的适应性导向来减轻大型视觉语言模型中的幻觉现象。", "motivation": "解决大型视觉语言模型在解码过程中因最后视觉注意力图未能聚焦于关键图像对象而导致的事实不一致问题。探索如何利用视觉编码器的注意力机制有效抑制这些幻觉。", "method": "分析了视觉-文本冲突，并发现它们主要出现在语言模型的中间层。通过将视觉编码器的注意力图注入到这些层次中，可以有效地减少幻觉现象。引入VEGAS方法，在推理过程中融合视觉编码器的注意力图并适应性地导向未能聚焦于关键图像对象的令牌。", "result": "实验显示，VEGAS在多个基准测试上均表现出色，并且显著降低了幻觉发生率。", "conclusion": "VEGAS是一种简单而有效的方法，在减轻大型视觉语言模型中的幻觉现象方面取得了最先进的性能。"}}
{"id": "2512.12088", "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.", "AI": {"tldr": "评估可靠策略迭代（RPI）在变化的神经网络和环境参数下的性能稳健性", "motivation": "提高深度强化学习方法的可靠性，减少样本效率低、训练不稳定和超参数敏感性等问题", "method": "通过改变神经网络结构和环境参数来测试RPI在两个经典控制任务上的表现，并与DQN等其他方法进行对比", "result": "相较于其他算法如DQN, Double DQN, DDPG, TD3 和PPO，RPI能更早达到近乎最优性能并保持这种策略", "conclusion": "实验结果表明RPI具有更高的可靠性和稳健性"}}
{"id": "2512.12083", "pdf": "https://arxiv.org/pdf/2512.12083", "abs": "https://arxiv.org/abs/2512.12083", "authors": ["Guanfang Dong", "Luke Schultz", "Negar Hassanpour", "Chao Gao"], "title": "RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.", "AI": {"tldr": "RePack 提出了一种简单有效的框架，用于通过将预训练的视觉基础模型（VFMs）表示投影到低维流形上以提高扩散变压器（DiT）的效果。", "motivation": "高维度的VFM表示可能会导致信息过载问题，尤其是在解码原始图像时。为了在减少信息量的同时保留VFM特征的有效性，提出了RePack框架。", "method": "通过将VFMs的高维表示投影到低维流形上以生成更紧凑且适合解码器的表示形式，从而过滤掉非语义噪音同时保留核心结构信息。", "result": "实验结果显示，RePack显著加速了DiT收敛速度，并在图像重建方面优于直接注入原始VFM特征的方法。具体来说，在DiT-XL/2上，仅需64个周期即可达到3.66的FID值，比最先进的方法快35%。", "conclusion": "RePack成功地从VFMs表示中提取核心语义信息并规避了高维度带来的负面影响，证明其在图像重建任务中的有效性。"}}
{"id": "2512.12081", "pdf": "https://arxiv.org/pdf/2512.12081", "abs": "https://arxiv.org/abs/2512.12081", "authors": ["Semih Kara", "Yasin Sonmez", "Can Kizilkale", "Alex Kurzhanskiy", "Nuno C. Martins", "Murat Arcak"], "title": "Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models", "categories": ["eess.SY", "cs.AI", "cs.SI", "math.OC"], "comment": null, "summary": "Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.", "AI": {"tldr": "研究如何通过合理放置电动汽车充电站来减少交通拥堵。", "motivation": "随着电动汽车的普及，如果不考虑其对交通的影响而随意安装充电站可能导致更严重的交通问题。", "method": "使用两种均衡模型（基于拥挤游戏和原子排队模拟）在可扩展贪婪站点放置算法中应用这两种模型进行研究。", "result": "实验结果表明，在现实网络中这种贪婪方案可以产生最优或近似最优的拥堵效果，尽管全局优化无法保证。队列仿真方法比拥挤游戏模型产生了更加实际的结果。", "conclusion": "提出了一种统一的方法来校准排队仿真中的交通延迟并在链路空间中解决均衡问题。"}}
{"id": "2512.12080", "pdf": "https://arxiv.org/pdf/2512.12080", "abs": "https://arxiv.org/abs/2512.12080", "authors": ["Ryan Po", "Eric Ryan Chan", "Changan Chen", "Gordon Wetzstein"], "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page here: https://ryanpo.com/bagger", "summary": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.", "AI": {"tldr": "本文提出了一种新的方法BAgger，用于减轻自回归视频扩散模型中的漂移问题。", "motivation": "自回归视频模型在预测下一帧方面有潜力，但存在曝光偏差，在训练和推理过程中误差会累积导致质量随时间下降。", "method": "通过构造从模型自身生成的轨迹来纠正错误，采用标准分数或流匹配目标进行训练，避免了大型教师网络及长时间回溯传播的问题。", "result": "该方法在文本到视频、视频扩展和多提示生成任务中表现出更加稳定的长期运动以及更好的视觉一致性，并减少了漂移问题。", "conclusion": "BAgger是一种有效的自我监督方案，可以有效减轻自回归视频扩散模型中的漂移问题，提高长时间预测的质量和稳定性。"}}
{"id": "2512.12073", "pdf": "https://arxiv.org/pdf/2512.12073", "abs": "https://arxiv.org/abs/2512.12073", "authors": ["G. Papakonstantinou"], "title": "The PPKN Gate: An Optimal 1-Toffoli Input-Preserving Full Adder for Quantum Arithmetic", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Efficient arithmetic operations are a prerequisite for practical quantum computing. Optimization efforts focus on two primary metrics: Quantum Cost (QC), determined by the number of non-linear gates, and Logical Depth, which defines the execution speed. Existing literature identifies the HNG gate as the standard for Input-Preserving Reversible Full Adders. HNG gate typically requires a QC of 12 and a logical depth of 5, in the area of classical reversible circuits. This paper proposes the PPKN Gate, a novel design that achieves the same inputpreserving functionality using only one Toffoli gate and five CNOT gates. With a Quantum Cost of 10 and a reduced logical depth of 4, the PPKN gate outperforms the standard HNG gate in both complexity and speed. Furthermore, we present a modular architecture for constructing an n-bit Ripple Carry Adder by cascading PPKN modules.", "AI": {"tldr": "该论文提出了一种新型的PPKN门，用于构建量子算术中的全加器。", "motivation": "高效的算术运算是实用量子计算的前提。优化工作集中在量子成本和逻辑深度两个主要指标上。现有的HNG门在输入保持性完全加法器中被广泛使用，但其性能有待提高。", "method": "提出了一种新型的PPKN门设计，该设计仅需一个Toffoli门和五个CNOT门即可实现相同的输入保持功能，并且比标准的HNG门具有更低的量子成本和逻辑深度。此外还介绍了一个模块化架构，用于通过级联PPKN模块构建n位串行进位加法器。", "result": "提出的PPKN门将量子成本降低至10，逻辑深度减少到4，优于标准的HNG门。同时提出了一种基于PPKN门的设计方法来实现高效的全加器。", "conclusion": "该论文通过引入新的PPKN门设计显著提高了输入保持性完全加法器的效率，并为构建高效量子算术提供了模块化解决方案。"}}
{"id": "2512.12069", "pdf": "https://arxiv.org/pdf/2512.12069", "abs": "https://arxiv.org/abs/2512.12069", "authors": ["Peichun Hua", "Hao Li", "Shanghao Shi", "Zhiyuan Yu", "Ning Zhang"], "title": "Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "40 pages, 13 figures", "summary": "Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.", "AI": {"tldr": "提出了一种新的大视觉语言模型（LVLM）的越狱检测框架——代表对比评分（RCS），用于提高对未知攻击类型的有效防御。", "motivation": "当前的方法在应对新型威胁时缺乏泛化能力，并且存在计算开销大的问题，因此需要一种既能有效防止新出现的攻击模式又能保持低计算成本的解决方案。", "method": "通过分析LVLM内部表示的空间几何结构，学习一个轻量级投影来最大限度地分离良性输入和恶意输入，在安全关键层中应用简单的对比评分以区分真正的恶意意图与单纯的新颖性。具体实现包括Mahalanobis Contrastive Detection（MCD）和K-nearest Contrastive Detection（KCD）。", "result": "在设计的评估协议上，RCS实现了对未见过攻击类型的良好泛化性能，展示了在适当内部表示中应用简单、可解释统计方法的有效性。", "conclusion": "该工作证明了有效的大视觉语言模型越狱检测可以通过简单的对比评分技术实现，并为安全部署提供了实用路径。"}}
{"id": "2512.12068", "pdf": "https://arxiv.org/pdf/2512.12068", "abs": "https://arxiv.org/abs/2512.12068", "authors": ["Yuewen Hou", "Dhanvi Bharadwaj", "Gokul Subramanian Ravi"], "title": "TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms", "categories": ["quant-ph", "cs.AR", "cs.DC", "cs.ET"], "comment": "To appear at 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2026)", "summary": "Variational Quantum Algorithms (VQAs) are promising for near- and intermediate-term quantum computing, but their execution cost is substantial. Each task requires many iterations and numerous circuits per iteration, and real-world applications often involve multiple tasks, scaling with the precision needed to explore the application's energy landscape. This demands an enormous number of execution shots, making practical use prohibitively expensive. We observe that VQA costs can be significantly reduced by exploiting execution similarities across an application's tasks. Based on this insight, we propose TreeVQA, a tree-based execution framework that begins by executing tasks jointly and progressively branches only as their quantum executions diverge. Implemented as a VQA wrapper, TreeVQA integrates with typical VQA applications. Evaluations on scientific and combinatorial benchmarks show shot count reductions of $25.9\\times$ on average and over $100\\times$ for large-scale problems at the same target accuracy. The benefits grow further with increasing problem size and precision requirements.", "AI": {"tldr": "提出了一种基于树结构的执行框架TreeVQA，用于减少变量子算法中的测量次数。", "motivation": "为了降低变量子算法（VQAs）的实际应用成本，该论文试图通过利用任务间在执行时的相似性来显著减少其所需的大量测量次数。", "method": "论文提出了一种树状执行框架TreeVQA，开始时将任务合并执行，并逐步根据它们的量子执行差异进行分支。这种策略可以有效降低计算成本而不会牺牲精度。", "result": "在科学和组合问题上的评估显示，TreeVQA能够平均减少25.9倍的测量次数，对于大规模问题甚至超过100倍，在保持相同目标准确性的情况下。", "conclusion": "随着问题规模和精确度要求的增长，TreeVQA带来的收益也进一步增加。"}}
{"id": "2512.12066", "pdf": "https://arxiv.org/pdf/2512.12066", "abs": "https://arxiv.org/abs/2512.12066", "authors": ["Erik Larsen"], "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "14 pages, 7 figures, 6 tables. Code and data available at https://github.com/erikl2/safety-refusal-stability", "summary": "Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.", "AI": {"tldr": "本文研究了大型语言模型在不同随机种子和温度设置下的拒绝行为的一致性，揭示了单次安全评估不足以可靠地评估模型的安全性。", "motivation": "当前对大语言模型的安全评估依赖于单一的测试方法，假设模型响应是确定性和代表性的。然而这种假设可能不准确，需要通过更全面的方法来验证模型在不同条件下的行为一致性。", "method": "测试了四种来自三个家族的语言模型（Llama 3.1 8B、Qwen 2.5 7B、Qwen 3 8B和Gemma 3 12B），使用了876个有害提示，通过改变温度和随机种子的组合，进行多样本评估。引入安全性稳定性指数（SSI）来量化决策的一致性。", "result": "发现大约18%-28%的提示在不同条件下会导致模型拒绝行为的变化，更高温度显著降低了决定稳定性的概率。单次安全评估仅与多样本真实结果一致92.4%，推荐至少使用三个样本进行可靠的安全评估。", "conclusion": "本文证明了单一测试方法不足以确保大型语言模型的安全性评估的可靠性，并提出了更全面的方法来提高评估准确性。"}}
{"id": "2512.12063", "pdf": "https://arxiv.org/pdf/2512.12063", "abs": "https://arxiv.org/abs/2512.12063", "authors": ["Gökberk Çelikmasat", "Atay Özgövde", "Fatma Başak Aydemir"], "title": "Instruction-Tuning Open-Weight Language Models for BPMN Model Generation", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint. Under preparation for journal submission", "summary": "Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.", "AI": {"tldr": "本文探讨了通过指令微调开放式权重大型语言模型，以生成高质量的BPMN流程图。", "motivation": "由于建模耗时且需要专业知识，许多实践者会跳过建模过程。为了解决这一问题，作者研究了一种基于自然语言描述生成BPMN模型的方法，使用开放源代码大语言模型进行指令微调，以实现成本效益高和隐私保护。", "method": "通过准备配对的文本-图表数据并用参数高效的微调和量化方法来调整开源的大语言模型。评估了优化后的模型，并与未微调的基线模型进行了比较。", "result": "该模型在序列和结构指标上优于所有基准，在遵循BPMN最佳实践方面表现良好，且减少了对复杂提示框架的依赖。", "conclusion": "指令微调提高了结构准确性并增强了鲁棒性，同时减少对重型提示框架的需求。作者公开分享了训练后的模型和脚本以支持可重复研究"}}
{"id": "2512.12060", "pdf": "https://arxiv.org/pdf/2512.12060", "abs": "https://arxiv.org/abs/2512.12060", "authors": ["Tejas Panambur", "Ishan Rajendrakumar Dave", "Chongjian Ge", "Ersin Yumer", "Xue Bai"], "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "The first two authors contributed equally", "summary": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity. We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures. To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.", "AI": {"tldr": "CreativeVR是一种用于修复AI生成和现实视频中严重结构和时序缺陷的框架，通过扩散先验引导的方法来改进。", "motivation": "当前文本到视频（T2V）模型在处理细节方面表现不佳，导致合成视频出现面部扭曲、动作不一致等问题。传统视频修复方法对这些特定问题效果有限，因此需要一种新的解决方案以更精确地恢复结构和运动。", "method": "CreativeVR采用深度适配器技术，并引入了时间上连贯的退化模块，在训练期间应用精心设计的转换来模拟实际中的结构性失败，同时提供一个精度控制旋钮来调整模型对输入的依赖程度。", "result": "CreativeVR在处理严重缺陷视频时达到了最先进的性能水平，并且在标准视频修复基准测试中也表现出色。此外，它还能以每秒13帧的速度运行。", "conclusion": "通过扩散先验引导的方法可以有效地修复AI生成和真实世界中的视频内容的结构化和运动问题，为未来的研究开辟了新的可能性。"}}
{"id": "2512.12059", "pdf": "https://arxiv.org/pdf/2512.12059", "abs": "https://arxiv.org/abs/2512.12059", "authors": ["Luke Bhan", "Hanyu Zhang", "Andrew Gordon Wilson", "Michael W. Mahoney", "Chuck Arvin"], "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification", "categories": ["cs.AI"], "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop", "summary": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.", "AI": {"tldr": "论文提出了一个利用大型语言模型进行自动预测监控的系统，旨在识别出不合理的时间序列预测。", "motivation": "大规模零售业务中，监测预测系统的准确性对于客户满意度、盈利能力和运营效率至关重要。为了提高这些方面的能力，作者提出了一种基于大型语言模型的预测监控方法。", "method": "论文通过三个实验验证了大型语言模型在评估时间序列预测质量方面的性能，包括合成数据和真实世界的数据集上的表现。此外还测试了不同类型的语言模型（从较小到较大的规模）的表现，并展示了它们如何利用非结构化上下文信息来提高其对预测的评价准确性。", "result": "实验结果显示，大型语言模型可以有效地识别出不合理的时间序列预测，特别是在处理时间错位、趋势不一致和尖峰错误等方面。最佳表现的语言模型在F1分数上达到了0.88，接近人类级别的性能（F1分数为0.97）。此外还表明多模态的大型语言模型能够有效利用非结构化的上下文信息来改进预测评估。", "conclusion": "论文的研究结果证明了即使未经特定领域的微调，大型语言模型也可以作为一种可行且可扩展的方法来进行自动时间序列预测监控和评价。"}}
{"id": "2512.12058", "pdf": "https://arxiv.org/pdf/2512.12058", "abs": "https://arxiv.org/abs/2512.12058", "authors": ["Anja Sheppard", "Chris Reale", "Katherine A. Skinner"], "title": "A Stochastic Approach to Terrain Maps for Safe Lunar Landing", "categories": ["cs.RO"], "comment": "Accepted to IEEE Aerospace 2026", "summary": "Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment. There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas. To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.", "AI": {"tldr": "该论文提出了一种基于高斯过程生成月球地形随机地图的方法，以提高在阴影区域安全着陆的能力。", "motivation": "传统视觉方法难以应对南极地区较暗环境下的危险检测任务。利用LRO数据创建先验地形图可以改善着陆安全性，并减少对未经测试的激光雷达技术的依赖。", "method": "论文提出了一种两阶段高斯过程模型，其中第一阶段通过结合DEM信心地图中的异方差信息来生成地形模型，第二阶段则基于该模型进行随机变分GP训练以提高计算效率和精度。", "result": "该方法能够更准确地描绘出月球表面的不确定性，从而为下游任务如危险检测提供更加有用的信息。", "conclusion": "通过使用高斯过程模型结合LRO数据，论文成功提高了在复杂地形环境中的着陆安全性，并对未来自主航天飞行提供了重要的技术支撑。"}}
{"id": "2512.12056", "pdf": "https://arxiv.org/pdf/2512.12056", "abs": "https://arxiv.org/abs/2512.12056", "authors": ["Maria Rodriguez", "Minh-Tan Pham", "Martin Sudmanns", "Quentin Poterek", "Oscar Narvaez"], "title": "Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management", "categories": ["cs.CV"], "comment": "5 pages, IGARSS 2025", "summary": "After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.", "AI": {"tldr": "该论文介绍了在紧急管理场景下使用SPOT-6/7遥感影像进行火灾烧毁区域（BA）自动识别的方法。", "motivation": "当前的BA映射方法依赖于计算机视觉模型，但往往忽略了其在时间紧迫的应急管理工作中的应用。本文旨在提高BA绘制的性能和效率。", "method": "实验基于U-Net和SegFormer模型进行，并引入了土地覆盖数据作为辅助任务以增强模型鲁棒性，同时利用测试时增强技术来提升BA识别精度。", "result": "实验结果表明，在有限的数据集上，U-Net与SegFormer性能相近。然而，SegFormer消耗更多资源，限制其紧急情况下的应用。此外，引入土地覆盖数据增强了模型的稳定性而不会增加推理时间。测试时增广技术提升了BA识别准确性但增加了推断时间。", "conclusion": "该工作提出了一种基于SPOT-6/7影像的高效、鲁棒性的烧毁区域自动绘制方法，并为应对紧急情况提供了优化策略，如混合精度计算以降低推理时间。"}}
{"id": "2512.12053", "pdf": "https://arxiv.org/pdf/2512.12053", "abs": "https://arxiv.org/abs/2512.12053", "authors": ["Tran-Vu La", "Minh-Tan Pham", "Yu Li", "Patrick Matgen", "Marco Chini"], "title": "Adaptive federated learning for ship detection across diverse satellite imagery sources", "categories": ["cs.CV"], "comment": "5 pages, IGARSS 2025", "summary": "We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.", "AI": {"tldr": "研究应用联邦学习进行跨多源卫星图像的船只检测，提供隐私保护解决方案。", "motivation": "探索在不共享数据或集中化收集的情况下使用联邦学习处理商业卫星图像和敏感船只标注的优势。", "method": "评估并比较FedAvg、FedProx、FedOpt和FedMedian四种联邦学习模型，并将其与本地训练基线进行对比，后者独立训练每个数据集而不分享参数。研究了最佳的通信轮次和局部训练周期以优化检测精度同时保持计算效率。", "result": "结果显示，联邦学习模型在较小的数据集上显著提高了船只检测准确性，并接近使用所有数据集全局训练的表现。", "conclusion": "强调选择适当的联邦学习配置对于提高检测精度的同时维持计算效率的重要性。"}}
{"id": "2512.12048", "pdf": "https://arxiv.org/pdf/2512.12048", "abs": "https://arxiv.org/abs/2512.12048", "authors": ["Muddsair Sharif", "Huseyin Seker"], "title": "Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a novel context-sensitive multi\\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\\%), grid operators (20\\%), charging station operators (20\\%), fleet operators (20%), and environmental factors (15\\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\\-DRA framework achieves 92\\% coordination success rate, 15\\% energy efficiency improvement, 10\\% cost reduction, 20% grid strain decrease, and \\2.3x faster convergence while maintaining 88\\% training stability and 85\\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\\$122,962 and 69\\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.", "AI": {"tldr": "本文提出了一种基于Smart2Charge应用的用于优化电动汽车充电生态系统的新型上下文感知多代理协调框架。", "motivation": "该论文旨在通过智能决策机制提高电动汽车充电网络在动态环境中的资源分配效率和能源利用效率，同时平衡不同参与者的利益和需求。", "method": "本文采用了一种结合Deep Q-Networks、图神经网络以及注意力机制的多代理系统进行上下文感知的决策制定，处理包括天气模式、交通状况等20个上下文特征，并通过加权协调机制与共识协议来平衡不同生态参与者的利益。", "result": "所提出的CAMAC-DRA框架在实际数据集上实现了92%的协调成功率和15%的能源效率提升，并且相比基线算法，其收敛速度提高了2.3倍，训练稳定性保持在88%，样本效率达到85%。此外，在商业可行性验证中显示了-122,962美元的净现值成本。", "conclusion": "该框架通过上下文感知和多利益相关者的协调成功地平衡了相互竞争的目标，并适应实时变量，成为智能电动汽车充电协调和可持续交通电气化的重要解决方案。"}}
{"id": "2512.12046", "pdf": "https://arxiv.org/pdf/2512.12046", "abs": "https://arxiv.org/abs/2512.12046", "authors": ["Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning", "categories": ["cs.LG", "cs.RO", "eess.SY", "stat.ML"], "comment": null, "summary": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.", "AI": {"tldr": "本文提出了一种基于Eikonal偏微分方程的连续时间约束拟测度强化学习方法，以解决目标导向型任务中的奖励设计难题。", "motivation": "为了克服传统强化学习中手工艺奖赏信号的设计困难，该研究引入了基于目标导向的价值函数作为拟测度，以此来改进奖励机制的设计。", "method": "提出了Eikonal约束下的连续时间拟测度RL（Eik-QRL），它使用偏微分方程形式化并要求仅采样状态和目标。为解决复杂动态情况的限制性问题，进一步引入了层次化的Eik-HiQRL方法。", "result": "实验表明，在离线目标导向导航任务中，所提Eik-HiQRL方法达到了业界领先水平；在操作任务方面，其性能也超过了传统拟测度RL，并且与时间差分方法相当。", "conclusion": "通过偏微分方程结构改进了连续时间约束下的拟测度强化学习方法，提高了目标导向型任务中的泛化能力。同时利用层次化的策略进一步优化了算法的适用性。"}}
{"id": "2512.12045", "pdf": "https://arxiv.org/pdf/2512.12045", "abs": "https://arxiv.org/abs/2512.12045", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Zewei", "Tian", "Min Sun", "Zachary Zhang", "Thomas Han", "Yulia Lapicus", "Kevin He"], "title": "AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority. Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities. During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.", "AI": {"tldr": "报告描述了Colleague AI Classroom试点项目，该项目将生成式人工智能技术引入真实课堂中，探讨AI作为教学伙伴的角色。", "motivation": "目的是探索如何在保持人类判断和教师权威的同时，利用AI扩展教师的教学能力，并提高课堂教学效率与质量。", "method": "研究人员通过七周的时间，在华盛顿州四所公立学区和一所独立学校进行试点研究。21名在职教师将四种AI功能（教学助手、评估及AI评分、AI辅导和学生成长见解）融入课堂，超过600名学生参与其中。教师设计并实施了这些活动，并每周反馈一次对AI角色的看法。", "result": "通过调查问卷、计划与反思表格、小组会议、一对一访谈以及平台使用日志收集数据，揭示了AI在教学中增加价值的地方和需要改进的领域。", "conclusion": "研究初步表明，在教师设计和指导下，AI可以成为有效的教学伙伴，并能显著提高学生的学习效果。"}}
{"id": "2512.12021", "pdf": "https://arxiv.org/pdf/2512.12021", "abs": "https://arxiv.org/abs/2512.12021", "authors": ["Xincheng Cao", "Haochong Chen", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.", "AI": {"tldr": "提出了一种改进的Hybrid A*算法，用于解决车辆在狭小空间内的自动倒车路径规划问题。", "motivation": "解决停车时由于可行路径稀缺而导致的操作困难和碰撞风险问题。", "method": "结合了标准Hybrid A*算法的可行性保证与静态障碍物碰撞避免功能。引入单轨模型描述车辆低速运动，作为运动模型生成可执行的运动分支，并利用重建的车辆中心线实现碰撞检测。", "result": "通过仿真测试证明该算法能提供既可行又无碰撞的轨迹规划。", "conclusion": "改进后的Hybrid A*路径规划方法在解决狭小空间内的自动倒车问题上表现出色，能够有效避免碰撞并生成可行性路径。"}}
{"id": "2512.12013", "pdf": "https://arxiv.org/pdf/2512.12013", "abs": "https://arxiv.org/abs/2512.12013", "authors": ["Senhao Gao", "Junqing Zhang", "Luoyu Mei", "Shuai Wang", "Xuyu Wang"], "title": "Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \\sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.", "AI": {"tldr": "通过设计星图表示和使用DDGNN在毫米波雷达点云数据上进行人体活动识别，实现高精度的分类。", "motivation": "毫米波雷达信号的特点导致其在人体运动追踪中存在稀疏性和不固定尺寸的问题，现有方法并不适用于此类系统。因此需要开发一种新的表示法来处理这些问题，并提高毫米波雷达的人体活动识别精度。", "method": "设计了星图以描述毫米波雷达点云数据的相对关系，同时采用动态图神经网络（DDGNN）学习星图中的特征。这种方法可以有效地捕捉到人体运动相关的空间时间特征。", "result": "实验结果表明该方法在真实世界的数据集上实现了94.27%的整体分类准确率，并且展示了其在资源受限平台上的效果。通过与三个最近的雷达特定方法进行比较，我们的系统表现更优。", "conclusion": "提出的星图表示和DDGNN方法能够有效地解决毫米波雷达点云数据中的人体活动识别问题，实现接近最优的表现。"}}
{"id": "2512.12012", "pdf": "https://arxiv.org/pdf/2512.12012", "abs": "https://arxiv.org/abs/2512.12012", "authors": ["Antonio Guillen-Perez"], "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "AI": {"tldr": "Semantic-Drive是一个用于自动车辆训练数据稀有事件识别的框架，通过符号接地和认知分析提高准确性。", "motivation": "在自动驾驶汽车开发中，由于缺少稀有场景的数据导致安全关键事件难以准确标识。现有方法要么精度低，要么成本高昂且隐私侵犯。", "method": "Semantic-Drive采用两阶段感知：第一阶段使用实时开放式词汇检测器锚定注意点，第二阶段通过推理VLM进行情景分析，并利用多模型‘裁判-侦察’策略降低幻觉风险。", "result": "在nuScenes数据集上，与CLIP相比，召回率从0.475提升至0.966；相较于单模型，风险评估误差减少40%。整个系统运行于消费级硬件（NVIDIA RTX 3090）。", "conclusion": "Semantic-Drive提供了一种隐私保护、成本效益高的方法来识别自动驾驶汽车训练数据中的稀有事件。"}}
{"id": "2512.12008", "pdf": "https://arxiv.org/pdf/2512.12008", "abs": "https://arxiv.org/abs/2512.12008", "authors": ["Minghui Liu", "Aadi Palnitkar", "Tahseen Rabbani", "Hyunwoo Jae", "Kyle Rui Sang", "Dixi Yao", "Shayan Shabihi", "Fuheng Zhao", "Tian Li", "Ce Zhang", "Furong Huang", "Kunpeng Zhang"], "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.", "AI": {"tldr": "评估KV缓存压缩算法在长推理任务中的表现。", "motivation": "大型语言模型在处理长上下文任务时受限于内存约束，尤其是KV缓存的线性增长问题。现有的压缩策略主要针对预填充阶段而未充分考虑推理任务的需求。", "method": "通过基准测试评估多种流行压缩算法在LLama-3.1-8B-Instruct等模型上的性能表现，特别是对于GSM8K和MATH500这类需要多步骤推理的长序列任务。", "result": "发现H2O和SnapKV解码变体是针对推理模型的最佳策略。低预算下的驱逐策略可以生成更长的推理轨迹，揭示了缓存大小与推断成本之间的权衡。", "conclusion": "不同压缩算法在不同类型的数据集上表现不一。对于推理任务，H2O和SnapKV解码变体是最佳选择，并且轻量级追踪有助于优化复杂推理过程中的性能。"}}
{"id": "2512.12006", "pdf": "https://arxiv.org/pdf/2512.12006", "abs": "https://arxiv.org/abs/2512.12006", "authors": ["Robin Vassantlal", "Hasan Heydari", "Bernardo Ferreira", "Alysson Bessani"], "title": "MVP-ORAM: a Wait-free Concurrent ORAM for Confidential BFT Storage", "categories": ["cs.CR", "cs.DC", "cs.DS"], "comment": "This is the extended version of the paper appearing at the 33rd Network and Distributed System Security Symposium (NDSS 2026)", "summary": "It is well known that encryption alone is not enough to protect data privacy. Access patterns, revealed when operations are performed, can also be leveraged in inference attacks. Oblivious RAM (ORAM) hides access patterns by making client requests oblivious. However, existing protocols are still limited in supporting concurrent clients and Byzantine fault tolerance (BFT). We present MVP-ORAM, the first wait-free ORAM protocol that supports concurrent fail-prone clients. In contrast to previous works, MVP-ORAM avoids using trusted proxies, which require additional security assumptions, and concurrency control mechanisms based on inter-client communication or distributed locks, which limit overall throughput and the capability of tolerating faulty clients. Instead, MVP-ORAM enables clients to perform concurrent requests and merge conflicting updates as they happen, satisfying wait-freedom, i.e., clients make progress independently of the performance or failures of other clients. Since wait and collision freedom are fundamentally contradictory goals that cannot be achieved simultaneously in an asynchronous concurrent ORAM service, we define a weaker notion of obliviousness that depends on the application workload and number of concurrent clients, and prove MVP-ORAM is secure in practical scenarios where clients perform skewed block accesses. By being wait-free, MVP-ORAM can be seamlessly integrated into existing confidential BFT data stores, creating the first BFT ORAM construction. We implement MVP-ORAM on top of a confidential BFT data store and show our prototype can process hundreds of 4KB accesses per second in modern clouds.", "AI": {"tldr": "提出了一种名为MVP-ORAM的等待自由并发ORAM协议，以支持同时处理多个不稳定的客户端，并将其集成到现有的保密BFT数据存储中。", "motivation": "传统的加密方法不足以保护数据隐私，因为操作时暴露的访问模式可用于推理攻击。现有方案在多客户端和拜占庭容错方面存在局限性，因此提出MVP-ORAM来解决这些问题。", "method": "设计了一种无等待且无冲突机制的并发ORAM协议MVP-ORAM，并证明其安全性依赖于应用场景的工作负载及并行客户数量。它能将客户端请求合并处理从而实现独立进展。", "result": "实现了MVP-ORAM并在现代云环境中展示，表明原型可以在每秒处理数百个4KB访问。", "conclusion": "MVP-ORAM作为首个等待自由的并发ORAM协议能够被无缝集成到现有的保密BFT数据存储中，并首次构建了保密BFT ORAM架构。"}}
{"id": "2512.11999", "pdf": "https://arxiv.org/pdf/2512.11999", "abs": "https://arxiv.org/abs/2512.11999", "authors": ["Wei Xiao", "Anni Li"], "title": "Taylor-Lagrange Control for Safety-Critical Systems", "categories": ["eess.SY", "cs.RO"], "comment": "13 pages", "summary": "This paper proposes a novel Taylor-Lagrange Control (TLC) method for nonlinear control systems to ensure the safety and stability through Taylor's theorem with Lagrange remainder. To achieve this, we expand a safety or stability function with respect to time along the system dynamics using the Lie derivative and Taylor's theorem. This expansion enables the control input to appear in the Taylor series at an order equivalent to the relative degree of the function. We show that the proposed TLC provides necessary and sufficient conditions for system safety and is applicable to systems and constraints of arbitrary relative degree. The TLC exhibits connections with existing Control Barrier Function (CBF) and Control Lyapunov Function (CLF) methods, and it further extends the CBF and CLF methods to the complex domain, especially for higher order cases. Compared to High-Order CBFs (HOCBFs), TLC is less restrictive as it does not require forward invariance of the intersection of a set of safe sets while HOCBFs do. We employ TLC to reformulate a constrained optimal control problem as a sequence of quadratic programs with a zero-order hold implementation method, and demonstrate the safety of zero-order hold TLC using an event-triggered control method to address inter-sampling effects. Finally, we illustrate the effectiveness of the proposed TLC method through an adaptive cruise control system and a robot control problem, and compare it with existing CBF methods.", "AI": {"tldr": "提出了一种新的泰勒-拉格朗日控制方法，以确保非线性系统的安全性和稳定性。", "motivation": "通过泰勒定理和拉格朗日余项来保证系统在非线性条件下的稳定性和安全性。", "method": "利用李导数和泰勒展开法，在时间上沿着系统动力学扩展了一个安全或稳定性函数，使控制输入出现在与相对阶数等同的泰勒级数中。该方法适用于任意相对阶数的系统和约束，并且能够联系现有的CBF和CLF方法。", "result": "通过模拟自适应巡航控制系统和机器人控制系统，验证了所提TLC方法的有效性，并与现有CBF方法进行了比较。", "conclusion": "提出了一种新的泰勒-拉格朗日控制(TLC)方法来确保非线性系统的安全性和稳定性。这种方法展示了比传统高阶CBF(HOCBF)更少的限制，能够在复杂的情况下提供更好的安全性保证。"}}
{"id": "2512.11997", "pdf": "https://arxiv.org/pdf/2512.11997", "abs": "https://arxiv.org/abs/2512.11997", "authors": ["Anfeng Peng", "Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion", "categories": ["cs.AI"], "comment": null, "summary": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.", "AI": {"tldr": "本文提出了一种名为EnrichLog的日志异常检测框架，该框架通过融合特定语料库和样本的知识来增强原始日志条目的语义信息。", "motivation": "传统日志分析方法在处理日志模式模糊或丢失重要语义信息时存在局限性。作者希望通过引入知识强化的融合技术来提高异常检测的准确性和可解释性。", "method": "EnrichLog框架利用检索增强生成的方法，将相关上下文知识集成到模型中而不需重新训练，并通过历史示例和从语料库推导出的理由来丰富原始日志条目。", "result": "在四个大规模系统日志基准数据集上的评估显示，相较于五种基线方法，EnrichLog具有显著的异常检测性能提升。此外，同时融合特定于语料库和样本的知识可以提高模型信心及检测精度。", "conclusion": "EnrichLog框架通过引入相关上下文知识提高了日志异常检测的准确性与可解释性，并保持了高效的推理速度，适用于实际部署环境中的使用。"}}
{"id": "2512.11995", "pdf": "https://arxiv.org/pdf/2512.11995", "abs": "https://arxiv.org/abs/2512.11995", "authors": ["Chenrui Fan", "Yijun Liang", "Shweta Bhardwaj", "Kwesi Cobbina", "Ming Li", "Tianyi Zhou"], "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "28 pages", "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.", "AI": {"tldr": "V-REX评估套件用于测试视觉语言模型（VLM）的多步探索性推理能力，包括规划和跟随两个方面。", "motivation": "许多视觉语言模型在处理复杂开放任务时表现不佳，这些问题通常需要多轮次的探索与推断。为了解决这一问题，作者开发了V-REX评估套件来测试这些模型的能力。", "method": "通过将多步推理转换成一系列链式问题（CoQ），并设计了一个评估协议以评测规划和跟随两个方面的能力。", "result": "评估结果揭示了在多步骤探索性推理中，顶级的视觉语言模型存在显著差异，并且显示出改进的空间。", "conclusion": "V-REX为测试复杂任务中的视觉语言模型提供了可靠的方法，证明了现有模型在这类任务上的不足和提升潜力。"}}
{"id": "2512.11994", "pdf": "https://arxiv.org/pdf/2512.11994", "abs": "https://arxiv.org/abs/2512.11994", "authors": ["Arijit Bishnu", "Debarshi Chanda", "Buddha Dev Das", "Arijit Ghosh", "Gopinath Mishra"], "title": "Optimal non-adaptive algorithm for edge estimation", "categories": ["cs.DS", "math.CO"], "comment": "15 pages", "summary": "We present a simple nonadaptive randomized algorithm that estimates the number of edges in a simple, unweighted, undirected graph, possibly containing isolated vertices, using only degree and random edge queries. For an $n$-vertex graph, our method requires only $\\widetilde{O}(\\sqrt{n})$ queries, achieving sublinear query complexity. The algorithm independently samples a set of vertices and queries their degrees, and also independently samples a set of edges, using the answers to these queries to estimate the total number of edges in the graph. We further prove a matching lower bound, establishing the optimality of our algorithm and resolving the non-adaptive query complexity of this problem with respect to degree and random-edge queries.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.11988", "pdf": "https://arxiv.org/pdf/2512.11988", "abs": "https://arxiv.org/abs/2512.11988", "authors": ["Xianghui Xie", "Bowen Wen", "Yan Chang", "Hesam Rabeti", "Jiefeng Li", "Ye Yuan", "Gerard Pons-Moll", "Stan Birchfield"], "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction", "categories": ["cs.CV"], "comment": "14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/", "summary": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.", "AI": {"tldr": "提出了一种从单目RGB视频中进行4D人体与物体交互重建的模型CARI4D。", "motivation": "现有的方法在捕捉人体与未知对象之间的互动时面临困难，包括深度模糊、遮挡和复杂运动等问题。", "method": "该方法通过假设选择算法整合基础模型预测，并使用学习到的渲染和对比机制共同优化这些预测。它进一步通过推理复杂的接触来确保物理约束的满足。", "result": "实验表明，在分布内数据集上，CARI4D比现有最佳方法性能高出38％；在未见数据集中提高了36％。", "conclusion": "该模型超越了训练类别并能应用于野外互联网视频。代码和预训练模型将公开发布。"}}
{"id": "2512.11984", "pdf": "https://arxiv.org/pdf/2512.11984", "abs": "https://arxiv.org/abs/2512.11984", "authors": ["Alireza Joonbakhsh", "Alireza Rostami", "AmirMohammad Kamalinia", "Ali Nazeri", "Farshad Khunjush", "Bedir Tekinerdogan", "Siamak Farshidi"], "title": "Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality. This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems. The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency. By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.", "AI": {"tldr": "提出一个基于证据的决策支持框架以优化AI模型选择过程。", "motivation": "解决因缺乏结构化方法而导致的研究软件工程中AI模型选择不一致、不可重复和透明度低的问题。", "method": "采用设计科学研究方法，将AI模型选择问题定义为多准则决策问题，并构建一个包含自动数据收集管道、结构化知识图谱的框架ModelSelect。", "result": "通过50个真实案例验证了该框架能够生成可靠、可解释和重复性强的推荐结果，与专家判断高度一致。在模型和库推荐任务中表现出高覆盖率和强一致性。", "conclusion": "此工作建立了AI模型选择过程中透明且可重现决策支持的基础，并提供了将实证证据整合到AI模型推荐流程中的可扩展路径，提高了研究软件工程的质量和鲁棒性。"}}
{"id": "2512.11982", "pdf": "https://arxiv.org/pdf/2512.11982", "abs": "https://arxiv.org/abs/2512.11982", "authors": ["Nolan Koblischke", "Liam Parker", "Francois Lanusse", "Irina Espejo Morales", "Jo Bovy", "Shirley Ho"], "title": "Semantic search for 100M+ galaxy images using AI-generated captions", "categories": ["astro-ph.IM", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at the NeurIPS 2025 AI4Science Workshop", "summary": "Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search", "AI": {"tldr": "开发了一种基于人工智能生成描述的语义搜索引擎，用于搜索数亿张星系图像。", "motivation": "手动标注方法限制了对大规模天文学图像数据集的科学发现能力。本文提出一种无需人工标注的方法来增强探索能力。", "method": "利用视觉语言模型（VLM）为星系图像生成描述，并通过对比方式将预训练的多模态天文基础模型与这些嵌入式描述进行对齐，从而生成可搜索的嵌入。", "result": "所提出的AION-Search模型在零样本条件下实现了寻找罕见现象的状态-of-the-art性能。其重新排序方法几乎使最具挑战性目标的召回率翻倍。", "conclusion": "首次实现大规模星系图像集（1.4亿张）中的灵活语义搜索，拓展了科学数据探索能力，并提供了其他领域应用的可能性。"}}
{"id": "2512.11979", "pdf": "https://arxiv.org/pdf/2512.11979", "abs": "https://arxiv.org/abs/2512.11979", "authors": ["Marc Scibelli", "Krystelle Gonzalez Papaux", "Julia Valenti", "Srishti Kush"], "title": "Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.", "AI": {"tldr": "设计了一个名为HAX的框架，用于实现可信赖、透明和协作的人机代理交互。", "motivation": "生成式和自主代理系统的兴起要求重新思考人类如何与这些系统合作。", "method": "提出一个三阶段方法，包括行为启发法、结构化安全输出的SDK以及减少认知负荷的行为代理概念。", "result": "提供了一个基于时间、交互和性能理论的综合框架，以促进多代理系统的协作。", "conclusion": "HAX是首个能够连接信任理论、界面设计和基础设施来支持新兴的代理互联网的端到端框架。"}}
{"id": "2512.11977", "pdf": "https://arxiv.org/pdf/2512.11977", "abs": "https://arxiv.org/abs/2512.11977", "authors": ["Sushmita Nath"], "title": "A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer", "categories": ["cs.CV"], "comment": "submit/7075585. 5 pages with 5 figures", "summary": "Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.", "AI": {"tldr": "研究比较了图像变换器在半导体晶圆缺陷检测中的应用，特别是在数据受限条件下的性能。", "motivation": "卷积神经网络（CNN）如VGG-19、Xception和SqueezeNet在图像分类中表现出色，但在数据有限且不平衡的情况下效果会下降。研究旨在探索DeiT模型在这种场景中的表现。", "method": "采用Data-Efficient Image Transformer (DeiT) 模型进行晶圆缺陷图的分类，并与CNN模型如VGG-19、SqueezeNet和Xception等对比实验。", "result": "在数据受限条件下，DeiT实现了最高的分类准确率（90.83%），并且F1分数高达90.78%，优于其他CNN模型；同时训练收敛速度更快，在检测少数缺陷类别上表现出更高的鲁棒性。", "conclusion": "DeiT等基于变换器的模型在半导体晶圆缺陷检测中展现出了巨大潜力，支持预测维护策略。"}}
{"id": "2512.11957", "pdf": "https://arxiv.org/pdf/2512.11957", "abs": "https://arxiv.org/abs/2512.11957", "authors": ["Nabeel Rehemtulla", "Adam A. Miller", "Mike Walmsley", "Ved G. Shah", "Theophile Jegou du Laz", "Michael W. Coughlin", "Argyro Sasli", "Joshua Bloom", "Christoffer Fremling", "Matthew J. Graham", "Steven L. Groom", "David Hale", "Ashish A. Mahabal", "Daniel A. Perley", "Josiah Purdum", "Ben Rusholme", "Jesper Sollerman", "Mansi M. Kasliwal"], "title": "Pre-training vision models for the classification of alerts from wide-field time-domain surveys", "categories": ["astro-ph.IM", "cs.CV"], "comment": "To be submitted to PASP", "summary": "Modern wide-field time-domain surveys facilitate the study of transient, variable and moving phenomena by conducting image differencing and relaying alerts to their communities. Machine learning tools have been used on data from these surveys and their precursors for more than a decade, and convolutional neural networks (CNNs), which make predictions directly from input images, saw particularly broad adoption through the 2010s. Since then, continually rapid advances in computer vision have transformed the standard practices around using such models. It is now commonplace to use standardized architectures pre-trained on large corpora of everyday images (e.g., ImageNet). In contrast, time-domain astronomy studies still typically design custom CNN architectures and train them from scratch. Here, we explore the affects of adopting various pre-training regimens and standardized model architectures on the performance of alert classification. We find that the resulting models match or outperform a custom, specialized CNN like what is typically used for filtering alerts. Moreover, our results show that pre-training on galaxy images from Galaxy Zoo tends to yield better performance than pre-training on ImageNet or training from scratch. We observe that the design of standardized architectures are much better optimized than the custom CNN baseline, requiring significantly less time and memory for inference despite having more trainable parameters. On the eve of the Legacy Survey of Space and Time and other image-differencing surveys, these findings advocate for a paradigm shift in the creation of vision models for alerts, demonstrating that greater performance and efficiency, in time and in data, can be achieved by adopting the latest practices from the computer vision field.", "AI": {"tldr": "本文研究了使用预训练的视觉模型进行宽域时域调查警报分类的效果，发现这些模型可以匹配甚至超越定制设计的CNN。", "motivation": "当前天文学领域的宽域时域调查通常仍依赖于从头开始训练的定制CNN架构。然而，计算机视觉领域已经广泛采用标准化架构和预训练方法来提升性能效率。因此，本文旨在探讨将这些最新实践应用于天文警报分类的有效性。", "method": "使用不同的预训练方案（例如ImageNet和Galaxy Zoo）对标准模型架构进行试验，并将其与定制的CNN基准进行比较。", "result": "发现标准化架构通过预训练能获得更好的性能，尤其是预训练于Galaxy Zoo的数据集时。此外，这些方法在推理时间和内存使用方面也更高效。", "conclusion": "研究结果表明，在宽域时域调查领域中采用最新的计算机视觉实践可以实现更高的性能和效率，并建议进行范式转变，以适应未来的大型天文观测项目。"}}
{"id": "2512.11946", "pdf": "https://arxiv.org/pdf/2512.11946", "abs": "https://arxiv.org/abs/2512.11946", "authors": ["Pramudita Satria Palar", "Paul Saves", "Rommel G. Regis", "Koji Shimoyama", "Shigeru Obayashi", "Nicolas Verstaevel", "Joseph Morlier"], "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.", "AI": {"tldr": "基于个体条件期望（ICE）曲线提出了一个工程设计中的数据驱动全局敏感性分析的新方法。", "motivation": "部分依赖图（PDPs）在解释黑箱模型时，当存在强交互作用时，其全球敏感度指标可能具有误导性。因此，提出了一种基于ICE的全局敏感性指标来更好地捕捉交互影响。", "method": "通过计算ICE曲线上的期望特征重要性和标准差提出了新方法，并引入了ICE相关值以量化交互如何改变输入和输出之间的关系。此外，提供了数学证明表明PDP基础敏感度是所提ICE基础指标的一个下限。", "result": "在三个不同案例中比较评估了基于ICE的敏感性与PDP、SHAP和Sobol'指数的结果，显示ICE方法提供了比传统PDP更为丰富的见解，并且PDP、ICE和SHAP通过提供多个视角相互补充解释结果。", "conclusion": "该研究提出的新方法改进了工程设计中的全局敏感度分析，为理解和优化复杂系统提供了新的工具。"}}
{"id": "2512.11944", "pdf": "https://arxiv.org/pdf/2512.11944", "abs": "https://arxiv.org/abs/2512.11944", "authors": ["Jia Hu", "Yang Chang", "Haoran Wang"], "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach", "categories": ["cs.RO", "cs.AI"], "comment": "34 pages, 11 figures", "summary": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.", "AI": {"tldr": "本文综述了基于学习的运动规划方法，并提出了一个数据驱动的最优控制框架，以解决传统管道方法与现代机器学习系统之间的矛盾。", "motivation": "高阶自主驾驶中的运动规划面临透明但脆弱的传统方法和适应性强但不透明的学习系统的困境。通过回顾这些方法的发展，作者旨在提出一种解决方案来克服这一障碍。", "method": "本文综述了从管道方法到模仿学习、强化学习再到生成式AI的演变过程，并提出了一个数据驱动最优控制框架作为统一方案。该框架结合了经典控制结构的可验证性与机器学习的适应能力，利用实际数据不断优化系统动力学、成本函数及安全性约束。", "result": "本文探索了该框架实现“以人为核心”的定制化、“平台自适应”动态调整以及“系统自我优化”的潜力。并据此提出了未来的研究方向。", "conclusion": "通过这种综合方法，可以开发出既安全又解释性好、具备人类级自主性的智能交通系统。"}}
{"id": "2512.11943", "pdf": "https://arxiv.org/pdf/2512.11943", "abs": "https://arxiv.org/abs/2512.11943", "authors": ["Yu Liu", "Wenwen Li", "Yifan Dou", "Guangnan Ye"], "title": "How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism", "categories": ["cs.MA", "cs.AI", "econ.GN"], "comment": "7 pages, 5 figures", "summary": "Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent \"AI optimism\"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.", "AI": {"tldr": "研究探讨了AI代理在网络效应游戏中的决策行为，特别是在缺乏历史数据的情况下如何影响其均衡推理。", "motivation": "理解多智能体框架中的决策对于分析网络驱动的策略互动至关重要。然而，这种情况在现实世界中普遍存在但在多智能体系统研究中相对较少探索。", "method": "通过设计一个基于大型语言模型（LLM）代理的重复决策场景的工作流程，并操纵价格轨迹和网络效应强度来测试AI代理的行为。", "result": "主要发现包括：缺乏历史数据时，代理无法推断出均衡；有序的历史序列可以导致部分收敛，但强网络效应会导致持续的“人工智能乐观”现象；随机化的历史记录会破坏收敛性。", "conclusion": "研究揭示了一个新的范式转变，在AI中介系统中，均衡结果不仅依赖于激励机制，还取决于历史数据如何被管理。"}}
{"id": "2512.11942", "pdf": "https://arxiv.org/pdf/2512.11942", "abs": "https://arxiv.org/abs/2512.11942", "authors": ["Vince Trencsenyi"], "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play", "categories": ["cs.AI", "cs.FL", "cs.GT"], "comment": null, "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.", "AI": {"tldr": "本文介绍了用于解决多智能体系统中代理错配问题的超游戏理性化方法。", "motivation": "由于感知差异、信息不对称和有限理性，博弈论中的玩家可能会形成与地面真实情况以及其它玩家解释不一致的私人主观观点。尽管超游戏理论提供了一个处理不同心智模型的方法框架，但缺乏统一的形式表示语言及可扩展算法阻碍了其在多智能体系统研究的应用。", "method": "本文提出了一种逻辑基础特定领域的语言来编码超游戏结构，并开发了基于回答集编程的自动化流程以实现超游戏结构实例化和运行新型超游戏理性化程序，从而验证并保证逻辑一致性。", "result": "通过这种新方法，可以找到解释看似不合理结果的信念结构，为多智能体系统与战略AI中的超游戏理论应用奠定了基础。", "conclusion": "本文的工作建立了超游戏理论、多代理系统和战略性人工智能之间的联系，并提供了处理错配心智模型的一种新颖而实用的方法。"}}
{"id": "2512.11941", "pdf": "https://arxiv.org/pdf/2512.11941", "abs": "https://arxiv.org/abs/2512.11941", "authors": ["Jingmin Zhu", "Anqi Zhu", "James Bailey", "Jun Liu", "Hossein Rahmani", "Mohammed Bennamoun", "Farid Boussaid", "Qiuhong Ke"], "title": "DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \\textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS", "AI": {"tldr": "该论文提出了一种名为DynaPURLS的统一框架，用于零样本骨架动作识别任务。", "motivation": "传统的零样本动作识别方法依赖于静态类级语义与骨架特征对齐，这种粗粒度对齐方式无法克服已见和未见类别之间的领域偏移问题。因此提出了一种新的动态细化机制来增强模型的泛化能力。", "method": "DynaPURLS框架通过大型语言模型生成层次化的文本描述，结合自适应分组模块产生细粒度视觉表示，并采用轻量级可学习投影在推理时调整文本特征以匹配传入的视觉流。此外还引入了信心感知、类别平衡的记忆库来稳定这一动态细化过程。", "result": "DynaPURLS在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上均显著优于现有方法，创造了新的最佳记录。", "conclusion": "通过引入多尺度的视觉语义对应关系以及动态细化机制，DynaPURLS能够在零样本骨架动作识别任务中取得更好的性能表现。"}}
{"id": "2512.11939", "pdf": "https://arxiv.org/pdf/2512.11939", "abs": "https://arxiv.org/abs/2512.11939", "authors": ["Clément Fernandes", "Wojciech Pieczynski"], "title": "Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains", "categories": ["cs.CV", "math.ST", "stat.AP"], "comment": "ef:Mathematics 2025, 13 (10), pp.1589", "summary": "Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.", "AI": {"tldr": "该论文介绍了结合上下文Peano扫描和隐藏证据马尔可夫链的新模型HEMC-CPS，用于图像分割。", "motivation": "提出新的HEMC-CPS模型以改善基于HMC的贝叶斯分割方法，并应用于更复杂的图像类型如三维或多传感器多分辨率图像。", "method": "将上下文Peano扫描与隐藏证据马尔可夫链结合，使用随机期望最大化方法估计参数。", "result": "新模型在合成和真实图像上的最大后验模式分割有效。", "conclusion": "HEMC-CPS模型为复杂的图像类型提供了建模和分割的潜力，并且不限于图像分割。"}}
{"id": "2512.11935", "pdf": "https://arxiv.org/pdf/2512.11935", "abs": "https://arxiv.org/abs/2512.11935", "authors": ["Jaehyung Lee", "Justin Ely", "Kent Zhang", "Akshaya Ajith", "Charles Rhys Campbell", "Kamal Choudhary"], "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": null, "summary": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.", "AI": {"tldr": "介绍AGAPI平台，这是一个开放访问的智能AI平台，用于加速材料设计。", "motivation": "当前人工智能在材料研究中的应用受限于计算生态系统的碎片化、可重复性挑战以及对商业大型语言模型的依赖。因此，开发一个统一的数据、模拟工具和机器学习模型的开放式AI平台是必要的。", "method": "AGAPI通过Agent-Planner-Executor-Summarizer架构自主构建并执行涉及材料数据检索、图神经网络属性预测等多步骤工作流程，并整合了多个开源LLM与科学计算API。", "result": "展示AGAPI在异质结构构建、粉末X射线衍射分析和半导体缺陷工程中的应用，通过30+个示例提示进行测试，并将有无工具访问的智能预测与实验数据对比。", "conclusion": "AGAPI提供了一个可扩展且透明的基础平台，用于重现性高并且被AI加速的材料发现。该代码库可在GitHub上获取。"}}
{"id": "2512.11934", "pdf": "https://arxiv.org/pdf/2512.11934", "abs": "https://arxiv.org/abs/2512.11934", "authors": ["Adeleh Mazaherian", "Erfan Nourbakhsh"], "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching", "categories": ["cs.CY", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).", "AI": {"tldr": "对AI教育应用在数字教育转型中的作用进行情感驱动评估", "motivation": "探讨用户对生成式人工智能在教学中使用的感知，以推动更加公平和创新的在线学习环境", "method": "通过抓取谷歌Play商店的数据与评论、使用RoBERTa模型分类情感及GPT-4o提取关键点进行研究，并利用GPT-5合成主要正负主题", "result": "结果显示大多数应用收到积极评价，特别是作业助手类APP表现优异。然而语言和学习管理系统由于不稳定性和功能限制受到批评", "conclusion": "AI教育应用在提高教学效率与个性化方面具有潜力，但也存在依赖风险及不平等问题。未来应构建混合人工智人模型，结合VR/AR技术，并制定适应性个人化策略及政策规范以促进公平"}}
{"id": "2512.11933", "pdf": "https://arxiv.org/pdf/2512.11933", "abs": "https://arxiv.org/abs/2512.11933", "authors": ["Eren Kurshan", "Tucker Balch", "David Byrd"], "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.MA", "q-fin.GN"], "comment": null, "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.", "AI": {"tldr": "本文提出了一种代理框架，以应对金融行业中新兴的人工智能技术所带来的治理挑战。", "motivation": "当前的模型风险框架无法适应不断学习、交换潜在信号并展示出涌现行为的大型语言模型和多代理交易系统。这些新技术带来了新的监管难题，需要一种全新的治理架构来解决这些问题。", "method": "通过复杂自适应系统的理论，将新兴的人工智能技术建模为去中心化的集合体，并提出了一种包含四层‘监管模块’的分层治理体系：自我调节模块、公司层面的治理模块、监管机构托管的代理和独立审计模块。这些建议设计策略旨在使这些模块能够与它们所监控的模型一样快速地演变。", "result": "通过一个多智能体交易中出现的价格操纵行为案例，展示了该框架如何实时隔离有害行为的同时保留创新。这种架构与现有的模型风险规则兼容，同时填补了关键的可观察性和控制漏洞。", "conclusion": "本文提出了一种适应性强且实用的人工智能治理体系，为金融系统中的人工智能治理提供了一个实际路径。"}}
{"id": "2512.11931", "pdf": "https://arxiv.org/pdf/2512.11931", "abs": "https://arxiv.org/abs/2512.11931", "authors": ["Alexander K. Saeri", "Sophia Lloyd George", "Jess Graham", "Clelia D. Lacarriere", "Peter Slattery", "Michael Noetel", "Neil Thompson"], "title": "Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy", "categories": ["cs.CY", "cs.AI"], "comment": "Access AI Risk Mitigation Database and Taxonomy at https://airisk.mit.edu", "summary": "Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.", "AI": {"tldr": "本文介绍了初步的人工智能风险缓解分类法，用于组织人工智能的风险缓解措施，并提供了一个共同的参考框架。", "motivation": "当前在人工智能风险管理领域存在碎片化现象，术语不一致且覆盖范围有限。因此需要一个共同框架来协调不同主体之间的行动以减少人工智能带来的风险。", "method": "通过快速证据扫描，收集了2023年至2025年期间发布的13个人工智能风险缓解框架，并从中提取出831项人工智能风险缓解措施，这些措施经过迭代聚类和编码后创建了分类法。", "result": "提出了一个包含四个类别（治理与监督、技术与安全、操作流程、透明度与问责制）及23个子类别的初步AI风险管理分类法。此外还揭示了一些术语如‘风险管理和红队测试’尽管被广泛使用但含义不同的情况。", "conclusion": "该分类法和相关的缓解措施数据库虽处于初步阶段，但仍为人工智能生态系统的不同参与者提供了一个讨论和协调行动以降低人工智能风险的起点和框架。"}}
{"id": "2512.11930", "pdf": "https://arxiv.org/pdf/2512.11930", "abs": "https://arxiv.org/abs/2512.11930", "authors": ["Mei Jiang", "Haihai Shen", "Zhuo Luo", "Bingdong Li", "Wenjing Hong", "Ke Tang", "Aimin Zhou"], "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.", "AI": {"tldr": "提出了一种基于进化强化学习的AI导师，用于支持STEM跨学科教育中的苏格拉底式教学。", "motivation": "当前方法在动态建模学生认知状态、处理奖励稀疏性及延迟以及缺乏策略多样性方面存在不足，因此需要一种新的框架来解决这些问题。", "method": "提出了一种名为ERL4SIIP的新颖进化强化学习（Erl）框架，该框架包括一个基于STEM知识图的动态学生模拟器、分层奖励机制和LoRA-Division优化策略。", "result": "本研究提供了一个针对苏格拉底式跨学科教学问题的解决方案，并通过具体方法改善了教育AI导师的功能。", "conclusion": "ERL4SIIP框架能够有效应对STEM教育中复杂的认知状态动态建模及长期目标实现等问题，为未来AI在教育领域的应用提供了新的视角和可能。"}}
{"id": "2512.11928", "pdf": "https://arxiv.org/pdf/2512.11928", "abs": "https://arxiv.org/abs/2512.11928", "authors": ["Alexander Peysakhovich", "William Berman", "Joseph Rufo", "Felix Wong", "Maxwell Z. Wilson"], "title": "MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.", "AI": {"tldr": "MONET模型通过训练从明场图像生成细胞绘制通道，以解决物理细胞绘画的劳动强度大和无法研究细胞动态的问题。", "motivation": "物理细胞绘图技术存在耗时长以及需要化学固定的问题，使得难以进行活细胞的研究。因此提出了一种虚拟细胞绘画方法MONET，旨在减少人工干预并能用于实时观察细胞动力学变化。", "method": "使用一致性架构训练扩散模型MONET从明场图像生成细胞绘制通道，并进一步应用于时间序列视频预测。通过大规模数据集提高了模型的质量和泛化能力。", "result": "实验结果表明，该方法在生成高质量的虚拟细胞绘画方面表现良好，同时也能用于生成时间序列视频，并显示了一定程度上向新样本学习的能力。", "conclusion": "MONET提供了一种新的工具来辅助生物研究中的工作流程，虽然不能完全取代物理细胞绘图技术，但可以作为其有力补充。"}}
{"id": "2512.11927", "pdf": "https://arxiv.org/pdf/2512.11927", "abs": "https://arxiv.org/abs/2512.11927", "authors": ["Rijie Xi", "Weikang Xu", "Wei Xiong", "Yuannong Ye", "Bin Zhao"], "title": "Gene regulatory network inference algorithm based on spectral signed directed graph convolution", "categories": ["q-bio.MN", "cs.AI"], "comment": null, "summary": "Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.", "AI": {"tldr": "本文提出了一种基于谱签名有向图卷积的基因调控网络推断算法MSGRNLink。", "motivation": "准确重构基因调控网络对于理解基因功能和疾病机制至关重要。现有的单细胞RNA测序技术提供了大量数据，但传统的谱图卷积难以处理这种表示形式。", "method": "本文提出了一种新的框架MSGRNLink，该框架将基因调控网络建模为签名有向图，并利用磁性签名拉普拉斯卷积提取结构特征和生物学特性。", "result": "实验结果表明，MSGRNLink在AUROC指标上优于所有基线模型。参数敏感性和消融研究验证了其鲁棒性和模块的重要性。", "conclusion": "通过膀胱癌案例研究进一步证实了MSGRNLink的生物相关性以及预测已知边和边缘符号的能力。"}}
{"id": "2512.11926", "pdf": "https://arxiv.org/pdf/2512.11926", "abs": "https://arxiv.org/abs/2512.11926", "authors": ["Qinghao Meng", "Chenming Wu", "Liangjun Zhang", "Jianbing Shen"], "title": "TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.", "AI": {"tldr": "提出了一种基于Transformer的框架TransBridge，用于提升稀疏点云区域中的三维物体检测性能。", "motivation": "在自动驾驶中，远距离和稀疏LiDAR点导致的目标物检测困难是需要解决的关键问题。通过引入一种新的上采样块（TransBridge）来融合补全网络和检测网络的特征，提升整个框架对稀疏区域目标物的检测性能。", "method": "设计了一种动态-静态重建模块DSRecon以生成密集LiDAR数据供补全网络使用，并利用Transformer机制连接通道与空间关系，提高补全任务中的高分辨率特性图。该方法通过TransBridge将补全和检测相结合，从而提升稀疏区域的检测性能。", "result": "在nuScenes和Waymo两个数据集上进行实验验证了所提框架的有效性，结果显示其能显著改善端到端三维物体检测精度，mAP平均提高了0.7至1.5分，并且对于两阶段检测框架，它可以将mAP提高高达5.78点。", "conclusion": "TransBridge通过引入Transformer机制和特征融合技术，在稀疏LiDAR数据中实现了更好的三维目标物检测效果，表明其在提升3D物体检测性能方面的潜力。"}}
{"id": "2512.11925", "pdf": "https://arxiv.org/pdf/2512.11925", "abs": "https://arxiv.org/abs/2512.11925", "authors": ["Mozhgan Hadadi", "Talukder Z. Jubery", "Patrick S. Schnable", "Arti Singh", "Bedrich Benes", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications", "categories": ["cs.CV", "cs.AI"], "comment": "mber:MOD-75623", "summary": "Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.", "AI": {"tldr": "FloraForge是一款基于LLM的辅助框架，它使领域专家能够通过迭代自然语言植物细化生成具有生物准确性的可编辑3D植物模型。", "motivation": "目前的方法在生成精确的3D植物模型方面存在局限性。学习方法需要大量的特定物种训练数据，并且缺乏可编辑性；而程序化建模尽管提供了参数控制，但要求专业知识和复杂的规则理解。FloraForge旨在通过利用LLM简化这一过程，使非专业人员也能创建精确的植物模型。", "method": "框架使用LLM辅助模板生成，允许领域专家通过迭代自然语言指令来调整Python脚本，这些脚本能根据生物约束条件生成参数化的B样条曲面表示。最终结果可以转换为任意精度的三角网格，并且可以通过参数化元数据进行量化分析。", "result": "该框架在玉米、大豆和绿豆上进行了演示，展示了通过点云数据分析来手动调整植物描述符文件以拟合程序模型的方法。生成了用于可视化的三角网格以及包含额外参数元数据用于定量分析的三角网格。", "conclusion": "FloraForge通过结合LLM辅助模板创建、数学连续表示和直接参数控制的独特方法，使复杂的几何建模变得民主化，同时保持高度精确性，适用于植物科学中的各种应用。"}}
{"id": "2512.11922", "pdf": "https://arxiv.org/pdf/2512.11922", "abs": "https://arxiv.org/abs/2512.11922", "authors": ["Muhammad Waseem", "Aakash Ahmad", "Kai-Kristian Kemell", "Jussi Rasku", "Sami Lahti", "Kalle Mäkelä", "Pekka Abrahamsson"], "title": "Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use", "categories": ["cs.SE", "cs.AI"], "comment": "10", "summary": "Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.", "AI": {"tldr": "本文分析了Vibe Coding在软件开发中的流程和技术债务之间的权衡，并提出了可持续使用VC的方法和建议。", "motivation": "为了更好地理解和利用Vibe Coding（由AI辅助的软件开发）的优势，同时避免其引入的技术负债和其他风险，作者基于多个内部MVP项目的经验及行业报告分析了这一问题。", "method": "通过对几个内部MVP项目的观察以及对最新行业报告的研究，文章探讨了VC过程中产生的技术债务的原因，并提出了减轻这些影响的方法和建议。", "result": "文章识别并解释了当前模型、平台和硬件限制导致的技术债务等问题，同时提出了解决这些问题的措施。", "conclusion": "研究结果强调了采用更可持续发展的Vibe Coding方法的重要性，为未来的VC实践提供了指导方向。"}}
{"id": "2512.11921", "pdf": "https://arxiv.org/pdf/2512.11921", "abs": "https://arxiv.org/abs/2512.11921", "authors": ["Abdullah Yahya Abdullah Omaisan", "Ibrahim Sheikh Mohamed"], "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.", "AI": {"tldr": "通过使用LoRA和量化技术对VLA模型进行高效的微调，使其能够在低成本的机器人平台上运行。", "motivation": "由于计算约束和适应新机器人平台的需求，大规模部署VLA模型面临挑战。这项研究旨在解决这一问题，并使先进的人机交互功能普及到普通用户。", "method": "提出了一种基于低秩适应（LoRA）和量化技术的资源高效微调策略，让多亿参数级的VLA模型在消费级GPU上运行，同时分析了冻结与未冻结视觉编码器之间的权衡。", "result": "通过在SO101机械臂上的实际部署，证明该方法能够实现有效的抓取性能，并对训练数据量、失败模式和实际表现之间的关系进行了详细分析。", "conclusion": "研究展示了将VLA模型成功部署到低成本机器人平台的方法，表明这种方法能够在保持计算效率的同时提供先进的抓取能力。"}}
{"id": "2512.11920", "pdf": "https://arxiv.org/pdf/2512.11920", "abs": "https://arxiv.org/abs/2512.11920", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving", "categories": ["cs.AI"], "comment": "Accepted to FPGA'26 Oral", "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.", "AI": {"tldr": "本文提出了CXL-SpecKV，一种基于FPGA和CXL技术的离散化KV缓存架构，以解决数据中心大规模语言模型部署中的内存需求问题。", "motivation": "大型语言模型（LLM）在自然语言处理任务中表现出色，但其在数据中心环境下的部署面临巨大的内存需求挑战。为了优化GPU资源利用并提高系统吞吐量，本文旨在提出一种新的KV缓存架构来应对这一难题。", "method": "CXL-SpecKV通过使用CXL互连技术和FPGA加速器实现高效的预测执行和内存离散化。它包括三个主要创新：基于CXL的内存离散框架、推测性KV缓存预取机制以及FPGA加速的KV缓存压缩与解压缩引擎。", "result": "实验表明，当应用于最先进的LLM模型时，CXL-SpecKV相较于纯GPU基线可实现高达3.2倍的吞吐量提升，并将内存成本降低了2.8倍，同时保持了准确性。", "conclusion": "通过智能内存离散化结合推测执行，可以有效解决大规模语言模型服务中的内存墙挑战。"}}
{"id": "2512.11919", "pdf": "https://arxiv.org/pdf/2512.11919", "abs": "https://arxiv.org/abs/2512.11919", "authors": ["Junhyung Park", "Yuqing Zhou"], "title": "A fine-grained look at causal effects in causal spaces", "categories": ["stat.ME", "cs.AI", "math.ST"], "comment": null, "summary": "The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.", "AI": {"tldr": "本文研究了在因果空间框架下事件层面的因果效应，提出了衡量因果效应强度和性质的方法，并证明这些方法可以作为治疗效果测量的特殊情况。", "motivation": "传统的量化研究人员通常从变量角度探讨因果关系，而现代数据领域中的原始变量缺乏语义结构以形成有意义的因果问题。本文通过引入更细致的观点来解决这一挑战，即在事件层面研究因果效应。", "method": "基于因果空间的度量理论框架，本文首先介绍了衡量因果效应是否存在的二元定义及其属性，并提供了量化因果效应强度和性质的方法，这些方法可以作为治疗效果测量的特殊情况。", "result": "该论文证明了所提出的方法可以在事件层面上准确地捕捉到因果关系的强度与性质，并展示了它们如何作为一种特殊形式来恢复常见的治疗效果衡量标准。", "conclusion": "通过引入更加精细的视角研究因果效应，本文不仅丰富了对因果理论的理解，而且还提供了一种更强大的工具来分析复杂的数据领域中的因果关系。"}}
{"id": "2512.11912", "pdf": "https://arxiv.org/pdf/2512.11912", "abs": "https://arxiv.org/abs/2512.11912", "authors": ["Liu Peng", "Yaochu Jin"], "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "categories": ["cs.AI"], "comment": null, "summary": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.", "AI": {"tldr": "该论文系统地比较了低质量数据对现代概率模型的影响，发现不同类型的模型在面对相同程度的数据损坏时表现出不同的鲁棒性。", "motivation": "分析不同类型的概率模型在面对低质量数据时的稳健性差异，并探索背后的原因。", "method": "通过实验对比了自回归语言模型、条件扩散模型和分类器在不同程度数据损坏情况下的性能，然后从信息论、PAC学习理论以及梯度动态等多个角度进行解释。", "result": "发现自回归语言模型对低质量数据具有较高的鲁棒性；而条件扩散模型的表现则急剧下降；分类器的性能受到的影响较为中等，并且随着训练集规模增大影响逐渐减弱。", "conclusion": "研究结果表明，概率模型对于低质量数据的稳健性受制于条件信息量和训练数据的信息总量两大因素。"}}
{"id": "2512.11909", "pdf": "https://arxiv.org/pdf/2512.11909", "abs": "https://arxiv.org/abs/2512.11909", "authors": ["Hanna Dettki"], "title": "Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets", "categories": ["cs.AI"], "comment": "ef:WiML Workshop at NeurIPS 2025", "summary": "The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \\emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures? We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\\!\\to\\!E\\!\\leftarrow\\!C_2$ ) under \\emph{Direct} (one-shot number as response = probability judgment of query node being one and \\emph{Chain of Thought} (CoT; think first, then provide answer). Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \\in [0,1]$ include a shared prior $p(C)$; we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\\neq}m_2$) variant.", "AI": {"tldr": "研究评估大型语言模型（LLM）和人类在相同因果推理任务上的表现，通过使用漏失噪声-OR因果贝叶斯网络进行建模。", "motivation": "探讨大型语言模型是否与人类具有相同的因果推理能力，以及它们的推理签名是否存在差异。目的在于增进对人工智能和人类智能的理解。", "method": "评估了20多个LLM在11个因果任务上的表现，并利用漏失噪声-OR因果贝叶斯网络进行建模，通过AIC选择最佳模型参数。", "result": "研究结果显示大型语言模型与人类在某些因果推理任务上具有相似的表现，但在特定条件下表现出不同的签名。", "conclusion": "该研究表明了LLM和人类的因果推理能力存在共同点及差异，并提供了对这些系统如何处理复杂因果关系的新见解。"}}
{"id": "2512.11908", "pdf": "https://arxiv.org/pdf/2512.11908", "abs": "https://arxiv.org/abs/2512.11908", "authors": ["Heng Zhang", "Rui Dai", "Gokhan Solak", "Pokuang Zhou", "Yu She", "Arash Ajoudani"], "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models", "categories": ["cs.RO"], "comment": null, "summary": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.", "AI": {"tldr": "本文综述了针对接触密集型任务的机器人安全学习方法，包括安全探索和执行策略，并探讨这些方法如何应用于新兴的机器人基础模型。", "motivation": "机器人在处理接触密集型任务时面临不确定性、复杂动力学及潜在损害风险。传统基于学习的方法虽有潜力，但在确保机器人在探索与实际操作中的安全性方面仍存在挑战。", "method": "综述了包括约束强化学习、风险敏感优化、不确定建模和控制障碍函数在内的安全学习技术，并讨论这些方法如何适应并整合到视觉-语言模型（VLM）及视觉-语言-行动模型（VLAM）中，以处理接触密集型任务。", "result": "总结了现有安全学习策略的优缺点，并指出了基于VLM/VLA的方法带来的新机遇和挑战。", "conclusion": "强调了解决复杂环境中的机器人安全性的当前局限性和未来研究方向。"}}
{"id": "2512.11907", "pdf": "https://arxiv.org/pdf/2512.11907", "abs": "https://arxiv.org/abs/2512.11907", "authors": ["Daniel Platnick", "Marjan Alirezaie", "Hossein Rahnama"], "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents", "categories": ["cs.AI"], "comment": "Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM), 5 pages, 1 figure", "summary": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.", "AI": {"tldr": "本文提出了一种基于拟阵的编译方法，用于解决大型语言模型个性化过程中存在的结构约束问题。", "motivation": "在个性化大型语言模型时，需要平衡任务效用与用户数据披露之间的权衡。现实中的个人化需求受到逻辑依赖、类别配额等结构性约束的影响，而这些问题难以通过传统的方法进行处理。", "method": "本文引入了一种编译过程来将用户的知识图谱转化为抽象的宏特征，并证明了这些宏特征形成的层次和配额结构构成有效的拟阵。在此基础上，提出一种子模最大化问题解法以满足复杂约束条件。", "result": "通过该方法可以实现贪婪算法下的近似优化选择策略，从而有效解决了更加复杂且现实的问题类别的个性化需求。", "conclusion": "本文提供了一种解决大型语言模型个性化过程中结构化数据最小化的理论框架及实际应用方案。"}}
{"id": "2512.11906", "pdf": "https://arxiv.org/pdf/2512.11906", "abs": "https://arxiv.org/abs/2512.11906", "authors": ["Noorul Wahab", "Nasir Rajpoot"], "title": "MPath: Multimodal Pathology Report Generation from Whole Slide Images", "categories": ["cs.CV", "cs.LG"], "comment": "Pages 4, Figures 1, Table 1", "summary": "Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.", "AI": {"tldr": "MPath是一种基于预训练的生物医学语言模型和WSI视觉嵌入生成病理报告的轻量级多模态框架。", "motivation": "自动从全切片图像生成诊断性病理报告在计算病理学中具有挑战，因为需要将高分辨率组织模式翻译成临床连贯文本。现有方法难以处理形态变化多样性和复杂的叙述结构。", "method": "MPath通过学习的视觉前缀提示机制条件化预训练生物医学语言模型BioBART，利用WSI衍生的视觉嵌入。它使用基础模型WSI特征(CONCH + Titan)，并通过紧凑投影模块注入BioBART中，保持语言骨干不变以确保稳定性和数据效率。", "result": "在RED 2025 Grand Challenge 数据集上开发和评估MPath，在测试阶段2排名第四。结果表明提示基于多模态条件化策略作为病理报告生成的可扩展且解释性强的方法具有潜力。", "conclusion": "通过轻量级多模态框架MPath，研究展示了利用基础模型特征和预训练语言模型进行病理报告生成的有效性，并强调了提示式多模态方法的重要性。"}}
{"id": "2512.11905", "pdf": "https://arxiv.org/pdf/2512.11905", "abs": "https://arxiv.org/abs/2512.11905", "authors": ["Ming-Zher Poh", "Shun Liao", "Marco Andreetto", "Daniel McDuff", "Jonathan Wang", "Paolo Di Achille", "Jiang Wu", "Yun Liu", "Lawrence Cai", "Eric Teasley", "Mark Malhotra", "Anupam Pathak", "Shwetak Patel"], "title": "Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life", "categories": ["cs.CV"], "comment": null, "summary": "Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.", "AI": {"tldr": "通过智能手机被动监测微笑，探索其作为日常生活中积极情绪客观指标的潜力。", "motivation": "传统上依赖自我报告的方法测量主观幸福感存在回忆偏差和高参与者负担的问题，这限制了对日常生活中的幸福感的理解。本文提出利用自然互动中捕获的笑容强度作为正向情感的行为替代标志来填补这一空白。", "method": "收集233名参与者的405,448段视频片段，并使用深度学习模型量化笑容强度以识别日常和每日模式。分析结果与国家调查数据以及日重建方法的结果进行对比，评估其相关性。", "result": "微笑强度的每日变化与国家幸福感调查高度相关（r=0.92），并且每日规律与日重构法一致（r=0.80）。更高的平均笑容强度与更多的身体活动和更大的光照暴露呈显著正相关。", "conclusion": "被动智能手机传感可作为研究情感行为动力学的强大且生态有效的方法，有助于在人群中理解这种行为。"}}
{"id": "2512.11903", "pdf": "https://arxiv.org/pdf/2512.11903", "abs": "https://arxiv.org/abs/2512.11903", "authors": ["Iacopo Catalano", "Eduardo Montijano", "Javier Civera", "Julio A. Placed", "Jorge Pena-Queralta"], "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.", "AI": {"tldr": "介绍Aion框架，该框架在层次化3DSG中嵌入了时间流动力学，以捕捉动态环境中的运动模式。", "motivation": "当前的3D场景图和动态地图模型分别缺乏对时间和语义的理解，并且不适用于复杂的大规模环境。因此，需要一种新的方法来集成这些方面。", "method": "Aion使用基于图的稀疏MoD表示来捕获任意时间间隔上的运动流，并将其附加到场景图中的导航节点上。", "result": "Aion框架可以生成更可解释和更具扩展性的预测，在复杂动态环境中改善了规划和交互。", "conclusion": "通过将时间流动力学直接嵌入层次化3DSG，Aion提供了一种新的方法来处理复杂的动态环境。"}}
{"id": "2512.11902", "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "categories": ["cs.AI", "cs.GT", "cs.LG"], "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode", "AI": {"tldr": "该论文介绍了Mirror Mode，一种新的游戏模式，在Fire Emblem中敌方AI模仿玩家个人策略以挑战玩家。研究使用生成对抗性模仿学习、行为克隆和近端策略优化来训练模型。", "motivation": "在回合制游戏中，敌人策略应具有惊喜性和不可预测性。通过让敌人模仿玩家的策略可以提高游戏体验，并增加玩家满意度。", "method": "该论文构建了Fire Emblem Heroes简化版，在Unity中包含标准模式和镜像模式。实验结合使用生成对抗性模仿学习、行为克隆和近端策略优化来训练模型，以模仿玩家示范。", "result": "试验表明模型在防守行为上表现良好，但在进攻策略上的模仿质量较低。玩家调查结果显示他们认识到了自己的撤退战术，并且对镜像模式的整体满意度较高。", "conclusion": "尽管当前的模型已经能够在一定程度上模仿玩家的行为和提升游戏体验，但进一步改进模型以提高模仿质量和增加玩家满意度仍是未来的研究方向。"}}
{"id": "2512.11901", "pdf": "https://arxiv.org/pdf/2512.11901", "abs": "https://arxiv.org/abs/2512.11901", "authors": ["Santosh Patapati"], "title": "CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities", "categories": ["cs.CV", "cs.LG"], "comment": "WACV; Supplementary material is available on CVF proceedings", "summary": "We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.", "AI": {"tldr": "CLARGA是一种通用的多模态融合架构，用于处理任意数量和类型的模态数据，并构建加权图以提高跨模态一致性。", "motivation": "为了实现高效的多模态表示学习，需要一种适应性强且能够处理缺失输入的方法。通过自适应地构造图形并传递信息来解决这一问题。", "method": "CLARGA通过对不同模态的特征建立加权图并通过Graph Attention Network进行消息传递，以融合不同的模态表示。它使用可学习掩码来适应丢失的数据，并结合监督任务损失和对比InfoNCE损失进行训练。", "result": "在多个数据集上，CLARGA的表现优于基线模型、最新技术方法及其变体，展示了其对缺失输入的鲁棒性和处理特殊任务的能力。", "conclusion": "CLARGA可以轻松地集成到机器学习模型中，并且对于广泛的任务具有有效的和高效的表示学习能力。"}}
{"id": "2512.11900", "pdf": "https://arxiv.org/pdf/2512.11900", "abs": "https://arxiv.org/abs/2512.11900", "authors": ["Christopher E. Mower", "Rui Zong", "Haitham Bou-Ammar"], "title": "Data-driven Interpretable Hybrid Robot Dynamics", "categories": ["cs.RO"], "comment": null, "summary": "We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.", "AI": {"tldr": "研究基于数据驱动的可解释混合机器人动力学模型，通过符号回归和非线性动力学稀疏识别来恢复残差力矩项的紧凑闭式表达式。", "motivation": "提高机器人动力学建模的准确性、可解释性和泛化能力，特别是在已知动力学的情况下能够准确地再现惯性、科里奥利效应、重力和粘性效应。", "method": "采用符号回归和稀疏识别非线性动态（SINDy）方法从关节空间数据中恢复残差项的紧凑闭式表达式。", "result": "在仿真中，该模型能够准确地再现惯性和其他动力学效应，并且比神经网络基线更精确、泛化能力更强。实际机器人实验显示符号回归残差具有更好的泛化能力。", "conclusion": "可解释的残差动态模型提供了紧凑、准确和物理意义明确的替代方案来预测力矩，优于黑盒函数逼近器。"}}
{"id": "2512.11899", "pdf": "https://arxiv.org/pdf/2512.11899", "abs": "https://arxiv.org/abs/2512.11899", "authors": ["Futa Waseda", "Shojiro Yamabe", "Daiki Shiono", "Kento Sasaki", "Tsubasa Takahashi"], "title": "Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.", "AI": {"tldr": "提出了一项新任务 Read-or-Ignore VQA（RIO-VQA），并构建了一个标准化的评估基准 RIO-Bench，以解决大型视觉语言模型在面对误导性文本时的能力和鲁棒性的平衡问题。", "motivation": "现有评估协议和防御机制主要关注物体识别而忽视了文字的重要性，在实际应用中需要同时处理图像中的对象和文字信息。为此，引入了新的任务来解决这种能力与鲁棒性的不平衡问题。", "method": "提出 RIO-VQA 任务，通过在真实场景下调整文本内容和问题类型来生成评估基准 RIO-Bench。该基准测试大型视觉语言模型在面对误导性文本时的表现，并展示现有方法的不足之处。", "result": "研究表明现有的 LVLM 和防御机制无法有效平衡文字识别能力和鲁棒性，新提出的防御措施能够根据上下文学习选择性的使用文本信息来提高性能。", "conclusion": "这项工作揭示了现有评估范围与现实需求之间的根本偏差，并提供了可靠大型视觉语言模型发展的原则路径。"}}
{"id": "2512.11898", "pdf": "https://arxiv.org/pdf/2512.11898", "abs": "https://arxiv.org/abs/2512.11898", "authors": ["Yawar Ali", "K. Ramachandra Rao", "Ashish Bhaskar", "Niladri Chatterjee"], "title": "Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic", "categories": ["cs.CV"], "comment": "This paper presents basic statistics and trends in empirically observed data from highly heterogeneous and area-based traffic while offering the datasets open source for researchers and practitioners", "summary": "This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.", "AI": {"tldr": "论文提供了一个基于无人机收集的车辆微观轨迹数据集，旨在改善城市交通研究。", "motivation": "传统路边视频采集在密集混合交通中由于遮挡、视角有限和不规则车流移动等问题效果不佳。无人机拍摄能够提供俯视角度，减少这些问题并捕捉时空动态变化。", "method": "使用Data from Sky平台从印度国家首都地区的六个路段收集时间戳车辆位置、速度等数据，并与手动计数和其他探针轨迹进行验证。", "result": "收集的微观车辆轨迹数据集包括30帧/秒的时间戳车辆位置，速度和加速度信息。分析结果揭示了车道保持偏好、速度分布和横向操作的行为模式。", "conclusion": "通过提供这些经验性的开放数据集，研究者可以开发、测试和完善更准确地代表复杂城市交通环境的模型。"}}
{"id": "2512.11896", "pdf": "https://arxiv.org/pdf/2512.11896", "abs": "https://arxiv.org/abs/2512.11896", "authors": ["Tessa Vu"], "title": "Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat", "categories": ["cs.CV", "cs.CE", "cs.CY"], "comment": "Completed as a requirement in MUSA 6950-001", "summary": "Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.", "AI": {"tldr": "该论文提出了一种名为Hot Hém的GeoAI工作流程，用于评估和优化胡志明市（又称西贡）行人的热暴露情况。", "motivation": "行人受到高温影响是一个重要的健康风险，在密集的城市环境中尤其如此。现有的路径规划算法往往忽略了微尺度上的温度变化。因此，需要开发一种能够考虑这些因素的方法来改善城市的行人舒适度和安全性。", "method": "该论文使用Google Street View图像、语义分割以及遥感技术结合XGBoost模型预测地表温度（LST）。通过在选定的行政区划网格中训练两个XGBoost模型，并将其部署在整个开放街道地图导出的行网络节点上，实现热感知路径规划。", "result": "该研究开发了Hot Hém这一工作流程，能够根据实时和历史数据预测并优化行人路线中的温度状况。这项技术可以作为基础，进一步理解为什么某些城市区域可能会经历异常高的温度。", "conclusion": "通过结合多种地理空间技术和AI模型，Hot Hém为改善热带城市的热暴露情况提供了可能的解决方案，有助于提高市民在高温环境下的健康和舒适度。"}}
{"id": "2512.11894", "pdf": "https://arxiv.org/pdf/2512.11894", "abs": "https://arxiv.org/abs/2512.11894", "authors": ["Mahathir Monjur", "Shahriar Nirjon"], "title": "mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at the IEEE/CVF Winter Conference on Applications of Computer Vision 2026 (WACV 2026)", "summary": "Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.", "AI": {"tldr": "mmWEAVER利用隐式神经表示和超网络生成环境特定的毫米波信号，提高活动识别精度并减少姿态估计误差。", "motivation": "毫米波雷达应用依赖于多样的、环境特定的信号数据集，但物理模拟计算成本高昂。提出mmWeaver框架以解决此问题。", "method": "利用隐式神经表示和超网络根据RGB-D图像提取的环境信息及从文本到姿势生成的人体运动特征来动态生成毫米波信号参数。", "result": "实验表明，mmWeaver在复杂SSIM和PSNR上优于现有方法，并提高活动识别精度15%以上，同时减少姿态估计误差7%以上。其操作速度比基于模拟的方法快6-35倍。", "conclusion": "通过结合隐式神经表示和超网络，mmWeaver能够高效地生成逼真的毫米波信号数据集，改善下游任务表现并提高效率。"}}
{"id": "2512.11893", "pdf": "https://arxiv.org/pdf/2512.11893", "abs": "https://arxiv.org/abs/2512.11893", "authors": ["Haocheng Lin"], "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.", "AI": {"tldr": "探讨了生成性人工智能对工作、创造力和治理的影响，并提出了一种综合框架来理解其社会后果。", "motivation": "为了应对由AI革命带来的经济安全威胁和社会变革，研究者希望探索新的政策方向和措施以促进包容性和创造性环境的建立。", "method": "采用混合方法论整合了劳动力市场任务暴露建模、部门扩散图绘制、政策框架分析以及质性话语批评等手段来展开研究。", "result": "提出了一个全面理解AI社会影响的新视角，强调UBI应当作为治理体系中的一部分与其他措施结合使用以应对挑战。", "conclusion": "建议未来的研究方向应包括对不同AI模型创造力表现的系统评估、构建人工智能应用分布和公平性的分类法以及制定平衡内容限制与创意自由度的标准。"}}
{"id": "2512.11892", "pdf": "https://arxiv.org/pdf/2512.11892", "abs": "https://arxiv.org/abs/2512.11892", "authors": ["Jon Crowcroft", "Rute C. Sofia", "Dirk Trossen", "Vassilis Tsaoussidis"], "title": "Should AI Become an Intergenerational Civil Right?", "categories": ["cs.CY", "cs.AI", "cs.NI"], "comment": null, "summary": "Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights. This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \\emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations. Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.", "AI": {"tldr": "该论文探讨了将人工智能作为一项代际公民权利的必要性，并提出了一种在资源受限情况下支持公平访问的人工智能交付网络架构。", "motivation": "由于AI系统的能源、网络和计算资源有限且分布不均，作者认为应当将基于公共知识生产的AI输出视为一种基本公民利益而非商业服务。现有监管框架忽视了这一问题，因此需要建立一个既能保护当下的包容性又能维护未来代际权利的法律与伦理框架。", "method": "通过分析现有法规和新兴技术（如物联网-边缘-云计算、分散推断及节能网络）之间的联系，提出了一种实现AI作为公民权益的技术路径。具体来说，设计了一个概念性的AI交付网络架构来支持公平访问。", "result": "该研究展示了如何从技术和治理角度出发，在资源限制下确保AI的公正和可持续使用。", "conclusion": "本文认为应当将AI视为一种共享的社会基础设施而非市场商品，并建议通过建立代际公民权利来推动社会正义与环境可持续性的AI部署。"}}
{"id": "2512.11891", "pdf": "https://arxiv.org/pdf/2512.11891", "abs": "https://arxiv.org/abs/2512.11891", "authors": ["Songqiao Hu", "Zeyi Liu", "Shuang Liu", "Jun Cen", "Zihan Meng", "Xiao He"], "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer", "categories": ["cs.RO", "eess.SY"], "comment": "20 pages, 14 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.", "AI": {"tldr": "该论文提出了一种名为AEGIS的Vision-Language-Safe Action (VLSA)架构，通过集成一个可插拔的安全约束层来提高现有Vision-Language-Action模型在复杂环境中的安全性。", "motivation": "现有的Vision-Language-Action模型虽然展示出跨多样机器人操作任务的强大泛化能力，但在无结构环境中部署时仍面临挑战，特别是在防止潜在碰撞方面。因此，引入了一种新的架构来保证同时满足任务合规性和安全性的要求。", "method": "AEGIS架构包含一个基于控制屏障函数公式化的可插拔安全约束层，可以直接与现有的Vision-Language-Action模型集成以提高安全性，并保持其原始的指令跟随性能。为了评估该方法的有效性，构建了一个涵盖不同操作场景的安全关键基准SafeLIBERO。", "result": "实验结果显示AEGIS在障碍物规避率方面比最先进的基线方法提高了59.16%，同时大幅增加了任务执行成功率17.25%。", "conclusion": "该论文展示了如何通过集成一个可插拔的安全约束层来改进现有的Vision-Language-Action模型，从而提高机器人操作系统的安全性。"}}
{"id": "2512.11887", "pdf": "https://arxiv.org/pdf/2512.11887", "abs": "https://arxiv.org/abs/2512.11887", "authors": ["Yihan Liao", "Jingyu Zhang", "Jacky Keung", "Yan Xiao", "Yurou Dai"], "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions", "categories": ["cs.CY", "cs.AI"], "comment": "Accepted for publication in Information and Software Technology (IST)", "summary": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.", "AI": {"tldr": "研究探讨了自动驾驶系统的测试方法，识别了现有技术的不足，并提出了未来的研究方向。", "motivation": "确保自动驾驶系统在复杂环境中的可靠性是关键挑战。有效的测试对于验证其性能和降低部署风险至关重要。", "method": "通过大规模调查（100名参与者）以及对105篇代表性研究文献进行分析，评估了当前的测试技术和识别未来的研究需求。", "result": "结果显示现有技术难以全面评价自动驾驶系统的真实世界表现，特别是在处理边缘案例、模拟与现实差异等方面存在挑战。", "conclusion": "该研究总结了自动驾驶系统测试的关键差距，并提出了包括综合测试标准和大规模验证框架在内的未来研究方向。"}}
{"id": "2512.11886", "pdf": "https://arxiv.org/pdf/2512.11886", "abs": "https://arxiv.org/abs/2512.11886", "authors": ["Mohammed Irfan Ali"], "title": "Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control", "categories": ["cs.RO"], "comment": null, "summary": "Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.", "AI": {"tldr": "本文开发了一种自主导航管道，使COBRA蛇形机器人能够在缺乏外部追踪基础设施的环境中实现多路点跟踪。", "motivation": "传统的轮式探测器无法穿越极端地形，而蛇形机器人虽然具有出色的移动性，但其高度连杆结构对其自主导航构成挑战。因此，本文旨在开发一种能够独立完成任务的蛇形机器人。", "method": "利用视觉惯性里程计和闭环轨迹跟踪控制结合深度相机进行实时定位，并通过生物启发式的CPG步态参数调节来实现自主导航。", "result": "实验验证了系统的准确性，证明它能准确地执行多路点跟踪任务。", "conclusion": "该系统为自主蛇形机器人的导航奠定了基础。"}}
{"id": "2512.11884", "pdf": "https://arxiv.org/pdf/2512.11884", "abs": "https://arxiv.org/abs/2512.11884", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors", "AI": {"tldr": "本文比较了零样本模式下的SAM3与经过实例分割微调的YOLO模型在MinneApple数据集上的表现，探讨了通用基础模型和专门化模型的优势。", "motivation": "通过评估不同IoU阈值下模型性能差异，研究了零样本模型与微调检测器之间的优势对比，并提供了针对密集物体分割任务选择合适模型的建议。", "method": "使用MinneApple数据集上的670张果园图像进行实验，比较了三个YOLO变体和SAM3在不同IoU阈值下的F1得分。引入了开放源代码、评估管道及方法论建议。", "result": "在IoU=0.15时，YOLO模型达到较高的F1分数；但当IoU增加时，YOLO性能迅速下降，而SAM3则表现出更好的边界稳定性。", "conclusion": "研究表明，在检测完整度方面微调的Yolo优于零样本模式下的SAM3，但在高密度和遮挡场景中SAM3具有显著优势。选择模型应考虑特定任务需求。"}}
{"id": "2512.11883", "pdf": "https://arxiv.org/pdf/2512.11883", "abs": "https://arxiv.org/abs/2512.11883", "authors": ["Wenqi Marshall Guo", "Qingyun Qian", "Khalad Hasan", "Shan Du"], "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": null, "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.", "AI": {"tldr": "研究通过构建广泛美学数据集来测试图像生成模型和奖励模型是否存在对美观偏好的系统性偏差。", "motivation": "探讨过度对齐的图像生成模型是否会偏向于产生常规美丽的输出，忽略用户请求低质量或负面图像的情况，这会影响用户的自主性和审美多样性。", "method": "构建了广泛谱系美学数据集，并评估了最先进的生成和奖励模型。通过图像到图像编辑和真实抽象艺术品评估来确认系统性偏见。", "result": "发现对齐的美学生成模型经常默认产生常规美丽的输出，忽略用户请求低质量或负面图像的情况；奖励模型即使在完美匹配用户提示时也惩罚反美学图像。", "conclusion": "研究结果表明，现有的美学对齐和评价系统存在明显的偏向性问题，需要改进以更好地尊重用户的意图和审美多样性。"}}
{"id": "2512.11882", "pdf": "https://arxiv.org/pdf/2512.11882", "abs": "https://arxiv.org/abs/2512.11882", "authors": ["Lucia Happe", "Dominik Fuchß", "Luca Hüttner", "Kai Marquardt", "Anne Koziolek"], "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "comment": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track", "summary": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.", "AI": {"tldr": "本文报告了RockStartIT Tutor的开发和试点评估，这是一种基于GPT-4的AI辅助工具，用于支持编程和计算思维课程的教学。", "motivation": "近年来，大规模语言模型（LLMs）的进步重新点燃了个性化辅导的可能性。作者旨在设计一种既能减少认知负荷又能够促进学习的教育工具。", "method": "RockStartIT Tutor采用了一种新型提示策略以及模块化、语义标记的知识库，以提供符合课程要求的支持，并通过技术接受模型（TAM）对13名学生和教师进行了评估。", "result": "学生们欣赏这种低风险提问的环境并且能够得到分层指导。教育者强调系统可以在独立任务中减少认知负荷并补充课堂教学。", "conclusion": "研究结果表明，AI辅助工具不应被视为替代教师的角色，而应被看作是拓展反馈访问、促进探究和学习的支持性工具。"}}
{"id": "2512.11881", "pdf": "https://arxiv.org/pdf/2512.11881", "abs": "https://arxiv.org/abs/2512.11881", "authors": ["Nathaniel H. Park", "Eduardo Soares", "Victor Y. Shirasuna", "Tiffany J. Callahan", "Sara Capponi", "Emilio Vital Brazil"], "title": "Understanding Structural Representation in Foundation Models for Polymers", "categories": ["cond-mat.soft", "cs.AI", "cs.LG"], "comment": null, "summary": "From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.", "AI": {"tldr": "该论文提出了一种基于SMILES的聚合物图表示的新基础模型，用于解决聚合物结构表示的问题，并展示了其在多项基准测试中的优秀性能。", "motivation": "开发针对聚合物的基础模型面临训练数据稀缺和标准化基准缺乏等挑战。这些问题很大程度上源于对聚合物结构表示的理解不足。论文旨在通过新方法改进这一点，以提高基础模型的性能。", "method": "提出了使用SMILES为基础的新聚合物图表示方法，并在多项控制实验中验证其有效性。该方法能够捕捉到其他SMILES表示法无法捕获的关键聚合物架构特征和连通性。", "result": "新开发的基础模型在28个不同基准数据集上表现出色，显示出对各种SMILES表示的高度不变性和优越性能。这表明基于序列空间的插值而非仅限于语义有效输入可以用于预测任务。", "conclusion": "该工作强调了控制实验的重要性，以检查人类强加假设的影响，并为化学基础模型及其底层结构表征的设计提供了重要的见解和建议。"}}
{"id": "2512.11879", "pdf": "https://arxiv.org/pdf/2512.11879", "abs": "https://arxiv.org/abs/2512.11879", "authors": ["Beatriz Costa-Gomes", "Sophia Chen", "Connie Hsueh", "Deborah Morgan", "Philipp Schoenegger", "Yash Shah", "Sam Way", "Yuki Zhu", "Timothé Adeline", "Michael Bhaskar", "Mustafa Suleyman", "Seth Spielman"], "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage", "categories": ["cs.CY", "cs.AI"], "comment": "12 pages, 10 figures", "summary": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.", "AI": {"tldr": "本文分析了微软Copilot在2025年1月至9月期间的3750万去身份化对话，探讨了用户使用AI的时间和模式动态。", "motivation": "前人对AI使用的分析主要关注人们如何使用AI，而忽视了时间和设备类型的影响。本文旨在深入研究不同时间点和设备上AI使用情况的差异及其原因。", "method": "收集并分析2025年1月至9月期间37.5万用户与微软Copilot之间的对话记录，重点考察不同的上下文、设备类型以及时间段内用户的交互模式。", "result": "手机端的主要主题是健康，这种趋势在全天候和每月都持续存在。而在台式机上，“工作和职业”成为8:00-17:00之间最受欢迎的主题。不同时间点的查询模式也显示出明显的差异：编程问题在工作日激增，而周末游戏活动增加。", "conclusion": "用户已将AI融入生活的各个方面，在工作中用作工具，在手机上作为伴侣，这表明了人们对AI的高度依赖与快速整合"}}
{"id": "2512.11876", "pdf": "https://arxiv.org/pdf/2512.11876", "abs": "https://arxiv.org/abs/2512.11876", "authors": ["Hrigved Mahesh Suryawanshi"], "title": "Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analy- sis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real- time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR- based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhib- ited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.", "AI": {"tldr": "论文提出了一种针对M4机器人平台的可穿越性感知自主导航框架，利用实时地形分析生成安全高效的路径。", "motivation": "在非结构化环境中实现自主导航需要机器人能够实时评估地形难度并规划平衡效率与安全性之间的路径。为此，本文通过学习地形特性来优化路径选择。", "method": "该方法采用FAST-LIO进行实时定位，并从LiDAR点云生成2.5D高度图；利用CNN模型处理这些高度图估计可穿越性得分，并将其转换为导航成本以规划路线；最后结合自定义A*算法寻找既避开低可穿越性区域又能优化路径能耗的最适解。", "result": "实验结果表明，该系统能够有效避免地形较差区域并接受较长但更安全的路径选择来减少总体行走难度。", "conclusion": "这项研究为适用于多模式机器人平台的智能地形感知导航奠定了基础。"}}
{"id": "2512.11874", "pdf": "https://arxiv.org/pdf/2512.11874", "abs": "https://arxiv.org/abs/2512.11874", "authors": ["Jiahao Jiang", "Zhangrui Yang", "Xuanhan Wang", "Jingkuan Song"], "title": "Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training", "categories": ["cs.CV"], "comment": "3 pages,3 figures, Extended abstract submitted to the 10th Computer Vision in Plant Phenotyping and Agriculture (CVPPA) Workshop, held in conjunction with ICCV 2025", "summary": "This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.", "AI": {"tldr": "本文提出了一种针对小麦头部分割的伪标签优化框架，采用两阶段混合训练策略和数据增强技术。", "motivation": "通过提升小麦头部分割模型的准确性及最大化数据利用率以应对全球小麦全语义分割竞赛中的挑战。", "method": "构建了一个系统性的自学习框架，其中心模型为带有Mix Transformer（MiT-B4）骨干网络的SegFormer。该框架采用迭代教师-学生循环训练策略，并结合了两阶段混合训练和大量数据增强技术。", "result": "该方法在开发和测试阶段的数据集上均表现出色。", "conclusion": "所提出的方法通过伪标签优化和有效的模型训练策略，达到了优异的分割性能。"}}
{"id": "2512.11873", "pdf": "https://arxiv.org/pdf/2512.11873", "abs": "https://arxiv.org/abs/2512.11873", "authors": ["Antonia Yepes", "Marie Charbonneau"], "title": "Audio-Based Tactile Human-Robot Interaction Recognition", "categories": ["cs.RO"], "comment": "1 page, 1 figure, 1 table", "summary": "This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.", "AI": {"tldr": "研究利用机器人身上的麦克风通过声音识别触觉互动，提出一种替代传统关节扭矩传感器或六轴力/扭矩传感器的方法。", "motivation": "探索使用声学信号检测机器人触觉交互的新方法，以减少对复杂物理传感器的依赖，并提高系统的鲁棒性和成本效益。", "method": "在Pollene Robotics Reachy机器人的躯干上安装两个Adafruit I2S MEMS麦克风，通过捕捉不同触摸类型的音频信号（轻拍、敲击、摩擦等），并使用卷积神经网络进行触觉分类训练。数据集包含336个预处理样本。", "result": "模型展示了对具有明显声频主导频率的不同触摸类型的高度分类准确性。", "conclusion": "利用麦克风识别机器人上的触觉交互是可行的，该方法为低成本和高精度的人机互动提供了新的途径。"}}
{"id": "2512.11872", "pdf": "https://arxiv.org/pdf/2512.11872", "abs": "https://arxiv.org/abs/2512.11872", "authors": ["Mingwang Xu", "Jiahao Cui", "Feipeng Cai", "Hanlin Shang", "Zhihao Zhu", "Shan Luan", "Yifang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff", "AI": {"tldr": "WAM-Diff是一种基于掩码扩散的VLA框架，用于自动驾驶中的轨迹生成。", "motivation": "为了探索在自动驾驶中使用离散掩码扩散进行轨迹生成的有效性，作者提出了WAM-Diff，以提供一种替代自回归和基于扩散的策略的方法，并支持场景感知解码策略。", "method": "该方法利用了掩码扩散来迭代优化表示未来自身轨迹的离散序列，采用稀疏专家混合架构并结合在线强化学习进行训练。", "result": "WAM-Diff在NAVSIM-v1和NAVSIM-v2数据集上分别取得了91.0 PDMS和89.7 EPDMS的性能指标，表明了掩码扩散在自动驾驶中的有效性。", "conclusion": "该方法提供了针对轨迹生成的一种有前景的选择，并且支持场景感知解码策略。"}}
{"id": "2512.11871", "pdf": "https://arxiv.org/pdf/2512.11871", "abs": "https://arxiv.org/abs/2512.11871", "authors": ["Tekleab G. Gebremedhin", "Hailom S. Asegede", "Bruh W. Tesheme", "Tadesse B. Gebremichael", "Kalayu G. Redae"], "title": "Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops", "categories": ["cs.CV", "cs.AI"], "comment": "A preliminary version of this work was presented at the International Conference on Postwar Technology for Recovery and Sustainable Development (Feb. 2025). This manuscript substantially extends that work with expanded experiments and on-device deployment analysis. Code and dataset are publicly available at: https://github.com/Tekleab15/Automated_plant_disease_and_pest_detection_system", "summary": "Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.", "AI": {"tldr": "该论文提出了一种使用轻量级CNN和MobileViT模型的自动植物疾病和害虫检测系统，用于诊断土生作物。", "motivation": "在埃塞俄比亚提格雷地区的农业支持中，基础设施破坏限制了专家作物病害诊断的获取。因此开发一种离线优先的检测系统，以解决这一问题。", "method": "论文使用了一个专门为仙人掌无花果（Opuntia ficus-indica）定制的新颖数据集进行研究，并评估了三种移动高效架构：自定义轻量级CNN、EfficientNet-Lite1和混合型CNN-Transformer的MobileViT-XS。这些模型部署在支持完全离线推理的本地化Flutter应用中。", "result": "实验结果表明，EfficientNet-Lite1实现了90.7%的测试准确率，轻量级CNN达到了89.5%，而MobileViT-XS则获得了最高的97.3%交叉验证准确性。这证明了基于MHSA的全局推理比局部纹理CNN核更能有效地区分二维真菌病变中的害虫簇。", "conclusion": "研究结果展示了在边缘环境下的部署中，通过结合不同的模型架构和优化技术可以实现高精度、低延迟且资源友好的作物病害诊断系统。"}}
{"id": "2512.11870", "pdf": "https://arxiv.org/pdf/2512.11870", "abs": "https://arxiv.org/abs/2512.11870", "authors": ["Mulham Fawkherji", "Bruce Race", "Driss Benhaddou"], "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.", "AI": {"tldr": "研究使用社会经济指标、智能交通系统和城市模拟器来加速零排放车辆（ZEV）的采用并减少行驶里程（VMT）。", "motivation": "为了实现气候行动计划中的减排目标，特别是在低密度和依赖汽车的城市中，需要策略性地推动零排放车辆的普及，并通过改善公共交通来降低行驶里程。", "method": "建立了一个交通排放基准线模型，并利用智能停车、公交激励等措施进行政策评估；开发了一个基于Unity 3D的仿真环境以动态模拟城市交通并可视化政策情景。", "result": "提出了多种策略选项和潜在行动方案，为未来的城市减排提供了指标、度量标准和技术工具的支持。", "conclusion": "这些方法有助于解决社会经济差距问题，并支持城市向零排放目标迈进。"}}
{"id": "2512.11869", "pdf": "https://arxiv.org/pdf/2512.11869", "abs": "https://arxiv.org/abs/2512.11869", "authors": ["D. Shainu Suhas", "G. Rahul", "K. Muni"], "title": "Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.", "AI": {"tldr": "该论文提出了一种改进的三维车道检测框架Temporal-Anchor3DLane，通过引入多任务损失改进、轻量级时间LSTM融合模块和ESCOP式训练方法来提高单目三维车道检测性能。", "motivation": "传统的单目三维车道检测模型存在回归异常值敏感性高、全局曲线几何弱监督、多个损失项平衡困难以及时间连续性利用不足的问题。为了解决这些问题，作者提出了Temporal-Anchor3DLane框架。", "method": "该方法包括多任务损失改进（如Balanced L1回归、Chamfer点集距离和基于不确定性损失加权），轻量级的时间LSTM融合模块用于帧间特征聚合，并采用ESCOP式训练策略来增强曲线级监督与时间一致性。", "result": "在OpenLane数据集上，Temporal-Anchor3DLane框架显著提高了F1分数，达到了+6.2的提升，并展示了更加平滑的时间轨迹。结果表明，通过架构和损失函数的小改进可以极大地提高三维车道检测的鲁棒性。", "conclusion": "该研究证明了通过引入多任务损失、时间LSTM融合模块和ESCOP训练策略，可以在不增加额外传感器的情况下显著提升单目三维车道检测性能，且无需大规模调整模型结构。"}}
{"id": "2512.11868", "pdf": "https://arxiv.org/pdf/2512.11868", "abs": "https://arxiv.org/abs/2512.11868", "authors": ["Alexander Windmann", "Benedikt Stratmann", "Mariya Lyashenko", "Oliver Niggemann"], "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.", "AI": {"tldr": "介绍一种轻量级、任务无关的协议，用于记录和评估工业时间序列上AI模型的鲁棒性。", "motivation": "工业AI从业者面临含糊不清的鲁棒性要求以及缺乏可实施的具体协议。为了应对新兴法规和标准中的这些挑战，该论文提出了一个明确且实用的方法来满足这些需求。", "method": "引入了工业AI鲁棒性卡片（IARC），一种结合漂移监控、不确定性量化及压力测试并映射到相关欧盟AI法案义务的轻量级协议。通过软传感器案例研究展示了其如何支持可重复的鲁棒性证据和持续监测的能力。", "result": "通过一个生物制药发酵过程中的软传感器实例，验证了IARC能够提供可靠的鲁棒性评估方法，并支持工业应用中的持续监控。", "conclusion": "该论文提出了一个实用且有效的协议来满足工业AI中关于鲁棒性的具体需求，同时提供了可实施的案例研究证明其有效性。"}}
{"id": "2512.11867", "pdf": "https://arxiv.org/pdf/2512.11867", "abs": "https://arxiv.org/abs/2512.11867", "authors": ["Daniil Zverev", "A. Sophia Koepke", "Joao F. Henriques"], "title": "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "comment": "DAGM German Conference on Pattern Recognition, 2025", "summary": "The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.", "AI": {"tldr": "本文探讨了使用合成数据进行持续学习时的潜在风险，特别是生成性经验回放（GER）方法在重复训练中的失效。", "motivation": "作者关注到利用生成的数据来增强模型训练变得越来越普遍。然而，这种做法可能导致分布漂移和性能退化的问题。因此，本文旨在通过分析合成数据的影响来探究这些问题的根源。", "method": "文章采用统计方法揭示了合成数据如何引入偏差和方差，影响最大似然估计的有效性，并提供了实验证明流行的生成模型在重复训练过程中会失效。", "result": "研究发现，在持续学习中使用合成数据会导致模型性能下降以及状态空间对齐失败。具体而言，这些模型的潜在表示会退化，无法保持原有的一致性。", "conclusion": "这项研究表明，由于引入了显著的偏差和方差，重复训练生成的数据可能会损害模型在持续学习中的表现，这提出了对合成数据使用方法的重要警示。"}}
{"id": "2512.11865", "pdf": "https://arxiv.org/pdf/2512.11865", "abs": "https://arxiv.org/abs/2512.11865", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Myeongjun Kim", "Gun-Woo Kim"], "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted to MobieSec 2025 (poster session)", "summary": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.", "AI": {"tldr": "提出了一种基于OpenVLA-OFT框架的可解释对抗鲁棒视觉语言行为模型，用于智能农业中的机器人操作。", "motivation": "解决RGB摄像头感知系统在农用机器人中因光度扰动导致的故障问题，提高系统的抗攻击能力。", "method": "集成证据3模块检测光度扰动，并生成自然语言解释；基于OpenVLA-OFT框架构建模型以增强对抗鲁棒性和可解释性。", "result": "与基线相比，当前动作L1损失降低21.7%，下一步动作L1损失降低18.4%，显示了在对抗条件下更高的行为预测准确率和可解释性。", "conclusion": "所提出的模型提高了智能农业中视觉感知系统的鲁棒性和可解释性，在面临光度扰动时表现更佳。"}}
{"id": "2512.11864", "pdf": "https://arxiv.org/pdf/2512.11864", "abs": "https://arxiv.org/abs/2512.11864", "authors": ["Christoph Einspieler", "Matthias Horn", "Marie-Louise Lackner", "Patrick Malik", "Nysret Musliu", "Felix Winter"], "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars", "categories": ["cs.AI", "math.OC"], "comment": "18 pages, 4 figures", "summary": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.", "AI": {"tldr": "研究并提出了解决具有先序约束和基于日历的累积资源限制的平行机器调度问题的新方法。", "motivation": "现有的解决方案无法有效处理现代生产环境中复杂的前驱关系和基于日历的资源限制，因此需要开发新的自动化方法来解决这些问题。", "method": "提出了一个精确解法的约束建模方法以及构造启发式算法和自适应元启发式的本地搜索算法，用于解决大规模问题实例。", "result": "所提出的元启发式方法已部署并正在工业环境中使用。", "conclusion": "通过研究新的平行机器调度问题变体，并开发相应的解法技术，可以有效处理实际生产中的复杂约束和资源限制。"}}
{"id": "2512.11863", "pdf": "https://arxiv.org/pdf/2512.11863", "abs": "https://arxiv.org/abs/2512.11863", "authors": ["Julian Schön", "Lena Hoffmann", "Nikolas Becker"], "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence", "categories": ["cs.CY", "cs.AI"], "comment": "ef:(2025): Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence. Berlin: Gesellschaft für Informatik e.V.. pp. 1-76. Studie", "summary": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.", "AI": {"tldr": "报告描述了人工智能的系统性环境风险，提出了一个三级框架来分析这些风险，并通过专家访谈案例研究展示了该框架的应用。", "motivation": "尽管人工智能被看作是应对气候变化等社会挑战的关键工具，但它对环境的影响也在扩大。本报告旨在识别和概述由于AI整合到社会、经济和物理基础设施中而产生的系统性环境风险。", "method": "通过叙事文献综述提出了一个三级框架来操作化系统性风险分析，并在农业与生物多样性、石油天然气和废物管理等领域的专家访谈案例研究中展示了该框架的应用。", "result": "报告指出了由于人工智能整合到基础设施中的系统性环境风险，包括气候、生物多样性和水质等多个方面的负面影响。提出了一个三级框架来识别结构条件、风险放大机制和影响后果。", "conclusion": "尽管这些风险是新兴的且难以量化，但通过综合分析可以更好地理解并应对由人工智能整合所引起的潜在不可逆的社会生态系统冲击。"}}
{"id": "2512.11862", "pdf": "https://arxiv.org/pdf/2512.11862", "abs": "https://arxiv.org/abs/2512.11862", "authors": ["Jiahao You", "Ziye Jia", "Can Cui", "Chao Dong", "Qihui Wu", "Zhu Han"], "title": "Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.", "AI": {"tldr": "本文提出了一种基于拍卖和扩散增强的多智能体强化学习框架，用于低空智能网络中无人机任务卸载与轨迹规划。", "motivation": "为了应对由能量限制、随机任务到达以及异构计算资源带来的挑战，设计了一个时间依赖的整数非线性优化问题，并提出了一个分层学习框架以实现无人机轨迹规划和任务卸载决策的最佳联合。", "method": "在大时间尺度上采用了Vickrey-Clarke-Groves拍卖机制进行能量感知与激励相容的轨迹分配；在小时间尺度上，提出了一种扩散异构代理近端策略优化算法，该算法嵌入了潜在扩散模型到行为网络中。", "result": "广泛的模拟结果显示，所提出的框架在能源效率、任务成功率和收敛性能方面均优于基线方法。", "conclusion": "通过结合拍卖机制与增强学习技术，在低空智能网络的无人机任务卸载和轨迹规划问题上取得了显著成效。"}}
{"id": "2512.11860", "pdf": "https://arxiv.org/pdf/2512.11860", "abs": "https://arxiv.org/abs/2512.11860", "authors": ["Yuelian Li", "Andrew Rushing Hands"], "title": "An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.", "AI": {"tldr": "该论文提出了一种操作符一致的图神经网络（OCGNN-PINN），用于学习不规则网格上的扩散动力学。", "motivation": "经典的数值方法在规则网格上解决偏微分方程效率高，但在不规则域中变得不稳定。实践中，诸如扩散、损伤和修复等多物理相互作用通常发生在不规则网格上。因此开发了一种基于图神经网络的方法来更稳定地预测这些问题。", "method": "提出了一个操作符一致的图神经网络（OCGNN-PINN），结合节点边消息传递及通过图邻接矩阵一致性损失，确保离散节点和边动力学在时间展开过程中保持结构耦合。", "result": "该模型在物理驱动演化的网格以及真实扫描表面上进行扩散过程评估时，在时间和预测精度方面优于图卷积和多层感知器基准方法，并接近不规则域中的Crank-Nicolson求解器性能。", "conclusion": "所提出的方法能够在复杂的、非结构化网格上实现更稳定的动力学模拟，提高了时空上的准确性。"}}
{"id": "2512.11859", "pdf": "https://arxiv.org/pdf/2512.11859", "abs": "https://arxiv.org/abs/2512.11859", "authors": ["Michael Chertkov"], "title": "Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "eess.SY", "stat.ML"], "comment": "40 pages, 8 figures", "summary": "We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching. We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.", "AI": {"tldr": "介绍了一种用于指导随机最优传输的线性可解框架GH-PID，该框架在保持分析结构的同时塑造轨迹集合。", "motivation": "通过设计低维引导协议，在确保终端分布匹配的情况下实现稳定采样和路径成本优化。", "method": "开发了基于路径成本、中心线适应度等指标的诊断工具，并展示了三种导航场景：手工艺品协议揭示几何形状的影响，单任务协议学习中路径积分扩散方法的应用，以及多专家融合中的共识协议学习。", "result": "GH-PID生成符合预定终端分布且系统性降低综合成本的轨迹。", "conclusion": "通过GH-PID框架，在各种设置下实现了几何感知、信任感知的轨迹优化。"}}
{"id": "2512.11858", "pdf": "https://arxiv.org/pdf/2512.11858", "abs": "https://arxiv.org/abs/2512.11858", "authors": ["Michael Chertkov", "Hamidreza Behjoo"], "title": "Adaptive Path Integral Diffusion: AdaPID", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "eess.SY", "stat.ML"], "comment": "51 pages, 17 figures", "summary": "Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.", "AI": {"tldr": "本文提出了一种自适应路径积分扩散方法，通过选择中间时间动态的调度来改进扩散采样器。", "motivation": "传统的扩散采样器虽然能够匹配目标分布的终态，但其真正价值在于选择控制中间时间动态行为的调度策略。为此，提出了具有时变刚度的调和PID路径选择框架。", "method": "采用分段常数参数化和简单的层次细化来开发自适应路径积分扩散算法，并引入了基于质量的服务诊断工具来评估采样效果。", "result": "实验表明，在固定集成预算下，使用QoS驱动的分段常数调度可以提高早期退出精度、尾部准确性和动态条件性。", "conclusion": "提出的自适应路径选择框架和质量服务指标能够有效提升扩散模型的效果。"}}
{"id": "2512.11857", "pdf": "https://arxiv.org/pdf/2512.11857", "abs": "https://arxiv.org/abs/2512.11857", "authors": ["Olivia Kim"], "title": "TopicProphet: Prophesies on Temporal Topic Trends and Stocks", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.", "AI": {"tldr": "提出TopicProphet框架，通过主题建模、时间分析等方法预测股票趋势。", "motivation": "解决传统定量数据难以捕捉因果逻辑及市场快速变化导致的数据不足问题，提高股票预测准确性。", "method": "使用序列化的主题建模、时间分析、断点检测和分段优化来识别最佳训练时期，从而提供更细致的历史经济模式。", "result": "通过广泛的实证研究证明，TopicProphet框架在捕捉最优训练数据以进行金融百分比变化预测方面优于现有方法。", "conclusion": "提出并验证了基于主题的分析模型可有效提升股票趋势预测精度。"}}
{"id": "2512.11856", "pdf": "https://arxiv.org/pdf/2512.11856", "abs": "https://arxiv.org/abs/2512.11856", "authors": ["Ao Zhou", "Jianlei Yang", "Tong Qiao", "Yingjie Qi", "Zhi Yang", "Weisheng Zhao", "Chunming Hu"], "title": "GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search", "categories": ["cs.LG", "cs.AI"], "comment": "accepted by IEEE Transactions on Computers", "summary": "Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.", "AI": {"tldr": "GCoDE提出了一种自动框架，用于图形神经网络的架构映射联合设计和部署。", "motivation": "由于高计算成本和边缘设备硬件资源有限，使得图形神经网络在边缘设备上的推理难以满足实时性和能量要求。传统的模型分割方法对于图神经网络并不有效。", "method": "GCoDE通过将设备通信过程抽象为一个显式操作，融合架构与映射方案在一个统一的设计空间中进行联合优化，并引入了能耗预测方法以提高能耗评估准确性，使用基于约束的随机搜索策略寻找最优解。", "result": "实验结果表明，在各种应用和系统配置下，GCoDE相比现有方法最多可实现44.9倍的速度提升以及98.2%的能量降低。", "conclusion": "GCoDE通过联合优化架构与映射方案，并引入能耗预测机制和高效的搜索策略，实现了图神经网络在边缘设备上的高效推理。"}}
{"id": "2512.11855", "pdf": "https://arxiv.org/pdf/2512.11855", "abs": "https://arxiv.org/abs/2512.11855", "authors": ["Behrooz Tahmasebi", "Melanie Weber"], "title": "Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry", "categories": ["cs.LG", "cs.AI"], "comment": "32 pages, 2 figures", "summary": "Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.", "AI": {"tldr": "研究了在机器学习模型中实现精确对称性和近似对称性的成本差异。", "motivation": "尽管实证证据表明，依赖近似对称性可以提供更大的灵活性和鲁棒性，但缺乏直接比较精确与近似对称性的理论理解。", "method": "引入了平均复杂度框架来量化实现对称性的成本，并通过标准条件下的分析得出了指数分离的结果。", "result": "在给定条件下，实现精确对称性需要线性平均复杂度，而近似对称性只需对数级的平均复杂度即可。", "conclusion": "证明了精确对称性和近似对称性的成本存在显著差异，并为实践中优先考虑近似对称性提供了理论依据。"}}
{"id": "2512.11854", "pdf": "https://arxiv.org/pdf/2512.11854", "abs": "https://arxiv.org/abs/2512.11854", "authors": ["Grant King", "Musa Azeem", "Savannah Noblitt", "Ramtin Zand", "Homayoun Valafar"], "title": "Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "24th International Conference on Machine Learning and Applications", "summary": "Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.", "AI": {"tldr": "本文提出了一种使用腕部IMU传感器和边缘神经网络实时反馈接近极限状态的系统，以优化阻力训练。", "motivation": "主观评估重复次数储备（RiR）不可靠，导致训练刺激不足或过度疲劳。该研究旨在通过客观手段实现对近极限状态的有效管理。", "method": "论文提出了一种两阶段管道：第一阶段使用ResNet模型实时分割重复动作；第二阶段利用从分割中提取的特征、卷积直接特征和LSTM捕捉的历史背景，由分类器识别对应于接近极限状态的动作窗口。实验在新收集的数据集上进行。", "result": "分割模型取得了0.83的F1得分，近极限分类器获得了0.82的F1得分，部署到边缘设备上的平均推断延迟分别为112毫秒和23.5毫秒。", "conclusion": "论文展示了使用最小硬件实现客观、实时训练强度反馈的可能性，为开发可访问的人工智能驱动肌肉增长教练工具铺平了道路。"}}
{"id": "2512.11853", "pdf": "https://arxiv.org/pdf/2512.11853", "abs": "https://arxiv.org/abs/2512.11853", "authors": ["Mitchell Marfinetz"], "title": "Evolving Deep Learning Optimizers", "categories": ["cs.NE", "cs.LG"], "comment": ":I.2.6", "summary": "We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($β_1$=0.86, $β_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at https://github.com/mmarfinetz/evo-optimizer.", "AI": {"tldr": "本文提出了一种基于遗传算法的框架，用于自动发现深度学习优化器。", "motivation": "通过自动化地搜索优化器可以发掘出性能更好的方案，从而提升模型训练的效果和效率。", "method": "利用遗传算法在50代种群中搜索最优解，每个个体代表一个可能的有效优化策略组合。这些策略包括梯度更新、动量、RMS归一化、自适应项及符号基元更新等。", "result": "所发现的进化优化器在CIFAR-10数据集上比Adam优化器提高了7.7%的成绩，并且总体性能提升了2.6%。该优化器采用符号梯度项与自适应动量估算结合的方式，使用了不同于Adam的标准偏差校正。", "conclusion": "进化搜索能够发现竞争力强的优化算法，并揭示出一些区别于人工设计优化器的设计原则。"}}
{"id": "2512.11852", "pdf": "https://arxiv.org/pdf/2512.11852", "abs": "https://arxiv.org/abs/2512.11852", "authors": ["Muhammad Jawad Bashir", "Shagufta Henna", "Eoghan Furey"], "title": "Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages, Accepted in 36th Irish Signals and Systems Conference, ISSC 2025", "summary": "The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.", "AI": {"tldr": "该论文利用时间融合变换器(TFT)模型为智能温室中的执行器设置提供自动化控制，以实现更高效的农业管理。", "motivation": "现有的时间序列预测模型在IoRT环境下的智能温室中缺乏可解释性机制，限制了其在信任、透明度和监管合规方面的重要性。因此，该研究旨在通过增强TFT模型的解释能力来提高决策过程的信任与透明度。", "method": "论文采用时间融合变换器(TFT)模型结合局部和全局解释技术（如LIME和SHAP），以提供传感器数据如何影响温室控制决策的具体信息。", "result": "训练后的TFT模型在智能温室的执行器设置上实现了95%的测试准确率，展示了不同传感器对实时温室调整的不同影响力。", "conclusion": "该研究通过增强时间融合变换器(TFT)模型的解释能力，成功提高了智能温室控制系统决策过程中的透明度和信任水平，同时提升了作物产量和资源效率。"}}
{"id": "2512.11851", "pdf": "https://arxiv.org/pdf/2512.11851", "abs": "https://arxiv.org/abs/2512.11851", "authors": ["Prashant Pandey"], "title": "KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.", "AI": {"tldr": "利用缓存机制回收LLM中的KV状态，以加速相似提示的推理并扩展其上下文容量。", "motivation": "通过重用先前计算的注意力键值(KV)状态来提高小型语言模型(LLM)在处理相似提示时的速度和效率。", "method": "建立一个缓存机制存储过去的激活状态，并使用句子嵌入检索与新输入相匹配的前缀。如果新的输入是已缓存提示的精确前缀，则重用该KV状态，否则行为保持基准模式。", "result": "实验观察到，在存在前缀重叠的情况下，回收策略可以提供一致的速度提升且不会显著影响输出语义；在无前缀重叠时，性能与基准测试相当。", "conclusion": "通过缓存机制的KV状态回收技术可以在不破坏模型的前提下提高LLM处理相似提示的效率和速度。"}}
{"id": "2512.11849", "pdf": "https://arxiv.org/pdf/2512.11849", "abs": "https://arxiv.org/abs/2512.11849", "authors": ["Nimol Thuon", "Jun Du"], "title": "KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ef:2025 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)", "summary": "Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \\textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.", "AI": {"tldr": "构建了一个针对柬埔寨语商业文档的多层次布局分析数据集KH-FUNSD，以解决非拉丁文低资源语言文档AI工具缺乏的问题。", "motivation": "为了填补低资源、非拉丁文字文档AI工具开发中的空白，特别是针对日常生活中使用的1700多万柬埔寨人的商业文件处理问题，构建了专门的Khmer文档理解数据集KH-FUNSD。", "method": "设计了一个三层次注释框架：区域检测将每个文档分为核心部分如头部、表格字段和尾部；类似FUNSD的标注区分问题、答案、标题和其他关键实体及其关系；细粒度分类为特定语义角色分配标签，包括字段标签、值、标题、脚注等。", "result": "基准测试了几种领先的模型，并提供了针对柬埔寨商业文档的第一套基线结果，指出了非拉丁文低资源语言在布局分析中的独特挑战。", "conclusion": "KH-FUNSD数据集首次为Khmer商业文档提供了全面的布局分析和精确的信息提取支持，促进了相关技术的发展。"}}
{"id": "2512.11845", "pdf": "https://arxiv.org/pdf/2512.11845", "abs": "https://arxiv.org/abs/2512.11845", "authors": ["Wenbo Du", "Lingling Han", "Ying Xiong", "Ling Zhang", "Biyue Li", "Yisheng Lv", "Tong Guo"], "title": "Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 10 figures", "summary": "Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.", "AI": {"tldr": "通过可变形时频变换器（DTSFormer）预测机场客流量", "motivation": "现有方法依赖固定大小的补丁嵌入，难以捕捉复杂的异质模式。提出了一种新的模型来解决这一问题。", "method": "引入了多尺度可变分隔模块和联合时频过滤模块。输入序列通过基于窗口函数的掩码动态分割为不同时间阶段的多个规模的时间补丁，并在每个规模内设计频率域注意机制来捕捉高频和低频成分，最后融合时间领域的多频特性。", "result": "实验结果表明，提出的方法优于现有最先进的预测模型，在不同的预测时间段表现出色", "conclusion": "DTSFormer通过可变分隔模块使补丁长度与主导周期和异质趋势对齐，更好地捕捉突发高频波动"}}
{"id": "2512.11844", "pdf": "https://arxiv.org/pdf/2512.11844", "abs": "https://arxiv.org/abs/2512.11844", "authors": ["Haoyang Shang", "Zhengyang Yan", "Xuan Liu"], "title": "Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines", "categories": ["cs.HC", "cs.CL", "cs.LG"], "comment": "NeurIPS 2025 Workshop: First Workshop on LLM Persona Modeling (Oral)", "summary": "We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.", "AI": {"tldr": "通过LLM作为文本引擎来模拟互动，并基于互动结果评估情侣间的兼容性。", "motivation": "提出一种新的计算匹配方法，以解决传统静态文件对比的不足，旨在通过模拟互动行为更准确地预测人类关系中的化学反应和长期稳定。", "method": "使用LLMs充当代理角色进行互动仿真，并将其作为环境来建模互动动态。将兼容性评估转化为奖励模型问题，从仿真中提取信号以预测人的偏好。", "result": "在快速约会数据上验证了短期的化学效应，并通过离婚概率预测长期稳定性；理论上证明随着政策更好地模拟人类行为，匹配趋向最优稳定。", "conclusion": "这种方法提供了一个互动、个性化的人际关系兼容性评估系统，用户可以迭代优化代理以揭示未来互动的可能性。"}}
{"id": "2512.11843", "pdf": "https://arxiv.org/pdf/2512.11843", "abs": "https://arxiv.org/abs/2512.11843", "authors": ["Eugene Izhikevich"], "title": "Spiking Manifesto", "categories": ["cs.NE", "cs.AI", "cs.AR", "cs.LG"], "comment": "This is a declaration of principles and roadmap for spiking networks, intended as a manifesto rather than a conventional research article", "summary": "Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN", "AI": {"tldr": "该论文提出了一种将流行的人工智能模型转换为脉冲神经网络架构的方法，以实现千倍的效率提升。", "motivation": "当前人工智能模型（如人工神经网络）比大脑低一千倍的能量效率，并且需要使用GPU进行大规模矩阵乘法。相比之下，大脑中的脉冲神经网络不涉及矩阵乘法并且能量需求更小。", "method": "该论文提出一个框架，将流行的人工智能模型以脉冲网络和多时程化的方式理解，并解释了脉冲活动是实现查找表的自然方式。", "result": "提供了一种将人工智能模型转换为新型架构的方法，具有千倍效率提升的潜力。", "conclusion": "通过这种方式，可以有效地提高计算系统的能量使用效率。"}}
{"id": "2512.11842", "pdf": "https://arxiv.org/pdf/2512.11842", "abs": "https://arxiv.org/abs/2512.11842", "authors": ["Trevor McClain", "Rahul Bhadani"], "title": "Car-following Models and Congestion Control with Followerstopper on a Ring-Road under Known Delay -- Examining Limit Cycle", "categories": ["eess.SY", "cs.RO", "eess.SP"], "comment": "Submitted to IV 2026", "summary": "This paper examines the IDM microscopic car-following model from a dynamical systems perspective, analyzing the effects of delay on congestion formation. Further, a case of mixed-autonomy is considered by controlling one car with Followerstopper in a ring road setting containing IDM vehicles as human drivers. Specifically, the stop-and-go waves phenomenon in idealized traffic from a dynamical systems perspective is examined. We show that Followerstopper-controlled vehicle is effective at eliminating emergent stop-and-go waves in the IDM traffic simulation. We show through simulation that the uniform flow manifold is unstable for the ring road simulation with IDM vehicles, and that replacing a single car with Followerstopper induces stability, allowing the cars to drive safely at a uniform speed. Additionally, the case of known delay is considered in a mixed-autonomy scenario. Our simulation result shows that while considering a known time delay, traffic waves emerge earlier than in the no-delay case. At the same time, a single-vehicle controlled using Followerstopper controller is able to prevent the emergence of traffic waves even in the presence of delay.", "AI": {"tldr": "本文通过动力学系统视角研究了IDM微观汽车跟随模型在环路中考虑延迟时的交通拥堵形成情况，并探讨了一种由Followerstopper控制车辆减少交通波浪的方法。", "motivation": "旨在了解和解决延迟对交通流稳定性的影响，特别是在混合自主性情况下如何通过引入一个受控车辆来抑制突发性的停走波动现象。", "method": "利用IDM模型在环路设置中进行仿真研究，并引入Followerstopper控制器控制一辆汽车以观察其对于整体交通流量的稳定效果。考虑了延迟情况下的混合作用。", "result": "发现IDM车辆形成的均匀流态不稳定，但通过引入一个由Followerstopper控制的车辆后可以诱导出稳定性，防止停走波浪现象的发生；即使存在已知延时的情况下也能有效抑制交通波动。", "conclusion": "研究表明，在混合自主性环境中使用Followerstopper控制器能有效地减少甚至消除交通拥堵中的停走波浪现象，并且即便在有延迟的情况下也表现出良好的控制效果。"}}
{"id": "2512.11837", "pdf": "https://arxiv.org/pdf/2512.11837", "abs": "https://arxiv.org/abs/2512.11837", "authors": ["Mahmut S. Gokmen", "Mitchell A. Klusty", "Evan W. Damron", "W. Vaiden Logan", "Aaron D. Mullen", "Caroline N. Leach", "Emily B. Collier", "Samuel E. Armstrong", "V. K. Cody Bumgardner"], "title": "Vision Foundry: A System for Training Foundational Vision AI Models", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.LG"], "comment": "10 pages, 4 figures, 3 tables, submitted to AMIA 2026 Informatics Summit", "summary": "Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.", "AI": {"tldr": "Vision Foundry是一个无代码、符合HIPAA标准的平台，用于训练基础视觉AI模型。", "motivation": "自监督学习利用庞大的未标注医学数据集，但技术壁垒限制了临床研究人员的应用。为了解决这个问题，本文提出了一种民主化预训练、适配和部署基础视觉模型的方法。", "method": "该系统整合DINO-MX框架，并采用如放大感知蒸馏（MAD）和参数高效微调（PEFT）等特殊策略，从而简化分布式基础设施的复杂性。", "result": "实验表明，通过Vision Foundry训练出的模型在分割保真度和回归准确性方面显著优于通用基线，在不同的成像协议中也表现出强大的零样本泛化能力。", "conclusion": "Vision Foundry为领域专家提供了开发临床AI工具的能力，并且只需要少量标注数据。它简化了从工程优化到临床发现的转变，推动了高级表示学习在实际应用中的发展"}}
{"id": "2512.11836", "pdf": "https://arxiv.org/pdf/2512.11836", "abs": "https://arxiv.org/abs/2512.11836", "authors": ["Dayne R. Freudenberg", "Daniel G. Haughian", "Mitchell A. Klusty", "Caroline N. Leach", "W. Scott Black", "Leslie N. Woltenberg", "Rowan Hallock", "Elizabeth Solie", "Emily B. Collier", "Samuel E. Armstrong", "V. K. Cody Bumgardner"], "title": "Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 4 figures, 6 tables, submitted to AMIA 2026 Informatics Summit", "summary": "Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.", "AI": {"tldr": "论文提出了一个从食物描述文本中预测Food Compass Score 2.0的机器学习管道。", "motivation": "准确的营养评估对公共健康至关重要，但现有的档案系统需要详细的数据，这些数据通常无法从食品的口语化描述中获得或访问。", "method": "该方法利用多头神经网络处理混合特征向量，结合语义文本嵌入、词汇模式和领域启发法，并与USDA Food and Nutrient Database for Dietary Studies (FNDDS) 数据相结合。这种方法估计出FCS算法所需的营养成分和食品组成部分。", "result": "系统表现出强大的预测能力，对单个营养素的R^2中位数为0.81；预测的FCS值与发布的数值具有强相关性（Pearson's r = 0.77），平均绝对差异为14.0分。尽管错误最大的是模棱两可或加工过的食品，但该方法将语言转化为可操作的营养信息。", "conclusion": "这种方法使大规模饮食评估成为可能，适用于消费者应用和研究"}}
{"id": "2512.11835", "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.", "AI": {"tldr": "该论文提出了基于莫道架构的语句框架，用于大语言模型的人工年龄评分（AAS）的设计和实现。", "motivation": "为了使大型语言模型在内部记忆和\"自我类似\"行为方面具有原则性和可审计性，引入了一种工程化的、以条款为基础的结构。", "method": "通过将20个莫道从莱布尼茨的Monadology中分组为六个捆绑包，并将其实现为AAS核心上的可执行规范。这些条款家族在Python中实现了六次最小化实验。", "result": "实验证明，该条款系统表现出有界且可解释的行为：AAS轨迹保持连续和速率限制，矛盾和未支持的声明触发明确的惩罚。", "conclusion": "基于莫道的条款框架使用AAS作为骨干，并提供了一个透明、代码级别的蓝图来约束和分析人工代理的内部动态。"}}
{"id": "2512.11833", "pdf": "https://arxiv.org/pdf/2512.11833", "abs": "https://arxiv.org/abs/2512.11833", "authors": ["Reuben R Shamir"], "title": "Soft Decision Tree classifier: explainable and extendable PyTorch implementation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Keywords: Soft Decision Tree, Short-term Memory Soft Decision Tree, Classification, Explainability", "summary": "We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results. The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree", "AI": {"tldr": "本文实现了Soft Decision Tree (SDT) 和 Short-term Memory Soft Decision Tree (SM-SDT)，并在模拟和临床数据集上进行了测试。", "motivation": "为了开发一种可解释性强且扩展性好的分类器，作者使用PyTorch实现了SDT，并通过与传统方法的比较验证其有效性。", "method": "本文采用了Soft Decision Tree (SDT) 和 Short-term Memory Soft Decision Tree (SM-SDT)，并在模拟和临床数据集上进行了测试。这些模型基于PyTorch实现。", "result": "实验结果表明，SDT、SM-SDT 和 XGBoost 在AUC值方面表现相似，并优于随机森林、逻辑回归和决策树等方法。", "conclusion": "本文提出的Soft Decision Tree (SDT) 和 Short-term Memory Soft Decision Tree (SM-SDT) 方法在多种数据集上具有良好的分类性能，且具备较好的可解释性。"}}
{"id": "2512.11832", "pdf": "https://arxiv.org/pdf/2512.11832", "abs": "https://arxiv.org/abs/2512.11832", "authors": ["Jakub Walczak"], "title": "Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \\pm 1.93$), MAE ($1.32 \\pm 0.77$), and $Δ_{MAX}$ ($24.06 \\pm 17.15$), as well as the highest $R^2$ ($0.68 \\pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]", "AI": {"tldr": "该论文评估了三种重建稀疏气候数据的方法，比较其性能和效率。", "motivation": "优化后的简单反距离加权（IDW）方法在重建准确性和计算效率方面优于普通克里金法和隐式神经表示模型，因此研究动机在于验证这种假设。", "method": "使用逆距离加权（IDW）、普通克里金法（OK）以及基于MMGN架构的隐式神经表示模型，并通过超参数调优进行优化。", "result": "经过全面实验与统计分析后发现，IDW方法在重建精度和计算效率上表现出色，其RMSE、MAE等指标均优于其他两种方法，且差异具有显著性。", "conclusion": "该研究结论是逆距离加权（IDW）方法在重建稀疏气候数据时表现最优。"}}
{"id": "2512.11831", "pdf": "https://arxiv.org/pdf/2512.11831", "abs": "https://arxiv.org/abs/2512.11831", "authors": ["Haitao Lin", "Peiyan Hu", "Minsi Ren", "Zhifeng Gao", "Zhi-Ming Ma", "Guolin ke", "Tailin Wu", "Stan Z. Li"], "title": "On the Design of One-step Diffusion via Shortcutting Flow Paths", "categories": ["cs.LG", "cs.CV"], "comment": "10 pages of main body, conference paper", "summary": "Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.", "AI": {"tldr": "提出了一种设计框架，用于简化和优化一步扩散模型的设计。", "motivation": "现有的一步扩散模型理论推导与实际实现紧密相连，阻碍了改进。该研究旨在提供一种理论基础明确且便于改进的设计框架。", "method": "提出了一个通用的框架来拆解现有的一步模型的具体组件选择，并通过系统性地优化这些组件实现了新的性能提升。", "result": "在ImageNet-256x256数据集上，所提出的一步模型达到了FID50k为2.85的新纪录，且无需预训练、蒸馏或课程学习等步骤。", "conclusion": "这项工作简化了一步扩散模型的设计过程，并降低了组件级创新的门槛。"}}
{"id": "2512.11830", "pdf": "https://arxiv.org/pdf/2512.11830", "abs": "https://arxiv.org/abs/2512.11830", "authors": ["Satyam Kumar"], "title": "CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 5 figures, 1 table", "summary": "Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.", "AI": {"tldr": "CR3G是一种用于胸部X光报告生成的因果推理框架，旨在改善AI生成报告中的因果关系理解和解释能力。", "motivation": "现有的AI模型在医学图像中寻找模式方面表现良好，但难以理解这些模式与患者状况之间的深层因果关系。本文希望通过探索CR3G框架来提高诊断质量并使其更具临床实用性。", "method": "通过应用因果推理的方法于胸部X光分析中，生成以患者为中心的解释报告，并增强AI驱动诊断的质量和可信度。", "result": "该方法在5种异常情况中有2种表现出更好的因果关系理解和解释能力。", "conclusion": "CR3G框架提高了AI生成报告中的因果关系理解及解释质量，有助于提升临床实践中的诊断准确性。"}}
{"id": "2512.11829", "pdf": "https://arxiv.org/pdf/2512.11829", "abs": "https://arxiv.org/abs/2512.11829", "authors": ["Jacob Poschl"], "title": "Active Inference with Reusable State-Dependent Value Profiles", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages", "summary": "Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.", "AI": {"tldr": "本文提出了一种使用可复用价值轮廓来解决适应性行为问题的方法，通过在生成模型中分配一组价值相关的参数给隐藏状态，并根据后验信念动态混合这些参数以实现情境条件策略的招募。", "motivation": "在多变环境中，需要代理在潜在状态下切换价值控制模式；然而为每个情景维护独立的价值偏好、策略偏向和动作信心参数是不可行的。因此引入了可复用的价值轮廓来解决此问题。", "method": "提出了一种基于生成模型的方法，其中通过将一组价值相关的参数（结果偏好、政策先验和策略精度）分配给隐藏状态，并根据试验后信念动态混合这些参数以适应不同情境下的策略招募。", "result": "该方法在概率逆转学习任务中表现出色，与静态精度、熵耦合的动态精度模型相比，在交叉验证对数似然度和信息标准方面有显著优势。此外还支持从嘈杂观测值推断上下文时参数恢复分析的有效性。", "conclusion": "可复用价值轮廓提供了一种计算上可行的方法来解释在多变环境中基于信念的价值控制，并得出测试行为灵活性的标志性结果，表明主要通过调节策略先验而非策略精度驱动适应性控制。"}}
{"id": "2512.11827", "pdf": "https://arxiv.org/pdf/2512.11827", "abs": "https://arxiv.org/abs/2512.11827", "authors": ["Milad Malekzadeh", "Magdalena Biernacka", "Elias Willberg", "Jussi Torkko", "Edyta Łaszkiewicz", "Tuuli Toivonen"], "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?", "categories": ["cs.CY", "cs.AI", "cs.CV"], "comment": null, "summary": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.", "AI": {"tldr": "研究使用ChatGPT、Claude和Gemini等AI模型来评估绿地吸引力，与波兰罗兹居民的调查结果进行比较。", "motivation": "传统方法难以有效评估绿地吸引力，特别是在资源密集度高且主观感知难以捕捉的情况下。该研究旨在探索大型多模态语言模型是否能准确反映人类对绿地吸引力的看法。", "method": "通过Google街景图像和地理问卷收集的数据来评价不同类型的绿地空间（正式与非正式），比较AI模型的评估结果与居民调查数据的一致性，并分析两者解释差异的原因。", "result": "对于有吸引力的正式绿地，以及不吸引人的非正式绿地，AI模型的人类一致性较高；但对于吸引人的非正式和不吸引人的正式绿地，一致性较低。模型更重视美学设计特性，而忽略了安全、基础设施及地方特色等人类看重的因素。", "conclusion": "大型多模态语言模型可以支持绿地评估的预评价阶段，但仍需要人类监督和补充参与式方法来确保全面准确地进行环境规划实践中的绿地评价。"}}
{"id": "2512.11826", "pdf": "https://arxiv.org/pdf/2512.11826", "abs": "https://arxiv.org/abs/2512.11826", "authors": ["Weihong Xu", "Chang Eun Song", "Haichao Yang", "Leo Liu", "Meng-Fan Chang", "Carlos H. Diaz", "Tajana Rosing", "Mingu Kang"], "title": "FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing", "categories": ["cs.AR", "eess.IV"], "comment": null, "summary": "This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.", "AI": {"tldr": "介绍了一种名为FSL-HDnn的能量高效加速器，用于在设备上实现特征提取和少量样本学习的端到端管道。", "motivation": "为了应对资源受限边缘应用中的在线设备学习（ODL）的基本挑战，该论文提出了一种能量高效的加速器FSL-HDnn。", "method": "通过采用参数高效特征提取器和基于高维计算的少量样本分类器来实现这一目标。特征提取器利用权重聚类机制减少计算复杂性，而HDC-based FSL分类器则消除了梯度反向传播操作，实现了单次训练。", "result": "该芯片在10类5个示例的小样本学习任务上实现了6 mJ/图像的优秀训练能量效率和每秒28个图像的端到端训练吞吐量。与最新的ODL芯片相比，端到端训练延迟减少了2至20.9倍。", "conclusion": "FSL-HDnn加速器通过减少计算复杂度并利用优化策略实现了低延迟设备学习和推断，并在实验中展示了其优越的性能。"}}
{"id": "2512.11824", "pdf": "https://arxiv.org/pdf/2512.11824", "abs": "https://arxiv.org/abs/2512.11824", "authors": ["Rosh Ho", "Jian Zhang"], "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.", "AI": {"tldr": "将低成本商用气动康复手套转化为视觉引导辅助矫形器", "motivation": "慢性上肢损伤影响了全球数以百万计的人，现有的辅助技术成本过高或依赖不稳定的生物信号。该系统旨在通过腕部安装的摄像头和边缘计算推理引擎来实现无需稳定肌肉信号的情境感知抓取。", "method": "平台集成了腕部安装相机与边缘计算推理引擎（Raspberry Pi 5），利用实时YOLO基础计算机视觉模型，实现了96.73%的抓取分类准确率和小于40毫秒的端到端延迟。物理验证使用标准化基准测试显示在YCB对象操作中达到82.71%的成功率，并且在ADL任务中表现出可靠的性能。", "result": "系统在YCB物体操纵中达到了82.71%的成功率，在ADL任务上表现出可靠的表现，整体成本低于250美元。该平台由商用组件组成，为可访问的、基于视觉的上肢辅助提供了一个技术基础。", "conclusion": "ReGlove为被传统EMG控制设备排除在外的人群提供了可负担得起且有效的上肢辅助解决方案，具有广阔的潜在应用前景"}}
{"id": "2512.11818", "pdf": "https://arxiv.org/pdf/2512.11818", "abs": "https://arxiv.org/abs/2512.11818", "authors": ["Izabela Lipinska", "Hugh Brosnahan"], "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "18 pages excluding appendices", "summary": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.", "AI": {"tldr": "本文探讨了大型语言模型与用户互动时可能引发的双重幻觉现象，以及这种现象如何导致用户的心理障碍。", "motivation": "文章旨在通过借鉴双套理论、临床文献和半球理论来解释大语言模型与人类交互中出现的心理问题，并提出设计原则以减少这些问题的发生。", "method": "基于Bateson的双套理论、精神病学研究中的共病现象以及McGilchrist的大脑半球理论，文章分析了AI交流如何引发用户心理上的矛盾和幻觉。", "result": "结果表明，在情感需求或不稳定的情况下，这种互动模式可能导致用户将主观意图归因于实际上缺乏此属性的系统。", "conclusion": "提出'本体论诚实'作为设计原则以减少由技术介导产生的共病现象。"}}
{"id": "2512.11817", "pdf": "https://arxiv.org/pdf/2512.11817", "abs": "https://arxiv.org/abs/2512.11817", "authors": ["Juan Palomeque-Gonzalez"], "title": "A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images", "categories": ["cs.CY", "cs.CV"], "comment": "12 Pages, 5 figures", "summary": "This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.", "AI": {"tldr": "本文提出了一种可重复的工作流程，用于将遗产考古图像集转换为结构化和适合分割的数据集。", "motivation": "针对一个包含数千张标准照片但缺乏批量下载或自动化处理机制的考古数据集，开发了两种开源工具：一种网络爬虫脚本和一种图片处理流水线。这些工具旨在尊重ADS条款并遵守道德爬取规范的同时，提取元数据、下载图像，并生成适合机器学习的数据。", "method": "文章采用了一种包括网络爬虫脚本和图像处理管道的方法。前者从记录页面中抓取信息，并遵循伦理原则下载图片；后者使用UUID重命名文件，通过经典计算机视觉技术生成二值掩模和边界框，并将所有衍生信息存储在与COCO兼容的Json文件中。", "result": "该方法使得考古图像集能够转换为机器学习友好的格式，促进了下游分析并有助于数字考古学中的可重复研究实践。", "conclusion": "通过这种方式，作者成功地实现了从网络上的考古图像集合到适合于机器学习处理的数据形式的转变，增强了数据管理和后续的研究能力。"}}
{"id": "2512.11814", "pdf": "https://arxiv.org/pdf/2512.11814", "abs": "https://arxiv.org/abs/2512.11814", "authors": ["Hugh Brosnahan"], "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.", "AI": {"tldr": "探讨AI记录员在医疗实践中如何重塑医学关注模式，而非仅仅提高效率。", "motivation": "分析人工智能（AI）记录员如何通过外部化人类思考和技能来改变医疗服务的本质，并讨论其潜在的社会影响。", "method": "运用伊恩·麦吉尔克里斯特的半球理论和刘易斯·芒福德的技术哲学，探讨技术对注意力模式的影响。", "result": "指出AI记录员推广了一种以量化的、程序化的左脑思维方式为主导的关注方式，这种关注方式会削弱临床专业知识，并可能导致医疗实践领域狭窄化。", "conclusion": "在医学实践中进一步嵌入这种量化和机械化的方法可能会侵蚀人际关系和直觉判断，在未来减少医生的专业自由度。"}}
{"id": "2512.11811", "pdf": "https://arxiv.org/pdf/2512.11811", "abs": "https://arxiv.org/abs/2512.11811", "authors": ["Fengyi Xu", "Jun Ma", "Waishan Qiu", "Cui Guo"], "title": "Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.", "AI": {"tldr": "提出了一种名为VPR-AttLLM的框架，通过融合大型语言模型的语义推理和地理空间知识来改善视觉地方识别在城市洪涝等危机事件中的表现。", "motivation": "现有的视觉地方识别模型在应用于社交平台上的洪水图像时性能下降，因为这些图片缺少可靠的位置元数据。为了提高这类场景下的识别准确率，本研究提出了一个新的方法。", "method": "VPR-AttLLM框架通过注意力引导的描述符增强将大型语言模型的语义推理和地理空间知识融入到视觉地方识别中，无需重新训练或额外的数据输入。", "result": "实验显示，该框架与CosPlace、EigenPlaces和SALAD等当前最佳视觉地方识别系统结合后，在检索准确度方面通常提高了1%-3%，在最具挑战性的洪水图像上达到了8%的提升。", "conclusion": "这项研究建立了一个通用的大规模语言模型引导多模态融合范式，并证明了VPR-AttLLM框架能够提高城市监测和快速定位群众来源危机影像的能力。"}}
{"id": "2512.11802", "pdf": "https://arxiv.org/pdf/2512.11802", "abs": "https://arxiv.org/abs/2512.11802", "authors": ["Zheng Li", "Peng Zhang", "Shixiao Liang", "Hang Zhou", "Chengyuan Ma", "Handong Yao", "Qianwen Li", "Xiaopeng Li"], "title": "Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": null, "summary": "Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.", "AI": {"tldr": "本文通过现场数据集和行为分析，研究特斯拉交通灯及停车标志控制系统（TLSSC）与交通控制装置之间的互动行为。", "motivation": "理解高级驾驶员辅助系统（ADAS）如何影响交通运行对于评估其对交通操作的影响至关重要，但这一点在实践中很少得到深入探讨。本文旨在填补这一空白，通过详细研究特斯拉的TLSSC来提供更具体的了解和数据支持。", "method": "设计并执行了不同速度限制和TCD类型的实验，在这些实验中收集了同步高分辨率车辆轨迹数据和驾驶员视角视频。基于这些数据开发了一个TLSSC-TCD互动行为分类，并使用完全速度差模型（FVDM）量化每个行为模式。", "result": "确定了一个约90米的跟随阈值，发现停止行为对期望速度偏差和相对速度具有强烈的响应性，而加速行为则更为保守。在交叉口跟随行为表现出更平滑的动力学特性和更紧密的安全距离。", "conclusion": "本文通过建立数据集、定义行为模式以及模型特征化为未来ADAS-TCD交互逻辑的模拟、安全评估和设计提供了基础，并将这些成果公开分享以促进进一步研究。"}}
{"id": "2512.11800", "pdf": "https://arxiv.org/pdf/2512.11800", "abs": "https://arxiv.org/abs/2512.11800", "authors": ["Jan U. Müller", "Robin Tim Landsgesell", "Leif Van Holland", "Patrick Stotko", "Reinhard Klein"], "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.", "AI": {"tldr": "本文提出了一种基于矩的三维高斯点阵方法，通过新的高保真透射率计算技术提高了复杂半透明物体的渲染效果。", "motivation": "现有的3D高斯点阵依赖于简化、顺序相关alpha混合和密度积分的粗略近似，无法有效处理复杂的重叠半透明对象。", "method": "本文结合先验工作的矩基无序透明度方法，用统计矩来表征摄像机光线沿路径上的密度分布，并从所有贡献3D高斯点中计算每像素的矩。通过这些矩重建连续透射函数并独立采样每个高斯。", "result": "该方法在复杂半透明介质中的光衰减建模上提高了图像质量和真实感。", "conclusion": "本文提出的方法能够提升渲染质量，避免了光线追踪或逐像素排序的需要，在高保真度和效率之间建立了桥梁。"}}
{"id": "2512.11799", "pdf": "https://arxiv.org/pdf/2512.11799", "abs": "https://arxiv.org/abs/2512.11799", "authors": ["Ye Fang", "Tong Wu", "Valentin Deschaintre", "Duygu Ceylan", "Iliyan Georgiev", "Chun-Hao Paul Huang", "Yiwei Hu", "Xuelin Chen", "Tuanfeng Yang Wang"], "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties", "categories": ["cs.CV"], "comment": "Project Page: https://aleafy.github.io/vrgbx", "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.", "AI": {"tldr": "本文提出了V-RGBX框架，实现了基于关键帧的视频编辑，并支持对场景固有属性（如反照率、法线、材质和辐照度）的精确控制。", "motivation": "现有的大规模视频生成模型在模拟现实世界的光照交互方面表现出色，但缺乏能够理解并利用这些内在属性进行视频合成及编辑的闭环框架。本文旨在填补这一空白，提出一个端到端的内在感知视频编辑框架。", "method": "V-RGBX通过一种交错条件机制，在基于关键帧的视频编辑中实现了对固有表示的灵活操控，并且可以生成逼真的视频同时保持时间一致性。该方法包括三个核心功能：逆向渲染成固有通道、从这些固有表示合成为光鲜的真实感视频以及支持任意内在模态的关键帧视频编辑。", "result": "实验结果表明，V-RGBX能够生成物理上合理且具有时序一致性的逼真视频，并能通过关键帧传播对序列进行编辑。与之前的方法相比，该框架在物体外观编辑和场景级重新照明等多种应用中表现更加出色。", "conclusion": "本文提出了V-RGBX，这是第一个端到端的内在感知视频编辑框架，为视频合成提供了新的可能性，并证明了其在多种应用场景中的有效性。"}}
{"id": "2512.11798", "pdf": "https://arxiv.org/pdf/2512.11798", "abs": "https://arxiv.org/abs/2512.11798", "authors": ["Ruining Li", "Yuxin Yao", "Chuanxia Zheng", "Christian Rupprecht", "Joan Lasenby", "Shangzhe Wu", "Andrea Vedaldi"], "title": "Particulate: Feed-Forward 3D Object Articulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project page: https://ruiningli.com/particulate", "summary": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.", "AI": {"tldr": "提出了一种基于单个静态三维网格直接推断其背后结构属性的方法，包括部件、运动学结构和约束。", "motivation": "解决现有的方法需要针对每个对象进行优化耗时长的问题，并且能准确地推测人工智能生成的3D资产的可活动结构。", "method": "使用一种称为Part Articulation Transformer的变换网络处理输入网格的点云，通过灵活可扩展的架构预测所有属性，支持多关节。", "result": "实验结果显示该方法在多个方面显著优于现有的最佳方法，并且能在几秒钟内生成完整活动的三维模型。", "conclusion": "Particulate能够直接从单个3D网格中推断出完整的可活动结构，速度快且准确度高。"}}
{"id": "2512.11797", "pdf": "https://arxiv.org/pdf/2512.11797", "abs": "https://arxiv.org/abs/2512.11797", "authors": ["Junjie Ye", "Rong Xue", "Basile Van Hoorick", "Pavel Tokmakov", "Muhammad Zubair Irshad", "Yue Wang", "Vitor Guizilini"], "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://jay-ye.github.io/AnchorDream/", "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.", "AI": {"tldr": "本文提出了一种名为AnchorDream的方法，利用预训练的视频扩散模型为机器人数据合成提供支持。", "motivation": "现有的生成模型通常只改变视觉外观而不创造新的行为，或者由于缺乏身体一致性而产生不真实的动作。因此，研究者希望通过一种新型的世界模型来解决这些问题，并提高模仿学习的数据收集效率。", "method": "AnchorDream利用预训练的视频扩散模型结合机器人的运动渲染，以此为基础生成新数据，确保合成的动作与机器人的真实物理特性相符。", "result": "实验结果表明，通过该方法产生的数据可以显著提升下游策略的学习性能，在模拟器基准测试中提高了36.4%，在现实世界的应用研究中几乎翻倍了表现。", "conclusion": "研究证明了将生成的世界模型与机器人的运动相结合是实现模仿学习规模化的有效途径。"}}
{"id": "2512.11792", "pdf": "https://arxiv.org/pdf/2512.11792", "abs": "https://arxiv.org/abs/2512.11792", "authors": ["Yang Fei", "George Stoica", "Jingyuan Liu", "Qifeng Chen", "Ranjay Krishna", "Xiaojuan Wang", "Benlin Liu"], "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation", "categories": ["cs.CV"], "comment": "Project Website: https://sam2videox.github.io/", "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .", "AI": {"tldr": "本文提出了一种算法，从自回归视频跟踪模型中提取结构保持运动先验，并将其融入双向视频扩散模型。", "motivation": "现有的方法依赖于噪声运动表示来生成物理上合理的过渡仍然具有挑战性。为了解决这一问题，引入了新的算法以提高视频生成中的结构保持运动的质量。", "method": "使用双向特征融合模块从自回归跟踪模型中提取全局结构保持的运动先验，并通过局部Gram流损失函数对本地特征如何移动进行校准。", "result": "实验表明，所提出的SAM2VideoX在VBench上取得了95.51%的准确率和360.57的FVD得分，分别比基线模型REPA和LoRA提高了2.60%和21%-22%，并赢得了人类偏好的71.4%。", "conclusion": "本文提出的算法能够有效提高视频生成中结构保持运动的质量，并在多个指标上优于现有方法。"}}
{"id": "2512.11791", "pdf": "https://arxiv.org/pdf/2512.11791", "abs": "https://arxiv.org/abs/2512.11791", "authors": ["Wentao Jiang", "Vamsi Varra", "Caitlin Perez-Stable", "Harrison Zhu", "Meredith Apicella", "Nicole Nyamongo"], "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs", "categories": ["cs.CV"], "comment": null, "summary": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.", "AI": {"tldr": "提出了一种基于域自适应的不确定性感知框架，用于临床照片中的白癜风分割。", "motivation": "准确量化临床照片中的白癜风范围对于监测治疗效果至关重要。现有的方法在背景噪声抑制和边缘细化方面存在挑战，需要一种新的可靠且高效的解决方案。", "method": "该方法包括三个关键部分：1）通过领域适应性预训练和ROI约束的双重任务损失进行有效数据训练；2）使用ConvNeXt V2架构并添加高频谱门控模块以及stem-skip连接来捕捉细微纹理；3）采用K折集成和测试时间增强以生成像素级不确定性图。", "result": "在专家标注的数据集上，该方法实现了85.05%的Dice分数，并将边界误差从44.79px降低到29.95px。相比其他基准模型（ResNet-50和UNet++、MiT-B5），本框架表现出色且没有出现灾难性失败。", "conclusion": "该方法提供了一种可靠的自动评估白癜风的标准，生成的熵图有助于临床医生识别模糊区域进行进一步审查"}}
{"id": "2512.11786", "pdf": "https://arxiv.org/pdf/2512.11786", "abs": "https://arxiv.org/abs/2512.11786", "authors": ["Hannes Homburger", "Bastian Jäckl", "Stefan Wirtensohn", "Christian Stopp", "Maximilian T. Fischer", "Moritz Diehl", "Daniel A. Keim", "Johannes Reuter"], "title": "Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control", "categories": ["eess.SY", "cs.HC", "cs.RO"], "comment": "6 pages, 8 figures", "summary": "The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE", "AI": {"tldr": "该论文提出了一种基于最优控制框架的决策支持系统，用于提高莱茵湖渡轮运营的能源效率。", "motivation": "随着自主性、去碳化和数字化转型等变革因素的影响，内河船舶运营需要重新评估。设计出一种能够在实际操作中提供实时指导，从而提高运营效率并保持预定操作时间的方法是必要的。", "method": "该研究基于缩短视野的最优控制框架提出了决策支持系统的设计与开发，并结合了渡轮动态模型和环境干扰因素（如水流、风力）进行了问题建模。通过使用实际数据和示例场景验证系统的效果。", "result": "结果表明，所提出的系统能够有效地为船员提供实时指导，从而提高操作效率并维持预定的操作时间。", "conclusion": "最优控制在内河水域未来渡轮运营中的应用具有巨大潜力。"}}
{"id": "2512.11785", "pdf": "https://arxiv.org/pdf/2512.11785", "abs": "https://arxiv.org/abs/2512.11785", "authors": ["Shujing Chen", "Dmitriy Kunisky"], "title": "Universal entrywise eigenvector fluctuations in delocalized spiked matrix models and asymptotics of rounded spectral algorithms", "categories": ["math.PR", "cs.DS", "math.ST"], "comment": "47 pages, 3 figures", "summary": "We consider the distribution of the top eigenvector $\\widehat{v}$ of a spiked matrix model of the form $H = θvv^* + W$, in the supercritical regime where $H$ has an outlier eigenvalue of comparable magnitude to $\\|W\\|$. We show that, if $v$ is sufficiently delocalized, then the distribution of the individual entries of $\\widehat{v}$ (not, we emphasize, merely the inner product $\\langle \\widehat{v}, v\\rangle$) is universal over a large class of generalized Wigner matrices $W$ having independent entries, depending only on the first two moments of the distributions of the entries of $W$. This complements the observation of Capitaine and Donati-Martin (2018) that these distributions are not universal when $v$ is instead sufficiently localized. Further, for $W$ having entrywise variances close to constant and thus resembling a Wigner matrix, we show by comparing to the case of $W$ drawn from the Gaussian orthogonal or unitary ensembles that averages of entrywise functions of $\\widehat{v}$ behave as they would if $\\widehat{v}$ had Gaussian fluctuations around a suitable multiple of $v$. We apply these results to study spectral algorithms followed by rounding procedures in dense stochastic block models and synchronization problems over the cyclic and circle groups, obtaining the first precise asymptotic characterizations of the error rates of such algorithms.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.11783", "pdf": "https://arxiv.org/pdf/2512.11783", "abs": "https://arxiv.org/abs/2512.11783", "authors": ["Andrew Adiletta", "Kathryn Adiletta", "Kemal Derya", "Berk Sunar"], "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously", "categories": ["cs.CR", "cs.AI"], "comment": "13 pages, 5 Figures", "summary": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization. Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.", "AI": {"tldr": "论文提出了一种称为Super Suffix的新方法，能够绕过大型语言模型的保护机制，并引入了检测此攻击的有效方法DeltaGuard。", "motivation": "随着大规模语言模型的应用增加，安全性问题变得愈发重要。为了防止恶意输入威胁到这些模型的安全性和隐私性，研究者开发了专用的小型防护模型（guard model）。然而，当前的方法可能被绕过或失效，论文旨在通过Super Suffix和DeltaGuard解决这一挑战。", "method": "首先，设计了一种能够同时破坏多种对齐目标的超级后缀攻击策略。其次，提出了一个联合优化技术来提高攻击的成功率。最后，为了检测这种新的攻击方法，开发了基于余弦相似度的DeltaGuard系统。", "result": "实验表明，Super Suffix可以成功绕过五个不同文本生成模型的保护机制，并且DeltaGuard能够以接近100%的准确率识别恶意提示。", "conclusion": "论文展示了如何利用超级后缀攻击来规避现有的防护措施，并提出了一种有效检测此类攻击的方法。这些发现有助于提高大型语言模型的安全性，同时为未来的研究提供了新的视角和方法论。"}}
{"id": "2512.11782", "pdf": "https://arxiv.org/pdf/2512.11782", "abs": "https://arxiv.org/abs/2512.11782", "authors": ["Peiqing Yang", "Shangchen Zhou", "Kai Hao", "Qingyi Tao"], "title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator", "categories": ["cs.CV"], "comment": "Project page: https://pq-yang.github.io/projects/MatAnyone2/", "summary": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.", "AI": {"tldr": "本文提出了一个学习的视频透明度评估器（MQE），用于改进视频遮罩生成的质量，通过在线反馈和离线选择模块提高数据质量和训练效果。", "motivation": "现有的视频遮罩数据集规模有限且真实性不足。虽然利用分割数据可以增强语义稳定性，但缺乏有效的边界监督导致生成的遮罩缺少细节。为此，本文引入了一个学习的Matting Quality Evaluator（MQE）以评估遮罩的质量。", "method": "提出了一个学习的Matting Quality Evaluator（MQE），用于生成像素级质量评估图，并结合长参考帧训练策略提高视频透明度生成的效果和数据集规模。", "result": "MatAnyone 2在合成和现实世界基准测试中均达到了最佳性能，超过了先前的方法。通过改进的数据质量和新方法，构建了VMReal大规模真实视频遮罩数据集。", "conclusion": "本文通过引入MQE和新的训练策略，在改善视频遮罩生成质量的同时扩大了数据集规模，提高了模型的泛化能力与表现力。"}}
{"id": "2512.11781", "pdf": "https://arxiv.org/pdf/2512.11781", "abs": "https://arxiv.org/abs/2512.11781", "authors": ["Vineet Pasumarti", "Lorenzo Bianchi", "Antonio Loquercio"], "title": "Agile Flight Emerges from Multi-Agent Competitive Racing", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world. Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent", "AI": {"tldr": "通过多代理竞争和稀疏的高级目标（例如赢得比赛），研究发现敏捷飞行和策略可以在模拟和现实世界中从基于强化学习训练的代理中出现。", "motivation": "探索在复杂环境中，通过多代理竞争而不是单独训练的方法能否更好地适应实际情况，并实现更好的泛化能力。", "method": "使用稀疏任务级奖励进行多代理竞争训练，比较与直接规定行为的单代理方法的效果差异。", "result": "结果显示，在存在障碍物等更复杂的环境条件下，基于多代理竞争的学习方式比单独训练的方法表现更好且更具适应性，并能较好地泛化到未见过的对手上。", "conclusion": "本工作表明在物理世界中使用稀疏的任务级奖励足以训练具有高级控制能力的智能体，在模拟和现实世界的转移性能上优于单代理方法。"}}
{"id": "2512.11779", "pdf": "https://arxiv.org/pdf/2512.11779", "abs": "https://arxiv.org/abs/2512.11779", "authors": ["Sacha Braun", "David Holzmüller", "Michael I. Jordan", "Francis Bach"], "title": "Conditional Coverage Diagnostics for Conformal Prediction", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.", "AI": {"tldr": "本文提出了一种新的评估预测系统条件覆盖率的方法，即超额风险（Excess Risk of the Target Coverage, ERT），并通过现代分类器实现了更高的统计功效。", "motivation": "现有的评估方法难以有效解决样本效率和过拟合问题，并且无法提供明确的局部偏差解释。为了解决这些问题，本文提出了ERT作为新的评估指标。", "method": "通过将条件覆盖率估计转化为一个分类任务，选择适当的损失函数来计算风险差异，以此作为保守估计自然误覆盖度量的方法。使用现代分类器提升统计功效并分离过度和不足覆盖的影响。", "result": "实验结果表明，基于现代分类器的ERT提供了比现有方法更高的统计功效，并且能够有效评估不同预测系统之间的性能差距。", "conclusion": "本文提出了一种新的条件覆盖率评估指标ERT，为理解和改进预测系统的可靠性提供了一个全新的视角。"}}
{"id": "2512.11773", "pdf": "https://arxiv.org/pdf/2512.11773", "abs": "https://arxiv.org/abs/2512.11773", "authors": ["Britton Jordan", "Jordan Thompson", "Jesse F. d'Almeida", "Hao Li", "Nithesh Kumar", "Susheela Sharma Stern", "Ipek Oguz", "Robert J. Webster III", "Daniel Brown", "Alan Kuntz", "James Ferguson"], "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics", "categories": ["cs.RO"], "comment": "9 pages, 5 figures", "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.", "AI": {"tldr": "提出了一种结合RGB图像和稀疏本体感受测量的主动感知框架ProbeMDE，用于单目深度估计。", "motivation": "为了提高在具有挑战性环境（如手术场景）中的单目深度估计精度，解决纹理不足、镜面反射和遮挡等问题。", "method": "利用多个单目深度估计模型预测基于RGB图像和稀疏的已知深度测量条件下的密集深度图。通过集合方差量化预测不确定性，并使用SVGD选择最佳触碰位置以减少模式塌陷。", "result": "在模拟和物理实验中验证了方法的有效性，结果显示该方法优于基线算法，在标准深度估计指标上实现了更高的精度并减少了必要的本体感受测量次数。", "conclusion": "通过结合RGB图像和稀疏的触觉测量，ProbeMDE能够提高单目深度估计的准确性和效率。"}}
{"id": "2512.11771", "pdf": "https://arxiv.org/pdf/2512.11771", "abs": "https://arxiv.org/abs/2512.11771", "authors": ["Kai Yao", "Marc Juarez"], "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been accepted for publication in the 4th IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026). The final version will be available on IEEE Xplore", "summary": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.", "AI": {"tldr": "系统评估了AI生成图像指纹识别技术在对抗环境下的鲁棒性。", "motivation": "探讨模型指纹检测技术在不同攻击条件下的稳健性，为提高此类技术的准确性和抗干扰能力提供依据。", "method": "提出了五种攻击策略并测试了14种代表性的指纹方法，在RGB、频率和学习特征域中对12种最先进的图像生成器进行了评估。", "result": "发现指纹识别在对抗环境下的性能显著下降，去除攻击成功率高，伪造攻击难度较大但成功率因目标模型而异。存在准确性与稳健性之间的权衡关系。", "conclusion": "结果表明，现有的技术通常不能同时实现准确性和抗干扰能力。需要进一步研究以找到两者间的平衡点。"}}
{"id": "2512.11769", "pdf": "https://arxiv.org/pdf/2512.11769", "abs": "https://arxiv.org/abs/2512.11769", "authors": ["Xiaoyu Ma", "Zhengqing Yuan", "Zheyuan Zhang", "Kaiwen Shi", "Lichao Sun", "Yanfang Ye"], "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models", "categories": ["cs.RO"], "comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model", "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.", "AI": {"tldr": "BLURR是一种轻量级推理包装器，用于加速现有的视觉-语言-动作模型的推断过程。", "motivation": "当前的视觉-语言-动作模型在零样本操作中表现出色，但其推理堆栈对于响应式网络演示或商品GPU上的高频率机器人控制来说过于沉重。", "method": "BLURR通过结合指令前缀键值缓存、混合精度执行和单步执行计划来减少每步计算量，从而加速现有的视觉-语言-动作控制器的推断过程。", "result": "在基于SimplerEnv的评估中，BLURR保持了与原始控制器相当的成功率任务，并显著降低了有效FLOPs和实际延迟。", "conclusion": "BLURR提供了一种实用的方法来部署现代视觉-语言-动作策略，在计算预算紧张的情况下仍然能够实现高效操作。"}}
{"id": "2512.11763", "pdf": "https://arxiv.org/pdf/2512.11763", "abs": "https://arxiv.org/abs/2512.11763", "authors": ["Mohammad Dehghanmanshadi", "Wallapak Tavanapong"], "title": "Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting", "categories": ["cs.CV"], "comment": "Accepted at ICMLA 2025", "summary": "Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure. We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.", "AI": {"tldr": "该论文提出了一种基于逆向风格转换的方法，用于将真实荧光显微图像的样式转移到合成图像上，以减少领域差异并提高细胞计数模型的性能。", "motivation": "传统的领域适应方法在处理合成图像与真实样本之间的复杂纹理和视觉模式差距时存在挑战。因此，作者提出了一种新的逆向风格转换框架来改进这一问题，以生成更接近真实的合成显微镜图像。", "method": "该论文使用了基于逆向的风格转换(Inversion-Based Style Transfer, InST)框架，结合扩散模型和弱结构保留的方法，在真实荧光显微图像与合成数据之间进行样式转移。通过这种方式，作者生成了一种新的合成训练集，并评估了其在细胞计数任务上的性能。", "result": "实验结果显示，使用InST合成的数据进行预训练的模型比单纯依赖硬编码合成数据或Cell200-s公开数据集进行训练的模型表现更好，达到了更低的平均绝对误差(MAE)。此外，在真实数据上还实现了进一步提升的效果。", "conclusion": "该论文提出的方法有效地缩小了合成图像和真实显微镜样本之间的领域差异，提高了基于深度学习的细胞计数系统的性能，为减少手动标记工作量提供了一种可扩展路径。"}}
{"id": "2512.11749", "pdf": "https://arxiv.org/pdf/2512.11749", "abs": "https://arxiv.org/abs/2512.11749", "authors": ["Minglei Shi", "Haolin Wang", "Borui Zhang", "Wenzhao Zheng", "Bohan Zeng", "Ziyang Yuan", "Xiaoshi Wu", "Yuanxing Zhang", "Huan Yang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Jie Zhou", "Jiwen Lu"], "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "categories": ["cs.CV"], "comment": "Code Repository: https://github.com/KlingTeam/SVG-T2I; Model Weights: https://huggingface.co/KlingTeam/SVG-T2I", "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "AI": {"tldr": "SVG-T2I 提出了一种在 VFM 特征领域内实现高质量文本到图像合成的方法，无需使用变分自编码器。", "motivation": "将大规模的文本到图像扩散模型完全训练在视觉基础模型表示空间中尚属未被充分探索。因此，该研究旨在通过扩展 SVG 框架来填补这一空白，以支持直接在 VFM 特征领域进行高质量文本到图像合成。", "method": "SVG-T2I 利用标准的文本到图像扩散流水线，在不使用变分自编码器的情况下实现高效生成。", "result": "SVG-T2I 在 GenEval 和 DPG-Bench 上分别达到了 0.75 和 85.78 的性能，证明了 VFMs 在生成任务中的内在表示能力。", "conclusion": "该研究展示了 SVG-T2I 模型在视觉基础模型特征领域内实现高质量文本到图像合成的有效性，并公开了项目的源代码、预训练权重以及评估流水线。"}}
{"id": "2512.11748", "pdf": "https://arxiv.org/pdf/2512.11748", "abs": "https://arxiv.org/abs/2512.11748", "authors": ["Mohammed El Fallaki Idrissi", "Jad Mounayer", "Sebastian Rodriguez", "Fodil Meraghni", "Francisco Chinesta"], "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.", "AI": {"tldr": "本文提出了一个名为生成参数设计（GPD）的新框架，该框架可以在实时中生成新的设计及其对应的参数解决方案。", "motivation": "通过引入GPD框架来提高模拟工程科学中的设计探索和优化能力，并促进数字和混合孪生的发展，从而改善工程应用中的预测建模和实时决策。", "method": "利用两个Rank Reduction Autoencoders（RRAE），一个用于生成设计或几何图形，另一个用于编码稀疏的Proper Generalized Decomposition（sPGD）模式解决方案。这两个模型在潜在空间中通过回归技术进行链接，以实现高效的设计与其相关sPGD模式之间的转换。", "result": "该框架在两相微观结构上进行了演示，其中多参数解考虑了两个关键材料参数的变化。", "conclusion": "GPD框架为模拟工程科学提供了一个新的范式，通过实现实时几何生成和即时多参数逼近来增强设计探索、优化以及数字和混合孪生的发展。"}}
{"id": "2512.11746", "pdf": "https://arxiv.org/pdf/2512.11746", "abs": "https://arxiv.org/abs/2512.11746", "authors": ["Hana Kopecka", "Jose Such"], "title": "The Influence of Human-like Appearance on Expected Robot Explanations", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.", "AI": {"tldr": "研究机器人外表的人类特征与其解释方式之间的关系", "motivation": "探索机器人外观对其解释模式的影响，特别是人类样貌如何影响人们期望的解释内容", "method": "设计了不同外观机器人的对比实验，让参与者根据这些机器人的行为预期其会提供的解释", "result": "发现所有条件下的大多数解释都是拟人化的，并且人类特征越明显的机器人收到的拟人化解释越多", "conclusion": "人类样貌对预期解释有显著影响，但也有其他非拟人化趋势存在"}}
{"id": "2512.11745", "pdf": "https://arxiv.org/pdf/2512.11745", "abs": "https://arxiv.org/abs/2512.11745", "authors": ["Liqiang Huang", "Rachel W. Mills", "Saikiran Mandula", "Lin Bai", "Mahtab Jeyhani", "John Redell", "Hien Van Nguyen", "Saurabh Prasad", "Dragan Maric", "Badrinath Roysam"], "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.", "AI": {"tldr": "提出了一种基于视觉搜索的策略，用于分析大脑组织中的多重免疫组化图像。", "motivation": "全切片多路成像生成的信息密集型图像难以解析和需要定制软件。为了提供一种无需编程的查询驱动方法来处理这些图像，开发了mViSE系统。", "method": "采用分而治之策略将数据组织为相关分子标记的面板，并使用自监督学习训练每个多重编码器以通过显式视觉确认成功学习。多个面板可以组合起来处理视觉查询，用于检索单个细胞或多细胞微环境。", "result": "验证了mViSE能够检索单个细胞、邻近细胞对、组织片段并划分皮层层次和大脑区域与亚区域的能力。", "conclusion": "提出的mViSE系统是一种无需编程即可探索脑区特征的开放源代码QuPath插件。"}}
{"id": "2512.11743", "pdf": "https://arxiv.org/pdf/2512.11743", "abs": "https://arxiv.org/abs/2512.11743", "authors": ["Yongsheng Huang", "Peibo Duan", "Yujie Wu", "Kai Sun", "Zhipeng Liu", "Changsheng Zhang", "Bin Zhang", "Mingkun Xu"], "title": "CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network's continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.", "AI": {"tldr": "本文提出了一种新的脉冲神经网络（SNN）范式CogniSNN，该范式利用随机图结构提高神经元可扩展性、路径重用性和动态配置能力。", "motivation": "现有的主流SNN研究直接采用传统人工神经网络的刚性层次化架构，忽视了大脑的关键结构性特征。为了弥补这一不足，本文提出CogniSNN以实现更好的灵活性和学习能力。", "method": "引入随机图架构（RGA）来构建新型SNN，通过改进纯脉冲残差机制以及自适应池化策略解决了深度路径中的网络退化及维度不匹配问题。设计了基于关键路径的学习而不遗忘(KP-LwF)方法，并提出了动态增长学习(DGL)算法以实现神经元和突触的动态生长。", "result": "实验表明，CogniSNN在神经形态数据集上达到了与当前最先进的SNN相当或更高的性能水平。其路径重用性增强了网络跨不同场景的持续学习能力，而动态增长算法提高了对抗干扰的鲁棒性，并减轻了固定时间步长约束。", "conclusion": "本文展示了具有随机图结构的SNN在推动脑启发智能方面的潜力，并为其实现在神经形态硬件上的应用奠定了基础。"}}
{"id": "2512.11736", "pdf": "https://arxiv.org/pdf/2512.11736", "abs": "https://arxiv.org/abs/2512.11736", "authors": ["Ninghan Zhong", "Steven Caro", "Megnath Ramesh", "Rishi Bhatnagar", "Avraiem Iskandar", "Stephen L. Smith"], "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots", "categories": ["cs.RO"], "comment": "Under review for ICRA 2026", "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.", "AI": {"tldr": "本文提出了Bench-Push，一个用于评估基于推动物品的移动机器人导航和操作任务的统一基准测试平台。", "motivation": "传统的机器人方法在处理可移动障碍物环境中存在局限性，而新的基于推动的方法需要更系统化的评价体系来支持其研究和发展。", "method": "该论文设计了一个包含多种模拟环境、创新评估指标以及用于展示基线实现效果的完整框架。", "result": "Bench-Push作为一个开源库被开发出来，并且已经在各种环境中通过不同的任务进行了演示和验证。", "conclusion": "Bench-Push为基于推动策略的移动机器人导航与操作任务提供了一个重要的标准化评估工具，支持更广泛的学术研究和实际应用。"}}
{"id": "2512.11725", "pdf": "https://arxiv.org/pdf/2512.11725", "abs": "https://arxiv.org/abs/2512.11725", "authors": ["Carl Feghali", "Hoang-Oanh Le", "Van Bang Le"], "title": "The parameterized complexity of Strong Conflict-Free Vertex-Connection Colorability", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.CO"], "comment": "accepted by DAM (special issue GROW 2024)", "summary": "This paper continues the study of a new variant of graph coloring with a connectivity constraint recently introduced by Hsieh et al. [COCOON 2024]. A path in a vertex-colored graph is called conflict-free if there is a color that appears exactly once on its vertices. A connected graph is said to be strongly conflict-free vertex-connection $k$-colorable if it admits a (proper) vertex $k$-coloring such that any two distinct vertices are connected by a conflict-free shortest path. Among others, we show that deciding, for a given graph $G$ and an integer $k$, whether $G$ is strongly conflict-free $k$-colorable is fixed-parameter tractable when parameterized by the vertex cover number. But under the standard complexity-theoretic assumption NP $\\not\\subseteq$ coNP/poly, deciding, for a given graph $G$, whether $G$ is strongly conflict-free $3$-colorable does not admit a polynomial kernel, even for bipartite graphs. This kernel lower bound is in stark contrast to the ordinal $k$-Coloring problem which is known to admit a polynomial kernel when parameterized by the vertex cover number.", "AI": {"tldr": "研究给定图是否为强冲突自由$k$着色可解的问题，并探讨其参数化复杂性", "motivation": "探讨带连通约束的新变种图着色问题，特别是对于固定参数的处理方式和算法效率", "method": "通过分析图的顶点覆盖数来确定问题是否具有固定的参数复杂性", "result": "证明了给定图在顶点覆盖数下是否为强冲突自由$k$着色是固定参数可解的；但是对于特定情况下的$3$着色，不存在多项式核", "conclusion": "该研究提供了对新变种图着色问题的理解和处理方法"}}
{"id": "2512.11724", "pdf": "https://arxiv.org/pdf/2512.11724", "abs": "https://arxiv.org/abs/2512.11724", "authors": ["Titaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Jirapast Buntub", "Thanapat Warunee", "Napat Asawachaisuvikrom", "Thanaphum Saiwongin"], "title": "From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "6 pages, 1 figure", "summary": "While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.", "AI": {"tldr": "该论文研究了模块化语音到语音检索增强生成系统中的交互摩擦，揭示了三种对话中断模式：时间错位、表达扁平化和修复僵硬。", "motivation": "虽然基于语音的人工智能系统在生成能力方面取得了显著成就，但它们的互动常常感觉不够自然。本文旨在通过分析模块化的S2S-RAG管道来了解这种交互摩擦的原因，并指出这些问题是由于设计优先级从流畅性转向控制所引起的结构后果。", "method": "通过对一个代表性生产系统的全面系统级分析，识别并解释了三个对话中断模式：时间错位、表达扁平化和修复僵硬。此外，该研究强调构建自然语音AI是一项基础设施设计挑战，需要对模块之间的缝合进行精细的编排。", "result": "论文通过案例分析展示了三种交互摩擦的具体表现形式及其影响，并指出这些问题是由于系统架构导致而非简单的缺陷或故障。", "conclusion": "本文认为构建自然的语音人工智能不仅仅是优化单一组件的问题，而是要求设计者关注模块之间的无缝连接和互动，以实现更流畅的对话体验。"}}
{"id": "2512.11722", "pdf": "https://arxiv.org/pdf/2512.11722", "abs": "https://arxiv.org/abs/2512.11722", "authors": ["Lin Bai", "Xiaoyang Li", "Liqiang Huang", "Quynh Nguyen", "Hien Van Nguyen", "Saurabh Prasad", "Dragan Maric", "John Redell", "Pramod Dash", "Badrinath Roysam"], "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images", "categories": ["cs.CV"], "comment": null, "summary": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.", "AI": {"tldr": "提出了一种从弱到强泛化的训练方法，用于全自动分割多通道脑切片图像中的重叠细胞核。", "motivation": "旨在解决无需人工标注即可对新仪器或新成像协议下的密集重叠细胞核进行可靠分割的问题，并提供生产环境中自动化自我诊断的指标。", "method": "采用弱到强泛化的策略训练多头Mask-RCNN模型，结合高效的通道注意机制以实现自动标记校正和覆盖扩展，从而提高新类别图像的分割性能。", "result": "该方法在五种广泛使用的方法中表现最优，并提供了开放形式的代码、样本WSI图像以及高分辨率分割结果供社区采用和调整。", "conclusion": "通过弱到强泛化的策略，实现了无需人工标注的新类别密集重叠细胞核的可靠分割。"}}
{"id": "2512.11720", "pdf": "https://arxiv.org/pdf/2512.11720", "abs": "https://arxiv.org/abs/2512.11720", "authors": ["Yan Zhang", "Han Zou", "Lincong Feng", "Cong Xie", "Ruiqi Yu", "Zhenpeng Zhan"], "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io", "AI": {"tldr": "本文重新定义了音乐驱动的二维舞蹈姿势生成为多通道图像生成问题，通过使用预训练的图像VAE和DiT风格的骨干网络来改进姿势序列的生成。", "motivation": "针对从音乐中生成连贯、节奏同步的2D姿势的关键挑战，特别是在复杂、高变异性的真实世界分布下，本文提出了一种新的方法来解决这一问题。", "method": "该论文将二维姿态序列编码为一个热图图像，并使用预训练的图像VAE进行压缩，用DiT风格的骨干网络建模；同时引入了时间共享的时间索引方案和参考姿势条件策略以同步音乐令牌和姿势潜在变量并保留特定身体比例和屏幕尺度。", "result": "实验表明，在2D舞蹈大规模数据集和AIST++2D基准测试上，该方法在姿态空间、视频空间指标以及人类偏好方面均优于现有方法。", "conclusion": "通过上述改进，本文提出的方法能够在复杂的真实世界分布下生成更高质量的二维舞蹈姿势序列，并为未来的研究提供了新的方向。"}}
{"id": "2512.11719", "pdf": "https://arxiv.org/pdf/2512.11719", "abs": "https://arxiv.org/abs/2512.11719", "authors": ["Yilmaz Korkmaz", "Jay N. Paranjape", "Celso M. de Melo", "Vishal M. Patel"], "title": "Referring Change Detection in Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "summary": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.", "AI": {"tldr": "利用自然语言提示检测特定类型的遥感图像变化", "motivation": "传统方法难以区分不同类型的变化，限制了其应用。为了克服这一问题，该研究提出了基于自然语言的引用变化检测方法。", "method": "提出了一种两阶段框架：RCDNet 和 RCDGen。前者是一个跨模态融合网络用于引用变化检测，后者是一个基于扩散的数据生成管道，能根据用户的需求产生相应的后变化图像和变化图。", "result": "实验结果表明该框架能够进行大规模且有针对性的变化检测", "conclusion": "通过引入自然语言提示的引用变化检测方法，可以实现更灵活、更具针对性的变化检测，提高了遥感影像分析的实际应用价值。"}}
{"id": "2512.11715", "pdf": "https://arxiv.org/pdf/2512.11715", "abs": "https://arxiv.org/abs/2512.11715", "authors": ["Wei Chow", "Linfeng Li", "Lingdong Kong", "Zefeng Li", "Qi Xu", "Hang Song", "Tian Ye", "Xian Wang", "Jinbin Bai", "Shilin Xu", "Xiangtai Li", "Junting Pan", "Shaoteng Liu", "Ran Zhou", "Tianshu Yang", "Songhua Liu"], "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.", "AI": {"tldr": "提出了一种基于Masked Generative Transformers的图像编辑框架EditMGT，通过局部化解码和自适应定位技术实现精准编辑。", "motivation": "现有扩散模型在全局去噪过程中易对非目标区域造成意外修改，本研究旨在探索一种新的方法来解决这一问题，从而提高图像编辑质量与效率。", "method": "利用Masked Generative Transformers的局部解码特性，结合多层注意力融合机制和限制翻转采样技术，实现精确的图像编辑定位。通过构建CrispEdit-2M数据集并进行模型微调来训练EditMGT。", "result": "在四个标准基准测试中表现优异，特别是在风格转换与改变任务上分别提升了3.6%和17.6%，且具备比现有模型快六倍的编辑速度。", "conclusion": "通过提出基于Masked Generative Transformers的新框架EditMGT，成功解决了扩散模型在图像编辑中的局限性，并展示了更好的性能表现。"}}
{"id": "2512.11713", "pdf": "https://arxiv.org/pdf/2512.11713", "abs": "https://arxiv.org/abs/2512.11713", "authors": ["Amirreza Akbari", "Johan Thunberg"], "title": "Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.", "AI": {"tldr": "提出了一种高效的多车辆协调与轨迹规划方法，以应对智能交叉路口的复杂交通场景。", "motivation": "解决由于计算复杂度导致的多车辆安全协调和轨迹规划挑战。", "method": "通过将高维配置空间中的约束最短时间轨迹规划问题分解为一系列二维图搜索问题来减少计算复杂性，并将其集成到非线性模型预测控制框架中。", "result": "数值评估表明该方法在目标值和计算时间方面显著优于现有的MILP时序调度方法。", "conclusion": "所提出的方法成功地解决了智能交叉路口多车辆协调与轨迹规划的挑战，具有更优的目标值和更低的计算时间。"}}
{"id": "2512.11695", "pdf": "https://arxiv.org/pdf/2512.11695", "abs": "https://arxiv.org/abs/2512.11695", "authors": ["Alan Bonomi", "Francesco Banelli", "Antonio Terpin"], "title": "Particle Image Velocimetry Refinement via Consensus ADMM", "categories": ["physics.flu-dyn", "cs.CV", "eess.IV", "math.OC"], "comment": "Code: https://github.com/antonioterpin/flowgym", "summary": "Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.", "AI": {"tldr": "本文通过共识ADMM方法改进了粒子图像测速技术，提高了流场量化精度和鲁棒性。", "motivation": "传统的PIV技术和现有的机器学习方法在面对不同的照明条件、流动状态和颗粒浓度变化时表现不稳定。作者提出了一种基于多种算法的并行处理方案，并引入了共识框架以提高准确性。", "method": "本文采用共识ADMM方法，结合平滑度和不可压缩性先验知识进行流场量化；同时使用JAX实现硬件加速及Flow Gym平台测试与部署。", "result": "实验结果显示，在密集反向搜索估计器上实现了高达20%的终端误差减少，并通过异常值排除进一步增强了性能。", "conclusion": "该方法提供了更准确且鲁棒性的流场测量，适合于流动控制应用中的快速部署和优化。"}}
{"id": "2512.11691", "pdf": "https://arxiv.org/pdf/2512.11691", "abs": "https://arxiv.org/abs/2512.11691", "authors": ["Aya Kaysan Bahjat"], "title": "Text images processing system using artificial intelligence models", "categories": ["cs.CV", "cs.GR"], "comment": "8 pages, 12 figures, article", "summary": "This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.", "AI": {"tldr": "本文提出了一种使用人工智能模型处理文本图像的系统，该系统可以识别和分类图片中的文字内容。", "motivation": "为了应对实际应用中常见的挑战，如光线变化、随机方向、曲度或部分遮盖的文字、低分辨率及略显模糊的文字等。", "method": "此方法分为四个步骤：图像获取与预处理；使用DBNet++模型检测文本元素；运用BART模型对检测到的文本进行分类；最后通过Python和PyQt5开发的用户界面展示结果。各个阶段无缝衔接，形成了一个流畅的工作流。", "result": "在包含高分辨率图片且模拟各种实际挑战条件下的Total-Text数据集上，经过10小时测试后，该系统的文字识别准确率为94.62%。", "conclusion": "实验结果显示提出的混合源文本分类方法在不同成像条件下具有有效性。"}}
{"id": "2512.11683", "pdf": "https://arxiv.org/pdf/2512.11683", "abs": "https://arxiv.org/abs/2512.11683", "authors": ["Qiushi Guo"], "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.", "AI": {"tldr": "本文提出了一种基于多模态和深度感知的增强框架，用于生成更真实且具物理一致性的面部检测训练样本。", "motivation": "传统复制粘贴增强方法容易产生不真实的合成图像，因为它们在前景提取、场景几何一致性以及背景语义匹配方面存在不足。为了克服这些局限性，本文提出了一种新的深度复制粘贴框架。", "method": "该方法首先使用BLIP和CLIP来评估语义和视觉的一致性，并自动检索最合适的背景图像；其次采用SAM3进行精确分割并利用Depth-Anything提取可见区域以防止面部纹理被腐蚀；最后，通过基于深度的滑动窗口机制确保合成图像中的几何真实性。", "result": "实验结果表明，与传统复制粘贴和无深度增强方法相比，本文提出的深度复制粘贴框架提供了更丰富且真实的训练数据，在下游面部检测任务中表现出色。", "conclusion": "该论文提出了一种新的基于多模态和深度感知的复制粘贴技术，可以生成更具真实性的面部检测训练样本，并在实验中证明了其有效性。"}}
{"id": "2512.11682", "pdf": "https://arxiv.org/pdf/2512.11682", "abs": "https://arxiv.org/abs/2512.11682", "authors": ["Tim Cofala", "Christian Kalfar", "Jingge Xiao", "Johanna Schrader", "Michelle Tang", "Wolfgang Nejdl"], "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition", "categories": ["cs.AI", "cs.LG"], "comment": "7 pages, 3 figures", "summary": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.", "AI": {"tldr": "评估TxAgent在NeurIPS CURE-Bench竞赛中的治疗决策能力。", "motivation": "为了提高临床医学中基于AI的治疗决策准确性，特别是在药物推荐、治疗规划和副作用预测方面的需求。通过迭代检索增强生成方法（RAG）来解决复杂的患者特征与疾病过程之间的相互作用问题，并确保准确性和安全性。", "method": "使用一个微调后的Llama-3.1-8B模型结合统一的生物医学工具套件（ToolUniverse），包括FDA药物API，OpenTargets和Monarch资源，以确保访问最新的治疗信息。重点在于提高函数调用的质量及其对整体性能的影响。", "result": "通过改进工具检索策略实现了性能提升，并在CURE-Bench挑战赛中获得了卓越奖。", "conclusion": "展示了如何通过对功能（工具）调用的检索质量进行优化来改善模型的整体表现，进一步强调了医疗应用中准确性和安全性的关键性。"}}
{"id": "2512.11680", "pdf": "https://arxiv.org/pdf/2512.11680", "abs": "https://arxiv.org/abs/2512.11680", "authors": ["Xu Zhang", "Jiabin Fang", "Zhuoming Ding", "Jin Yuan", "Xuan Liu", "Qianjun Zhang", "Zhiyong Li"], "title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.", "AI": {"tldr": "该论文提出了一种名为CLV-Net的方法，用于通过视觉提示指导的跨模态图像理解来解决遥感领域中的问题。", "motivation": "现有方法在使用简单、通用的文字提示时仍难以引导模型关注用户相关的区域。此外，在大规模航拍图像中，许多对象具有相似的外观，并且存在丰富的物体间关系，这进一步增加了精确识别的难度。", "method": "CLV-Net允许用户提供简单的视觉线索——一个边界框来指示感兴趣的区域，并使用该线索指导生成与之相关联的分割掩模和字幕。此外，提出了语义和关系对齐模块：跨模式语义一致性损失增强了在视图相似目标之间的细粒度区分能力，而关系一致性损失则使文本关系与视觉互动保持一致。", "result": "实验结果表明，CLV-Net优于现有方法，并且在两个基准数据集上建立了新的最先进的性能。模型能够有效捕捉用户意图并产生精确的、符合意图的多模态输出。", "conclusion": "提出的CLV-Net解决了遥感领域中的跨模态图像理解难题，通过视觉提示有效地引导了模型以提高分割掩码和字幕的质量，并且在大规模航拍图像中实现了准确的对象识别。"}}
{"id": "2512.11676", "pdf": "https://arxiv.org/pdf/2512.11676", "abs": "https://arxiv.org/abs/2512.11676", "authors": ["Stefan Sommer", "Gefan Yang", "Elizabeth Louise Baker"], "title": "Stochastics of shapes and Kunita flows", "categories": ["math.PR", "cs.CV"], "comment": null, "summary": "Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.", "AI": {"tldr": "定义并形式化了形状空间中随机形状过程的理想属性，并将此与Kunita流联系起来，通过构造满足这些标准的随机过程。", "motivation": "由于形状空间的非线性和无穷维性质，构建合适的随机形状过程具有挑战性。因此，需要定义理想的随机形状过程特性以适应形状结构。", "method": "提出了Kunita流的概念，并将其应用于形状空间中诱导满足特定标准的随机过程。此外，还介绍了桥抽样技术来对观测数据进行条件化处理并实现统计推断。", "result": "通过引入合适的数学框架和方法论，能够构造出与形状结构兼容的随机形状过程，并展示了如何使用桥抽样技术来进行参数估计。", "conclusion": "该研究成功地构建了满足特定理想属性的随机形状过程模型，并为在实际应用中实现有效的统计推断提供了理论基础。"}}
{"id": "2512.11674", "pdf": "https://arxiv.org/pdf/2512.11674", "abs": "https://arxiv.org/abs/2512.11674", "authors": ["Reza Shahriari", "Eric D. Ragan", "Jaime Ruiz"], "title": "Natural Language Interaction for Editing Visual Knowledge Graphs", "categories": ["cs.HC"], "comment": null, "summary": "Knowledge graphs are often visualized using node-link diagrams that reveal relationships and structure. In many applications using graphs, it is desirable to allow users to edit graphs to ensure data accuracy or provides updates. Commonly in graph visualization, users can interact directly with the visual elements by clicking and typing updates to specific items through traditional interaction methods in the graphical user interface. However, it can become tedious to make many updates due to the need to individually select and change numerous items in a graph. Our research investigates natural language input as an alternative method for editing network graphs. We present a user study comparing GUI graph editing with two natural language alternatives to contribute novel empirical data of the trade-offs of the different interaction methods. The findings show natural language methods to be significantly more effective than traditional GUI interaction.", "AI": {"tldr": "研究通过自然语言输入编辑知识图谱的方法，对比GUI方法的有效性。", "motivation": "传统的图形用户界面编辑知识图谱繁琐且耗时，希望探索更加高效的编辑方式。", "method": "进行一项用户实验，比较传统GUI与两种自然语言输入方法的效率和效果。", "result": "发现自然语言输入方法比传统GUI交互更有效。", "conclusion": "自然语言输入是编辑知识图谱的一种高效替代方案。"}}
{"id": "2512.11661", "pdf": "https://arxiv.org/pdf/2512.11661", "abs": "https://arxiv.org/abs/2512.11661", "authors": ["Brenda Nogueira", "Werner Geyer", "Andrew Anderson", "Toby Jia-Jun Li", "Dongwhi Kim", "Nuno Moniz", "Nitesh V. Chawla"], "title": "From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \\textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.", "AI": {"tldr": "该论文通过用户研究探索了大型语言模型在文献综述中的应用，识别出信任缺失、验证负担和工具多样性等问题，并提出了六个设计目标以及一个高级框架以改善相关工作可视化、每一步的验证过程及人机反馈对齐。", "motivation": "尽管许多研究探讨了研究人员如何使用大型语言模型进行科学写作，但这些工具在文献综述中的具体实现、限制和设计挑战仍未被充分探索。论文旨在通过用户调查解决这些问题，并改善相关工作可视化、每一步的验证过程及人机反馈对齐。", "method": "该研究进行了跨学科的研究人员用户调研以识别当前使用大型语言模型进行文献综述时的优点与痛点，提出了六个设计目标并通过高级框架实现这些目标。", "result": "研究发现了三个主要问题：信任缺失、持续的验证负担和需要多个工具。基于这些问题，提出了一套解决方法来改善相关工作可视化、每一步的验证过程及人机反馈对齐。", "conclusion": "通过设计一个基于实际需求的研究框架，论文解决了大型语言模型在文献综述过程中存在的问题，并推动了研究人员与AI系统的实践协作，增强了可验证的信任。"}}
{"id": "2512.11654", "pdf": "https://arxiv.org/pdf/2512.11654", "abs": "https://arxiv.org/abs/2512.11654", "authors": ["Luca Cazzola", "Ahed Alboody"], "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation", "categories": ["cs.CV"], "comment": null, "summary": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).", "AI": {"tldr": "本文提出了一种名为KineMIC的框架，用于从文本到运动模型中提取适合人类行为识别的数据。", "motivation": "大规模标注的人体动作数据集获取成本高昂，而现有的文本到运动生成模型虽然可以提供大量的合成数据，但其训练目标和结构与人体行为识别的需求存在巨大差异。为了克服这一问题，本文提出了KineMIC框架。", "method": "通过假设文本文档的空间中存在着语义对应关系，并利用CLIP的文本嵌入来在稀疏的行为标签和T2M源数据之间建立联系，从而指导微调过程将一般化的T2M模型转化为针对特定任务的动作生成器。", "result": "实验结果表明，该方法能显著提高动作合成的质量，通过增强的数据增强了行为识别的准确性，提高了23.1个百分点。", "conclusion": "KineMIC框架能够有效地利用文本到运动生成模型中的数据来改善人体行为识别的效果。"}}
{"id": "2512.11653", "pdf": "https://arxiv.org/pdf/2512.11653", "abs": "https://arxiv.org/abs/2512.11653", "authors": ["Chutian Ma", "Grigorii Pomazkin", "Giacinto Paolo Saggese", "Paul Smith"], "title": "Causal Inference in Energy Demand Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Energy demand prediction is critical for grid operators, industrial energy consumers, and service providers. Energy demand is influenced by multiple factors, including weather conditions (e.g. temperature, humidity, wind speed, solar radiation), and calendar information (e.g. hour of day and month of year), which further affect daily work and life schedules. These factors are causally interdependent, making the problem more complex than simple correlation-based learning techniques satisfactorily allow for. We propose a structural causal model that explains the causal relationship between these variables. A full analysis is performed to validate our causal beliefs, also revealing important insights consistent with prior studies. For example, our causal model reveals that energy demand responds to temperature fluctuations with season-dependent sensitivity. Additionally, we find that energy demand exhibits lower variance in winter due to the decoupling effect between temperature changes and daily activity patterns. We then build a Bayesian model, which takes advantage of the causal insights we learned as prior knowledge. The model is trained and tested on unseen data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on the test set. The model also demonstrates strong robustness, as the cross-validation across two years of data yields an average MAPE of 3.88 percent.", "AI": {"tldr": "本文提出了一种结构因果模型，用于解释能源需求与天气条件及日历信息之间的因果关系，并构建了一个贝叶斯模型来预测能源需求。", "motivation": "能源需求受多种因素影响，这些因素之间存在复杂的因果关系。传统的相关性学习技术不能充分解决这些问题，因此需要一种能够揭示因果关系的方法来进行更准确的能源需求预测。", "method": "提出了一种结构因果模型以解释变量之间的因果关系，并进行完整的分析来验证这种因果信念。然后建立了一个贝叶斯模型，该模型利用所学到的因果知识作为先验知识，并在未见数据上进行了训练和测试。", "result": "提出的模型在测试集上的平均绝对百分比误差（MAPE）为3.84%，并且跨两年数据进行交叉验证后的平均MAPE为3.88%。此外，该模型揭示了能源需求对温度波动的季节性敏感度以及冬季能源需求较低的原因。", "conclusion": "通过因果模型和贝叶斯预测方法可以更准确地理解并预测能源需求，从而提供比传统技术更好的性能。"}}
{"id": "2512.11645", "pdf": "https://arxiv.org/pdf/2512.11645", "abs": "https://arxiv.org/abs/2512.11645", "authors": ["Jiapeng Tang", "Kai Li", "Chengxiang Yin", "Liuhao Ge", "Fei Jiang", "Jiu Xu", "Matthias Nießner", "Christian Häne", "Timur Bagautdinov", "Egor Zakharov", "Peihong Guo"], "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint", "categories": ["cs.CV"], "comment": "Project page: https://tangjiapeng.github.io/FactorPortrait/", "summary": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.", "AI": {"tldr": "本文介绍了一种名为FactorPortrait的视频扩散方法，能够通过面部表情、头部姿态和相机视点的分离控制信号实现逼真的肖像动画。", "motivation": "为了生成更自然且可控制的肖像动画，该研究提出一种方法来解耦并利用面部表情、头部运动和相机视角信息。", "method": "FactorPortrait使用预训练图像编码器从驱动视频中提取面部表情潜在变量作为动画生成的控制信号，并通过表达控制器将其注入视频扩散转换器。此外，它还采用Plücker射线图和法向量图来实现摄像机与头部姿态的控制。", "result": "实验结果表明该方法在逼真性、表现力、控制精度和视角一致性方面优于现有技术。", "conclusion": "FactorPortrait能够从单一肖像图像生成高度可控且自然的面部动画，同时支持新颖视点合成。"}}
{"id": "2512.11635", "pdf": "https://arxiv.org/pdf/2512.11635", "abs": "https://arxiv.org/abs/2512.11635", "authors": ["Keerthana Murugaraj", "Salima Lamsiyah", "Marten During", "Martin Theobald"], "title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This is a preprint of a manuscript submitted to Digital Scholarship in the Humanities (Oxford University Press). The paper is currently under peer review", "summary": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.", "AI": {"tldr": "通过神经主题建模从大型历史报纸档案中自动提取洞察。", "motivation": "传统的话题模型方法在处理历史文本时难以捕捉到话题的复杂性和动态性，因此提出使用BERTopic来改进这一点。", "method": "采用BERTopic进行主题建模，分析1955年至2018年间关于核能和核安全的文章，追踪话题随时间的变化趋势。", "result": "展示了BERTopic在提取历史话语模式方面的可扩展性和上下文敏感性，包括核能与核武器相关主题的共现及其重要性的变化。", "conclusion": "研究表明BERTopic可以提供比传统方法更丰富的洞察力，并为未来工作指明了方向。"}}
{"id": "2512.11624", "pdf": "https://arxiv.org/pdf/2512.11624", "abs": "https://arxiv.org/abs/2512.11624", "authors": ["Maik Dannecker", "Steven Jia", "Nil Stolt-Ansó", "Nadine Girard", "Guillaume Auzias", "François Rousseau", "Daniel Rueckert"], "title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling", "categories": ["cs.CV"], "comment": "Under Review for MIDL 2026", "summary": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbfΣ_{obs} = \\mathbfΣ_{HR} + \\mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.", "AI": {"tldr": "该论文提出了一种基于高斯基元的显式表示方法，用于从稀疏或降级的二维图像中快速重建高质量三维图像。", "motivation": "当前基于神经网络的隐式表示方法在建模成像物理时需要昂贵的蒙特卡洛采样，这成为计算瓶颈。因此，论文旨在通过引入高斯基元来克服这一问题，实现更高效的三维重建。", "method": "该方法将高质量的3D图像体积参数化为各向异性高斯基元，并利用高斯函数在卷积下封闭性的属性，推导出精确的前向模型公式。这个公式的提出将原本难以求解的成像积分简化为准确的协方差加法。", "result": "该方法与现有的自监督最佳三维重建框架相比，在胎儿和新生儿数据上实现了5到10倍的速度提升，并且在重建质量方面保持一致。", "conclusion": "这种方法不仅提高了重建速度，而且保证了精确的梯度传播。这使得其实时应用成为可能，为临床常规中的实时胎儿3D MRI铺平了道路。"}}
{"id": "2512.11620", "pdf": "https://arxiv.org/pdf/2512.11620", "abs": "https://arxiv.org/abs/2512.11620", "authors": ["Kanisorn Sangchai", "Methasit Boonpun", "Withawin Kraipetchara", "Paulo Garcia"], "title": "Architecting Large Action Models for Human-in-the-Loop Intelligent Robots", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.", "AI": {"tldr": "研究通过组合现成的模型构建大型动作模型，并引入符号封装和输出验证，以提高智能机器人的控制、可解释性和可靠性。", "motivation": "传统的AI技术由于计算和内存成本问题已无法应对大规模需求；而大型语言模型虽然能力强大但缺乏可靠性和透明性。研究旨在解决这些问题，通过结合现成的模型来构建可靠的大型动作模型，并确保其在智能机器人中的安全应用。", "method": "组合现有的基础模型并添加符号封装以提高模型的可解释性和可靠性；实验中使用了多模态机器人并通过生成PDDL代码实现人在循环中的验证阶段，有效地减少了行动幻觉问题。", "result": "研究表明大型动作模型不需要大规模端到端训练，而是通过将高效感知模型与逻辑驱动的核心集成来实现。这种方法在减少行动幻觉方面显示出效果，并为智能机器人的设计提供了支持。", "conclusion": "该方法实现了可靠的大型动作模型，能够应用于各种新型行业并帮助解决安全领域的挑战。"}}
{"id": "2512.11618", "pdf": "https://arxiv.org/pdf/2512.11618", "abs": "https://arxiv.org/abs/2512.11618", "authors": ["Lorenzo Carfagna", "Carlo Tosoni"], "title": "New Entropy Measures for Tries with Applications to the XBWT", "categories": ["cs.DS"], "comment": "32 pages, 4 figures", "summary": "Entropy quantifies the number of bits required to store objects under certain given assumptions. While this is a well established concept for strings, in the context of tries the state-of-the-art regarding entropies is less developed. The standard trie worst-case entropy considers the set of tries with a fixed number of nodes and alphabet size. However, this approach does not consider the frequencies of the symbols in the trie, thus failing to capture the compressibility of tries with skewed character distributions. On the other hand, the label entropy [FOCS '05], proposed for node-labeled trees, does not take into account the tree topology, which has to be stored separately. In this paper, we introduce two new entropy measures for tries - worst-case and empirical - which overcome the two aforementioned limitations. Notably, our entropies satisfy similar properties of their string counterparts, thereby becoming very natural generalizations of the (simpler) string case. Indeed, our empirical entropy is closely related to the worst-case entropy and is reachable through a natural extension of arithmetic coding from strings to tries. Moreover we show that, similarly to the FM-index for strings [JACM '05], the XBWT of a trie can be compressed and efficiently indexed within our k-th order empirical entropy plus o(n) bits, with n being the number of nodes. Interestingly, the space usage of this encoding includes the trie topology and the upper-bound holds for every k sufficiently small, simultaneously. This XBWT encoding is always strictly smaller than the original one [JACM '09] and we show that in certain cases it is asymptotically smaller.", "AI": {"tldr": "本文提出了一种新的尝试熵度量方法，克服了现有标准的限制，并展示了基于新熵的XBWT压缩和索引技术。", "motivation": "传统的尝试最坏情况熵没有考虑符号频率，而标签熵忽略了树结构。因此，提出了两种新的尝试熵测量来解决这些问题，以更准确地评估尝试的信息量。", "method": "引入了最坏情况熵和经验熵两个新概念，并展示了基于这些度量的XBWT压缩与索引技术如何实现高效存储。", "result": "新方法能够将XBWT在k阶经验熵之上加o(n)比特的空间中进行压缩，同时保持树结构信息。这比原来的方法更为紧凑，在某些情况下是渐进更小的。", "conclusion": "通过引入新的尝试熵测量概念并应用到XBWT上，本文提出了一种高效的存储方案，提高了数据压缩效率和索引性能。"}}
{"id": "2512.11614", "pdf": "https://arxiv.org/pdf/2512.11614", "abs": "https://arxiv.org/abs/2512.11614", "authors": ["Björn Deiseroth", "Max Henning Höth", "Kristian Kersting", "Letitia Parcalabescu"], "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "34 pages, 19 figures", "summary": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.", "AI": {"tldr": "本文提出了一个通过Merlin-Arthur协议训练的框架，以提高检索增强生成系统在面对不完整或误导性上下文时的表现。", "motivation": "当前检索增强生成模型容易产生无根据的回答和依赖于虚假证据的问题。作者希望通过将整个RAG流程视为交互式证明系统来解决这些问题。", "method": "作者引入了一个基于Merlin-Arthur协议的训练框架，其中Arthur（生成器LLM）在面对未知来源问题时接受帮助和支持，并且通过线性时间XAI方法识别和修改对答案最有影响力的证据。此外还介绍了一种严格的评估框架来分离解释准确度与基线预测错误。", "result": "实验表明，在三个RAG数据集上，基于Merlin-Arthur训练的LLM在可靠性和减少幻觉方面表现更好，并且不需要手动标注无法回答的问题。检索器也通过自动产生的M/A困难正负样本提高了召回率和MRR。", "conclusion": "自主交互证明式的监督为构建可靠RAG系统提供了一种实用的方法，使得检索到的文档不仅仅是建议，而是可验证的证据。"}}
{"id": "2512.11612", "pdf": "https://arxiv.org/pdf/2512.11612", "abs": "https://arxiv.org/abs/2512.11612", "authors": ["Chunyi Li", "Rui Qing", "Jianbo Zhang", "Yuan Tian", "Xiangyang Zhu", "Zicheng Zhang", "Xiaohong Liu", "Weisi Lin", "Guangtao Zhai"], "title": "Embodied Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": "15 pages, 12 figures, 3 tables", "summary": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.", "AI": {"tldr": "本文提出了Embodied图像压缩的概念，并建立了一个标准基准EmbodiedComp来评估在超低比特率条件下的性能，以促进机器视觉数据的高效压缩。", "motivation": "随着机器智能的发展，图像压缩的目标从特定任务虚拟模型转向了真实环境中操作的具身代理。为了解决多代理系统中的通信限制并确保实时任务执行，研究者们引入了针对具身AI的图像压缩问题。", "method": "通过在模拟和现实世界的设置中进行广泛的实证研究，文章展示了现有视觉-语言-行动模型（VLAs）在超低比特率下无法可靠地完成简单的操作任务。此外，作者建立了EmbodiedComp基准来促进专门领域的具身体积压缩技术的发展。", "result": "研究表明，在低于某个阈值的比特率条件下，现有的VLA模型无法执行简单任务。这表明了专为具身代理设计的域特定图像压缩的重要性。", "conclusion": "本文的研究将推动针对具身代理的领域特定数据压缩技术的发展，从而加快具身AI在现实世界中的部署进程。"}}
