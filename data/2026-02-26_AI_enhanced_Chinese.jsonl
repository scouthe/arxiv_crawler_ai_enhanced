{"id": "2602.21204", "pdf": "https://arxiv.org/pdf/2602.21204", "abs": "https://arxiv.org/abs/2602.21204", "authors": ["Junchen Liu", "Sven Elflein", "Or Litany", "Zan Gojcic", "Ruilong Li"], "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Webpage: https://research.nvidia.com/labs/sil/projects/tttla/", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "AI": {"tldr": "本文重新审视了测试时训练（TTT）的解释，揭示其本质上是学习到的一种线性注意力操作。", "motivation": "发现现有的测试时训练方法与记忆基解释不符，促使作者重新审查和改进模型理解。", "method": "分析表明一系列TTT架构可以被表达为一种学习型线性注意力算子，并提供了平行化公式以提高效率。", "result": "提出的新视角不仅解释了之前令人困惑的模型行为，还提供了一系列实用好处，包括简化架构、并行计算和性能提升。", "conclusion": "结论是TTT本质上是一种改进表示能力的学习线性注意力操作，而非测试时记忆。"}}
{"id": "2602.21203", "pdf": "https://arxiv.org/pdf/2602.21203", "abs": "https://arxiv.org/abs/2602.21203", "authors": ["Abdulaziz Almuzairee", "Henrik I. Christensen"], "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "For website and code, see https://aalmuzairee.github.io/squint", "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.", "AI": {"tldr": "提出了一种名为Squint的快速视觉强化学习方法，用于仿真到现实的机器人控制任务。", "motivation": "为了提高基于图像输入的视觉强化学习的速度和效率，解决高维图像带来的训练复杂性问题以及存储和编码开销。", "method": "引入了平行模拟、分布式评论家、分辨率调整（squinting）、层归一化等技术，并优化了更新与数据的比例及实施方式。具体为基于视觉的软演员批评家方法，使用并行仿真、分辨率降低策略以及其它多项优化措施来提高训练效率。", "result": "在SO-101任务集上的实验表明Squint可以比之前的视觉得分和在线学习方法更快地进行墙时钟训练，并能实现在真实机器人上的模拟到现实迁移，大多数任务在6分钟内收敛。", "conclusion": "通过引入Squint技术，成功实现了基于视觉的快速强化学习，提高了仿真环境中获得的知识向实际应用转移的速度与效率。"}}
{"id": "2602.21202", "pdf": "https://arxiv.org/pdf/2602.21202", "abs": "https://arxiv.org/abs/2602.21202", "authors": ["Hanxiang Qin", "Alexander Martin", "Rohan Jha", "Chunsheng Zuo", "Reno Kriz", "Benjamin Van Durme"], "title": "Multi-Vector Index Compression in Any Modality", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": "12 pages, 4 figures", "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "AI": {"tldr": "本文研究了多向量检索在任何模态中的有效压缩方法，引入了一种注意引导的聚类（AGC）技术，以提高索引效率。", "motivation": "为了降低延迟互动中计算和存储成本线性增长的问题，特别是针对图像、视频和音频丰富的数据库，本文探索了无查询偏见的方法来压缩多向量文档表示，并在固定向量预算下进行了研究。", "method": "本文提出了四种索引压缩方法：序列调整大小、内存标记、分层池化以及一种新颖的注意力引导聚类（AGC）。其中AGC使用注意力机制识别出文档中最具语义重要性的区域作为聚簇中心，并加权令牌聚合。", "result": "在涵盖文本，视觉文件和视频检索任务上评估这些方法后发现，注意引导聚类技术始终优于其他参数化压缩方法，并且比非参数分层聚类提供了更大的索引大小灵活性。同时取得了与未压缩索引相当或更好的性能表现。", "conclusion": "本文提出的方法有效解决了在各种模态下多向量检索的效率问题，特别是通过AGC技术实现了更高的性能和更灵活的索引管理能力。"}}
{"id": "2602.21201", "pdf": "https://arxiv.org/pdf/2602.21201", "abs": "https://arxiv.org/abs/2602.21201", "authors": ["Tony Feng", "Junehyuk Jung", "Sang-hyun Kim", "Carlo Pagano", "Sergei Gukov", "Chiang-Chiang Tsai", "David Woodruff", "Adel Javanmard", "Aryan Mokhtari", "Dawsen Hwang", "Yuri Chervonyi", "Jonathan N. Lee", "Garrett Bingham", "Trieu H. Trinh", "Vahab Mirrokni", "Quoc V. Le", "Thang Luong"], "title": "Aletheia tackles FirstProof autonomously", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia", "summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.", "AI": {"tldr": "报告了Aletheia在首届FirstProof挑战中自主解决数学问题的表现。", "motivation": "展示由Gemini 3 Deep Think驱动的数学研究代理Aletheia的能力，特别是在处理复杂证明任务方面。", "method": "利用Aletheia参加为期规定的比赛时间内的FirstProof挑战，并根据专家评估结果进行性能分析。", "result": "在10个问题中自主解决了6个（2,5,7,8,9,10），其中对第8个问题的解决方案存在争议，但总体上达到了较高的准确度。", "conclusion": "Aletheia展示了其解决数学证明问题的能力，并为未来进一步的研究和改进提供了参考。"}}
{"id": "2602.21198", "pdf": "https://arxiv.org/pdf/2602.21198", "abs": "https://arxiv.org/abs/2602.21198", "authors": ["Yining Hong", "Huang Huang", "Manling Li", "Li Fei-Fei", "Jiajun Wu", "Yejin Choi"], "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.", "AI": {"tldr": "本文提出了一种新的测试时间计划方法，即反思性测试时间规划（Reflective Test-Time Planning），用于改进机器人在执行任务时的学习和决策过程。", "motivation": "传统的Embodied LLMs无法从错误中学习，导致部署过程中重复犯同样的错误。作者希望通过借鉴人类的反思实践来提高机器人的适应性和学习能力。", "method": "该方法引入了两种类型的反思：行动中的反思（reflection-in-action）和对行动后的反思（reflection-on-action），并通过回顾性反思允许机器人重新评估之前的决策并进行模型更新。", "result": "实验结果表明，这种方法在长时序家庭任务和MuJoCo橱柜组装基准上显著优于基线模型。定性的分析显示了通过反思纠正行为的有效性。", "conclusion": "该方法能够有效提高Embodied LLMs的决策质量，并且通过反思机制可以更好地利用经验来改进未来的行动策略。"}}
{"id": "2602.21195", "pdf": "https://arxiv.org/pdf/2602.21195", "abs": "https://arxiv.org/abs/2602.21195", "authors": ["Xingyi Cheng", "Julien Maufront", "Aurélie Di Cicco", "Daniël M. Pelt", "Manuela Dezi", "Daniel Lévy"], "title": "Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography", "categories": ["cs.CV"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.", "AI": {"tldr": "开发了一个用于直接分割感兴趣区域并进行形态表面分析的两步框架，适用于冷冻电子断层扫描中的膜结构", "motivation": "现有的ROI识别方法需要通过全结构分割后的后处理来实现，这种方法对复杂和几何形状多变的结构如膜来说效率低下。因此开发了新的直接ROI分割和形态表面分析工具。", "method": "TomoROIS-SurfORA框架由两个步骤组成：第一步使用深度学习进行感兴趣区域分割；第二步通过点云和曲面网格处理提取定量形态特征，支持封闭和开放的表面。", "result": "在体外重构膜系统中展示了该工具的效果，并且实现了自动量化分析膜接触位点以及如凹陷等重塑事件。适用于冷冻电子断层扫描中的膜数据以及其他科学成像上下文。", "conclusion": "TomoROIS-SurfORA为复杂的生物结构提供了一种新的直接ROI分割和定量形态表面分析的方法，特别适合于冷冻电子显微镜应用，并且在更广泛的科学成像场景中具有潜在的应用价值。"}}
{"id": "2602.21191", "pdf": "https://arxiv.org/pdf/2602.21191", "abs": "https://arxiv.org/abs/2602.21191", "authors": ["Ilias Diakonikolas", "Daniel M. Kane"], "title": "Statistical Query Lower Bounds for Smoothed Agnostic Learning", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": null, "summary": "We study the complexity of smoothed agnostic learning, recently introduced by~\\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\\tilde{O}(1/σ^2) \\log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.21189", "pdf": "https://arxiv.org/pdf/2602.21189", "abs": "https://arxiv.org/abs/2602.21189", "authors": ["Anas Barakat", "Souradip Chakraborty", "Khushbu Pahwa", "Amrit Singh Bedi"], "title": "Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.", "AI": {"tldr": "研究为什么优化Pass@k会导致Pass@1性能下降，通过理论分析和实验验证了这种现象的原因。", "motivation": "在对大型语言模型进行后训练时，虽然优化Pass@k可以提高任务的总体成功率，但常导致单次推理准确率（Pass@1）下降。这限制了模型的实际应用效果，因为许多场景下需要可靠的一次性回答。", "method": "从理论上分析并提出了一种关于Prompt干扰的新理论解释：当优化Pass@k时，低成功概率的提示被重新加权，若这些提示是负向干扰型，则会导致Pass@1性能下降。通过在数学推理任务上的实验验证了这一结论。", "result": "实验证明了理论上预测的现象，在数学推理等验证性任务中，优化Pass@k确实可能导致Pass@1的下降。", "conclusion": "揭示并解释了大型语言模型后训练过程中Pass@k与Pass@1之间的性能冲突，并提出了改进策略的方向。"}}
{"id": "2602.21188", "pdf": "https://arxiv.org/pdf/2602.21188", "abs": "https://arxiv.org/abs/2602.21188", "authors": ["Tiantian Wang", "Chun-Han Yao", "Tao Hu", "Mallikarjun Byrasandra Ramalinga Reddy", "Ming-Hsuan Yang", "Varun Jampani"], "title": "Human Video Generation from a Single Image with 3D Pose and View Control", "categories": ["cs.CV"], "comment": null, "summary": "Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.", "AI": {"tldr": "从单张图像生成高质量、多视角的人体视频", "motivation": "解决从单张图像生成人体视频时视图一致性和运动依赖性衣物褶皱的难题", "method": "提出HVG模型，通过关节双维骨图捕捉三维骨骼关系并引入三维信息处理自遮挡问题；利用视图和时间对齐确保多视角一致性；采用渐进时空采样保持长动画的平滑过渡", "result": "实验表明HVG在从多样人体图像和姿态输入生成高质量4D人体视频方面优于现有方法", "conclusion": "HVG是一种有效的单图到视频生成模型，能生成视图一致且动作连贯的人体视频"}}
{"id": "2602.21186", "pdf": "https://arxiv.org/pdf/2602.21186", "abs": "https://arxiv.org/abs/2602.21186", "authors": ["Haoyi Jiang", "Liu Liu", "Xinjie Wang", "Yonghao He", "Wei Sui", "Zhizhong Su", "Wenyu Liu", "Xinggang Wang"], "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.", "AI": {"tldr": "Spa3R是一种自我监督框架，用于从多视角图像中学习统一、视角不变的空间表示。", "motivation": "现有的视觉语言模型在理解二维图像方面表现出色，但在理解和推理三维空间方面的表现较为浅薄。当前的方法要么依赖显式的三维模态，要么通过增加部分的视图条件几何先验来弥补这种差距，但这些方法限制了可扩展性，并迫使语言模型从稀疏线索中隐式重建完整的三维结构。", "method": "Spa3R基于预测空间场建模（PSFM）范例，学习任意未见视角的特征场，通过紧凑的潜在表示进行条件化。此外，将预训练的Spa3R编码器集成到现有的视觉语言模型中，形成Spa3-VLM。", "result": "在VSI-Bench上的实验表明，Spa3-VLM在三维问题回答任务上达到了58.6％的准确率，显著优于现有方法。", "conclusion": "PSFM为提高空间智能提供了一条可扩展的道路。"}}
{"id": "2602.21183", "pdf": "https://arxiv.org/pdf/2602.21183", "abs": "https://arxiv.org/abs/2602.21183", "authors": ["Ratnajit Dhar", "Arpita Mallik"], "title": "823-OLT @ BUET DL Sprint 4.0: Context-Aware Windowing for ASR and Fine-Tuned Speaker Diarization in Bengali Long Form Audio", "categories": ["cs.SD"], "comment": null, "summary": "Bengali, despite being one of the most widely spoken languages globally, remains underrepresented in long form speech technology, particularly in systems addressing transcription and speaker attribution. We present frameworks for long form Bengali speech intelligence that address automatic speech recognition using a Whisper Medium based model and speaker diarization using a finetuned segmentation model. The ASR pipeline incorporates vocal separation, voice activity detection, and a gap aware windowing strategy to construct context preserving segments for stable decoding. For diarization, a pretrained speaker segmentation model is finetuned on the official competition dataset (provided as part of the DL Sprint 4.0 competition organized under BUET CSE Fest), to better capture Bengali conversational patterns. The resulting systems deliver both efficient transcription of long form audio and speaker aware transcription to provide scalable speech technology solutions for low resource languages.", "AI": {"tldr": "该论文提出了针对孟加拉语长音频的自动语音识别和说话人归一化的框架。", "motivation": "孟加拉语在全球范围内使用广泛，但在长形式演讲技术方面仍然代表性不足，特别是在处理转录和说话者归属系统时。因此开发了基于Whisper Medium模型的ASR管道和经过预训练后在官方比赛数据集上微调的分割模型。", "method": "ASR管道包括声带分离、语音活动检测以及采用间隙感知窗口策略构建保持上下文的片段，以实现稳定的解码。说话人归一化方面则使用了预先训练过的分割模型，并对其进行了微调，以便更好地捕获孟加拉语对话模式。", "result": "这些系统提供了长格式音频的有效转录和基于说话人的转录，为低资源语言提供可扩展的演讲技术解决方案。", "conclusion": "该论文通过开发针对孟加拉语长形式音频自动语音识别和说话人归一化的框架，展示了如何克服在低资源语言环境中进行高效、精确的语音处理挑战。"}}
{"id": "2602.21179", "pdf": "https://arxiv.org/pdf/2602.21179", "abs": "https://arxiv.org/abs/2602.21179", "authors": ["Nicolás Gaggion", "Maria J. Ledesma-Carbayo", "Stergios Christodoulidis", "Maria Vakalopoulou", "Enzo Ferrante"], "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision", "categories": ["cs.CV"], "comment": null, "summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.", "AI": {"tldr": "提出Mask-HybridGNet框架，使用标准像素级掩码训练图模型进行医学图像分割，并实现解剖对应性。", "motivation": "现有方法需要手动标注的点对点对应地标数据集，限制了基于图形的医学图像分割的应用。本研究旨在通过像素级监督消除这一需求。", "method": "Mask-HybridGNet结合Chamfer距离监督和边基正则化来调整地面真实边界以适应固定长度的地标预测，并使用可微渲染进一步细化。框架隐式学习解剖对应性，不需显式的对应监督。", "result": "实验显示，Mask-HybridGNet在胸部X射线、心脏超声波等多个医学图像数据集上的分割结果与最先进的像素级方法相当，同时确保了边界连通性和拓扑完整性。", "conclusion": "提出的方法利用大量标准掩码进行训练，生成保持解剖一致性的结构化模型，促进了基于图的医学图像分割的应用。"}}
{"id": "2602.21178", "pdf": "https://arxiv.org/pdf/2602.21178", "abs": "https://arxiv.org/abs/2602.21178", "authors": ["Sepehr Salem Ghahfarokhi", "M. Moein Esfahani", "Raj Sunderraman", "Vince Calhoun", "Mohammed Alser"], "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in ICCABS 2026: The 14th International Conference on Computational Advances in Bio and Medical Sciences", "summary": "Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.", "AI": {"tldr": "XMorph是一个解释性框架，用于通过结合深度学习和大语言模型来分类三种脑肿瘤类型：胶质瘤、脑膜瘤和垂体瘤。", "motivation": "传统的深度学习模型在自动诊断脑肿瘤方面取得了显著进步，但由于可解释性和计算限制，临床应用受到阻碍。为此，研究人员提出了XMorph框架，旨在提高这些系统的透明度和性能。", "method": "XMorph引入了信息加权边界归一化机制以增强对关键边界的识别，并通过双通道的可解释AI模块结合GradCAM++视觉提示和大语言模型生成的文本理由来提供临床解读。", "result": "该框架在脑肿瘤分类中达到了96.0%的准确率，证明了高性能与透明度可以共存于基于AI的医学成像系统中。", "conclusion": "XMorph框架通过提高解释性和计算效率展示了其在改善脑肿瘤自动诊断中的潜力。"}}
