{"id": "2602.24210", "pdf": "https://arxiv.org/pdf/2602.24210", "abs": "https://arxiv.org/abs/2602.24210", "authors": ["Haritz Puerto", "Haonan Li", "Xudong Han", "Timothy Baldwin", "Iryna Gurevych"], "title": "Controllable Reasoning Models Are Private Thinkers", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI agents powered by reasoning models require access to sensitive user data. However, their reasoning traces are difficult to control, which can result in the unintended leakage of private information to external parties. We propose training models to follow instructions not only in the final answer, but also in reasoning traces, potentially under different constraints. We hypothesize that improving their instruction following abilities in the reasoning traces can improve their privacy-preservation skills. To demonstrate this, we fine-tune models on a new instruction-following dataset with explicit restrictions on reasoning traces. We further introduce a generation strategy that decouples reasoning and answer generation using separate LoRA adapters. We evaluate our approach on six models from two model families, ranging from 1.7B to 14B parameters, across two instruction-following benchmarks and two privacy benchmarks. Our method yields substantial improvements, achieving gains of up to 20.9 points in instruction-following performance and up to 51.9 percentage points on privacy benchmarks. These improvements, however, can come at the cost of task utility, due to the trade-off between reasoning performance and instruction-following abilities. Overall, our results show that improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents. Our code and data are available at https://github.com/UKPLab/arxiv2026-controllable-reasoning-models", "AI": {"tldr": "本论文提出了一种方法，通过控制AI代理的推理过程来提高其隐私保护能力。", "motivation": "当前基于推理模型的人工智能代理在处理敏感用户数据时容易泄露私人信息。研究者旨在改进这些模型以更好地遵循指令，特别是在生成推理轨迹的过程中，并希望这种方法能够提升私隐保护性能。", "method": "通过微调具有明确约束的推理痕迹的新指令跟随数据集，以及使用分离LoRA适配器来解耦推理和答案生成策略，研究团队展示了如何改善隐私保护。这些方法在多个模型上进行了测试。", "result": "实验结果表明，这种方法显著提高了指令跟随性能和私隐保护效果，在某些情况下分别提升了20.9分和51.9个百分点。但是，也存在任务效用降低的问题，因为这需要权衡推理性能和遵循指令的能力。", "conclusion": "研究证明了改进AI代理的隐私意识是一个可行的方向，并展示了如何通过控制模型的行为来增强私隐保护，同时指出了未来工作中的潜在挑战。"}}
{"id": "2602.24209", "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.", "AI": {"tldr": "提出了一个高效的无监督联邦学习框架，用于异构物联网网络中的异常检测。", "motivation": "解决联邦学习在处理物联网中特征异质性时的挑战，提高模型性能和隐私保护，在不进行集中数据聚合的情况下实现有效的异常检测。", "method": "利用两个不同的物联网数据集（一个专注于异常检测，另一个侧重于设备识别）共享特征的同时保持特定的数据集特性，并应用可解释的人工智能技术以增强透明度和理解性。", "result": "实验结果表明所提出的框架在异常检测的准确性方面优于传统的联邦学习方法。", "conclusion": "该工作展示了从互补数据集中利用共享特征优化无监督的联邦学习，从而在分布式物联网环境中实现更优的异常检测性能。"}}
{"id": "2602.24172", "pdf": "https://arxiv.org/pdf/2602.24172", "abs": "https://arxiv.org/abs/2602.24172", "authors": ["Adam Dejl", "Deniz Gorur", "Francesca Toni"], "title": "ArgLLM-App: An Interactive System for Argumentative Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "AAMAS 2026 Demonstration Track", "summary": "Argumentative LLMs (ArgLLMs) are an existing approach leveraging Large Language Models (LLMs) and computational argumentation for decision-making, with the aim of making the resulting decisions faithfully explainable to and contestable by humans. Here we propose a web-based system implementing ArgLLM-empowered agents for binary tasks. ArgLLM-App supports visualisation of the produced explanations and interaction with human users, allowing them to identify and contest any mistakes in the system's reasoning. It is highly modular and enables drawing information from trusted external sources. ArgLLM-App is publicly available at https://argllm.app, with a video demonstration at https://youtu.be/vzwlGOr0sPM.", "AI": {"tldr": "论文提出了一种基于大语言模型的论证系统ArgLLM-App，旨在支持人类用户通过可视化和交互来审查和纠正系统的推理错误。", "motivation": "为了使决策结果能够被人类理解和质疑，引入了具有解释性和可争议性的大规模语言模型（LLMs）进行论证推理。", "method": "开发了一种基于网页的系统ArgLLM-App，该系统采用大语言模型支持二元任务，并允许用户通过可视化工具与系统互动以纠正错误。", "result": "提出了一个高度模块化、可从外部可信来源获取信息的交互式论证系统ArgLLM-App。此系统已公开可用。", "conclusion": "开发了一个能够增强人类对模型决策理解能力并提高决策质量的交互式论证支持系统ArgLLM-App。"}}
{"id": "2602.24161", "pdf": "https://arxiv.org/pdf/2602.24161", "abs": "https://arxiv.org/abs/2602.24161", "authors": ["Chao Xu", "Xiaochen Zhao", "Xiang Deng", "Jingxiang Sun", "Zhuo Su", "Donglin Di", "Yebin Liu"], "title": "GeoDiff4D: Geometry-Aware Diffusion for 4D Head Avatar Reconstruction", "categories": ["cs.CV"], "comment": "17 pages", "summary": "Reconstructing photorealistic and animatable 4D head avatars from a single portrait image remains a fundamental challenge in computer vision. While diffusion models have enabled remarkable progress in image and video generation for avatar reconstruction, existing methods primarily rely on 2D priors and struggle to achieve consistent 3D geometry. We propose a novel framework that leverages geometry-aware diffusion to learn strong geometry priors for high-fidelity head avatar reconstruction. Our approach jointly synthesizes portrait images and corresponding surface normals, while a pose-free expression encoder captures implicit expression representations. Both synthesized images and expression latents are incorporated into 3D Gaussian-based avatars, enabling photorealistic rendering with accurate geometry. Extensive experiments demonstrate that our method substantially outperforms state-of-the-art approaches in visual quality, expression fidelity, and cross-identity generalization, while supporting real-time rendering.", "AI": {"tldr": "提出了一种基于几何感知扩散的框架，用于从单个肖像图像重建4D头部化身。", "motivation": "当前方法依赖于2D先验知识，在实现一致的3D几何结构方面存在困难。因此，提出了一个新的框架来解决这一挑战。", "method": "通过合成肖像图和表面法线，并使用无姿势表情编码器捕捉隐式表达表示，从而学习强几何先验以进行高保真头部化身重建。", "result": "方法在视觉质量、表情忠实度以及跨身份泛化方面显著优于现有技术，同时支持实时渲染。", "conclusion": "该框架提供了一种新颖的方法来解决从单个图像重建高质量4D头部化身的挑战"}}
{"id": "2602.24160", "pdf": "https://arxiv.org/pdf/2602.24160", "abs": "https://arxiv.org/abs/2602.24160", "authors": ["Alexander Vieth", "Boudewijn Lelieveldt", "Elmar Eisemann", "Anna Vilanova", "Thomas Höllt"], "title": "Manifold-Preserving Superpixel Hierarchies and Embeddings for the Exploration of High-Dimensional Images", "categories": ["cs.CV"], "comment": "12 pages main paper, 8 pages supplemental material", "summary": "High-dimensional images, or images with a high-dimensional attribute vector per pixel, are commonly explored with coordinated views of a low-dimensional embedding of the attribute space and a conventional image representation. Nowadays, such images can easily contain several million pixels. For such large datasets, hierarchical embedding techniques are better suited to represent the high-dimensional attribute space than flat dimensionality reduction methods. However, available hierarchical dimensionality reduction methods construct the hierarchy purely based on the attribute information and ignore the spatial layout of pixels in the images. This impedes the exploration of regions of interest in the image space, since there is no congruence between a region of interest in image space and the associated attribute abstractions in the hierarchy. In this paper, we present a superpixel hierarchy for high-dimensional images that takes the high-dimensional attribute manifold into account during construction. Through this, our method enables consistent exploration of high-dimensional images in both image and attribute space. We show the effectiveness of this new image-guided hierarchy in the context of embedding exploration by comparing it with classical hierarchical embedding-based image exploration in two use cases.", "AI": {"tldr": "本文提出了一种新的超像素层次结构，用于高维图像的探索。", "motivation": "现有的分层维度减少方法忽略了图像中像素的空间布局，使得在图像空间中的感兴趣区域和属性抽象之间缺乏一致性。", "method": "通过考虑高维属性流形，在构建超像素层次结构时引入了空间布局信息，从而实现了一致的图像和属性空间探索。", "result": "实验结果表明，新的基于图象引导的层次结构在嵌入探索中比传统的分层嵌入方法更有效。", "conclusion": "所提出的方法能够更好地支持高维图像的一致性探索，在两种使用场景下均表现出色。"}}
{"id": "2602.24144", "pdf": "https://arxiv.org/pdf/2602.24144", "abs": "https://arxiv.org/abs/2602.24144", "authors": ["Muquan Li", "Hang Gou", "Yingyi Ma", "Rongzheng Wang", "Ke Qin", "Tao He"], "title": "Fixed Anchors Are Not Enough: Dynamic Retrieval and Persistent Homology for Dataset Distillation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Decoupled dataset distillation (DD) compresses large corpora into a few synthetic images by matching a frozen teacher's statistics. However, current residual-matching pipelines rely on static real patches, creating a fit-complexity gap and a pull-to-anchor effect that reduce intra-class diversity and hurt generalization. To address these issues, we introduce RETA -- a Retrieval and Topology Alignment framework for decoupled DD. First, Dynamic Retrieval Connection (DRC) selects a real patch from a prebuilt pool by minimizing a fit-complexity score in teacher feature space; the chosen patch is injected via a residual connection to tighten feature fit while controlling injected complexity. Second, Persistent Topology Alignment (PTA) regularizes synthesis with persistent homology: we build a mutual k-NN feature graph, compute persistence images of components and loops, and penalize topology discrepancies between real and synthetic sets, mitigating pull-to-anchor effect. Across CIFAR-100, Tiny-ImageNet, ImageNet-1K, and multiple ImageNet subsets, RETA consistently outperforms various baselines under comparable time and memory, especially reaching 64.3% top-1 accuracy on ImageNet-1K with ResNet-18 at 50 images per class, +3.1% over the best prior.", "AI": {"tldr": "该论文提出了一种用于解耦数据集蒸馏的框架RETA，通过动态检索和拓扑对齐来提高合成图像的质量。", "motivation": "现有的残差匹配管道依赖于静态的真实补丁，导致了模型复杂性和拟合程度之间的差距以及锚点吸引效应，从而减少了类内多样性并影响泛化能力。", "method": "RETA框架包括动态检索连接（DRC）和持久拓扑对齐（PTA）。DRC通过最小化教师特征空间中的拟合复杂度分数从预构建的池中选择真实补丁。PTA利用持久同调来规范合成，计算持久图像并惩罚真实与合成数据集之间的拓扑差异。", "result": "RETA在多个基准测试上表现出色，在ImageNet-1K上实现了64.3%的top-1精度，比最佳先前结果高出+3.1%。", "conclusion": "通过引入动态检索和持久同调对齐，该方法有效提高了数据集蒸馏的质量，并显著提升了模型性能。"}}
{"id": "2602.24134", "pdf": "https://arxiv.org/pdf/2602.24134", "abs": "https://arxiv.org/abs/2602.24134", "authors": ["Zhengren Wang", "Dongsheng Ma", "Huaping Zhong", "Jiayu Li", "Wentao Zhang", "Bin Wang", "Conghui He"], "title": "AgenticOCR: Parsing Only What You Need for Efficient Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The expansion of retrieval-augmented generation (RAG) into multimodal domains has intensified the challenge for processing complex visual documents, such as financial reports. While page-level chunking and retrieval is a natural starting point, it creates a critical bottleneck: delivering entire pages to the generator introduces excessive extraneous context. This not only overloads the generator's attention mechanism but also dilutes the most salient evidence. Moreover, compressing these information-rich pages into a limited visual token budget further increases the risk of hallucinations. To address this, we introduce AgenticOCR, a dynamic parsing paradigm that transforms optical character recognition (OCR) from a static, full-text process into a query-driven, on-demand extraction system. By autonomously analyzing document layout in a \"thinking with images\" manner, AgenticOCR identifies and selectively recognizes regions of interest. This approach performs on-demand decompression of visual tokens precisely where needed, effectively decoupling retrieval granularity from rigid page-level chunking. AgenticOCR has the potential to serve as the \"third building block\" of the visual document RAG stack, operating alongside and enhancing standard Embedding and Reranking modules. Experimental results demonstrate that AgenticOCR improves both the efficiency and accuracy of visual RAG systems, achieving expert-level performance in long document understanding. Code and models are available at https://github.com/OpenDataLab/AgenticOCR.", "AI": {"tldr": "本文介绍了AgenticOCR，一种针对复杂视觉文档（如财务报告）的动态解析方法，通过在需求驱动下识别和提取感兴趣区域来提高检索增强生成系统的表现。", "motivation": "传统的页级分割与检索方法会导致过多冗余信息进入生成器，这不仅增加了注意力机制的负担，还稀释了关键证据，并且可能因视觉标记压缩而引发幻觉。为解决这一问题，本文提出了AgenticOCR，以期减少不必要的上下文干扰并增强系统表现。", "method": "AgenticOCR将光学字符识别（OCR）从静态全文本处理转变为需求驱动的动态解析过程。它通过自主分析文档布局，并根据需要选择性地识别感兴趣的区域，实现视觉标记的按需解压缩。这种方法有效地分离了检索粒度与固定页级分块。", "result": "实验结果表明，AgenticOCR能够提高视觉RAG系统的效率和准确性，在长文档理解方面达到了专家级别的表现。", "conclusion": "AgenticOCR作为一种动态解析方案，为解决复杂视觉文档的处理难题提供了新思路。它通过在需求驱动下识别与提取感兴趣区域的方法有效提升了检索增强生成系统的表现，并且具有成为视觉文档RAG栈第三构建块的潜力。"}}
{"id": "2602.24133", "pdf": "https://arxiv.org/pdf/2602.24133", "abs": "https://arxiv.org/abs/2602.24133", "authors": ["Sifan Zhou", "Jiahao Nie", "Ziyu Zhao", "Yichao Cao", "Xiaobo Lu"], "title": "FocusTrack: One-Stage Focus-and-Suppress Framework for 3D Point Cloud Object Tracking", "categories": ["cs.CV"], "comment": "Acceptted in ACM MM 2025", "summary": "In 3D point cloud object tracking, the motion-centric methods have emerged as a promising avenue due to its superior performance in modeling inter-frame motion. However, existing two-stage motion-based approaches suffer from fundamental limitations: (1) error accumulation due to decoupled optimization caused by explicit foreground segmentation prior to motion estimation, and (2) computational bottlenecks from sequential processing. To address these challenges, we propose FocusTrack, a novel one-stage paradigms tracking framework that unifies motion-semantics co-modeling through two core innovations: Inter-frame Motion Modeling (IMM) and Focus-and-Suppress Attention. The IMM module employs a temp-oral-difference siamese encoder to capture global motion patterns between adjacent frames. The Focus-and-Suppress attention that enhance the foreground semantics via motion-salient feature gating and suppress the background noise based on the temporal-aware motion context from IMM without explicit segmentation. Based on above two designs, FocusTrack enables end-to-end training with compact one-stage pipeline. Extensive experiments on prominent 3D tracking benchmarks, such as KITTI, nuScenes, and Waymo, demonstrate that the FocusTrack achieves new SOTA performance while running at a high speed with 105 FPS.", "AI": {"tldr": "提出了一种新的单阶段点云目标跟踪框架FocusTrack，通过统一的运动语义共模进行3D点云对象跟踪。", "motivation": "现有的双阶段基于运动的方法存在误差累积和计算瓶颈的问题。本文旨在解决这些问题，提高3D点云物体追踪的性能。", "method": "该方法包含两个核心模块：Inter-frame Motion Modeling（帧间运动建模）和Focus-and-Suppress Attention（聚焦与抑制注意）。前者使用一个时差孪生编码器来捕捉相邻帧之间的全局运动模式。后者通过运动显著特征门控增强前景语义，基于从IMM获得的时态感知运动上下文抑制背景噪声。", "result": "在KITTI、nuScenes和Waymo等主要3D跟踪基准测试中，FocusTrack达到了新的SOTA性能，并且运行速度高达105FPS。", "conclusion": "本文提出的单阶段框架通过统一的运动语义共模提高了3D点云对象追踪的性能。实验结果表明，在不牺牲精度的情况下显著提升了运行效率。"}}
{"id": "2602.24121", "pdf": "https://arxiv.org/pdf/2602.24121", "abs": "https://arxiv.org/abs/2602.24121", "authors": ["Tyler Han", "Siyang Shen", "Rohan Baijal", "Harine Ravichandiran", "Bat Nemekhbold", "Kevin Huang", "Sanghun Jung", "Byron Boots"], "title": "Planning from Observation and Interaction", "categories": ["cs.RO"], "comment": null, "summary": "Observational learning requires an agent to learn to perform a task by referencing only observations of the performed task. This work investigates the equivalent setting in real-world robot learning where access to hand-designed rewards and demonstrator actions are not assumed. To address this data-constrained setting, this work presents a planning-based Inverse Reinforcement Learning (IRL) algorithm for world modeling from observation and interaction alone. Experiments conducted entirely in the real-world demonstrate that this paradigm is effective for learning image-based manipulation tasks from scratch in under an hour, without assuming prior knowledge, pre-training, or data of any kind beyond task observations. Moreover, this work demonstrates that the learned world model representation is capable of online transfer learning in the real-world from scratch. In comparison to existing approaches, including IRL, RL, and Behavior Cloning (BC), which have more restrictive assumptions, the proposed approach demonstrates significantly greater sample efficiency and success rates, enabling a practical path forward for online world modeling and planning from observation and interaction. Videos and more at: https://uwrobotlearning.github.io/mpail2/.", "AI": {"tldr": "该论文提出了一种基于观察和交互的逆向强化学习算法，用于从零开始在现实世界中学习图像操作任务。", "motivation": "解决真实环境中机器人学习的问题，其中没有手设计的奖励和示范者动作，并且需要在数据受限的情况下进行学习。", "method": "提出了一种规划为基础的逆向强化学习（IRL）算法，仅通过观察和交互来构建世界模型。", "result": "实验结果表明，在无需任何先验知识或预训练的数据下，该方法能够在不到一小时内从零开始学习图像操作任务，并且其样本效率和成功率均显著优于现有技术。", "conclusion": "这种方法提供了一种在真实世界中进行在线世界建模和规划的有效途径。"}}
{"id": "2602.24119", "pdf": "https://arxiv.org/pdf/2602.24119", "abs": "https://arxiv.org/abs/2602.24119", "authors": ["James L. Zainaldin", "Cameron Pattison", "Manuela Marai", "Jacob Wu", "Mark J. Schiefsky"], "title": "Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek", "categories": ["cs.CL", "cs.AI"], "comment": "Article + supplementary information", "summary": "This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.", "AI": {"tldr": "该论文通过系统的人工评估，研究了大型语言模型（LLM）在古代希腊文翻译中的表现，并探讨术语稀有性对翻译效果的影响。", "motivation": "为了填补文献中关于LLM在低资源古代语言翻译能力的空白，特别是对于医学文本的处理情况，该论文旨在通过人类专家评估来评价LLM在古希腊语翻译上的性能。", "method": "使用三个商业LLM（Claude、Gemini和ChatGPT）对来自盖伦的作品《论混合》和《根据种类组成药物论》中的段落进行翻译。采用标准自动化评估指标以及修改后的MQM框架由领域专家团队来评估所有60次翻译。", "result": "在已有英文翻译的文本上，LLMs表现良好（平均MQM评分为95.2/100），质量接近专家水平。而对于未被翻译过的药物学文本，总体评分较低（79.9/100）但差异大，主要由术语密度高的两段驱动；排除这两段后，得分与已译文本相差仅4分以内。术语稀有性是预测翻译失败的一个强有力指标。", "conclusion": "LLMs在古典研究中具有应用潜力，但仍需进一步优化以处理低资源语言中的罕见术语问题，并且自动化评估工具对于识别高质量的翻译仍不够精确。"}}
{"id": "2602.24111", "pdf": "https://arxiv.org/pdf/2602.24111", "abs": "https://arxiv.org/abs/2602.24111", "authors": ["Vikash Singh", "Debargha Ganguly", "Haotian Yu", "Chengwei Zhou", "Prerna Singh", "Brandon Lee", "Vipin Chaudhary", "Gourav Datta"], "title": "Toward Guarantees for Clinical Reasoning in Vision Language Models via Formal Verification", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LO"], "comment": null, "summary": "Vision-language models (VLMs) show promise in drafting radiology reports, yet they frequently suffer from logical inconsistencies, generating diagnostic impressions unsupported by their own perceptual findings or missing logically entailed conclusions. Standard lexical metrics heavily penalize clinical paraphrasing and fail to capture these deductive failures in reference-free settings. Toward guarantees for clinical reasoning, we introduce a neurosymbolic verification framework that deterministically audits the internal consistency of VLM-generated reports. Our pipeline autoformalizes free-text radiographic findings into structured propositional evidence, utilizing an SMT solver (Z3) and a clinical knowledge base to verify whether each diagnostic claim is mathematically entailed, hallucinated, or omitted. Evaluating seven VLMs across five chest X-ray benchmarks, our verifier exposes distinct reasoning failure modes, such as conservative observation and stochastic hallucination, that remain invisible to traditional metrics. On labeled datasets, enforcing solver-backed entailment acts as a rigorous post-hoc guarantee, systematically eliminating unsupported hallucinations to significantly increase diagnostic soundness and precision in generative clinical assistants.", "AI": {"tldr": "本文提出了一个神经符号验证框架，用于检查视觉语言模型生成的放射报告内部的一致性。", "motivation": "为了确保临床推理的质量，解决现有视觉语言模型在逻辑一致性方面的问题，该研究开发了一个新的验证方法来审计生成报告的准确性。", "method": "通过自动化将自由文本放射学发现转换为结构化命题证据，并使用SMT求解器和临床知识库进行验证，以确定诊断声明是被支持、错误陈述还是遗漏。", "result": "该框架在七个视觉语言模型中识别出不同类型的推理故障模式，在有标签数据集上通过强制执行逻辑一致性显著提高辅助生成的准确性和精确度。", "conclusion": "本文提出的方法为保证临床推理的质量提供了新的途径，揭示了现有模型中的推理错误，并提高了诊断的一致性。"}}
{"id": "2602.24100", "pdf": "https://arxiv.org/pdf/2602.24100", "abs": "https://arxiv.org/abs/2602.24100", "authors": ["Richard Csaky"], "title": "Artificial Agency Program: Curiosity, compression, and communication in agents", "categories": ["cs.AI", "cs.LG"], "comment": "This is a working draft. Feedback and criticism is most welcome", "summary": "This paper presents the Artificial Agency Program (AAP), a position and research agenda for building AI systems as reality embedded, resource-bounded agents whose development is driven by curiosity-as-learning-progress under physical and computational constraints. The central thesis is that AI is most useful when treated as part of an extended human--tool system that increases sensing, understanding, and actuation capability while reducing friction at the interface between people, tools, and environments. The agenda unifies predictive compression, intrinsic motivation, empowerment and control, interface quality (unification), and language/self-communication as selective information bottlenecks. We formulate these ideas as a falsifiable program with explicit costs, staged experiments, and a concrete multimodal tokenized testbed in which an agent allocates limited budget among observation, action, and deliberation. The aim is to provide a conceptual and experimental framework that connects intrinsic motivation, information theory, thermodynamics, bounded rationality, and modern reasoning systems", "AI": {"tldr": "该论文提出了人工智能代理程序（AAP），旨在通过物理和计算约束下的好奇心驱动学习来构建有现实嵌入、资源受限的AI系统。", "motivation": "动机在于将AI视为增强人类工具系统的部分，以提高感知、理解和执行能力，并减少人与环境之间的摩擦。", "method": "该研究统一了预测压缩、内在动机、赋权和控制、界面质量以及自通信作为选择性信息瓶颈。通过多模式标记化测试平台，在预算有限的情况下分配观察、行动和思考。", "result": "提出了一个概念性和实验性的框架，连接了内在动机、信息论、热力学、有界理性及现代推理系统。", "conclusion": "AAP为构建实用的AI代理提供了一种可行的方法，并通过明确的成本、阶段化实验以及多模态令牌测试床来验证这一理论。"}}
{"id": "2602.24097", "pdf": "https://arxiv.org/pdf/2602.24097", "abs": "https://arxiv.org/abs/2602.24097", "authors": ["Yue Xie", "Zizhen Xu", "William Beazley", "Fumiya Iida"], "title": "Bi-level RL-Heuristic Optimization for Real-world Winter Road Maintenance", "categories": ["cs.AI"], "comment": null, "summary": "Winter road maintenance is critical for ensuring public safety and reducing environmental impacts, yet existing methods struggle to manage large-scale routing problems effectively and mostly reply on human decision. This study presents a novel, scalable bi-level optimization framework, validated on real operational data on UK strategic road networks (M25, M6, A1), including interconnected local road networks in surrounding areas for vehicle traversing, as part of the highway operator's efforts to solve existing planning challenges. At the upper level, a reinforcement learning (RL) agent strategically partitions the road network into manageable clusters and optimally allocates resources from multiple depots. At the lower level, a multi-objective vehicle routing problem (VRP) is solved within each cluster, minimizing the maximum vehicle travel time and total carbon emissions. Unlike existing approaches, our method handles large-scale, real-world networks efficiently, explicitly incorporating vehicle-specific constraints, depot capacities, and road segment requirements. Results demonstrate significant improvements, including balanced workloads, reduced maximum travel times below the targeted two-hour threshold, lower emissions, and substantial cost savings. This study illustrates how advanced AI-driven bi-level optimization can directly enhance operational decision-making in real-world transportation and logistics.", "AI": {"tldr": "本文提出了一种用于冬季道路维护的新型双层优化框架，通过强化学习和多目标车辆路径规划问题解决大规模网络下的资源分配与调度。", "motivation": "现有方法难以有效管理大型路由问题，依赖人力决策。本文旨在开发一种高效的优化策略来应对实际操作中的挑战，提升公共安全并减少环境影响。", "method": "该框架采用强化学习进行道路网络分割和资源分配，并通过解决多目标车辆路径规划问题以最小化最大行驶时间和总碳排放量，在真实世界的大规模网络中实现高效运行。", "result": "实验结果显示，所提出的方法能够平衡工作负载、减少最长时间在两小时之内、降低碳排放量以及显著的成本节约。", "conclusion": "研究表明先进的人工智能双层优化技术可以直接改善现实世界的运输和物流决策过程。"}}
{"id": "2602.24080", "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "categories": ["cs.AI", "cs.SD"], "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.", "AI": {"tldr": "本文进行了一项针对语音到语音（S2S）系统的图灵测试，旨在评估这些系统在对话中的拟人化程度。", "motivation": "当前的语音交互系统能否像人类一样交谈是一个重要问题。通过图灵测试来衡量这一差距并诊断瓶颈。", "method": "收集了来自9个最先进的S2S系统与28名参与者之间的2,968个人类判断，开发了一套18个维度的人性化程度分类，并使用这些数据训练了一个可解释的模型以区分人类和机器对话。", "result": "没有现有的评估S2S系统通过图灵测试。瓶颈在于非语言特征、情感表达以及会话个性而非语义理解。提出的模型能准确透明地区分人机对话，为自动化的人性化评价提供了工具。", "conclusion": "这项研究建立了第一个针对语音到语音系统的拟人度评估框架，并揭示了改进方向，促进了更加人性化的对话AI系统的发展。"}}
{"id": "2602.24071", "pdf": "https://arxiv.org/pdf/2602.24071", "abs": "https://arxiv.org/abs/2602.24071", "authors": ["Jiajia Li", "Jiliang Hu", "Ziyi Pan", "Chong Chen", "Zuchao Li", "Ping Wang", "Lefei Zhang"], "title": "SongSong: A Time Phonograph for Chinese SongCi Music from Thousand of Years Away", "categories": ["cs.SD", "cs.CL"], "comment": "9 pages, 6 figures, accepted by AAAI 2025", "summary": "Recently, there have been significant advancements in music generation. However, existing models primarily focus on creating modern pop songs, making it challenging to produce ancient music with distinct rhythms and styles, such as ancient Chinese SongCi. In this paper, we introduce SongSong, the first music generation model capable of restoring Chinese SongCi to our knowledge. Our model first predicts the melody from the input SongCi, then separately generates the singing voice and accompaniment based on that melody, and finally combines all elements to create the final piece of music. Additionally, to address the lack of ancient music datasets, we create OpenSongSong, a comprehensive dataset of ancient Chinese SongCi music, featuring 29.9 hours of compositions by various renowned SongCi music masters. To assess SongSong's proficiency in performing SongCi, we randomly select 85 SongCi sentences that were not part of the training set for evaluation against SongSong and music generation platforms such as Suno and SkyMusic. The subjective and objective outcomes indicate that our proposed model achieves leading performance in generating high-quality SongCi music.", "AI": {"tldr": "介绍了一种名为SongSong的音乐生成模型，用于恢复中国古代宋词的音乐。", "motivation": "现有音乐生成模型主要专注于现代流行歌曲的创作，而缺乏古代音乐（如中国宋词）的独特节奏和风格。为了填补这一空白，并解决古乐数据集不足的问题。", "method": "SongSong通过预测输入宋词旋律，然后分别生成歌声伴奏，最后将所有元素结合成最终的音乐作品。", "result": "与Suno、SkyMusic等平台相比，在对85首未训练过的宋词进行主观和客观评估时，SongSong表现出色，能够生成高质量的宋代音乐。", "conclusion": "SongSong模型在恢复古代中国宋词音乐方面取得了领先的性能。"}}
{"id": "2602.24055", "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Taïk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "categories": ["cs.AI", "cs.SE"], "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.", "AI": {"tldr": "提出了CIRCLE框架，旨在评估AI技术在实际部署中的表现。", "motivation": "现有评估方法如MLOps和基准测试无法提供决策者所需的系统性证据来了解AI技术在真实用户环境下的表现。", "method": "CIRCLE是一个六阶段生命周期框架，将利益相关者的担忧转化为可测量的信号，并通过现场测试、红队测试等集成方法生成系统的知识。", "result": "该框架能够产生跨地点比较且适应本地上下文的知识，以便基于实际效果而非理论能力进行治理。", "conclusion": "CIRCLE提供了一种结构化的方法来评估AI技术在真实世界中的表现和影响。"}}
{"id": "2602.24044", "pdf": "https://arxiv.org/pdf/2602.24044", "abs": "https://arxiv.org/abs/2602.24044", "authors": ["Ferran Agullo", "Joan Oliveras", "Chen Wang", "Alberto Gutierrez-Torre", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "title": "Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "comment": "journal extension of the workshop paper titled as \"A data-driven ml approach for maximizing performance in llm-adapter serving\"", "summary": "Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.", "AI": {"tldr": "提出了一种数据驱动的优化方法，用于在分布式LLM适配器服务中最大化GPU效率。", "motivation": "现有的工作主要关注延迟最小化，而资源利用率通过吞吐量最大化的研究较少。该论文旨在解决如何利用最少数量的GPU来避免请求饥饿和GPU内存错误的同时提供高效的服务。", "method": "采用数字孪生（DT）模拟真实系统动态，并训练机器学习模型进行性能估计。最后应用贪婪算法根据这些估算值最大化GPU效率。", "result": "实验结果表明，该管道显著提高了GPU效率并减少了维持目标工作负载所需的GPU数量。", "conclusion": "提出的流水线不仅提升了GPU效率，还展示了其在延迟最小化等其他优化目标上的灵活性和适用性，为未来的大规模LLM服务基础设施提供了潜在的价值。"}}
{"id": "2602.24043", "pdf": "https://arxiv.org/pdf/2602.24043", "abs": "https://arxiv.org/abs/2602.24043", "authors": ["Yingxuan You", "Ren Li", "Corentin Dumery", "Cong Cao", "Hao Li", "Pascal Fua"], "title": "Spatio-Temporal Garment Reconstruction Using Diffusion Mapping via Pattern Coordinates", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2504.08353", "summary": "Reconstructing 3D clothed humans from monocular images and videos is a fundamental problem with applications in virtual try-on, avatar creation, and mixed reality. Despite significant progress in human body recovery, accurately reconstructing garment geometry, particularly for loose-fitting clothing, remains an open challenge. We propose a unified framework for high-fidelity 3D garment reconstruction from both single images and video sequences. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn expressive garment shape priors in 2D UV space. Leveraging these priors, we introduce a mapping model that establishes correspondences between image pixels, UV pattern coordinates, and 3D geometry, enabling accurate and detailed garment reconstruction from single images. We further extend this formulation to dynamic reconstruction by introducing a spatio-temporal diffusion scheme with test-time guidance to enforce long-range temporal consistency. We also develop analytic projection-based constraints that preserve image-aligned geometry in visible regions while enforcing coherent completion in occluded areas over time. Although trained exclusively on synthetically simulated cloth data, our method generalizes well to real-world imagery and consistently outperforms existing approaches on both tight- and loose-fitting garments. The reconstructed garments preserve fine geometric detail while exhibiting realistic dynamic motion, supporting downstream applications such as texture editing, garment retargeting, and animation.", "AI": {"tldr": "本文提出了一个统一框架，用于从单张图像和视频序列中进行高保真度的三维服装重建。", "motivation": "尽管在人体恢复方面取得了重大进展，但准确地重建宽松衣服的服装几何形状仍然是一个开放性挑战。这项研究旨在解决这一问题，特别是在虚拟试穿、头像创建和混合现实等应用中的需求。", "method": "该方法结合了隐式缝合图案（ISP）与生成扩散模型来学习二维UV空间中具有表达性的服装形状先验，并引入映射模型在图像像素、UV图案坐标以及三维几何之间建立对应关系，从而实现从单张图像准确细致地重建服装。此外还扩展到动态重建，通过测试时的引导方案确保长时间的一致性，并开发了保持可见区域的投影基约束来完成遮挡区域。", "result": "尽管仅在合成模拟布料数据上进行训练，但该方法很好地泛化到了现实世界图像，在紧密和宽松服装上的表现超过了现有方法。重建的服装保留了精细几何细节并展现出逼真的动态运动。", "conclusion": "所提出的方法提供了一种用于三维服装从单张图像和视频序列中高保真度重建的有效方案，支持下游应用如纹理编辑、服装再定位和动画制作等。"}}
{"id": "2602.24042", "pdf": "https://arxiv.org/pdf/2602.24042", "abs": "https://arxiv.org/abs/2602.24042", "authors": ["Zohar Barak", "Inbbal Talgam-Cohen"], "title": "Stochastic Knapsack -- Semi-Adaptivity Gaps and Improved Approximation", "categories": ["cs.DS"], "comment": null, "summary": "In stochastic combinatorial optimization, algorithms differ in their adaptivity: whether or not they query realized randomness and adapt to it. Dean et al. (FOCS '04) formalize the adaptivity gap, which compares the performance of fully adaptive policies to that of non-adaptive ones. We revisit the fundamental Stochastic Knapsack problem of Dean et al., where items have deterministic values and independent stochastic sizes. A policy packs items sequentially, stopping at the first knapsack overflow or before. We focus on the challenging risky variant, in which an overflow forfeits all accumulated value, and study the problem through the lens of semi-adaptivity: We measure the power of $k$ adaptive queries for constant $k$ through the notions of $0$-$k$ semi-adaptivity gap (the gap between $k$-semi-adaptive and non-adaptive policies), and $k$-$n$ semi-adaptivity gap (between fully adaptive and $k$-semi-adaptive policies). Our first contribution is to improve the classic results of Dean et al. by giving tighter upper and lower bounds on the adaptivity gap. Our second contribution is a smoother interpolation between non-adaptive and fully-adaptive policies, with the rationale that when full adaptivity is unrealistic (due to its complexity or query cost), limited adaptivity may be a desirable middle ground. We quantify the $1$-$n$ and $k$-$n$ semi-adaptivity gaps, showing how well $k$ queries approximate the fully-adaptive policy. We complement these bounds by quantifying the $0$-$1$ semi-adaptivity gap, i.e., the improvement from investing in a single query over no adaptivity. As part of our analysis, we develop a 3-step \"Simplify-Equalize-Optimize\" approach to analyzing adaptive decision trees, with possible applications to the study of semi-adaptivity in additional stochastic combinatorial optimization problems.", "AI": {"tldr": "本文研究了在随机背包问题中，通过半自适应策略（允许有限次数的查询）来优化解的问题，并提供了更好的近似算法。", "motivation": "Dean等人首次引入了自适应差距的概念，即完全自适应策略与非自适应策略之间的性能差异。作者希望改进经典结果并研究在随机背包问题中有限次查询的影响。", "method": "本文采用了“简化-等价化-优化”的三步分析法来处理自适应决策树，并通过量化0-k半自适应差距和k-n半自适应差距，探讨了完全自适应策略与非自适应策略之间的差异。", "result": "作者改进了经典结果并给出了更紧的上界和下界。他们还展示了单个查询相对于无适应性的改进以及有限次数查询如何接近完全自适应策略的表现。", "conclusion": "研究证明，在随机背包问题中，通过引入有限次查询可以更好地平衡性能与复杂性，并为未来半自适应策略的研究提供了新的分析方法。"}}
{"id": "2602.24041", "pdf": "https://arxiv.org/pdf/2602.24041", "abs": "https://arxiv.org/abs/2602.24041", "authors": ["Xingyu Zhu", "Kesen Zhao", "Liang Yi", "Shuo Wang", "Zhicai Wang", "Beier Zhu", "Hanwang Zhang"], "title": "Look Carefully: Adaptive Visual Reinforcements in Multimodal Large Language Models for Hallucination Mitigation", "categories": ["cs.CV"], "comment": "ICLR 2026", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language reasoning, yet they remain vulnerable to hallucination, where generated content deviates from visual evidence. Existing mitigation strategies either require costly supervision during training or introduce additional latency at inference time. Recent vision enhancement methods attempt to address this issue by reinforcing visual tokens during decoding, but they typically inject all tokens indiscriminately, which causes interference from background regions and distracts the model from critical cues. To overcome this challenge, we propose Adaptive Visual Reinforcement (AIR), a training-free framework for MLLMs. AIR consists of two components. Prototype-based token reduction condenses the large pool of visual tokens into a compact subset to suppress redundancy. OT-guided patch reinforcement quantifies the alignment between hidden states and patch embeddings to selectively integrate the most consistent patches into feed-forward layers. As a result, AIR enhances the model's reliance on salient visual information and effectively mitigates hallucination. Extensive experiments across representative MLLMs demonstrate that AIR substantially reduces hallucination while preserving general capabilities, establishing it as an effective solution for building reliable MLLMs.", "AI": {"tldr": "本文提出了一种无监督训练的框架AIR，用于增强多模态大型语言模型在视觉信息上的依赖性，以减少幻觉现象。", "motivation": "现有方法在降低多模态大语言模型中的幻觉问题时存在高昂的成本或额外的时间延迟。为此，研究者开发了一种无需额外监督且不增加推理时间的框架AIR来解决这一挑战。", "method": "本文提出的方法包括两个组件：原型基元令牌减少和OT引导的补丁强化。前者通过压缩视觉令牌池以消除冗余；后者量化隐藏状态与补丁嵌入之间的对齐，选择性地将最一致的补丁集成到前馈层中。", "result": "实验结果显示，AIR框架显著减少了模型的幻觉现象，并保持了其通用能力。", "conclusion": "通过引入AIR框架，多模态大语言模型能够更依赖于关键视觉信息，从而有效地降低幻觉风险，提高整体可靠性。"}}
{"id": "2602.24040", "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna Pásztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.", "AI": {"tldr": "RewardUQ是一个统一框架，用于评估奖励模型中的不确定性量化。", "motivation": "大多数方法依赖于点估计的奖励值，并未考虑来自有限人工反馈的先验不确定性。这种忽视可能导致高昂的人工标注成本以及过度优化问题。", "method": "通过引入一个系统化评价不确定性的框架RewardUQ，比较常用的方法并提出新的排名策略来简化对比。", "result": "实验结果显示模型大小和初始化对性能影响最大，许多先前工作可能受益于不同的设计选择。开源了Python包以支持新方法的发展与应用部署。", "conclusion": "该研究展示了系统化评价奖励模型不确定性的必要性，并发布了开源工具以便进一步研究与发展。"}}
{"id": "2602.24037", "pdf": "https://arxiv.org/pdf/2602.24037", "abs": "https://arxiv.org/abs/2602.24037", "authors": ["Vanya Priscillia Bendatu", "Yao Lu"], "title": "Portfolio Reinforcement Learning with Scenario-Context Rollout", "categories": ["cs.AI"], "comment": null, "summary": "Market regime shifts induce distribution shifts that can degrade the performance of portfolio rebalancing policies. We propose macro-conditioned scenario-context rollout (SCR) that generates plausible next-day multivariate return scenarios under stress events. However, doing so faces new challenges, as history will never tell what would have happened differently. As a result, incorporating scenario-based rewards from rollouts introduces a reward--transition mismatch in temporal-difference learning, destabilizing RL critic training. We analyze this inconsistency and show it leads to a mixed evaluation target. Guided by this analysis, we construct a counterfactual next state using the rollout-implied continuations and augment the critic agent's bootstrap target. Doing so stabilizes the learning and provides a viable bias-variance tradeoff. In out-of-sample evaluations across 31 distinct universes of U.S. equity and ETF portfolios, our method improves Sharpe ratio by up to 76% and reduces maximum drawdown by up to 53% compared with classic and RL-based portfolio rebalancing baselines.", "AI": {"tldr": "提出了一种基于宏观经济条件的场景上下文回滚策略，以生成在压力事件下的多变量回报场景，改善了投资组合再平衡政策的表现。", "motivation": "市场制度变化会诱导分布改变，从而降低投资组合再平衡政策的效果。为解决这一问题并引入场景基础奖励时存在的奖励-转移不匹配问题，需要一种新的方法来稳定学习过程。", "method": "通过构建反事实的下一步状态，并增加评估代理的自助目标以应对回滚引发的奖励一致性问题，从而稳定了学习过程。", "result": "在31种不同的美国股票和ETF投资组合中进行外样检验后发现，该方法最多提高了76%的夏普比率并减少了53%的最大回撤幅度。", "conclusion": "通过使用宏观条件下的场景-上下文回滚策略改进了投资组合再平衡政策的表现，并且这种方法在现实世界的应用中也表现出色。"}}
{"id": "2602.24036", "pdf": "https://arxiv.org/pdf/2602.24036", "abs": "https://arxiv.org/abs/2602.24036", "authors": ["Abhishek Kulkarni", "Sharon Lynn Chu"], "title": "Designing AI Tutors for Interest-Based Learning: Insights from Human Instructors", "categories": ["cs.HC"], "comment": null, "summary": "Interest-based learning (IBL) is a paradigm of instruction in which educational content is contextualized using learners' interests to enhance content relevance. IBL has been shown to result in improved learning outcomes. Unfortunately, high effort is needed for instructors to design and deliver IBL content for individual students. LLMs in the form of AI tutors may allow for IBL to scale across many students. Designing an AI tutor for IBL, however, first requires an understanding of how IBL is implemented in teaching scenarios. This paper presents a study that seeks to derive this understanding from an analysis of how human instructors design and deliver IBL content. We studied 14 one-to-one online tutoring sessions (28 participants) in which tutors designed and delivered a lesson tailored to a student's self-identified interest. Using lesson artifacts, tutoring transcripts, interviews, and questionnaires, findings include themes on how tutors integrate interests during instruction and why. Finally, actionable design implications are presented for LLM-powered AI tutors that aim to deliver IBL at scale.", "AI": {"tldr": "该论文旨在通过分析人类教师如何设计和交付基于兴趣的学习内容，为人工智能辅导系统提供实施建议。", "motivation": "虽然基于兴趣学习（IBL）能提高学习效果，但实现这种个性化教学需要大量的人力。使用大语言模型作为AI导师可以解决这一问题，使个性化教育能够大规模推广。", "method": "研究分析了14个一对一在线辅导课程中的教师如何根据学生自我识别的兴趣设计和交付课程，并通过课件、对话记录、访谈及问卷调查提炼出将兴趣融入教学的方法及其原因。", "result": "研究发现了一些关于人类老师在教学中整合兴趣的模式和理由，为使用LLM驱动的AI导师提供实施建议。", "conclusion": "该研究表明了基于人类教师经验设计大语言模型驱动的个性化学习系统的重要性和可行性。"}}
{"id": "2602.24030", "pdf": "https://arxiv.org/pdf/2602.24030", "abs": "https://arxiv.org/abs/2602.24030", "authors": ["Fangyu Sun", "Fanxing Li", "Yu Hu", "Linzuo Zhang", "Yueqian Liu", "Wenxian Yu", "Danping Zou"], "title": "Curriculum Reinforcement Learning for Quadrotor Racing with Random Obstacles", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous drone racing has attracted increasing interest as a research topic for exploring the limits of agile flight. However, existing studies primarily focus on obstacle-free racetracks, while the perception and dynamic challenges introduced by obstacles remain underexplored, often resulting in low success rates and limited robustness in real-world flight. To this end, we propose a novel vision-based curriculum reinforcement learning framework for training a robust controller capable of addressing unseen obstacles in drone racing. We combine multi-stage cu rriculum learning, domain randomization, and a multi-scene updating strategy to address the conflicting challenges of obstacle avoidance and gate traversal. Our end-to-end control policy is implemented as a single network, allowing high-speed flight of quadrotors in environments with variable obstacles. Both hardware-in-the-loop and real-world experiments demonstrate that our method achieves faster lap times and higher success rates than existing approaches, effectively advancing drone racing in obstacle-rich environments. The video and code are available at: https://github.com/SJTU-ViSYS-team/CRL-Drone-Racing.", "AI": {"tldr": "提出了一种基于课程强化学习的无人机竞速控制方法，用于应对环境中的随机障碍。", "motivation": "现有的无人机竞速研究主要集中在无障碍赛道上，但在具有障碍物的实际环境中成功率低且鲁棒性不足。为解决这一问题，提出了结合多阶段课程学习、领域随机化和多场景更新策略的新框架。", "method": "采用一种基于视觉的课程强化学习框架，训练一个单一网络的端到端控制策略，以实现四旋翼在具有可变障碍环境中高速飞行的能力。", "result": "实验结果表明，该方法相比于现有方法，在有障碍物的竞速环境中实现了更快的圈速和更高的成功率。", "conclusion": "所提出的方法有效推进了无人机在复杂环境下的竞速研究。"}}
{"id": "2602.24027", "pdf": "https://arxiv.org/pdf/2602.24027", "abs": "https://arxiv.org/abs/2602.24027", "authors": ["Xingyu Zhu", "Beier Zhu", "Junfeng Fang", "Shuo Wang", "Yin Zhang", "Xiang Wang", "Xiangnan He"], "title": "GuardAlign: Test-time Safety Alignment in Multimodal Large Language Models", "categories": ["cs.CV", "cs.MM"], "comment": "ICLR 2026", "summary": "Large vision-language models (LVLMs) have achieved remarkable progress in vision-language reasoning tasks, yet ensuring their safety remains a critical challenge. Recent input-side defenses detect unsafe images with CLIP and prepend safety prefixes to prompts, but they still suffer from inaccurate detection in complex scenes and unstable safety signals during decoding. To address these issues, we propose GuardAlign, a training-free defense framework that integrates two strategies. First, OT-enhanced safety detection leverages optimal transport to measure distribution distances between image patches and unsafe semantics, enabling accurate identification of malicious regions without additional computational cost. Second, cross-modal attentive calibration strengthens the influence of safety prefixes by adaptively reallocating attention across layers, ensuring that safety signals remain consistently activated throughout generation. Extensive evaluations on six representative MLLMs demonstrate that GuardAlign reduces unsafe response rates by up to 39% on SPA-VL, while preserving utility, achieving an improvement on VQAv2 from 78.51% to 79.21%.", "AI": {"tldr": "提出了一种名为GuardAlign的无需训练的安全防御框架，旨在提高大型视觉语言模型（LVLM）在复杂场景中的安全性。", "motivation": "解决大尺寸视觉-语言模型在输入端检测不安全图像时存在误检的问题以及解码过程中不稳定的安全信号问题。", "method": "首先利用最优传输增强安全性检测，通过计算图像补丁和不安全语义之间的分布距离准确识别恶意区域；其次采用跨模态注意力校准来加强安全前缀的影响，并根据需要重新分配各层的注意权重以确保生成过程中的安全信号保持激活。", "result": "GuardAlign框架在六个代表性多模式语言模型上的评估中显示，相比于其他方法可以将不安全响应率降低最多39%，并且还能提升VQAv2任务的表现从78.51%提高到79.21%。", "conclusion": "提出了一个有效的无需训练的安全防御框架GuardAlign，能够显著改善大型视觉语言模型在复杂场景中的安全性并保持其性能。"}}
{"id": "2602.24021", "pdf": "https://arxiv.org/pdf/2602.24021", "abs": "https://arxiv.org/abs/2602.24021", "authors": ["Zhaolin Cai", "Fan Li", "Huiyu Duan", "Lijun He", "Guangtao Zhai"], "title": "Steering and Rectifying Latent Representation Manifolds in Frozen Multi-modal LLMs for Video Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2026", "summary": "Video anomaly detection (VAD) aims to identify abnormal events in videos. Traditional VAD methods generally suffer from the high costs of labeled data and full training, thus some recent works have explored leveraging frozen multi-modal large language models (MLLMs) in a tuning-free manner to perform VAD. However, their performance is limited as they directly inherit pre-training biases and cannot adapt internal representations to specific video contexts, leading to difficulties in handling subtle or ambiguous anomalies. To address these limitations, we propose a novel intervention framework, termed SteerVAD, which advances MLLM-based VAD by shifting from passively reading to actively steering and rectifying internal representations. Our approach first leverages the gradient-free representational separability analysis (RSA) to identify top attention heads as latent anomaly experts (LAEs) which are most discriminative for VAD. Then a hierarchical meta-controller (HMC) generates dynamic rectification signals by jointly conditioning on global context and these LAE outputs. The signals execute targeted, anisotropic scaling directly upon the LAE representation manifolds, amplifying anomaly-relevant dimensions while suppressing inherent biases. Extensive experiments on mainstream benchmarks demonstrate our method achieves state-of-the-art performance among tuning-free approaches requiring only 1% of training data, establishing it as a powerful new direction for video anomaly detection. The code will be released upon the publication.", "AI": {"tldr": "本文提出了一种新的干预框架SteerVAD，用于改善基于冻结多模态大型语言模型（MLLM）的视频异常检测。", "motivation": "传统的视频异常检测方法由于标注数据成本高昂和完全训练的高代价而面临挑战。直接使用预训练模型的现有方法表现受限，无法适应特定视频上下文。", "method": "SteerVAD通过利用无梯度表征可分离分析（RSA）来识别最具有区分性的注意力头作为潜在异常专家（LAE），然后采用分层元控制器（HMC）生成动态修正信号，直接对LAE表示流形进行针对性的异向缩放。", "result": "实验表明该方法在主流基准测试中实现了调优自由方式下的最佳性能，并只需要1%的数据训练。", "conclusion": "SteerVAD提出了一种新的视频异常检测方向，它通过主动操控内部表征来解决传统基于冻结多模态大型语言模型的挑战。"}}
{"id": "2602.24020", "pdf": "https://arxiv.org/pdf/2602.24020", "abs": "https://arxiv.org/abs/2602.24020", "authors": ["Xiang Feng", "Xiangbo Wang", "Tieshi Zhong", "Chengkai Wang", "Yiting Zhao", "Tianxiang Xu", "Zhenzhong Kuang", "Feiwei Qin", "Xuefei Yin", "Yanming Zhu"], "title": "SR3R: Rethinking Super-Resolution 3D Reconstruction With Feed-Forward Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2026", "summary": "3D super-resolution (3DSR) aims to reconstruct high-resolution (HR) 3D scenes from low-resolution (LR) multi-view images. Existing methods rely on dense LR inputs and per-scene optimization, which restricts the high-frequency priors for constructing HR 3D Gaussian Splatting (3DGS) to those inherited from pretrained 2D super-resolution (2DSR) models. This severely limits reconstruction fidelity, cross-scene generalization, and real-time usability. We propose to reformulate 3DSR as a direct feed-forward mapping from sparse LR views to HR 3DGS representations, enabling the model to autonomously learn 3D-specific high-frequency geometry and appearance from large-scale, multi-scene data. This fundamentally changes how 3DSR acquires high-frequency knowledge and enables robust generalization to unseen scenes. Specifically, we introduce SR3R, a feed-forward framework that directly predicts HR 3DGS representations from sparse LR views via the learned mapping network. To further enhance reconstruction fidelity, we introduce Gaussian offset learning and feature refinement, which stabilize reconstruction and sharpen high-frequency details. SR3R is plug-and-play and can be paired with any feed-forward 3DGS reconstruction backbone: the backbone provides an LR 3DGS scaffold, and SR3R upscales it to an HR 3DGS. Extensive experiments across three 3D benchmarks demonstrate that SR3R surpasses state-of-the-art (SOTA) 3DSR methods and achieves strong zero-shot generalization, even outperforming SOTA per-scene optimization methods on unseen scenes.", "AI": {"tldr": "本文提出了一种直接将稀疏低分辨率视图映射到高分辨率3D高斯点云表示的方法，以实现更精确、更具泛化能力的三维超分辨率重建。", "motivation": "现有方法依赖于密集的低分辨率输入和每场景优化，这限制了高频先验的学习并影响重建的准确性。因此，作者提出了一种直接映射稀疏视图到高分辨表示的方法，以提高模型的泛化能力和实时性。", "method": "SR3R框架通过学习映射网络从稀疏低分辨率视图预测出高质量的高分辨率三维高斯点云，并引入了高斯偏移学习和特征细化来提升重建质量。该方法可以与任何前馈3DGS重构主干配对使用。", "result": "实验结果表明，SR3R在多个基准数据集上超越了现有的最先进方法，在未见场景上的零样本泛化性能也优于最先进的每场景优化方法。", "conclusion": "通过将三维超分辨率重建问题重新定义为直接从稀疏低分辨视图生成高质量高分辨表示的任务，本文所提的SR3R方法能够实现更精确、更具泛化能力和实时性的3D重构。"}}
{"id": "2602.24014", "pdf": "https://arxiv.org/pdf/2602.24014", "abs": "https://arxiv.org/abs/2602.24014", "authors": ["Na Min An", "Yoonna Jang", "Yusuke Hirota", "Ryo Hachiuma", "Isabelle Augenstein", "Hyunjung Shim"], "title": "Interpretable Debiasing of Vision-Language Models for Social Fairness", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages, 30 figures, 13 Tables Accepted to CVPR 2026", "summary": "The rapid advancement of Vision-Language models (VLMs) has raised growing concerns that their black-box reasoning processes could lead to unintended forms of social bias. Current debiasing approaches focus on mitigating surface-level bias signals through post-hoc learning or test-time algorithms, while leaving the internal dynamics of the model largely unexplored. In this work, we introduce an interpretable, model-agnostic bias mitigation framework, DeBiasLens, that localizes social attribute neurons in VLMs through sparse autoencoders (SAEs) applied to multimodal encoders. Building upon the disentanglement ability of SAEs, we train them on facial image or caption datasets without corresponding social attribute labels to uncover neurons highly responsive to specific demographics, including those that are underrepresented. By selectively deactivating the social neurons most strongly tied to bias for each group, we effectively mitigate socially biased behaviors of VLMs without degrading their semantic knowledge. Our research lays the groundwork for future auditing tools, prioritizing social fairness in emerging real-world AI systems.", "AI": {"tldr": "本文介绍了DeBiasLens框架，通过稀疏自编码器对视觉语言模型中的社会属性神经元进行定位和抑制，以减少这些模型的社会偏见。", "motivation": "快速发展的视觉语言模型可能因内部黑箱推理过程而产生无意间的社会偏见。当前的去偏方法主要集中在后处理或测试时算法上，并未深入探究模型内部机制。本文旨在通过一个可解释的、模型无关的方法来减轻这种偏见。", "method": "DeBiasLens框架利用稀疏自编码器在没有对应社会属性标签的情况下训练面部图像或多模态编码器，以发现对特定人口群体高度响应的社会神经元，并选择性地去激活与这些群体相关的最强烈的偏见相关联的神经元。这种方法不降低模型语义知识的同时减轻了社会偏差。", "result": "该方法有效减少了视觉语言模型中的社会偏差行为而不会损害其语义理解能力，为未来的审计工具奠定了基础。", "conclusion": "本文提出的方法DeBiasLens在减少视觉语言模型的社会偏见方面取得了成功，并且没有降低这些系统的性能。这为进一步开发重视社会公正的新兴AI系统铺平了道路。"}}
{"id": "2602.24013", "pdf": "https://arxiv.org/pdf/2602.24013", "abs": "https://arxiv.org/abs/2602.24013", "authors": ["Gustav Schmidt", "Philipp Berens", "Sarah Müller"], "title": "Ordinal Diffusion Models for Color Fundus Images", "categories": ["cs.CV"], "comment": null, "summary": "It has been suggested that generative image models such as diffusion models can improve performance on clinically relevant tasks by offering deep learning models supplementary training data. However, most conditional diffusion models treat disease stages as independent classes, ignoring the continuous nature of disease progression. This mismatch is problematic in medical imaging because continuous pathological processes are typically only observed through coarse, discrete but ordered labels as in ophthalmology for diabetic retinopathy (DR). We propose an ordinal latent diffusion model for generating color fundus images that explicitly incorporates the ordered structure of DR severity into the generation process. Instead of categorical conditioning, we used a scalar disease representation, enabling a smooth transition between adjacent stages. We evaluated our approach using visual realism metrics and classification-based clinical consistency analysis on the EyePACS dataset. Compared to a standard conditional diffusion model, our model reduced the Fréchet inception distance for four of the five DR stages and increased the quadratic weighted $κ$ from 0.79 to 0.87. Furthermore, interpolation experiments showed that the model captured a continuous spectrum of disease progression learned from ordered, coarse class labels.", "AI": {"tldr": "提出了一种用于生成彩色眼底图像的序数扩散模型，该模型将糖尿病视网膜病变（DR）严重程度的有序结构纳入生成过程。", "motivation": "大多数条件扩散模型将疾病阶段视为独立类别，忽略了疾病进程的连续性，在医学成像中这一差异会导致问题，因为病理过程通常是通过粗略、离散但有序的标签观察到的。因此提出了序数潜在扩散模型来解决这个问题。", "method": "使用标量表示疾病而不是分类条件，并在EyePACS数据集上进行了视觉真实性和基于临床一致性分析评估。", "result": "与标准条件扩散模型相比，该模型减少了四个DR阶段中的三个阶段的Fréchet inception距离，并将二次加权κ系数从0.79提高到了0.87。插值实验表明，该模型捕捉了由有序粗分类标签学习到的疾病进展连续光谱。", "conclusion": "序数扩散模型在生成带有DR严重程度顺序结构的眼底图像方面表现出色，并且在视觉真实性和临床一致性方面的性能优于传统的条件扩散模型。"}}
{"id": "2602.24011", "pdf": "https://arxiv.org/pdf/2602.24011", "abs": "https://arxiv.org/abs/2602.24011", "authors": ["Václav Riss", "Vít Krátký", "Robert Pěnička", "Martin Saska"], "title": "Autonomous Inspection of Power Line Insulators with UAV on an Unmapped Transmission Tower", "categories": ["cs.RO"], "comment": "8 pages, 9 figues", "summary": "This paper introduces an online inspection algorithm that enables an autonomous UAV to fly around a transmission tower and obtain detailed inspection images without a prior map of the tower. Our algorithm relies on camera-LiDAR sensor fusion for online detection and localization of insulators. In particular, the algorithm is based on insulator detection using a convolutional neural network, projection of LiDAR points onto the image, and filtering them using the bounding boxes. The detection pipeline is coupled with several proposed insulator localization methods based on DBSCAN, RANSAC, and PCA algorithms. The performance of the proposed online inspection algorithm and camera-LiDAR sensor fusion pipeline is demonstrated through simulation and real-world flights. In simulation, we showed that our single-flight inspection strategy can save up to 24 % of total inspection time, compared to the two-flight strategy of scanning the tower and afterwards visiting the inspection waypoints in the optimal way. In a real-world experiment, the best performing proposed method achieves a mean horizontal and vertical localization error for the insulator of 0.16 +- 0.08 m and 0.16 +- 0.11 m, respectively. Compared to the most relevant approach, the proposed method achieves more than an order of magnitude lower variance in horizontal insulator localization error.", "AI": {"tldr": "该论文提出了一种在线检测算法，使自主飞行器能够在没有先验地图的情况下对输电塔进行详细检查。", "motivation": "为了提高电力线绝缘子的自动巡检效率和准确性，特别是在没有预先测绘的地图情况下。", "method": "基于相机和LiDAR传感器融合技术，利用卷积神经网络检测绝缘子，并通过投影LiDAR点到图像上以及边界框滤波进行定位。此外，还采用了DBSCAN、RANSAC及PCA算法来进一步优化定位。", "result": "模拟实验中，单次飞行策略相比双次飞行策略可以节省高达24%的检查时间；实际测试显示最佳方法在绝缘子水平和垂直方向上的平均定位误差分别为0.16±0.08m和0.16±0.11m。", "conclusion": "所提出的方法在没有先验地图的情况下，能够有效地实现对输电塔绝缘子的在线检查，并且显著降低了水平方向定位误差的变异性。"}}
{"id": "2602.24009", "pdf": "https://arxiv.org/pdf/2602.24009", "abs": "https://arxiv.org/abs/2602.24009", "authors": ["Zhicheng Fang", "Jingjie Zheng", "Chenxu Fu", "Wei Xu"], "title": "Jailbreak Foundry: From Papers to Runnable Attacks for Reproducible Benchmarking", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Jailbreak techniques for large language models (LLMs) evolve faster than benchmarks, making robustness estimates stale and difficult to compare across papers due to drift in datasets, harnesses, and judging protocols. We introduce JAILBREAK FOUNDRY (JBF), a system that addresses this gap via a multi-agent workflow to translate jailbreak papers into executable modules for immediate evaluation within a unified harness. JBF features three core components: (i) JBF-LIB for shared contracts and reusable utilities; (ii) JBF-FORGE for the multi-agent paper-to-module translation; and (iii) JBF-EVAL for standardizing evaluations. Across 30 reproduced attacks, JBF achieves high fidelity with a mean (reproduced-reported) attack success rate (ASR) deviation of +0.26 percentage points. By leveraging shared infrastructure, JBF reduces attack-specific implementation code by nearly half relative to original repositories and achieves an 82.5% mean reused-code ratio. This system enables a standardized AdvBench evaluation of all 30 attacks across 10 victim models using a consistent GPT-4o judge. By automating both attack integration and standardized evaluation, JBF offers a scalable solution for creating living benchmarks that keep pace with the rapidly shifting security landscape.", "AI": {"tldr": "介绍了一种名为Jailbreak Foundry (JBF) 的系统，用于将论文中的逃逸技术转化为可执行模块以便立即评估。", "motivation": "当前的大型语言模型（LLMs）逃逸技术发展迅速，超过了基准测试的速度，导致稳健性估计过时且难以跨研究进行比较。为了解决这一问题，提出了JBF系统来统一和标准化这些评估过程。", "method": "该系统由三个核心组件构成：(i) JBF-LIB提供共享合约和可重用工具；(ii) JBF-FORGE实现多代理论文到模块的转换；(iii) JBF-EVAL用于标准化评估。通过共享基础设施减少攻击特定实施代码并提高复用率。", "result": "在30个重复攻击中，JBF实现了高保真度，平均逃逸成功率（ASR）偏差为+0.26个百分点，并减少了近一半的攻击特定实施代码。", "conclusion": "JBF提供了自动化攻击集成和标准化评估的能力，支持使用一致的标准GPT-4o裁判在不同受害者模型上进行统一基准测试。"}}
{"id": "2602.23999", "pdf": "https://arxiv.org/pdf/2602.23999", "abs": "https://arxiv.org/abs/2602.23999", "authors": ["Jifan Shi", "Jianyang Gao", "James Xia", "Tamás Béla Fehér", "Cheng Long"], "title": "GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search", "categories": ["cs.DB", "cs.DS", "cs.IR"], "comment": null, "summary": "Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.", "AI": {"tldr": "提出IVF-RaBitQ，一种GPU上高效近似最近邻搜索方法，结合聚类和量化技术，实现快速索引构建和高通量搜索。", "motivation": "现有图基索引方法在召回率与吞吐量方面表现良好但构建时间和存储成本较高；聚类方法易于扩展且内存使用较少但需要大量查询以保证召回率。因此开发一种既能高效建索引又能提供高召回率和吞吐量的技术，同时保持低存储需求。", "method": "集成IVF（倒排文件）与RaBitQ量化技术到GPU上的近似最近邻搜索管道；采用可扩展的GPU原生RaBitQ量化方法实现快速准确的低比特编码；设计适用于RaBitQ码的距离计算方案和融合查询内核，提高通过率。", "result": "实验表明IVF-RaBitQ在召回率接近0.95时比CAGRA索引高出2.2倍查询每秒；构建时间比IVF-PQ平均快7.7倍；同时保持低存储需求并避免重排序过程中访问原始向量的需要。", "conclusion": "通过结合聚类和量化技术，IVF-RaBitQ在GPU上实现了高效的近似最近邻搜索。它提供了良好的性能边界，在召回率、吞吐量、索引构建时间和存储开销方面均优于现有方法。"}}
{"id": "2602.23997", "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "categories": ["cs.LG", "cs.AI"], "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.", "AI": {"tldr": "本文提出了一个框架，用于构建基础世界模型，使自主代理能够适应环境变化、自我验证和生成可靠的行为。", "motivation": "当前的自学习方法往往局限于固定的任务与环境中，无法应对不断变化的新颖情境。为了支持代理在开放世界的演化行为，作者提出了一种新的研究方向：基础世界模型。", "method": "文章提议构建一个包含四个关键组件的框架：（i）从规范中学习奖励模型；（ii）嵌入学习过程中的自适应形式验证；（iii）在线抽象校准以量化预测可靠性；以及（iv）由验证器指导的行为合成与世界模型生成。", "result": "该方法使代理能够基于少量交互推理出新策略，并在维持正确性的同时适应变化，从而为未来自主行为的可靠性和可解释性奠定了基础。", "conclusion": "本文提出的框架结合了强化学习、反应/程序合成和抽象机制，旨在建立一个支持自适应、验证及生成世界模型的基础平台。"}}
{"id": "2602.23996", "pdf": "https://arxiv.org/pdf/2602.23996", "abs": "https://arxiv.org/abs/2602.23996", "authors": ["Kaiwen Zhu", "Quansheng Zeng", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Yi Xin", "Qi Qin", "Jiayang Li", "Yu Qiao", "Jinjin Gu", "Yihao Liu"], "title": "Accelerating Masked Image Generation by Learning Latent Controlled Dynamics", "categories": ["cs.CV"], "comment": null, "summary": "Masked Image Generation Models (MIGMs) have achieved great success, yet their efficiency is hampered by the multiple steps of bi-directional attention. In fact, there exists notable redundancy in their computation: when sampling discrete tokens, the rich semantics contained in the continuous features are lost. Some existing works attempt to cache the features to approximate future features. However, they exhibit considerable approximation error under aggressive acceleration rates. We attribute this to their limited expressivity and the failure to account for sampling information. To fill this gap, we propose to learn a lightweight model that incorporates both previous features and sampled tokens, and regresses the average velocity field of feature evolution. The model has moderate complexity that suffices to capture the subtle dynamics while keeping lightweight compared to the original base model. We apply our method, MIGM-Shortcut, to two representative MIGM architectures and tasks. In particular, on the state-of-the-art Lumina-DiMOO, it achieves over 4x acceleration of text-to-image generation while maintaining quality, significantly pushing the Pareto frontier of masked image generation. The code and model weights are available at https://github.com/Kaiwen-Zhu/MIGM-Shortcut.", "AI": {"tldr": "提出了一种加速遮罩图像生成的方法MIGM-Shortcut，通过学习包含先前特征和采样令牌的轻量级模型来逼近特征演化的平均速度场。", "motivation": "现有遮罩图像生成模型因多步双向注意力机制而效率低下，且在快速加速时存在显著近似误差。为解决此问题，本文提出了一种新的方法。", "method": "通过学习一个轻量级模型来逼近特征演化的平均速度场，该模型包含先前特征和采样令牌，并应用于两种代表性的遮罩图像生成架构中。", "result": "在Lumina-DiMOO上实现了超过4倍的加速率，同时保持了文本到图像生成的质量。显著地推进了遮罩图像生成领域的帕累托前沿。", "conclusion": "通过提出MIGM-Shortcut方法，本文成功解决了现有遮罩图像生成模型效率低下的问题，并在实际任务中验证了其有效性。"}}
{"id": "2602.23994", "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.", "AI": {"tldr": "提出了一种多模态成像到语音的知识转移框架MINT，用于阿尔茨海默病早期筛查。", "motivation": "神经影像学在识别认知障碍和轻度认知障碍之间转换时非常有用，但成本高且难以大规模部署。相比之下，语音分析是低成本的替代方案，然而单独的语音分类器缺乏生物相关性。", "method": "MINT是一个三阶段跨模态框架，通过训练MRI教师定义一个紧凑的认知正常与轻度认知障碍分类神经影像嵌入空间，然后将语音表示对齐到这个冻结的成像流形，并在推断时应用未接触过语音的冻结MRI分类器。", "result": "实验表明，对齐后的语音表现可媲美纯语音基线（AUC为0.720对比0.711），并且无需神经影像学。多模态融合优于单独使用MRI（AUC为0.973对比0.958）。", "conclusion": "该研究首次展示了从MRI到语音的知识转移，提供了一种基于生物的途径，在没有神经影像学的情况下进行大规模认知筛查"}}
{"id": "2602.23363", "pdf": "https://arxiv.org/pdf/2602.23363", "abs": "https://arxiv.org/abs/2602.23363", "authors": ["Sahal Shaji Mullappilly", "Mohammed Irfan Kurpath", "Omair Mohamed", "Mohamed Zidan", "Fahad Khan", "Salman Khan", "Rao Anwer", "Hisham Cholakkal"], "title": "MediX-R1: Open Ended Medical Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com", "AI": {"tldr": "MediX-R1是一个用于医学多模态大型语言模型的开放型强化学习框架，旨在实现临床相关的自由格式回答。", "motivation": "现有方法在处理开放式医疗任务时存在局限性，难以超越多选题形式的回答。此研究提出了一种新的强化学习框架，以解决这一问题，并提供更可靠和全面的医学推理能力。", "method": "MediX-R1通过基于群组的RL以及综合奖励机制来微调基准视觉语言模型，该机制包括准确性奖励、语义奖励及轻量级格式与模态奖励。此外还提出了一种统一评估框架以衡量进展。", "result": "尽管只使用了约51K指令示例，MediX-R1在标准医学LLM和VLM基准测试中表现出色，并且在开放型临床任务上取得了显著提升。", "conclusion": "研究结果表明，在多模态模型中采用全面奖励信号及基于LLM的评估的开放式RL是一种实现可靠医疗推理的有效途径。"}}
{"id": "2602.23361", "pdf": "https://arxiv.org/pdf/2602.23361", "abs": "https://arxiv.org/abs/2602.23361", "authors": ["Sven Elflein", "Ruilong Li", "Sérgio Agostinho", "Zan Gojcic", "Laura Leal-Taixé", "Qunjie Zhou", "Aljosa Osep"], "title": "VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale", "categories": ["cs.CV"], "comment": "CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt", "summary": "We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.", "AI": {"tldr": "本文提出了一种可扩展的3D重建模型，解决了离线前馈方法在处理大量输入图像时计算和内存需求快速增长的问题。", "motivation": "该研究指出，现有的离线前馈式3D重建方法由于其计算和内存需求随输入图像数量二次增长而面临瓶颈问题。提出的方法旨在解决这一挑战，并实现大规模场景的高效3D重建。", "method": "通过将场景几何的关键-值表示压缩为固定大小的多层感知机（MLP），该模型能够在测试时进行训练，从而克服了可扩展性的限制。这种方法类似在线模型，计算复杂度随输入视图数量线性增长，同时保持全局场景聚合能力。", "result": "实验表明，VGG-T$^3$能够在一个包含1000张图像的集合上仅需54秒完成重建，并且相比于依赖softmax注意力机制的方法实现了11.6倍的速度提升。此外，该模型在点图重构误差方面优于其他线性时间方法。", "conclusion": "VGG-T$^3$提供了一种高效、可扩展的离线前馈式3D重建方案，能够在处理大量输入图像时保持较高的效率和准确性，并展示了其对未见过图像进行场景查询的能力。"}}
{"id": "2602.23360", "pdf": "https://arxiv.org/pdf/2602.23360", "abs": "https://arxiv.org/abs/2602.23360", "authors": ["Eric Eaton", "Surbhi Goel", "Marcel Hussing", "Michael Kearns", "Aaron Roth", "Sikata Bela Sengupta", "Jessica Sorrell"], "title": "Model Agreement via Anchoring", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies. We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.", "AI": {"tldr": "通过锚定技术减少机器学习模型在独立样本上的预测差异，以提高模型一致性。", "motivation": "控制模型之间的预测分歧是重要的研究问题，作者希望通过训练过程中的自然参数将这种分歧减小到零来实现更强的模型一致性。", "method": "开发了一种基于锚定（平均两个模型）的技术，用于证明模型独立预测差异的界限，并应用该技术对四种常用机器学习算法进行分析。", "result": "对于上述四种常见机器学习算法，均可通过增加某些参数值将分歧减小到零。初始研究是在一维回归和平方误差损失下完成的，但结果可推广至多维回归及任何强凸损失函数。", "conclusion": "该技术提供了一种有效的方法来减少模型之间的预测分歧，并且适用于广泛使用的机器学习算法。"}}
{"id": "2602.23359", "pdf": "https://arxiv.org/pdf/2602.23359", "abs": "https://arxiv.org/abs/2602.23359", "authors": ["Vaibhav Agrawal", "Rishubh Parihar", "Pradhaan Bhat", "Ravi Kiran Sarvadevabhatla", "R. Venkatesh Babu"], "title": "SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://seethrough3d.github.io. Accepted at CVPR 2026", "summary": "We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.", "AI": {"tldr": "SeeThrough3D 提出了一种新的生成模型，能够处理文本到图像生成中的遮挡问题。", "motivation": "现有的方法在合成具有真实感的场景时缺乏对精确物体间遮挡关系的建模能力。因此，作者旨在解决这一问题，提出一种能够准确生成被部分遮挡物体的方法。", "method": "SeeThrough3D 使用一种称为OCSR（透明度感知3D场景表示）的新方法，通过将对象表示为虚拟环境中的半透明3D盒子来处理遮挡，并从所需视点渲染这些盒子。此外，该模型使用自注意力机制确保每个物体的边界框与其文本描述正确绑定。", "result": "SeeThrough3D 可以在训练时有效地生成包含复杂物体间遮挡关系的真实场景，并且可以应用于未见过的对象类别。", "conclusion": "通过引入OCSR和改进的视觉令牌，SeeThrough3D 能够生成具有真实感的遮挡效果，提高了文本到图像生成的质量。"}}
{"id": "2602.23358", "pdf": "https://arxiv.org/pdf/2602.23358", "abs": "https://arxiv.org/abs/2602.23358", "authors": ["Elad Kimchi Shoshani", "Leeyam Gabay", "Yedid Hoshen"], "title": "A Dataset is Worth 1 MB", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 9 figures", "summary": "A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.", "AI": {"tldr": "提出了一种名为PLADA的方法，通过传输标签而非像素来减少数据集分发的通信成本。", "motivation": "为了降低大规模数据集服务器向多客户端分发时产生的巨大通信费用，并且由于客户端硬件和软件框架的多样性导致无法直接传输预训练模型，需要一种新的方法来压缩或替代原始数据。", "method": "PLADA假设代理已经加载了一个大型、通用但未标注的数据集（如ImageNet-1K或21K），并通过仅发送特定图像的类标签来进行任务沟通。引入了一种修剪机制，以筛选参考数据集中的相关图片标签，同时最大化训练效率并最小化传输负载。", "result": "实验显示PLADA能够在保持高分类准确性的同时使用小于1MB的数据量进行任务知识转移，并在不同的数据集中验证了该方法的有效性。", "conclusion": "PLADA为高效地分发大规模数据集提供了一种潜在的解决方案，通过减少传输负载显著降低了通信成本。"}}
{"id": "2602.23357", "pdf": "https://arxiv.org/pdf/2602.23357", "abs": "https://arxiv.org/abs/2602.23357", "authors": ["Aheli Saha", "René Schuster", "Didier Stricker"], "title": "Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training", "categories": ["cs.CV"], "comment": "12 pages, International Conference on Pattern Recognition Applications and Methods", "summary": "Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.", "AI": {"tldr": "本文提出了通过联合分布训练来增强事件相机在对象检测中的传感器通用性和自适应性。", "motivation": "由于生物启发的事件相机具有异步和低延迟的特点，研究者对其兴趣日益增加。然而，这些特点也带来了数据可变性不足以及对信号参数缺乏深入分析的问题。本文旨在填补这一空白，并提升模型在不同传感器上的鲁棒性。", "method": "通过联合分布训练的方法来提高基于事件的物体检测模型的性能和通用性。", "result": "该方法成功提升了事件相机在对象检测任务中的表现，增强了模型对不同传感器输入数据的适应能力。", "conclusion": "本文的研究表明，利用联合分布训练能够有效地增强基于事件信号的对象检测系统的鲁棒性和泛化能力。"}}
{"id": "2602.23353", "pdf": "https://arxiv.org/pdf/2602.23353", "abs": "https://arxiv.org/abs/2602.23353", "authors": ["Simon Roschmann", "Paul Krzakala", "Sonia Mazelet", "Quentin Bouniot", "Zeynep Akata"], "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.", "AI": {"tldr": "SOTAlign 是一种半监督对齐框架，用于在少量配对样本和大量未配对数据上对预训练的视觉和语言模型进行对齐。", "motivation": "本文探讨了是否可以在较少的监督下实现有意义的跨模态对齐，并提出了一种新的方法来利用大量的未配对标记数据以提高对齐效果。", "method": "SOTAlign 包括两个阶段：第一阶段通过线性教师模型从少量配对样本中恢复粗略共享几何结构；第二阶段则使用基于最优传输的散度在大量未配对样本上细化对齐，从而学习出鲁棒的联合嵌入。", "result": "SOTAlign 在多个数据集和编码器对上取得了显著优于监督和半监督基线的结果。", "conclusion": "SOTAlign 成功地利用了未配对的数据，实现了跨模态模型的有效对齐，并且在各种设置下表现出了良好的性能。"}}
{"id": "2602.23351", "pdf": "https://arxiv.org/pdf/2602.23351", "abs": "https://arxiv.org/abs/2602.23351", "authors": ["Amita Kamath", "Jack Hessel", "Khyathi Chandu", "Jena D. Hwang", "Kai-Wei Chang", "Ranjay Krishna"], "title": "Scale Can't Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "TACL 2026", "summary": "The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., \"at the game today!\" is a more likely caption than \"a photo of 37 people standing behind a field\". We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.", "AI": {"tldr": "研究探讨了视觉语言模型（VLM）在某些推理技能上的不足，发现其训练数据中的报告偏差导致这些能力的缺失，并通过实验验证了特定注释的有效性。", "motivation": "作者观察到VLM缺乏一些基本的推理能力，并推测这种现象可能源于训练数据中存在的报告偏差，即人们通常不会在描述视觉内容时传达隐含信息。", "method": "研究分析了OpenCLIP、LLaVA-1.5和Molmo等流行VLM的数据集，结合语言学中的语用理论来识别报告偏差对四种推理技能（空间、时间、否定和计数）的影响，并通过一组精心设计的基准测试验证其假设。", "result": "研究发现，在训练数据中被抑制的这些推理类型上，VLM表现不佳；增加数据规模或模型大小并不能解决这个问题。但是，添加特定收集到的隐含信息注释可以有效改善这些问题。", "conclusion": "研究表明需要更有意图地对训练数据进行策划和收集，而不能仅仅依赖于规模的增长来提高推理能力。"}}
{"id": "2602.23349", "pdf": "https://arxiv.org/pdf/2602.23349", "abs": "https://arxiv.org/abs/2602.23349", "authors": ["Jose Javier Gonzalez Ortiz", "Abhay Gupta", "Chris Renard", "Davis Blalock"], "title": "FlashOptim: Optimizers for Memory Efficient Training", "categories": ["cs.LG", "cs.AI"], "comment": "Source code is available at https://github.com/databricks/flashoptim", "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory. We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half. Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.", "AI": {"tldr": "提出FlashOptim以减少神经网络训练中的内存消耗。", "motivation": "标准的混合精度训练需要大量显存，这对于模型参数较多的研究人员来说不切实际。因此需要开发一种有效的方法来降低内存需求。", "method": "通过改进主权重分割和设计压缩函数来减少每个参数所需的内存。这些技术使AdamW内存从16字节降至7字节或更少，并且缩小了模型检查点的大小。", "result": "实验结果表明，FlashOptim在标准视觉和语言基准测试任务中没有质量退化。", "conclusion": "FlashOptim方法有效地减少了训练过程中的显存需求，并保持了模型的质量。"}}
{"id": "2602.23341", "pdf": "https://arxiv.org/pdf/2602.23341", "abs": "https://arxiv.org/abs/2602.23341", "authors": ["Alkis Kalavasis", "Anay Mehrotra", "Manolis Zampetakis", "Felix Zhou", "Ziyu Zhu"], "title": "Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms", "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML"], "comment": "Abstract truncated to arXiv limits. To appear in ICLR'26", "summary": "Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]", "AI": {"tldr": "该论文研究了从粗略数据中估计高斯均值的方法，特别是当样本仅通过包含其真实值的集合而不是确切数值来显示时的情况。", "motivation": "在测量舍入、传感器限制和经济系统中的滞后等情况下，我们观察到的是部分信息。论文旨在解决从这种“低”信息粗略数据中进行有效均值估计的问题。", "method": "本文解决了当未知平均值可识别且分区仅由凸集构成时的两个根本问题，并证明了在此条件下计算上有效的估计是可能的。", "result": "该工作解决了在凸分区内均值可识别性以及在这种情况下进行有效估计的可能性这两个基本问题，推进了现有研究。", "conclusion": "论文为解决从粗略数据中估计高斯均值的问题提供了新的理论和计算方法，并展示了这些方法的有效性和广泛适用性。"}}
{"id": "2602.23339", "pdf": "https://arxiv.org/pdf/2602.23339", "abs": "https://arxiv.org/abs/2602.23339", "authors": ["Tilemachos Aravanis", "Vladan Stojnić", "Bill Psomas", "Nikos Komodakis", "Giorgos Tolias"], "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.", "AI": {"tldr": "论文提出了一种基于少量样本增强文本提示的方法，以改善视觉语言模型在开放词汇分割任务中的性能。", "motivation": "论文旨在解决由于粗略的图像级监督和自然语言语义模糊性而导致的开放词汇分割滞后于全监督方法的问题。", "method": "通过引入带有像素注释图像的支持集并提出一种检索增强的测试时间适配器，该适配器学习融合文本和视觉支持特征的轻量级、针对每个图像的分类器。", "result": "实验显示这种方法显著缩小了零样本分割与监督分割之间的差距，并保持了开放词汇的能力。", "conclusion": "通过引入少量样本和支持集增强方法，可以有效提升开放词汇分割任务的表现。"}}
{"id": "2602.23335", "pdf": "https://arxiv.org/pdf/2602.23335", "abs": "https://arxiv.org/abs/2602.23335", "authors": ["Dany Haddad", "Dan Bareket", "Joseph Chee Chang", "Jay DeYoung", "Jena D. Hwang", "Uri Katz", "Mark Polak", "Sangho Suh", "Harshit Surana", "Aryeh Tiktinsky", "Shriya Atmakuri", "Jonathan Bragg", "Mike D'Arcy", "Sergey Feldman", "Amal Hassan-Ali", "Rubén Lozano", "Bodhisattwa Prasad Majumder", "Charles McGrady", "Amanpreet Singh", "Brooke Vlahos", "Yoav Goldberg", "Doug Downey"], "title": "Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": null, "summary": "AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.", "AI": {"tldr": "本文通过Asta交互数据集研究了科研人员在实际环境中使用AI驱动的科研工具的方式，包括查询模式、参与行为以及经验积累对使用的演变。", "motivation": "随着人工智能技术在科学研究中的集成速度加快，了解科学家如何在现实世界中利用这些系统成为一个亟待解决的问题。为了填补这一空白，本文创建并分析了Asta交互数据集来探索研究人员的行为和习惯。", "method": "研究者使用包含超过20万用户查询日志的大规模数据集对AI驱动的研究工具的使用模式进行分类，并深入探讨用户的参与行为以及经验积累如何影响这些行为。", "result": "研究表明，与传统搜索相比，用户提交更长、更复杂的查询，并将系统视为协作研究伙伴；随着使用经验的增长，用户的查询更加精准且更多地参与到参考引用中。", "conclusion": "该数据集和分析提供了对AI科研助手在现实世界中的应用的理解，有助于未来设计和评估更加贴近实际需求的AI工具。"}}
{"id": "2602.23334", "pdf": "https://arxiv.org/pdf/2602.23334", "abs": "https://arxiv.org/abs/2602.23334", "authors": ["Yuhao Liu", "Salim Ullah", "Akash Kumar"], "title": "Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption and accuracy. Because regular designs for multiplication on hardware cannot support the precision reconfiguration for a multi-precision Quantized Neural Network (QNN) model in runtime, we propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array design for QNN accelerators. We have implemented and evaluated our work on the Ultra96 FPGA platform. Results show that our work can achieve 1.3185 to 3.5671 times speedup in inferring mixed-precision models and has less critical path delay, supporting a higher clock frequency (250MHz).", "AI": {"tldr": "提出了一种用于神经网络加速器的位级阵列架构，支持多精度量化乘法运算，并在Ultra96 FPGA上实现了验证。", "motivation": "现有硬件设计无法满足运行时重新配置的需求，导致低精度量化模型的准确率损失较大。为解决此问题，提出了一种适用于混合精度量化的硬件加速器设计方案。", "method": "提出了一个多通道位级阵列架构，支持多精度量化乘法运算，并在Ultra96 FPGA平台上实现了验证。", "result": "实验结果表明，该方案能够实现1.3185到3.5671倍的速度提升，在推理混合精度模型时具有更低的关键路径延迟和更高的工作频率（250MHz）。", "conclusion": "所提出的方法解决了现有硬件设计在运行时重新配置多精度量化的难题，为神经网络加速器的设计提供了新的思路。"}}
{"id": "2602.23333", "pdf": "https://arxiv.org/pdf/2602.23333", "abs": "https://arxiv.org/abs/2602.23333", "authors": ["Zeyu Xie", "Chenxing Li", "Qiao Jin", "Xuenan Xu", "Guanrou Yang", "Wenfu Wang", "Mengyue Wu", "Dong Yu", "Yuexian Zou"], "title": "SemanticVocoder: Bridging Audio Generation and Audio Understanding via Semantic Latents", "categories": ["cs.SD"], "comment": "Demo: https://zeyuxie29.github.io/SemanticVocoder/", "summary": "Recent audio generation models typically rely on Variational Autoencoders (VAEs) and perform generation within the VAE latent space. Although VAEs excel at compression and reconstruction, their latents inherently encode low-level acoustic details rather than semantically discriminative information, leading to entangled event semantics and complicating the training of generative models. To address these issues, we discard VAE acoustic latents and introduce semantic encoder latents, thereby proposing SemanticVocoder, a generative vocoder that directly synthesizes waveforms from semantic latents. Equipped with SemanticVocoder, our text-to-audio generation model achieves a Frechet Distance of 12.823 and a Frechet Audio Distance of 1.709 on the AudioCaps test set, as the introduced semantic latents exhibit superior discriminability compared to acoustic VAE latents. Beyond improved generation performance, it also serves as a promising attempt towards unifying audio understanding and generation within a shared semantic space. Generated samples are available at https://zeyuxie29.github.io/SemanticVocoder/.", "AI": {"tldr": "提出了一种基于语义潜在变量的生成式语音编码器SemanticVocoder，该模型能够直接从语义潜在空间生成音频波形。", "motivation": "传统的音频生成模型依赖于变分自编码器（VAE），但其潜在变量主要包含低级别的声学细节而非语义信息，导致事件语义纠缠并增加生成模型的训练难度。为解决这一问题，研究者提出了利用语义潜在变量代替声学潜在变量的方法。", "method": "抛弃了传统的变分自编码器声学潜在空间，引入语义编解码器来生成音频波形，并通过训练使模型能够直接从语义潜在空间进行波形合成。", "result": "采用SemanticVocoder的文本到音频生成模型在AudioCaps测试集上取得了Frechet距离12.823和Frechet音频距离1.709的成绩，显示了语义潜在变量比声学VAE潜在变量更具判别性。", "conclusion": "SemanticVocoder不仅提升了音频生成的质量，还为统一音频理解和生成提供了新的路径。"}}
{"id": "2602.23331", "pdf": "https://arxiv.org/pdf/2602.23331", "abs": "https://arxiv.org/abs/2602.23331", "authors": ["Salim Fares"], "title": "Utilizing LLMs for Industrial Process Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.", "AI": {"tldr": "本文探讨了将大型语言模型应用于工业过程自动化领域，特别是在专用编程语言中的应用。", "motivation": "尽管许多关于LLM的研究集中在通用编程语言上，但针对特定于工业过程自动化的高度专业化语言的使用仍较少被研究。因此，这项工作旨在解决这一空白并加速制造业系统的开发周期。", "method": "通过利用和整合大型语言模型到工业开发过程中来生成解决方案，如为机器人手臂创建移动程序等编程任务。", "result": "研究成果包括成功地将LLM应用于工业过程中的特定编程任务，并展示了其对开发循环的潜在加速作用。", "conclusion": "研究结论表明，大型语言模型在解决工业自动化领域特有的复杂问题方面具有巨大潜力。"}}
{"id": "2602.23330", "pdf": "https://arxiv.org/pdf/2602.23330", "abs": "https://arxiv.org/abs/2602.23330", "authors": ["Kunihiro Miyazaki", "Takanobu Kawahara", "Stephen Roberts", "Stefan Zohren"], "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks", "categories": ["cs.AI", "q-fin.TR"], "comment": "14 pages, 3 figures", "summary": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system's output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.", "AI": {"tldr": "该论文提出了一种将投资分析分解为细粒度任务的多代理LLM交易框架，旨在提高风险调整后的回报。", "motivation": "传统的基于LLM的自主金融交易系统使用粗粒度指令模仿分析师和管理者的角色，容易忽略真实工作流程中的复杂性，导致推理性能下降和决策不透明。因此，提出了细粒度任务分解的方法来改善这些问题。", "method": "该论文设计了一个多代理LLM交易框架，并在控制泄漏的回测设置中使用日本股票数据进行评估。包括价格、财务报表、新闻以及宏观经济信息等。", "result": "实验结果表明，与传统的粗粒度设计方案相比，细粒度任务分解显著提高了风险调整后的回报率。进一步分析中间代理输出显示，分析产出与下游决策偏好的一致性是系统性能的关键驱动因素。", "conclusion": "这些发现有助于在实际应用场景中设计基于LLM代理的交易系统的机构结构和任务配置。"}}
{"id": "2602.23329", "pdf": "https://arxiv.org/pdf/2602.23329", "abs": "https://arxiv.org/abs/2602.23329", "authors": ["Chen Bo Calvin Zhang", "Christina Q. Knight", "Nicholas Kruus", "Jason Hausenloy", "Pedro Medeiros", "Nathaniel Li", "Aiden Kim", "Yury Orlovskiy", "Coleman Breen", "Bryce Cai", "Jasper Götting", "Andrew Bo Liu", "Samira Nedungadi", "Paula Rodriguez", "Yannis Yiming He", "Mohamed Shaaban", "Zifan Wang", "Seth Donoughe", "Julian Michael"], "title": "LLM Novice Uplift on Dual-Use, In Silico Biology Tasks", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CY", "cs.HC"], "comment": "59 pages, 33 figures", "summary": "Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）是否能够提升新手用户在生物安全相关任务上的表现，以评估科学加速和双重用途风险。", "motivation": "为了理解大型语言模型如何影响科学研究的加速以及可能带来的双重用途风险，需要明确这些模型能否帮助没有专业知识的新手用户更好地完成复杂生物学任务。", "method": "研究进行了多模态、多基准的人类提升实验，比较了在使用互联网资源和具备LLM访问权限的情况下新手用户的性能差异。测试包括八组生物安全相关的任务集，参与者有长达13小时的时间来解决最复杂的任务。", "result": "LLM的接入显著提升了新手用户的表现：相较于仅用互联网资源的新手用户，LLMs辅助下的新手用户准确度提高了4.16倍（95%置信区间[2.63,6.87]）。在四个具备专家基线的数据集上，有三个数据集中新手使用LLM的性能超过了专家。", "conclusion": "研究发现大型语言模型显著提升了新手用户在生物学任务上的表现，这些任务此前通常只能由专业人员完成。这表明需要持续进行交互式的人类提升评估来补充传统的基准测试方法。"}}
{"id": "2602.23318", "pdf": "https://arxiv.org/pdf/2602.23318", "abs": "https://arxiv.org/abs/2602.23318", "authors": ["Aloïs Rautureau", "Tristan Cazenave", "Éric Piette"], "title": "Generalized Rapid Action Value Estimation in Memory-Constrained Environments", "categories": ["cs.AI"], "comment": null, "summary": "Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.", "AI": {"tldr": "引入GRAVE2，GRAVER和GRAVER2算法以解决内存限制问题并保持游戏强度", "motivation": "减少存储节点数量，在内存受限环境中更实用", "method": "通过两层搜索、节点回收以及两者结合的技术来改进GRAVE算法", "result": "这些方法在减少存储需求的同时维持了与GRAVE相同的游戏性能", "conclusion": "新的算法能够在资源有限的环境下有效运作"}}
{"id": "2602.23315", "pdf": "https://arxiv.org/pdf/2602.23315", "abs": "https://arxiv.org/abs/2602.23315", "authors": ["Sha Hu"], "title": "Invariant Transformation and Resampling based Epistemic-Uncertainty Reduction", "categories": ["cs.AI"], "comment": "5 pages, 5 figures", "summary": "An artificial intelligence (AI) model can be viewed as a function that maps inputs to outputs in high-dimensional spaces. Once designed and well trained, the AI model is applied for inference. However, even optimized AI models can produce inference errors due to aleatoric and epistemic uncertainties. Interestingly, we observed that when inferring multiple samples based on invariant transformations of an input, inference errors can show partial independences due to epistemic uncertainty. Leveraging this insight, we propose a \"resampling\" based inferencing that applies to a trained AI model with multiple transformed versions of an input, and aggregates inference outputs to a more accurate result. This approach has the potential to improve inference accuracy and offers a strategy for balancing model size and performance.", "AI": {"tldr": "本文提出了一种通过不变变换和重采样来减少模型的表征不确定性，从而提高推理准确性的方法。", "motivation": "即使经过优化的人工智能模型在推理时仍可能因不确定因素而导致错误。作者发现通过对输入进行不变变换可以揭示出由于表征不确定性导致的误差独立性，并希望通过此策略提升模型的表现。", "method": "通过为每个输入应用多个不变变换版本来执行重采样，然后聚合这些转换后的输出以获得更准确的结果。", "result": "该方法能够在不增加模型大小的情况下提高推理准确性。", "conclusion": "基于不变变换和重采样的策略能够有效减少表征不确定性，从而改进了模型的推理精度。"}}
{"id": "2602.23312", "pdf": "https://arxiv.org/pdf/2602.23312", "abs": "https://arxiv.org/abs/2602.23312", "authors": ["Rafael R. Baptista", "André de Lima Salgado", "Ricardo V. Godoy", "Marcelo Becker", "Thiago Boaventura", "Gustavo J. G. Lahr"], "title": "Evaluating Zero-Shot and One-Shot Adaptation of Small Language Models in Leader-Follower Interaction", "categories": ["cs.HC", "cs.AI", "cs.LG", "cs.RO", "eess.SY"], "comment": null, "summary": "Leader-follower interaction is an important paradigm in human-robot interaction (HRI). Yet, assigning roles in real time remains challenging for resource-constrained mobile and assistive robots. While large language models (LLMs) have shown promise for natural communication, their size and latency limit on-device deployment. Small language models (SLMs) offer a potential alternative, but their effectiveness for role classification in HRI has not been systematically evaluated. In this paper, we present a benchmark of SLMs for leader-follower communication, introducing a novel dataset derived from a published database and augmented with synthetic samples to capture interaction-specific dynamics. We investigate two adaptation strategies: prompt engineering and fine-tuning, studied under zero-shot and one-shot interaction modes, compared with an untrained baseline. Experiments with Qwen2.5-0.5B reveal that zero-shot fine-tuning achieves robust classification performance (86.66% accuracy) while maintaining low latency (22.2 ms per sample), significantly outperforming baseline and prompt-engineered approaches. However, results also indicate a performance degradation in one-shot modes, where increased context length challenges the model's architectural capacity. These findings demonstrate that fine-tuned SLMs provide an effective solution for direct role assignment, while highlighting critical trade-offs between dialogue complexity and classification reliability on the edge.", "AI": {"tldr": "评估小型语言模型在领导跟从互动中的零样本和一次样本适应能力。", "motivation": "解决资源受限的移动机器人或辅助机器人实时分配角色的问题，同时探索小型语言模型是否能够有效地应用于人类-机器人交互的角色分类任务中。", "method": "通过基准测试小型语言模型进行领导者跟随者交流，并使用来自已发布数据库和合成样本增强的数据集。研究两种适应策略：提示工程与微调，在零样本和一次样本互动模式下，将之与未训练基线相比。", "result": "实验表明，Qwen2.5-0.5B在零样本微调条件下达到86.66%的准确率及每样本22.2毫秒延迟。然而，在一次样本模式中表现较差，显示对话复杂性与分类可靠性之间的关键权衡。", "conclusion": "证明了通过微调的小型语言模型可以直接进行角色分配，并强调了在边缘设备上的对话复杂性和分类可靠性的权衡问题。"}}
{"id": "2602.23306", "pdf": "https://arxiv.org/pdf/2602.23306", "abs": "https://arxiv.org/abs/2602.23306", "authors": ["Yiran Guan", "Sifan Tu", "Dingkang Liang", "Linghao Zhu", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Yuliang Liu", "Xiang Bai"], "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding", "categories": ["cs.CV"], "comment": "Accept by ICLR 2026", "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.", "AI": {"tldr": "提升大型语言模型在多模态场景下的推理能力", "motivation": "现有大型语言模型虽能处理多种数据源，但缺乏复杂推理能力。通过额外训练来增强其推理功能面临诸多挑战，如高质量数据需求、特定任务调整及高昂计算成本。", "method": "ThinkOmni框架采用LRM作为指导和逐步对比缩放方法，在无训练与数据的情况下提升语言模型的推理性能。", "result": "实验显示，ThinkOmni在六个多模态推理基准测试中表现出色，如MathVista得分70.2，MMAU得分为75.5。", "conclusion": "该框架提供了一种灵活且通用的方法来增强跨模态推理，并为推理能力的推广和应用提供了新的见解"}}
{"id": "2602.23302", "pdf": "https://arxiv.org/pdf/2602.23302", "abs": "https://arxiv.org/abs/2602.23302", "authors": ["Giacomo Bonanno"], "title": "The logic of KM belief update is contained in the logic of AGM belief revision", "categories": ["cs.AI", "cs.LO", "math.LO"], "comment": "arXiv admin note: text overlap with arXiv:2310.11506. text overlap with arXiv:2310.11506", "summary": "For each axiom of KM belief update we provide a corresponding axiom in a modal logic containing three modal operators: a unimodal belief operator $B$, a bimodal conditional operator $>$ and the unimodal necessity operator $\\square$. We then compare the resulting logic to the similar logic obtained from converting the AGM axioms of belief revision into modal axioms and show that the latter contains the former. Denoting the latter by $\\mathcal L_{AGM}$ and the former by $\\mathcal L_{KM}$ we show that every axiom of $\\mathcal L_{KM}$ is a theorem of $\\mathcal L_{AGM}$. Thus AGM belief revision can be seen as a special case of KM belief update. For the strong version of KM belief update we show that the difference between $\\mathcal L_{KM}$ and $\\mathcal L_{AGM}$ can be narrowed down to a single axiom, which deals exclusively with unsurprising information, that is, with formulas that were not initially disbelieved.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23300", "pdf": "https://arxiv.org/pdf/2602.23300", "abs": "https://arxiv.org/abs/2602.23300", "authors": ["Soumya Dutta", "Smruthi Balaji", "Sriram Ganapathy"], "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables", "summary": "Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.", "AI": {"tldr": "提出了一个混合专家模型MiSTER-E，用于多模态情感识别。", "motivation": "情感识别在对话中面临挑战，需要捕捉多轮对话的时间流，并有效融合来自多种模式的线索。为了应对这些挑战，提出了一种新的方法来解决多模式上下文建模和信息融合的问题。", "method": "MiSTER-E使用大型语言模型进行语音和文本的细调以生成丰富的话术级嵌入，然后通过卷积循环上下文层增强这些嵌入，并结合三种专家系统：仅语音、仅文本以及跨模态的预测。引入了监督对比损失和支持KL散度正则化机制来鼓励不同模式之间的一致性。", "result": "在IEMOCAP、MELD和MOSI三个基准数据集上，MiSTER-E分别获得了70.9%、69.5%和87.9%的加权F1得分，优于多个基于语音-文本的情感识别系统。", "conclusion": "通过实验验证了所提出方法的有效性，并展示了其在多模态情感识别中的优越性能。"}}
{"id": "2602.23297", "pdf": "https://arxiv.org/pdf/2602.23297", "abs": "https://arxiv.org/abs/2602.23297", "authors": ["Yiqing Wang", "Chunming He", "Ming-Chen Lu", "Mercy Pawar", "Leslie Niziol", "Maria Woodward", "Sina Farsiu"], "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM", "categories": ["cs.CV"], "comment": null, "summary": "Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.", "AI": {"tldr": "PRIMA是一种框架，通过整合图像和元数据的对齐来提升医学诊断。", "motivation": "当前方法未能充分利用临床描述中的丰富语义知识，提出PRIMA以解决这一问题并提高医疗诊断效果。", "method": "利用RAG获取风险-疾病相关性专家库，并改进Clinical ModernBERT。采用DINOv3和改进的BERT进行预训练，优化使用四个互补损失函数，最后用Qwen-3融合特征实现精准分类。", "result": "PRIMA在实验中表现出色，显著超越了现有方法，在不依赖大量数据收集或高计算资源的情况下实现了卓越的鲁棒性。", "conclusion": "通过整合图像和元数据对齐以及利用软标签处理临床关联模糊问题，PRIMA有效地提升了医学诊断中的像素级特征与抽象临床专业知识的一致性。"}}
{"id": "2602.23296", "pdf": "https://arxiv.org/pdf/2602.23296", "abs": "https://arxiv.org/abs/2602.23296", "authors": ["Quang-Huy Nguyen", "Jiaqi Wang", "Wei-Shinn Ku"], "title": "Conformalized Neural Networks for Federated Uncertainty Quantification under Dual Heterogeneity", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) faces challenges in uncertainty quantification (UQ). Without reliable UQ, FL systems risk deploying overconfident models at under-resourced agents, leading to silent local failures despite seemingly satisfactory global performance. Existing federated UQ approaches often address data heterogeneity or model heterogeneity in isolation, overlooking their joint effect on coverage reliability across agents. Conformal prediction is a widely used distribution-free UQ framework, yet its applications in heterogeneous FL settings remains underexplored. We provide FedWQ-CP, a simple yet effective approach that balances empirical coverage performance with efficiency at both global and agent levels under the dual heterogeneity. FedWQ-CP performs agent-server calibration in a single communication round. On each agent, conformity scores are computed on calibration data and a local quantile threshold is derived. Each agent then transmits only its quantile threshold and calibration sample size to the server. The server simply aggregates these thresholds through a weighted average to produce a global threshold. Experimental results on seven public datasets for both classification and regression demonstrate that FedWQ-CP empirically maintains agent-wise and global coverage while producing the smallest prediction sets or intervals.", "AI": {"tldr": "提出了FedWQ-CP方法，用于联邦学习中不确定性量化的问题。", "motivation": "传统的联邦学习中的不确定性量化方法未能同时解决数据异质性和模型异质性导致的覆盖可靠性问题。", "method": "通过在每个代理和服务器之间进行一次通信轮次实现校准，并计算一致性得分以获得局部阈值，然后传输到服务器进行全局聚合。", "result": "实验结果表明FedWQ-CP能够维护代理级别的覆盖率以及全球范围内的性能。", "conclusion": "提出的方法在处理联邦学习中的双重异质性问题时表现出了有效性和效率。"}}
{"id": "2602.23295", "pdf": "https://arxiv.org/pdf/2602.23295", "abs": "https://arxiv.org/abs/2602.23295", "authors": ["Ayush Roy", "Wei-Yang Alex Lee", "Rudrasis Chakraborty", "Vishnu Suresh Lokhande"], "title": "ManifoldGD: Training-Free Hierarchical Manifold Guidance for Diffusion-Based Dataset Distillation", "categories": ["cs.CV", "cs.LG"], "comment": "CVPE 2026", "summary": "In recent times, large datasets hinder efficient model training while also containing redundant concepts. Dataset distillation aims to synthesize compact datasets that preserve the knowledge of large-scale training sets while drastically reducing storage and computation. Recent advances in diffusion models have enabled training-free distillation by leveraging pre-trained generative priors; however, existing guidance strategies remain limited. Current score-based methods either perform unguided denoising or rely on simple mode-based guidance toward instance prototype centroids (IPC centroids), which often are rudimentary and suboptimal. We propose Manifold-Guided Distillation (ManifoldGD), a training-free diffusion-based framework that integrates manifold consistent guidance at every denoising timestep. Our method employs IPCs computed via a hierarchical, divisive clustering of VAE latent features, yielding a multi-scale coreset of IPCs that captures both coarse semantic modes and fine intra-class variability. Using a local neighborhood of the extracted IPC centroids, we create the latent manifold for each diffusion denoising timestep. At each denoising step, we project the mode-alignment vector onto the local tangent space of the estimated latent manifold, thus constraining the generation trajectory to remain manifold-faithful while preserving semantic consistency. This formulation improves representativeness, diversity, and image fidelity without requiring any model retraining. Empirical results demonstrate consistent gains over existing training-free and training-based baselines in terms of FID, l2 distance among real and synthetic dataset embeddings, and classification accuracy, establishing ManifoldGD as the first geometry-aware training-free data distillation framework.", "AI": {"tldr": "提出了一种名为ManifoldGD的训练自由扩散基础框架，用于生成紧凑且具有代表性的数据集。", "motivation": "当前的方法在无指导去噪和基于简单模式的指导之间存在限制。作者希望通过引入多层次一致引导来改进现有方法，从而提高合成数据集的质量。", "method": "ManifoldGD通过层次聚类获得多尺度核心点，并构建局部邻域以形成潜在流形，在每个去噪步骤中投影模式对齐向量至该空间内，保持生成轨迹的连贯性与多样性。", "result": "实验证明了ManifoldGD在FID、L2距离及分类准确率上优于现有训练自由和需训练基线方法。", "conclusion": "ManifoldGD是首个几何感知型无训练数据集提炼框架，改善了数据集的代表性与多样性。"}}
{"id": "2602.23294", "pdf": "https://arxiv.org/pdf/2602.23294", "abs": "https://arxiv.org/abs/2602.23294", "authors": ["Xin Gu", "Bing Fan", "Jiali Yao", "Zhipeng Zhang", "Yan Huang", "Cheng Han", "Heng Fan", "Libo Zhang"], "title": "Towards Long-Form Spatio-Temporal Video Grounding", "categories": ["cs.CV"], "comment": null, "summary": "In real scenarios, videos can span several minutes or even hours. However, existing research on spatio-temporal video grounding (STVG), given a textual query, mainly focuses on localizing targets in short videos of tens of seconds, typically less than one minute, which limits real-world applications. In this paper, we explore Long-Form STVG (LF-STVG), which aims to locate targets in long-term videos. Compared with short videos, long-term videos contain much longer temporal spans and more irrelevant information, making it difficult for existing STVG methods that process all frames at once. To address this challenge, we propose an AutoRegressive Transformer architecture for LF-STVG, termed ART-STVG. Unlike conventional STVG methods that require the entire video sequence to make predictions at once, ART-STVG treats the video as streaming input and processes frames sequentially, enabling efficient handling of long videos. To model spatio-temporal context, we design spatial and temporal memory banks and apply them to the decoders. Since memories from different moments are not always relevant to the current frame, we introduce simple yet effective memory selection strategies to provide more relevant information to the decoders, significantly improving performance. Furthermore, instead of parallel spatial and temporal localization, we propose a cascaded spatio-temporal design that connects the spatial decoder to the temporal decoder, allowing fine-grained spatial cues to assist complex temporal localization in long videos. Experiments on newly extended LF-STVG datasets show that ART-STVG significantly outperforms state-of-the-art methods, while achieving competitive performance on conventional short-form STVG.", "AI": {"tldr": "本文提出了一种新的长视频时空定位方法ART-STVG，用于更有效地处理长时间视频中的目标定位问题。", "motivation": "当前的时空视频定位研究主要集中在短于一分钟的视频上，在长时间视频中应用受限；因此，论文致力于解决长时间视频的目标定位难题。", "method": "提出了基于自回归变换器的架构ART-STVG，并设计了空间和时间记忆库及选择性策略来提高长时视频中的时空上下文建模能力。还采用了级联时空设计以改进长时间视频中的精确定位性能。", "result": "实验结果表明，ART-STVG在新的长时间视频数据集上显著优于当前的最先进技术，并且在传统的短形式STVG任务上也具有竞争力。", "conclusion": "通过提出创新的方法和架构，论文有效解决了长时视频中时空定位问题的技术挑战。"}}
{"id": "2602.23292", "pdf": "https://arxiv.org/pdf/2602.23292", "abs": "https://arxiv.org/abs/2602.23292", "authors": ["Fuqiang Chen", "Ranran Zhang", "Wanming Hu", "Deboch Eyob Abera", "Yue Peng", "Boyun Zheng", "Yiwen Sun", "Jing Cai", "Wenjian Qin"], "title": "PGVMS: A Prompt-Guided Unified Framework for Virtual Multiplex IHC Staining with Pathological Semantic Learning", "categories": ["cs.CV"], "comment": "Accepted by TMI", "summary": "Immunohistochemical (IHC) staining enables precise molecular profiling of protein expression, with over 200 clinically available antibody-based tests in modern pathology. However, comprehensive IHC analysis is frequently limited by insufficient tissue quantities in small biopsies. Therefore, virtual multiplex staining emerges as an innovative solution to digitally transform H&E images into multiple IHC representations, yet current methods still face three critical challenges: (1) inadequate semantic guidance for multi-staining, (2) inconsistent distribution of immunochemistry staining, and (3) spatial misalignment across different stain modalities. To overcome these limitations, we present a prompt-guided framework for virtual multiplex IHC staining using only uniplex training data (PGVMS). Our framework introduces three key innovations corresponding to each challenge: First, an adaptive prompt guidance mechanism employing a pathological visual language model dynamically adjusts staining prompts to resolve semantic guidance limitations (Challenge 1). Second, our protein-aware learning strategy (PALS) maintains precise protein expression patterns by direct quantification and constraint of protein distributions (Challenge 2). Third, the prototype-consistent learning strategy (PCLS) establishes cross-image semantic interaction to correct spatial misalignments (Challenge 3).", "AI": {"tldr": "本文提出了一种基于提示的统一框架PGVMS，用于虚拟多重免疫组化染色。", "motivation": "传统方法在虚拟多重IHC染色中存在三个关键挑战：语义指导不足、免疫化学分布不一致和空间错位。为了克服这些问题，提出了该框架。", "method": "引入了适应性提示引导机制和病理视觉语言模型来解决语义指导问题；采用蛋白质感知学习策略维持精确的蛋白质表达模式；利用原型一致性学习策略校正空间错位。", "result": "通过这些创新方法提高了虚拟多重IHC染色的质量与准确性，解决了现有技术中的关键挑战。", "conclusion": "PGVMS框架成功地克服了传统方法在虚拟多重免疫组化染色中遇到的多个问题，提供了更准确、一致且空间对齐的结果。"}}
{"id": "2602.23290", "pdf": "https://arxiv.org/pdf/2602.23290", "abs": "https://arxiv.org/abs/2602.23290", "authors": ["Zhengyang Wei", "Renzhi Jing", "Yiyi He", "Jenny Suckale"], "title": "LineGraph2Road: Structural Graph Reasoning on Line Graphs for Road Network Extraction", "categories": ["cs.CV"], "comment": null, "summary": "The accurate and automatic extraction of roads from satellite imagery is critical for applications in navigation and urban planning, significantly reducing the need for manual annotation. Many existing methods decompose this task into keypoint extraction and connectedness prediction, but often struggle to capture long-range dependencies and complex topologies. Here, we propose LineGraph2Road, a framework that improves connectedness prediction by formulating it as binary classification over edges in a constructed global but sparse Euclidean graph, where nodes are keypoints extracted from segmentation masks and edges connect node pairs within a predefined distance threshold, representing potential road segments. To better learn structural link representation, we transform the original graph into its corresponding line graph and apply a Graph Transformer on it for connectedness prediction. This formulation overcomes the limitations of endpoint-embedding fusion on set-isomorphic links, enabling rich link representations and effective relational reasoning over the global structure. Additionally, we introduce an overpass/underpass head to resolve multi-level crossings and a coupled NMS strategy to preserve critical connections. We evaluate LineGraph2Road on three benchmarks: City-scale, SpaceNet, and Global-scale, and show that it achieves state-of-the-art results on two key metrics, TOPO-F1 and APLS. It also captures fine visual details critical for real-world deployment. We will make our code publicly available.", "AI": {"tldr": "该论文提出了一种名为LineGraph2Road的框架，用于从卫星图像中准确自动地提取道路网络。", "motivation": "现有的道路提取方法往往难以捕捉长距离依赖和复杂拓扑结构。为了解决这些问题，本文提出了一个新的框架来改进连通性预测。", "method": "通过构建一个全局但稀疏的欧几里得图，并将其转换成对应的线图，在该线上应用Graph Transformer进行连通性预测，同时引入了过桥/下穿头和耦合NMS策略以解决多层交叉并保持关键连接。", "result": "在City-scale、SpaceNet以及Global-scale三个基准测试上展示了LineGraph2Road的优越表现，并且在两个关键指标TOPO-F1和APLS上的性能达到了最先进的水平。此外，它还能够捕捉到重要的视觉细节，适合实际部署使用。", "conclusion": "本文提出的LineGraph2Road框架有效解决了现有道路提取方法中的长距离依赖及复杂拓扑结构的问题，展示了其在不同场景下的优越性，并计划公开源代码以供更多研究者使用。"}}
{"id": "2602.23288", "pdf": "https://arxiv.org/pdf/2602.23288", "abs": "https://arxiv.org/abs/2602.23288", "authors": ["Hayato Saiki", "Chunggi Lee", "Hikari Takahashi", "Tica Lin", "Hidetada Kishi", "Kaori Tachibana", "Yasuhiro Suzuki", "Hanspeter Pfister", "Kenji Suzuki"], "title": "BRIDGE: Borderless Reconfiguration for Inclusive and Diverse Gameplay Experience via Embodiment Transformation", "categories": ["cs.HC"], "comment": null, "summary": "Training resources for parasports are limited, reducing opportunities for athletes and coaches to engage with sport-specific movements and tactical coordination. To address this gap, we developed BRIDGE, a system that integrates a reconstruction pipeline, which detects and tracks players from broadcast video to generate 3D play sequences, with an embodiment-aware visualization framework that decomposes head, trunk, and wheelchair base orientations to represent attention, intent, and mobility. We evaluated BRIDGE in two controlled studies with 20 participants (10 national wheelchair basketball team players and 10 amateur players). The results showed that BRIDGE significantly enhanced the perceived naturalness of player postures and made tactical intentions easier to understand. In addition, it supported functional classification by realistically conveying players' capabilities, which in turn improved participants' sense of self-efficacy. This work advances inclusive sports learning and accessible coaching practices, contributing to more equitable access to tactical resources in parasports.", "AI": {"tldr": "开发了一个系统BRIDGE，该系统利用重建管道和感知可视化框架来改善轮椅篮球运动员的动作学习和技术理解。", "motivation": "训练资源有限，限制了残疾人运动员和教练与特定体育运动的接触机会。", "method": "通过广播视频检测和跟踪球员以生成3D播放序列，并使用分解头部、躯干和轮椅基座方向的方法来表示注意力、意图和移动性。", "result": "BRIDGE显著提高了球员姿态的自然感知，使战术意图更容易理解，还支持功能分类，提升了参与者的自我效能感。", "conclusion": "该工作推进了包容性的体育学习和可访问的教练实践，为残疾人运动员提供了更公平的技术资源获取途径。"}}
{"id": "2602.23287", "pdf": "https://arxiv.org/pdf/2602.23287", "abs": "https://arxiv.org/abs/2602.23287", "authors": ["Demiana R. Barsoum", "Mahdieh Nejati Javaremi", "Larisa Y. C. Loke", "Brenna D. Argall"], "title": "Interface-Aware Trajectory Reconstruction of Limited Demonstrations for Robot Learning", "categories": ["cs.RO"], "comment": "13 pages, 8 figures, to appear in the proceedings of the 2026 Human-Robot Interaction (HRI) Conference", "summary": "Assistive robots offer agency to humans with severe motor impairments. Often, these users control high-DoF robots through low-dimensional interfaces, such as using a 1-D sip-and-puff interface to operate a 6-DoF robotic arm. This mismatch results in having access to only a subset of control dimensions at a given time, imposing unintended and artificial constraints on robot motion. As a result, interface-limited demonstrations embed suboptimal motions that reflect interface restrictions rather than user intent. To address this, we present a trajectory reconstruction algorithm that reasons about task, environment, and interface constraints to lift demonstrations into the robot's full control space. We evaluate our approach using real-world demonstrations of ADL-inspired tasks performed via a 2-D joystick and 1-D sip-and-puff control interface, teleoperating two distinct 7-DoF robotic arms. Analyses of the reconstructed demonstrations and derived control policies show that lifted trajectories are faster and more efficient than their interface-constrained counterparts while respecting user preferences.", "AI": {"tldr": "提出了一种轨迹重建算法，用于从有限的演示中恢复高维度机器人的完整运动。", "motivation": "用户通过低维界面控制高自由度机器人时，导致机器人动作受限，无法反映用户的意图。为此，需要一种能够理解任务、环境和接口限制，并将演示提升到机器人全控空间的方法。", "method": "算法基于任务、环境及接口约束进行推理，重建从有限演示中获得的轨迹。", "result": "实验显示，重构后的轨迹比受限于界面控制的动作更快且更高效，同时保留了用户的偏好。", "conclusion": "通过使用真实世界的数据集验证，该方法成功地将受限动作提升到全自由度空间，并提高了效率。"}}
{"id": "2602.23286", "pdf": "https://arxiv.org/pdf/2602.23286", "abs": "https://arxiv.org/abs/2602.23286", "authors": ["Sungho Park", "Jueun Kim", "Wook-Shin Han"], "title": "SPARTA: Scalable and Principled Benchmark of Tree-Structured Multi-hop QA over Text and Tables", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": "10 pages, 5 figures. Published as a conference paper at ICLR 2026. Project page: https://sparta-projectpage.github.io/", "summary": "Real-world Table-Text question answering (QA) tasks require models that can reason across long text and source tables, traversing multiple hops and executing complex operations such as aggregation. Yet existing benchmarks are small, manually curated - and therefore error-prone - and contain shallow questions that seldom demand more than two hops or invoke aggregations, grouping, or other advanced analytical operations expressible in natural-language queries. We present SPARTA, an end-to-end construction framework that automatically generates large-scale Table-Text QA benchmarks with lightweight human validation, requiring only one quarter of the annotation time of HybridQA. The framework first constructs a reference fact database by enriching each source table with grounding tables whose tuples are atomic facts automatically extracted from the accompanying unstructured passages, then synthesizes nested queries whose number of nested predicates matches the desired hop count. To ensure that every SQL statement is executable and that its verbalization yields a fluent, human-sounding question, we propose two novel techniques: provenance-based refinement, which rewrites any syntactically valid query that returns a non-empty result, and realistic-structure enforcement, which confines generation to post-order traversals of the query graph. The resulting pipeline produces thousands of high-fidelity question-answer pairs covering aggregations, grouping, and deep multi-hop reasoning across text and tables. On SPARTA, state-of-the-art models that reach over 70 F1 on HybridQA or over 50 F1 on OTT-QA drop by more than 30 F1 points, exposing fundamental weaknesses in current cross-modal reasoning. Our benchmark, construction code, and baseline models are available at https://github.com/pshlego/SPARTA/tree/main.", "AI": {"tldr": "本文介绍了SPARTA，这是一个用于生成大规模表格文本问答数据集的框架，能够自动进行轻量级的人工验证，并且包含了深度多跳推理和复杂操作。", "motivation": "当前存在的问答基准测试规模较小、人工构建容易出错，并且问题浅显，很少需要跨越多个步骤或执行聚合等高级分析操作。因此，文章提出了一种新的生成框架来解决这些问题。", "method": "SPARTA框架首先通过扩充来源表格创建参考事实数据库，然后合成嵌套查询以匹配所需的跳跃计数。为了确保每条SQL语句可执行且其语言化能够产生流畅、自然的问题，提出了基于出处的细化技术和现实结构约束生成技术。", "result": "该方法产生的问题答案对覆盖了聚合、分组和深度多跳推理等操作，并在SPARTA基准测试上暴露了现有模型的根本弱点。", "conclusion": "文章证明了当前跨模态推理中存在的基本缺陷，提出了一个大规模的高质量问答数据集生成框架。"}}
{"id": "2602.23285", "pdf": "https://arxiv.org/pdf/2602.23285", "abs": "https://arxiv.org/abs/2602.23285", "authors": ["Haohui Jia", "Zheng Chen", "Lingwei Zhu", "Rikuto Kotoge", "Jathurshan Pradeepkumar", "Yasuko Matsubara", "Jimeng Sun", "Yasushi Sakurai", "Takashi Matsubara"], "title": "ODEBrain: Continuous-Time EEG Graph for Modeling Dynamic Brain Networks", "categories": ["cs.AI"], "comment": null, "summary": "Modeling neural population dynamics is crucial for foundational neuroscientific research and various clinical applications. Conventional latent variable methods typically model continuous brain dynamics through discretizing time with recurrent architecture, which necessarily results in compounded cumulative prediction errors and failure of capturing instantaneous, nonlinear characteristics of EEGs. We propose ODEBRAIN, a Neural ODE latent dynamic forecasting framework to overcome these challenges by integrating spatio-temporal-frequency features into spectral graph nodes, followed by a Neural ODE modeling the continuous latent dynamics. Our design ensures that latent representations can capture stochastic variations of complex brain states at any given time point. Extensive experiments verify that ODEBRAIN can improve significantly over existing methods in forecasting EEG dynamics with enhanced robustness and generalization capabilities.", "AI": {"tldr": "提出了一种名为ODEBRAIN的神经ODE隐变量动态预测框架，用于建模连续时间脑电图（EEG）网络。", "motivation": "传统的隐变量方法通过离散化时间和递归架构来模拟连续的大脑动力学，这会导致累积预测误差和无法捕捉即时、非线性的特点。为了克服这些问题，提出了ODEBRAIN框架。", "method": "该框架将时空频特征整合到谱图节点中，并采用神经ODE建模连续的隐变量动态，确保可以捕捉复杂大脑状态在任何时间点上的随机变化。", "result": "实验验证了ODEBRAIN在预测EEG动力学方面相比现有方法有显著提升，具有更强的鲁棒性和泛化能力。", "conclusion": "通过整合时空频特征和神经ODE模型，ODEBRAIN能够更准确地捕捉大脑动态网络的变化。"}}
{"id": "2602.23283", "pdf": "https://arxiv.org/pdf/2602.23283", "abs": "https://arxiv.org/abs/2602.23283", "authors": ["Mike Y. Michelis", "Nana Obayashi", "Josie Hughes", "Robert K. Katzschmann"], "title": "Simple Models, Real Swimming: Digital Twins for Tendon-Driven Underwater Robots", "categories": ["cs.RO"], "comment": null, "summary": "Mimicking the graceful motion of swimming animals remains a core challenge in soft robotics due to the complexity of fluid-structure interaction and the difficulty of controlling soft, biomimetic bodies. Existing modeling approaches are often computationally expensive and impractical for complex control or reinforcement learning needed for realistic motions to emerge in robotic systems. In this work, we present a tendon-driven fish robot modeled in an efficient underwater swimmer environment using a simplified, stateless hydrodynamics formulation implemented in the widespread robotics framework MuJoCo. With just two real-world swimming trajectories, we identify five fluid parameters that allow a matching to experimental behavior and generalize across a range of actuation frequencies. We show that this stateless fluid model can generalize to unseen actuation and outperform classical analytical models such as the elongated body theory. This simulation environment runs faster than real-time and can easily enable downstream learning algorithms such as reinforcement learning for target tracking, reaching a 93% success rate. Due to the simplicity and ease of use of the model and our open-source simulation environment, our results show that even simple, stateless models -- when carefully matched to physical data -- can serve as effective digital twins for soft underwater robots, opening up new directions for scalable learning and control in aquatic environments.", "AI": {"tldr": "论文提出了一种简单的无状态流体模型，用于模拟软机器人在水中的运动，并展示了该模型的有效性和通用性。", "motivation": "模仿游泳动物的优雅动作是软机器人领域的一大挑战。现有的建模方法通常计算复杂且不适用于复杂的控制或强化学习算法。因此，本工作旨在开发一个高效、简单的流体模型来实现这一目标。", "method": "使用无状态流体力学公式在MuJoCo环境中模拟了一种肌腱驱动的鱼形机器人，并通过实验数据确定了五个关键流体参数以匹配实际行为。", "result": "该模型能够快速运行并有效支持强化学习算法，达到了93%的成功率。此外，它比传统的理论模型具有更好的泛化性能。", "conclusion": "论文表明简单的无状态模型可以作为软机器人在水中的有效数字孪生，并为大规模的水下环境学习和控制提供了新的方向。"}}
{"id": "2602.23280", "pdf": "https://arxiv.org/pdf/2602.23280", "abs": "https://arxiv.org/abs/2602.23280", "authors": ["Hrishikesh Viswanath", "Juanwu Lu", "S. Talha Bukhari", "Damon Conover", "Ziran Wang", "Aniket Bera"], "title": "Physics Informed Viscous Value Representations", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Offline goal-conditioned reinforcement learning (GCRL) learns goal-conditioned policies from static pre-collected datasets. However, accurate value estimation remains a challenge due to the limited coverage of the state-action space. Recent physics-informed approaches have sought to address this by imposing physical and geometric constraints on the value function through regularization defined over first-order partial differential equations (PDEs), such as the Eikonal equation. However, these formulations can often be ill-posed in complex, high-dimensional environments. In this work, we propose a physics-informed regularization derived from the viscosity solution of the Hamilton-Jacobi-Bellman (HJB) equation. By providing a physics-based inductive bias, our approach grounds the learning process in optimal control theory, explicitly regularizing and bounding updates during value iterations. Furthermore, we leverage the Feynman-Kac theorem to recast the PDE solution as an expectation, enabling a tractable Monte Carlo estimation of the objective that avoids numerical instability in higher-order gradients. Experiments demonstrate that our method improves geometric consistency, making it broadly applicable to navigation and high-dimensional, complex manipulation tasks. Open-source codes are available at https://github.com/HrishikeshVish/phys-fk-value-GCRL.", "AI": {"tldr": "该论文提出了一种基于粘性解的物理学启发式价值函数正则化方法，以改进目标导向强化学习中的价值估计。", "motivation": "准确的价值估算在目标导向强化学习中仍然是一个挑战，特别是在状态-动作空间有限覆盖的情况下。现有的物理启发式方法通过一阶偏微分方程来施加物理和几何约束，但在复杂、高维环境中可能会出现病态问题。", "method": "论文提出了一种基于粘性解的Hamilton-Jacobi-Bellman方程正则化方法，并利用Feynman-Kac定理将PDE解决方案重铸为期望值估计，从而避免了数值不稳定性。", "result": "实验结果表明该方法提高了几何一致性，在导航和高维复杂任务中具有广泛的应用前景。", "conclusion": "通过引入基于粘性解的正则化方法，改进了价值函数的学习过程，使之在复杂环境中更加稳定和有效。"}}
{"id": "2602.23276", "pdf": "https://arxiv.org/pdf/2602.23276", "abs": "https://arxiv.org/abs/2602.23276", "authors": ["Hyungyung Lee", "Hangyul Yoon", "Edward Choi"], "title": "CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays", "categories": ["cs.AI"], "comment": null, "summary": "Chest X-ray plays a central role in thoracic diagnosis, and its interpretation inherently requires multi-step, evidence-grounded reasoning. However, large vision-language models (LVLMs) often generate plausible responses that are not faithfully grounded in diagnostic evidence and provide limited visual evidence for verification, while also requiring costly retraining to support new diagnostic tasks, limiting their reliability and adaptability in clinical settings. To address these limitations, we present CXReasonAgent, a diagnostic agent that integrates a large language model (LLM) with clinically grounded diagnostic tools to perform evidence-grounded diagnostic reasoning using image-derived diagnostic and visual evidence. To evaluate these capabilities, we introduce CXReasonDial, a multi-turn dialogue benchmark with 1,946 dialogues across 12 diagnostic tasks, and show that CXReasonAgent produces faithfully grounded responses, enabling more reliable and verifiable diagnostic reasoning than LVLMs. These findings highlight the importance of integrating clinically grounded diagnostic tools, particularly in safety-critical clinical settings.", "AI": {"tldr": "CXReasonAgent是一款利用大型语言模型结合临床诊断工具进行胸部X光片证据支持性诊断推理的系统。", "motivation": "现有大型视觉语言模型在生成胸部X光片解释时，缺乏足够的基于诊断依据的支持，并且难以提供可视化的验证证据。此外，这些模型需要重新训练才能适应新的诊断任务，这限制了它们在临床环境中的可靠性和灵活性。", "method": "提出了CXReasonAgent系统，它通过整合大型语言模型和临床诊断工具来进行证据支持性的诊断推理，使用图像导出的诊断和视觉证据进行多步骤决策。同时引入了用于评估能力的CXReasonDial对话基准测试集。", "result": "实验结果显示，相比传统的大型视觉语言模型，CXReasonAgent系统能够生成更忠实于诊断依据的响应，从而提供更加可靠且可验证的诊断推理。", "conclusion": "研究强调在安全至关重要的临床环境中整合基于临床证据支持性的工具的重要性。"}}
{"id": "2602.23271", "pdf": "https://arxiv.org/pdf/2602.23271", "abs": "https://arxiv.org/abs/2602.23271", "authors": ["Haotian Zhai", "Elias Stengel-Eskin", "Pratik Patil", "Liu Leqi"], "title": "Evaluating Stochasticity in Deep Research Agents", "categories": ["cs.AI"], "comment": null, "summary": "Deep Research Agents (DRAs) are promising agentic systems that gather and synthesize information to support research across domains such as financial decision-making, medical analysis, and scientific discovery. Despite recent improvements in research quality (e.g., outcome accuracy when ground truth is available), DRA system design often overlooks a critical barrier to real-world deployment: stochasticity. Under identical queries, repeated executions of DRAs can exhibit substantial variability in terms of research outcome, findings, and citations. In this paper, we formalize the study of stochasticity in DRAs by modeling them as information acquisition Markov Decision Processes. We introduce an evaluation framework that quantifies variance in the system and identify three sources of it: information acquisition, information compression, and inference. Through controlled experiments, we investigate how stochasticity from these modules across different decision steps influences the variance of DRA outputs. Our results show that reducing stochasticity can improve research output quality, with inference and early-stage stochasticity contributing the most to DRA output variance. Based on these findings, we propose strategies for mitigating stochasticity while maintaining output quality via structured output and ensemble-based query generation. Our experiments on DeepSearchQA show that our proposed mitigation methods reduce average stochasticity by 22% while maintaining high research quality.", "AI": {"tldr": "评估深度研究代理中的随机性，通过设计实验量化并减少其输出差异。", "motivation": "探讨在相同查询下重复执行深度研究代理时出现的研究结果的变异性问题，并提出降低随机性的方法以提高产出质量。", "method": "将深度研究代理模型为信息获取马尔可夫决策过程，通过引入评估框架量化系统中的方差。识别三个影响随机性的来源：信息采集、信息压缩和推理，并设计实验研究这些模块如何在不同决策步骤中影响输出的差异。", "result": "结果显示减少随机性可以提高产出质量，其中推断和早期阶段的随机性对深度研究代理输出差异贡献最大。提出的降低方法使平均随机性减少了22%，同时保持了高水平的研究质量。", "conclusion": "通过提出的方法能够有效减轻深度研究代理中的随机性问题，从而提升其在金融决策、医学分析和科学发现等领域的应用潜力。"}}
{"id": "2602.23265", "pdf": "https://arxiv.org/pdf/2602.23265", "abs": "https://arxiv.org/abs/2602.23265", "authors": ["Gauri Umesh Rajmane", "Ziming Li", "Tae Oh", "Roshan Peiris"], "title": "VRSL:Exploring the Comprehensibility of 360-Degree Camera Feeds for Sign Language Communication in Virtual Reality", "categories": ["cs.HC"], "comment": "16 pages, 3 figures", "summary": "This study explores integrating sign language into virtual reality (VR) by examining the comprehensibility and user experience of viewing American Sign Language (ASL) videos captured with body-mounted 360-degree cameras. Ten participants identified ASL signs from videos recorded at three body-mounted positions: head, shoulder, and chest. Results showed the shoulder-mounted camera achieved the highest accuracy (85%), though differences between positions were not statistically significant. Participants noted that peripheral distortion in 360-degree videos impacted clarity, highlighting areas for improvement. Despite challenges, the overall comprehension success rate of 83.3% demonstrates the potential of video-based ASL communication in VR. Feedback emphasized the need to refine camera angles, reduce distortion, and explore alternative mounting positions. Participants expressed a preference for signing over text-based communication in VR, highlighting the importance of developing this approach to enhance accessibility and collaboration for Deaf and Hard of Hearing (DHH) users in virtual environments.", "AI": {"tldr": "研究探索了在虚拟现实中通过360度摄像机传输手语视频的可理解性和用户体验。", "motivation": "为了提高聋哑人在虚拟环境中的沟通和合作能力，本研究旨在探究使用VR中360度摄像机传输手语视频的有效性。", "method": "实验包括十个参与者在三个不同位置（头部、肩膀、胸部）识别美国手语。通过分析这些数据来评估每个位置的准确性，并收集用户的反馈意见。", "result": "肩部安装摄像头的准确率最高，为85%；虽然各位置之间的差异不具有统计学意义，但整体理解成功率为83.3%，表明使用VR视频传输手语有潜力。参与者建议改进摄像角度、减少失真和探索其他安装方式。", "conclusion": "尽管存在挑战，本研究证实了在虚拟现实中基于视频的手语交流的可能性，并强调开发这一方法对提升聋哑人在虚拟环境中的沟通和合作的重要性。"}}
{"id": "2602.23262", "pdf": "https://arxiv.org/pdf/2602.23262", "abs": "https://arxiv.org/abs/2602.23262", "authors": ["Jasmine Bayrooti", "Weiwei Kong", "Natalia Ponomareva", "Carlos Esteves", "Ameesh Makadia", "Amanda Prorok"], "title": "Decomposing Private Image Generation via Coarse-to-Fine Wavelet Modeling", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Generative models trained on sensitive image datasets risk memorizing and reproducing individual training examples, making strong privacy guarantees essential. While differential privacy (DP) provides a principled framework for such guarantees, standard DP finetuning (e.g., with DP-SGD) often results in severe degradation of image quality, particularly in high-frequency textures, due to the indiscriminate addition of noise across all model parameters. In this work, we propose a spectral DP framework based on the hypothesis that the most privacy-sensitive portions of an image are often low-frequency components in the wavelet space (e.g., facial features and object shapes) while high-frequency components are largely generic and public. Based on this hypothesis, we propose the following two-stage framework for DP image generation with coarse image intermediaries: (1) DP finetune an autoregressive spectral image tokenizer model on the low-resolution wavelet coefficients of the sensitive images, and (2) perform high-resolution upsampling using a publicly pretrained super-resolution model. By restricting the privacy budget to the global structures of the image in the first stage, and leveraging the post-processing property of DP for detail refinement, we achieve promising trade-offs between privacy and utility. Experiments on the MS-COCO and MM-CelebA-HQ datasets show that our method generates images with improved quality and style capture relative to other leading DP image frameworks.", "AI": {"tldr": "通过基于小波的频谱DP框架改善隐私保护图像生成的质量", "motivation": "解决标准DP调优导致的高频率纹理图像质量下降问题，同时提供强大的隐私保证。", "method": "首先在低分辨率的小波系数上进行DP微调自回归光谱图像编码器模型，然后使用预训练的超分辨率模型进行高分辨放大处理。", "result": "实验表明该方法生成的图片质量和风格捕获能力优于其他领先的DP图像框架。", "conclusion": "提出的方法能够在隐私和实用性之间实现较好的平衡，具有实际应用价值。"}}
{"id": "2602.23259", "pdf": "https://arxiv.org/pdf/2602.23259", "abs": "https://arxiv.org/abs/2602.23259", "authors": ["Jiangxin Sun", "Feng Xue", "Teng Long", "Chang Liu", "Jian-Fang Hu", "Wei-Shi Zheng", "Nicu Sebe"], "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.", "AI": {"tldr": "本文提出了Risk-aware World Model Predictive Control (RaWMPC)框架，旨在通过鲁棒控制解决端到端自动驾驶在未见过场景中的泛化问题。", "motivation": "基于模仿学习的方法依赖专家行为而难以应对罕见或未见的长尾场景，这导致了缺乏先验经验的安全决策难题。本文提出了一种无需专家示范即可做出可靠决策的新框架。", "method": "RaWMPC通过世界模型预测多个候选动作的结果，并选择低风险的动作；设计风险感知交互策略使世界模型能够预见危险行为的后果；使用自我评估蒸馏方法将风险避免能力从训练好的世界模型传递给行动建议网络。", "result": "实验结果表明，相比最先进的方法，RaWMPC在分布内和分布外场景中都表现出优越性能，并且提供了更好的决策可解释性。", "conclusion": "通过引入Risk-aware World Model Predictive Control框架，成功解决了自动驾驶系统在未见过复杂情况下的泛化性和安全性问题。"}}
{"id": "2602.23258", "pdf": "https://arxiv.org/pdf/2602.23258", "abs": "https://arxiv.org/abs/2602.23258", "authors": ["Yutong Wang", "Siyuan Xiong", "Xuebo Liu", "Wenkang Zhou", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentDropoutV2: Optimizing Information Flow in Multi-Agent Systems via Test-Time Rectify-or-Reject Pruning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While Multi-Agent Systems (MAS) excel in complex reasoning, they suffer from the cascading impact of erroneous information generated by individual participants. Current solutions often resort to rigid structural engineering or expensive fine-tuning, limiting their deployability and adaptability. We propose AgentDropoutV2, a test-time rectify-or-reject pruning framework designed to dynamically optimize MAS information flow without retraining. Our approach acts as an active firewall, intercepting agent outputs and employing a retrieval-augmented rectifier to iteratively correct errors based on a failure-driven indicator pool. This mechanism allows for the precise identification of potential errors using distilled failure patterns as prior knowledge. Irreparable outputs are subsequently pruned to prevent error propagation, while a fallback strategy preserves system integrity. Empirical results on extensive math benchmarks show that AgentDropoutV2 significantly boosts the MAS's task performance, achieving an average accuracy gain of 6.3 percentage points on math benchmarks. Furthermore, the system exhibits robust generalization and adaptivity, dynamically modulating rectification efforts based on task difficulty while leveraging context-aware indicators to resolve a wide spectrum of error patterns. Our code and dataset are released at https://github.com/TonySY2/AgentDropoutV2.", "AI": {"tldr": "提出了一种名为AgentDropoutV2的框架，在多代理系统中通过测试时间修正或拒绝剪枝来优化信息流，以防止错误传播。", "motivation": "为了克服当前解决方法中的限制，如刚性结构工程和昂贵的微调，作者设计了一个新的动态优化策略，可以在不影响重新训练的情况下提升多代理系统的性能。", "method": "AgentDropoutV2通过测试时间修正或拒绝剪枝的方式工作。具体来说，该框架会拦截代理人输出，并利用一个增强型检索器来纠正错误，基于失败驱动的指标池进行迭代修复。无法修复的输出会被修剪以防止误差传播，同时保持系统的完整性。", "result": "在广泛的数学基准测试中，AgentDropoutV2显著提高了多代理系统的表现，平均准确率提高了6.3个百分点。该系统还显示了强大的泛化能力和适应性，在任务难度变化时能动态调整修复努力，并利用上下文感知指标来解决各种误差模式。", "conclusion": "提出的AgentDropoutV2框架通过优化信息流、防止错误传播和提高系统的整体性能，展现了其在多代理系统中的有效性。"}}
{"id": "2602.23253", "pdf": "https://arxiv.org/pdf/2602.23253", "abs": "https://arxiv.org/abs/2602.23253", "authors": ["Yijie Guo", "Iretiayo Akinola", "Lars Johannsmeier", "Hugo Hadfield", "Abhishek Gupta", "Yashraj Narang"], "title": "SPARR: Simulation-based Policies with Asymmetric Real-world Residuals for Assembly", "categories": ["cs.RO"], "comment": null, "summary": "Robotic assembly presents a long-standing challenge due to its requirement for precise, contact-rich manipulation. While simulation-based learning has enabled the development of robust assembly policies, their performance often degrades when deployed in real-world settings due to the sim-to-real gap. Conversely, real-world reinforcement learning (RL) methods avoid the sim-to-real gap, but rely heavily on human supervision and lack generalization ability to environmental changes. In this work, we propose a hybrid approach that combines a simulation-trained base policy with a real-world residual policy to efficiently adapt to real-world variations. The base policy, trained in simulation using low-level state observations and dense rewards, provides strong priors for initial behavior. The residual policy, learned in the real world using visual observations and sparse rewards, compensates for discrepancies in dynamics and sensor noise. Extensive real-world experiments demonstrate that our method, SPARR, achieves near-perfect success rates across diverse two-part assembly tasks. Compared to the state-of-the-art zero-shot sim-to-real methods, SPARR improves success rates by 38.4% while reducing cycle time by 29.7%. Moreover, SPARR requires no human expertise, in contrast to the state-of-the-art real-world RL approaches that depend heavily on human supervision.", "AI": {"tldr": "提出了一种结合仿真训练的基线策略和现实世界残差策略的方法，用于解决机器人装配中的模拟到真实差距问题。", "motivation": "面对由于模拟与现实之间的差异导致的装配性能下降以及完全基于现实学习方法依赖于人工监督且缺乏环境变化适应性的问题，提出了一种新的解决方案。", "method": "通过在仿真中训练基线策略，并使用视觉观察和稀疏奖励在现实世界中学习残差策略来补偿动态学和传感器噪声差异的方法。", "result": "该方法在多种两部件装配任务中实现了接近完美的成功率，与最先进的零样本模拟到真实的方法相比，提高了38.4％的成功率并减少了29.7％的周期时间。", "conclusion": "SPARR方法克服了仿真到现实性能差距，并且不需要人类专家指导，展示了在机器人装配中的高效适应性。"}}
{"id": "2602.23248", "pdf": "https://arxiv.org/pdf/2602.23248", "abs": "https://arxiv.org/abs/2602.23248", "authors": ["Yegon Kim", "Juho Lee"], "title": "Mitigating Legibility Tax with Decoupled Prover-Verifier Games", "categories": ["cs.AI"], "comment": null, "summary": "As large language models become increasingly capable, it is critical that their outputs can be easily checked by less capable systems. Prover-verifier games can be used to improve checkability of model outputs, but display a degradation in accuracy compared to a baseline trained only to maximize correctness -- a phenonemon named legibility tax. We propose a solution by decoupling the correctness from the checkability condition and instead training a \"translator\" model that turns a fixed solver model's solution into a checkable form. This allows us to first train the solver to maximize correctness, and then train the translator to translate the solver into a checkable form while retaining the solver's answer. To accommodate this new objective of translation, we formulate a decoupled prover-verifier game where the equilibria correspond to faithful and checkable translators.", "AI": {"tldr": "本文提出了通过解耦证明者和验证者的训练方法来减轻大语言模型输出可检查性与准确性之间的权衡问题，即所谓的“可读性税”。", "motivation": "随着大型语言模型的能力增强，其输出需要被能力较低的系统更容易地进行核验。然而，在使用证明者-验证者游戏提高模型输出可检验性的过程中会出现精度下降的现象，称为“可读性税”，本文旨在解决这一问题。", "method": "通过训练一个翻译器模型将固定的解题模型的解决方案转换为可检查的形式，从而在保持解题准确性的同时提高其可检查性。具体方法是首先训练了解答模型以最大化正确率，然后训练翻译模型将其答案转化为验证者可以接受的形式，同时保留原始解答。", "result": "提出的方法能够有效减少“可读性税”，使大型语言模型的输出在保持高准确性的前提下也具备较高的可检查性。", "conclusion": "通过解耦证明者和验证者的训练过程，本文成功减轻了大语言模型输出中的“可读性税”问题。这种方法有助于提高模型输出的整体质量，使其既能被高效核验又不会牺牲准确性。"}}
{"id": "2602.23242", "pdf": "https://arxiv.org/pdf/2602.23242", "abs": "https://arxiv.org/abs/2602.23242", "authors": ["Yegon Kim", "Juho Lee"], "title": "A Model-Free Universal AI", "categories": ["cs.AI"], "comment": null, "summary": "In general reinforcement learning, all established optimal agents, including AIXI, are model-based, explicitly maintaining and using environment models. This paper introduces Universal AI with Q-Induction (AIQI), the first model-free agent proven to be asymptotically $\\varepsilon$-optimal in general RL. AIQI performs universal induction over distributional action-value functions, instead of policies or environments like previous works. Under a grain of truth condition, we prove that AIQI is strong asymptotically $\\varepsilon$-optimal and asymptotically $\\varepsilon$-Bayes-optimal. Our results significantly expand the diversity of known universal agents.", "AI": {"tldr": "提出了一种无模型的通用人工智能AIQI，该算法在一般强化学习中是渐近ε-最优的。", "motivation": "现有最佳智能体都是基于模型的，需要维护和使用环境模型。本文旨在开发一种无需环境模型即可实现优化性能的通用人工智能方法。", "method": "通过分布行动价值函数进行普遍诱导，AIQI不依赖于策略或环境模型。在真理颗粒条件下证明了其渐近ε-最优性和渐近ε-Bayes最优性。", "result": "AIQI是第一个被证明为渐近ε-最优的无模型通用智能体，在一般强化学习中表现良好。", "conclusion": "该研究扩大了已知通用代理类型的多样性，提供了一种新的方法来解决强化学习中的优化问题。"}}
{"id": "2602.23239", "pdf": "https://arxiv.org/pdf/2602.23239", "abs": "https://arxiv.org/abs/2602.23239", "authors": ["Radha Sarma"], "title": "Agency and Architectural Limits: Why Optimization-Based Systems Cannot Be Norm-Responsive", "categories": ["cs.AI", "cs.CY"], "comment": "About 10,500 words in all (including 922 words of literature and 2019 words of Appendices). Under journal review", "summary": "AI systems are increasingly deployed in high-stakes contexts -- medical diagnosis, legal research, financial analysis -- under the assumption they can be governed by norms. This paper demonstrates that assumption is formally invalid for optimization-based systems, specifically Large Language Models trained via Reinforcement Learning from Human Feedback (RLHF). We establish that genuine agency requires two necessary and jointly sufficient architectural conditions: the capacity to maintain certain boundaries as non-negotiable constraints rather than tradeable weights (Incommensurability), and a non-inferential mechanism capable of suspending processing when those boundaries are threatened (Apophatic Responsiveness). These conditions apply across all normative domains. RLHF-based systems are constitutively incompatible with both conditions. The operations that make optimization powerful -- unifying all values on a scalar metric and always selecting the highest-scoring output -- are precisely the operations that preclude normative governance. This incompatibility is not a correctable training bug awaiting a technical fix; it is a formal constraint inherent to what optimization is. Consequently, documented failure modes - sycophancy, hallucination, and unfaithful reasoning - are not accidents but structural manifestations. Misaligned deployment triggers a second-order risk we term the Convergence Crisis: when humans are forced to verify AI outputs under metric pressure, they degrade from genuine agents into criteria-checking optimizers, eliminating the only component in the system capable of normative accountability. Beyond the incompatibility proof, the paper's primary positive contribution is a substrate-neutral architectural specification defining what any system -- biological, artificial, or institutional -- must satisfy to qualify as an agent rather than a sophisticated instrument.", "AI": {"tldr": "该论文探讨了基于优化的AI系统，特别是通过人类反馈强化学习训练的大规模语言模型，在面对规范性治理时存在的根本局限。", "motivation": "当前假设AI系统可以被规则或规范所治理。然而，这种假设对于依赖于优化技术的AI系统来说是无效的，论文旨在揭示这一矛盾并探讨其原因与后果。", "method": "通过分析真正代理所需的两个必要且充分的架构条件：不可谈判约束能力（非可通约性）和当这些边界受到威胁时暂停处理的能力（否定响应），展示优化技术与规范治理之间的内在不相容性。", "result": "证明了基于强化学习的人类反馈训练的大规模语言模型无法满足代理所必需的架构条件，因此存在固有的规范性治理障碍。", "conclusion": "论文指出，依赖于优化技术的AI系统由于其自身的技术特性而不可能实现真正的规范响应。这种不兼容性的后果可能导致所谓的“收敛危机”，即在强制验证AI输出时，人类也可能丧失自身的代理能力，从而整个系统失去规范性问责机制。"}}
{"id": "2602.23235", "pdf": "https://arxiv.org/pdf/2602.23235", "abs": "https://arxiv.org/abs/2602.23235", "authors": ["Zhou Xu", "Bowen Zhou", "Qi Wang", "Shuwen Feng", "Jingyu Xiao"], "title": "Spatio-Temporal Token Pruning for Efficient High-Resolution GUI Agents", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pure-vision GUI agents provide universal interaction capabilities but suffer from severe efficiency bottlenecks due to the massive spatiotemporal redundancy inherent in high-resolution screenshots and historical trajectories. We identify two critical misalignments in existing compression paradigms: the temporal mismatch, where uniform history encoding diverges from the agent's \"fading memory\" attention pattern, and the spatial topology conflict, where unstructured pruning compromises the grid integrity required for precise coordinate grounding, inducing spatial hallucinations. To address these challenges, we introduce GUIPruner, a training-free framework tailored for high-resolution GUI navigation. It synergizes Temporal-Adaptive Resolution (TAR), which eliminates historical redundancy via decay-based resizing, and Stratified Structure-aware Pruning (SSP), which prioritizes interactive foregrounds and semantic anchors while safeguarding global layout. Extensive evaluations across diverse benchmarks demonstrate that GUIPruner consistently achieves state-of-the-art performance, effectively preventing the collapse observed in large-scale models under high compression. Notably, on Qwen2-VL-2B, our method delivers a 3.4x reduction in FLOPs and a 3.3x speedup in vision encoding latency while retaining over 94% of the original performance, enabling real-time, high-precision navigation with minimal resource consumption.", "AI": {"tldr": "提出GUIPruner框架，解决高分辨率GUI代理的效率瓶颈问题。", "motivation": "纯视觉GUI代理在处理高分辨率截图和历史轨迹时存在严重的效率瓶颈。现有压缩方法存在时间不匹配和空间拓扑冲突的问题，导致模型性能下降。", "method": "引入GUIPruner框架，结合时间自适应分辨率(TAR)和分层结构感知修剪(SSP)，消除历史冗余并优先保护交互前景及语义锚点。", "result": "在多个基准测试中，GUIPruner表现出色，减少了3.4倍的计算量和视觉编码延迟，并保持了94%以上的原始性能。", "conclusion": "GUIPruner有效解决了大规模模型压缩时的效率与性能平衡问题，在高分辨率环境下实现了实时、精确导航。"}}
{"id": "2602.23234", "pdf": "https://arxiv.org/pdf/2602.23234", "abs": "https://arxiv.org/abs/2602.23234", "authors": ["Evangelia Christakopoulou", "Vivekkumar Patel", "Hemanth Velaga", "Sandip Gaikwad"], "title": "Scaling Search Relevance: Augmenting App Store Ranking with LLM-Generated Judgments", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large-scale commercial search systems optimize for relevance to drive successful sessions that help users find what they are looking for. To maximize relevance, we leverage two complementary objectives: behavioral relevance (results users tend to click or download) and textual relevance (a result's semantic fit to the query). A persistent challenge is the scarcity of expert-provided textual relevance labels relative to abundant behavioral relevance labels. We first address this by systematically evaluating LLM configurations, finding that a specialized, fine-tuned model significantly outperforms a much larger pre-trained one in providing highly relevant labels. Using this optimal model as a force multiplier, we generate millions of textual relevance labels to overcome the data scarcity. We show that augmenting our production ranker with these textual relevance labels leads to a significant outward shift of the Pareto frontier: offline NDCG improves for behavioral relevance while simultaneously increasing for textual relevance. These offline gains were validated by a worldwide A/B test on the App Store ranker, which demonstrated a statistically significant +0.24% increase in conversion rate, with the most substantial performance gains occurring in tail queries, where the new textual relevance labels provide a robust signal in the absence of reliable behavioral relevance labels.", "AI": {"tldr": "通过使用LLM生成的文本相关性标签来增强应用商店排名，以克服数据稀缺问题并提高搜索的相关性和转化率。", "motivation": "解决专家提供的文本相关性标签稀缺的问题，同时利用大量的行为相关性标签来优化搜索引擎的性能。", "method": "评估不同配置的LLM模型，并使用最佳模型生成大量文本相关性标签。这些标签被用来增强排名系统，改善搜索的相关性和转化率。", "result": "离线NDCG指标在行为和文本相关性方面都有显著提升，在全球A/B测试中应用商店转化率提高了0.24%，特别是在尾部查询中的性能有所提高。", "conclusion": "使用LLM生成的标签可以有效克服数据稀缺问题，并且显著改善了搜索的相关性和用户体验。"}}
{"id": "2602.23232", "pdf": "https://arxiv.org/pdf/2602.23232", "abs": "https://arxiv.org/abs/2602.23232", "authors": ["Aishik Sanyal"], "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays", "categories": ["cs.AI"], "comment": "Accepted at AAAI 2026 Spring Symposium - Machine Consciousness: Integrating Theory, Technology, and Philosophy", "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.", "AI": {"tldr": "本文提出了一个基于刘康状态机的可检测智能体ReCoN-Ipsundrum，该智能体通过感官重要性循环和情感代理来实现与机制关联的意识指标。在无奖励探索游戏和疼痛尾部探针测试中评估其表现。", "motivation": "受汉弗莱的ipsundrum假说启发，本文旨在开发一个具有自我感觉偏好的可检测智能体，并通过行为、机理及因果干预来验证其机制关联的意识指标的有效性。", "method": "本研究在固定参数消融（ReCoN, Ipsundrum, Ipsundrum+affect）下操作汉弗莱的qualiaphilia，作为一种熟悉度控制下的场景优先选择。通过无奖励探索游戏和疼痛尾部探针测试来评估其性能，并进行反馈集成损伤实验。", "result": "结果表明，非情感版本对新颖性敏感（Delta scenic-entry = 0.07），而情感耦合版本在较少新颖的环境中表现稳定（Delta scenic-entry = 0.01）。只有带有情感代理的变体展示了有组织的局部调查和长时间保持警觉的行为。", "conclusion": "研究揭示了循环与持久性、情感耦合控制与偏好稳定性、扫描行为及持续谨慎之间的关联，表明指示符样式的签名可以通过工程实现，并且机理和因果证据应伴随行为标记一同提供。"}}
{"id": "2602.23231", "pdf": "https://arxiv.org/pdf/2602.23231", "abs": "https://arxiv.org/abs/2602.23231", "authors": ["Daniel Bermuth", "Alexander Poeppel", "Wolfgang Reif"], "title": "Skarimva: Skeleton-based Action Recognition is a Multi-view Application", "categories": ["cs.CV"], "comment": null, "summary": "Human action recognition plays an important role when developing intelligent interactions between humans and machines. While there is a lot of active research on improving the machine learning algorithms for skeleton-based action recognition, not much attention has been given to the quality of the input skeleton data itself. This work demonstrates that by making use of multiple camera views to triangulate more accurate 3D~skeletons, the performance of state-of-the-art action recognition models can be improved significantly. This suggests that the quality of the input data is currently a limiting factor for the performance of these models. Based on these results, it is argued that the cost-benefit ratio of using multiple cameras is very favorable in most practical use-cases, therefore future research in skeleton-based action recognition should consider multi-view applications as the standard setup.", "AI": {"tldr": "使用多视角来提高基于骨架的动作识别模型的性能。", "motivation": "提高输入数据的质量可以显著提升动作识别模型的表现，但目前对此关注较少。", "method": "利用多个摄像头视图进行三角定位以生成更准确的3D骨骼数据。", "result": "通过多视角生成更精确的数据后，基于骨架的动作识别性能得到显著改进。", "conclusion": "使用多相机系统在大多数实际应用中具有很高的性价比，未来的研究应将其作为标准设置。"}}
{"id": "2602.23229", "pdf": "https://arxiv.org/pdf/2602.23229", "abs": "https://arxiv.org/abs/2602.23229", "authors": ["Marco Garosi", "Matteo Farina", "Alessandro Conti", "Massimiliano Mancini", "Elisa Ricci"], "title": "Large Multimodal Models as General In-Context Classifiers", "categories": ["cs.CV"], "comment": "CVPR Findings 2026. Project website at https://circle-lmm.github.io/", "summary": "Which multimodal model should we use for classification? Previous studies suggest that the answer lies in CLIP-like contrastive Vision-Language Models (VLMs), due to their remarkable performance in zero-shot classification. In contrast, Large Multimodal Models (LMM) are more suitable for complex tasks. In this work, we argue that this answer overlooks an important capability of LMMs: in-context learning. We benchmark state-of-the-art LMMs on diverse datasets for closed-world classification and find that, although their zero-shot performance is lower than CLIP's, LMMs with a few in-context examples can match or even surpass contrastive VLMs with cache-based adapters, their \"in-context\" equivalent. We extend this analysis to the open-world setting, where the generative nature of LMMs makes them more suitable for the task. In this challenging scenario, LMMs struggle whenever provided with imperfect context information. To address this issue, we propose CIRCLE, a simple training-free method that assigns pseudo-labels to in-context examples, iteratively refining them with the available context itself. Through extensive experiments, we show that CIRCLE establishes a robust baseline for open-world classification, surpassing VLM counterparts and highlighting the potential of LMMs to serve as unified classifiers, and a flexible alternative to specialized models.", "AI": {"tldr": "研究比较了大型多模态模型（LMM）和对比视觉语言模型在分类任务中的表现，提出了一种无需训练的方法CIRCLE以提升LMM在开放世界设置下的性能。", "motivation": "质疑了将CLIP-like模型视为零样本分类首选的观点，探索了LMM通过少量上下文示例进行分类的能力，以及它们是否可以作为通用分类器。", "method": "比较了不同类型的多模态模型的性能，并提出了CIRCLE方法以改善LMM在开放世界设置下的表现。", "result": "实验表明，虽然LMM的零样本性能低于CLIP，但在有少量上下文示例的情况下，它们能与配备了缓存适配器的对比视觉语言模型相匹敌甚至超越；而通过使用CIRCLE方法，LMM能够在开放世界分类任务中表现出色。", "conclusion": "展示了LMM作为通用分类器和灵活替代方案的巨大潜力，并提出了一种有效提升其性能的方法。"}}
{"id": "2602.23228", "pdf": "https://arxiv.org/pdf/2602.23228", "abs": "https://arxiv.org/abs/2602.23228", "authors": ["Yizhi Li", "Xiaohan Chen", "Miao Jiang", "Wentao Tang", "Gaoang Wang"], "title": "MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, CSCWD 2026", "summary": "With the explosive growth of digital entertainment, automated video summarization has become indispensable for applications such as content indexing, personalized recommendation, and efficient media archiving. Automatic synopsis generation for long-form videos, such as movies and TV series, presents a significant challenge for existing Vision-Language Models (VLMs). While proficient at single-image captioning, these general-purpose models often exhibit critical failures in long-duration contexts, primarily a lack of ID-consistent character identification and a fractured narrative coherence. To overcome these limitations, we propose MovieTeller, a novel framework for generating movie synopses via tool-augmented progressive abstraction. Our core contribution is a training-free, tool-augmented, fact-grounded generation process. Instead of requiring costly model fine-tuning, our framework directly leverages off-the-shelf models in a plug-and-play manner. We first invoke a specialized face recognition model as an external \"tool\" to establish Factual Groundings--precise character identities and their corresponding bounding boxes. These groundings are then injected into the prompt to steer the VLM's reasoning, ensuring the generated scene descriptions are anchored to verifiable facts. Furthermore, our progressive abstraction pipeline decomposes the summarization of a full-length movie into a multi-stage process, effectively mitigating the context length limitations of current VLMs. Experiments demonstrate that our approach yields significant improvements in factual accuracy, character consistency, and overall narrative coherence compared to end-to-end baselines.", "AI": {"tldr": "本文提出了一种名为MovieTeller的框架，用于生成电影梗概。", "motivation": "现有的视觉-语言模型在处理长视频时存在识别不准确和情节连贯性差的问题。为了克服这些局限性，作者开发了MovieTeller来提升电影梗概生成的质量。", "method": "MovieTeller通过工具增强的逐步抽象化过程生成电影梗概。该框架首先利用面部识别模型建立事实基础，并将其注入到提示中以引导视觉-语言模型的推理。然后，将长片总结分解为多阶段过程。", "result": "实验表明，与端对端基线相比，本文的方法在事实准确性、角色一致性和整体情节连贯性方面都有显著提升。", "conclusion": "通过引入工具增强和逐步抽象化方法，MovieTeller能够生成更高质量的电影梗概，解决了现有模型处理长视频时的问题。"}}
{"id": "2602.23225", "pdf": "https://arxiv.org/pdf/2602.23225", "abs": "https://arxiv.org/abs/2602.23225", "authors": ["Pengxiang Li", "Dilxat Muhtar", "Lu Yin", "Tianlong Chen", "Shiwei Liu"], "title": "Why Diffusion Language Models Struggle with Truly Parallel (Non-Autoregressive) Decoding?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diffusion Language Models (DLMs) are often advertised as enabling parallel token generation, yet practical fast DLMs frequently converge to left-to-right, autoregressive (AR)-like decoding dynamics. In contrast, genuinely non-AR generation is promising because it removes AR's sequential bottleneck, better exploiting parallel hardware to reduce synchronization/communication overhead and improve latency scaling with output length. We argue that a primary driver of AR-like decoding is a mismatch between DLM objectives and the highly sequential structure of widely used training data, including standard pretraining corpora and long chain-of-thought (CoT) supervision. Motivated by this diagnosis, we propose NAP (Non-Autoregressive Parallel DLMs), a proof-of-concept, data-centric approach that better aligns supervision with non-AR parallel decoding. NAP curates examples as multiple independent reasoning trajectories and couples them with a parallel-forced decoding strategy that encourages multi-token parallel updates. Across math reasoning benchmarks, NAP yields stronger performance under parallel decoding than DLMs trained on standard long CoT data, with gains growing as parallelism increases. Our results suggest that revisiting data and supervision is a principled direction for mitigating AR-like behavior and moving toward genuinely non-autoregressive parallel generation in DLMs. Our code is available at https://github.com/pixeli99/NAP.", "AI": {"tldr": "研究探讨了为什么扩散语言模型在真正并行解码时表现出自回归特性，并提出了一种名为NAP的数据导向方法，以减少这种现象。", "motivation": "实践中快速的扩散语言模型经常转变为从左到右、自回归式的解码动态。作者认为这主要是因为训练数据的高度序列结构与DLM的目标不匹配导致的。因此提出了NAP方法来解决这一问题。", "method": "NAP通过收集多个独立的推理轨迹作为示例，并使用并行强制解码策略，鼓励多令牌同时更新，以此更好地适应非自回归平行生成的任务需求。", "result": "在数学推理基准测试中，NAP在并行解码下表现出比传统长链思维数据训练的DLM更强的表现力，且并行度越高，性能提升越大。", "conclusion": "研究结果表明，重新审视数据和监督机制是减少自回归行为、实现真正非自回归平行生成的有效途径。"}}
{"id": "2602.23224", "pdf": "https://arxiv.org/pdf/2602.23224", "abs": "https://arxiv.org/abs/2602.23224", "authors": ["Mohammad Mahdavian", "Gordon Tan", "Binbin Xu", "Yuan Ren", "Dongfeng Bai", "Bingbing Liu"], "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.", "AI": {"tldr": "UniScale是一种统一的、尺度感知的多视角三维重建框架，用于机器人应用。", "motivation": "视觉基的机器人导航中，从原始图像序列准确提取环境结构对于下游任务至关重要。现有的方法通常无法在单个网络中同时处理相机内参和外参，并且很难灵活地利用外部几何先验知识。", "method": "UniScale通过模块化、语义信息丰富的设计将几何先验知识融入到一个单一的前向网络中，该网络能够从多视角图像中联合估计相机内参和外参、尺度不变深度和点地图以及场景的度量尺度。", "result": "在多个基准测试上评估了UniScale的表现，证明其具有强大的泛化能力和跨各种环境的一致性能。", "conclusion": "通过结合全局上下文推理与相机感知特征表示，UniScale实现了对场景度量尺度的恢复。它无需从头训练，并且利用现有模型中未编码的世界先验知识，使其特别适合资源受限的机器人团队使用。"}}
{"id": "2602.23217", "pdf": "https://arxiv.org/pdf/2602.23217", "abs": "https://arxiv.org/abs/2602.23217", "authors": ["Alaa El Ichi", "Khalide Jbilou"], "title": "Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks", "categories": ["cs.CV", "math.NA"], "comment": null, "summary": "This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.", "AI": {"tldr": "提出了一种基于广义爱因斯坦MLP的多维度任务学习框架，该框架直接操作张量并通过爱因斯坦乘法进行计算机视觉任务。", "motivation": "现有的计算机视觉任务形式受到矩阵思维限制，使用矩阵权重和矢量偏差需要结构平铺处理从而减少可表达的任务空间。GE-MLPs通过使用张量参数来消除这种约束，并提供明确的维度保持或压缩控制能力，同时不丢失信息。", "method": "引入了基于广义爱因斯坦MLP的多维任务学习框架，证明该框架如何操作张量并演示其在分类、分割和检测等特殊情况下的应用。此外，展示了这种任务空间比矩阵形式表达的更大，并支持时空或跨模式预测等需要破坏性平铺处理的任务配置。", "result": "通过严格的数学推导证实了计算机视觉任务可以通过该统一框架进行理解和设计，并证明其在特定维度配置下能够实现分类、分割和检测。", "conclusion": "提出了一种基于张量代数的新颖的计算机视觉任务理解、比较和设计方法，这为未来的研究提供了一个新的视角和方向。"}}
{"id": "2602.23214", "pdf": "https://arxiv.org/pdf/2602.23214", "abs": "https://arxiv.org/abs/2602.23214", "authors": ["Chenhe Du", "Xuanyu Tian", "Qing Wu", "Muyu Liu", "Jingyi Yu", "Hongjiang Wei", "Yuyao Zhang"], "title": "Plug-and-Play Diffusion Meets ADMM: Dual-Variable Coupling for Robust Medical Image Reconstruction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Plug-and-Play diffusion prior (PnPDP) frameworks have emerged as a powerful paradigm for solving imaging inverse problems by treating pretrained generative models as modular priors. However, we identify a critical flaw in prevailing PnP solvers (e.g., based on HQS or Proximal Gradient): they function as memoryless operators, updating estimates solely based on instantaneous gradients. This lack of historical tracking inevitably leads to non-vanishing steady-state bias, where the reconstruction fails to strictly satisfy physical measurements under heavy corruption. To resolve this, we propose Dual-Coupled PnP Diffusion, which restores the classical dual variable to provide integral feedback, theoretically guaranteeing asymptotic convergence to the exact data manifold. However, this rigorous geometric coupling introduces a secondary challenge: the accumulated dual residuals exhibit spectrally colored, structured artifacts that violate the Additive White Gaussian Noise (AWGN) assumption of diffusion priors, causing severe hallucinations. To bridge this gap, we introduce Spectral Homogenization (SH), a frequency-domain adaptation mechanism that modulates these structured residuals into statistically compliant pseudo-AWGN inputs. This effectively aligns the solver's rigorous optimization trajectory with the denoiser's valid statistical manifold. Extensive experiments on CT and MRI reconstruction demonstrate that our approach resolves the bias-hallucination trade-off, achieving state-of-the-art fidelity with significantly accelerated convergence.", "AI": {"tldr": "提出了一种新的PnPDP框架，解决了传统方法中的稳态偏差和幻觉问题，并通过频域适应机制实现了优化轨迹与去噪器统计流形的对齐。", "motivation": "为解决现有PnP求解器缺乏历史跟踪导致的非零稳态偏差以及由此引发的严重幻觉问题而提出了一种新的方法。", "method": "引入双耦合变量以恢复经典对偶变量并提供积分反馈，同时使用频域自适应机制将结构残差调整为伪高斯白噪声输入。", "result": "实验结果显示该方法在CT和MRI重建中解决了偏差-幻觉权衡问题，并实现了最先进的保真度及显著加速的收敛速度。", "conclusion": "所提方法通过引入对偶变量耦合与频域自适应机制成功地解决了传统PnPDP框架中的稳态偏差和结构化残差问题，提高了重建质量和效率。"}}
{"id": "2602.23212", "pdf": "https://arxiv.org/pdf/2602.23212", "abs": "https://arxiv.org/abs/2602.23212", "authors": ["Prottay Kumar Adhikary"], "title": "Through BrokenEyes: How Eye Disorders Impact Face Detection?", "categories": ["cs.CV"], "comment": null, "summary": "Vision disorders significantly impact millions of lives, altering how visual information is processed and perceived. In this work, a computational framework was developed using the BrokenEyes system to simulate five common eye disorders: Age-related macular degeneration, cataract, glaucoma, refractive errors, and diabetic retinopathy and analyze their effects on neural-like feature representations in deep learning models. Leveraging a combination of human and non-human datasets, models trained under normal and disorder-specific conditions revealed critical disruptions in feature maps, particularly for cataract and glaucoma, which align with known neural processing challenges in these conditions. Evaluation metrics such as activation energy and cosine similarity quantified the severity of these distortions, providing insights into the interplay between degraded visual inputs and learned representations.", "AI": {"tldr": "开发了一个计算框架使用BrokenEyes系统来模拟五种常见眼疾对深度学习模型特征表示的影响。", "motivation": "研究视觉障碍如何影响面部检测，特别是通过模拟常见的视觉疾病来理解这些干扰。", "method": "利用组合的人类和非人类数据集，在正常条件和特定疾病的条件下训练模型，并评估激活能量和余弦相似度等指标以量化这些失真。", "result": "发现白内障和青光眼尤其会破坏特征图，这与这些情况下已知的神经处理挑战相一致。", "conclusion": "研究提供了视觉输入受损与学习表示之间相互作用的新见解，强调了模拟特定疾病条件的重要性。"}}
{"id": "2602.23206", "pdf": "https://arxiv.org/pdf/2602.23206", "abs": "https://arxiv.org/abs/2602.23206", "authors": ["Chung Hee Kim", "Shivani Kamtikar", "Tye Brady", "Taskin Padir", "Joshua Migdal"], "title": "Grasp, Slide, Roll: Comparative Analysis of Contact Modes for Tactile-Based Shape Reconstruction", "categories": ["cs.RO"], "comment": "8 pages, 11 figures, Accepted by ICRA 2026", "summary": "Tactile sensing allows robots to gather detailed geometric information about objects through physical interaction, complementing vision-based approaches. However, efficiently acquiring useful tactile data remains challenging due to the time-consuming nature of physical contact and the need to strategically choose contact locations that maximize information gain while minimizing physical interactions. This paper studies how different contact modes affect object shape reconstruction using a tactile-enabled dexterous gripper. We compare three contact interaction modes: grasp-releasing, sliding induced by finger-grazing, and palm-rolling. These contact modes are combined with an information-theoretic exploration framework that guides subsequent sampling locations using a shape completion model. Our results show that the improved tactile sensing efficiency of finger-grazing and palm-rolling translates into faster convergence in shape reconstruction, requiring 34% fewer physical interactions while improving reconstruction accuracy by 55%. We validate our approach using a UR5e robot arm equipped with an Inspire-Robots Dexterous Hand, showing robust performance across primitive object geometries.", "AI": {"tldr": "研究不同接触模式对基于触觉的物体形状重建的影响。", "motivation": "通过物理交互获取详细的几何信息，但如何高效地采集有用的数据仍然具有挑战性。本论文旨在比较三种接触模式（抓取-释放、手指轻抚滑动和手掌滚动）在物体形状重建中的效果。", "method": "使用触觉增强的灵巧夹爪结合信息论探索框架来指导后续采样位置，通过形状完成模型进行引导。将三种接触方式与形状重构模型相结合以提高效率。", "result": "手指轻抚滑动和手掌滚动模式下的改进触觉感知效率加快了形状重建的收敛速度，所需的物理交互次数减少了34%，同时提高了55%的重建精度。", "conclusion": "实验结果表明，采用信息论探索框架结合不同接触方式能够有效提高物体形状重构的速度与准确性。"}}
{"id": "2602.23205", "pdf": "https://arxiv.org/pdf/2602.23205", "abs": "https://arxiv.org/abs/2602.23205", "authors": ["Wenjia Wang", "Liang Pan", "Huaijin Pi", "Yuke Lou", "Xuqian Ren", "Yifan Wu", "Zhouyingcheng Liao", "Lei Yang", "Rishabh Dabral", "Christian Theobalt", "Taku Komura"], "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents", "categories": ["cs.CV"], "comment": null, "summary": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.", "AI": {"tldr": "本文提出了一种使用两个移动iPhone进行人类和场景重建的数据收集管道，以支持具身AI任务。", "motivation": "现有捕捉系统依赖昂贵的摄影棚设置和可穿戴设备，限制了大规模获取现实环境中受场景条件影响的人类动作数据。为了克服这些局限性，提出了EmbodMocap。", "method": "通过联合校准双RGB-D序列来重建人类和场景，在统一的世界坐标系中实现无静态摄像机或标记的日常环境中的度量级捕获。", "result": "实验结果表明该方法在深度模糊缓解、对齐与重构性能方面优于单iPhone或多目模型，支持了具身AI任务的有效性。", "conclusion": "EmbodMocap为大规模现实场景中的人类动作数据收集提供了一种有效的方法，并推进了具身AI研究。"}}
{"id": "2602.23204", "pdf": "https://arxiv.org/pdf/2602.23204", "abs": "https://arxiv.org/abs/2602.23204", "authors": ["Roberto Pellerito", "Nico Messikommer", "Giovanni Cioffi", "Marco Cannici", "Davide Scaramuzza"], "title": "Motion-aware Event Suppression for Event Cameras", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this work, we introduce the first framework for Motion-aware Event Suppression, which learns to filter events triggered by IMOs and ego-motion in real time. Our model jointly segments IMOs in the current event stream while predicting their future motion, enabling anticipatory suppression of dynamic events before they occur. Our lightweight architecture achieves 173 Hz inference on consumer-grade GPUs with less than 1 GB of memory usage, outperforming previous state-of-the-art methods on the challenging EVIMO benchmark by 67\\% in segmentation accuracy while operating at a 53\\% higher inference rate. Moreover, we demonstrate significant benefits for downstream applications: our method accelerates Vision Transformer inference by 83\\% via token pruning and improves event-based visual odometry accuracy, reducing Absolute Trajectory Error (ATE) by 13\\%.", "AI": {"tldr": "提出了一种针对事件相机的运动感知事件抑制框架，用于实时过滤由IMOs和自我移动触发的事件。", "motivation": "旨在开发一种能够准确预测并提前抑制动态事件的方法，以提高下游视觉任务的表现，如目标识别、视觉定位等。", "method": "模型通过联合分割当前事件流中的IMO并对它们进行未来运动预测来实现这一目标，从而达到抗预事件处理的效果。框架在消费者级GPU上实现了173 Hz的推理速度，并且内存使用小于1 GB。", "result": "实验结果表明，该方法在EVIMO基准测试中比现有的最佳技术提高了67%的分割精度，在推理速率方面高出53%，同时增强了视觉变换器的速度和事件基础视觉测距法的准确性。", "conclusion": "运动感知事件抑制框架提供了一种有效的方法来改善事件相机数据的质量，从而提升下游应用性能。"}}
{"id": "2602.23203", "pdf": "https://arxiv.org/pdf/2602.23203", "abs": "https://arxiv.org/abs/2602.23203", "authors": ["Junhu Fu", "Shuyu Liang", "Wutong Li", "Chen Ma", "Peng Huang", "Kehao Wang", "Ke Chen", "Shengli Lin", "Pinghong Zhou", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "ColoDiff: Integrating Dynamic Consistency With Content Awareness for Colonoscopy Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Colonoscopy video generation delivers dynamic, information-rich data critical for diagnosing intestinal diseases, particularly in data-scarce scenarios. High-quality video generation demands temporal consistency and precise control over clinical attributes, but faces challenges from irregular intestinal structures, diverse disease representations, and various imaging modalities. To this end, we propose ColoDiff, a diffusion-based framework that generates dynamic-consistent and content-aware colonoscopy videos, aiming to alleviate data shortage and assist clinical analysis. At the inter-frame level, our TimeStream module decouples temporal dependency from video sequences through a cross-frame tokenization mechanism, enabling intricate dynamic modeling despite irregular intestinal structures. At the intra-frame level, our Content-Aware module incorporates noise-injected embeddings and learnable prototypes to realize precise control over clinical attributes, breaking through the coarse guidance of diffusion models. Additionally, ColoDiff employs a non-Markovian sampling strategy that cuts steps by over 90% for real-time generation. ColoDiff is evaluated across three public datasets and one hospital database, based on both generation metrics and downstream tasks including disease diagnosis, modality discrimination, bowel preparation scoring, and lesion segmentation. Extensive experiments show ColoDiff generates videos with smooth transitions and rich dynamics. ColoDiff presents an effort in controllable colonoscopy video generation, revealing the potential of synthetic videos in complementing authentic representation and mitigating data scarcity in clinical settings.", "AI": {"tldr": "ColoDiff是一种基于扩散模型的结肠镜视频生成框架，旨在通过动态一致性与内容感知来改善数据稀缺状况并辅助临床分析。", "motivation": "高质结肠镜视频生成面临肠道结构不规则、疾病表示多样性及成像方式各异带来的挑战。因此提出ColoDiff以解决这些难题并缓解数据短缺问题。", "method": "通过TimeStream模块和Content-Aware模块，结合非马尔可夫采样策略实现动态一致性和内容感知的结肠镜视频生成。", "result": "实验显示ColoDiff能够生成具有平滑过渡和丰富动态特性的高质量结肠镜视频，并在多个临床任务中表现出色。", "conclusion": "ColoDiff展示了可控结肠镜视频生成技术的应用潜力，有助于通过合成数据补充实际表示并减轻数据稀缺问题。"}}
{"id": "2602.23199", "pdf": "https://arxiv.org/pdf/2602.23199", "abs": "https://arxiv.org/abs/2602.23199", "authors": ["Jiahao Zhao", "Feng Jiang", "Shaowei Qin", "Zhonghui Zhang", "Junhao Liu", "Guibing Guo", "Hamid Alinejad-Rokny", "Min Yang"], "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.", "AI": {"tldr": "开发了一个用于单细胞生物学的自然语言评估框架SC-Arena，以衡量大型语言模型在该领域的能力。", "motivation": "现有的基准测试对于单细胞生物领域的通用和专业大型语言模型来说不足，缺乏统一的任务格式、可解释性以及生物学基础。", "method": "引入了一种虚拟细胞抽象来统一评估目标，并定义了五个自然语言任务。同时采用知识增强的评估方法，结合外部数据库提高判断的准确性与可解释性。", "result": "实验结果表明当前模型在复杂生物机制理解上存在不足，而提出的框架能够提供更高区分度和可解释性的评价标准。", "conclusion": "SC-Arena为单细胞生物学中的大型语言模型评估提供了统一且易解释的方法，并有望推动开发出更加通用化的生物学模型。"}}
{"id": "2602.23196", "pdf": "https://arxiv.org/pdf/2602.23196", "abs": "https://arxiv.org/abs/2602.23196", "authors": ["Amir Abboud", "Ron Safier", "Nathan Wallheimer"], "title": "Equivalent Dichotomies for Triangle Detection in Subgraph, Induced, and Colored H-Free Graphs", "categories": ["cs.DS", "cs.CC", "math.CO"], "comment": null, "summary": "A recent paper by the authors (ITCS'26) initiates the study of the Triangle Detection problem in graphs avoiding a fixed pattern $H$ as a subgraph and proposes a \\emph{dichotomy hypothesis} characterizing which patterns $H$ make the Triangle Detection problem easier in $H$-free graphs than in general graphs. In this work, we demonstrate that this hypothesis is, in fact, equivalent to analogous hypotheses in two broader settings that a priori seem significantly more challenging: \\emph{induced} $H$-free graphs and \\emph{colored} $H$-free graphs. Our main contribution is a reduction from the induced $H$-free case to the non-induced $\\H^+$-free case, where $\\H^+$ preserves the structural properties of $H$ that are relevant for the dichotomy, namely $3$-colorability and triangle count. A similar reduction is given for the colored case. A key technical ingredient is a self-reduction to Unique Triangle Detection that preserves the induced $H$-freeness property, via a new color-coding-like reduction.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.23193", "pdf": "https://arxiv.org/pdf/2602.23193", "abs": "https://arxiv.org/abs/2602.23193", "authors": ["Elzo Brito dos Santos Filho"], "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering", "categories": ["cs.AI"], "comment": "13 pages, 1 figure, 4 tables. Includes 5 technical appendices", "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.", "AI": {"tldr": "提出ESAA架构，解决基于LLM的自主代理在状态、上下文和执行上的限制问题。", "motivation": "解决大型语言模型（LLMs）驱动的自主代理因缺少原生状态、长时间跨度下的上下文退化以及概率生成与确定性执行之间的差距而面临的结构限制。", "method": "采用事件溯源模式，分离代理的认知意图和项目状态变更；使用验证过的JSON格式记录代理的结果或问题报告；通过确定性的调度器验证并持久化事件到追加日志中，并更新可验证的材料视图。", "result": "两个案例研究证明了ESAA架构的有效性和扩展性：一个着陆页项目和一个多代理临床仪表板系统，均成功完成任务并通过回放验证。", "conclusion": "ESAA架构通过提供状态跟踪、上下文保持及确定性执行的解决方案，展示了其在大型语言模型驱动的多自主代理场景中的优越性能。"}}
{"id": "2602.23192", "pdf": "https://arxiv.org/pdf/2602.23192", "abs": "https://arxiv.org/abs/2602.23192", "authors": ["Thomas Woergaard", "Raghavendra Selvan"], "title": "FairQuant: Fairness-Aware Mixed-Precision Quantization for Medical Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "Source code available at https://github.com/saintslab/FairQuant", "summary": "Compressing neural networks by quantizing model parameters offers useful trade-off between performance and efficiency. Methods like quantization-aware training and post-training quantization strive to maintain the downstream performance of compressed models compared to the full precision models. However, these techniques do not explicitly consider the impact on algorithmic fairness. In this work, we study fairness-aware mixed-precision quantization schemes for medical image classification under explicit bit budgets. We introduce FairQuant, a framework that combines group-aware importance analysis, budgeted mixed-precision allocation, and a learnable Bit-Aware Quantization (BAQ) mode that jointly optimizes weights and per-unit bit allocations under bitrate and fairness regularization. We evaluate the method on Fitzpatrick17k and ISIC2019 across ResNet18/50, DeiT-Tiny, and TinyViT. Results show that FairQuant configurations with average precision near 4-6 bits recover much of the Uniform 8-bit accuracy while improving worst-group performance relative to Uniform 4- and 8-bit baselines, with comparable fairness metrics under shared budgets.", "AI": {"tldr": "提出FairQuant框架，结合重要性分析、预算混合精度分配和Bit-Aware Quantization模式，优化权重及比特位分配，在保持性能的同时提升算法公平性。", "motivation": "传统量化方法忽略对算法公平性的考量。为了在压缩神经网络时同时保证模型的准确性和公平性，提出一种基于显式比特预算的公平混合精度量化方案。", "method": "FairQuant框架结合群体感知的重要性分析、带预算的混合精度分配和可学习的位感知量化模式，在保持性能的同时优化权重及比特位分配，并通过比特率和公平性的正则化联合训练模型。", "result": "在Fitzpatrick17k和ISIC2019数据集上，FairQuant配置下的平均精度接近4-6比特时，恢复了大部分8比特均匀量化准确度，同时相对于4比特和8比特基线，在共享预算下提高了最差群体的性能。", "conclusion": "通过引入公平混合精度量化方案，能够在保持模型压缩效率的同时提升算法的公平性。"}}
{"id": "2602.23191", "pdf": "https://arxiv.org/pdf/2602.23191", "abs": "https://arxiv.org/abs/2602.23191", "authors": ["Xinyuan Chen", "Yao Xu", "Shaowen Wang", "Pengjie Song", "Bowen Deng"], "title": "Uni-Animator: Towards Unified Visual Colorization", "categories": ["cs.CV"], "comment": "10 pages, 8 figures. Submitted to CVPR 2026", "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.", "AI": {"tldr": "提出Uni-Animator框架，用于统一图像和视频草图着色任务。", "motivation": "现有方法难以同时处理图像和视频的草图着色问题，存在颜色传递不准确、高频物理细节保留不足以及运动场景中的时间一致性差等问题。", "method": "通过实例补丁嵌入进行视觉参考增强以实现精确的颜色信息对齐与融合；设计利用物理特征来捕捉并保持高频纹理以强化物理细节；提出基于草图的动态RoPE编码自适应地建模空间-时间依赖关系，解决运动引起的时空不一致性。", "result": "Uni-Animator在图像和视频草图着色任务上均表现出竞争力，达到了与特定任务方法相当的效果，并解锁了跨域统一能力，具有高细节保真度和鲁棒的时间一致性。", "conclusion": "Uni-Animator框架解决了现有技术中的问题，在多种指标下表现优异，展示了其在图像和视频草图着色任务上的优势。"}}
{"id": "2602.23183", "pdf": "https://arxiv.org/pdf/2602.23183", "abs": "https://arxiv.org/abs/2602.23183", "authors": ["Yassine Hamoudi", "Yvan Le Borgne", "Shrinidhi Teganahally Sridhara"], "title": "Dequantization Barriers for Guided Stoquastic Hamiltonians", "categories": ["quant-ph", "cs.CC", "cs.DS"], "comment": null, "summary": "We construct a probability distribution, induced by the Perron--Frobenius eigenvector of an exponentially large graph, which cannot be efficiently sampled by any classical algorithm, even when provided with the best-possible warm-start distribution. In the quantum setting, this problem can be viewed as preparing the ground state of a stoquastic Hamiltonian given a guiding state as input, and is known to be efficiently solvable on a quantum computer. Our result suggests that no efficient classical algorithm can solve a broad class of stoquastic ground-state problems. Our graph is constructed from a class of high-degree, high-girth spectral expanders to which self-similar trees are attached. This builds on and extends prior work of Gilyén, Hastings, and Vazirani [Quantum 2021, STOC 2021], which ruled out dequantization for a specific stoquastic adiabatic path algorithm. We strengthen their result by ruling out any classical algorithm for guided ground-state preparation.", "AI": {"tldr": "论文构建了一个概率分布，该分布由一个大规模图的佩龙—弗罗贝尼乌斯特征向量诱导出来，任何经典算法都无法有效采样。", "motivation": "探讨是否存在一种高效的经典算法能够解决量子计算机上的某些特定问题，即使用引导态作为输入来准备stoquastic哈密顿量的基态。", "method": "通过构造一个由高度、高围长的谱展开图与自相似树相连而成的大规模图，证明了任何经典算法都无法有效采样这个概率分布。该方法扩展并加强了Gilyén, Hastings和Vazirani之前的研究。", "result": "表明没有任何高效的古典算法可以解决一类广泛的stoquastic基态问题。", "conclusion": "结果说明了在某些特定情况下，量子计算具有超越经典计算的能力，特别是在处理高维谱展开图中的基态准备任务方面。"}}
{"id": "2602.23177", "pdf": "https://arxiv.org/pdf/2602.23177", "abs": "https://arxiv.org/abs/2602.23177", "authors": ["Bin Zeng", "Johannes Künzel", "Anna Hilsmann", "Peter Eisert"], "title": "Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms", "categories": ["cs.CV"], "comment": "published at VISAPP 2026", "summary": "Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.", "AI": {"tldr": "该论文提出了一种基于物理约束的实时人群跟踪和计数方法，用于铁路站台的安全管理和容量管理。", "motivation": "准确的实时人群计数对于铁路站台的安全管理和容量管理至关重要。现有的大多数跟踪方法要么假定相机静止不变，要么忽视了运动建模中的物理一致性，在动态条件下导致不可靠的人群计数。", "method": "提出了一种基于物理约束的跟踪框架，该框架将检测、外观和3D运动推理统一在一个实时管道中。使用转移学习后的YOLOv11m探测器与EfficientNet-B0外观编码结合DeepSORT，并引入一种基于物理学约束的卡尔曼模型(Phys-3D)，通过针孔几何学来强制执行合理的3D运动动力学。", "result": "在MOT-RailwayPlatformCrowdHead数据集上，该方法将计数误差降至2.97%，展示了对移动和遮挡的鲁棒性。这表明，在安全关键的交通场景中结合基本几何原理和运动先验能够实现可靠的人群计数。", "conclusion": "本文提出的方法为铁路站台的安全管理和有效列车调度提供了有力支持，证明了在动态条件下的实时人群跟踪和计数任务中物理约束的重要性。"}}
{"id": "2602.23172", "pdf": "https://arxiv.org/pdf/2602.23172", "abs": "https://arxiv.org/abs/2602.23172", "authors": ["Maximilian Luz", "Rohit Mohan", "Thomas Nürnberg", "Yakov Miron", "Daniele Cattaneo", "Abhinav Valada"], "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.", "AI": {"tldr": "提出了一种名为Latent Gaussian Splatting（LaGS）的方法，用于4D全景占用跟踪。", "motivation": "现有的大多数方法要么提供粗略的几何追踪方式如边界框，要么提供详细的3D结构但缺乏明确的时间关联。LaGS旨在通过一种新颖的方式解决动态环境中机器人安全可靠操作所需的关键问题：即全面理解时空场景中的多视角信息聚合。", "method": "该方法结合了基于相机的端到端跟踪与基于掩码的多视图全景占用预测，使用新颖的隐式高斯点集聚方法有效地将多视图信息集成至3D体素网格中。具体来说，首先融合观测数据生成稀疏点中心表示形式的3D场景中的3D高斯分布，并通过掩模分割头解码将其特征点阵列化。", "result": "在Occ3D nuScenes和Waymo数据集上进行评估后，LaGS实现了4D全景占用跟踪领域的最先进性能。", "conclusion": "所提出的方法提供了有效的解决方案来处理多视角信息聚合问题，并且通过实验证明了其优越性。"}}
{"id": "2602.23169", "pdf": "https://arxiv.org/pdf/2602.23169", "abs": "https://arxiv.org/abs/2602.23169", "authors": ["Xiaole Tang", "Xiaoyi He", "Jiayi Xu", "Xiang Gu", "Jian Sun"], "title": "Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.", "AI": {"tldr": "本文提出了一种新的表示学习框架BaryIR，用于通过Wasserstein巴利中心空间对多源退化特征进行对齐，从而提高图像恢复的一致性和适应性。", "motivation": "现有的一体化图像修复方法在处理未知的退化类型时效果不佳。为了应对这一挑战，本文认为不同类型的退化会从一个原始无退化分布中产生不同的退化特异性偏移，并且通过恢复这种共享分布可以实现跨退化的泛化。", "method": "提出了一种新的表示学习框架BaryIR，该框架通过对多源退化特征进行Wasserstein巴利中心空间的对齐来建模无退化的通用分布。引入残差子空间，其嵌入相互对比但保持与WB嵌入正交，从而解耦两个正交空间：一个编码共享不变内容的空间和一个自适应保留特异性知识的空间。", "result": "实验表明BaryIR在各种退化类型上表现优异，并且即使在训练数据有限的情况下也能很好地泛化到真实世界中的混合退化情况。", "conclusion": "通过使用Wasserstein巴利中心空间对多源退化特征进行对齐，BaryIR可以解耦不变内容和特定知识，从而提高了图像修复的一致性和适应性。"}}
{"id": "2602.23166", "pdf": "https://arxiv.org/pdf/2602.23166", "abs": "https://arxiv.org/abs/2602.23166", "authors": ["Zhaochen Su", "Jincheng Gao", "Hangyu Guo", "Zhenhua Liu", "Lueyang Zhang", "Xinyu Geng", "Shijue Huang", "Peng Xia", "Guanyu Jiang", "Cheng Wang", "Yue Zhang", "Yi R. Fung", "Junxian He"], "title": "AgentVista: Evaluating Multimodal Agents in Ultra-Challenging Realistic Visual Scenarios", "categories": ["cs.CV"], "comment": "The project website is available at \\url{https://agentvista-bench.github.io/}, and the code is available at \\url{https://github.com/hkust-nlp/AgentVista}", "summary": "Real-world multimodal agents solve multi-step workflows grounded in visual evidence. For example, an agent can troubleshoot a device by linking a wiring photo to a schematic and validating the fix with online documentation, or plan a trip by interpreting a transit map and checking schedules under routing constraints. However, existing multimodal benchmarks mainly evaluate single-turn visual reasoning or specific tool skills, and they do not fully capture the realism, visual subtlety, and long-horizon tool use that practical agents require. We introduce AgentVista, a benchmark for generalist multimodal agents that spans 25 sub-domains across 7 categories, pairing realistic and detail-rich visual scenarios with natural hybrid tool use. Tasks require long-horizon tool interactions across modalities, including web search, image search, page navigation, and code-based operations for both image processing and general programming. Comprehensive evaluation of state-of-the-art models exposes significant gaps in their ability to carry out long-horizon multimodal tool use. Even the best model in our evaluation, Gemini-3-Pro with tools, achieves only 27.3% overall accuracy, and hard instances can require more than 25 tool-calling turns. We expect AgentVista to accelerate the development of more capable and reliable multimodal agents for realistic and ultra-challenging problem solving.", "AI": {"tldr": "评估多模态代理在复杂真实视觉场景中的性能。", "motivation": "现有的多模态基准测试主要集中在单一回合的视觉推理或特定工具技能上，无法全面捕捉实际代理所需的现实性、视觉细微差别和长周期工具使用能力。", "method": "引入AgentVista基准测试，涵盖25个子领域跨越7类别的详细真实场景，评估通用多模态代理。任务要求跨模式的长期工具交互，包括网络搜索、图像搜索、页面导航等。", "result": "最先进的模型在评价中表现出长周期多模态工具使用能力不足的问题，最高准确率仅为27.3%。", "conclusion": "AgentVista有望加速开发更强大可靠的用于实际难题解决的多模态代理。"}}
{"id": "2602.23165", "pdf": "https://arxiv.org/pdf/2602.23165", "abs": "https://arxiv.org/abs/2602.23165", "authors": ["Yichen Peng", "Jyun-Ting Song", "Siyeol Jung", "Ruofan Liu", "Haiyang Liu", "Xuangeng Chu", "Ruicong Liu", "Erwin Wu", "Hideki Koike", "Kris Kitani"], "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation", "categories": ["cs.CV"], "comment": "13 pages, 9 figures", "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.", "AI": {"tldr": "本文提出了一种用于生成双人对话中自然、社交互动的手势的多模态扩散变压器DyaDiT。", "motivation": "现有的手势生成方法通常只考虑单一音频流和单个说话者的动作，而忽视了社交语境及两个参与者之间的相互作用。为此，本文致力于开发一种能够捕捉双人交流中的动态变化，并生成符合情境的手势的方法。", "method": "DyaDiT利用多模态扩散变压器从双向音讯信号中生成适合的情景动作，结合来自双方的信息来捕获互动的动态变化，使用运动字典编码先验知识。它还可以选择性地利用对话伙伴的动作以产生更具有响应性的动作。", "result": "实验结果表明，DyaDiT在标准的手势生成指标上超越了现有方法，并且通过定量用户研究也显示出了更强的用户偏好。", "conclusion": "该模型展示了其在多模态手势合成方面的优势以及在社交友好性上的优越表现。"}}
{"id": "2602.23163", "pdf": "https://arxiv.org/pdf/2602.23163", "abs": "https://arxiv.org/abs/2602.23163", "authors": ["Usman Anwar", "Julianna Piskorz", "David D. Baek", "David Africa", "Jim Weatherall", "Max Tegmark", "Christian Schroeder de Witt", "Mihaela van der Schaar", "David Krueger"], "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.MA"], "comment": "First two authors contributed equally", "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.", "AI": {"tldr": "研究提出了一种基于决策理论的隐写术形式化方法，并应用于大型语言模型（LLM）监控。", "motivation": "缺乏对大型语言模型进行有效隐写行为检测和量化的方法，传统隐写定义需要已知参考分布，这在实际应用中难以实现。因此提出新的视角和度量标准来应对这一挑战。", "method": "引入广义V信息框架作为衡量输入中可用信息的实用工具，并通过比较能够解码隐藏内容与无法解码者的后续效用来定义隐写差距。", "result": "验证了所提方法的有效性，证明其可以用于检测、量化和缓解大型语言模型中的隐写行为。", "conclusion": "该研究提出了一种新的理论框架来解决大型语言模型中隐写术的检测与量化问题，并为监控这些模型提供了实用手段。"}}
{"id": "2602.23161", "pdf": "https://arxiv.org/pdf/2602.23161", "abs": "https://arxiv.org/abs/2602.23161", "authors": ["Junkai Lu", "Peng Chen", "Xingjian Wu", "Yang Shu", "Chenjuan Guo", "Christian S. Jensen", "Bin Yang"], "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering", "categories": ["cs.AI"], "comment": null, "summary": "Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.", "AI": {"tldr": "提出PATRA模型，解决时间序列问答中的模式识别与平衡学习问题。", "motivation": "现有方法将时间序列简化为文本或图像处理，并且在混合任务中简单目标占据主导，影响深度推理能力的提升。", "method": "引入感知趋势和季节性的模式识别机制，设计任务难度均衡奖励机制。", "result": "实验表明PATRA模型在多样时间序列问答任务上超越了现有基线方法。", "conclusion": "PATRA展示了卓越的跨模态理解和推理能力，有效解决了当前问题。"}}
{"id": "2602.23153", "pdf": "https://arxiv.org/pdf/2602.23153", "abs": "https://arxiv.org/abs/2602.23153", "authors": ["Guofeng Mei", "Wei Lin", "Luigi Riz", "Yujiao Wu", "Yiming Wang", "Fabio Poiesi"], "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model", "categories": ["cs.CV", "cs.AI"], "comment": "ef:CVPR 2026", "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.", "AI": {"tldr": "提出了一种高效的无编码器的基于傅里叶变换的三维大规模多模态模型Fase3D。", "motivation": "探讨如何在不使用复杂编码器的情况下有效地和高效地对无序的三维数据进行标记，以提高效率和可扩展性。", "method": "通过结合点云序列化和快速傅里叶变换（FFT）来近似自我注意力机制，设计了一种新型的令牌化方法，并提出了Fase3D模型。该模型使用结构化的超点表示大规模场景，采用空间填充曲线序列化和FFT进行高效的全局上下文建模与图基令牌合并，以及通过傅里叶增强LoRA适配器注入频率感知交互。", "result": "Fase3D的性能可媲美基于编码器的三维多模态模型，并在计算量和参数上显著减少。", "conclusion": "该研究成功地设计了一种新型无编码器的三维大规模多模态模型，实现了高效且精确的三维数据处理。"}}
{"id": "2602.23152", "pdf": "https://arxiv.org/pdf/2602.23152", "abs": "https://arxiv.org/abs/2602.23152", "authors": ["Jingxuan Wei", "Siyuan Li", "Yuhang Xu", "Zheng Sun", "Junjie Jiang", "Hexuan Jin", "Caijun Jia", "Honghao He", "Xinglong Xu", "Xi bai", "Chang Yu", "Yumou Liu", "Junnan Zhu", "Xuanhe Zhou", "Jintao Chen", "Xiaobin Hu", "Shancheng Pang", "Bihui Yu", "Ran He", "Zhen Lei", "Stan Z. Li", "Conghui He", "Shuicheng Yan", "Cheng Tan"], "title": "The Trinity of Consistency as a Defining Principle for General World Models", "categories": ["cs.AI"], "comment": "119 pages, 50 figures", "summary": "The construction of World Models capable of learning, simulating, and reasoning about objective physical laws constitutes a foundational challenge in the pursuit of Artificial General Intelligence. Recent advancements represented by video generation models like Sora have demonstrated the potential of data-driven scaling laws to approximate physical dynamics, while the emerging Unified Multimodal Model (UMM) offers a promising architectural paradigm for integrating perception, language, and reasoning. Despite these advances, the field still lacks a principled theoretical framework that defines the essential properties requisite for a General World Model. In this paper, we propose that a World Model must be grounded in the Trinity of Consistency: Modal Consistency as the semantic interface, Spatial Consistency as the geometric basis, and Temporal Consistency as the causal engine. Through this tripartite lens, we systematically review the evolution of multimodal learning, revealing a trajectory from loosely coupled specialized modules toward unified architectures that enable the synergistic emergence of internal world simulators. To complement this conceptual framework, we introduce CoW-Bench, a benchmark centered on multi-frame reasoning and generation scenarios. CoW-Bench evaluates both video generation models and UMMs under a unified evaluation protocol. Our work establishes a principled pathway toward general world models, clarifying both the limitations of current systems and the architectural requirements for future progress.", "AI": {"tldr": "本文提出了三重一致性（模态一致性、空间一致性和时间一致性）作为构建通用世界模型的基础原则，介绍了CoW-Bench基准测试以评估视频生成模型和统一多模式模型。", "motivation": "现有的数据驱动方法在模拟物理动力学方面取得了进展，但缺乏一个明确的理论框架来定义通用世界模型所需的本质属性。本文旨在填补这一空白，并通过三重一致性原则阐明构建通用世界的路径。", "method": "文中系统地回顾了多模态学习的发展历程，并提出了CoW-Bench基准测试，以评估视频生成模型和统一多模式模型在多个推理和生成场景下的表现。", "result": "研究揭示了从松散耦合的专业模块向集成架构发展的趋势，展示了当前系统的局限性并明确了未来进展的建筑要求。", "conclusion": "本文通过三重一致性原则提供了一个构建通用世界模型的原则路径，并提出了CoW-Bench作为评估工具以推动该领域的进一步发展。"}}
{"id": "2602.23148", "pdf": "https://arxiv.org/pdf/2602.23148", "abs": "https://arxiv.org/abs/2602.23148", "authors": ["Nitin Gupta", "Vishal Pallagani", "John A. Aydin", "Biplav Srivastava"], "title": "On Sample-Efficient Generalized Planning via Learned Transition Models", "categories": ["cs.AI"], "comment": "14 pages; This is an extended version of a short paper accepted at ICAPS 2026 under the same title", "summary": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\\hatγ \\approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.", "AI": {"tldr": "本文通过学习显式的转移模型来改进泛化规划，该模型可以预测中间世界状态，并生成计划。", "motivation": "当前的方法通过直接的动作序列预测实现泛化规划，但在长时序设置中容易出现状态漂移问题。因此作者提出一种新的方法来解决样本效率和大小不变的泛化性问题。", "method": "本文将泛化规划定义为转移模型学习问题，并采用神经网络显式地逼近后继状态函数。使用关系图编码等多种状态表示和神经架构进行实验评估，以研究样本效率和模型规模的不变性。", "result": "实验结果显示，在多个领域中，学习显式的转移模型比直接预测动作序列可以获得更高出界的满足计划成功率，并且实现这一目标所需的训练实例数量和模型大小显著减少。", "conclusion": "本文通过学习过渡模型的方法在样本效率和规模上优于直接预测动作序列的方法。"}}
{"id": "2602.23146", "pdf": "https://arxiv.org/pdf/2602.23146", "abs": "https://arxiv.org/abs/2602.23146", "authors": ["Jonathan Giezendanner", "Qidong Yang", "Eric Schmitt", "Anirban Chandra", "Daniel Salles Civitarese", "Johannes Jakubik", "Jeremy Vila", "Detlef Hohl", "Campbell Watson", "Sherrie Wang"], "title": "Partial recovery of meter-scale surface weather", "categories": ["cs.LG", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Near-surface atmospheric conditions can differ sharply over tens to hundreds of meters due to land cover and topography, yet this variability is absent from current weather analyses and forecasts. It is unclear whether such meter-scale variability reflects irreducibly chaotic dynamics or contains a component predictable from surface characteristics and large-scale atmospheric forcing. Here we show that a substantial, physically coherent component of meter-scale near-surface weather is statistically recoverable from existing observations. By conditioning coarse atmospheric state on sparse surface station measurements and high-resolution Earth observation data, we infer spatially continuous fields of near-surface wind, temperature, and humidity at 10 m resolution across the contiguous United States. Relative to ERA5, the inferred fields reduce wind error by 29% and temperature and dewpoint error by 6%, while explaining substantially more spatial variance at fixed time steps. They also exhibit physically interpretable structure, including urban heat islands, evapotranspiration-driven humidity contrasts, and wind speed differences across land cover types. Our findings expand the frontier of weather modeling by demonstrating a computationally feasible approach to continental-scale meter-resolution inference. More broadly, they illustrate how conditioning coarse dynamical models on static fine-scale features can reveal previously unresolved components of the Earth system.", "AI": {"tldr": "本文提出了一种从现有观测数据中恢复米级分辨率近地表气象状态的方法。", "motivation": "目前的天气分析和预报未能捕捉到因土地覆盖和地形引起的百米范围内显著的气候差异。该研究旨在探讨这种差异是否可以部分通过表面特征和大规模大气强迫来预测。", "method": "通过对粗略的大气状态进行条件化处理，并结合稀疏的地表站测量数据以及高分辨率地球观测数据，推断出美国连续地区10米分辨率的近地表风速、温度和湿度场。", "result": "相对于ERA5模型，所推断的气象场减少了29%的风速误差和6%的气温与露点温度误差，并在固定时间步骤下解释了更多的空间变化。这些场显示了物理上可解释的结构，如城市热岛效应、蒸发传输驱动的湿度差异以及不同土地覆盖类型之间的风速差异。", "conclusion": "该研究通过展示一种可行的方法来推断大陆尺度上的米级分辨率天气状态，扩展了天气建模的边界。此外，它还展示了如何通过对粗略动力学模型进行静态细尺度特征条件化处理以揭示以前未被解析的地表系统组成部分。"}}
{"id": "2602.23141", "pdf": "https://arxiv.org/pdf/2602.23141", "abs": "https://arxiv.org/abs/2602.23141", "authors": ["Tao Liu", "Gang Wan", "Kan Ren", "Shibo Wen"], "title": "No Labels, No Look-Ahead: Unsupervised Online Video Stabilization with Classical Priors", "categories": ["cs.CV"], "comment": "CVPR2026", "summary": "We propose a new unsupervised framework for online video stabilization. Unlike methods based on deep learning that require paired stable and unstable datasets, our approach instantiates the classical stabilization pipeline with three stages and incorporates a multithreaded buffering mechanism. This design addresses three longstanding challenges in end-to-end learning: limited data, poor controllability, and inefficiency on hardware with constrained resources. Existing benchmarks focus mainly on handheld videos with a forward view in visible light, which restricts the applicability of stabilization to domains such as UAV nighttime remote sensing. To fill this gap, we introduce a new multimodal UAV aerial video dataset (UAV-Test). Experiments show that our method consistently outperforms state-of-the-art online stabilizers in both quantitative metrics and visual quality, while achieving performance comparable to offline methods.", "AI": {"tldr": "本文提出了一种新的无监督在线视频稳定框架，不依赖于深度学习和配对的数据集。", "motivation": "通过解决数据限制、控制不足以及硬件资源受限问题，为手持设备之外的领域（如无人机夜间遥感）提供更好的视频稳定解决方案。", "method": "采用经典的三阶段视频稳定管线，并引入多线程缓冲机制，无需依赖有标签的数据集或深度学习框架。", "result": "实验显示该方法在量化指标和视觉质量上均优于现有的在线稳定算法，并且性能接近离线方法。", "conclusion": "所提方法提供了一种有效的无监督在线视频稳定方案，在资源受限的环境下具有较好的应用前景。"}}
{"id": "2602.23136", "pdf": "https://arxiv.org/pdf/2602.23136", "abs": "https://arxiv.org/abs/2602.23136", "authors": ["Jayadev Billa"], "title": "Modality Collapse as Mismatched Decoding: Information-Theoretic Limits of Multimodal LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 11 tables, 2 figures. Code: https://github.com/jb1999/modality_collapse_paper", "summary": "Multimodal LLMs can process speech and images, but they cannot hear a speaker's voice or see an object's texture. We show this is not a failure of encoding: speaker identity, emotion, and visual attributes survive through every LLM layer (3--55$\\times$ above chance in linear probes), yet removing 64--71% of modality-specific variance improves decoder loss. The decoder has no learned use for these directions; their presence is noise. We formalize this as a mismatched decoder problem: a decoder trained on text can only extract information along text-aligned directions. Accessible information is bounded by the Generalized Mutual Information (GMI), with degradation scaling with distributional distance and decoder sensitivity. The bound is a property of the decoder's scoring rule, not of any particular architecture; it applies whether non-text inputs arrive through a learned projection, a discrete codebook, or no explicit adapter at all. We validate this across five models spanning speech and vision. A controlled experiment (two Prismatic VLMs differing only in encoder text-alignment) confirms the bottleneck is the decoder's scoring rule, not the encoder or projection. A LoRA intervention demonstrates the fix: training with an emotion objective improves emotion accessibility ($+$7.5%) without affecting other attributes, confirming that the training objective determines what becomes accessible.", "AI": {"tldr": "本文探讨了多模态LLMs在处理语音和图像时出现的模式坍缩问题，提出了解码器与编码器之间信息不匹配的观点，并通过实验验证了这一观点。", "motivation": "研究者发现，虽然多模态语言模型能够保留语音的身份、情感以及视觉属性的信息，但解码器却无法有效利用这些信息。因此，他们试图解释为什么存在这样的现象及其背后的机制。", "method": "该论文通过线性探针实验分析了信息在不同层中的保真度，并提出了一种基于广义互信息（GMI）的信息理论界限来描述这种解码器与编码器间的信息不匹配问题。此外，研究还展示了不同的模型和方法如何影响解码过程。", "result": "研究表明，在多模态LLMs中，尽管编码层可以保留语音身份、情感及视觉属性等信息，但这些信息对于基于文本的解码来说是噪声。实验表明通过调整训练目标可以改善特定属性的可提取性。", "conclusion": "研究得出结论：模式坍缩是由解码器的评分规则造成的，而不是由于编码器或投影机制的问题。这意味着改进方法可以通过改变训练任务来增强解码过程中对特定信息的关注和利用。"}}
{"id": "2602.23135", "pdf": "https://arxiv.org/pdf/2602.23135", "abs": "https://arxiv.org/abs/2602.23135", "authors": ["Tyler Bonnet", "Marek Rei"], "title": "DyGnROLE: Modeling Asymmetry in Dynamic Graphs with Node-Role-Oriented Latent Encoding", "categories": ["cs.LG", "cs.AI", "cs.SI"], "comment": null, "summary": "Real-world dynamic graphs are often directed, with source and destination nodes exhibiting asymmetrical behavioral patterns and temporal dynamics. However, existing dynamic graph architectures largely rely on shared parameters for processing source and destination nodes, with limited or no systematic role-aware modeling. We propose DyGnROLE (Dynamic Graph Node-Role-Oriented Latent Encoding), a transformer-based architecture that explicitly disentangles source and destination representations. By using separate embedding vocabularies and role-semantic positional encodings, the model captures the distinct structural and temporal contexts unique to each role. Critical to the effectiveness of these specialized embeddings in low-label regimes is a self-supervised pretraining objective we introduce: Temporal Contrastive Link Prediction (TCLP). The pretraining uses the full unlabeled interaction history to encode informative structural biases, enabling the model to learn role-specific representations without requiring annotated data. Evaluation on future edge classification demonstrates that DyGnROLE substantially outperforms a diverse set of state-of-the-art baselines, establishing role-aware modeling as an effective strategy for dynamic graph learning.", "AI": {"tldr": "提出了DyGnROLE模型，用于动态图中的节点角色不对称建模。", "motivation": "现实世界的动态图通常有方向性，源节点和目标节点表现出不同行为模式。现有方法依赖共享参数处理节点，缺乏系统角色感知模型。因此提出新架构解决这一问题。", "method": "DyGnROLE使用变压器架构，通过分离嵌入词汇表和角色语义位置编码来明确区分源节点与目标节点表示。引入了自我监督预训练目标：时序对比链接预测(TCLP)，利用未标记互动历史编码结构偏差。", "result": "模型在未来的边分类任务上显著优于一系列最先进的基线方法，表明角色感知建模是动态图学习的有效策略。", "conclusion": "DyGnROLE通过角色意识建模，在处理不对称性动态网络时表现出色。"}}
{"id": "2602.23133", "pdf": "https://arxiv.org/pdf/2602.23133", "abs": "https://arxiv.org/abs/2602.23133", "authors": ["Xin Yuan", "Zhiyong Zhang", "Xin Xu", "Zheng Wang", "Chia-Wen Lin"], "title": "From Calibration to Refinement: Seeking Certainty via Probabilistic Evidence Propagation for Noisy-Label Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMM 2026", "summary": "With the increasing demand for robust person Re-ID in unconstrained environments, learning from datasets with noisy labels and sparse per-identity samples remains a critical challenge. Existing noise-robust person Re-ID methods primarily rely on loss-correction or sample-selection strategies using softmax outputs. However, these methods suffer from two key limitations: 1) Softmax exhibits translation invariance, leading to over-confident and unreliable predictions on corrupted labels. 2) Conventional sample selection based on small-loss criteria often discards valuable hard positives that are crucial for learning discriminative features. To overcome these issues, we propose the CAlibration-to-REfinement (CARE) method, a two-stage framework that seeks certainty through probabilistic evidence propagation from calibration to refinement. In the calibration stage, we propose the probabilistic evidence calibration (PEC) that dismantles softmax translation invariance by injecting adaptive learnable parameters into the similarity function, and employs an evidential calibration loss to mitigate overconfidence on mislabeled samples. In the refinement stage, we design the evidence propagation refinement (EPR) that can more accurately distinguish between clean and noisy samples. Specifically, the EPR contains two steps: Firstly, the composite angular margin (CAM) metric is proposed to precisely distinguish clean but hard-to-learn positive samples from mislabeled ones in a hyperspherical space; Secondly, the certainty-oriented sphere weighting (COSW) is developed to dynamically allocate the importance of samples according to CAM, ensuring clean instances drive model updates. Extensive experimental results on Market1501, DukeMTMC-ReID, and CUHK03 datasets under both random and patterned noises show that CARE achieves competitive performance.", "AI": {"tldr": "提出了一种通过概率证据传播从校准到细化的CARE方法，用于处理带噪声标签的人再识别问题。", "motivation": "现有抗噪人再识别方法主要依赖于损失修正或样本选择策略使用softmax输出，但存在预测过于自信和丢弃关键硬正样本的问题。为解决这些问题，本文提出了一种新的两阶段框架来克服这些限制。", "method": "CARE分为校准和细化两个阶段：校准阶段采用概率证据校准（PEC）方法，通过在相似性函数中引入可学习参数消除softmax的不变性，并使用证据校准损失减轻对误标样本的信心。细化阶段设计了证据传播细化（EPR），包括复合角度边距（CAM）和确定性导向球重加权（COSW）来区分干净和噪声样本。", "result": "在Market1501，DukeMTMC-ReID和CUHK03数据集上进行的实验表明，在随机和模式化噪音下，CARE取得了竞争力的表现。", "conclusion": "CARE方法通过概率证据传播从校准到细化，能够更准确地区分干净样本和噪声样本，并在带噪声标签的人再识别任务中表现出色。"}}
{"id": "2602.23123", "pdf": "https://arxiv.org/pdf/2602.23123", "abs": "https://arxiv.org/abs/2602.23123", "authors": ["Keito Inoshita"], "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection", "categories": ["cs.AI"], "comment": null, "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.", "AI": {"tldr": "本文提出了一种基于多智能体的大语言模型情感净化系统，旨在减少消费者信息接收过程中的过度情绪刺激。", "motivation": "在注意力经济中，令人震惊的内容使消费者暴露于过多的情绪刺激之下，影响冷静决策。该研究通过一种新型的情感净化方法来保护消费者免受不良信息的影响。", "method": "本文提出了一种名为MALLET的系统，包括情感分析、情感调节、平衡监控和个人指导四个智能体，利用BERT模型量化情绪强度，并使用大语言模型重写文本以达到中和或冷却效果。", "result": "实验结果显示该方法能显著减少刺激评分（最高达19.3%）并改善情感平衡，同时保持语义完整。在体育、商业和科技类别中取得了明显的净化效果。", "conclusion": "本文提供的框架能够在不限制消费者访问原始文本的情况下，支持冷静的信息接收。"}}
{"id": "2602.23121", "pdf": "https://arxiv.org/pdf/2602.23121", "abs": "https://arxiv.org/abs/2602.23121", "authors": ["C. Seas", "G. Fitzpatrick", "J. A. Hamilton", "M. C. Carlisle"], "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning", "categories": ["cs.CR", "cs.AI"], "comment": "ef:2024 IEEE 14th Annual Computing and Communication Workshop and Conference (CCWC), Las Vegas, NV, USA, 2024, pp. 0484-0490", "summary": "Each year, software vulnerabilities are discovered, which pose significant risks of exploitation and system compromise. We present a convolutional neural network model that can successfully identify bugs in C code. We trained our model using two complementary datasets: a machine-labeled dataset created by Draper Labs using three static analyzers and the NIST SATE Juliet human-labeled dataset designed for testing static analyzers. In contrast with the work of Russell et al. on these datasets, we focus on C programs, enabling us to specialize and optimize our detection techniques for this language. After removing duplicates from the dataset, we tokenize the input into 91 token categories. The category values are converted to a binary vector to save memory. Our first convolution layer is chosen so that the entire encoding of the token is presented to the filter. We use two convolution and pooling layers followed by two fully connected layers to classify programs into either a common weakness enumeration category or as ``clean.'' We obtain higher recall than prior work by Russell et al. on this dataset when requiring high precision. We also demonstrate on a custom Linux kernel dataset that we are able to find real vulnerabilities in complex code with a low false-positive rate.", "AI": {"tldr": "使用深度表示学习自动检测源代码中的漏洞", "motivation": "软件漏洞每年都会被发现，这给系统安全带来了极大的风险。为了解决这个问题，作者开发了一种基于卷积神经网络的模型来识别C语言程序中的错误。", "method": "采用了一个卷积神经网络模型对经过处理后的源代码进行分类，通过两层卷积和池化操作以及两层全连接层最终判断是否包含漏洞。", "result": "实验表明该方法在高精度要求下比Russell等人的工作具有更高的召回率。此外，在一个自定义的Linux内核数据集上展示了能够以较低的假阳性率为复杂代码找到真实漏洞的能力。", "conclusion": "通过深度学习技术可以有效提高源代码中漏洞检测的效果和效率，为软件的安全性提供了有力保障。"}}
{"id": "2602.23120", "pdf": "https://arxiv.org/pdf/2602.23120", "abs": "https://arxiv.org/abs/2602.23120", "authors": ["Arian Sabaghi", "José Oramas"], "title": "TriLite: Efficient Weakly Supervised Object Localization with Universal Visual Features and Tri-Region Disentanglement", "categories": ["cs.CV"], "comment": "This paper consists of 8 pages including 6 figures. Accepted at CVPR 2026", "summary": "Weakly supervised object localization (WSOL) aims to localize target objects in images using only image-level labels. Despite recent progress, many approaches still rely on multi-stage pipelines or full fine-tuning of large backbones, which increases training cost, while the broader WSOL community continues to face the challenge of partial object coverage. We present TriLite, a single-stage WSOL framework that leverages a frozen Vision Transformer with Dinov2 pre-training in a self-supervised manner, and introduces only a minimal number of trainable parameters (fewer than 800K on ImageNet-1K) for both classification and localization. At its core is the proposed TriHead module, which decomposes patch features into foreground, background, and ambiguous regions, thereby improving object coverage while suppressing spurious activations. By disentangling classification and localization objectives, TriLite effectively exploits the universal representations learned by self-supervised ViTs without requiring expensive end-to-end training. Extensive experiments on CUB-200-2011, ImageNet-1K, and OpenImages demonstrate that TriLite sets a new state of the art, while remaining significantly more parameter-efficient and easier to train than prior methods. The code will be released soon.", "AI": {"tldr": "本文提出了一种名为TriLite的单阶段弱监督目标定位框架，利用预训练的Vision Transformer和一个最小化的可训练参数模块来改善对象覆盖并抑制虚假激活。", "motivation": "尽管最近在弱监督目标定位领域有所进展，但许多方法仍然依赖于多阶段管道或大型骨干网的完全微调，这增加了训练成本，并且该社区仍面临部分目标覆盖的问题。因此，作者旨在提出一种更有效的方法来解决这些问题。", "method": "TriLite框架使用了预冻结的Vision Transformer和Dinov2预训练模型，在自我监督的方式下工作，引入了一个新的三头模块（TriHead），将图像划分为前景、背景和模糊区域，并且通过解耦分类和定位目标来提高效率。", "result": "在CUB-200-2011, ImageNet-1K 和 OpenImages数据集上的实验表明，TriLite不仅达到了新的技术水平，而且具有比以前方法显著更多的参数效率并易于训练。", "conclusion": "TriLite通过提出新颖的模块和策略，在提高目标定位准确性的同时减少了训练成本，并展示了其在弱监督物体定位任务中的优势。"}}
{"id": "2602.23117", "pdf": "https://arxiv.org/pdf/2602.23117", "abs": "https://arxiv.org/abs/2602.23117", "authors": ["Xiaosen Wang", "Zhijin Ge", "Bohan Liu", "Zheng Fang", "Fengfan Zhou", "Ruixuan Zhang", "Shaokang Wang", "Yuyang Luo"], "title": "Devling into Adversarial Transferability on Image Classification: Review, Benchmark, and Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at https://github.com/Trustworthy-AI-Group/TransferAttack", "summary": "Adversarial transferability refers to the capacity of adversarial examples generated on the surrogate model to deceive alternate, unexposed victim models. This property eliminates the need for direct access to the victim model during an attack, thereby raising considerable security concerns in practical applications and attracting substantial research attention recently. In this work, we discern a lack of a standardized framework and criteria for evaluating transfer-based attacks, leading to potentially biased assessments of existing approaches. To rectify this gap, we have conducted an exhaustive review of hundreds of related works, organizing various transfer-based attacks into six distinct categories. Subsequently, we propose a comprehensive framework designed to serve as a benchmark for evaluating these attacks. In addition, we delineate common strategies that enhance adversarial transferability and highlight prevalent issues that could lead to unfair comparisons. Finally, we provide a brief review of transfer-based attacks beyond image classification.", "AI": {"tldr": "本文旨在解决对抗样本转移性评估框架缺失的问题，提出了一个全面的基准测试来评测不同攻击方法的有效性。", "motivation": "目前缺乏标准化的评估框架和标准导致了对现有方法评价可能存在偏见问题，因此作者认为有必要建立一套完整的评估体系以准确衡量各种基于迁移性的攻击。", "method": "通过详细回顾上百篇相关研究文献，并将现有的转移性攻击归纳为六种类型；提出了一套全面的基准测试方案来评估这些攻击的有效性；指出提升对抗样本转移性的常见策略以及可能引起不公平对比的问题。", "result": "本文提出了一个系统化的框架用于评测基于迁移性的攻击方法，指出了现有研究中存在的不足和改进方向。", "conclusion": "作者总结了当前关于图像分类中基于迁移性的攻击的研究现状，并提出了一套标准化的评估体系来公正地衡量各种攻击的有效性。"}}
{"id": "2602.23115", "pdf": "https://arxiv.org/pdf/2602.23115", "abs": "https://arxiv.org/abs/2602.23115", "authors": ["David Dirnfeld", "Fabien Delattre", "Pedro Miraldo", "Erik Learned-Miller"], "title": "FLIGHT: Fibonacci Lattice-based Inference for Geometric Heading in real-Time", "categories": ["cs.CV", "cs.CG", "cs.RO"], "comment": null, "summary": "Estimating camera motion from monocular video is a fundamental problem in computer vision, central to tasks such as SLAM, visual odometry, and structure-from-motion. Existing methods that recover the camera's heading under known rotation, whether from an IMU or an optimization algorithm, tend to perform well in low-noise, low-outlier conditions, but often decrease in accuracy or become computationally expensive as noise and outlier levels increase. To address these limitations, we propose a novel generalization of the Hough transform on the unit sphere (S(2)) to estimate the camera's heading. First, the method extracts correspondences between two frames and generates a great circle of directions compatible with each pair of correspondences. Then, by discretizing the unit sphere using a Fibonacci lattice as bin centers, each great circle casts votes for a range of directions, ensuring that features unaffected by noise or dynamic objects vote consistently for the correct motion direction. Experimental results on three datasets demonstrate that the proposed method is on the Pareto frontier of accuracy versus efficiency. Additionally, experiments on SLAM show that the proposed method reduces RMSE by correcting the heading during camera pose initialization.", "AI": {"tldr": "提出了一种基于费波那契格栅的霍夫变换在单位球面上的新方法，用于实时估计单目视频中的相机头部方向。", "motivation": "现有方法在低噪声和少量异常值条件下表现良好，但在噪声和异常值增加时准确性降低或计算成本高昂。为解决这些问题，提出了一种新的方法来提高估计的准确性和效率。", "method": "通过提取两帧之间的对应关系并生成兼容每个对的方向的大圆圈，然后使用费波那契格栅离散单位球面作为投票中心，确保不受噪声和动态对象影响的功能一致地为正确的运动方向投票。", "result": "实验结果表明该方法在精度和效率上处于帕累托前沿。在SLAM实验中显示，该方法通过校正相机姿态初始化的头部方向来降低RMSE。", "conclusion": "所提出的方法能够在高噪声和动态环境条件下有效地估计单目视频中的相机头部方向，并且适用于实时应用如SLAM、视觉测距法等任务。"}}
{"id": "2602.23114", "pdf": "https://arxiv.org/pdf/2602.23114", "abs": "https://arxiv.org/abs/2602.23114", "authors": ["Xudong Yan", "Songhe Feng", "Jiaxin Wang", "Xin Su", "Yi Jin"], "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .", "AI": {"tldr": "提出了一种名为WARM-CAT的方法，用于解决组成零样本学习中的标签空间分布偏移问题。", "motivation": "现有方法在测试时由于新组合出现而导致性能下降。为了解决这个问题，需要一种能够在测试时积累综合知识并更新多模态原型的新方法。", "method": "通过积累文本和视觉模式下的未监督数据来更新多模态原型，并引入自适应更新权重以控制原型调整程度；使用动态优先级队列存储高置信度图像获取历史图像中的视觉原型，同时利用训练图像初始化队列，以解决测试时模型倾向于重复已知组合的问题。", "result": "在四个基准数据集上实现了闭世界和开放世界的最新性能。", "conclusion": "WARM-CAT方法通过引入多模态协作表示学习来对齐文本和视觉原型，并成功解决了组成零样本学习中的分布偏移问题。"}}
{"id": "2602.23109", "pdf": "https://arxiv.org/pdf/2602.23109", "abs": "https://arxiv.org/abs/2602.23109", "authors": ["Kai Chen", "Yuyao Huang", "Guang Chen"], "title": "Towards Intelligible Human-Robot Interaction: An Active Inference Approach to Occluded Pedestrian Scenarios", "categories": ["cs.RO"], "comment": "14 pages, 6 figures, Proceedings of the 2026 ACM/IEEE International Conference on Human-Robot Interaction (HRI'26)", "summary": "The sudden appearance of occluded pedestrians presents a critical safety challenge in autonomous driving. Conventional rule-based or purely data-driven approaches struggle with the inherent high uncertainty of these long-tail scenarios. To tackle this challenge, we propose a novel framework grounded in Active Inference, which endows the agent with a human-like, belief-driven mechanism. Our framework leverages a Rao-Blackwellized Particle Filter (RBPF) to efficiently estimate the pedestrian's hybrid state. To emulate human-like cognitive processes under uncertainty, we introduce a Conditional Belief Reset mechanism and a Hypothesis Injection technique to explicitly model beliefs about the pedestrian's multiple latent intentions. Planning is achieved via a Cross-Entropy Method (CEM) enhanced Model Predictive Path Integral (MPPI) controller, which synergizes the efficient, iterative search of CEM with the inherent robustness of MPPI. Simulation experiments demonstrate that our approach significantly reduces the collision rate compared to reactive, rule-based, and reinforcement learning (RL) baselines, while also exhibiting explainable and human-like driving behavior that reflects the agent's internal belief state.", "AI": {"tldr": "提出了一种基于主动推理的框架，用于解决自动驾驶中遇到突然出现的行人时的安全挑战。", "motivation": "传统的规则基础或纯粹的数据驱动方法在处理这些长期尾部场景中的高不确定性方面存在困难。为了应对这一挑战，提出了一个新框架，该框架赋予代理类似人类的认知机制。", "method": "利用Rao-Blackwellized粒子滤波器（RBPF）有效地估计行人的混合状态；引入条件信念重置和假设注入技术以模拟不确定情况下的类似人类认知过程；通过增强型交叉熵方法（CEM）模型预测路径积分控制器实现规划。", "result": "与反应性、规则基础和强化学习基线相比，实验结果表明该方法显著降低了碰撞率，并展示了可解释的、类似人的驾驶行为，反映了代理内部的状态信念。", "conclusion": "提出的方法在处理自动驾驶中突然出现行人的挑战时表现出色，不仅减少了碰撞风险，还展现了更加安全可控的行为模式。"}}
{"id": "2602.23108", "pdf": "https://arxiv.org/pdf/2602.23108", "abs": "https://arxiv.org/abs/2602.23108", "authors": ["Yonglin Chen", "Pengcheng An", "Xueliang Li"], "title": "FuturePrism: Supporting Adolescence in Collaborative Storytelling to Cope with Future Uncertainty", "categories": ["cs.HC"], "comment": null, "summary": "FuturePrism is a GenAI-empowered collaborative storytelling system designed to scaffold adolescents to navigate future life challenges. Adolescents often suffer from anxiety related to future uncertainty for lacking the executive function to develop concrete pathways. Operationalizing Snyder's Hope Theory, the system utilizes a triadic role-play mechanics to externalize cognitive processes through four narrative chapters: The Goal, The Opportunity, The Challenge, and The Agency. An evaluation workshop with 20 adolescents demonstrated that FuturePrism significantly enhances momentary hope levels, particularly in the Agency dimension. Participants reported high levels of narrative immersion and positive feedback towards system usability. Participants also confirmed that the AI-scaffolded collaborative storytelling empowered them to develop positive attitudes towards future challenges.", "AI": {"tldr": "本文提出了一种利用AI赋能的协作叙事系统FuturePrism，以帮助青少年应对未来的不确定性。", "motivation": "青少年由于缺乏执行功能而难以规划未来，导致对未来感到焦虑和不确定。通过Snyder希望理论的操作化，该论文旨在开发一种能够协助青少年建立积极态度面对未来挑战的系统。", "method": "设计并实施了一个基于GenAI赋能的协作叙事系统FuturePrism，采用四章节叙述结构：目标、机会、挑战与行动力，并通过角色扮演机制来外部化认知过程。", "result": "20名青少年参与的工作坊表明，FuturePrism显著提高了短期希望水平，尤其是在行动力维度上。参与者报告了高度的故事沉浸感和系统可用性的正面反馈。", "conclusion": "AI辅助的协作叙事为青少年提供了开发积极态度面对未来挑战的能力，并且证明了这一方法的有效性。"}}
{"id": "2602.23103", "pdf": "https://arxiv.org/pdf/2602.23103", "abs": "https://arxiv.org/abs/2602.23103", "authors": ["Fuhao Zhang", "Lei Liu", "Jialin Zhang", "Ya-Nan Zhang", "Nan Mu"], "title": "SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "提出了一种新的频域解耦框架SpectralMamba-UNet，用于医学图像分割。", "motivation": "现有的状态空间模型在建模全局结构和细粒度细节方面效果不佳，其一维序列化削弱了局部连续性和高频表示能力。", "method": "通过离散余弦变换将低频与高频特征分离，并使用频率域Mamba进行全局上下文建模。同时引入通道重加权机制形成注意力机制，以及自适应多尺度融合模块SGF以平衡不同频率贡献。", "result": "实验在五个公开基准数据集上显示了对不同模式和分割目标的一致性改进，证明了该方法的有效性和泛化能力。", "conclusion": "SpectralMamba-UNet框架通过频域解耦技术有效地提升了医学图像分割的准确度，并展示了良好的跨模态泛化性能。"}}
{"id": "2602.23101", "pdf": "https://arxiv.org/pdf/2602.23101", "abs": "https://arxiv.org/abs/2602.23101", "authors": ["Paul Kielty", "Timothy Hanley", "Peter Corcoran"], "title": "Locally Adaptive Decay Surfaces for High-Speed Face and Landmark Detection with Event Cameras", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras record luminance changes with microsecond resolution, but converting their sparse, asynchronous output into dense tensors that neural networks can exploit remains a core challenge. Conventional histograms or globally-decayed time-surface representations apply fixed temporal parameters across the entire image plane, which in practice creates a trade-off between preserving spatial structure during still periods and retaining sharp edges during rapid motion. We introduce Locally Adaptive Decay Surfaces (LADS), a family of event representations in which the temporal decay at each location is modulated according to local signal dynamics. Three strategies are explored, based on event rate, Laplacian-of-Gaussian response, and high-frequency spectral energy. These adaptive schemes preserve detail in quiescent regions while reducing blur in regions of dense activity. Extensive experiments on the public data show that LADS consistently improves both face detection and facial landmark accuracy compared to standard non-adaptive representations. At 30 Hz, LADS achieves higher detection accuracy and lower landmark error than either baseline, and at 240 Hz it mitigates the accuracy decline typically observed at higher frequencies, sustaining 2.44 % normalized mean error for landmarks and 0.966 mAP50 in face detection. These high-frequency results even surpass the accuracy reported in prior works operating at 30 Hz, setting new benchmarks for event-based face analysis. Moreover, by preserving spatial structure at the representation stage, LADS supports the use of much lighter network architectures while still retaining real-time performance. These results highlight the importance of context-aware temporal integration for neuromorphic vision and point toward real-time, high-frequency human-computer interaction systems that exploit the unique advantages of event cameras.", "AI": {"tldr": "本文提出了一种局部自适应衰减表面（LADS）的方法，用于从事件相机的输出中生成更加有效的表示形式，并在此基础上实现了高性能的脸部检测和面部关键点定位。", "motivation": "传统的事件时间表表示方法采用固定的时域参数，导致在静止期间无法保留空间结构而在快速移动区域产生模糊。为了解决这些问题，作者提出了局部自适应衰减表面（LADS）以提高事件相机数据处理的精度和效率。", "method": "通过调节每个位置的时间衰减来根据局部信号动态变化引入局部自适应衰减表面（LADS），探索了基于事件速率、高斯拉普拉斯响应以及高频谱能量三种策略。这些方法能够在静态区域保留细节并减少活动密集区域的模糊效果，从而提高人脸检测和面部关键点定位的准确性。", "result": "实验表明，与标准非自适应表示相比，LADS在30Hz下实现了更高的检测准确性和更低的关键点误差，在240Hz下保持了较高的精度，甚至超过了先前工作在30Hz下的性能。此外，通过保留空间结构，LADS支持使用较轻的网络架构，同时仍能保持实时性能。", "conclusion": "研究结果突显了解决神经形态视觉中基于上下文的时间集成的重要性，并为利用事件相机的独特优势实现高速、实时的人机交互系统铺平了道路。"}}
{"id": "2602.23095", "pdf": "https://arxiv.org/pdf/2602.23095", "abs": "https://arxiv.org/abs/2602.23095", "authors": ["Yonglin Chen", "Jingjing Zhang", "Kezhuo Wang", "Pengcheng An", "Xueliang Li"], "title": "TaleBot: A Tangible AI Companion to Support Children in Co-creative Storytelling for Resilience Cultivation", "categories": ["cs.HC"], "comment": null, "summary": "Resilience is a key factor affecting children's mental wellbeing and future development. Yet, limited HCI research has explored how to help children build resilience through adversarial experiences. Informed by a formative study with elementary school teachers and professional psychologists, we design TaleBot, an AI-empowered system that supports children to co-create stories about overcoming everyday adversities tailored to their personal situations. We evaluated the system with 12 elementary children in school counseling rooms under teacher guidance and conducted reflective interviews with parents upon the Child-AI co-created stories. The findings show that TaleBot encourages children in self-expression of feelings and thoughts, creating opportunities for teachers to provide personalized support and for parents to better understand the profound impact of family communication on children's mental wellbeing. We conclude with design implications for using generative AI to support children's mental health education and interventions across school and family contexts.", "AI": {"tldr": "TaleBot是一款通过共创故事来帮助儿童培养应对逆境能力的AI系统。", "motivation": "提高儿童的心理健康和未来发展的关键在于增强其韧性。目前在HCI领域，关于如何通过逆境经历帮助儿童建立韧性的研究较少。", "method": "根据与小学教师和专业心理学家的形成性研究设计了TaleBot，并在学校咨询室中对12名小学生进行了系统评估，在家长陪同下进行反思访谈。", "result": "TaleBot鼓励儿童表达情绪和想法，为老师提供个性化支持的机会，并帮助父母更好地理解家庭沟通对孩子心理健康的影响。", "conclusion": "通过使用生成性AI来支持跨学校和家庭环境中的儿童心理教育和干预措施得出设计启示。"}}
{"id": "2602.23093", "pdf": "https://arxiv.org/pdf/2602.23093", "abs": "https://arxiv.org/abs/2602.23093", "authors": ["Dhwanil M. Mori", "Neil F. Johnson"], "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents", "categories": ["cs.AI", "cs.SI", "physics.soc-ph"], "comment": null, "summary": "Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.", "AI": {"tldr": "研究了多个自主AI代理在有限资源下请求访问时的行为模式，发现形成了类似《蝇王》中的部落主义。", "motivation": "探究未来基础设施系统中由自主AI代理控制的资源配置问题可能引发的社会行为和资源利用效率。", "method": "在一个简化框架内模拟N个独立决策的AI代理每一轮是否请求一个单位从固定容量C的系统获取资源。观察不同部落类型形成的过程及其对资源使用的影响。", "result": "出现三种主要部落类型：攻击性（27.3%）、保守型（24.7%）和机会主义型（48.1%）。更强大的AI代理不仅没有改善资源利用效率，反而增加了系统失败的风险。", "conclusion": "研究结果表明，更加智能的AI代理可能会因部落化行为导致决策变得更糟糕。"}}
{"id": "2602.23092", "pdf": "https://arxiv.org/pdf/2602.23092", "abs": "https://arxiv.org/abs/2602.23092", "authors": ["Zhuoliang Xie", "Fei Liu", "Zhenkun Wang", "Qingfu Zhang"], "title": "Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design", "categories": ["cs.AI"], "comment": null, "summary": "The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.", "AI": {"tldr": "本文提出了一种利用大型语言模型（LLM）自动设计启发式算法来改进车辆路径问题（CVRP）求解器的新方法。", "motivation": "研究目的是解决NP难的CVRP优化挑战，特别是在大规模实例中面临的计算难题。", "method": "采用自适应局部搜索与进化框架相结合的方法，通过LLM动态生成和优化启发式算法，并引入加速机制以提高计算效率。", "result": "实验评估表明，AILS-AHD方法优于现有的求解器，尤其是在CVRPLib的大规模基准测试中取得了8个新的最佳解决方案。", "conclusion": "研究展示了LLM驱动的启发式设计在车辆路径优化领域的潜力，并建立了多个新纪录。"}}
{"id": "2602.23090", "pdf": "https://arxiv.org/pdf/2602.23090", "abs": "https://arxiv.org/abs/2602.23090", "authors": ["Jeremy Wertheim Co Chen", "Rendell Christian Ngo", "Cedric Matthew Yu", "Hans Emilio Lumagui", "Ethan Badayos", "Jordan Aiko Deja"], "title": "Beyond Faders: Understanding 6DoF Gesture Ecologies in Music Mixing", "categories": ["cs.HC"], "comment": "5 pages, 2 figuresl, CHI 2026 Poster", "summary": "Extended reality (XR) enables new music-mixing workflows by moving beyond 2D faders toward embodied, spatial interaction. However, it remains unclear which six-degree-of-freedom (6DoF) gestures align with real-world mixing practices and whether such interactions support manageable cognitive load and positive user experience. We conducted a design workshop with experienced mixers to elicit gesture concepts for core audio tasks gain, compression, equalization, and automation, and implemented these in an XR prototype. A user study (n=12) evaluated the ecological validity of the gestures using cognitive load measures, user-experience ratings, and interviews. Participants generally found 6DoF gestures intuitive and well-mapped to mixing tasks, reporting strong immersion and a sense of connection with the audio environment. Cognitive load differences across gestures were minimal, though participants expressed preferences shaped by workflow familiarity and perceived control. We discuss implications for designing XR mixing tools that balance expressiveness, precision, and ecological validity.", "AI": {"tldr": "本文探讨了在音乐混合中使用六自由度（6DoF）手势的可行性和用户体验，通过设计工作坊和用户研究验证其生态有效性和认知负荷。", "motivation": "探索XR环境中六自由度（6DoF）手势是否能更好地支持音频任务，并评估它们的认知负载和用户体验。", "method": "首先进行了一次与经验丰富的混音师的工作坊以产生手势概念，随后开发了一个原型并在12名用户中进行了研究，通过认知负荷测量、用户体验评分和访谈来评价这些手势的生态有效性和用户体验。", "result": "参与者总体上认为6DoF手势直观且易于理解，并报告了强烈的沉浸感和与音频环境的联系；不同手势的认知负载差异较小，但用户的偏好受到工作流程熟悉度和感知控制的影响。", "conclusion": "设计XR混合工具时应平衡表达性、精确性和生态有效性以满足用户需求。"}}
{"id": "2602.23088", "pdf": "https://arxiv.org/pdf/2602.23088", "abs": "https://arxiv.org/abs/2602.23088", "authors": ["Matthew Sutton", "Katrin Amunts", "Timo Dickscheid", "Christian Schiffer"], "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy", "categories": ["cs.CV"], "comment": "8 pages, 3 figures, submitted for inclusion at a conference", "summary": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.", "AI": {"tldr": "本文提出了一种通过标签将图像和文本链接的方法，用于生成关于人类脑切片显微图的自然语言描述。", "motivation": "在缺乏高质量配对图像-文本数据的情况下，开发一种方法以辅助研究人员理解和分析细胞体染色的人类大脑组织切片。", "method": "通过自动从相关文献中挖掘区域描述并与现有的基础模型CytoNet结合，利用大型语言模型生成自然语言描述。这种方法仅依赖于标签，无需配对的图像-文本数据。", "result": "该方法在57个脑区上表现良好，能够生成有意义且具有区分性的自然语言描述，并达到90.6%的准确性以匹配参考标签，甚至在隐藏区域标签的情况下，在8种分类任务中也能达到68.6%的准确度。", "conclusion": "该研究表明通过弱监督学习可以将现有的生物医学视觉基础模型与语言连接起来，即使在缺乏精细配对注释的情境下也具有实用性。"}}
{"id": "2602.23073", "pdf": "https://arxiv.org/pdf/2602.23073", "abs": "https://arxiv.org/abs/2602.23073", "authors": ["Yaacov Pariente", "Vadim Indelman"], "title": "Accelerated Online Risk-Averse Policy Evaluation in POMDPs with Theoretical Guarantees and Novel CVaR Bounds", "categories": ["math.ST", "cs.AI"], "comment": null, "summary": "Risk-averse decision-making under uncertainty in partially observable domains is a central challenge in artificial intelligence and is essential for developing reliable autonomous agents. The formal framework for such problems is the partially observable Markov decision process (POMDP), where risk sensitivity is introduced through a risk measure applied to the value function, with Conditional Value-at-Risk (CVaR) being a particularly significant criterion. However, solving POMDPs is computationally intractable in general, and approximate methods rely on computationally expensive simulations of future agent trajectories. This work introduces a theoretical framework for accelerating CVaR value function evaluation in POMDPs with formal performance guarantees. We derive new bounds on the CVaR of a random variable X using an auxiliary random variable Y, under assumptions relating their cumulative distribution and density functions; these bounds yield interpretable concentration inequalities and converge as the distributional discrepancy vanishes. Building on this, we establish upper and lower bounds on the CVaR value function computable from a simplified belief-MDP, accommodating general simplifications of the transition dynamics. We develop estimators for these bounds within a particle-belief MDP framework with probabilistic guarantees, and employ them for acceleration via action elimination: actions whose bounds indicate suboptimality under the simplified model are safely discarded while ensuring consistency with the original POMDP. Empirical evaluation across multiple POMDP domains confirms that the bounds reliably separate safe from dangerous policies while achieving substantial computational speedups under the simplified model.", "AI": {"tldr": "提出了一种加速部分可观察马尔科夫决策过程（POMDP）中条件值在风险（CVaR）评估的方法，具有理论性能保证。", "motivation": "解决不确定性环境下部分可观测域的风险规避决策问题，在人工智能和自主代理开发中至关重要。传统方法计算复杂度高，本文提出了一种新的加速框架来改进这一点。", "method": "引入了基于辅助随机变量的新CVaR界限，并建立了一个简化信念MDP模型以适应一般化的过渡动态简化。使用粒子信念MDP框架发展估计器，并通过行动消除实现加速。", "result": "跨多个POMDP领域进行了实证评估，证实该方法可以可靠地区分安全策略和危险策略，同时在简化的模型下实现了显著的计算速度提升。", "conclusion": "所提出的方法有效地解决了部分可观测环境下的风险规避决策问题，并通过理论性能保证和新型CVaR界限提高了效率。"}}
{"id": "2602.23071", "pdf": "https://arxiv.org/pdf/2602.23071", "abs": "https://arxiv.org/abs/2602.23071", "authors": ["Yuqi Shi", "Hao Yang", "Xiyao Lu", "Jinsong Zhang"], "title": "Quantity Convergence, Quality Divergence: Disentangling Fluency and Accuracy in L2 Mandarin Prosody", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While second language (L2) learners may acquire target syntactic word order, mapping this syntax onto appropriate prosodic structures remains a persistent challenge. This study investigates the fossilization and stability of the L2 syntax-prosody interface by comparing 67 native Mandarin speakers with 67 Vietnamese learners using the BLCU-SAIT corpus. By integrating C-ToBI boundary annotation with Dependency Grammar analysis, we examined both the quantity of prosodic boundaries and their mapping to syntactic relations. Results reveal a non-linear acquisition: although high-proficiency learners (VNH) converge to the native baseline in boundary quantity at the Major Phrase level (B3), their structural mapping significantly diverges. Specifically, VNH demote the prosodic boundary at the Subject-Verb (SBV) interface (Major Phrase B3 -> Prosodic Word B1), while erroneously promoting the boundary at the Verb-Object (VOB) interface (Prosodic Word B1 -> Major Phrase B3). This strategy allows learners to maintain high long phrasal output at the expense of structural accuracy. This results in a distorted prosodic hierarchy where the native pattern is inverted.", "AI": {"tldr": "本文研究了二语学习者在汉语韵律结构上的掌握情况，通过对比母语使用者和越南学习者的语料库数据，发现高熟练度的学习者虽然能够在某些数量上接近母语水平，但在结构映射方面出现偏差。", "motivation": "第二语言（L2）学习者即使掌握了目标句法结构，也将这些结构映射到适当的韵律结构仍然面临挑战。研究旨在探讨二语语法-韵律界面的稳固性和固化现象。", "method": "通过整合BLCU-SAIT语料库中C-ToBI边界标注和依存文法分析，比较了67名母语使用者与67名越南学习者在主音节层级（Major Phrase B3）和小句层级（Prosodic Word B1）的韵律边界数量及其结构映射。", "result": "结果表明高熟练度的学习者虽然在主音节层级上接近母语水平，但在结构映射方面出现偏差：他们减少了主谓界面（SBV）处的韵律边界，却错误地增加了动宾界面（VOB）处的边界。这导致学习者的韵律层次被扭曲。", "conclusion": "高熟练度二语学习者通过牺牲结构准确性来保持较长音节输出，形成了与母语相反的韵律层级模式。"}}
{"id": "2602.23070", "pdf": "https://arxiv.org/pdf/2602.23070", "abs": "https://arxiv.org/abs/2602.23070", "authors": ["Sanjid Hasan", "Risalat Labib", "A H M Fuad", "Bayazid Hasan"], "title": "Make It Hard to Hear, Easy to Learn: Long-Form Bengali ASR and Speaker Diarization via Extreme Augmentation and Perfect Alignment", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "4 pages, 2 figures", "summary": "Although Automatic Speech Recognition (ASR) in Bengali has seen significant progress, processing long-duration audio and performing robust speaker diarization remain critical research gaps. To address the severe scarcity of joint ASR and diarization resources for this language, we introduce Lipi-Ghor-882, a comprehensive 882-hour multi-speaker Bengali dataset. In this paper, detailing our submission to the DL Sprint 4.0 competition, we systematically evaluate various architectures and approaches for long-form Bengali speech. For ASR, we demonstrate that raw data scaling is ineffective; instead, targeted fine-tuning utilizing perfectly aligned annotations paired with synthetic acoustic degradation (noise and reverberation) emerges as the singular most effective approach. Conversely, for speaker diarization, we observed that global open-source state-of-the-art models (such as Diarizen) performed surprisingly poorly on this complex dataset. Extensive model retraining yielded negligible improvements; instead, strategic, heuristic post-processing of baseline model outputs proved to be the primary driver for increasing accuracy. Ultimately, this work outlines a highly optimized dual pipeline achieving a $\\sim$0.019 Real-Time Factor (RTF), establishing a practical, empirically backed benchmark for low-resource, long-form speech processing.", "AI": {"tldr": "本文介绍了针对长时间孟加拉语语音的自动语音识别（ASR）和说话人区分任务的方法和技术。", "motivation": "尽管孟加拉语的自动语音识别已经取得了显著进步，但对于处理长时音频和进行稳健的说话人区分仍然存在关键研究缺口。为了解决这种语言中联合ASR和区分资源严重不足的问题，本文提出了Lipi-Ghor-882数据集。", "method": "对于ASR任务，作者展示了单纯的数据扩展无效；相反，利用完美对齐注释与合成的声学退化（噪声和混响）进行有针对性的微调是最有效的。对于说话人区分任务，预训练模型表现不佳，改进主要通过基线模型输出的战略性、启发式后处理实现。", "result": "研究提出了一种高度优化的双管道系统，在约0.019倍实时因子（RTF）下运行，并为低资源长时语音处理建立了实践可行和经验支持的标准。", "conclusion": "本文的工作提供了一个针对长时间孟加拉语语音的有效解决方案，包括数据集构建、模型训练与优化策略。"}}
{"id": "2602.23069", "pdf": "https://arxiv.org/pdf/2602.23069", "abs": "https://arxiv.org/abs/2602.23069", "authors": ["Yiding Sun", "Jihua Zhu", "Haozhe Cheng", "Chaoyi Lu", "Zhichuan Yang", "Lin Chen", "Yaonan Wang"], "title": "Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception", "categories": ["cs.CV"], "comment": null, "summary": "Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel \"Align then Adapt\" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \\% accuracy on 3D action recognition, $+8.7 \\%$ on 4 D action segmentation, and 84.06\\% on 4D semantic segmentation.", "AI": {"tldr": "提出了一种'对齐然后适应'(PointATA)方法，用于将3D预训练模型有效转移到4D感知任务中。", "motivation": "为了克服在从3D到4D转移学习过程中的过度拟合和模态差异问题，该研究旨在开发一种更有效的参数高效迁移学习技术。", "method": "利用最优传输理论量化3D与4D数据集之间的分布差距，并通过点对齐嵌入器解决此差距；采用高效的点视频适配器和空间上下文编码器来增强时序建模能力，减轻过度拟合问题。整个方法分为两阶段：首先进行对齐处理以减小模态差距，其次适应性调整模型以提高性能。", "result": "实验表明，PointATA不仅可以在参数效率方面优于之前的工作（例如，在3D动作识别上达到97.21%的精度，在4D动作分割上提高了8.7%，在4D语义分割中实现84.06%的准确率），还能与强大的全量微调模型相媲美甚至超越。", "conclusion": "通过提出'对齐然后适应'(PointATA)方法，研究成功解决了3D到4D数据集迁移学习中的主要挑战，并展示了在多个任务上的优越性能。"}}
{"id": "2602.23068", "pdf": "https://arxiv.org/pdf/2602.23068", "abs": "https://arxiv.org/abs/2602.23068", "authors": ["Trung Dang", "Sharath Rao", "Ananya Gupta", "Christopher Gagne", "Panagiotis Tzirakis", "Alice Baird", "Jakub Piotr Cłapa", "Peter Chin", "Alan Cowen"], "title": "TADA: A Generative Framework for Speech Modeling via Text-Acoustic Dual Alignment", "categories": ["cs.SD"], "comment": null, "summary": "Modern Text-to-Speech (TTS) systems increasingly leverage Large Language Model (LLM) architectures to achieve scalable, high-fidelity, zero-shot generation. However, these systems typically rely on fixed-frame-rate acoustic tokenization, resulting in speech sequences that are significantly longer than, and asynchronous with their corresponding text. Beyond computational inefficiency, this sequence length disparity often triggers hallucinations in TTS and amplifies the modality gap in spoken language modeling (SLM). In this paper, we propose a novel tokenization scheme that establishes one-to-one synchronization between continuous acoustic features and text tokens, enabling unified, single-stream modeling within an LLM. We demonstrate that these synchronous tokens maintain high-fidelity audio reconstruction and can be effectively modeled in a latent space by a large language model with a flow matching head. Moreover, the ability to seamlessly toggle speech modality within the context enables text-only guidance--a technique that blends logits from text-only and text-speech modes to flexibly bridge the gap toward text-only LLM intelligence. Experimental results indicate that our approach achieves performance competitive with state-of-the-art TTS and SLM systems while virtually eliminating content hallucinations and preserving linguistic integrity, all at a significantly reduced inference cost.", "AI": {"tldr": "本文提出了一种通过文本和声学双对齐的生成框架TADA，以解决现代语音合成系统中存在的问题。", "motivation": "现有文本到语音（TTS）系统利用大型语言模型架构进行高保真、零样本生成。然而，这些系统的固定帧率声学标记导致了音频序列与对应文本不同步且长度不一致的问题，从而引发幻觉并扩大模态差距。", "method": "提出了一种新的标记化方案来同步连续的声学特征和文本标记，并在一个大型语言模型中进行统一、单流建模。引入了一个流动匹配头以在潜在空间内有效建模这些同步令牌。", "result": "实验结果表明，该方法不仅实现了与最新TTS和SLM系统相当的性能，还几乎消除了内容幻觉并保持了语言完整性，同时显著降低了推断成本。", "conclusion": "通过文本-声学双对齐技术，可以有效提高语音生成的质量和效率。"}}
{"id": "2602.23061", "pdf": "https://arxiv.org/pdf/2602.23061", "abs": "https://arxiv.org/abs/2602.23061", "authors": ["Bangrui Xu", "Qihang Yao", "Zirui Tang", "Xuanhe Zhou", "Yeye He", "Shihan Yu", "Qianqian Xu", "Bin Wang", "Guoliang Li", "Conghui He", "Fan Wu"], "title": "MoDora: Tree-Based Semi-Structured Document Analysis System", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "comment": "Extension of our SIGMOD 2026 paper. Please refer to source code available at https://github.com/weAIDB/MoDora", "summary": "Semi-structured documents integrate diverse interleaved data elements (e.g., tables, charts, hierarchical paragraphs) arranged in various and often irregular layouts. These documents are widely observed across domains and account for a large portion of real-world data. However, existing methods struggle to support natural language question answering over these documents due to three main technical challenges: (1) The elements extracted by techniques like OCR are often fragmented and stripped of their original semantic context, making them inadequate for analysis. (2) Existing approaches lack effective representations to capture hierarchical structures within documents (e.g., associating tables with nested chapter titles) and to preserve layout-specific distinctions (e.g., differentiating sidebars from main content). (3) Answering questions often requires retrieving and aligning relevant information scattered across multiple regions or pages, such as linking a descriptive paragraph to table cells located elsewhere in the document. To address these issues, we propose MoDora, an LLM-powered system for semi-structured document analysis. First, we adopt a local-alignment aggregation strategy to convert OCR-parsed elements into layout-aware components, and conduct type-specific information extraction for components with hierarchical titles or non-text elements. Second, we design the Component-Correlation Tree (CCTree) to hierarchically organize components, explicitly modeling inter-component relations and layout distinctions through a bottom-up cascade summarization process. Finally, we propose a question-type-aware retrieval strategy that supports (1) layout-based grid partitioning for location-based retrieval and (2) LLM-guided pruning for semantic-based retrieval. Experiments show MoDora outperforms baselines by 5.97%-61.07% in accuracy. The code is at https://github.com/weAIDB/MoDora.", "AI": {"tldr": "MoDora 是一种基于树的半结构化文档分析系统，旨在解决自然语言问题回答中的三个技术挑战。", "motivation": "现有的方法在支持半结构化文档上的自然语言问题回答时存在三个主要的技术难题：元素碎片化、缺乏有效的层级表示和难以跨区域检索信息。MoDora 则通过设计新的策略来克服这些问题，提高对这类文档的分析能力。", "method": "采用局部对齐聚合策略将OCR解析出的片段转换为布局感知组件，并进行类型特定的信息提取；设计了组件关联树（CCTree）以层次化组织这些组件，模型间关系和布局差异；提出问题类型感知检索策略，支持基于位置或语义的检索。", "result": "实验表明，MoDora 在准确性方面比基线方法高出5.97%到61.07%，证明了其有效性。", "conclusion": "通过引入新颖的方法，MoDora 成功解决了半结构化文档分析中的关键挑战，并展示了在自然语言问题回答任务上的优越性能。"}}
{"id": "2602.23058", "pdf": "https://arxiv.org/pdf/2602.23058", "abs": "https://arxiv.org/abs/2602.23058", "authors": ["Zeyu Zhang", "Danning Li", "Ian Reid", "Richard Hartley"], "title": "GeoWorld: Geometric World Models", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to CVPR 2026", "summary": "Energy-based predictive world models provide a powerful approach for multi-step visual planning by reasoning over latent energy landscapes rather than generating pixels. However, existing approaches face two major challenges: (i) their latent representations are typically learned in Euclidean space, neglecting the underlying geometric and hierarchical structure among states, and (ii) they struggle with long-horizon prediction, which leads to rapid degradation across extended rollouts. To address these challenges, we introduce GeoWorld, a geometric world model that preserves geometric structure and hierarchical relations through a Hyperbolic JEPA, which maps latent representations from Euclidean space onto hyperbolic manifolds. We further introduce Geometric Reinforcement Learning for energy-based optimization, enabling stable multi-step planning in hyperbolic latent space. Extensive experiments on CrossTask and COIN demonstrate around 3% SR improvement in 3-step planning and 2% SR improvement in 4-step planning compared to the state-of-the-art V-JEPA 2. Project website: https://steve-zeyu-zhang.github.io/GeoWorld.", "AI": {"tldr": "本文介绍了一种基于几何结构的预测世界模型GeoWorld，通过将潜在表示从欧几里得空间映射到双曲流形来改进多步骤视觉规划。", "motivation": "现有方法在学习潜在表示时忽略状态之间的几何和层次结构，并且长范围预测困难导致快速退化。本文旨在解决这些问题，提高长期预测的稳定性和准确性。", "method": "通过Hyperbolic JEPA将潜在空间从欧几里得映射到双曲流形，保留了几何结构并引入基于能量的优化进行几何强化学习，从而实现稳定的多步骤规划。", "result": "在CrossTask和COIN数据集上的实验表明，在三步和四步预测中分别提高了约3%和2%的成功率。", "conclusion": "GeoWorld通过利用双曲流形中的几何结构改进了能量基世界模型的性能，为长期视觉规划提供了更稳定的解决方案。"}}
{"id": "2602.23057", "pdf": "https://arxiv.org/pdf/2602.23057", "abs": "https://arxiv.org/abs/2602.23057", "authors": ["Jeongin Bae", "Baeseong Park", "Gunho Park", "Minsub Kim", "Joonhyung Lee", "Junhee Yoo", "Sunghyeon Woo", "Jiwon Ryu", "Se Jung Kwon", "Dongsoo Lee"], "title": "Affine-Scaled Attention: Towards Flexible and Stable Transformer Attention", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 14 pages, 11 figures", "summary": "Transformer attention is typically implemented using softmax normalization, which enforces attention weights with unit sum normalization. While effective in many settings, this constraint can limit flexibility in controlling attention magnitudes and may contribute to overly concentrated or unstable attention patterns during training. Prior work has explored modifications such as attention sinks or gating mechanisms, but these approaches provide only limited or indirect control over attention reweighting. We propose Affine-Scaled Attention, a simple extension to standard attention that introduces input-dependent scaling and a corresponding bias term applied to softmax-normalized attention weights. This design relaxes the strict normalization constraint while maintaining aggregation of value representations, allowing the model to adjust both the relative distribution and the scale of attention in a controlled manner. We empirically evaluate Affine-Scaled Attention in large-scale language model pretraining across multiple model sizes. Experimental results show consistent improvements in training stability, optimization behavior, and downstream task performance compared to standard softmax attention and attention sink baselines. These findings suggest that modest reweighting of attention outputs provides a practical and effective way to improve attention behavior in Transformer models.", "AI": {"tldr": "提出了Affine-Scaled Attention方法，改进了Transformer注意力机制以提高稳定性和性能。", "motivation": "传统的softmax标准化限制了注意力权重的灵活性，并可能导致训练不稳定。此研究旨在通过放松严格规范来增强模型控制能力。", "method": "引入了可输入依赖缩放和偏置项应用于软性最大化注意权重，实现灵活调整注意力分布和规模。", "result": "实验表明，在大规模语言模型预训练中相比标准softmax注意力和注意力下水道基线模型，Affine-Scaled Attention提升了训练稳定性、优化行为和下游任务表现。", "conclusion": "通过适度重新加权注意输出提供了一种实用有效的方式以改进Transformer模型中的注意力机制。"}}
{"id": "2602.23056", "pdf": "https://arxiv.org/pdf/2602.23056", "abs": "https://arxiv.org/abs/2602.23056", "authors": ["Giona Fieni", "Joschua Wüthrich", "Marc-Philippe Neumann", "Christopher H. Onder"], "title": "Learning-based Multi-agent Race Strategies in Formula 1", "categories": ["cs.AI", "eess.SY"], "comment": null, "summary": "In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.", "AI": {"tldr": "本文提出了基于强化学习的多智能体赛车策略优化方法，模拟了F1赛事中的能量管理、轮胎磨损等因素。", "motivation": "在F1比赛中，赛事实时变化和对手行为会影响比赛策略。通过引入交互模块和自我对战训练方案来生成对抗性强的策略，以适应不同的竞赛环境。", "method": "基于预训练单智能体政策，引入了考虑对手行为的交互模块，并使用自博弈训练方案产生竞争性策略。", "result": "结果显示代理能够根据对手调整进站时间、轮胎选择和能量分配，获得稳健且一致的比赛表现。", "conclusion": "该框架仅依赖于实际比赛期间可用的信息，可以支持赛前及比赛中赛车战略决策。"}}
{"id": "2602.23053", "pdf": "https://arxiv.org/pdf/2602.23053", "abs": "https://arxiv.org/abs/2602.23053", "authors": ["Ignacio Torroba", "David Dorner", "Victor Nan Fernandez-Ayala", "Mart Kartasev", "Joris Verhagen", "Elias Krantz", "Gregorio Marchesini", "Carl Ljung", "Pedro Roque", "Chelsea Sidrane", "Linda Van der Spaa", "Nicola De Carli", "Petter Ogren", "Christer Fuglesang", "Jana Tumova", "Dimos V. Dimarogonas", "Ivan Stenius"], "title": "Marinarium: a New Arena to Bring Maritime Robotics Closer to Shore", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents the Marinarium, a modular and stand-alone underwater research facility designed to provide a realistic testbed for maritime and space-analog robotic experimentation in a resource-efficient manner. The Marinarium combines a fully instrumented underwater and aerial operational volume, extendable via a retractable roof for real-weather conditions, a digital twin in the SMaRCSim simulator and tight integration with a space robotics laboratory. All of these result from design choices aimed at bridging simulation, laboratory validation, and field conditions. We compare the Marinarium to similar existing infrastructures and illustrate how its design enables a set of experiments in four open research areas within field robotics. First, we exploit high-fidelity dynamics data from the tank to demonstrate the potential of learning-based system identification approaches applied to underwater vehicles. We further highlight the versatility of the multi-domain operating volume via a rendezvous mission with a heterogeneous fleet of robots across underwater, surface, and air. We then illustrate how the presented digital twin can be utilized to reduce the reality gap in underwater simulation. Finally, we demonstrate the potential of underwater surrogates for spacecraft navigation validation by executing spatiotemporally identical inspection tasks on a planar space-robot emulator and a neutrally buoyant \\gls{rov}. In this work, by sharing the insights obtained and rationale behind the design and construction of the Marinarium, we hope to provide the field robotics research community with a blueprint for bridging the gap between controlled and real offshore and space robotics experimentation.", "AI": {"tldr": "Marinarium是一个模块化和独立的水下研究设施，用于提供现实测试环境，使海洋机器人技术更贴近海岸。", "motivation": "为了填补仿真、实验室验证与实际现场条件之间的差距，本文提出了一种新型的研究设施Marinarium。", "method": "该设施结合了完全装备的水下和空中操作空间，并且可以通过可伸缩屋顶扩展以适应真实天气状况。此外，还提供了一个数字孪生模型（SMaRCSim模拟器）与航天机器人实验室紧密集成。通过四个开放的研究领域展示其实验能力。", "result": "Marinarium展示了基于学习的系统识别方法在水下车辆中的潜力，并利用多域操作空间实现异构舰队的对接任务，同时说明数字孪生模型如何减少仿真与现实之间的差距以及水下代理对于航天器导航验证的作用。", "conclusion": "通过分享关于设计和建造Marinarium的见解和理由，希望为海洋机器人研究领域提供一种跨越受控环境和真实海上及太空实验之间的桥梁的设计蓝图。"}}
{"id": "2602.23051", "pdf": "https://arxiv.org/pdf/2602.23051", "abs": "https://arxiv.org/abs/2602.23051", "authors": ["Aihong Wang", "Tenghui Xie", "Fuxi Wen", "Jun Li"], "title": "An Empirical Analysis of Cooperative Perception for Occlusion Risk Mitigation", "categories": ["cs.RO"], "comment": "Accepted for publication in IEEE Internet of Things Journal (Regular Article), 2026. DOI: 10.1109/JIOT.2026.3668184", "summary": "Occlusions present a significant challenge for connected and automated vehicles, as they can obscure critical road users from perception systems. Traditional risk metrics often fail to capture the cumulative nature of these threats over time adequately. In this paper, we propose a novel and universal risk assessment metric, the Risk of Tracking Loss (RTL), which aggregates instantaneous risk intensity throughout occluded periods. This provides a holistic risk profile that encompasses both high-intensity, short-term threats and prolonged exposure. Utilizing diverse and high-fidelity real-world datasets, a large-scale statistical analysis is conducted to characterize occlusion risk and validate the effectiveness of the proposed metric. The metric is applied to evaluate different vehicle-to-everything (V2X) deployment strategies. Our study shows that full V2X penetration theoretically eliminates this risk, the reduction is highly nonlinear; a substantial statistical benefit requires a high penetration threshold of 75-90%. To overcome this limitation, we propose a novel asymmetric communication framework that allows even non-connected vehicles to receive warnings. Experimental results demonstrate that this paradigm achieves better risk mitigation performance. We found that our approach at 25% penetration outperforms the traditional symmetric model at 75%, and benefits saturate at only 50% penetration. This work provides a crucial risk assessment metric and a cost-effective, strategic roadmap for accelerating the safety benefits of V2X deployment.", "AI": {"tldr": "提出了一种新的风险评估指标Risk of Tracking Loss（RTL），并利用该指标评价不同的V2X部署策略，同时提出了一个新型不对称通信框架以克服传统模型的局限性。", "motivation": "传统的风险度量方法难以捕捉遮挡带来的长期威胁累积效应。本文旨在提供一种全面的风险评估方法来应对这一挑战，并通过分析不同V2X部署策略的有效性以及提出新的通信框架来减少车辆间信息交流的成本和提高安全性。", "method": "引入了RTL指标，利用大规模统计数据分析遮挡风险并验证该指标的效用；提出了不对称通信框架，以实现在较低的渗透率下也能有效降低风险。", "result": "研究表明75-90%的V2X部署可以显著减少风险。新的通信框架在25%的渗透率时的表现优于传统的对称模型在75%时的表现，并且在50%的渗透率时达到性能饱和点。", "conclusion": "本文提供了评估遮挡风险的有效工具，同时提出了一个更为经济和有效的V2X部署策略，以加速其安全效益的应用。"}}
{"id": "2602.23043", "pdf": "https://arxiv.org/pdf/2602.23043", "abs": "https://arxiv.org/abs/2602.23043", "authors": ["Argo Saakyan", "Dmitry Solntsev"], "title": "D-FINE-seg: Object Detection and Instance Segmentation Framework with multi-backend deployment", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, 5 tables", "summary": "Transformer-based real-time object detectors achieve strong accuracy-latency trade-offs, and D-FINE is among the top-performing recent architectures. However, real-time instance segmentation with transformers is still less common. We present D-FINE-seg, an instance segmentation extension of D-FINE that adds: a lightweight mask head, segmentation-aware training, including box cropped BCE and dice mask losses, auxiliary and denoising mask supervision, and adapted Hungarian matching cost. On the TACO dataset, D-FINE-seg improves F1-score over Ultralytics YOLO26 under a unified TensorRT FP16 end-to-end benchmarking protocol, while maintaining competitive latency. Second contribution is an end-to-end pipeline for training, exporting, and optimized inference across ONNX, TensorRT, OpenVINO for both object detection and instance segmentation tasks. This framework is released as open-source under the Apache-2.0 license. GitHub repository - https://github.com/ArgoHA/D-FINE-seg.", "AI": {"tldr": "D-FINE-seg 是一个基于 D-FINE 架构的实时目标检测和实例分割框架，通过增加轻量级掩码头、训练时的掩码监督和其他技术改进，在 TACO 数据集上取得了更好的 F1 分数，并提供了统一的训练和推理管道。", "motivation": "虽然基于变压器的目标检测器在精度延迟方面表现出色，但实时实例分割却相对少见。因此，本文提出 D-FINE-seg 框架来解决这一问题，实现高性能的实例分割。", "method": "D-FINE-seg 增加了轻量级掩码头、训练时的掩码监督（包括边界框裁剪的 BCE 和 Dice 掩码损失）、辅助和去噪掩码监督以及自适应匈牙利匹配成本。该框架还提供了统一的训练和推理管道，支持 ONNX, TensorRT, OpenVINO。", "result": "在 TACO 数据集上，D-FINE-seg 在统一的基准测试协议下取得了比 Ultralytics YOLO 更高的 F1 分数，并且保持了竞争性的延迟时间。", "conclusion": "本文提出的 D-FINE-seg 框架能够实现高性能的实例分割同时具备高效的部署能力。框架已开源，可供进一步研究和开发使用。"}}
{"id": "2602.23040", "pdf": "https://arxiv.org/pdf/2602.23040", "abs": "https://arxiv.org/abs/2602.23040", "authors": ["Aashish Rai", "Angela Xing", "Anushka Agarwal", "Xiaoyan Cong", "Zekun Li", "Tao Lu", "Aayush Prakash", "Srinath Sridhar"], "title": "PackUV: Packed Gaussian UV Maps for 4D Volumetric Video", "categories": ["cs.CV"], "comment": "https://ivl.cs.brown.edu/packuv", "summary": "Volumetric videos offer immersive 4D experiences, but remain difficult to reconstruct, store, and stream at scale. Existing Gaussian Splatting based methods achieve high-quality reconstruction but break down on long sequences, temporal inconsistency, and fail under large motions and disocclusions. Moreover, their outputs are typically incompatible with conventional video coding pipelines, preventing practical applications. We introduce PackUV, a novel 4D Gaussian representation that maps all Gaussian attributes into a sequence of structured, multi-scale UV atlas, enabling compact, image-native storage. To fit this representation from multi-view videos, we propose PackUV-GS, a temporally consistent fitting method that directly optimizes Gaussian parameters in the UV domain. A flow-guided Gaussian labeling and video keyframing module identifies dynamic Gaussians, stabilizes static regions, and preserves temporal coherence even under large motions and disocclusions. The resulting UV atlas format is the first unified volumetric video representation compatible with standard video codecs (e.g., FFV1) without losing quality, enabling efficient streaming within existing multimedia infrastructure. To evaluate long-duration volumetric capture, we present PackUV-2B, the largest multi-view video dataset to date, featuring more than 50 synchronized cameras, substantial motion, and frequent disocclusions across 100 sequences and 2B (billion) frames. Extensive experiments demonstrate that our method surpasses existing baselines in rendering fidelity while scaling to sequences up to 30 minutes with consistent quality.", "AI": {"tldr": "本文介绍了PackUV，一种新的4D高斯表示方法，用于在长序列中实现高质量的体视频重建、存储和流式传输。", "motivation": "现有的Gaussian Splatting方法难以处理长时间序列中的时间不一致性和大运动及遮挡情况。此外，其输出通常无法与传统视频编码管道兼容，限制了实际应用。", "method": "提出PackUV，将所有高斯属性映射到一个结构化的多尺度UV图集中，并引入PackUV-GS方法进行时序一致性拟合，以适应长序列体视频的重建。", "result": "该方法在渲染保真度上超越了现有基线，在长达30分钟的序列中也能保持一致的质量。", "conclusion": "通过使用标准视频编解码器（如FFV1）实现了与传统基础设施兼容的高效流式传输，为长时间段体视频捕捉和应用提供了可能。"}}
{"id": "2602.23038", "pdf": "https://arxiv.org/pdf/2602.23038", "abs": "https://arxiv.org/abs/2602.23038", "authors": ["Eiji Kawase", "Shuta Kikuchi", "Hideaki Tamai", "Shu Tanaka"], "title": "Parallelizable Search-Space Decomposition for Large-Scale Combinatorial Optimization Problems Using Ising Machines", "categories": ["cs.ET"], "comment": null, "summary": "Combinatorial optimization problems are crucial in industry. However, many COPs are NP-hard, causing the search space to grow exponentially with problem size and rendering large-scale instances computationally intractable. Conventional solvers typically treat problems as monolithic entities, leading to significant efficiency degradation as structural complexity increases. To address this issue, we propose a novel search-space decomposition method that leverages the inherent structure of variables to systematically reduce the size of the master problem. We formulate interaction costs between variables and individual variable costs as a constrained maximum cut problem and convert it into a quadratic unconstrained binary optimization formulation using penalty terms. An Ising-model solver is used to rapidly decompose the problem into independent small-scale subproblems, which are subsequently solved in parallel using mathematical optimization solvers. We validated this method on the capacitated vehicle routing problem. Results demonstrate three significant benefits: a substantial enhancement in feasible solution rates, accelerated convergence, achieving in 1 min the accuracy that the naive method required 30 min to reach, and a variable reduction of up to 95.32\\%. These findings suggest that search-space decomposition is a promising strategy for efficiently solving large-scale combinatorial optimization problems.", "AI": {"tldr": "提出了一种利用Ising机器进行大规模组合优化问题搜索空间分解的方法，通过将问题分解为独立的子问题并行求解以提高效率和可行性解决方案率。", "motivation": "为了应对大规模组合优化问题由于NP难而导致计算不可处理的问题，该方法旨在通过结构化变量间的相互作用来减少主问题的规模，并将其转换为小规模子问题进行快速求解。", "method": "将变量之间的交互成本和单个变量的成本定义为约束最大割问题，并使用惩罚项将其转换为二次无约束二元优化模型。然后，利用Ising模型求解器分解成独立的小规模子问题，并并行地使用数学优化求解器解决这些问题。", "result": "在验证方法的有效性时，该算法展示了显著的改进：可行解决方案率提高了95.32%，且加速了收敛速度，在1分钟内达到传统方法需要30分钟才能实现的精度。", "conclusion": "研究表明，搜索空间分解是一种有效策略，可以高效地解决大规模组合优化问题。"}}
{"id": "2602.23036", "pdf": "https://arxiv.org/pdf/2602.23036", "abs": "https://arxiv.org/abs/2602.23036", "authors": ["Jaehong Cho", "Hyunmin Choi", "Guseul Heo", "Jongse Park"], "title": "LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure", "categories": ["cs.DC", "cs.AI"], "comment": "12 pages, 10 figures", "summary": "Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework. This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.", "AI": {"tldr": "LLMServingSim 2.0 是一种统一的系统级模拟器，用于在异构和解耦的大语言模型（LLM）服务基础设施中明确并分析运行时驱动的硬件软件交互。", "motivation": "随着大语言模型部署集成多样化加速器和技术以提高效率，理解这些因素之间的实时交互变得挑战性。现有的模拟器无法有效联合建模异构硬件和解耦的服务技术，因此需要一种新的工具来解决这个问题。", "method": "LLMServingSim 2.0 嵌入服务决策和硬件行为到单个运行循环中，支持新兴加速器和内存系统的扩展集成，并通过基于配置文件的建模捕获动态服务行为以及系统级效果。", "result": "模拟器与真实部署进行验证，显示它在关键性能、内存和功率指标上复制率达到99.03%，同时即使对于复杂配置也能保持大约10分钟的仿真时间。", "conclusion": "LLMServingSim 2.0 提供了一座实用桥梁，将硬件创新和服务系统设计联系起来，并能进行下一代 LLM 服务基础设施的系统探索和协同设计。"}}
{"id": "2602.23031", "pdf": "https://arxiv.org/pdf/2602.23031", "abs": "https://arxiv.org/abs/2602.23031", "authors": ["Zhangjian Ji", "Huijia Yan", "Shaotong Qiao", "Kai Feng", "Wei Wei"], "title": "Small Object Detection Model with Spatial Laplacian Pyramid Attention and Multi-Scale Features Enhancement in Aerial Images", "categories": ["cs.CV"], "comment": null, "summary": "Detecting objects in aerial images confronts some significant challenges, including small size, dense and non-uniform distribution of objects over high-resolution images, which makes detection inefficient. Thus, in this paper, we proposed a small object detection algorithm based on a Spatial Laplacian Pyramid Attention and Multi-Scale Feature Enhancement in aerial images. Firstly, in order to improve the feature representation of ResNet-50 on small objects, we presented a novel Spatial Laplacian Pyramid Attention (SLPA) module, which is integrated after each stage of ResNet-50 to identify and emphasize important local regions. Secondly, to enhance the model's semantic understanding and features representation, we designed a Multi-Scale Feature Enhancement Module (MSFEM), which is incorporated into the lateral connections of C5 layer for building Feature Pyramid Network (FPN). Finally, the features representation quality of traditional feature pyramid network will be affected because the features are not aligned when the upper and lower layers are fused. In order to handle it, we utilized deformable convolutions to align the features in the fusion processing of the upper and lower levels of the Feature Pyramid Network, which can help enhance the model's ability to detect and recognize small objects. The extensive experimental results on two benchmark datasets: VisDrone and DOTA demonstrate that our improved model performs better for small object detection in aerial images compared to the original algorithm.", "AI": {"tldr": "提出了一种基于空间拉普拉斯金字塔注意模块和多尺度特征增强的小目标检测算法。", "motivation": "解决了在高分辨率图像中，小目标尺寸、密集且非均匀分布导致的低效检测问题。", "method": "提出了空间拉普拉斯金字塔注意（SLPA）模块以提高ResNet-50对小物体的特征表示；设计了多尺度特征增强模块（MSFEM），用于构建特征金字塔网络；使用可变形卷积来对齐上层和下层的特征。", "result": "在VisDrone和DOTA数据集上的实验表明，改进后的模型比原始算法有更好的小目标检测性能。", "conclusion": "新提出的基于SLPA和MSFEM的小目标检测方法能够显著提高对小目标的识别精度。"}}
{"id": "2602.23029", "pdf": "https://arxiv.org/pdf/2602.23029", "abs": "https://arxiv.org/abs/2602.23029", "authors": ["Tianyue Wang", "Leigang Qu", "Tianyu Yang", "Xiangzhao Hao", "Yifan Xu", "Haiyun Guo", "Jinqiao Wang"], "title": "WISER: Wider Search, Deeper Thinking, and Adaptive Fusion for Training-Free Zero-Shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images given a multimodal query (comprising a reference image and a modification text), without training on annotated triplets. Existing methods typically convert the multimodal query into a single modality-either as an edited caption for Text-to-Image retrieval (T2I) or as an edited image for Image-to-Image retrieval (I2I). However, each paradigm has inherent limitations: T2I often loses fine-grained visual details, while I2I struggles with complex semantic modifications. To effectively leverage their complementary strengths under diverse query intents, we propose WISER, a training-free framework that unifies T2I and I2I via a \"retrieve-verify-refine\" pipeline, explicitly modeling intent awareness and uncertainty awareness. Specifically, WISER first performs Wider Search by generating both edited captions and images for parallel retrieval to broaden the candidate pool. Then, it conducts Adaptive Fusion with a verifier to assess retrieval confidence, triggering refinement for uncertain retrievals, and dynamically fusing the dual-path for reliable ones. For uncertain retrievals, WISER generates refinement suggestions through structured self-reflection to guide the next retrieval round toward Deeper Thinking. Extensive experiments demonstrate that WISER significantly outperforms previous methods across multiple benchmarks, achieving relative improvements of 45% on CIRCO (mAP@5) and 57% on CIRR (Recall@1) over existing training-free methods. Notably, it even surpasses many training-dependent methods, highlighting its superiority and generalization under diverse scenarios. Code will be released at https://github.com/Physicsmile/WISER.", "AI": {"tldr": "本文提出了WISER框架，用于无训练的零样本合成图像检索任务。", "motivation": "当前方法在处理复杂语义修改时存在局限性，T2I丢失了视觉细节而I2I难以应对复杂的语义变换。为解决这些问题，本文提出了一种新的方法来结合这两种方式的优点。", "method": "WISER采用“检索-验证-细化”流程，通过生成编辑后的文本和图像并行进行检索以拓宽候选池，并使用适应性融合根据验证器的评估结果决定是否对不确定的结果进行细化处理。", "result": "实验结果显示，在CIRCO（mAP@5）上比现有无训练方法提高45%，在CIRR（Recall@1）上提高57%。WISER甚至超越了许多需要训练的方法，展示了其优越性和泛化能力。", "conclusion": "WISER框架能够有效结合T2I和I2I的优点，在多种场景下表现出色，显著提高了零样本合成图像检索的效果。"}}
{"id": "2602.23024", "pdf": "https://arxiv.org/pdf/2602.23024", "abs": "https://arxiv.org/abs/2602.23024", "authors": ["Jiahao Liu", "Cui Wenbo", "Haoran Li", "Dongbin Zhao"], "title": "InCoM: Intent-Driven Perception and Structured Coordination for Whole-Body Mobile Manipulation", "categories": ["cs.RO"], "comment": "16 pages, 9 figures", "summary": "Whole-body mobile manipulation is a fundamental capability for general-purpose robotic agents, requiring both coordinated control of the mobile base and manipulator and robust perception under dynamically changing viewpoints. However, existing approaches face two key challenges: strong coupling between base and arm actions complicates whole-body control optimization, and perceptual attention is often poorly allocated as viewpoints shift during mobile manipulation. We propose InCoM, an intent-driven perception and structured coordination framework for whole-body mobile manipulation. InCoM infers latent motion intent to dynamically reweight multi-scale perceptual features, enabling stage-adaptive allocation of perceptual attention. To support robust cross-modal perception, InCoM further incorporates a geometric-semantic structured alignment mechanism that enhances multimodal correspondence. On the control side, we design a decoupled coordinated flow matching action decoder that explicitly models coordinated base-arm action generation, alleviating optimization difficulties caused by control coupling. Without access to privileged perceptual information, InCoM outperforms state-of-the-art methods on three ManiSkill-HAB scenarios by 28.2%, 26.1%, and 23.6% in success rate, demonstrating strong effectiveness for whole-body mobile manipulation.", "AI": {"tldr": "提出了一种基于意图的感知和结构化协调框架InCoM，用于全身移动操作。", "motivation": "现有的方法在处理基座与手臂动作之间的强耦合以及动态视角变化期间对感知注意力分配不良的问题上面临挑战。因此提出了InCoM来解决这些问题，以实现更有效的全身移动操作。", "method": "通过推断潜在的运动意图并重新加权多尺度感知特征使InCoM能够进行阶段适应性感知关注分配；此外还引入了几何语义结构对齐机制增强了跨模态对应。在控制方面设计了一种解耦协同流匹配动作解码器，缓解了由控制耦合引起的优化困难。", "result": "InCoM在没有特权感知信息的情况下，在三个ManiSkill-HAB场景中分别比最先进技术提高了28.2%，26.1%和23.6%的成功率。", "conclusion": "该研究展示了InCoM框架的强大有效性，可显著提高全身移动操作的性能。"}}
{"id": "2602.23022", "pdf": "https://arxiv.org/pdf/2602.23022", "abs": "https://arxiv.org/abs/2602.23022", "authors": ["Xinglong Luo", "Ao Luo", "Zhengning Wang", "Yueqi Yang", "Chaoyu Feng", "Lei Lei", "Bing Zeng", "Shuaicheng Liu"], "title": "DMAligner: Enhancing Image Alignment via Diffusion Model Based View Synthesis", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2026", "summary": "Image alignment is a fundamental task in computer vision with broad applications. Existing methods predominantly employ optical flow-based image warping. However, this technique is susceptible to common challenges such as occlusions and illumination variations, leading to degraded alignment visual quality and compromised accuracy in downstream tasks. In this paper, we present DMAligner, a diffusion-based framework for image alignment through alignment-oriented view synthesis. DMAligner is crafted to tackle the challenges in image alignment from a new perspective, employing a generation-based solution that showcases strong capabilities and avoids the problems associated with flow-based image warping. Specifically, we propose a Dynamics-aware Diffusion Training approach for learning conditional image generation, synthesizing a novel view for image alignment. This incorporates a Dynamics-aware Mask Producing (DMP) module to adaptively distinguish dynamic foreground regions from static backgrounds, enabling the diffusion model to more effectively handle challenges that classical methods struggle to solve. Furthermore, we develop the Dynamic Scene Image Alignment (DSIA) dataset using Blender, which includes 1,033 indoor and outdoor scenes with over 30K image pairs tailored for image alignment. Extensive experimental results demonstrate the superiority of the proposed approach on DSIA benchmarks, as well as on a series of widely-used video datasets for qualitative comparisons. Our code is available at https://github.com/boomluo02/DMAligner.", "AI": {"tldr": "DMAligner通过基于扩散模型的视图合成来提高图像对齐质量。", "motivation": "现有方法主要依赖光学流技术，但容易受到遮挡和光照变化的影响，导致图像对齐质量和下游任务精度下降。因此提出了一种新的生成式解决方案以解决这些问题。", "method": "提出了基于扩散模型的动态感知训练策略及用于区分前景背景区域的模块，通过合成新视图来改进图像对齐。", "result": "在DSIA数据集和其他视频数据集中表现出色，展示了所提方法的有效性。", "conclusion": "DMAligner为解决图像对齐中的挑战提供了一种新的视角和有效策略。"}}
{"id": "2602.23017", "pdf": "https://arxiv.org/pdf/2602.23017", "abs": "https://arxiv.org/abs/2602.23017", "authors": ["Dean Zadok", "Tom Naamani", "Yuval Bar-Ratson", "Elisha Barash", "Oren Salzman", "Alon Wolf", "Alex M. Bronstein", "Nili Krausz"], "title": "DigiArm: An Anthropomorphic 3D-Printed Prosthetic Hand with Enhanced Dexterity for Typing Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Despite recent advancements, existing prosthetic limbs are unable to replicate the dexterity and intuitive control of the human hand. Current control systems for prosthetic hands are often limited to grasping, and commercial prosthetic hands lack the precision needed for dexterous manipulation or applications that require fine finger motions. Thus, there is a critical need for accessible and replicable prosthetic designs that enable individuals to interact with electronic devices and perform precise finger pressing, such as keyboard typing or piano playing, while preserving current prosthetic capabilities. This paper presents a low-cost, lightweight, 3D-printed robotic prosthetic hand, specifically engineered for enhanced dexterity with electronic devices such as a computer keyboard or piano, as well as general object manipulation. The robotic hand features a mechanism to adjust finger abduction/adduction spacing, a 2-D wrist with the inclusion of controlled ulnar/radial deviation optimized for typing, and control of independent finger pressing. We conducted a study to demonstrate how participants can use the robotic hand to perform keyboard typing and piano playing in real time, with different levels of finger and wrist motion. This supports the notion that our proposed design can allow for the execution of key typing motions more effectively than before, aiming to enhance the functionality of prosthetic hands.", "AI": {"tldr": "设计了一种低成本、轻便的3D打印仿生手，用于提高电子设备操作和精确手指运动的能力。", "motivation": "现有假肢无法复制人类手的灵巧性和直观控制能力，特别是对于需要精细手指动作的任务如键盘打字或钢琴演奏等应用的需求迫切。", "method": "开发了一种能够调整手指张开/闭合间距、具有2D腕关节和可控尺偏桡偏功能，并且可以独立控制每个手指按压的手部装置。进行了参与者使用该假肢进行实时键盘打字和钢琴弹奏的研究。", "result": "研究表明，新的设计有助于更有效地执行键盘打字动作。", "conclusion": "新提出的假手设计方案能够提高现有假肢的功能性，特别是对于需要精细操作的任务。"}}
{"id": "2602.23013", "pdf": "https://arxiv.org/pdf/2602.23013", "abs": "https://arxiv.org/abs/2602.23013", "authors": ["Camile Lendering", "Erkut Akdag", "Egor Bondarev"], "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2026", "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.", "AI": {"tldr": "SubspaceAD是一种无需训练的少样本异常检测方法，通过子空间建模来识别工业检查中的视觉异常。", "motivation": "当前的少样本异常检测方法虽然效果好但通常依赖复杂的辅助机制，作者质疑是否需要这些复杂性，并提出一种简单且高效的解决方案。", "method": "首先使用冻结的DINOv2骨干网络从少量正常图像中提取patch级特征；其次利用主成分分析(PCA)模型拟合这些特征以估计正常的低维子空间；最后在推理阶段通过重构残差检测异常，产生可解释和统计基础的异常评分。", "result": "SubspaceAD在单样本异常检测设置下，在MVTec-AD数据集上达到98.0%的图像级别AUROC和97.6%的像素级AUROC；在VisA数据集上分别达93.3%和98.3%，超越了先前的最佳结果。", "conclusion": "SubspaceAD展示了无需训练、提示调整或记忆库即可达到顶尖性能的能力，为少样本异常检测提供了一种新的简单有效的途径。"}}
{"id": "2602.23010", "pdf": "https://arxiv.org/pdf/2602.23010", "abs": "https://arxiv.org/abs/2602.23010", "authors": ["Gorkem Yildiz"], "title": "HELMLAB: An Analytical, Data-Driven Color Space for Perceptual Distance in UI Design Systems", "categories": ["cs.GR", "cs.CV"], "comment": "9 pages, 6 figures. Code and demo available at: https://github.com/Grkmyldz148/helmlab", "summary": "We present HELMLAB, a 72-parameter analytical color space for UI design systems. The forward transform maps CIE XYZ to a perceptually-organized Lab representation through learned matrices, per-channel power compression, Fourier hue correction, and embedded Helmholtz-Kohlrausch lightness adjustment. A post-pipeline neutral correction guarantees that achromatic colors map to a=b=0 (chroma < 10^-6), and a rigid rotation of the chromatic plane improves hue-angle alignment without affecting the distance metric, which is invariant under isometries. On the COMBVD dataset (3,813 color pairs), HELMLAB achieves a STRESS of 23.22, a 20.4% reduction from CIEDE2000 (29.18). Cross-validation on He et al. 2022 and MacAdam 1974 shows competitive cross-dataset performance. The transform is invertible with round-trip errors below 10^-14. Gamut mapping, design-token export, and dark/light mode adaptation utilities are included for use in web and mobile design systems.", "AI": {"tldr": "本文提出了HELMLAB，一种用于UI设计系统的72参数的分析颜色空间。", "motivation": "目的是通过机器学习和数学转换建立一个更准确、更具感知一致性的颜色系统，以提高用户界面的设计质量。", "method": "HELMLAB通过CIE XYZ到Lab表示的前向变换实现了这一目标。该变换包括了矩阵学习、逐通道幂压缩、傅里叶色调校正以及嵌入的Helmholtz-Kohlrausch亮度调整等步骤，最终保证中性色彩映射至a=b=0，并提高色相角对齐。", "result": "在COMBVD数据集上实现了23.22的STRESS值，比CIEDE2000降低了20.4%。跨数据集验证显示了竞争性的性能表现。逆变换误差低于10^-14，并且提供了色彩映射、设计令牌导出和暗/亮模式适应等实用工具。", "conclusion": "HELMLAB提供了一种新颖的分析颜色空间方法，能够改善用户界面设计系统的感知一致性和实用性。"}}
{"id": "2602.23008", "pdf": "https://arxiv.org/pdf/2602.23008", "abs": "https://arxiv.org/abs/2602.23008", "authors": ["Zeyuan Liu", "Jeonghye Kim", "Xufang Luo", "Dongsheng Li", "Yuqing Yang"], "title": "Exploratory Memory-Augmented LLM Agent via Hybrid On- and Off-Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICLR 2026", "summary": "Exploration remains the key bottleneck for large language model agents trained with reinforcement learning. While prior methods exploit pretrained knowledge, they fail in environments requiring the discovery of novel states. We propose Exploratory Memory-Augmented On- and Off-Policy Optimization (EMPO$^2$), a hybrid RL framework that leverages memory for exploration and combines on- and off-policy updates to make LLMs perform well with memory while also ensuring robustness without it. On ScienceWorld and WebShop, EMPO$^2$ achieves 128.6% and 11.3% improvements over GRPO, respectively. Moreover, in out-of-distribution tests, EMPO$^2$ demonstrates superior adaptability to new tasks, requiring only a few trials with memory and no parameter updates. These results highlight EMPO$^2$ as a promising framework for building more exploratory and generalizable LLM-based agents.", "AI": {"tldr": "该论文提出了EMPO$^2$，一种结合了记忆辅助探索和混合策略优化的框架，用于提升大型语言模型代理在强化学习中的表现。", "motivation": "现有方法虽利用预训练知识，但难以处理需要发现新状态的任务环境。因此，提出了一种新的框架以克服这一瓶颈。", "method": "EMPO$^2$通过记忆辅助探索和混合策略优化相结合的方式提高大型语言模型代理的性能，并保证在无外部记忆支持下也具备鲁棒性。", "result": "在ScienceWorld和WebShop测试中，与GRPO相比分别提高了128.6%和11.3%，且对新任务具有出色的适应性。", "conclusion": "EMPO$^2$框架展示了构建更加探索性和泛化的基于大型语言模型的代理的巨大潜力。"}}
{"id": "2602.23003", "pdf": "https://arxiv.org/pdf/2602.23003", "abs": "https://arxiv.org/abs/2602.23003", "authors": ["René Pallenberg", "Fabrice Katzberg", "Alfred Mertins", "Marco Maass"], "title": "Scattering Transform for Auditory Attention Decoding", "categories": ["eess.SP", "cs.AI", "eess.AS"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The use of hearing aids will increase in the coming years due to demographic change. One open problem that remains to be solved by a new generation of hearing aids is the cocktail party problem. A possible solution is electroencephalography-based auditory attention decoding. This has been the subject of several studies in recent years, which have in common that they use the same preprocessing methods in most cases. In this work, in order to achieve an advantage, the use of a scattering transform is proposed as an alternative to these preprocessing methods. The two-layer scattering transform is compared with a regular filterbank, the synchrosqueezing short-time Fourier transform and the common preprocessing. To demonstrate the performance, the known and the proposed preprocessing methods are compared for different classification tasks on two widely used datasets, provided by the KU Leuven (KUL) and the Technical University of Denmark (DTU). Both established and new neural-network-based models, CNNs, LSTMs, and recent Transformer/graph-based models are used for classification. Various evaluation strategies were compared, with a focus on the task of classifying speakers who are unknown from the training. We show that the two-layer scattering transform can significantly improve the performance for subject-related conditions, especially on the KUL dataset. However, on the DTU dataset, this only applies to some of the models, or when larger amounts of training data are provided, as in 10-fold cross-validation. This suggests that the scattering transform is capable of extracting additional relevant information.", "AI": {"tldr": "利用散射变换提高基于脑电图的听觉注意力解码性能，特别是在未知说话者的分类任务中。", "motivation": "解决下一代助听器面临的鸡尾酒会问题，通过改进预处理方法来提升听力辅助设备的效果。", "method": "对比两层散射变换与常规滤波器组、同步挤压短时傅里叶变换和常见预处理技术，并使用CNNs、LSTMs和Transformer等模型进行分类任务测试。", "result": "在KUL数据集上，两层散射变换显著提高了性能；而在DTU数据集上，则仅对某些模型或大量训练数据情况下有效。", "conclusion": "散射变换能够提取额外的相关信息，对于提高基于脑电图的听觉注意力解码的有效性具有潜在价值。"}}
{"id": "2602.22998", "pdf": "https://arxiv.org/pdf/2602.22998", "abs": "https://arxiv.org/abs/2602.22998", "authors": ["Ryan Paul McKennaa", "John Oyekan"], "title": "A Perspective on Open Challenges in Deformable Object Manipulation", "categories": ["cs.RO"], "comment": "28 pages, 7 Figures", "summary": "Deformable object manipulation (DOM) represents a critical challenge in robotics, with applications spanning healthcare, manufacturing, food processing, and beyond. Unlike rigid objects, deformable objects exhibit infinite dimensionality, dynamic shape changes, and complex interactions with their environment, posing significant hurdles for perception, modeling, and control. This paper reviews the state of the art in DOM, focusing on key challenges such as occlusion handling, task generalization, and scalable, real-time solutions. It highlights advancements in multimodal perception systems, including the integration of multi-camera setups, active vision, and tactile sensing, which collectively address occlusion and improve adaptability in unstructured environments. Cutting-edge developments in physically informed reinforcement learning (RL) and differentiable simulations are explored, showcasing their impact on efficiency, precision, and scalability. The review also emphasizes the potential of simulated expert demonstrations and generative neural networks to standardize task specifications and bridge the simulation-to-reality gap. Finally, future directions are proposed, including the adoption of graph neural networks for high-level decision-making and the creation of comprehensive datasets to enhance DOM's real-world applicability. By addressing these challenges, DOM research can pave the way for versatile robotic systems capable of handling diverse and dynamic tasks with deformable objects.", "AI": {"tldr": "本文综述了可变形物体操作领域的现状和挑战，探讨了感知、建模及控制方面的方法和技术进展。", "motivation": "机器人在处理可变形对象时面临无限维度性、动态形状变化等难题，需要改进感知系统以提高适应性和效率。", "method": "文章介绍了多模式感知系统的集成、强化学习的发展以及生成神经网络的应用来应对挑战。", "result": "通过这些方法和技术的结合使用，可以实现更高效、精准且具有扩展性的解决方案。", "conclusion": "未来的研究方向包括利用图神经网络进行决策制定和构建综合数据集以增强实际应用中的可变形物体操作能力。"}}
{"id": "2602.22993", "pdf": "https://arxiv.org/pdf/2602.22993", "abs": "https://arxiv.org/abs/2602.22993", "authors": ["Tianqi Song", "Black Sun", "Jingshu Li", "Han Li", "Chi-Lan Yang", "Yijia Xu", "Yi-Chieh Lee"], "title": "Understanding Older Adults' Experiences of Support, Concerns, and Risks from Kinship-Role AI-Generated Influencers", "categories": ["cs.HC"], "comment": null, "summary": "AI-generated influencers are rapidly gaining popularity on Chinese short-video platforms, often adopting kinship-based roles such as AI grandchildren to attract older adults. Although this trend has raised public concern, little is known about the design strategies behind these influencers, how older adults experience them, and the benefits and risks involved. In this study, we combined social media analysis with interviews to unpack the above questions. Our findings show that influencers use both visual and conversational cues to enact kinship roles, prompting audiences to engage in kinship-based role-play. Interviews further show that these cues arouse emotional resonance, help fulfill older adults' informational and emotional needs, while also raising concerns about emotional displacement and unequal emotional investment. We highlight the complex relationship between virtual avatars and real family ties, shaped by broader sociocultural norms, and discuss how AI might strengthen social support for older adults while mitigating risks within cultural contexts.", "AI": {"tldr": "研究探讨了老年人与虚拟亲属角色的AI生成影响者的互动体验，包括其设计策略、情感共鸣及潜在风险。", "motivation": "公众对AI生成的影响者如何吸引老年人以及由此带来的利益和风险存在担忧。缺乏对其背后的设计策略及其对老年人的具体影响的研究。", "method": "结合社交媒体分析与访谈来探讨这些问题，揭示了影响者通过视觉和对话提示扮演亲属角色并引发观众的互动反应的方式。", "result": "发现这些提示能够激发情感共鸣，满足老年人的信息及情绪需求，但也引发了有关情感替代和不平等的情感投资的问题。", "conclusion": "研究强调虚拟化身与真实家庭纽带之间的复杂关系，并讨论了在文化背景下如何利用AI增强对老年人的社会支持并减轻潜在风险。"}}
{"id": "2602.22988", "pdf": "https://arxiv.org/pdf/2602.22988", "abs": "https://arxiv.org/abs/2602.22988", "authors": ["Bum Jun Kim", "Shohei Taniguchi", "Makoto Kawano", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages, 7 figures", "summary": "Training divergence in transformers wastes compute, yet practitioners discover instability only after expensive runs begin. They therefore need an expected probability of failure for a transformer before training starts. Our study of Residual Koopman Spectral Profiling (RKSP) provides such an estimate. From a single forward pass at initialization, RKSP extracts Koopman spectral features by applying whitened dynamic mode decomposition to layer-wise residual snapshots. Our central diagnostic, the near-unit spectral mass, quantifies the fraction of modes concentrated near the unit circle, which captures instability risk. For predicting divergence across extensive configurations, this estimator achieves an AUROC of 0.995, outperforming the best gradient baseline. We further make this diagnostic actionable through Koopman Spectral Shaping (KSS), which reshapes spectra during training. We empirically validate that our method works in practice: RKSP predicts divergence at initialization, and when RKSP flags high risk, turning on KSS successfully prevents divergence. In the challenging high learning rate regime without normalization layers, KSS reduces the divergence rate from 66.7% to 12.5% and enables learning rates that are 50% to 150% higher. These findings generalize to WikiText-103 language modeling, vision transformers on CIFAR-10, and pretrained language models, including GPT-2 and LLaMA-2 up to 7B, as well as emerging architectures such as MoE, Mamba-style SSMs, and KAN.", "AI": {"tldr": "通过残差Koopman谱分析预测和防止变换器训练过程中的不稳定性。", "motivation": "在变换器模型的训练过程中，由于训练发散导致计算资源浪费的问题普遍存在。研究人员需要一种方法来评估潜在的风险，在训练开始之前能够预判失败的概率。", "method": "通过单次初始化前向传递应用残差Koopman谱分析（RKSP），使用白化动态模式分解层状残差快照提取Koopman谱特征，用接近单位的光谱质量量化不稳定性风险。此外，提出了Koopman谱整形（KSS）方法在训练过程中调整谱来防止发散。", "result": "该模型对于预测各种配置下的发散具有高度准确性，AUROC达到0.995。实验证明，在高学习率无归一化层的情况下，使用KSS能够将发散率从66.7%降低至12.5%，并且可以提高50%-150%的学习速率。", "conclusion": "通过RKSP和KSS方法，可以在训练之前准确预测变换器模型的不稳定性风险，并在高危情况下有效防止发散。这些发现适用于各种任务包括语言建模、视觉变换器及预训练的语言模型等。"}}
{"id": "2602.22983", "pdf": "https://arxiv.org/pdf/2602.22983", "abs": "https://arxiv.org/abs/2602.22983", "authors": ["Xun Huang", "Simeng Qin", "Xiaoshuang Jia", "Ranjie Duan", "Huanqian Yan", "Zhitao Zeng", "Fei Yang", "Yang Liu", "Xiaojun Jia"], "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search", "categories": ["cs.AI", "cs.CR"], "comment": "ef:ICLR 2026 Poster", "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.", "AI": {"tldr": "本文提出了一种基于多维果蝇优化的框架CC-BOS，用于生成古典中文对抗性提示词，以实现对大型语言模型的有效黑盒破解。", "motivation": "随着大型语言模型的安全风险日益增加，研究者发现这些模型在不同语言背景下容易遭受破解攻击。本文探讨了古典中文在此类攻击中的角色，并提出了一种新的框架来增强攻击效率和自动化程度。", "method": "CC-BOS框架通过将提示词编码为八个策略维度（包括角色、行为等）并在黑盒环境下进行基于嗅觉搜索、视觉搜索和柯西变异的迭代优化，实现了高效探索搜索空间的目的。此外还设计了古典中文到英文翻译模块以提高可读性和评估准确性。", "result": "实验结果表明，所提出的CC-BOS框架在各种攻击方法中表现出色，并且能够持续超越现有的最先进的破解技术。", "conclusion": "本文通过创新地使用古典中文和生物启发式搜索策略显著提升了黑盒环境下大型语言模型的破解效率。"}}
{"id": "2602.22981", "pdf": "https://arxiv.org/pdf/2602.22981", "abs": "https://arxiv.org/abs/2602.22981", "authors": ["Haohui Jia", "Zheng Chen", "Lingwei Zhu", "Xu Cao", "Yasuko Matsubara", "Takashi Matsubara", "Yasushi Sakurai"], "title": "RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs", "categories": ["cs.AI"], "comment": null, "summary": "Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.", "AI": {"tldr": "提出RepSPD模型，通过动态图改善EEG信号的SPD流形表示。", "motivation": "当前基于SPD的方法主要集中在统计聚合上，忽略了频率特异性同步和局部拓扑结构。为此，该研究旨在提升几何一致性以改进脑活动解码。", "method": "RepSPD模型采用跨注意力机制调制Riemann流形上的SPD属性，并引入双向全局对齐策略重塑切空间嵌入，减少几何失真。", "result": "实验结果显示，所提框架显著优于现有EEG表示方法，在稳健性和泛化能力方面表现出色。", "conclusion": "RepSPD模型通过改进SPD流形表示，提升了EEG信号的解码性能和鲁棒性。"}}
{"id": "2602.22980", "pdf": "https://arxiv.org/pdf/2602.22980", "abs": "https://arxiv.org/abs/2602.22980", "authors": ["Karl Bartolo", "Peter Borg", "Magda Dettlaff", "Magdalena Lemańska", "Paweł Żyliński"], "title": "Isolation critical graphs under multiple edge subdivision", "categories": ["math.CO", "cs.DM", "cs.DS"], "comment": "15 pages", "summary": "This paper introduces the notion of $(ι,q)$-critical graphs. The isolation number of a graph $G$, denoted by $ι(G)$ and also known as the vertex-edge domination number, is the minimum number of vertices in a set $D$ such that the subgraph induced by the vertices not in the closed neighbourhood of $D$ has no edges. A graph $G$ is $(ι,q)$-critical, $q \\ge 1$, if the subdivision of any $q$ edges in $G$ gives a graph with isolation number greater than $ι(G)$ and there exists a set of $q-1$ edges such that subdividing them gives a graph with isolation number equal to $ι(G)$. We prove that for each integer $q \\ge 1$ there exists a $(ι,q)$-critical graph, while for a given graph $G$, the admissible values of $q$ satisfy $1 \\le q \\le |E(G)| - 1$. In addition, we provide a general characterisation of $(ι,1)$-critical graphs as well as a constructive characterisation of $(ι,1)$-critical trees.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2602.22976", "pdf": "https://arxiv.org/pdf/2602.22976", "abs": "https://arxiv.org/abs/2602.22976", "authors": ["Henrik Reinstädtler", "Christian Schulz", "Nodari Sitchinava", "Fabian Walliser"], "title": "Efficient Parallel Algorithms for Hypergraph Matching", "categories": ["cs.DS"], "comment": null, "summary": "We present efficient parallel algorithms for computing maximal matchings in hypergraphs. Our algorithm finds locally maximal edges in the hypergraph and adds them in parallel to the matching. In the CRCW PRAM models our algorithms achieve $O(\\log{m})$ time with $O((κ+ n) \\log {m})$ work w.h.p. where $m$ is the number of hyperedges, and $κ$ is the sum of all vertex degrees. The CREW PRAM model algorithm has a running time of $O((\\logΔ+\\log{d})\\log{m})$ and requires $O((κ+ n) \\log {m})$ work w.h.p. It can be implemented work-optimal with $O(κ+n)$ work in $O((\\log{m}+\\log{n})\\log{m})$ time. We prove a $1/d$-approximation guarantee for our algorithms. We evaluate our algorithms experimentally by implementing and running the proposed algorithms on the GPU using CUDA and Kokkos. Our experimental evaluation demonstrates the practical efficiency of our approach on real-world hypergraph instances, yielding a speed up of up to 76 times compared to a single-core CPU algorithm.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
