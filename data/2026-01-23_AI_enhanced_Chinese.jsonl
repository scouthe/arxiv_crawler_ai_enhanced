{"id": "2601.16216", "pdf": "https://arxiv.org/pdf/2601.16216", "abs": "https://arxiv.org/abs/2601.16216", "authors": ["Clémentine Sacré"], "title": "Scalable Board Expansion within a General Game System", "categories": ["cs.AI", "cs.GT", "cs.SE"], "comment": "65 pages, 41 figures", "summary": "This thesis explores the use of a General Game System (GGS) to support the automatic expansion of game boards in boardless games. Traditional implementations of such games often rely on oversized static boards defined from the start, even though large portions of these boards may never be used during gameplay. This approach leads to unnecessary complexity. To address this issue, this thesis propose a dynamic board expansion mechanism in which the game board grows automatically during play.", "AI": {"tldr": "论文探讨了在通用游戏系统中实现自动扩展棋盘的方法，以减少传统静态大棋盘带来的不必要的复杂性。", "motivation": "传统的棋盘游戏往往使用从一开始就定义好的巨大静态棋盘，这导致了不必要的复杂性和资源浪费。", "method": "提出了一种动态的棋盘扩张机制，在游戏中根据需要自动扩展棋盘大小。", "result": "实现了更灵活和高效的棋盘管理方式，减少了未使用的空间和复杂性。", "conclusion": "动态棋盘扩张机制能够在不影响游戏体验的情况下有效减少资源浪费，并提高了系统的可扩展性和灵活性。"}}
{"id": "2601.16214", "pdf": "https://arxiv.org/pdf/2601.16214", "abs": "https://arxiv.org/abs/2601.16214", "authors": ["Wenhang Ge", "Guibao Shen", "Jiawei Feng", "Luozhou Wang", "Hao Lu", "Xingye Tian", "Xin Tao", "Ying-Cong Chen"], "title": "CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \\href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.", "AI": {"tldr": "本文提出了一种高效的相机感知3D解码器，用于改进视频扩散模型中的相机控制。", "motivation": "尽管相机控制的视频扩散模型已经有所进步，但仍然存在挑战，如现有奖励反馈学习方法在评估视像-摄像机对齐、计算奖励时效率低下等问题。为了解决这些问题并改善摄像机可控性，作者提出了新的解决方案。", "method": "引入了一种高效的3D解码器，将视频潜在变量与相机姿态解码成3D高斯分布，并通过优化渲染的新视图和真实视图之间的像素级一致性作为奖励进行训练。同时引入了可见性项来监督确定区域。", "result": "在RealEstate10K和WorldScore基准上的广泛实验表明，所提出的方法是有效的。", "conclusion": "本文提出的高效相机感知3D解码器可以显著提高视频扩散模型中的摄像机控制能力。"}}
{"id": "2601.16212", "pdf": "https://arxiv.org/pdf/2601.16212", "abs": "https://arxiv.org/abs/2601.16212", "authors": ["Siddhant Haldar", "Lars Johannsmeier", "Lerrel Pinto", "Abhishek Gupta", "Dieter Fox", "Yashraj Narang", "Ajay Mandlekar"], "title": "Point Bridge: 3D Representations for Cross Domain Policy Learning", "categories": ["cs.RO"], "comment": null, "summary": "Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/", "AI": {"tldr": "本文介绍了Point Bridge框架，该框架通过使用统一的、领域无关的点基表示法来实现从合成数据到现实世界的零样本策略迁移。", "motivation": "尽管机器人基础模型已经开始兑现其作为通用机器人的承诺，但缺乏大规模的真实世界操作数据集限制了进步。仿真和合成数据生成提供了一种可扩展的选择，但由于模拟与现实之间存在视觉领域差距，这种选择的效果受到限制。", "method": "Point Bridge框架结合了通过Vision-Language Models（VLMs）自动提取点基表示、基于变压器的策略学习以及高效的推理时间管道，在仅使用合成数据的情况下训练能够执行真实世界操作任务的机器人代理。", "result": "在零样本模拟到现实世界的迁移中，该方法获得了高达44%的性能提升；与有限的真实数据进行联合训练后，其表现进一步提高，最高可达66%，这超越了之前的视觉基础仿真和现实共训方法。", "conclusion": "Point Bridge框架利用点基表示法克服了模拟与现实之间的视觉领域差距，并且通过高效的策略学习管道实现了从合成数据到现实世界的零样本迁移。"}}
{"id": "2601.16211", "pdf": "https://arxiv.org/pdf/2601.16211", "abs": "https://arxiv.org/abs/2601.16211", "authors": ["Geo Ahn", "Inwoong Lee", "Taeoh Kim", "Minho Shim", "Dongyoon Wee", "Jinwoo Choi"], "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "The code is available at https://github.com/KHU-VLL/RCORE", "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.", "AI": {"tldr": "本文研究了零样本组合动作识别中的对象驱动的动作捷径问题，并提出了一种名为RCORE的框架来解决这个问题。", "motivation": "现有的零样本组合动作识别模型由于对象驱动的动作捷径而失败。这种行为源于组成监督的严重稀疏性和偏差，以及动词和名词之间学习难度的不对称性。", "method": "本文提出了RCORE框架，包括一种能够多样化动词-名词组合而不破坏运动线索的组成感知增强技术，以及一个惩罚捷径行为的时序顺序正则化损失。", "result": "在两个基准测试Sth-com和EK100-com上，RCORE显著提高了未见过动作组合的识别精度，减少了对共现偏差的依赖，并实现了持续的正向组成差距。", "conclusion": "研究揭示了对象驱动的动作捷径是零样本组合动作识别中的一个关键限制因素，解决这一问题对于实现稳健的组合视频理解至关重要。"}}
{"id": "2601.16210", "pdf": "https://arxiv.org/pdf/2601.16210", "abs": "https://arxiv.org/abs/2601.16210", "authors": ["Onkar Susladkar", "Tushar Prakash", "Adheesh Juvekar", "Kiet A. Nguyen", "Dong-Hwan Jang", "Inderjit S Dhillon", "Ismini Lourentzou"], "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.", "AI": {"tldr": "本文提出了PyraTok，一种语言对齐的金字塔形编解码器，用于视频理解和生成。", "motivation": "现有的视频编码器通常在单一尺度上学习视觉代码本，具有词汇量有限和浅层的语言监督，导致跨模态不对齐和零样本迁移性能差。", "method": "PyraTok基于预训练的视频变分自编码器（VAE）和一个新的语言对齐金字塔量化模块（LaPQ），在多个时空分辨率上学习语义结构化的离散潜变量。通过多尺度文本引导的量化和全局自回归目标优化，紧密耦合视觉标记与语言。", "result": "PyraTok在十个基准测试中提供最先进的视频重建、提升文本到视频的质量，并且在视频分割、时间动作定位和视频理解上的零样本性能达到新高，能够稳健地处理高达4K/8K分辨率的视频。", "conclusion": "研究表明，通过学习多尺度的离散标记序列并将其与语言紧密耦合，PyraTok能够显著改善视频理解和生成系统的跨模态对齐和零样本迁移能力。"}}
{"id": "2601.16208", "pdf": "https://arxiv.org/pdf/2601.16208", "abs": "https://arxiv.org/abs/2601.16208", "authors": ["Shengbang Tong", "Boyang Zheng", "Ziteng Wang", "Bingda Tang", "Nanye Ma", "Ellis Brown", "Jihan Yang", "Rob Fergus", "Yann LeCun", "Saining Xie"], "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "categories": ["cs.CV"], "comment": "website: https://rae-dit.github.io/scale-rae/", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "AI": {"tldr": "该论文研究了使用表示自动编码器（RAE）扩展大规模自由文本到图像生成的可行性。", "motivation": "动机在于探索在更大规模和更复杂的文本到图像生成任务中，表示自动编码器框架是否能像在ImageNet数据集上一样有效，并通过简化的设计来提高效率和性能。", "method": "该论文首先扩展了RAE解码器的训练范围，超出了冻结表示编码器SigLIP-2的数据集，包括网络、合成以及文本渲染数据。然后对原为ImageNet设计的RAE架构选择进行了严格的测试，并通过对比实验评估了不同参数规模下的性能。", "result": "研究发现，随着模型规模扩大，简化后的框架表现更为出色；在预训练阶段，RAEs始终优于VAEs，且在精调时更稳定、过拟合现象较少。最终表明，在大规模文本到图像生成任务中，RAE是比VAE更好的基础。", "conclusion": "表示自动编码器（RAE）作为一种简化而强大的框架，适用于大规模的自由文本到图像生成任务，并表现出优于变分自编码器（VAE）的性能和稳定性。"}}
{"id": "2601.16207", "pdf": "https://arxiv.org/pdf/2601.16207", "abs": "https://arxiv.org/abs/2601.16207", "authors": ["Jongwoo Park", "Kanchana Ranasinghe", "Jinhyeok Jang", "Cristina Mata", "Yoo Sung Jang", "Michael S Ryoo"], "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance", "categories": ["cs.RO"], "comment": null, "summary": "Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA", "AI": {"tldr": "IVRA是一种轻量级、无需训练的方法，通过利用模型内置的视觉编码器中的亲和力提示来改进空间理解。", "motivation": "许多Vision-Language-Action (VLA) 模型将图像块扁平化为1D令牌序列，削弱了所需的2D空间线索以实现精确操作。IVRA旨在改善这种状况，无需额外的外部编码器或重新训练。", "method": "IVRA在推理时选择性地向语言模型层中注入这些亲和力信号，该层包含实例级特征，从而重新调整视觉令牌之间的互动，并保留几何结构。", "result": "IVRA提高了不同VLA架构在2D和3D操作基准上的性能，在低数据条件下，它将LLaRA的平均成功率提升了+4.2%，并在接近饱和的基线准确率（96.3%至97.1%）上取得了稳定增益。", "conclusion": "IVRA证明了其在改善机器人行动策略方面的能力，尤其是在不需要重新训练的情况下增强了空间理解能力。"}}
{"id": "2601.16206", "pdf": "https://arxiv.org/pdf/2601.16206", "abs": "https://arxiv.org/abs/2601.16206", "authors": ["Daixuan Cheng", "Shaohan Huang", "Yuxian Gu", "Huatong Song", "Guoxin Chen", "Li Dong", "Wayne Xin Zhao", "Ji-Rong Wen", "Furu Wei"], "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://llm-in-sandbox.github.io", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "AI": {"tldr": "本文介绍了LLM-in-Sandbox，一种使语言模型在代码沙箱中探索并激发非编码领域中的普遍智能的方法。", "motivation": "动机是利用现有的强语言模型，在不进行额外训练的情况下展示它们通过代码沙箱实现通用能力的能力，并进一步增强这种代理能力。", "method": "方法包括使用代码沙箱使语言模型自发访问外部资源、处理长时间上下文并执行脚本。此外，还介绍了LLM-in-Sandbox-RL，用非代理数据来训练沙箱探索。", "result": "实验显示，在无训练和后训练设置中，LLM-in-Sandbox在数学、物理、化学、生物医学、长时上下文理解和指令遵循方面实现了稳健的泛化能力。", "conclusion": "结论是，LLM-in-Sandbox能有效提升语言模型的一般智能，并且其效率从计算和系统角度进行了分析。此外，该方法作为Python包开源，便于实际部署。"}}
{"id": "2601.16205", "pdf": "https://arxiv.org/pdf/2601.16205", "abs": "https://arxiv.org/abs/2601.16205", "authors": ["Patrick Altmeyer", "Aleksander Buszydlik", "Arie van Deursen", "Cynthia C. S. Liem"], "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations", "categories": ["cs.LG", "cs.AI"], "comment": "This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore", "summary": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.", "AI": {"tldr": "本文提出了一种新型训练方法——反事实训练，利用反事实解释来提高模型的解释能力。", "motivation": "现有的研究主要集中在开发事后的方法生成满足可操作性和现实性的反事实解释，但这些方法并未直接在模型训练中考虑。因此，作者希望通过反事实训练使模型直接负责产生理想的反事实解释。", "method": "反事实训练使用反事实解释作为训练阶段的一部分，以最小化学习表示与合理、可操作的解释之间的差异。", "result": "实验和理论分析表明，该方法能够帮助训练出既能提供理想的反事实解释又能提高模型对抗鲁棒性的模型。", "conclusion": "反事实训练可以有效地在训练过程中考虑模型的可解释性和对抗鲁棒性，从而提升模型的整体性能。"}}
{"id": "2601.16200", "pdf": "https://arxiv.org/pdf/2601.16200", "abs": "https://arxiv.org/abs/2601.16200", "authors": ["Song Xia", "Meiwen Ding", "Chenqi Kong", "Wenhan Yang", "Xudong Jiang"], "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing", "categories": ["cs.LG", "cs.CV"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\\% to about 1\\%.", "AI": {"tldr": "本文提出了一种名为特征空间平滑（FS）的技术，为多模态大型语言模型提供可证明的鲁棒性保证，并引入了Purifier和Smoothness Mapper (PSM)模块以提高其性能。", "motivation": "旨在解决多模态大型语言模型在面对对抗扰动时易受攻击的问题，这些扰动会扭曲特征表示并导致错误预测。", "method": "提出Feature-space Smoothing（FS）技术，并理论上证明了它对MLLMs的特性表征提供了可验证的鲁棒性保证。同时引入Purifier和Smoothness Mapper (PSM)模块来提高模型的高斯稳健得分，增强其在FS下的认证鲁棒性。", "result": "实验表明，在不重新训练的情况下，FS与PSM组合不仅提供强大的理论鲁棒性保障，还在多种多模态语言模型和下游任务中表现出优越的实际性能，将各种白盒攻击的成功率从近90%降低至约1%。", "conclusion": "证明了特征空间平滑技术及其相关模块的有效性和实用性，显著提高了多模态大型语言模型对对抗扰动的鲁棒性。"}}
{"id": "2601.16192", "pdf": "https://arxiv.org/pdf/2601.16192", "abs": "https://arxiv.org/abs/2601.16192", "authors": ["Ziyi Wu", "Daniel Watson", "Andrea Tagliasacchi", "David J. Fleet", "Marcus A. Brubaker", "Saurabh Saxena"], "title": "360Anything: Geometry-Free Lifting of Images and Videos to 360°", "categories": ["cs.CV"], "comment": "Project page: https://360anything.github.io/", "summary": "Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.", "AI": {"tldr": "将透视图像和视频提升到360°全景，无需几何校准。", "motivation": "现有方法依赖于显式的几何对齐，需要精确的相机元数据，这限制了其在野外数据的应用。因此提出了一个无需几何信息的方法来生成360度全景图。", "method": "提出了一种基于预训练扩散变换器的框架（360Anything），通过将透视输入和全景目标视为令牌序列，以纯数据驱动的方式学习透视到球面等距投影的空间映射。", "result": "该方法在图像和视频透视到360度生成任务上达到最先进水平，并解决了ERP边界的缝合问题，同时展示了零样本相机视场角和方位估计的竞争力。", "conclusion": "360Anything证明了其在无需几何信息的情况下实现高质量全景图生成的能力，且显示出对计算机视觉更广泛任务的适用性。"}}
{"id": "2601.16182", "pdf": "https://arxiv.org/pdf/2601.16182", "abs": "https://arxiv.org/abs/2601.16182", "authors": ["Arshia Ataee Naeini", "Amir-Parsa Mobed", "Masoud Seddighin", "Saeed Seddighin"], "title": "Dynamic Pattern Matching with Wildcards", "categories": ["cs.DS"], "comment": null, "summary": "We study the fully dynamic pattern matching problem where the pattern may contain up to kwildcard symbols, each matching any symbol of the alphabet. Both the text and the pattern are subject to updates (insert, delete, change). We design an algorithm with O(nlog^2 n) preprocessing and update/query time O(knk/k+1 + k2 log n). The bound is truly sublinear for a constant k, and sublinear when k= o(log n). We further complement our results with a conditional lower bound: assuming subquadratic preprocessing time, achieving truly sublinear update time for the case k = Ω(log n) would contradict the Strong Exponential Time Hypothesis (SETH). Finally, we develop sublinear algorithms for two special cases: - If the pattern contains w non-wildcard symbols, we give an algorithm with preprocessing time O(nw) and update time O(w + log n), which is truly sublinear whenever wis truly sublinear. - Using FFT technique combined with block decomposition, we design a deterministic truly sublinear algorithm with preprocessing time O(n^1.8) and update time O(n^0.8 log n) for the case that there are at most two non-wildcards.", "AI": {"tldr": "本文研究了包含通配符的动态模式匹配问题，提出了一种具有O(nlog^2 n)预处理和更新/查询时间复杂度为O(knk/k+1 + k2 log n)的算法。", "motivation": "动机在于解决文本和模式都可能发生变化（插入、删除或修改）的情况下的通配符动态模式匹配问题，特别是当通配符数量k是常数或对数级别时，寻求真正次线性的更新时间复杂度。", "method": "提出了一种具有O(nlog^2 n)预处理时间和O(knk/k+1 + k2 log n)的更新/查询时间算法，并探讨了两种特殊情况下的子线性算法：一种是基于模式中非通配符的数量w，另一种结合FFT技术与块分解实现。", "result": "该研究得到了对于k为常数或对数级别的真正次线性更新时间复杂度结果。并且，在假设预处理时间为次二次级别时，达到真正的子线性更新时间的算法将与强指数时间假说矛盾。", "conclusion": "结论是提出了有效解决包含通配符的动态模式匹配问题的方法，并在特定条件下提供了真正次线性的解决方案，同时也指出了当k=Ω(log n)时获得真次线性更新时间复杂度可能存在的理论限制。"}}
{"id": "2601.16175", "pdf": "https://arxiv.org/pdf/2601.16175", "abs": "https://arxiv.org/abs/2601.16175", "authors": ["Mert Yuksekgonul", "Daniel Koceja", "Xinhao Li", "Federico Bianchi", "Jed McCaleb", "Xiaolong Wang", "Jan Kautz", "Yejin Choi", "James Zou", "Carlos Guestrin", "Yu Sun"], "title": "Learning to Discover at Test Time", "categories": ["cs.LG", "cs.AI"], "comment": "Code: https://github.com/test-time-training/discover", "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "AI": {"tldr": "通过在测试阶段使用强化学习，该论文提出了一种名为TTT-Discover的方法来发现科学问题的新解决方案。", "motivation": "旨在探索如何利用人工智能在特定的测试问题上继续训练以寻找新的最佳解决方案，而非依赖固定的大型语言模型进行搜索。", "method": "采用测试时间持续学习方法（TTT-Discover），通过强化学习在测试时对连续奖励的问题进行优先排序和优化。", "result": "该方法在多个领域如数学、GPU内核工程、算法设计和生物学中取得了新的最佳结果，所有实验均使用开源模型OpenAI gpt-oss-120b，并提供了可公开复制的代码。", "conclusion": "通过测试时间训练发现新解决方案的方法（TTT-Discover）在多个连续奖励问题上展现了优越性，且成本低、易于复现。"}}
{"id": "2601.16172", "pdf": "https://arxiv.org/pdf/2601.16172", "abs": "https://arxiv.org/abs/2601.16172", "authors": ["Zachary Burton"], "title": "Structured Hints for Sample-Efficient Lean Theorem Proving", "categories": ["cs.AI"], "comment": "9 pages, 1 figure", "summary": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.", "AI": {"tldr": "本文探讨了在推理阶段通过提供简单的结构化指导是否能进一步提升已高度训练的神经定理证明器的表现。", "motivation": "作者希望通过研究发现，即使对于已经过精细训练的模型，在推理时给予简单结构化的提示也能带来性能上的提升。", "method": "使用固定提示计划（包含15种常见策略骨架）在miniF2F基准测试上进行评估。", "result": "实验结果显示，与标准采样相比，使用该方法得到了43%的相对改善，通过率从15.2%提高到了21.7%，同时保持相同样本数量（k=16）和最大生成长度（1024个令牌）。", "conclusion": "即使对于通过深度强化学习训练出的高度复杂的模型而言，在推理阶段提供简单的结构化指导仍然可以带来显著的性能提升，且成本低廉。"}}
{"id": "2601.16169", "pdf": "https://arxiv.org/pdf/2601.16169", "abs": "https://arxiv.org/abs/2601.16169", "authors": ["Robert Walkup", "Juha Jäykkä", "Igor Pasichnyk", "Zachary Streeter", "Kasia Świrydowicz", "Mikko Tukiainen", "Yasuko Eckert", "Luke Bertels", "Daniel Claudino", "Peter Groszkowski", "Travis S. Humble", "Constantinos Evangelinos", "Javier Robledo-Moreno", "William Kirby", "Antonio Mezzacapo", "Antonio Córcoles", "Seetharami Seelam"], "title": "Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload", "categories": ["cs.ET", "cs.DC"], "comment": "12 pages", "summary": "Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work. GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.", "AI": {"tldr": "本论文主要任务是在GPU加速系统上使用OpenMP offload实现基于样本的量子对角化的高效可扩展和便携式计算。", "motivation": "动机是提高经典计算机在混合量子-HPC算法中执行复杂对角化任务的效率，尤其是在处理大规模电子结构配置时。", "method": "本论文采用了GPU作为主要计算引擎，并使用了Davidson算法来计算基态能量和波函数。通过OpenMP offload实现了代码转换、数据移动策略等优化。", "result": "实验结果表明，在Frontier超级计算机和其他五个GPU加速系统上，与CPU相比，GPU在节点级性能提升了约100倍。", "conclusion": "结论是使用GPU进行对角化计算可以显著提高效率，将经典处理时间从小时减少到分钟，这对于提取基态和激发态能量至关重要。"}}
{"id": "2601.16163", "pdf": "https://arxiv.org/pdf/2601.16163", "abs": "https://arxiv.org/abs/2601.16163", "authors": ["Moo Jin Kim", "Yihuai Gao", "Tsung-Yi Lin", "Yen-Chen Lin", "Yunhao Ge", "Grace Lam", "Percy Liang", "Shuran Song", "Ming-Yu Liu", "Chelsea Finn", "Jinwei Gu"], "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/", "AI": {"tldr": "介绍Cosmos Policy，通过单阶段后训练将预训练视频模型转化为有效机器人策略的方法。", "motivation": "现有方法利用视频生成模型进行政策学习时引入了复杂性，需要多阶段后续训练和新的架构组件。本文旨在简化这一过程，并提高性能。", "method": "Cosmos Policy使用一个预训练的视频模型（Cosmos-Predict2），通过单阶段后训练直接从机器人演示数据中生成动作，无需修改架构。同时，它还能生成未来状态图像和值，用于测试时的动作轨迹规划。", "result": "在LIBERO和RoboCasa仿真基准测试中的成功率分别达到98.5%和67.1%，并在现实世界双臂操作任务中表现最佳，超越了从头训练的扩散策略、视频模型基于政策和其他最先进的视觉语言动作模型。", "conclusion": "Cosmos Policy展示了通过简化的方法实现高效率机器人行为学习的能力，并证明在复杂任务中通过模型预测和价值函数的学习能进一步提高成功率。"}}
{"id": "2601.16158", "pdf": "https://arxiv.org/pdf/2601.16158", "abs": "https://arxiv.org/abs/2601.16158", "authors": ["Prakash Dhungana", "Sayed Ahmad Salehi"], "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems", "categories": ["cs.SD", "cs.LG"], "comment": "12 pages, 8 figures, and 3 tables", "summary": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\\% accuracy on clean data and maintaining robust performance (exceeding 94\\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.", "AI": {"tldr": "本文提出了一种针对资源受限系统的域增量持续学习框架，用于提高关键词识别（KWS）的准确性和鲁棒性。", "motivation": "由于噪声和录音条件的变化导致领域迁移，小型模型在边缘设备上的KWS系统面临准确性与鲁棒性的挑战，因此提出了这个新的解决方案。", "method": "该框架包括使用MFCC和Mel谱图特征的双输入卷积神经网络，并结合多阶段去噪过程（离散小波变换和频谱减法技术），以及模型和原型更新模块。通过基于类原型和置信度驱动过滤选择样本，然后进行伪标记并与回放缓冲区一起用于增量模型重新训练。", "result": "实验结果表明，在干净数据上的准确率为99.63%，在各种噪声环境下（即使在-10 dB的信噪比下）仍能保持超过94%的准确性，展示了框架的有效性。", "conclusion": "将高效的去噪与基于原型的持续学习相结合，使KWS模型能够在资源受限且动态变化的环境中自主、稳健地运行。"}}
{"id": "2601.16156", "pdf": "https://arxiv.org/pdf/2601.16156", "abs": "https://arxiv.org/abs/2601.16156", "authors": ["Artem Kaznatcheev", "Willemijn Volgering"], "title": "All ascents exponential from valued constraint graphs of pathwidth three", "categories": ["cs.DM", "cs.DS"], "comment": "18 pages, 4 figures", "summary": "Many combinatorial optimization problems can be formulated as finding as assignment that maximized some pseudo-Boolean function (that we call the fitness function). Strict local search starts with some assignment and follows some update rule to proceed to an adjacent assignment of strictly higher fitness. This means that strict local search algorithms follow ascents in the fitness landscape of the pseudo-Boolean function. The complexity of the pseudo-Boolean function (and the fitness landscapes that it represents) can be parameterized by properties of the valued constraint satisfaction problem (VCSP) that encodes the pseudo-Boolean function. We focus on properties of the constraint graphs of the VCSP, with the intuition that spare graphs are less complex than dense ones. Specifically, we argue that pathwidth is the natural sparsity parameter for understanding limits on the power of strict local search. We show that prior constructions of sparse VCSPs where all ascents are exponentially long had pathwidth greater than or equal to four. We improve this this with our controlled doubling construction: a valued constraint satisfaction problem of pathwidth three where all ascents are exponentially long from a designated initial assignment. From this, we conclude that all strict local search algorithms can be forced to take an exponential number of steps even on simple valued constraint graphs of pathwidth three.", "AI": {"tldr": "本文通过构造一个路径宽度为三的VCSP实例，展示了所有上升路径在初始分配点都是指数长度的情况，从而说明严格的局部搜索算法可能需要指数时间来解决此类问题。", "motivation": "动机在于探讨约束图稀疏性对严格局部搜索算法复杂度的影响，特别关注路径宽度作为衡量稀疏性的参数。", "method": "提出了控制加倍构造方法，构建一个路径宽度为三的VCSP实例，其中从指定初始分配开始的所有上升路径都是指数长度。", "result": "证明了在路径宽度为三的情况下，所有严格的局部搜索算法都可能需要采取指数步骤才能完成优化过程。", "conclusion": "即使是在简单的路径宽度为三的值约束图上，所有的严格局部搜索算法也可能被迫执行指数数量的步骤。"}}
{"id": "2601.16155", "pdf": "https://arxiv.org/pdf/2601.16155", "abs": "https://arxiv.org/abs/2601.16155", "authors": ["Zequn Xie", "Xin Liu", "Boyun Zhang", "Yuxiao Lin", "Sihang Cai", "Tao Jin"], "title": "HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": "Accepted by ICASSP 2026", "summary": "The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from \"blind\" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.", "AI": {"tldr": "该论文提出了HVD模型，通过模仿人类视觉行为来改进文本视频检索任务中的特征交互问题。", "motivation": "当前方法在文本-视频检索中存在“盲”特征交互的问题，即模型难以从背景噪声中辨别出关键的视觉信息。为了弥补这一差距，作者借鉴了人类的认知行为提出了HVD模型。", "method": "该框架包含两个主要组件：帧特征选择模块（FFSM）和补丁特征压缩模块（PFCM）。FFSM通过模仿人类宏观感知能力来选择关键帧以消除时间冗余；PFCM则模拟微观感知，通过先进的注意力机制将补丁特征聚合为显眼的视觉实体。", "result": "在五个基准上的广泛实验表明，HVD模型不仅能够捕捉类似人类的视觉焦点，并且达到了最先进的性能水平。", "conclusion": "HVD模型的有效性和优越性得到验证，它模仿了人类视觉行为，在文本视频检索任务中实现了显著的改进。"}}
{"id": "2601.16152", "pdf": "https://arxiv.org/pdf/2601.16152", "abs": "https://arxiv.org/abs/2601.16152", "authors": ["Denise M. Case"], "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates", "categories": ["cs.LO", "cs.AI"], "comment": "29 pages", "summary": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.", "AI": {"tldr": "本文探讨了在持续存在的法律、政治和分析分歧条件下，保持中立知识表示稳定性的结构约束。", "motivation": "随着现代数据系统面临越来越多的分歧情况，无法依赖共享解释或中心化权威来保证互操作性。因此需要研究一种能够在不兼容扩展下仍能保持稳定引用的中立知识表示方法。", "method": "基于中立框架，该论文探讨了在问责关系可引用且可比较的要求下所需的最简本体结构，并确保减少不会重新引入稳定性关键区分。", "result": "本文证明了一个条件下的最小值结果：任何能够支持在持续分歧条件下问责的本体必须实现至少六个不同的身份和持久性制度。同时，具有恰好六个这种制度的构造足以满足要求而不将因果或规范承诺嵌入到基底中。", "conclusion": "该研究不是对通用本体的提案，而是在中立性和稳定引用被视为不可谈判设计目标时对可能范围的一个约束条件。"}}
{"id": "2601.16150", "pdf": "https://arxiv.org/pdf/2601.16150", "abs": "https://arxiv.org/abs/2601.16150", "authors": ["Maximos Kaliakatsos-Papakostas", "Dimos Makris", "Konstantinos Soiledis", "Konstantinos-Theodoros Tsamis", "Vassilis Katsouros", "Emilios Cambouropoulos"], "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.", "AI": {"tldr": "本文提出了一种新的训练课程FF（完整到完整），以增强旋律与和声之间的交互作用，改进单编码器变压器模型在生成和谐伴奏方面的性能。", "motivation": "现有的基于离散扩散的训练课程导致了旋律和和声之间的弱交叉注意力，特别是在领域外场景中对旋律线索利用不足，因此提出了新的训练方法来解决这些问题。", "method": "该研究提出了一种名为FF（完整到完整）的训练课程，在训练初期阶段保持所有和弦符号被遮掩，并逐步揭示整个序列，以加强旋律与和声之间的互动。", "result": "实验结果表明，所提出的FF课程在几乎所有评估指标上都优于基线模型，尤其是在领域外评价中表现尤为出色。此外，还发现四分音符量化、小节标记交织以及使用音调类表示法对旋律有益。", "conclusion": "研究结果强调了训练课程的重要性，并证明了完整到完整的遮掩策略是单编码器和声化的有效方法，有助于更有效地条件化旋律信息。"}}
{"id": "2601.16148", "pdf": "https://arxiv.org/pdf/2601.16148", "abs": "https://arxiv.org/abs/2601.16148", "authors": ["Remy Sabathier", "David Novotny", "Niloy J. Mitra", "Tom Monnier"], "title": "ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes \"in action\" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed \"temporal 3D diffusion\". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.", "AI": {"tldr": "本文介绍了ActionMesh，一个能够生成动画3D网格的模型，在不同输入下实现快速、高质量的3D动画效果。", "motivation": "现有的高级工作难以在实际应用中推广，因为它们设置受限、运行时间长或质量有限，因此作者旨在开发一种实用性强且高质量的动画3D网格生成方法。", "method": "ActionMesh采用\"时空3D扩散\"框架，结合了修改过的3D扩散模型和时空3D自编码器，能够从单目视频、文本描述或带文本提示的3D网格等输入中生成动画3D网格。", "result": "该方法在标准的视频到4D基准测试上表现优秀，无论是几何精度还是时间一致性都达到了最新技术水平，并实现了前所未有的速度和质量。", "conclusion": "ActionMesh通过创新的方法提升了动画3D网格的质量与生成效率，为实际应用提供了快速迭代及无缝集成的可能性。"}}
{"id": "2601.16140", "pdf": "https://arxiv.org/pdf/2601.16140", "abs": "https://arxiv.org/abs/2601.16140", "authors": ["Sylvestre-Alvise Rebuffi", "Tuan Tran", "Valeriu Lacatusu", "Pierre Fernandez", "Tomáš Souček", "Nikola Jovanović", "Tom Sander", "Hady Elsahar", "Alexandre Mourachko"], "title": "Learning to Watermark in the Latent Space of Generative Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Code and models are available at https://github.com/facebookresearch/distseal", "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.", "AI": {"tldr": "该论文提出了一种在生成模型的潜在空间中进行水印的新方法，称为DistSeal，实现了高效的、鲁棒性强且不易察觉的水印。", "motivation": "现有的AI生成图像水印方法通常依赖于像素空间中的后处理技术，这不仅增加了计算开销，还可能导致视觉伪影。因此，该论文旨在寻找一种更高效和鲁棒性的替代方案。", "method": "通过训练潜在空间的后处理水印模型，并将其蒸馏到生成模型本身或潜在解码器中，实现了潜在空间中的水印技术DistSeal。", "result": "实验表明，这种方法在保持竞争力的鲁棒性的同时，提供了相似的不易察觉性和高达20倍的速度提升，优于像素空间基线方法。", "conclusion": "研究证明，在潜在空间进行水印的方法比在像素空间更高效且更为强大，这为AI生成内容的安全和版权保护提供了一个有力的技术支持。"}}
{"id": "2601.16134", "pdf": "https://arxiv.org/pdf/2601.16134", "abs": "https://arxiv.org/abs/2601.16134", "authors": ["Langdon Holmes", "Adam Coscia", "Scott Crossley", "Joon Suh Choi", "Wesley Morris"], "title": "LLM Prompt Evaluation for Educational Applications", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.", "AI": {"tldr": "本文介绍了一种系统的方法来评估和设计适合教育应用的大语言模型（LLM）提示词。", "motivation": "随着大语言模型在教育中的广泛应用，有必要开发基于证据的方法来个性化并确保与教学目标一致的输出。", "method": "研究中设计了六种模板，并通过一个锦标赛式的评价框架进行了比较。该框架使用Glicko2评级系统，并由八个评判者从格式、对话支持和适合学习者的三个方面对问题配对进行评估。", "result": "结果显示，与阅读策略相关的单一提示词在与其他模板的成对比较中表现出色，胜率为81%到100%。该提示词结合了角色扮演和上下文管理模式，并设计用于支持元认知学习策略。", "conclusion": "本研究展示了一种系统评估和改进提示词设计的方法，为教育技术研究人员从随意的提示工程转向基于证据的设计提供了路径。"}}
{"id": "2601.16130", "pdf": "https://arxiv.org/pdf/2601.16130", "abs": "https://arxiv.org/abs/2601.16130", "authors": ["Neeley Pate", "Adiba Mahbub Proma", "Hangfeng He", "James N. Druckman", "Daniel Molden", "Gourab Ghoshal", "Ehsan Hoque"], "title": "Replicating Human Motivated Reasoning Studies with LLMs", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.", "AI": {"tldr": "本文研究了基础LLM是否能复制人类动机性推理的行为，并发现它们未能很好地模仿这种行为。", "motivation": "尽管动机性推理在人类中被广泛研究，但尚不清楚基础语言模型（LLM）是否会表现出类似的动机变化。因此，作者希望通过这项研究探索这一问题。", "method": "通过重复先前的4个政治动机性推理研究，来观察基础LLM的行为是否与预期的人类行为一致。", "result": "发现基础LLM的行为不完全符合人类预期的行为，且不同模型之间存在一些相似之处，例如标准差较小和对论点强度评估不准确。", "conclusion": "强调这些发现对于使用LLMs自动化任务（如调查数据收集和论证评估）的研究人员的重要性。"}}
{"id": "2601.16127", "pdf": "https://arxiv.org/pdf/2601.16127", "abs": "https://arxiv.org/abs/2601.16127", "authors": ["Alphaeus Dmonte", "Vidhi Gupta", "Daniel J Perry", "Mark Arehart"], "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.", "AI": {"tldr": "通过语言特定模型合并提高训练效率并降低维护成本。", "motivation": "在多语言大型语言模型中，针对特定任务的微调涉及使用所有所需语言的数据集进行训练。更新一个或多个支持的语言或添加新语言需要重新训练模型，这可能计算效率低下且增加维护难度。", "method": "研究了合并多语言多任务模型的策略，并从效率角度进行了评估，涵盖三个独立的任务。", "result": "展示了显著的效率提升：初始训练时间最多减少50%；个体语言更新和重新合并作为模型维护减少了超过60%的培训成本。这些效果在公共数据集和专有行业数据集中均得到证实。", "conclusion": "这种模型合并的方法不仅适用于学术研究，也适合工业用例，在保证质量的同时提高了效率并降低了维护成本。"}}
{"id": "2601.16125", "pdf": "https://arxiv.org/pdf/2601.16125", "abs": "https://arxiv.org/abs/2601.16125", "authors": ["Tingyu Song", "Yanzhao Zhang", "Mingxin Li", "Zhuoning Guo", "Dingkun Long", "Pengjun Xie", "Siyue Zhang", "Yilun Zhao", "Shu Wu"], "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing", "categories": ["cs.CV", "cs.CL", "cs.IR"], "comment": "Under review", "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.", "AI": {"tldr": "本文提出了一个新的细粒度组成的图像检索基准EDIR，通过合成查询来评估多模态嵌入模型。", "motivation": "现有的组成图像检索基准在类别和多样化需求方面存在局限性，无法准确反映现实世界场景的要求。因此需要一个更加精细的评价基准。", "method": "利用图像编辑技术实现对修改类型和内容的精确控制，构建了一个涵盖5000个高质量查询的新型细粒度组成的图像检索基准EDIR，并评估了13个多模态嵌入模型。", "result": "结果表明现有最先进的模型在所有子类别上都难以保持一致性表现，这说明了新提出的基准EDIR的严格性。实验还揭示了现有基准中存在的模式偏见和分类覆盖不足的问题。", "conclusion": "通过比较分析和领域内训练实验验证了新型基准EDIR的有效性和挑战性，并指出了当前模型架构的内在局限。"}}
{"id": "2601.16118", "pdf": "https://arxiv.org/pdf/2601.16118", "abs": "https://arxiv.org/abs/2601.16118", "authors": ["Marco Ronzani", "Cristina Silvano"], "title": "A Case for Hypergraphs to Model and Map SNNs on Neuromorphic Hardware", "categories": ["cs.AR", "cs.NE"], "comment": null, "summary": "Executing Spiking Neural Networks (SNNs) on neuromorphic hardware poses the problem of mapping neurons to cores. SNNs operate by propagating spikes between neurons that form a graph through synapses. Neuromorphic hardware mimics them through a network-on-chip, transmitting spikes, and a mesh of cores, each managing several neurons. Its operational cost is tied to spike movement and active cores. A mapping comprises two tasks: partitioning the SNN's graph to fit inside cores and placement of each partition on the hardware mesh. Both are NP-hard problems, and as SNNs and hardware scale towards billions of neurons, they become increasingly difficult to tackle effectively. In this work, we propose to raise the abstraction of SNNs from graphs to hypergraphs, redesigning mapping techniques accordingly. The resulting model faithfully captures the replication of spikes inside cores by exposing the notion of hyperedge co-membership between neurons. We further show that the overlap and locality of hyperedges strongly correlate with high-quality mappings, making these properties instrumental in devising mapping algorithms. By exploiting them directly, grouping neurons through shared hyperedges, communication traffic and hardware resource usage can be reduced be yond what just contracting individual connections attains. To substantiate this insight, we consider several partitioning and placement algorithms, some newly devised, others adapted from literature, and compare them over progressively larger and bio-plausible SNNs. Our results show that hypergraph based techniques can achieve better mappings than the state-of-the-art at several execution time regimes. Based on these observations, we identify a promising selection of algorithms to achieve effective mappings at any scale.", "AI": {"tldr": "本文提出将SNNs的抽象从图提升到超图，并重新设计映射技术，以优化神经形态硬件上的执行。", "motivation": "随着SNNs和硬件规模向数十亿个神经元扩展，如何有效地将神经元映射到核心成为了一个挑战。传统的图形模型无法有效解决这一问题，因此需要一个新的抽象来更好地捕捉和处理这种复杂性。", "method": "本文通过使用超图模型来更准确地反映神经元内部的尖峰复制，并设计了新的分区和放置算法以利用超边的重叠和局部性特性。这些改进旨在减少通信流量和硬件资源的使用。", "result": "实验结果表明，基于超图的技术在多个执行时间范围内可以实现比现有技术更好的映射效果。", "conclusion": "研究者们确定了一些能够实现任何规模有效映射的算法，并指出超图模型为神经形态硬件上SNNs的有效执行提供了新的可能性。"}}
{"id": "2601.16117", "pdf": "https://arxiv.org/pdf/2601.16117", "abs": "https://arxiv.org/abs/2601.16117", "authors": ["Abdul Hannan", "Daniele Falavigna", "Shah Nawaz", "Mubashir Noman", "Markus Schedl", "Alessio Brutti"], "title": "Distillation-based Layer Dropping (DLD) Effective End-to-end Framework for Dynamic Speech Networks", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted at ICASSP 2026", "summary": "Edge devices operate in constrained and varying resource settings, requiring dynamic architectures that can adapt to limitations of the available resources. To meet such demands, layer dropping ($\\mathcal{LD}$) approach is typically used to transform static models into dynamic ones by skipping parts of the network along with reducing overall computational complexity. However, existing $\\mathcal{LD}$ methods greatly impact the dynamic model's performance for low and high dropping cases, deteriorating the performance-computation trade-off. To this end, we propose a distillation-based layer dropping (DLD) framework that effectively combines the capabilities of knowledge distillation and $\\mathcal{LD}$ in an end-to-end fashion, thereby achieving state-of-the-art performance for dynamic speech networks. Comprehensive experimentation utilizing well-known speech recognition methods, including conformer and WavLM, on three public benchmarks demonstrates the effectiveness of our framework, reducing the word error rate by $9.32\\%$ and $2.25\\%$ for high and no dropping cases with $33.3\\%$ reduction in training time.", "AI": {"tldr": "本文提出了基于知识蒸馏的层丢弃（DLD）框架，以提高动态语音网络在边缘设备上的性能和计算效率。", "motivation": "为了满足边缘设备在资源受限条件下对动态架构的需求，现有层丢弃方法严重影响了模型在高低丢弃情况下的性能。", "method": "本文提出了一种基于知识蒸馏的层丢弃（DLD）框架，将知识蒸馏和层丢弃结合，在端到端的方式下优化动态语音网络。", "result": "实验表明，该框架能显著降低高丢弃和无丢弃情况下单词错误率，分别减少了9.32%和2.25%，同时训练时间减少33.3%。", "conclusion": "DLD框架在保持高性能的同时有效降低了计算复杂度，并提高了动态语音网络的适应性。"}}
{"id": "2601.16113", "pdf": "https://arxiv.org/pdf/2601.16113", "abs": "https://arxiv.org/abs/2601.16113", "authors": ["Haq Nawaz Malik", "Kh Mohmad Shafi", "Tanveer Ahmad Reshi"], "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text. We present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts. We demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.", "AI": {"tldr": "本文介绍了一个名为SynthOCR-Gen的开源合成OCR数据集生成器，旨在解决低资源语言的OCR挑战。", "motivation": "由于大型标注训练数据集稀缺，低资源语言如克什米尔语在主要OCR系统中缺乏支持。手动创建数据集成本高、耗时且易出错。", "method": "该工具通过将数字Unicode文本语料库转换为现成的训练数据集，解决了OCR开发中的基本瓶颈。它涵盖了多个级别的文本分割、Unicode规范化、多字体渲染和25多种数据增强技术。", "result": "作者生成并公开发布了一个包含60万个样本的克什米尔语单词分段OCR数据集。", "conclusion": "这项工作为低资源语言进入视觉语言AI模型时代提供了实用路径，工具对全球研究人员开放。"}}
{"id": "2601.16109", "pdf": "https://arxiv.org/pdf/2601.16109", "abs": "https://arxiv.org/abs/2601.16109", "authors": ["Yashuai Yan", "Tobias Egle", "Christian Ott", "Dongheui Lee"], "title": "Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision", "categories": ["cs.RO"], "comment": null, "summary": "We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.", "AI": {"tldr": "本文提出了一种结合模型基的双足行走控制与残差强化学习（RL）的框架，以实现对现实世界不确定性具有鲁棒性和适应性的行进。", "motivation": "为了解决由于不准确的动力学建模和传感器噪声导致的实际不确定性的挑战，提高机器人在真实环境中的行进能力。", "method": "该方法包括一个基于模型的控制器（包含DCM轨迹规划器和全身控制器）作为可靠的基础策略，并通过带有领域随机化的残差强化学习来解决不确定性问题。还引入了一个拥有地面真相动力学访问权的模型基Oracle政策，利用新的监督损失对残差政策进行指导。", "result": "该方法在一系列随机化条件下展示了更好的鲁棒性和泛化能力，为双足行走提供了可扩展的模拟到现实世界的转移解决方案。", "conclusion": "研究表明，结合模型基础和强化学习的方法可以有效提高机器人在复杂环境中的行进性能，并且能够更好地适应不确定性因素的影响。"}}
{"id": "2601.16108", "pdf": "https://arxiv.org/pdf/2601.16108", "abs": "https://arxiv.org/abs/2601.16108", "authors": ["Marzieh Adeli Shamsabad", "Hamed Ghodrati"], "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources", "categories": ["cs.AI"], "comment": null, "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.", "AI": {"tldr": "本文提出了一种结合视觉语言模型与外部知识源的气候虚假信息检测方法。", "motivation": "随着社交媒体上误导性图像和视频的广泛传播，气候虚假信息已成为当今数字世界的一大挑战。现有的视觉语言模型受限于训练时的知识，难以处理实时事件或更新。", "method": "通过集成视觉语言模型与外部知识源（如逆向图像搜索、在线事实核查及专家内容），以增强模型识别真假信息的能力。", "result": "该方法提升了系统评估图像及其声明准确性的能力，增强了对实际气候虚假信息的检测效果。", "conclusion": "结合视觉语言模型和外部知识源的方法能够更好地处理实时气候变化相关信息，保护公众科学认知。"}}
{"id": "2601.16098", "pdf": "https://arxiv.org/pdf/2601.16098", "abs": "https://arxiv.org/abs/2601.16098", "authors": ["Zack Dewis", "Yimin Zhu", "Zhengsen Xu", "Mabel Heffring", "Saeid Taleghanidoozdoozan", "Quinn Ledingham", "Lincoln Linlin Xu"], "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "5 pages, 3 figures", "summary": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.", "AI": {"tldr": "本文提出了一种基于聚类引导的空间光谱Mamba框架（CSSMamba），用于提高高光谱图像分类的性能。", "motivation": "尽管Mamba模型显著提升了高光谱图像分类的效果，但在定义有效且自适应的标记序列方面存在挑战。因此，该论文旨在通过引入新的方法来改进现有技术。", "method": "提出了CSSMamba框架，包括聚类引导的空间Mamba模块（CSpaMamba）、空间和光谱信息学习的集成、注意力驱动令牌选择机制以及可学习聚类模块。", "result": "在Pavia University, Indian Pines, 和 Liao-Ning 01 数据集上的实验结果表明，CSSMamba在分类精度和边界保持方面优于现有的CNN、Transformer和基于Mamba的方法。", "conclusion": "通过引入新的框架和机制，论文实现了更高效的高光谱图像分类性能，证明了提出的CSSMamba的有效性。"}}
{"id": "2601.16096", "pdf": "https://arxiv.org/pdf/2601.16096", "abs": "https://arxiv.org/abs/2601.16096", "authors": ["Hyunsoo Kim", "Ehsan Pajouheshgar", "Sabine Süsstrunk", "Wenzel Jakob", "Jinah Park"], "title": "Neural Particle Automata: Learning Self-Organizing Particle Dynamics", "categories": ["cs.NE", "cs.CV"], "comment": "15 pages, 15 figures", "summary": "We introduce Neural Particle Automata (NPA), a Lagrangian generalization of Neural Cellular Automata (NCA) from static lattices to dynamic particle systems. Unlike classical Eulerian NCA where cells are pinned to pixels or voxels, NPA model each cell as a particle with a continuous position and internal state, both updated by a shared, learnable neural rule. This particle-based formulation yields clear individuation of cells, allows heterogeneous dynamics, and concentrates computation only on regions where activity is present. At the same time, particle systems pose challenges: neighborhoods are dynamic, and a naive implementation of local interactions scale quadratically with the number of particles. We address these challenges by replacing grid-based neighborhood perception with differentiable Smoothed Particle Hydrodynamics (SPH) operators backed by memory-efficient, CUDA-accelerated kernels, enabling scalable end-to-end training. Across tasks including morphogenesis, point-cloud classification, and particle-based texture synthesis, we show that NPA retain key NCA behaviors such as robustness and self-regeneration, while enabling new behaviors specific to particle systems. Together, these results position NPA as a compact neural model for learning self-organizing particle dynamics.", "AI": {"tldr": "介绍神经粒子自动机（NPA），这是一种将神经细胞自动机从静态格子推广到动态粒子系统的拉格朗日泛化。", "motivation": "改进经典的欧拉式神经元胞自动机，解决其在固定像素或体素上的局限性，并为粒子系统提供更高效的计算方法和自我组织行为的实现。", "method": "将每个细胞视为具有连续位置和内部状态的粒子，通过共享且可学习的神经规则更新这些属性；采用平滑粒子流体力学（SPH）操作符解决动态邻域和局部交互扩展性问题。", "result": "在形态发生、点云分类和基于粒子的纹理合成任务中展示出稳健性和自我再生等行为，并实现新的特定于粒子系统的功能。", "conclusion": "NPA作为一种紧凑的神经模型，能够学习自组织粒子动力学，为动态系统提供了一种有效的建模方法。"}}
{"id": "2601.16093", "pdf": "https://arxiv.org/pdf/2601.16093", "abs": "https://arxiv.org/abs/2601.16093", "authors": ["Yikang Zhou", "Tao Zhang", "Dengxian Gong", "Yuanzheng Wu", "Ye Tian", "Haochen Wang", "Haobo Yuan", "Jiacong Wang", "Lu Qi", "Hao Fei", "Anran Wang", "Zhuochen Wang", "Yujing Wang", "Cheng Chen", "Shunping Ji", "Xiangtai Li"], "title": "SAMTok: Representing Any Mask with Two Words", "categories": ["cs.CV"], "comment": "27 pages, 11 figures", "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "AI": {"tldr": "本文介绍了SAMTok，一种将任何区域掩码转换为两个特殊标记的方法，并利用这些标记重建掩码。", "motivation": "现有的像素级多模态大语言模型难以扩展，因为需要复杂的区域编码器、专门的分割解码器和不兼容的训练目标。为此，提出了SAMTok来解决这些问题。", "method": "SAMTok将任何区域掩码转换为两个特殊标记，并使用这些标记以高保真度重建掩码，使基础多模态语言模型能够通过标准的下一个词预测和简单的强化学习获取像素级能力。", "result": "通过5M SAMTok格式化的数据样本，QwenVL-SAMTok在区域描述、区域VQA、基于语境对话、引用分割、场景图解析和多轮交互式分割等任务上达到或接近最新水平，并引入了一种文本答案匹配奖励机制来改进掩码生成。", "conclusion": "研究结果表明，SAMTok提供了一个可扩展且简单的范例，使多模态语言模型具备强大的像素级能力。"}}
{"id": "2601.16091", "pdf": "https://arxiv.org/pdf/2601.16091", "abs": "https://arxiv.org/abs/2601.16091", "authors": ["Saar Cohen"], "title": "Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026", "summary": "Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point's location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points' locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.", "AI": {"tldr": "本文提出了一个在线非质心聚类框架，其中元素以随机顺序到达，并允许延迟分配决定。", "motivation": "传统的在线聚类算法假设所有决策必须立即做出，这可能导致次优结果。本文动机在于通过引入可延迟的分配来优化总距离成本和延迟成本之间的权衡。", "method": "研究了具有延迟成本的在线非质心聚类问题，在随机到达模型下设计了一个常数竞争性算法。", "result": "提出的算法在元素数量增加时，其输出聚类的预期总体成本与最优离线聚类的成本比被一个常数上界所约束。", "conclusion": "允许延迟决策可以克服传统在线算法的竞争比率局限，在随机到达模型中实现更优的聚类效果。"}}
{"id": "2601.16087", "pdf": "https://arxiv.org/pdf/2601.16087", "abs": "https://arxiv.org/abs/2601.16087", "authors": ["Sukesh Subaharan"], "title": "Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics", "categories": ["cs.AI", "cs.CL"], "comment": "Supplementary materials can be found here: https://github.com/drsukeshs/agent-behavior-ext-dynamics", "summary": "Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.", "AI": {"tldr": "本研究探讨了在多轮对话中，通过对外部情感状态施加动力学结构来实现时间连贯性和可控恢复的方法。", "motivation": "大型语言模型代理在长时间交互过程中常常表现出语气和人设的突然变化，这是因为缺乏明确的时间结构来控制代理级状态。本研究旨在探索是否可以通过施加动力学结构到外部情感状态中，来改善多轮对话中的时间连贯性和可控恢复。", "method": "引入了一个独立于语言模型的情感子系统，该子系统维护一个连续的VAD（效价-唤醒-优势）状态，并通过一阶和二阶更新规则进行控制。采用固定的记忆无关估计器提取即时情感信号并通过指数平滑或动量动力学在时间上进行集成。", "result": "使用固定的25轮对话协议，比较了无状态、一阶和二阶情感动态的代理行为。结果表明，无状态代理无法表现出连贯的行为轨迹或恢复能力，而状态持久性使延迟响应和可靠的恢复成为可能。第二级动力学引入了情感惯性和滞后效应，随动量增加，呈现出稳定性和响应性之间的权衡。", "conclusion": "施加明确的情感动态结构可以显著提高大型语言模型代理在多轮对话中的时间连贯性和可控恢复能力，尤其是通过使用二阶动力学能够实现更加稳定的长期行为。"}}
{"id": "2601.16083", "pdf": "https://arxiv.org/pdf/2601.16083", "abs": "https://arxiv.org/abs/2601.16083", "authors": ["Matthew Shorvon", "Frederik Mallmann-Trenn", "David S. Watson"], "title": "Probably Approximately Correct Maximum A Posteriori Inference", "categories": ["cs.LG", "cs.AI"], "comment": "7 pages main text, 16 total, 3 figures", "summary": "Computing the conditional mode of a distribution, better known as the $\\mathit{maximum\\ a\\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\\mathit{probably\\ approximately\\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.", "AI": {"tldr": "本文提出了用于最大后验概率（MAP）推理的可能近似正确（PAC）算法，该算法在给定计算预算下提供最优解。", "motivation": "尽管最大后验概率估计通常不可行，但此论文旨在开发一种方法来解决这一问题，并提供严格的保证和优化解决方案。", "method": "本文使用信息论度量来表征PAC-MAP的可处理性条件，并通过具有适当架构的概率电路有效实现PAC-MAP求解器。", "result": "实验表明，该方法在各种基准测试中表现良好，能提供有保证的优化解决方案。", "conclusion": "随机化策略不仅作为独立的MAP推理技术使用，还可以提高流行启发式算法的效果，并为其解决方案添加严格的保证。"}}
{"id": "2601.16079", "pdf": "https://arxiv.org/pdf/2601.16079", "abs": "https://arxiv.org/abs/2601.16079", "authors": ["Zhiyin Qian", "Siwei Zhang", "Bharat Lal Bhatnagar", "Federica Bogo", "Siyu Tang"], "title": "Masked Modeling for Human Motion Recovery Under Occlusions", "categories": ["cs.CV"], "comment": "Project page: https://mikeqzy.github.io/MoRo", "summary": "Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.", "AI": {"tldr": "本文提出了一种名为MoRo的生成框架，用于从RGB视频中恢复被遮挡的人体运动。", "motivation": "现有的基于回归的方法对缺失数据敏感，而优化和扩散方法虽然提高了鲁棒性但降低了推理速度。为了克服这些限制并提高在遮挡情况下的运动重建效果，本文提出了MoRo框架。", "method": "MoRo通过视频条件化任务和掩码建模来恢复人体运动，并设计了跨模式学习方案以从多种异构数据集中学习多模态先验知识。", "result": "实验结果表明，与现有方法相比，MoRo在遮挡情况下的准确性及动作真实性上取得了显著改善，在非遮挡场景中也表现出色，实现了实时推理。", "conclusion": "本文提出的MoRo框架通过结合跨模式学习和视频条件化任务，有效提高了人体运动恢复的鲁棒性和效率。"}}
{"id": "2601.16078", "pdf": "https://arxiv.org/pdf/2601.16078", "abs": "https://arxiv.org/abs/2601.16078", "authors": ["Jiarui Cui", "Maosong Wang", "Wenqi Wu", "Peiqi Li", "Xianfei Pan"], "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.", "AI": {"tldr": "本文通过改进SE2(3)李群框架下的扩展卡尔曼滤波器，提高集成导航系统的自主性，并进行实际实验和蒙特卡洛模拟验证其性能。", "motivation": "动机在于提升非惯性导航模型的完全自主性，利用SE2(3)李群框架优化误差传播特性。", "method": "提出一种基于SE2(3)群的导航模型构建方法，并结合SINS/里程计实验和蒙特卡洛模拟进行性能验证。", "result": "改进后的高精度导航模型在实际应用中表现出色，增强了系统的自主性。", "conclusion": "通过实验与仿真证明了基于SE2(3)群的扩展卡尔曼滤波器对提高集成导航系统自主性的有效性。"}}
{"id": "2601.16076", "pdf": "https://arxiv.org/pdf/2601.16076", "abs": "https://arxiv.org/abs/2601.16076", "authors": ["Xi Chen", "William Pires", "Toniann Pitassi", "Rocco A. Servedio"], "title": "DNF formulas are efficiently testable with relative error", "categories": ["cs.CC", "cs.DS"], "comment": null, "summary": "We give a poly$(s,1/ε)$-query algorithm for testing whether an unknown and arbitrary function $f: \\{0,1\\}^n \\to \\{0,1\\}$ is an $s$-term DNF, in the challenging relative-error framework for Boolean function property testing that was recently introduced and studied in a number of works [CDH+25b, CPPS25a, CPPS25b, CDH+25a]. This gives the first example of a rich and natural class of functions which may depend on a super-constant number of variables and yet is efficiently testable in the relative-error model with constant query complexity. A crucial new ingredient enabling our approach is a novel decomposition of any $s$-term DNF formula into ``local clusters'' of terms. Our results demonstrate that this new decomposition can be usefully exploited for algorithms even when the $s$-term DNF is not explicitly given; we believe that this decomposition may have applications in other contexts.", "AI": {"tldr": "本文提出了一种针对未知任意函数是否为s项DNF公式的高效测试算法。", "motivation": "研究动机在于寻找一种可以在相对误差框架下，对依赖于超常数个变量的布尔函数进行有效测试的方法。", "method": "通过将任何s项DNF公式分解成“局部簇”，提出了一个查询次数为多项式级别的测试算法。", "result": "实现了对于任意未知函数是否属于s项DNF公式的高效测试，具有常数查询复杂度。", "conclusion": "结果表明新的分解方法可以用于不在显式给定的情况下对s项DNF公式进行有效处理，并可能在其他场景中找到应用。"}}
{"id": "2601.16073", "pdf": "https://arxiv.org/pdf/2601.16073", "abs": "https://arxiv.org/abs/2601.16073", "authors": ["Hanwen Zhang", "Qiaojin Shen", "Yuxi Liu", "Yuesheng Zhu", "Guibo Luo"], "title": "DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.", "AI": {"tldr": "该论文提出了DSFedMed，一个双尺度联邦框架，通过基础模型和轻量级客户端模型之间的互知识蒸馏来进行医疗图像分割。", "motivation": "虽然基础模型在各种视觉任务中表现出强大的泛化能力，但它们在联邦设置中的部署受限于高计算需求、大量的通信开销和显著的推理成本。因此，提出DSFedMed以解决这些问题。", "method": "该论文提出了一个双尺度联邦框架DSFedMed，支持知识蒸馏，并通过生成高质量的医学图像来替代真实公共数据集。同时采用可学习性引导样本选择策略提高效率和效果。互知识蒸馏使基础模型能够将通用知识传递给轻量级客户端模型，同时也融入了客户端特定的见解以优化基础模型。", "result": "在五个医疗影像分割数据集上的评估表明，DSFedMed平均提高了2％的Dice得分，同时通信成本和推理时间分别减少了近90%相比现有的联邦基础模型基线。", "conclusion": "这些结果展示了显著的效率提升和对资源受限的联邦部署的良好扩展性。"}}
{"id": "2601.16065", "pdf": "https://arxiv.org/pdf/2601.16065", "abs": "https://arxiv.org/abs/2601.16065", "authors": ["Chenyang Li", "Jieyuan Liu", "Bin Li", "Bo Gao", "Yilin Yuan", "Yangfan He", "Yuchen Li", "Jingqun Tang"], "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.", "AI": {"tldr": "本文提出了一种简单而有效的Distracting Token Pruning (DTP)框架，用于改进Vision-Language Action模型的性能。", "motivation": "论文动机在于解决VLA模型在执行任务时过于关注与任务无关区域的问题，这会干扰模型生成期望的动作令牌，并影响任务成功率。", "method": "该方法通过动态检测并修剪这些分散注意力的图像标记来纠正视觉注意力模式，提高任务成功率，且无需改变模型原始架构或增加额外输入。", "result": "实验结果显示，在SIMPLER Benchmark上，本方法在不同类型的新型VLA模型中均取得相对较高的任务成功率提升，表明该方法具有广泛的适用性。", "conclusion": "研究揭示了任务无关区域注意力量与任务成功率之间的负相关关系，指出了VLA模型的一个普遍现象，并为未来的研究提供了指导。"}}
{"id": "2601.16064", "pdf": "https://arxiv.org/pdf/2601.16064", "abs": "https://arxiv.org/abs/2601.16064", "authors": ["Shams Nafisa Ali", "Taufiq Hasan"], "title": "Phi-SegNet: Phase-Integrated Supervision for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages, 7 figures", "summary": "Deep learning has substantially advanced medical image segmentation, yet achieving robust generalization across diverse imaging modalities and anatomical structures remains a major challenge. A key contributor to this limitation lies in how existing architectures, ranging from CNNs to Transformers and their hybrids, primarily encode spatial information while overlooking frequency-domain representations that capture rich structural and textural cues. Although few recent studies have begun exploring spectral information at the feature level, supervision-level integration of frequency cues-crucial for fine-grained object localization-remains largely untapped. To this end, we propose Phi-SegNet, a CNN-based architecture that incorporates phase-aware information at both architectural and optimization levels. The network integrates Bi-Feature Mask Former (BFMF) modules that blend neighboring encoder features to reduce semantic gaps, and Reverse Fourier Attention (RFA) blocks that refine decoder outputs using phase-regularized features. A dedicated phase-aware loss aligns these features with structural priors, forming a closed feedback loop that emphasizes boundary precision. Evaluated on five public datasets spanning X-ray, US, histopathology, MRI, and colonoscopy, Phi-SegNet consistently achieved state-of-the-art performance, with an average relative improvement of 1.54+/-1.26% in IoU and 0.98+/-0.71% in F1-score over the next best-performing model. In cross-dataset generalization scenarios involving unseen datasets from the known domain, Phi-SegNet also exhibits robust and superior performance, highlighting its adaptability and modality-agnostic design. These findings demonstrate the potential of leveraging spectral priors in both feature representation and supervision, paving the way for generalized segmentation frameworks that excel in fine-grained object localization.", "AI": {"tldr": "Phi-SegNet通过在CNN架构中集成相位感知信息，提高了医学图像分割的鲁棒性和精确性。", "motivation": "现有的深度学习方法主要关注空间信息而忽略频域表示，这限制了其在跨多种成像模式和解剖结构上的泛化能力。因此提出Phi-SegNet来解决这些问题。", "method": "Phi-SegNet使用Bi-Feature Mask Former模块减少编码器特征之间的语义差距，并通过Reverse Fourier Attention块利用相位正则化的特征优化解码器输出，同时引入了专门的相位感知损失函数以增强边界精度。", "result": "在五个公共数据集上的实验结果表明Phi-SegNet达到了最先进的性能水平，在IoU和F1-score上分别有平均1.54%和0.98%的相对提升，并且具有良好的跨数据集泛化能力。", "conclusion": "研究表明，利用频域先验信息在特征表示和监督中可以增强医学图像分割框架的性能，尤其适用于细粒度物体定位任务。"}}
{"id": "2601.16062", "pdf": "https://arxiv.org/pdf/2601.16062", "abs": "https://arxiv.org/abs/2601.16062", "authors": ["Jiarui Cui", "Maosong Wang", "Wenqi Wu", "Peiqi Li", "Xianfei Pan"], "title": "Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.", "AI": {"tldr": "本文分析了SE2(3)群框架下扩展卡尔曼滤波在高精度导航中的自主性问题，并提出了一种改进方法。", "motivation": "当前研究显示，基于Lie群的扩展卡尔曼滤波器在低精度应用中能够保持误差传播的自主性，但在考虑地球自转和惯性设备偏置时，难以维持这种自主性。因此，本文旨在通过理论分析解决高精度导航中的这一问题。", "method": "本文进行了SE2(3)群框架下的高精度导航模型在不同参考系（惯性、地心和世界坐标）中自主性的理论分析，并提出了一种改进的构造方法以克服传统方法中存在的局限性。", "result": "通过理论分析发现，传统SE2(3)群导航建模方法的局限在于非惯性框架下由速度引入的科里奥利力项。本文提出的方法有助于使导航模型更接近完全自主性。", "conclusion": "本文提出的改进方法能够更好地保持高精度导航中的自主性，克服了传统SE2(3)群导航建模方法中存在的局限性。"}}
{"id": "2601.16060", "pdf": "https://arxiv.org/pdf/2601.16060", "abs": "https://arxiv.org/abs/2601.16060", "authors": ["Yuan Lin", "Murong Xu", "Marc Hölle", "Chinmay Prabhakar", "Andreas Maier", "Vasileios Belagiannis", "Bjoern Menze", "Suprosanna Shit"], "title": "ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation", "categories": ["cs.CV"], "comment": "5 pages, 4 figures. It has been accepted by IEEE ISBI", "summary": "Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.", "AI": {"tldr": "本文提出了一个名为ProGiDiff的新框架，利用现有的图像生成模型进行医学影像分割，并展示其在CT和MRI图像上的性能。", "motivation": "传统的医学影像分割方法虽然高效但缺乏对自然语言提示的支持，限制了其适应多种情况的能力。文本到图像扩散模型显示出了潜力，但训练这样的模型需要大量的数据，在医学图像上存在局限性。", "method": "提出了一种基于ControlNet样式的调节机制，结合定制编码器来控制预训练的扩散模型输出分割掩码，并展示如何通过自然语言提示扩展到多类设置。", "result": "实验结果表明，在CT影像上的器官分割任务中，该方法表现优于先前的方法。并且还证明了学习到的调节机制可以通过低秩、少量样本适应来转移以分割MRI图像。", "conclusion": "ProGiDiff框架通过利用现有的图像生成模型并结合定制编码器和扩散模型实现了有效的医学影像分割，并展示了在多模态下的潜力，特别是在专家介入的情况下能够更好地利用多个提案。"}}
{"id": "2601.16056", "pdf": "https://arxiv.org/pdf/2601.16056", "abs": "https://arxiv.org/abs/2601.16056", "authors": ["Ruizhi Liu", "Liming Xu", "Xulin Huang", "Jingyan Sui", "Shizhe Ding", "Boyang Xia", "Chungong Yu", "Dongbo Bu"], "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path", "categories": ["cs.AI"], "comment": null, "summary": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.", "AI": {"tldr": "本文提出DeepBound算法，通过深度学习自动学习最优路径选择策略来提高混合整数线性规划（MILP）问题的求解效率。", "motivation": "传统的分支定界算法依赖于手工设计的启发式策略，其性能不稳定且难以预测，因此需要一种自动化的方法来优化节点选择过程。", "method": "DeepBound利用多级特征融合网络捕捉节点表示，并采用成对训练范式解决分支定界树中固有的节点不平衡问题，从而学习到优先包含最优解的节点。", "result": "在三个NP难MILP基准测试上的实验表明，与传统启发规则和现有的基于学习的方法相比，DeepBound显著减少了计算时间并获得了更优的可行解。", "conclusion": "DeepBound展示了强大的泛化能力，并且能够自动发现更加灵活和稳健的特征选择方法，这可能有效提升甚至替代人工设计的启发式策略。"}}
{"id": "2601.16050", "pdf": "https://arxiv.org/pdf/2601.16050", "abs": "https://arxiv.org/abs/2601.16050", "authors": ["Xiaowei Chen", "Mindy Tran", "Yue Deng", "Bhupendra Acharya", "Yixin Zou"], "title": "From Harm to Healing: Understanding Individual Resilience after Cybercrimes", "categories": ["cs.HC"], "comment": "To appear in Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI 26)", "summary": "How do individuals recover from cybercrimes? Victims experience various types of harm after cybercrimes, including monetary loss, data breaches, negative emotions, and even psychological trauma. The aspects that support their recovery process and contribute to individual cyber resilience remain underinvestigated. To address this gap, we interviewed 18 cybercrime victims from Western Europe using a trauma-informed approach. We identified four common stages following victimization: recognition, coping, processing, and recovery. Participants adopted various strategies to mitigate the impact of cybercrime and used different indicators to describe recovery. While they mostly relied on social support and self-regulation for emotional coping, service providers largely determined whether victims were able to recover their money. Internal factors, external support, and context sensitivity collectively contribute to individuals' cyber resilience. We recommend trauma-informed support for cybercrime victims. Extending our conceptualization of individual cyber resilience, we propose collaborative and context-sensitive strategies to address the harmful impacts of cybercrime.", "AI": {"tldr": "研究了网络犯罪受害者如何恢复，并识别出四个阶段：认识、应对、处理和康复。", "motivation": "由于个体在网络犯罪后的复原力尚未得到充分研究，本研究旨在探讨支撑受害者恢复过程的关键因素。", "method": "通过对18名来自西欧的网络犯罪受害者的访谈，采用创伤知情方法来理解其恢复过程。", "result": "发现受害者主要依赖社会支持和自我调节进行情感应对，并且服务提供商很大程度上决定了受害人能否挽回经济损失。内部因素、外部支持以及情境敏感性共同作用于个体的网络复原力。", "conclusion": "建议为网络犯罪受害者提供创伤知情的支持，提出合作性和情境敏感性的策略以减轻网络犯罪带来的负面影响。"}}
{"id": "2601.16046", "pdf": "https://arxiv.org/pdf/2601.16046", "abs": "https://arxiv.org/abs/2601.16046", "authors": ["Junha Lee", "Eunha Park", "Minsu Cho"], "title": "DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.", "AI": {"tldr": "DextER通过引入基于接触的具身推理，生成复杂的多指操作抓取动作。", "motivation": "现有方法直接将观察映射为抓取参数，忽略了物理交互中的中间推理，因此提出了DextER来改进这一过程。", "method": "DextER采用自回归方式生成体现手部与物体接触的token，接着编码手部配置的抓取token。", "result": "在DexGYS数据集上，DextER达到了67.14%的成功率，比现有最佳方法高出3.83%，并且意图对齐提高了96.4%。", "conclusion": "DextER通过具身接触推理实现了更优的抓取生成性能，并展示了通过部分接触指定进行可控抓取合成的能力。"}}
{"id": "2601.16045", "pdf": "https://arxiv.org/pdf/2601.16045", "abs": "https://arxiv.org/abs/2601.16045", "authors": ["Yue Shi", "Liangxiu Han", "Xin Zhang", "Tam Sobeih", "Thomas Gaiser", "Nguyen Huu Thuy", "Dominik Behrend", "Amit Kumar Srivastava", "Krishnagopal Halder", "Frank Ewert"], "title": "AgriPINN: A Process-Informed Neural Network for Interpretable and Scalable Crop Biomass Prediction Under Water Stress", "categories": ["cs.AI"], "comment": null, "summary": "Accurate prediction of crop above-ground biomass (AGB) under water stress is critical for monitoring crop productivity, guiding irrigation, and supporting climate-resilient agriculture. Data-driven models scale well but often lack interpretability and degrade under distribution shift, whereas process-based crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are difficult to deploy over large spatial domains. To address these limitations, we propose AgriPINN, a process-informed neural network that integrates a biophysical crop-growth differential equation as a differentiable constraint within a deep learning backbone. This design encourages physiologically consistent biomass dynamics under water-stress conditions while preserving model scalability for spatially distributed AGB prediction. AgriPINN recovers latent physiological variables, including leaf area index (LAI), absorbed photosynthetically active radiation (PAR), radiation use efficiency (RUE), and water-stress factors, without requiring direct supervision. We pretrain AgriPINN on 60 years of historical data across 397 regions in Germany and fine-tune it on three years of field experiments under controlled water treatments. Results show that AgriPINN consistently outperforms state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer) and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to $43\\%$) and computational efficiency. By combining the scalability of deep learning with the biophysical rigor of process-based modeling, AgriPINN provides a robust and interpretable framework for spatio-temporal AGB prediction, offering practical value for planning of irrigation infrastructure, yield forecasting, and climate-adaptation planning.", "AI": {"tldr": "提出AgriPINN，一种将生物物理作物生长微分方程作为深度学习框架中可微约束的过程知情神经网络，以实现对水胁迫下作物地上生物量的准确、可解释和可扩展预测。", "motivation": "现有数据驱动模型缺乏可解释性且在分布偏移时表现不佳；过程基作物模型需要大量校准且难以部署。研究旨在结合二者优势，开发一种既能保持深度学习规模效应又能整合生物物理严谨性的模型。", "method": "AgriPINN采用了一种将生物物理作物生长方程作为可微约束嵌入到深度学习框架中的设计，通过这种组合来促进生理上一致的生物量动态预测，并在无需直接监督的情况下恢复潜在生理变量如叶面积指数、吸收光合有效辐射等。", "result": "AgriPINN在历史数据和控制水处理实验中显示出对现有深度学习模型（ConvLSTM-ViT, SLTF, CNN-Transformer）以及过程基的LINTUL5模型在准确性和计算效率上的显著优势，RMSE减少高达43%。", "conclusion": "AgriPINN通过结合深度学习的可扩展性与基于过程建模的严谨性，提供了一个强大且可解释框架用于时空生物量预测，对灌溉基础设施规划、产量预测及气候适应计划具有实际应用价值。"}}
{"id": "2601.16038", "pdf": "https://arxiv.org/pdf/2601.16038", "abs": "https://arxiv.org/abs/2601.16038", "authors": ["Olga Bunkova", "Lorenzo Di Fruscia", "Sophia Rupprecht", "Artur M. Schweidtmann", "Marcel J. T. Reinders", "Jana M. Weber"], "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval", "categories": ["cs.AI"], "comment": "Accepted at ML4Molecules 2025 (ELLIS UnConference workshop), Copenhagen, Denmark, December 2, 2025. Workshop page: https://moleculediscovery.github.io/workshop2025/", "summary": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.", "AI": {"tldr": "本文研究了大型语言模型在化学合成规划中的应用，通过将其与反应知识图谱交互，提出了一个基于Text2Cypher生成问题的框架来提高查询的有效性和检索准确性。", "motivation": "标准的提示方法经常导致大型语言模型提出幻觉或过时的建议。为了解决这个问题，并改进化学合成规划，作者研究了将大型语言模型与反应知识图谱交互的方法。", "method": "作者将反应路径检索转换成Text2Cypher生成问题并定义单步和多步检索任务。他们对比了零样本提示、一样本静态示例选择、随机示例选择以及基于嵌入的示例选择，还评估了一个清单驱动验证/纠正循环。", "result": "结果表明，使用对齐示例的一样本提示在查询有效性和检索准确性上表现最好。检查表式的自我修正回路主要提升了零样本设置下的可执行性，并且当存在一个好的示例时提供有限的额外检索收益。", "conclusion": "一样本提示结合对齐示例显著提高了合成规划任务中的反应路径检索性能，而清单驱动验证/纠正循环在没有良好示例的情况下改进了执行能力。"}}
{"id": "2601.16035", "pdf": "https://arxiv.org/pdf/2601.16035", "abs": "https://arxiv.org/abs/2601.16035", "authors": ["Han Xue", "Sikai Liang", "Zhikai Zhang", "Zicheng Zeng", "Yun Liu", "Yunrui Lian", "Jilong Wang", "Qingtao Liu", "Xuesong Shi", "Li Yi"], "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes", "categories": ["cs.RO"], "comment": null, "summary": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.", "AI": {"tldr": "本文研究了机器人在杂乱室内场景中无碰撞穿越的问题，提出了一种有效的方法来实现这一目标。", "motivation": "缺乏一种有效的表示方法来捕捉人形机器人与障碍物之间的关系，使得直接学习这种映射变得困难。因此，提出了新的解决办法以克服这一难题。", "method": "本文提出了Humanoid Potential Field (HumanoidPF) 来编码这些关系作为无碰撞运动方向，并提出了一种混合场景生成方法，结合了现实的3D室内场景和程序合成障碍物。", "result": "通过广泛的实验验证了该方法的有效性，在模拟和真实世界中都取得了成功转移策略并开发了一个远程操作系统。", "conclusion": "该研究展示了使用HumanoidPF实现人形机器人在杂乱环境中的无碰撞穿越，并证明了其从模拟到现实的转化能力及有效性。"}}
{"id": "2601.16032", "pdf": "https://arxiv.org/pdf/2601.16032", "abs": "https://arxiv.org/abs/2601.16032", "authors": ["Yifan Zhu", "Yekai Pan", "Chen Ding"], "title": "Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA GB10", "categories": ["cs.PF", "cs.AI", "cs.LG", "cs.OS"], "comment": null, "summary": "High-performance attention kernels are essential for Large Language Models. This paper presents analysis of CuTile-based Flash Attention memory behavior and a technique to improve its cache performance. In particular, our analysis on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache miss. Leveraging this insight, we introduce a new programming technique called Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both CUDA and CuTile, observing 50\\% or greater reduction in L2 misses and up to 60\\% increase in throughput on GB10.", "AI": {"tldr": "本文介绍了一种名为Sawtooth Wavefront Reordering的新技术，旨在提高CuTile FlashAttention在NVIDIA GB10上的性能，并通过减少L2缓存缺失来实现这一目标。", "motivation": "高性能注意力内核对于大型语言模型至关重要。本文旨在分析CuTile Flash Attention的内存行为并改善其缓存性能，尤其是在NVIDIA GB10上识别主要的L2缓存缺失原因。", "method": "通过对NVIDIA GB10（Grace Blackwell）上的CuTile Flash Attention进行详细分析，提出了一种名为Sawtooth Wavefront Reordering的新编程技术以减少L2缓存缺失。", "result": "实验结果显示，在CUDA和CuTile中验证了该技术的有效性，减少了50%以上的L2缓存缺失，并在GB10上实现了最多60%的吞吐量提升。", "conclusion": "Sawtooth Wavefront Reordering是一种有效的编程技术，能够显著提高NVIDIA GB10上的CuTile FlashAttention性能，通过减少L2缓存缺失来实现更高的吞吐量。"}}
{"id": "2601.16027", "pdf": "https://arxiv.org/pdf/2601.16027", "abs": "https://arxiv.org/abs/2601.16027", "authors": ["Yiran Qiao", "Xiang Ao", "Jing Chen", "Yang Liu", "Qiwei Zhong", "Qing He"], "title": "Deja Vu in Plots: Leveraging Cross-Session Evidence with Retrieval-Augmented LLMs for Live Streaming Risk Assessment", "categories": ["cs.AI"], "comment": null, "summary": "The rise of live streaming has transformed online interaction, enabling massive real-time engagement but also exposing platforms to complex risks such as scams and coordinated malicious behaviors. Detecting these risks is challenging because harmful actions often accumulate gradually and recur across seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session Evidence-Aware Retrieval-Augmented Detector) for live streaming risk assessment. In CS-VAR, a lightweight, domain-specific model performs fast session-level risk inference, guided during training by a Large Language Model (LLM) that reasons over retrieved cross-session behavioral evidence and transfers its local-to-global insights to the small model. This design enables the small model to recognize recurring patterns across streams, perform structured risk assessment, and maintain efficiency for real-time deployment. Extensive offline experiments on large-scale industrial datasets, combined with online validation, demonstrate the state-of-the-art performance of CS-VAR. Furthermore, CS-VAR provides interpretable, localized signals that effectively empower real-world moderation for live streaming.", "AI": {"tldr": "本文提出了一种名为CS-VAR的跨会话证据感知检索增强检测器，用于实时直播风险评估。", "motivation": "随着实时直播平台的增长，复杂的如诈骗和恶意行为的风险也随之增加。这些风险往往在不同直播流中逐渐积累并重复出现，难以检测。", "method": "CS-VAR采用了轻量级领域特定模型进行快速会话级别风险推理，并利用大型语言模型（LLM）来分析检索到的跨会话行为证据，并将局部到全局的风险见解传递给小型模型。", "result": "在大规模工业数据集上的离线实验和在线验证中，CS-VAR展示了最先进的性能。它还提供了可解释性的、局部化的信号，有效地增强了实时直播的审核能力。", "conclusion": "通过结合轻量级模型和大型语言模型，CS-VAR成功地实现了跨会话风险检测，并且能够高效地部署在实时环境中进行风险评估。"}}
{"id": "2601.16024", "pdf": "https://arxiv.org/pdf/2601.16024", "abs": "https://arxiv.org/abs/2601.16024", "authors": ["Rongze Ma", "Mengkang Lu", "Zhenyu Xiang", "Yongsheng Pan", "Yicheng Wu", "Qingjie Zeng", "Yong Xia"], "title": "PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry", "categories": ["cs.CV"], "comment": null, "summary": "Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.", "AI": {"tldr": "本文提出了一种名为PAINT的视觉自回归框架，用于从常规H&E图像中合成虚拟免疫组化染色模式。", "motivation": "传统物理染色成本高且耗材多，而现有方法在直接外观合成中常出现语义不一致的问题。因此，本研究旨在通过结构引导的自回归模型提高生成的结构性保真度和临床任务性能。", "method": "PAINT框架将合成过程重新定义为一个以结构为主的条件生成任务，引入了空间结构起始图（3S-Map），确保初始化基于观察到的形态学特征，从而实现确定性和空间对齐的合成。", "result": "实验结果显示，相较于现有方法，PAINT在结构保真度和临床下游任务中表现出色，在IHC4BC和MIST数据集上验证了其优越性。", "conclusion": "研究证明了以结构为导向的自回归模型在虚拟免疫组化染色合成中的潜在价值，并展示了该方法在提高结构性保真度和改进临床应用方面的能力。"}}
{"id": "2601.16023", "pdf": "https://arxiv.org/pdf/2601.16023", "abs": "https://arxiv.org/abs/2601.16023", "authors": ["Lalaram Arya", "Mrinmoy Bhattacharjee", "Adarsh C. R.", "S. R. Mahadeva Prasanna"], "title": "Timbre-Aware LLM-based Direct Speech-to-Speech Translation Extendable to Multiple Language Pairs", "categories": ["eess.AS", "cs.HC"], "comment": "13 pages", "summary": "Direct Speech-to-Speech Translation (S2ST) has gained increasing attention for its ability to translate speech from one language to another, while reducing error propagation and latency inherent in traditional cascaded pipelines. However, existing direct S2ST systems continue to face notable challenges, including instability in semantic-acoustic alignment when parallel speech data is scarce, difficulty in preserving speaker identity, and limited multilingual scalability. In this work, we introduce DS2ST-LM, a scalable, single-stage direct S2ST framework leveraging a multilingual Large Language Model (LLM). The architecture integrates a Whisper speech encoder, a learnable projection module, a Qwen2-0.5B LLM, and a timbre-controlled vocoder. We construct GigaS2S-1000, a 1000-hour bilingual corpus by extending the GigaST dataset with high-fidelity synthetic target speech, and show that this synthetic data alleviates data scarcity to some extent. We investigate two semantic token generation strategies: speech-derived S3 tokens and text-derived tokens generated by a pre-trained LLM, and analyze their impact on training stability and semantic consistency. We further evaluate three projection architectures (Linear, Conv1D-Linear, and Q-Former) and observe that while higher-capacity projectors converge faster, the simple Linear projector achieves higher performance. Extensive experiments demonstrate that DS2ST-LM outperforms traditional cascaded and ST (Qwen-Audio) + TTS baselines across both lexical (BLEU, METEOR) and semantic (BLEURT, COMET) metrics, while extending to multiple language pairs, including French, Spanish, German, Hindi, Bengali, and Urdu. Furthermore, we incorporate timbre-aware speech synthesis to preserve speaker information, enabling DS2ST-LM to surpass prior direct S2ST systems in both speaker similarity and perceptual naturalness.", "AI": {"tldr": "本文介绍了DS2ST-LM，一种利用多语言大语言模型的可扩展单阶段直接语音到语音翻译框架。", "motivation": "传统的级联管道在语音翻译时存在误差传播和延迟问题。现有直接S2ST系统还面临语义-声学对齐不稳定、难以保留说话人身份以及多语言拓展能力有限等挑战。", "method": "DS2ST-LM架构包括Whisper语音编码器、可学习的投影模块、Qwen2-0.5B大语言模型和带音色控制的声码器。研究了两种语义标记生成策略，并评估了几种不同的投影架构，以提高训练稳定性和性能。", "result": "实验结果表明，DS2ST-LM在词法和语义指标上超过了传统级联系统和基准模型，并且拓展到多种语言对，在保留说话人信息方面也表现出色。", "conclusion": "DS2ST-LM通过多语言大语言模型实现了高效的直接S2ST，解决了现有系统的多个挑战，展示了良好的跨语言泛化能力和音质表现。"}}
{"id": "2601.16020", "pdf": "https://arxiv.org/pdf/2601.16020", "abs": "https://arxiv.org/abs/2601.16020", "authors": ["Weichen Dai", "Wenhan Su", "Da Kong", "Yuhang Ming", "Wanzeng Kong"], "title": "Keyframe-Based Feed-Forward Visual Odometry", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.", "AI": {"tldr": "本文提出了一种基于关键帧的前馈视觉里程计方法，使用强化学习来适应性地选择关键帧。", "motivation": "传统的视觉里程计利用关键帧提高效率和准确性，但目前基于基础模型的方法直接处理原始图像序列，存在计算冗余和性能下降的问题。因此，需要将传统几何启发式与当前的前馈方法结合。", "method": "作者提出一种新的基于关键帧的前馈视觉里程计方法，通过强化学习来数据驱动地生成适应性关键帧策略，以匹配基础模型内在特性，并在TartanAir数据集上训练该代理。", "result": "实验结果表明，所提方法在多个真实世界数据集中实现了显著优于当前最优前馈视觉里程计方法的性能提升。", "conclusion": "该研究成功地将关键帧概念与现代基础模型融合，并通过强化学习优化了关键帧选择策略，在保持高效率的同时提高了位置估计精度。"}}
{"id": "2601.16011", "pdf": "https://arxiv.org/pdf/2601.16011", "abs": "https://arxiv.org/abs/2601.16011", "authors": ["Theodor Forgaard", "Jarle H. Reksten", "Anders U. Waldeland", "Valerio Marsocci", "Nicolas Longépé", "Michael Kampffmeyer", "Arnt-Børre Salberg"], "title": "THOR: A Versatile Foundation Model for Earth Observation Climate and Society Applications", "categories": ["eess.IV", "cs.AI"], "comment": "25 pages", "summary": "Current Earth observation foundation models are architecturally rigid, struggle with heterogeneous sensors and are constrained to fixed patch sizes. This limits their deployment in real-world scenarios requiring flexible computeaccuracy trade-offs. We propose THOR, a \"computeadaptive\" foundation model that solves both input heterogeneity and deployment rigidity. THOR is the first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI & SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a single model. We pre-train THOR with a novel randomized patch and input image size strategy. This allows a single set of pre-trained weights to be deployed at inference with any patch size, enabling a dynamic trade-off between computational cost and feature resolution without retraining. We pre-train THOR on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate state-of-the-art performance on downstream benchmarks, particularly in data-limited regimes like the PANGAEA 10% split, validating that THOR's flexible feature generation excels for diverse climate and society applications.", "AI": {"tldr": "本文提出了THOR，一个适应性计算的地球观测基础模型，能够处理来自不同传感器的数据，并支持动态调整计算成本和特征分辨率。", "motivation": "现有的地球观测基础模型存在架构刚性、难以处理异构传感器数据以及固定补丁尺寸的问题，这些问题限制了它们在真实场景中的应用。", "method": "THOR是第一个统一处理Copernicus Sentinel-1, -2 和 -3 (OLCI & SLSTR)卫星数据的架构，并采用新颖的随机化补丁和输入图像尺寸策略进行预训练，允许使用单一预训练权重集部署于不同补丁大小。", "result": "THOR在THOR Pretrain数据集上进行了预训练并在下游基准测试中展示了最先进的性能，在数据受限的情况下表现尤为出色。", "conclusion": "研究表明，THOR的灵活性和适应性使其成为地球观测、气候和社会应用的理想选择。"}}
{"id": "2601.16007", "pdf": "https://arxiv.org/pdf/2601.16007", "abs": "https://arxiv.org/abs/2601.16007", "authors": ["Chak-Wing Mak", "Guanyu Zhu", "Boyi Zhang", "Hongji Li", "Xiaowei Chi", "Kevin Zhang", "Yichen Wu", "Yangfan He", "Chun-Kai Fan", "Wentao Lu", "Kuangzhi Ge", "Xinyu Fang", "Hongyang He", "Kuan Lu", "Tianxiang Xu", "Li Zhang", "Yongxin Ni", "Youhua Li", "Shanghang Zhang"], "title": "PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.", "AI": {"tldr": "本文介绍了PhysicsMind，一个用于评估基础多模态大型语言模型和视频世界模型在物理推理和预测方面能力的统一基准。", "motivation": "当前的研究表明，尽管现代的基础多模态大语言模型和视频世界模型在数学、常识和视觉推理方面取得了显著进展，但在物理规律方面的理解仍不足。现有的一些评估这些领域的指标依赖于合成的数据或只关注感知视频质量，并不能很好地衡量视频是否遵守物理定律。", "method": "PhysicsMind通过使用真实和模拟环境来测试三种基本的物理学原则：质心、杠杆平衡和牛顿第一定律，包括视觉问题回答任务（VQA）和视频生成任务（VG）。", "result": "评估结果显示，现有的模型在处理物理推理时依赖于外观上的启发式方法，并经常违反基础力学原理。这表明当前的训练规模和技术仍然不足以实现稳健的物理理解。", "conclusion": "PhysicsMind作为一个专注于测试具有物理学意识的多模态模型的平台，揭示了现有模型在物理理解方面的局限性。"}}
{"id": "2601.16004", "pdf": "https://arxiv.org/pdf/2601.16004", "abs": "https://arxiv.org/abs/2601.16004", "authors": ["Christopher Altman"], "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware", "categories": ["quant-ph", "cs.ET"], "comment": "11 pages, 6 figures. Includes reproducibility code and archived data release", "summary": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts. Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise. This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.", "AI": {"tldr": "本文在IBM Quantum硬件上实现了Violaris提出的电路家族，用于估计操作性的分支间通信见证者。", "motivation": "动机在于通过实验验证量子系统中分支间通信的存在性，并评估实际设备噪声和编译约束下的行为。", "method": "使用五量子比特的协议实现一个内部寄存器消息传输模式，编码依赖于控制量子比特的观察子系统的分支条件演化，随后进行探索单个电路内条件测量上下文的相关性的受控转移操作。", "result": "在ibm_fez后端执行20000次实验，观察到基于人口统计学可见度为0.877，沿正交轴的相干性见证者分别为0.840和-0.811，相位敏感量约为1.17。", "conclusion": "本研究提供了一个可重复的操作约束管道，用于评估相对于校准设备噪声非理想信道的检测能力。"}}
{"id": "2601.15995", "pdf": "https://arxiv.org/pdf/2601.15995", "abs": "https://arxiv.org/abs/2601.15995", "authors": ["Liang Wang", "Kanzhong Yao", "Yang Liu", "Weikai Qin", "Jun Wu", "Zhe Sun", "Qiuguo Zhu"], "title": "PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot's real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA's exceptional agility and robustness in challenging scenarios.", "AI": {"tldr": "本文介绍了PUMA，一种集成了视觉感知和立足点优先级的端到端学习框架，用于提高四足机器人在障碍穿越任务中的敏捷性和鲁棒性。", "motivation": "人类运动员能够有效识别环境特征来选择合适的立足点进行障碍穿越，而赋予腿部机器人类似的感知推理能力仍是一个重要挑战。现有的方法通常依赖于层次控制器，使用预计算的立足点约束了机器人的实时适应能力和强化学习的探索潜力。为了克服这些挑战，本文提出了PUMA框架。", "method": "PUMA是一种端到端的学习框架，它将视觉感知和立足点优先级整合成一个训练过程。该方法利用地形特征来估计以自我为中心的极坐标立足点优先级，包括相对距离和航向角，指导机器人进行主动姿态适应，适用于各种障碍穿越任务。", "result": "在模拟环境和真实世界的各种复杂地形中进行了广泛的实验，证明了PUMA框架在挑战性场景中的卓越敏捷性和鲁棒性。", "conclusion": "实验证明，PUMA能够显著提升四足机器人在执行Parkour任务时的适应能力和性能表现，显示出其作为未来研究和开发的重要潜力。"}}
{"id": "2601.15968", "pdf": "https://arxiv.org/pdf/2601.15968", "abs": "https://arxiv.org/abs/2601.15968", "authors": ["Xin Xie", "Jiaxian Guo", "Dong Gong"], "title": "HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.", "AI": {"tldr": "本文提出HyperAlign，一种基于超网络的高效测试时间对齐框架，用于改进扩散模型生成输出与人类偏好的一致性。", "motivation": "现有方法在提高扩散模型生成质量时面临多样性和计算效率之间的权衡问题。为了克服这些局限，作者提出了一个新颖的方法来改善模型生成结果的语义一致性和视觉吸引力。", "method": "HyperAlign通过训练超网络动态生成低秩适应权重，以调整扩散模型的生成操作符，而不是修改潜在状态。这种方法可以在输入潜势、时间步长和提示的基础上自适应地调节去噪轨迹。", "result": "实验表明，在Stable Diffusion和FLUX等多种扩展生成范式中，HyperAlign显著优于现有的微调和测试时缩放基线方法，提高了语义一致性和视觉吸引力。", "conclusion": "研究证明了HyperAlign在提高扩散模型生成输出质量方面的有效性，并提出了平衡性能与效率的变体。"}}
{"id": "2601.15953", "pdf": "https://arxiv.org/pdf/2601.15953", "abs": "https://arxiv.org/abs/2601.15953", "authors": ["Yongyi Wang", "Hanyu Liu", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "Decoupling Return-to-Go for Efficient Decision Transformer", "categories": ["cs.AI"], "comment": null, "summary": "The Decision Transformer (DT) has established a powerful sequence modeling approach to offline reinforcement learning. It conditions its action predictions on Return-to-Go (RTG), using it both to distinguish trajectory quality during training and to guide action generation at inference. In this work, we identify a critical redundancy in this design: feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction. We show that this redundancy can impair DT's performance through experiments. To resolve this, we propose the Decoupled DT (DDT). DDT simplifies the architecture by processing only observation and action sequences through the Transformer, using the latest RTG to guide the action prediction. This streamlined approach not only improves performance but also reduces computational cost. Our experiments show that DDT significantly outperforms DT and establishes competitive performance against state-of-the-art DT variants across multiple offline RL tasks.", "AI": {"tldr": "本文提出了Decoupled DT（DDT）方法，通过简化Decision Transformer的架构来提高离线强化学习的效率。", "motivation": "识别并解决Decision Transformer在使用Return-to-Go时存在的冗余问题，以提升模型性能和降低计算成本。", "method": "提出Decoupled DT（DDT），仅处理观察和动作序列，并使用最新的RTG指导行动预测。", "result": "实验表明DDT显著优于传统的DT，并且在多个离线强化学习任务中展现出与最先进的DT变体竞争的性能。", "conclusion": "通过减少冗余，DDT不仅提升了模型性能，还降低了计算成本，证明了其作为改进方法的有效性。"}}
{"id": "2601.15951", "pdf": "https://arxiv.org/pdf/2601.15951", "abs": "https://arxiv.org/abs/2601.15951", "authors": ["Sheng Miao", "Sijin Li", "Pan Wang", "Dongfeng Bai", "Bingbing Liu", "Yue Wang", "Andreas Geiger", "Yiyi Liao"], "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.", "AI": {"tldr": "提出EVolSplat4D，一种高效的体积基础高斯点合成框架，用于四维城市场景综合。", "motivation": "现有方法在重建时间和质量之间难以平衡，而神经辐射场和3D高斯点合成的方法虽然实现了照片级真实度，但依赖耗时的每场景优化。新兴的前馈方法通常采用像素基础高斯表示，这导致复杂动态环境中多视图预测聚合时出现3D不一致性。", "method": "EVolSplat4D框架通过三个专有分支统一了体积基础和像素基础的高斯点预测。对于近距离静态区域，直接从3D特征体中预测3D高斯点的一致几何结构，并结合语义增强图像渲染模块预测其外观。对于动态物体，使用基于对象中心的标准空间和调整后的运动渲染模块聚合时间特性。远处场景通过高效的像素基础高斯分支处理以确保全场景覆盖。", "result": "在KITTI-360、KITTI、Waymo和PandaSet数据集上的实验表明，EVolSplat4D能够更准确且一致地重建静态和动态环境，超越了现有的每场景优化和前沿前馈基线方法。", "conclusion": "EVolSplat4D框架有效解决了现有方法在四维城市场景合成中面临的挑战，并展示了其在保持高质量的同时减少了重建时间的优势。"}}
{"id": "2601.15949", "pdf": "https://arxiv.org/pdf/2601.15949", "abs": "https://arxiv.org/abs/2601.15949", "authors": ["Yiran Wang", "Shuoyuan Wang", "Zhaoran Wei", "Jiannan Zhao", "Zhonghua Yao", "Zejian Xie", "Songxin Zhang", "Jun Huang", "Bingyi Jing", "Hongxin Wei"], "title": "Natural Language-Driven Global Mapping of Martian Landforms", "categories": ["cs.AI", "astro-ph.IM"], "comment": null, "summary": "Planetary surfaces are typically analyzed using high-level semantic concepts in natural language, yet vast orbital image archives remain organized at the pixel level. This mismatch limits scalable, open-ended exploration of planetary surfaces. Here we present MarScope, a planetary-scale vision-language framework enabling natural language-driven, label-free mapping of Martian landforms. MarScope aligns planetary images and text in a shared semantic space, trained on over 200,000 curated image-text pairs. This framework transforms global geomorphic mapping on Mars by replacing pre-defined classifications with flexible semantic retrieval, enabling arbitrary user queries across the entire planet in 5 seconds with F1 scores up to 0.978. Applications further show that it extends beyond morphological classification to facilitate process-oriented analysis and similarity-based geomorphological mapping at a planetary scale. MarScope establishes a new paradigm where natural language serves as a direct interface for scientific discovery over massive geospatial datasets.", "AI": {"tldr": "该论文介绍了一种名为MarScope的行星级视觉语言框架，它能够通过自然语言驱动对火星地貌进行无标签映射。", "motivation": "现有的行星表面分析依赖于高级语义概念和自然语言，但轨道图像档案通常是按像素组织的，这种不匹配限制了行星表面的大规模、开放性探索。", "method": "MarScope将行星图像和文本对齐在一个共享的语义空间中，该框架通过20多万个经过策划的图像-文本对进行训练，并实现了灵活的语义检索。", "result": "该系统能够在5秒内处理任意用户查询，并且在火星上整个星球范围内的F1分数高达0.978。MarScope不仅能够用于形态分类，还能促进过程导向分析和基于相似性的地貌映射。", "conclusion": "MarScope为大规模地理空间数据的科学发现提供了一种新的范式，其中自然语言直接作为用户与行星级图像档案交互的接口。"}}
{"id": "2601.15946", "pdf": "https://arxiv.org/pdf/2601.15946", "abs": "https://arxiv.org/abs/2601.15946", "authors": ["Zijie Chen", "Xiaowei Liu", "Yong Xu", "Shenghai Yuan", "Jianping Li", "Lihua Xie"], "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems", "categories": ["cs.RO"], "comment": "This article has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Personal use is permitted. All other uses require IEEE permission", "summary": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}", "AI": {"tldr": "本文提出了一种无目标LiDAR-电机校准（LM-Calibr）和环境自适应LiDAR惯性里程计（EVA-LIO），以提高旋转驱动LiDAR系统的准确性和鲁棒性。", "motivation": "现有方法在不同安装配置下需要参数化外参，限制了其通用性。此外，旋转驱动的激光雷达不可避免地会扫描无特征区域，增加了覆盖范围和定位鲁棒性的平衡复杂度。", "method": "基于Denavit-Hartenberg约定提出了一种无目标LiDAR-电机校准方法LM-Calibr，并开发了一种环境自适应LiDAR惯性里程计EVA-LIO。EVA-LIO根据空间尺度自适应选择下采样率和地图分辨率。", "result": "实验表明，LM-Calibr在不同场景、安装角度及初始值条件下具有准确性和收敛性。EVA-LIO能提高扫描完整度并确保鲁棒定位，即使LiDAR短暂扫描无特征区域。", "conclusion": "本文提出的方法提高了旋转驱动LiDAR系统的校准和定位性能，增强了其在不同环境下的适应性和可靠性。"}}
{"id": "2601.15931", "pdf": "https://arxiv.org/pdf/2601.15931", "abs": "https://arxiv.org/abs/2601.15931", "authors": ["Xiangyu Wang", "Zhixin Lv", "Yongjiao Sun", "Anrui Han", "Ye Yuan", "Hangxu Ji"], "title": "ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors for Text-Based Person Search", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Text-Based Person Search (TBPS) holds unique value in real-world surveillance bridging visual perception and language understanding, yet current paradigms utilizing pre-training models often fail to transfer effectively to complex open-world scenarios. The reliance on \"Passive Observation\" leads to multifaceted spurious correlations and spatial semantic misalignment, causing a lack of robustness against distribution shifts. To fundamentally resolve these defects, this paper proposes ICON (Invariant Counterfactual Optimization with Neuro-symbolic priors), a framework integrating causal and topological priors. First, we introduce Rule-Guided Spatial Intervention to strictly penalize sensitivity to bounding box noise, forcibly severing location shortcuts to achieve geometric invariance. Second, Counterfactual Context Disentanglement is implemented via semantic-driven background transplantation, compelling the model to ignore background interference for environmental independence. Then, we employ Saliency-Driven Semantic Regularization with adaptive masking to resolve local saliency bias and guarantee holistic completeness. Finally, Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to constrain feature matching, ensuring activated regions are topologically consistent with human structural logic. Experimental results demonstrate that ICON not only maintains leading performance on standard benchmarks but also exhibits exceptional robustness against occlusion, background interference, and localization noise. This approach effectively advances the field by shifting from fitting statistical co-occurrences to learning causal invariance.", "AI": {"tldr": "本文提出ICON框架，用于解决基于文本的人物搜索任务中存在的鲁棒性问题。", "motivation": "当前方法在处理复杂开放世界场景时表现出不足，特别是在分布变化和环境干扰方面存在局限性。", "method": "ICON通过引入规则引导的空间干预、反事实背景解耦、显着性驱动的语义正则化以及神经符号拓扑对齐来实现几何不变性和环境独立性。", "result": "实验结果表明，ICON在标准基准测试中保持了领先性能，并且对遮挡、背景干扰和定位噪声表现出色的鲁棒性。", "conclusion": "该方法通过从拟合统计共现到学习因果不变性的转变有效推进了领域发展。"}}
{"id": "2601.15930", "pdf": "https://arxiv.org/pdf/2601.15930", "abs": "https://arxiv.org/abs/2601.15930", "authors": ["Tianjun Wei", "Enneng Yang", "Yingpeng Du", "Huizhong Guo", "Jie Zhang", "Zhu Sun"], "title": "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging", "categories": ["cs.IR", "cs.AI"], "comment": "https://github.com/Joinn99/MMGRid", "summary": "Model merging (MM) offers an efficient mechanism for integrating multiple specialized models without access to original training data or costly retraining. While MM has demonstrated success in domains like computer vision, its role in recommender systems (RSs) remains largely unexplored. Recently, Generative Recommendation (GR) has emerged as a new paradigm in RSs, characterized by rapidly growing model scales and substantial computational costs, making MM particularly appealing for cost-sensitive deployment scenarios. In this work, we present the first systematic study of MM in GR through a contextual lens. We focus on a fundamental yet underexplored challenge in real-world: how to merge generative recommenders specialized to different real-world contexts, arising from temporal evolving user behaviors and heterogeneous application domains. To this end, we propose a unified framework MMGRid, a structured contextual grid of GR checkpoints that organizes models trained under diverse contexts induced by temporal evolution and domain diversity. All checkpoints are derived from a shared base LLM but fine-tuned on context-specific data, forming a realistic and controlled model space for systematically analyzing MM across GR paradigms and merging algorithms. Our investigation reveals several key insights. First, training GR models from LLMs can introduce parameter conflicts during merging due to token distribution shifts and objective disparities; such conflicts can be alleviated by disentangling task-aware and context-specific parameter changes via base model replacement. Second, incremental training across contexts induces recency bias, which can be effectively balanced through weighted contextual merging. Notably, we observe that optimal merging weights correlate with context-dependent interaction characteristics, offering practical guidance for weight selection in real-world deployments.", "AI": {"tldr": "本文介绍了MMGRid框架，用于跨领域和时间感知的生成推荐系统中的模型合并。", "motivation": "研究动机在于探索在生成推荐系统中应用模型合并技术来解决训练成本高昂的问题，并应对用户行为随时间和不同应用场景变化所带来的挑战。", "method": "提出MMGRid框架，通过一个结构化的上下文网格组织不同的生成推荐模型检查点，在此基础上分析了模型合并方法及其算法。", "result": "研究结果表明，参数冲突可以通过替换基线模型来缓解，增量训练中的近因偏差可通过加权的上下文合并来平衡，最优合并权重与交互特征相关。", "conclusion": "结论指出，MMGRid框架为生成推荐系统的成本敏感部署提供了实用指导，并揭示了不同情境下的最佳实践。"}}
{"id": "2601.15929", "pdf": "https://arxiv.org/pdf/2601.15929", "abs": "https://arxiv.org/abs/2601.15929", "authors": ["Liuyun Jiang", "Yizhuo Lu", "Yanchao Zhang", "Jiazheng Liu", "Hua Han"], "title": "NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.", "AI": {"tldr": "本文提出了NeuroMamba，一种用于神经元分割的多视角特征交互框架。", "motivation": "现有的基于CNN和Transformer的方法在处理神经元形态不规则、结构密集交织的问题时存在局限性，无法有效捕捉长距离依赖关系或保持细粒度细节。", "method": "NeuroMamba结合了Visual Mamba的全局建模能力和局部特征建模方法，设计了边界判别特征提取器（BDFE）和空间连续特征提取器（SCFE），并通过交叉调制机制融合多视角特征。", "result": "该方法在四个公共电镜数据集上展示了最先进的性能，并验证了其对各向同性和非各向同性分辨率的优越适应能力。", "conclusion": "NeuroMamba通过结合全局建模和局部细节捕捉，显著提高了神经元分割的质量和精确度。"}}
{"id": "2601.15924", "pdf": "https://arxiv.org/pdf/2601.15924", "abs": "https://arxiv.org/abs/2601.15924", "authors": ["Brainard Philemon Jagati", "Jitendra Tembhurne", "Harsh Goud", "Rudra Pratap Singh", "Chandrashekhar Meshram"], "title": "Class Confidence Aware Reweighting for Long Tailed Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 3 figures, IEEE Transaction on Neural Networks and Learning Systems (Submitted)", "summary": "Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.", "AI": {"tldr": "本文提出了一种基于类和信心感知重新加权方案，用于解决长尾数据分布中类别不平衡问题。", "motivation": "在长尾数据分布下，深度神经网络模型性能显著下降，因为大部分训练数据被少数头部类占据，而尾部类别的训练样本较少，现有方法主要集中在决策空间的调整上，对优化过程中信心差异的影响关注不足。", "method": "本文提出了一种基于损失级别的重新加权方案，通过Ω(p_t, f_c)函数根据预测的信心值和对应类别出现频率来调整个别样例在训练中的贡献度。", "result": "实验结果表明，在CIFAR-100-LT、ImageNet-LT和iNaturalist2018数据集上，该方案在不同失衡程度下均表现出了显著的改进效果，验证了理论讨论的有效性。", "conclusion": "研究表明基于信心值和类别频率重新加权的方法可以有效改善长尾学习中的性能下降问题，并且具有补充现有方法（调整logits）的作用。"}}
{"id": "2601.15918", "pdf": "https://arxiv.org/pdf/2601.15918", "abs": "https://arxiv.org/abs/2601.15918", "authors": ["Valery Fischer", "Alan Magdaleno", "Anna-Katharina Calek", "Nicola Cavalcanti", "Nathan Hoffman", "Christoph Germann", "Joschua Wüthrich", "Max Krähenmann", "Mazda Farshad", "Philipp Fürnstahl", "Lilian Calvet"], "title": "A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training. Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity. Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error. Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.", "AI": {"tldr": "本研究提出了一种用于手术环境下三维手部姿态估计的多视角管道，并建立了一个包含超过68000帧和3000个手动标注二维手部姿态的新基准数据集。", "motivation": "准确的三维手部姿势估计支持外科应用如技能评估、机器人辅助干预和几何感知工作流程分析，但手术环境具有强烈的局部照明、频繁遮挡以及由于手套导致的手部外观均匀等问题，并且缺乏可靠模型训练所需的标注数据集。", "method": "研究提出了一种在手术环境中无需特定领域微调的鲁棒多视角管道，依赖于现成的预训练模型，包括可靠的人员检测、全身姿态估计和最先进的二维手关键点预测以及受限三维优化。", "result": "定量实验表明该方法优于基线模型，实现了2D关节平均误差减少31%和3D均方根位置误差减少76%。", "conclusion": "本工作为手术中的三维手部姿态估计建立了一个强有力的基线，提供了一个无需训练的管道以及一个综合注释数据集，以促进未来外科计算机视觉研究的发展。"}}
{"id": "2601.15915", "pdf": "https://arxiv.org/pdf/2601.15915", "abs": "https://arxiv.org/abs/2601.15915", "authors": ["Chen Xu"], "title": "Progressive Power Homotopy for Non-convex Optimization", "categories": ["math.OC", "cs.AI", "cs.LG"], "comment": ":65K05; 68T07; 90C30; 68W40", "summary": "We propose a novel first-order method for non-convex optimization of the form $\\max_{\\bm{w}\\in\\mathbb{R}^d}\\mathbb{E}_{\\bm{x}\\sim\\mathcal{D}}[f_{\\bm{w}}(\\bm{x})]$, termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic gradient ascent to a surrogate objective obtained by first performing a power transformation and then Gaussian smoothing, $F_{N,σ}(\\bmμ):=\\mathbb{E}_{\\bm{w}\\sim\\mathcal{N}(\\bmμ,σ^2I_d),\\bm{x}\\sim\\mathcal{D}}[e^{Nf_w(\\bm{x})}]$, while progressively increasing the power parameter $N$ and decreasing the smoothing scale $σ$ along the optimization trajectory. We prove that, under mild regularity conditions, Prog-PowerHP converges to a small neighborhood of the global optimum with an iteration complexity scaling nearly as $O(d^2\\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear advantages in phase retrieval when the samples-to-dimension ratio approaches the information-theoretic limit, and in training two-layer neural networks in under-parameterized regimes. These results suggest that Prog-PowerHP is particularly effective for navigating cluttered non-convex landscapes where standard first-order methods struggle.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.15914", "pdf": "https://arxiv.org/pdf/2601.15914", "abs": "https://arxiv.org/abs/2601.15914", "authors": ["Yarin Benyamin"], "title": "The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars", "categories": ["cs.CV", "cs.HC"], "comment": "Technical Report benchmarking off-the-shelf CV latencies on commodity CPU hardware for therapeutic VR applications", "summary": "In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a \"Latency Wall\" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.", "AI": {"tldr": "评估用于实时虚拟角色情感识别的现成深度学习模型，以满足严格的时间限制和准确性要求。", "motivation": "在VR和HCI领域，实时情感识别技术有潜力帮助自闭症患者提高社交技能。然而，大多数现成的深度学习模型更注重准确性而不是时间效率，本研究旨在评估这些模型是否能满足严格的延时要求，以实现可访问的虚拟现实疗法。", "method": "使用UIBVFED数据集对SOTA面部表情识别模型进行了基准测试，包括YOLO（v8, v11, v12）和CLIP、SigLIP、ViT-FER等通用视觉变换器。评估了这些模型在CPU上的推理性能。", "result": "研究显示，在实时循环中使用现成的深度学习模型存在“延迟墙”。YOLOv11n架构提供了最佳检测平衡（约54毫秒），但CLIP和SigLIP等通用变压器无法实现可接受的速度或准确性。", "conclusion": "该研究表明，为了实现实时AI在治疗环境中的应用，需要开发轻量级、特定领域的体系结构。"}}
{"id": "2601.15912", "pdf": "https://arxiv.org/pdf/2601.15912", "abs": "https://arxiv.org/abs/2601.15912", "authors": ["Ariyan Bighashdel", "Kevin Sebastian Luck"], "title": "TeNet: Text-to-Network for Compact Policy Synthesis", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.", "AI": {"tldr": "提出TeNet框架，从自然语言描述中生成紧凑的任务特定机器人策略。", "motivation": "解决机器人遵循自然语言指令时面临的高阶规划或依赖难以实时部署的大模型的问题。", "method": "使用超网络对由预训练大语言模型产生的文本嵌入进行条件处理，以生成完全可执行的政策。", "result": "在MuJoCo和Meta-World基准测试中，TeNet生成的策略比基于序列的基线小几个数量级，同时在多任务和元学习设置中表现出色，并支持高频控制。", "conclusion": "文本条件下的超网络为资源受限且有实时要求的机器人控制任务提供了构建紧凑的语言驱动控制器的实际方法。"}}
{"id": "2601.15909", "pdf": "https://arxiv.org/pdf/2601.15909", "abs": "https://arxiv.org/abs/2601.15909", "authors": ["Soufiane Jhilal", "Stéphanie Martin", "Anne-Lise Giraud"], "title": "Transfer Learning from ImageNet for MEG-Based Decoding of Imagined Speech", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at IEEE ISBI 2026", "summary": "Non-invasive decoding of imagined speech remains challenging due to weak, distributed signals and limited labeled data. Our paper introduces an image-based approach that transforms magnetoencephalography (MEG) signals into time-frequency representations compatible with pretrained vision models. MEG data from 21 participants performing imagined speech tasks were projected into three spatial scalogram mixtures via a learnable sensor-space convolution, producing compact image-like inputs for ImageNet-pretrained vision architectures. These models outperformed classical and non-pretrained models, achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs. silent reading, and 60.6% for vowel decoding. Cross-subject evaluation confirmed that pretrained models capture shared neural representations, and temporal analyses localized discriminative information to imagery-locked intervals. These findings show that pretrained vision models applied to image-based MEG representations can effectively capture the structure of imagined speech in non-invasive neural signals.", "AI": {"tldr": "本文介绍了将预先训练好的图像识别模型应用于非侵入性脑电图信号的转化，以提高想象言语解码的准确性。", "motivation": "由于弱信号和标记数据有限，非侵入性的想象言语解码是一个挑战。为了克服这些困难，研究者提出了利用预训练视觉模型来分析MEG信号的新方法。", "method": "通过可学习的传感器空间卷积将来自21名参与者的MEG信号转换成与预训练视觉模型兼容的时间-频率表示，产生了类似图像的输入。", "result": "使用这种新方法，预先训练好的视觉架构在想象言语解码中达到了最高90.4%平衡准确率（相较于沉默），81.0%（相较于无声阅读）和60.6%（元音解码）。交叉受试者评估表明，预训练模型捕捉到了共享的神经表征。", "conclusion": "研究表明，应用于基于图像的MEG表示中的预先训练视觉模型能够有效地识别非侵入性神经信号中想象言语的结构。"}}
{"id": "2601.15906", "pdf": "https://arxiv.org/pdf/2601.15906", "abs": "https://arxiv.org/abs/2601.15906", "authors": ["Zhen Zhang", "Runhao Zeng", "Sicheng Zhao", "Xiping Hu"], "title": "Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\\texttt{gate\\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \\texttt{gate\\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\\% of the parameters tuned by AffectGPT, our approach achieves 96.6\\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \\texttt{gate\\_proj} as a central architectural locus of affective modeling.", "AI": {"tldr": "本文研究了大规模多模态基础模型中的情感建模机制，揭示了情感适应主要集中在前馈门控投射（gate_proj）模块上。", "motivation": "理解大型基础模型中情绪的表示方式是一个开放性问题，尤其是在多模态情感设置下。尽管近期的情感模型表现出色，但支持情感理解和生成的内部架构机制仍不明确。", "method": "通过多种架构、训练策略和情感任务分析了以情感为导向的监督如何重塑内部模型参数，并采用受控模块传输、单模块适应和破坏性消融等方法进行研究。", "result": "结果显示，情感适应主要集中在前馈门控投射（gate_proj）上，并且仅通过调整大约24.5%的AffectGPT调整的参数即可实现其平均性能的96.6%，突出了显著的参数效率。", "conclusion": "研究结果提供了实证证据，表明基础模型的情感能力由前馈门控机制结构化中介，并将gate_proj确认为情感建模的关键架构位置。"}}
{"id": "2601.15897", "pdf": "https://arxiv.org/pdf/2601.15897", "abs": "https://arxiv.org/abs/2601.15897", "authors": ["Zhaoqi Su", "Shihai Chen", "Xinyan Lin", "Liqin Huang", "Zhipeng Su", "Xiaoqiang Lu"], "title": "ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.", "AI": {"tldr": "本文提出了ThermoSplat，一种新的框架，通过主动特征调制和自适应几何解耦来实现深谱感知的重建。", "motivation": "多模态场景重建结合RGB和热红外数据对于在各种照明和天气条件下进行鲁棒的环境感知至关重要。然而，扩展3D高斯喷涂到多光谱场景仍然具有挑战性。当前的方法难以充分利用多模态数据的互补信息。", "method": "引入跨模式FiLM调制机制动态调节共享潜特征以热结构先验为条件，并提出适应性几何解耦方案学习独立不透明度偏移并执行独立栅格化通道，采用混合渲染管道整合显式球谐函数与隐含神经解码。", "result": "在RGBT-Scenes数据集上的广泛实验表明，ThermoSplat实现了跨可见和热光谱的最先进的渲染质量。", "conclusion": "该研究提出的方法有效改善了多模态场景重建的质量，并能够适应不同的几何不一致性，为鲁棒环境感知提供新的解决方案。"}}
{"id": "2601.15895", "pdf": "https://arxiv.org/pdf/2601.15895", "abs": "https://arxiv.org/abs/2601.15895", "authors": ["Anne Arzberger", "Enrico Liscio", "Maria Luce Lupetti", "Inigo Martinez de Rituerto de Troya", "Jie Yang"], "title": "Co-Constructing Alignment: A Participatory Approach to Situate AI Values", "categories": ["cs.HC"], "comment": null, "summary": "As AI systems become embedded in everyday practice, value misalignment has emerged as a pressing concern. Yet, dominant alignment approaches remain model centric, treating users as passive recipients of prespecified values rather than as epistemic agents who encounter and respond to misalignment during interactions. Drawing on situated perspectives, we frame alignment as an interactional practice co-constructed during human AI interaction. We investigate how users understand and wish to contribute to this process through a participatory workshop that combines misalignment diaries with generative design activities. We surface how misalignments materialise in practice and how users envision acting on them, grounded in the context of researchers using Large Language Models as research assistants. Our findings show that misalignments are experienced less as abstract ethical violations than as unexpected responses, and task or social breakdowns. Participants articulated roles ranging from adjusting and interpreting model behaviour to deliberate non-engagement as an alignment strategy. We conclude with implications for designing systems that support alignment as an ongoing, situated, and shared practice.", "AI": {"tldr": "本文提出了一种参与式的AI价值观对齐方法，通过用户与AI系统的互动来共同构建对齐。", "motivation": "随着AI系统嵌入日常生活，价值不对齐成为一个紧迫的问题。传统的对齐方法主要以模型为中心，将用户视为被动接收者而不是积极参与的主体。", "method": "研究采用了一种参与式工作坊的方法，结合了错误对齐日记和生成设计活动，探索用户如何理解并希望参与到这一过程中来。", "result": "研究发现，不对齐的问题更多地表现为意外响应或任务及社会互动中的故障。参与者表达了从调整解释模型行为到故意不参与的多种角色。", "conclusion": "结论强调了支持对齐作为一种持续、定位和共享实践的设计系统的重要性。"}}
{"id": "2601.15894", "pdf": "https://arxiv.org/pdf/2601.15894", "abs": "https://arxiv.org/abs/2601.15894", "authors": ["Simon W. Penninga", "Ruud J. G. van Sloun"], "title": "Iterative Amortized Hierarchical VAE", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this paper we propose the Iterative Amortized Hierarchical Variational Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid scheme containing an initial amortized guess and iterative refinement with decoder gradients. We achieve this by creating a linearly separable decoder in a transform domain (e.g. Fourier space), enabling real-time applications with very high model depths. The architectural change leads to a 35x speed-up for iterative inference with respect to the traditional HVAE. We show that our hybrid approach outperforms fully amortized and fully iterative equivalents in accuracy and speed respectively. Moreover, the IAHVAE shows improved reconstruction quality over a vanilla HVAE in inverse problems such as deblurring and denoising.", "AI": {"tldr": "本文提出了迭代近似分层变分自编码器（IA-HVAE），结合初始的近似推断和使用解码器梯度进行的迭代细化，实现了实时应用中模型深度非常高的情况下的高效推理。", "motivation": "动机是改进传统的HVAE，在保持精度的同时提高速度，并通过在变换域中创建线性可分解码器来实现这一点。", "method": "本文提出的方法是在变换域（如傅里叶空间）中创建线性可分的解码器，结合初始近似推断和使用解码器梯度进行迭代细化，从而加速推理过程。", "result": "与传统HVAE相比，IA-HVAE在迭代推理上实现了35倍的速度提升，并且在精度和速度方面优于完全近似的和完全迭代的方法。此外，在逆问题如去模糊和降噪中，其重建质量也超过了一般的HVAE。", "conclusion": "本文结论是IA-HVAE通过结合初始近似推断与迭代细化，不仅提高了推理的速度，同时也提升了模型在复杂任务中的精度，特别是在实时应用场合展现了显著的优势。"}}
{"id": "2601.15891", "pdf": "https://arxiv.org/pdf/2601.15891", "abs": "https://arxiv.org/abs/2601.15891", "authors": ["Anas Anwarul Haq Khan", "Mariam Husain", "Kshitij Jadhav"], "title": "RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.", "AI": {"tldr": "介绍RadJEPA，一种仅使用未标记的胸部X光图像进行预训练的自监督框架，在多个医学影像任务中表现超过现有方法。", "motivation": "探索在没有语言监督的情况下，是否能通过自我监督学习构建强大的放射学编码器以克服医疗视觉语言模型对配对图文数据的需求限制。", "method": "RadJEPA基于联合嵌入预测架构，在未标记的胸部X光图像上进行预训练，任务是预测被遮蔽区域的潜在表示。", "result": "RadJEPA在疾病分类、语义分割和报告生成等任务中表现优于现有方法，包括Rad-DINO。", "conclusion": "展示了通过自我监督学习构建强大的放射学编码器的可能性，在没有语言指导的情况下也能取得卓越性能。"}}
{"id": "2601.15889", "pdf": "https://arxiv.org/pdf/2601.15889", "abs": "https://arxiv.org/abs/2601.15889", "authors": ["Zhengding Luo", "Haozhe Ma", "Boxiang Wang", "Ziyi Yang", "Dongyuan Shi", "Woon-Seng Gan"], "title": "A Stabilized Hybrid Active Noise Control Algorithm of GFANC and FxNLMS with Online Clustering", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted by 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)", "summary": "The Filtered-x Normalized Least Mean Square (FxNLMS) algorithm suffers from slow convergence and a risk of divergence, although it can achieve low steady-state errors after sufficient adaptation. In contrast, the Generative Fixed-Filter Active Noise Control (GFANC) method offers fast response speed, but its lack of adaptability may lead to large steady-state errors. This paper proposes a hybrid GFANC-FxNLMS algorithm to leverage the complementary advantages of both approaches. In the hybrid GFANC-FxNLMS algorithm, GFANC provides a frame-level control filter as an initialization for FxNLMS, while FxNLMS performs continuous adaptation at the sampling rate. Small variations in the GFANC-generated filter may repeatedly reinitialize FxNLMS, interrupting its adaptation process and destabilizing the system. An online clustering module is introduced to avoid unnecessary re-initializations and improve system stability. Simulation results show that the proposed algorithm achieves fast response, very low steady-state error, and high stability, requiring only one pre-trained broadband filter.", "AI": {"tldr": "提出了一种结合GFANC和FxNLMS的混合主动噪声控制算法，以实现快速响应、低稳态误差和高稳定性。", "motivation": "解决了FxNLMS收敛慢且可能发散的问题，并克服了GFANC适应性差导致大稳态误差的缺点。", "method": "通过将GFANC生成的滤波器初始化给FxNLMS，并引入在线聚类模块避免不必要的重新初始化，以提高系统稳定性。", "result": "仿真结果显示提出的算法能够实现快速响应、非常低的稳态误差和高稳定性，且只需一个预先训练好的宽带滤波器。", "conclusion": "混合GFANC-FxNLMS算法结合了两种方法的优点，并通过在线聚类模块提高了系统的稳定性和性能。"}}
{"id": "2601.15888", "pdf": "https://arxiv.org/pdf/2601.15888", "abs": "https://arxiv.org/abs/2601.15888", "authors": ["Shiqi Huang", "Yipei Wang", "Natasha Thorley", "Alexander Ng", "Shaheer Saeed", "Mark Emberton", "Shonit Punwani", "Veeru Kasivisvanathan", "Dean Barratt", "Daniel Alexander", "Yipeng Hu"], "title": "Understanding the Transfer Limits of Vision Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": "accepted in ISBI 2026", "summary": "Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.", "AI": {"tldr": "本文研究了视觉基础模型在迁移学习中的限制，并通过评估两种不同的视觉基础模型在前列腺多参数MRI任务上的表现，探讨预训练目标与下游任务的对齐度对迁移性能的影响。", "motivation": "尽管视觉基础模型已经投入了大量的计算资源进行预训练，但它们在下游视觉和图像任务中往往表现出不均匀的进步。本文旨在探究这种限制是否源于预训练目标与实际应用需求之间的不匹配。", "method": "作者评估了两种视觉基础模型：一种是基于MAE（Masked Autoencoder）的重建导向型ProFound模型，另一种是基于对比学习的ProViCNet模型，在前列腺多参数MRI任务上测试它们的表现，并使用最大平均差异（MMD）等简单度量来衡量预训练和下游任务之间的对齐程度。", "result": "研究发现，当预训练目标与实际应用需求更好地对齐时，可以通过简单的度量指标如MMD观察到更高的性能提升和更快的收敛速度。", "conclusion": "本文强调了在设计和分析视觉基础模型的预训练目标时需要考虑其在下游任务中的适用性，并指出更好的对齐程度可以带来迁移学习性能上的改进。"}}
{"id": "2601.15884", "pdf": "https://arxiv.org/pdf/2601.15884", "abs": "https://arxiv.org/abs/2601.15884", "authors": ["Yifan Chen", "Fei Yin", "Hao Chen", "Jia Wu", "Chao Li"], "title": "PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.", "AI": {"tldr": "本文介绍了PMPBench，一个涵盖11个人体器官的公共、完全配对的多模态泛癌医学图像数据集，以促进基于AI的医学影像合成研究。", "motivation": "现有公开数据集中存在限制，包括专注于脑部相关配对MR模式的数据集不足，其他集合包含部分配对数据但存在缺失模式/时间戳和空间不对齐问题。本文旨在填补这些空白，推动安全有效的对比度合成。", "method": "构建了一个公共、完全配对的泛癌医学图像数据集，该数据集包括完整的动态对比增强序列和非对比CT与对比CT成像，并建立一个综合基准以评估不同设置下的图像翻译效果。", "result": "基于此资源，报告了当代图像到图像转换代表性基线的结果，提供了全面的评估。", "conclusion": "通过提供公开的数据集和基准，本文旨在催化医学影像合成的研究进展，特别是多器官肿瘤成像工作流程的安全有效性。"}}
{"id": "2601.15876", "pdf": "https://arxiv.org/pdf/2601.15876", "abs": "https://arxiv.org/abs/2601.15876", "authors": ["Taofeng Xue", "Chong Peng", "Mianqiu Huang", "Linsen Guo", "Tiancheng Han", "Haozhe Wang", "Jianing Wang", "Xiaocheng Zhang", "Xin Yang", "Dengchang Zhao", "Jinrui Ding", "Xiandi Ma", "Yuchen Xie", "Peng Pei", "Xunliang Cai", "Xipeng Qiu"], "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "categories": ["cs.AI"], "comment": "26 pages, 8 figures", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "AI": {"tldr": "介绍了一种名为EvoCUA的原生计算机使用代理模型，该模型通过学习可扩展合成经验来进化。", "motivation": "现有的依赖静态数据集模仿的方法无法捕捉到长时间跨度计算任务中复杂的因果动态，限制了代理能力的发展。因此，提出一种能够自主生成多样任务并进行大规模经验获取的学习策略。", "method": "EvoCUA整合数据生成与政策优化为一个自我维持的进化循环，并设计了一个可扩展基础设施来协调成千上万的异步沙盒回放，通过迭代演化学习策略有效地吸收这些经验，动态调节策略更新以强化成功惯例并改进失败轨迹。", "result": "在OSWorld基准上的实证评估显示EvoCUA达到了56.7%的成功率，并且显著优于以前的最佳开源模型和领先闭源权重模型。", "conclusion": "该研究结果表明，通过经验学习驱动的进化范式可以带来跨不同规模基础模型的一致性能提升，为推进原生代理能力提供了一条稳健而可扩展的道路。"}}
{"id": "2601.15872", "pdf": "https://arxiv.org/pdf/2601.15872", "abs": "https://arxiv.org/abs/2601.15872", "authors": ["Jaekwon Im", "Natalia Polouliakh", "Taketo Akama"], "title": "PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music Generation", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "comment": "4 pages, 2 figures", "summary": "Dance-to-music generation aims to generate music that is aligned with dance movements. Existing approaches typically rely on body motion features extracted from a single human dancer and limited dance-to-music datasets, which restrict their performance and applicability to real-world scenarios involving multiple dancers and non-human dancers. In this paper, we propose PF-D2M, a universal diffusion-based dance-to-music generation model that incorporates visual features extracted from dance videos. PF-D2M is trained with a progressive training strategy that effectively addresses data scarcity and generalization challenges. Both objective and subjective evaluations show that PF-D2M achieves state-of-the-art performance in dance-music alignment and music quality.", "AI": {"tldr": "本文提出了PF-D2M，一个基于扩散模型的通用舞蹈生成音乐系统，能够从舞蹈视频中提取视觉特征并生成与之同步的音乐。", "motivation": "现有方法依赖单一舞者的身体动作特征和有限的数据集，限制了其在涉及多个舞者或非人类舞者的现实场景中的性能和适用性。", "method": "PF-D2M采用一种渐进式训练策略来有效应对数据稀缺性和泛化挑战，并利用从舞蹈视频中提取的视觉特征生成音乐。", "result": "客观和主观评估表明，PF-D2M在舞蹈-音乐同步和音乐质量方面达到了最先进的性能水平。", "conclusion": "研究展示了PF-D2M作为一种通用且高效的方法，在舞蹈到音乐生成任务中的优越性能。"}}
{"id": "2601.15871", "pdf": "https://arxiv.org/pdf/2601.15871", "abs": "https://arxiv.org/abs/2601.15871", "authors": ["Jidong Jin"], "title": "Why Inference in Large Models Becomes Decomposable After Training", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Inference in large-scale AI models is typically performed on dense parameter matrices, leading to inference cost and system complexity that scale unsustainably with model size. This limitation does not arise from insufficient model capacity, but from treating post-training inference systems as monolithic operators while ignoring internal structures formed during learning. We show that gradient update events in large models are highly localized and selective, leaving many parameter dependencies statistically indistinguishable from their initialization distribution after training. As a result, post-training inference systems are structurally non-uniform and inherently decomposable. Based on this observation, we introduce a post-training statistical criterion and a structural annealing procedure that removes unsupported dependencies and reveals stable, independent substructures. This work establishes a post-training, model-agnostic structural view of inference systems and enables structured, parallel inference without modifying model functionality or interfaces.", "AI": {"tldr": "论文提出了一个方法，通过去除无支持的依赖关系来揭示稳定且独立的子结构，从而实现大型模型推理系统的分解。", "motivation": "在大规模AI模型中执行推理时，通常使用密集参数矩阵导致成本和系统复杂性不可持续地增加。这一限制并非源自模型容量不足，而是因为在训练后将推理系统视为单块操作而忽略了学习过程中形成的内部结构。", "method": "论文提出了一种基于梯度更新事件高度局部化和选择性的统计标准及结构退火过程的方法，来移除无支持的依赖关系并揭示稳定独立的子结构。", "result": "研究表明，大型模型中的参数依赖性在训练后与初始化分布统计上难以区分，使得推理系统在结构上非均匀且本质上可分解。通过提出的新方法实现了结构化和并行推理。", "conclusion": "研究确立了一种针对大规模模型的无模型特定结构视角，并为实现结构化的平行推理提供了解决方案，而无需修改模型功能或接口。"}}
{"id": "2601.15869", "pdf": "https://arxiv.org/pdf/2601.15869", "abs": "https://arxiv.org/abs/2601.15869", "authors": ["Francisco Portillo López"], "title": "Artificial Rigidities vs. Biological Noise: A Comparative Analysis of Multisensory Integration in AV-HuBERT and Human Observers", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 6 figures", "summary": "This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its response to incongruent audiovisual stimuli (McGurk effect) against human observers (N=44). Results reveal a striking quantitative isomorphism: AI and humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%), suggesting the model captures biological thresholds for auditory resistance. However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%), significantly exceeding human rates (47.7%). While humans displayed perceptual stochasticity and diverse error profiles, the model remained strictly categorical. Findings suggest that current self-supervised architectures mimic multisensory outcomes but lack the neural variability inherent to human speech perception.", "AI": {"tldr": "评估AV-HuBERT模型在处理不一致视听刺激时的生物保真度，并将其与人类观察者进行比较。", "motivation": "研究当前自监督架构能否模仿多感官整合的结果，同时探讨这些模型是否能够捕捉到人类言语感知中的神经变异。", "method": "通过基准测试AV-HuBERT和44个人类观察者对不一致视听刺激（麦克格效应）的反应来评估其生物保真度。", "result": "AI与人类表现出几乎相同的听觉优势率，但AI在语音融合方面存在决定性偏差，显著超过人类。人类显示感知随机性和多样的误差配置文件，而模型保持严格的类别性。", "conclusion": "当前自监督架构能够模仿多感官整合的结果，但在模拟人类言语感知中的神经变异方面仍有不足。"}}
{"id": "2601.15867", "pdf": "https://arxiv.org/pdf/2601.15867", "abs": "https://arxiv.org/abs/2601.15867", "authors": ["Dabiao Ma", "Zhiba Su", "Jian Yang", "Haojun Fei"], "title": "Out-of-Distribution Detection Based on Total Variation Estimation", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.", "AI": {"tldr": "本文提出了一种新的机器学习模型部署安全方法——基于总变差估计的出分布检测（TV-OOD）方法。", "motivation": "现有的方法已经取得了令人满意的结果，但TV-OOD通过改进的方法来提高安全性，并且在实际应用中防止潜在的数据分布变化带来的影响。", "method": "TV-OOD利用总变差网络估计器计算每个输入对整体总变差的贡献，定义为总变差得分来区分分布内和分布外的数据。", "result": "该方法在图像分类任务中的效果要么与领先的出分布检测技术相当，要么更优，并且这种优势体现在所有的评估指标上。", "conclusion": "TV-OOD展示了其在实际应用中提高机器学习模型部署安全性方面的潜力，特别是在处理数据分布变化时的鲁棒性。"}}
{"id": "2601.15865", "pdf": "https://arxiv.org/pdf/2601.15865", "abs": "https://arxiv.org/abs/2601.15865", "authors": ["Jingsong Xia", "Siqi Wang"], "title": "A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.", "AI": {"tldr": "本文提出了一种基于轻量级脑启发机器学习框架的冠状动脉造影分类方法，该方法结合了混合神经表征和鲁棒性学习策略。", "motivation": "在实际临床环境中，冠状动脉造影图像面临复杂的病变形态、严重的类别不平衡、标签不确定性以及有限的计算资源等挑战，这给传统的深度学习方法带来了稳健性和泛化性的难题。", "method": "本文提出的方法基于预训练的卷积神经网络构建轻量级混合神经表征，并引入选择性神经可塑性训练策略以实现高效的参数调整。同时采用结合焦点损失和标签平滑的脑启发注意力调节损失函数，以及类别不平衡采样和余弦退火加周期重启技术来模仿生物神经系统中的节奏调控和注意力分配机制。", "result": "实验结果显示，所提出的轻量级脑启发模型在二元冠状动脉造影分类中取得了稳定且强劲的表现，同时保持了较高的计算效率，并达到了竞争性的准确性、召回率、F1值和AUC指标。", "conclusion": "本研究表明，基于生物灵感的学习机制在轻量化医疗图像分析中的有效性，为有限计算资源下的智能临床决策支持提供了生物合理且可部署的解决方案。"}}
{"id": "2601.15864", "pdf": "https://arxiv.org/pdf/2601.15864", "abs": "https://arxiv.org/abs/2601.15864", "authors": ["Tanmay Inamdar", "Pallavi Jain", "Pranjal Pandey"], "title": "Minimum Envy Graphical House Allocation Beyond Identical Valuations", "categories": ["cs.GT", "cs.DS"], "comment": "21 pages, submitted to IJCAI 2026", "summary": "House allocation is an extremely well-studied problem in the field of fair allocation, where the goal is to assign $n$ houses to $n$ agents while satisfying certain fairness criterion, e.g., envy-freeness. To model social interactions, the Graphical House Allocation framework introduces a social graph $G$, in which each vertex corresponds to an agent, and an edge $(u, v)$ corresponds to the potential of agent $u$ to envy the agent $v$, based on their allocations and valuations. In undirected social graphs, the potential for envy is in both the directions. In the Minimum Envy Graphical House Allocation (ME-GHA) problem, given a set of $n$ agents, $n$ houses, a social graph, and agent's valuation functions, the goal is to find an allocation that minimizes the total envy summed up over all the edges of $G$. Recent work, [Hosseini et al., AAMAS 2023, AAMAS 2024] studied ME-GHA in the regime of polynomial-time algorithms, and designed exact and approximation algorithms, for certain graph classes under identical agent valuations. We initiate the study of \\gha with non-identical valuations, a setting that has so far remained unexplored. We investigate the multivariate (parameterized) complexity of \\gha by identifying structural restrictions on the social graph and valuation functions that yield tractability. We also design moderately exponential-time algorithms for several graph classes, and a polynomial-time algorithm for {binary valuations that returns an allocation with envy at most one when the social graph has maximum degree at most one.", "AI": {"tldr": "本文探讨了在非相同估值函数下最小嫉妒图形房屋分配问题的多变量复杂性，并设计了几种图类的适度指数时间算法以及二进制估值下的多项式时间算法。", "motivation": "该研究动机在于扩展对图形房屋分配中最小嫉妒问题的理解，特别是当代理之间的估值不同时的情况。现有工作主要关注于相同估值函数的情形。", "method": "本文通过识别社会图结构和估值函数的结构性限制来分析复杂性，并设计了针对特定图类的适度指数时间算法以及多项式时间算法。", "result": "研究结果表明，在二进制估值且社会图的最大度数不超过一时，可以找到嫉妒值至多为一的分配方案。此外，对于某些图类，提出了新的算法以解决最小嫉妒问题。", "conclusion": "本文填补了非相同估值下图形房屋分配的研究空白，并提出了解决此问题的新方法和复杂性分析策略。"}}
{"id": "2601.15861", "pdf": "https://arxiv.org/pdf/2601.15861", "abs": "https://arxiv.org/abs/2601.15861", "authors": ["Daniel Lokshtanov", "Michał Pilipczuk", "Paweł Rzążewski"], "title": "Finding large sparse induced subgraphs in graphs of small (but not very small) tree-independence number", "categories": ["cs.DS"], "comment": null, "summary": "The independence number of a tree decomposition is the size of a largest independent set contained in a single bag. The tree-independence number of a graph $G$ is the minimum independence number of a tree decomposition of $G$. As shown recently by Lima et al. [ESA~2024], a large family of optimization problems asking for a maximum-weight induced subgraph of bounded treewidth, satisfying a given \\textsf{CMSO}$_2$ property, can be solved in polynomial time in graphs whose tree-independence number is bounded by some constant~$k$. However, the complexity of the algorithm of Lima et al. grows rapidly with $k$, making it useless if the tree-independence number is superconstant. In this paper we present a refined version of the algorithm. We show that the same family of problems can be solved in time~$n^{\\mathcal{O}(k)}$, where $n$ is the number of vertices of the instance, $k$ is the tree-independence number, and the $\\mathcal{O}(\\cdot)$-notation hides factors depending on the treewidth bound of the solution and the considered \\textsf{CMSO}$_2$ property. This running time is quasipolynomial for classes of graphs with polylogarithmic tree-independence number; several such classes were recently discovered. Furthermore, the running time is subexponential for many natural classes of geometric intersection graphs -- namely, ones that admit balanced clique-based separators of sublinear size.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.15859", "pdf": "https://arxiv.org/pdf/2601.15859", "abs": "https://arxiv.org/abs/2601.15859", "authors": ["Lina Felsner", "Henriette Bast", "Tina Dorosti", "Florian Schaff", "Franz Pfeiffer", "Daniela Pfeiffer", "Julia Schnabel"], "title": "Uncertainty-guided Generation of Dark-field Radiographs", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "X-ray dark-field radiography provides complementary diagnostic information to conventional attenuation imaging by visualizing microstructural tissue changes through small-angle scattering. However, the limited availability of such data poses challenges for developing robust deep learning models. In this work, we present the first framework for generating dark-field images directly from standard attenuation chest X-rays using an Uncertainty-Guided Progressive Generative Adversarial Network. The model incorporates both aleatoric and epistemic uncertainty to improve interpretability and reliability. Experiments demonstrate high structural fidelity of the generated images, with consistent improvement of quantitative metrics across stages. Furthermore, out-of-distribution evaluation confirms that the proposed model generalizes well. Our results indicate that uncertainty-guided generative modeling enables realistic dark-field image synthesis and provides a reliable foundation for future clinical applications.", "AI": {"tldr": "本文提出了一种基于不确定性引导的生成对抗网络框架，用于从常规X射线图像中直接生成暗场辐射图。", "motivation": "由于暗场放射影像数据有限，难以开发出稳健的深度学习模型，因此本研究旨在通过一种新的方法来补充这一不足。", "method": "使用了一个不确定性引导的渐进式生成对抗网络，该模型结合了偶然不确定性和认知不确定性以提高解释能力和可靠性。", "result": "实验结果表明，生成的图像具有高结构保真度，并且在各个阶段都有定量指标的一致改进，证明模型能够很好地泛化到分布外数据。", "conclusion": "研究结论认为，基于不确定性的生成建模可以实现真实的暗场图象合成，并为未来的临床应用奠定了可靠基础。"}}
{"id": "2601.15838", "pdf": "https://arxiv.org/pdf/2601.15838", "abs": "https://arxiv.org/abs/2601.15838", "authors": ["Toan Gian", "Dung T. Tran", "Viet Quoc Pham", "Francesco Restuccia", "Van-Dinh Nguyen"], "title": "TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing", "categories": ["cs.CV"], "comment": "10 pages. This paper has been accepted for publication in IEEE PerCom 2026", "summary": "With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.", "AI": {"tldr": "本文提出了TinySense，一种基于矢量量化生成对抗网络(VQGAN)的高效压缩框架，用于减少Wi-Fi传感中所需的CSI数据量，同时保持高精度的人体姿势估计。", "motivation": "随着对设备无关和隐私保护感知解决方案需求的增长，现有的Wi-Fi传感方法处理大量CSI数据，导致网络资源紧张。因此，需要一种有效的压缩方案来提高Wi-Fi传感的可扩展性和准确性。", "method": "TinySense采用基于VQGAN的新框架，并结合K-means算法动态调整压缩比特率，将大规模预训练代码簿划分为更小的部分。此外，通过引入Transformer模型来减轻比特率损失，在不稳定的网络条件下增强鲁棒性。", "result": "实验结果表明，与现有压缩方案相比，TinySense在相同压缩率下实现了最高1.5倍的HPE准确度(PCK20)，同时降低了5倍的延迟和2.5倍的网络资源使用。", "conclusion": "TinySense通过高效压缩CSI数据提高了Wi-Fi传感的可扩展性和准确性，在保持高精度的同时显著减少了网络资源消耗。"}}
{"id": "2601.15830", "pdf": "https://arxiv.org/pdf/2601.15830", "abs": "https://arxiv.org/abs/2601.15830", "authors": ["Abdul Hasib", "A. S. M. Ahsanul Sarkar Akib"], "title": "An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics", "categories": ["cs.CV"], "comment": null, "summary": "The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \\$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.", "AI": {"tldr": "本文介绍了一种基于物联网的智能植物监控和灌溉系统，通过实时环境感应、自动化警报和云分析优化农业资源利用。", "motivation": "全球对可持续农业的需求促使开发智能化监测系统来优化资源利用并管理植物健康。传统农业方法依赖于人工观察和定期浇水，导致水资源浪费等问题。", "method": "该系统使用ESP32微控制器收集来自DHT22、HC-SR04和土壤湿度传感器的实时数据，并通过OLED显示屏提供视觉反馈并通过蜂鸣器发出音频警报。所有传感数据被无线传输到ThingSpeak云平台进行远程监测和历史分析。", "result": "实验结果表明，该系统在维持最佳土壤水分水平方面的准确率为92%，实现了水资源节约约40%的效果，并提供了实时的环境监控功能。", "conclusion": "集成的网络仪表板提供了对植物健康参数的全面可视化，适用于小规模园艺和商业农业应用。整个系统的实施成本为45.20美元，提供了一个经济实惠、可扩展的精准农业解决方案。"}}
{"id": "2601.15829", "pdf": "https://arxiv.org/pdf/2601.15829", "abs": "https://arxiv.org/abs/2601.15829", "authors": ["Yonghao Xu", "Pedram Ghamisi", "Qihao Weng"], "title": "Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).", "AI": {"tldr": "本文提出一种基于判别原型引导扩散的遥感数据集蒸馏方法，用于生成具有代表性和多样性的合成样本，以减少大规模训练数据的需求。", "motivation": "深度学习在遥感图像解释中取得了显著成功，但依赖于大量训练数据带来了存储和计算成本高昂以及潜在的数据泄露风险。因此，本文旨在通过引入数据集蒸馏来解决这些问题。", "method": "提出了一种文本到图像的扩散模型，并结合分类一致性损失以提高合成样本的质量；同时，在潜在空间进行聚类选择具有代表性的原型作为视觉风格指导，并使用视觉语言模型提供聚合文本描述。", "result": "实验结果表明，所提出的方法能够在三个高分辨率遥感场景分类基准上生成逼真且多样的样本用于下游模型训练。", "conclusion": "本文通过引入数据集蒸馏技术解决了大规模训练数据的需求问题，并展示了其在生成高质量合成样本方面的有效性。"}}
{"id": "2601.15828", "pdf": "https://arxiv.org/pdf/2601.15828", "abs": "https://arxiv.org/abs/2601.15828", "authors": ["Michael Farrell"], "title": "Can professional translators identify machine-generated text?", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "This study investigates whether professional translators can reliably identify short stories generated in Italian by artificial intelligence (AI) without prior specialized training. Sixty-nine translators took part in an in-person experiment, where they assessed three anonymized short stories - two written by ChatGPT-4o and one by a human author. For each story, participants rated the likelihood of AI authorship and provided justifications for their choices. While average results were inconclusive, a statistically significant subset (16.2%) successfully distinguished the synthetic texts from the human text, suggesting that their judgements were informed by analytical skill rather than chance. However, a nearly equal number misclassified the texts in the opposite direction, often relying on subjective impressions rather than objective markers, possibly reflecting a reader preference for AI-generated texts. Low burstiness and narrative contradiction emerged as the most reliable indicators of synthetic authorship, with unexpected calques, semantic loans and syntactic transfer from English also reported. In contrast, features such as grammatical accuracy and emotional tone frequently led to misclassification. These findings raise questions about the role and scope of synthetic-text editing in professional contexts.", "AI": {"tldr": "研究探讨专业翻译人员是否能可靠地识别由人工智能生成的意大利短篇故事。", "motivation": "了解专业译员在未经专门培训的情况下能否辨别AI生成文本的能力，以评估未来工作中合成文本编辑的角色和范围。", "method": "69名专业翻译人员参与了现场实验，他们评估三个匿名短篇故事（其中两个由ChatGPT-4o生成，一个由人类撰写）的AI作者身份可能性，并提供理由。", "result": "总体结果不确定，但有16.2%的人成功区分合成文本和人工文本。低突发性、叙述矛盾是最可靠的指标，而语法准确性、情感基调常导致误分类。", "conclusion": "研究提出了关于专业环境中合成文本编辑角色和范围的问题，并指出一些特征如低爆发性和叙事矛盾是识别AI生成内容的可靠标志。"}}
{"id": "2601.15824", "pdf": "https://arxiv.org/pdf/2601.15824", "abs": "https://arxiv.org/abs/2601.15824", "authors": ["Joan Vendrell Farreny", "Martí Jordà Roca", "Miquel Cornudella Gaya", "Rodrigo Fernández Baón", "Víctor García Martínez", "Eduard Camacho Sucarrat", "Alessandro Pignati"], "title": "Introducing the Generative Application Firewall (GAF)", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "This paper introduces the Generative Application Firewall (GAF), a new architectural layer for securing LLM applications. Existing defenses -- prompt filters, guardrails, and data-masking -- remain fragmented; GAF unifies them into a single enforcement point, much like a WAF coordinates defenses for web traffic, while also covering autonomous agents and their tool interactions.", "AI": {"tldr": "介绍了一种新的架构层——生成式应用防火墙（GAF），用于保护大型语言模型应用程序的安全。", "motivation": "现有的防御手段如提示过滤、护栏和数据掩码仍然分散，缺乏统一的防护点。因此提出了GAF来解决这一问题，以提供更全面的应用安全措施。", "method": "GAF通过整合各种防御机制并将其统一到一个执行点上，类似于Web应用防火墙（WAF）协调针对网络流量的防御措施一样。", "result": "该论文没有具体的结果部分，重点在于介绍和解释GAF的概念及其设计思路。", "conclusion": "GAF作为保护LLM应用程序的新方法具有潜力，它不仅整合了现有的安全措施，还扩展到了对自主代理及其工具交互的支持。"}}
{"id": "2601.15816", "pdf": "https://arxiv.org/pdf/2601.15816", "abs": "https://arxiv.org/abs/2601.15816", "authors": ["Shiqi Wei", "Qiqing Wang", "Kaidi Yang"], "title": "Virtual Traffic Police: Large Language Model-Augmented Traffic Signal Control for Unforeseen Incidents", "categories": ["eess.SY", "cs.AI"], "comment": null, "summary": "Adaptive traffic signal control (TSC) has demonstrated strong effectiveness in managing dynamic traffic flows. However, conventional methods often struggle when unforeseen traffic incidents occur (e.g., accidents and road maintenance), which typically require labor-intensive and inefficient manual interventions by traffic police officers. Large Language Models (LLMs) appear to be a promising solution thanks to their remarkable reasoning and generalization capabilities. Nevertheless, existing works often propose to replace existing TSC systems with LLM-based systems, which can be (i) unreliable due to the inherent hallucinations of LLMs and (ii) costly due to the need for system replacement. To address the issues of existing works, we propose a hierarchical framework that augments existing TSC systems with LLMs, whereby a virtual traffic police agent at the upper level dynamically fine-tunes selected parameters of signal controllers at the lower level in response to real-time traffic incidents. To enhance domain-specific reliability in response to unforeseen traffic incidents, we devise a self-refined traffic language retrieval system (TLRS), whereby retrieval-augmented generation is employed to draw knowledge from a tailored traffic language database that encompasses traffic conditions and controller operation principles. Moreover, we devise an LLM-based verifier to update the TLRS continuously over the reasoning process. Our results show that LLMs can serve as trustworthy virtual traffic police officers that can adapt conventional TSC methods to unforeseen traffic incidents with significantly improved operational efficiency and reliability.", "AI": {"tldr": "本文提出了一种层次框架，通过大型语言模型增强现有的交通信号控制系统，以应对突发事件。", "motivation": "传统的自适应交通信号控制方法在面对突发事件时效率低下且依赖人工干预。而现有利用大型语言模型替代传统系统的方案存在不稳定和成本高的问题。", "method": "提出一个层次框架，在上层使用虚拟交通警察代理通过实时交通事件动态微调下层控制器参数；设计了一个自我精炼的交通语言检索系统，并引入了基于LLM的验证器来持续更新该系统。", "result": "实验结果表明，大型语言模型可以作为值得信赖的虚拟交通警察，在应对突发事件时显著提升操作效率和可靠性。", "conclusion": "通过层次框架增强现有的交通信号控制系统，能够有效利用大型语言模型的优势解决传统方法在面对突发事件时存在的问题。"}}
{"id": "2601.15814", "pdf": "https://arxiv.org/pdf/2601.15814", "abs": "https://arxiv.org/abs/2601.15814", "authors": ["Ryosuke Yamano", "Tetsuo Shibuya"], "title": "Improved Approximation Ratios for the Shortest Common Superstring Problem with Reverse Complements", "categories": ["cs.DS"], "comment": "Accepted to CPM 2026", "summary": "The Shortest Common Superstring (SCS) problem asks for the shortest string that contains each of a given set of strings as a substring. Its reverse-complement variant, the Shortest Common Superstring problem with Reverse Complements (SCS-RC), naturally arises in bioinformatics applications, where for each input string, either the string itself or its reverse complement must appear as a substring of the superstring. The well-known MGREEDY algorithm for the standard SCS constructs a superstring by first computing an optimal cycle cover on the overlap graph and then concatenating the strings corresponding to the cycles, while its refined variant, TGREEDY, further improves the approximation ratio. Although the original 4- and 3-approximation bounds of these algorithms have been successively improved for the standard SCS, no such progress has been made for the reverse-complement setting. A previous study extended MGREEDY to SCS-RC with a 4-approximation guarantee and briefly suggested that extending TGREEDY to the reverse-complement setting could achieve a 3-approximation. In this work, we strengthen these results by proving that the extensions of MGREEDY and TGREEDY to the reverse-complement setting achieve 3.75- and 2.875-approximation ratios, respectively. Our analysis extends the classical proofs for the standard SCS to handle the bidirectional overlaps introduced by reverse complements. These results provide the first formal improvement of approximation guarantees for SCS-RC, with the 2.875-approximate algorithm currently representing the best known bound for this problem.", "AI": {"tldr": "本文改进了包含逆反补问题的最短公共超串（SCS-RC）算法，分别对MGREEDY和TGREEDY进行了扩展并证明了它们在SCS-RC中的近似比分别为3.75和2.875。", "motivation": "动机在于改善生物信息学应用中逆反补问题的最短公共超串（SCS-RC）算法的近似比率，因为在此之前没有针对该问题进行类似的改进。", "method": "通过对MGREEDY和TGREEDY算法在处理SCS-RC时的扩展，并且通过经典证明方法来应对由逆反补引入的双向重叠。", "result": "扩展后的MGREEDY算法达到了3.75近似比，而扩展后的TGREEDY算法达到了2.875近似比。这些结果是SCS-RC中首次正式改善了近似的保证。", "conclusion": "本文的改进为解决包含逆反补问题的最短公共超串（SCS-RC）提供了更优的近似比率，其中2.875的近似算法目前代表了该问题的最佳已知界限。"}}
{"id": "2601.15813", "pdf": "https://arxiv.org/pdf/2601.15813", "abs": "https://arxiv.org/abs/2601.15813", "authors": ["Clare Chemery", "Hendrik Edelhoff", "Ludwig Bothmann"], "title": "Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.", "AI": {"tldr": "介绍了一种轻量级的机器学习实验管道，以降低生态学家使用图像数据进行分类研究时应用机器学习方法的门槛。", "motivation": "为了使生态学家能够独立实验不同的机器学习模型，并针对本地数据集和特定分类任务生成定制见解，从而超越现成的模型。", "method": "结合简单命令行接口用于预处理、训练和评估，以及图形用户界面进行注释、错误分析和模型比较的设计方法。", "result": "在德国Veldenstein森林收集的3392张相机陷阱图像中对红鹿（Cervus elaphus）按年龄和性别分类进行了应用，最佳模型达到了年龄分类90.77%准确率和性别分类96.15%准确率。", "conclusion": "证明了即使使用有限的数据，也能可靠地回答狭窄、明确的生态问题，并为广泛采用机器学习在野生动物监测和人口统计分析中铺平道路。"}}
{"id": "2601.15812", "pdf": "https://arxiv.org/pdf/2601.15812", "abs": "https://arxiv.org/abs/2601.15812", "authors": ["Shir Ashury-Tahan", "Yifan Mai", "Elron Bandel", "Michal Shmueli-Scheuer", "Leshem Choshen"], "title": "ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLM) benchmarks tell us when models fail, but not why they fail. A wrong answer on a reasoning dataset may stem from formatting issues, calculation errors, or dataset noise rather than weak reasoning. Without disentangling such causes, benchmarks remain incomplete and cannot reliably guide model improvement. We introduce ErrorMap, the first method to chart the sources of LLM failure. It extracts a model's unique \"failure signature\", clarifies what benchmarks measure, and broadens error identification to reduce blind spots. This helps developers debug models, aligns benchmark goals with outcomes, and supports informed model selection. ErrorMap works on any model or dataset with the same logic. Applying our method to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model errors, revealing recurring failure patterns. ErrorAtlas highlights error types that are currently underexplored in LLM research, such as omissions of required details in the output and question misinterpretation. By shifting focus from where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced evaluation - one that exposes hidden weaknesses and directs progress. Unlike success, typically measured by task-level metrics, our approach introduces a deeper evaluation layer that can be applied globally across models and tasks, offering richer insights into model behavior and limitations. We make the taxonomy and code publicly available with plans to periodically update ErrorAtlas as new benchmarks and models emerge.", "AI": {"tldr": "本文介绍了ErrorMap和ErrorAtlas，用于揭示大型语言模型失败的原因，并提供了一个错误分类法。", "motivation": "现有的基准测试只能告诉我们模型何时失败，但无法解释其背后的具体原因。为了更全面地理解失败并指导改进，需要一种方法来分离失败的不同来源。", "method": "本文提出了一种名为ErrorMap的方法，用于绘制模型的失败景观，并生成了一个错误分类法ErrorAtlas。", "result": "通过将该方法应用于35个数据集和83个模型，揭示了重复出现的失败模式，特别是某些目前研究中较少关注的错误类型。", "conclusion": "ErrorMap和ErrorAtlas通过转向对失败原因的关注，提供了更深层次的评估方式，并有助于发现隐藏的弱点。这种方法可以广泛应用于不同的模型和任务，为理解和改进大型语言模型提供丰富的洞察力。"}}
{"id": "2601.15810", "pdf": "https://arxiv.org/pdf/2601.15810", "abs": "https://arxiv.org/abs/2601.15810", "authors": ["Mustafa Yurdakul", "Enes Ayan", "Fahrettin Horasan", "Sakir Tasdemir"], "title": "A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.", "AI": {"tldr": "开发了一个基于卷积神经网络（CNN）的移动应用程序，用于识别不同类型的花卉。", "motivation": "专家知识难以随时随地获取，此研究旨在为非专业人士提供快速、便捷地识别花卉类型的方法。", "method": "使用了三种不同的CNN模型（MobileNet、DenseNet121和Xception），并通过七种优化算法评估其分类性能。", "result": "采用随机梯度下降（SGD）优化算法的DenseNet-121模型表现最佳，准确率达到了95.84%，精度、召回率和F1-score均为96.00%。", "conclusion": "研究表明CNN可以用于移动应用程序中的花卉分类任务。"}}
{"id": "2601.15808", "pdf": "https://arxiv.org/pdf/2601.15808", "abs": "https://arxiv.org/abs/2601.15808", "authors": ["Yuxuan Wan", "Tianqing Fang", "Zaitang Li", "Yintong Huo", "Wenxuan Wang", "Haitao Mi", "Dong Yu", "Michael R. Lyu"], "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.", "AI": {"tldr": "本文提出了DeepVerifier，一种基于精心设计的评分标准，在推理过程中自我改进的深度研究代理系统。", "motivation": "现有大多数努力集中在训练后提高策略能力，而本文旨在通过测试时的指导性验证来实现自主进化。", "method": "使用自动构建的DRA故障分类法作为基础创建了DeepVerifier，该方法在推理时间对模型输出进行迭代验证并提供反馈以改进性能。", "result": "实验结果显示，与基线相比，DeepVerifier在元评估F1分数上提高了12%-48%，并在GAIA和XBench-DeepResearch的挑战性子集上的准确性有了8%-11%的提升。", "conclusion": "通过插件式模块集成验证器，在测试时间实现自我进化，并释放了DeepVerifier-4K数据集以促进开源进展，使开放模型能够开发强大的验证能力。"}}
{"id": "2601.15807", "pdf": "https://arxiv.org/pdf/2601.15807", "abs": "https://arxiv.org/abs/2601.15807", "authors": ["Tobias Boege", "Antony Della Vecchia", "Marina Garrote-López", "Benjamin Hollering"], "title": "Algebraic Statistics in OSCAR", "categories": ["stat.CO", "cs.NE", "math.AC", "math.ST"], "comment": null, "summary": "We introduce the AlgebraicStatistics section of the OSCAR computer algebra system. We give an overview of its extensible design and highlight its features including serialization of data types for sharing results and creating databases, and state-of-the-art implicitization algorithms.", "AI": {"tldr": "介绍了OSCAR计算机代数系统中的AlgebraicStatistics部分，概述了其可扩展设计和特性。", "motivation": "为了提供一个强大的工具来处理代数统计问题，并能够分享结果和创建数据库。", "method": "开发了一个具有可序列化数据类型和最新隐式化算法的代数统计模块。", "result": "实现了有效的数据共享机制和先进的隐式化算法，增强了OSCAR系统在代数统计领域的能力。", "conclusion": "AlgebraicStatistics部分为用户提供了强大的工具来解决代数统计问题，并通过其特性促进了研究结果的有效分享和应用。"}}
{"id": "2601.15804", "pdf": "https://arxiv.org/pdf/2601.15804", "abs": "https://arxiv.org/abs/2601.15804", "authors": ["Zoë Breed", "Elvin Karana", "Alessandro Bozzon", "Katherine W. Song"], "title": "Entangled Life and Code: A Computational Design Taxonomy for Synergistic Bio-Digital Systems", "categories": ["cs.HC"], "comment": "Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI '26), April 13--17, 2026, Barcelona, Spain. ACM, New York, NY, USA, 25 pages", "summary": "Bio-digital systems that merge microbial life with technology promise new modes of computation, combining biological adaptability with digital precision. Yet realizing this potential symbiotically -- where biological and digital agents co-adapt and co-process -- remains elusive, largely due to the absence of a shared vocabulary bridging biology and computing. Consequently, microbes are often constrained to uni-directional roles, functioning as sensors or actuators rather than as active, computational partners in bio-digital systems. In response, we propose a taxonomy and pathways that articulate and expand the roles of biological and digital entities for synergetic bio-digital computation. Using this taxonomy, we analysed 70 systems across HCI, design, and engineering, identifying how biological mechanisms can be mapped onto computational abstractions. We argue that such mappings enable computationally actionable directions that foster richer and reciprocal relationships in bio-digital systems, supporting regenerative ecologies across time and scale while inspiring new paradigms for computation in HCI.", "AI": {"tldr": "提出一种分类法和路径，以明确并扩展生物和数字实体在协同生物数字计算中的角色。", "motivation": "现有的生物数字系统中，微生物往往被限制为单向角色，缺乏与数字技术的双向互动。为了克服这一障碍，促进更复杂的共生关系和新的计算范式，作者提出了一个新的分类方法。", "method": "通过分析70个跨人机交互、设计和工程领域的系统，将生物机制映射到计算抽象上，以此来扩展生物实体在数字生态系统中的角色。", "result": "这项研究识别了如何将生物机制映射至计算模型，并主张这种映射可以推动更丰富且互惠的生物数字关系，支持跨越时间和规模的再生生态。", "conclusion": "通过开发这个新的分类法和路径，该论文为促进生物与数字化系统之间的协同作用提供了一个框架，鼓励在人机交互领域探索新的计算范式。"}}
{"id": "2601.15802", "pdf": "https://arxiv.org/pdf/2601.15802", "abs": "https://arxiv.org/abs/2601.15802", "authors": ["Alexandre Albore", "Humbert Fiorino", "Damien Pellier"], "title": "A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages. IEEE TechDefense 2025", "summary": "Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.", "AI": {"tldr": "本文提出了一种基于信标系统的解决方案，用于自主无人水下航行器（UUVs）在没有全球导航卫星系统（GNSS）情况下的隐秘导航。", "motivation": "军事和民用隐蔽行动要求在沿海地区进行操作而不依赖于支持船只或GNSS。这种操作对于表面通道不可用且需要隐秘导航的受限环境至关重要，例如保护区域或危险区域。", "method": "通过由空中无人机或水面无人机部署的一系列信标建立合成地标网络来确保精确的位置信息和路径规划。这些信标会发出声学信号用于UUVs的定位和导航，并采用层次化计划生成适应性路线。", "result": "该方法实现了在GNSS受限环境中的自主航行，通过不断监控和重新规划以保持轨迹精度，为UUVs提供了一条从大陆架到海岸目标的安全路径。", "conclusion": "基于信标系统的解决方案可以在没有GNSS的情况下支持自主无人水下航行器的隐蔽导航，提高任务执行的灵活性和安全性。"}}
{"id": "2601.15798", "pdf": "https://arxiv.org/pdf/2601.15798", "abs": "https://arxiv.org/abs/2601.15798", "authors": ["Zhikai Xue", "Tianqianjin Lin", "Pengwei Yan", "Ruichun Wang", "Yuxin Liu", "Zhuoren Jiang", "Xiaozhong Liu"], "title": "VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and Chronic Disease Management", "categories": ["cs.AI"], "comment": "Accepted by AAAI 2026 Demo", "summary": "Chronic diseases have become the leading cause of death worldwide, a challenge intensified by strained medical resources and an aging population. Individually, patients often struggle to interpret early signs of deterioration or maintain adherence to care plans. In this paper, we introduce VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease management from passive monitoring to proactive, interactive engagement. By integrating continuous data from wearable devices with the reasoning capabilities of LLMs, the system addresses both acute health anomalies and routine adherence. It analyzes triggers through context-aware inquiries, produces provisional insights within a collaborative patient-clinician workflow, and offers personalized guidance. This approach aims to promote a more proactive and cooperative care paradigm, with the potential to enhance patient self-management and reduce avoidable clinical workload.", "AI": {"tldr": "介绍VitalDiagnosis，一个基于LLM的生态系统，旨在通过持续监测和主动干预来改善慢性疾病管理。", "motivation": "鉴于慢性病成为全球主要死因、医疗资源紧张及人口老龄化等问题，患者在自我管理和遵守治疗计划方面存在困难。因此，需要一种新的方法来实现更有效的慢性疾病管理。", "method": "通过整合来自可穿戴设备的连续数据和LLM的推理能力，VitalDiagnosis系统可以主动检测健康异常并提供个性化建议，促进患者与医生之间的协作。", "result": "该系统有望提高患者的自我管理水平，并减轻不必要的临床工作负担。", "conclusion": "VitalDiagnosis通过集成AI技术改善了慢性疾病的管理方式，提高了患者的参与度和护理效率。"}}
{"id": "2601.15797", "pdf": "https://arxiv.org/pdf/2601.15797", "abs": "https://arxiv.org/abs/2601.15797", "authors": ["James S. Pearson", "Matthew J. Dennis", "Marc Cheong"], "title": "Creativity in the Age of AI: Rethinking the Role of Intentional Agency", "categories": ["cs.AI"], "comment": "27 pages, 2 figures", "summary": "Many theorists of creativity maintain that intentional agency is a necessary condition of creativity. We argue that this requirement, which we call the Intentional Agency Condition (IAC), should be rejected as a general condition of creativity, while retaining its relevance in specific contexts. We show that recent advances in generative AI have rendered the IAC increasingly problematic, both descriptively and functionally. We offer two reasons for abandoning it at the general level. First, we present corpus evidence indicating that authors and journalists are increasingly comfortable ascribing creativity to generative AI, despite its lack of intentional agency. This development places pressure on the linguistic intuitions that have traditionally been taken to support the IAC. Second, drawing on the method of conceptual engineering, we argue that the IAC no longer fulfils its core social function. Rather than facilitating the identification and encouragement of reliable sources of novel and valuable products, it now feeds into biases that distort our assessments of AI-generated outputs. We therefore propose replacing the IAC with a consistency requirement, according to which creativity tracks the reliable generation of novel and valuable products. Nonetheless, we explain why the IAC should be retained in specific local domains.", "AI": {"tldr": "本文重新审视了有意图的能动性作为创造力必要条件的传统观点，并提出应以一致性要求取代之，尤其是在人工智能时代。", "motivation": "作者认为传统的有意图的能动性条件在当前由AI驱动的新环境中变得过时且存在问题。他们希望通过这个研究来调整人们对创造力的理解，使其更加适应现代技术和文化的变化。", "method": "通过分析语料库数据和使用概念工程方法，本文探讨了有意图的能动性作为创造力必要条件的观点，并评估其在描述性和功能性上的有效性。", "result": "研究表明，人们越来越接受将创造力归于缺乏有意图能动性的AI系统。同时，有意图的能动性条件不再有效地实现其社会功能，反而可能引起对AI产出的偏见。", "conclusion": "文章提出应当在广泛的意义上放弃传统的有意图能动性要求，并转而关注一致性要求来衡量创造力；但在特定领域内仍需保留有意图能动性的标准。"}}
{"id": "2601.15780", "pdf": "https://arxiv.org/pdf/2601.15780", "abs": "https://arxiv.org/abs/2601.15780", "authors": ["Pascal Benschop", "Justin Dauwels", "Jan van Gemert"], "title": "Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video", "categories": ["cs.CV"], "comment": null, "summary": "Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.", "AI": {"tldr": "该论文介绍了用于评估视觉语言模型（VLM）情境和空间意识的合成视频基准测试。", "motivation": "由于细微的时间或几何线索，当前VLM在进行空间推理时表现脆弱，因此需要一个测试框架来探究其识别活动是否危险以及追踪对象间位置和运动的能力。", "method": "通过使用最小视频对来测试区分暴力与良性活动、绑定施暴者角色及判断精细轨迹对齐等挑战，并评估未经过训练的近期VLM的表现。", "result": "结果显示，模型在所有任务上的表现仅略高于随机水平，即使引入稳定颜色线索也只能部分缓解角色混淆问题。", "conclusion": "通过发布数据和代码，该基准测试旨在提供可重复诊断的方法并促进对轻量级空间先验探索以补充大规模预训练。"}}
{"id": "2601.15779", "pdf": "https://arxiv.org/pdf/2601.15779", "abs": "https://arxiv.org/abs/2601.15779", "authors": ["Liuyun Jiang", "Yanchao Zhang", "Jinyue Guo", "Yizhuo Lu", "Ruining Zhou", "Hua Han"], "title": "Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.", "AI": {"tldr": "提出基于扩散模型的数据增强框架以提高神经元分割性能。", "motivation": "当前深度学习方法在电子显微镜中进行神经元分割受限于大规模训练数据和大量手动标注，传统数据增强方式生成的样本与原始图像高度相关且缺乏结构多样性。", "method": "提出一个基于扩散模型的数据增强框架，该框架使用分辨率感知条件扩散模型结合多尺度条件和EM分辨率先验以实现从3D掩模到体素级别图像合成，并采用生物引导的掩模重塑模块生成具有更强结构真实性的增强掩模。", "result": "在AC3和AC4数据集低标注条件下，该方法分别提升了ARAND指标32.1%和30.7%，结合两种不同的后处理方法。", "conclusion": "基于扩散模型的数据增强框架能够有效丰富训练集并提升神经元分割性能。"}}
{"id": "2601.15778", "pdf": "https://arxiv.org/pdf/2601.15778", "abs": "https://arxiv.org/abs/2601.15778", "authors": ["Jiaxin Zhang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Confidence Calibration", "categories": ["cs.AI", "cs.CL"], "comment": "37 pages, 15 figures, 12 tables", "summary": "AI agents are rapidly advancing from passive language models to autonomous systems executing complex, multi-step tasks. Yet their overconfidence in failure remains a fundamental barrier to deployment in high-stakes settings. Existing calibration methods, built for static single-turn outputs, cannot address the unique challenges of agentic systems, such as compounding errors along trajectories, uncertainty from external tools, and opaque failure modes. To address these challenges, we introduce, for the first time, the problem of Agentic Confidence Calibration and propose Holistic Trajectory Calibration (HTC), a novel diagnostic framework that extracts rich process-level features ranging from macro dynamics to micro stability across an agent's entire trajectory. Powered by a simple, interpretable model, HTC consistently surpasses strong baselines in both calibration and discrimination, across eight benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance, HTC delivers three essential advances: it provides interpretability by revealing the signals behind failure, enables transferability by applying across domains without retraining, and achieves generalization through a General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE) on the out-of-domain GAIA benchmark. Together, these contributions establish a new process-centric paradigm for confidence calibration, providing a framework for diagnosing and enhancing the reliability of AI agents.", "AI": {"tldr": "本文介绍了Agentic Confidence Calibration问题，并提出了一种名为Holistic Trajectory Calibration（HTC）的诊断框架，用于提高自主系统在执行多步骤任务时的信心校准。", "motivation": "AI代理正从被动的语言模型发展成能够执行复杂、多步骤任务的自主系统。然而，它们在失败情况下的过度自信仍然是一个基本障碍，现有校准方法无法解决这些独特挑战，如沿路径累积错误、来自外部工具的不确定性等。", "method": "提出了Holistic Trajectory Calibration（HTC），这是一个诊断框架，它提取代理整个轨迹中的丰富过程级特征，从宏观动态到微观稳定性。HTC使用一个简单的可解释模型来实现。", "result": "HTC在八项基准测试、多种大型语言模型和不同代理框架中均超越了强基线，在校准和区分能力上表现出色。它还提供了可解释性、可迁移性和泛化能力，通过General Agent Calibrator（GAC）在GAIA基准测试中的最佳校准表现。", "conclusion": "HTC建立了新的过程中心范式用于信心校准，为诊断和增强AI代理的可靠性提供了一个框架。"}}
{"id": "2601.15777", "pdf": "https://arxiv.org/pdf/2601.15777", "abs": "https://arxiv.org/abs/2601.15777", "authors": ["Steffen Holter", "Eunyee Koh", "Mustafa Doga Dogan", "Gromit Yeuk-Yin Chan"], "title": "UXCascade: Scalable Usability Testing with Simulated User Agents", "categories": ["cs.HC"], "comment": "12 pages, 8 figures, under review", "summary": "Simulated user agents are increasingly used in usability testing to support fast, iterative UX workflows, as they generate rich data such as action logs and think-aloud reasoning, but the unstructured nature of this output often obscures actionable insights. We present UXCascade, an interactive tool for extracting, aggregating, and presenting agent-generated usability feedback at scale. Our core contribution is a multi-level analysis workflow that (1) highlights patterns across persona traits, goals, and outcomes, (2) links agent reasoning to specific issues, and (3) supports actionable design improvements. UXCascade operationalizes this approach by listing agent goals, traits, and issues in a structured overview. Practitioners can explore detailed reasoning traces and annotated views, propose interface edits, and assess their impact across personas. This enables a top-down, exploration-driven analysis from patterns to concrete UX interventions. A user study with eight UX professionals demonstrates that UXCascade integrates into existing workflows, enabling iterative feedback during early-stage interface development.", "AI": {"tldr": "UXCascade是一个用于从模拟用户代理生成的海量数据中提取、聚合和展示可用性反馈的交互工具。", "motivation": "由于模拟用户代理产生的输出通常是无结构化的，难以从中获取可操作性的见解，因此开发了UXCascade来支持快速迭代的用户体验工作流程。", "method": "通过多层次分析流程，该工具实现了将模拟用户的目标、特征和问题进行结构化概述，并允许从业者探索详细的推理轨迹和注释视图，提出界面编辑建议并评估其影响。", "result": "一项包含八个用户体验专业人士的研究表明，UXCascade能够融入现有的工作流程，在早期阶段的界面开发中实现迭代反馈。", "conclusion": "UXCascade通过提供结构化、可操作性的信息，支持了从模式分析到具体用户经验干预的顶层驱动探索。"}}
{"id": "2601.15775", "pdf": "https://arxiv.org/pdf/2601.15775", "abs": "https://arxiv.org/abs/2601.15775", "authors": ["Amir Habel", "Ivan Snegirev", "Elizaveta Semenyakina", "Miguel Altamirano Cabrera", "Jeffrin Sam", "Fawad Mehboob", "Roohan Ahmed Khan", "Muhammad Ahsan Mustafa", "Dzmitry Tsetserukou"], "title": "Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV", "categories": ["cs.RO"], "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.", "AI": {"tldr": "介绍了一种名为Glove2UAV的可穿戴IMU手套，用于通过手部和手指手势直观控制无人驾驶飞行器（UAV）。", "motivation": "旨在设计一个轻便且易于部署的可穿戴接口，以促进动态飞行中更安全、更可预测的操作，并提供实时操作功能。", "method": "Glove2UAV使用紧凑处理管道结合中值滤波和Madgwick算法来估计手掌和手指方向，将运动估计映射到一组控制原语用于定向飞行，当超出预定义速度阈值时触发振动反馈。", "result": "通过仿真和实际飞行测试验证了实时可行性，显示手势命令执行快速、手势动态与平台动作稳定耦合、核心指令集操作正确及及时发送振动警告提示。", "conclusion": "实验结果表明Glove2UAV在实时性、稳定性以及提供准确反馈方面表现良好。"}}
{"id": "2601.15772", "pdf": "https://arxiv.org/pdf/2601.15772", "abs": "https://arxiv.org/abs/2601.15772", "authors": ["Yuhan Chen", "Wenxuan Yu", "Guofa Li", "Yijun Xu", "Ying Fang", "Yicui Shi", "Long Cao", "Wenbo Chu", "Keqiang Li"], "title": "LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.", "AI": {"tldr": "本文提出了LL-GaussianImage，一种用于零样本无监督低光照增强的框架，直接在2D高斯投射压缩域中进行处理。", "motivation": "现有的低光照图像增强算法主要在像素域操作，导致使用2DGS压缩图像时效率低下且引入二次降质。为了解决这些问题并提高效率与视觉质量，提出了LL-GaussianImage框架。", "method": "该方法设计了一个语义引导的专家混合增强框架，并通过多目标合作损失函数系统严格约束平滑度和保真度，同时使用两阶段优化过程以实现重建即增强。", "result": "实验结果证明了在压缩域直接处理图像的可行性及其优越性，实现了高视觉质量的低光照图像增强并保持高压缩比。", "conclusion": "LL-GaussianImage框架成功地解决了2DGS压缩图像高效无损增强的问题，并展示了其在压缩域内直接处理的优势。"}}
{"id": "2601.15766", "pdf": "https://arxiv.org/pdf/2601.15766", "abs": "https://arxiv.org/abs/2601.15766", "authors": ["Yuhan Chen", "Ying Fang", "Guofa Li", "Wenxuan Yu", "Yicui Shi", "Jingrui Zhang", "Kefei Qian", "Wenbo Chu", "Keqiang Li"], "title": "LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps", "categories": ["cs.CV"], "comment": null, "summary": "Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.", "AI": {"tldr": "LL-GaussianMap提出了一种无监督框架，通过使用二维高斯溅射（2DGS）进行低光照图像增强。", "motivation": "现有的低光照图像增强方法主要在像素域或依赖隐式特征表示工作，忽视了图像的内在几何结构先验。而2DGS作为一种显式的场景表示技术具有优秀的结构拟合能力且渲染效率高，在低级别视觉任务中未被充分探索。", "method": "LL-GaussianMap首先利用2DGS执行高保真结构重建，然后通过Gaussian splatting的栅格化机制生成数据驱动的增强字典系数。这种方法将2DGS的结构感知能力融入到增益图生成过程中，有效保持边缘并减少增强过程中的伪影。", "result": "实验结果显示，LL-GaussianMap在低存储占用下实现了优越的图像增强性能，证实了显式Gaussian表示对图像增强的有效性。", "conclusion": "该论文展示了2DGS在无监督框架下用于低光照图像增强的优势，并证明了其在提升视觉质量方面的有效性。"}}
{"id": "2601.15761", "pdf": "https://arxiv.org/pdf/2601.15761", "abs": "https://arxiv.org/abs/2601.15761", "authors": ["Xiefeng Wu", "Mingyu Hu", "Shu Zhang"], "title": "Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning", "categories": ["cs.AI"], "comment": "7 pages main text 2 page reference", "summary": "Deploying reinforcement learning in the real world remains challenging due to sample inefficiency, sparse rewards, and noisy visual observations. Prior work leverages demonstrations and human feedback to improve learning efficiency and robustness. However, offline-to-online methods need large datasets and can be unstable, while VLA-assisted RL relies on large-scale pretraining and fine-tuning. As a result, a low-cost real-world RL method with minimal data requirements has yet to emerge. We introduce \\textbf{SigEnt-SAC}, an off-policy actor-critic method that learns from scratch using a single expert trajectory. Our key design is a sigmoid-bounded entropy term that prevents negative-entropy-driven optimization toward out-of-distribution actions and reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against representative baselines. Experiments show that SigEnt-SAC substantially alleviates Q-function oscillations and reaches a 100\\% success rate faster than prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks across multiple embodiments, where agents learn from raw images and sparse rewards; results demonstrate that SigEnt-SAC can learn successful policies with only a small number of real-world interactions, suggesting a low-cost and practical pathway for real-world RL deployment.", "AI": {"tldr": "本文介绍了SigEnt-SAC，一种新的离线强化学习方法，能够利用单个专家轨迹从零开始学习，并在稀疏奖励和噪声视觉观察下实现实用的机器人任务。", "motivation": "为了解决实际部署强化学习时面临的样本效率低、奖励稀疏以及视觉观测噪声等问题，特别是在需要最小化数据需求的同时实现低成本且高效的学习方法。", "method": "SigEnt-SAC是一种离线行为者-批评家方法，引入了sigmoid限定的熵项来防止优化过程中向分布外动作偏移，并减少Q函数振荡。", "result": "实验显示，与代表性的基线相比，SigEnt-SAC显著减轻了Q函数振荡问题，并且比先前的方法更快达到100%的成功率，在多个真实机器人任务上使用少量的真实世界交互学习成功策略。", "conclusion": "SigEnt-SAC展示了在实际机器人任务中实现低成本和实用的强化学习路径，能够通过极少的真实数据交互来学习有效的政策。"}}
{"id": "2601.15759", "pdf": "https://arxiv.org/pdf/2601.15759", "abs": "https://arxiv.org/abs/2601.15759", "authors": ["Qi Zeng", "Weide Liu", "Bo Li", "Ryne Didier", "P. Ellen Grant", "Davood Karimi"], "title": "Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.", "AI": {"tldr": "本论文提出了FeTal-SAM，一种专门为胎儿脑部MRI分割设计的Segment Anything Model（SAM）的新型适应版本。", "motivation": "传统的深度学习方法需要大量的标注数据集，并且当临床或研究需求改变时不够灵活。因此，该论文旨在解决模型对于不同标签定义重新训练的需求以及缺乏对图像对比度与空间先验理解的问题。", "method": "通过结合多图谱配准生成的空间对齐标签模板作为密集提示，FeTal-SAM使用边界框提示和SAM的分割解码器在每种结构上进行二元分割，随后融合这些分割以重构完整的3D分割体积。", "result": "实验结果表明，FeTal-SAM在两个数据集上的表现稳健，并且其Dice得分与针对每个数据集和标签定义训练的最先进的基线模型相当。虽然对于对比度低的结构如海马体、杏仁核等准确性稍低，但该方法展示了作为无需大量重新训练的通用分割模型的潜力。", "conclusion": "FeTal-SAM的方法为临床适应性胎儿脑部MRI分析工具的发展提供了有希望的进步。"}}
{"id": "2601.15757", "pdf": "https://arxiv.org/pdf/2601.15757", "abs": "https://arxiv.org/abs/2601.15757", "authors": ["Yimin Zhu", "Lincoln Linlin Xu", "Zhengsen Xu", "Zack Dewis", "Mabel Heffring", "Saeid Taleghanidoozdoozan", "Motasem Alkayid", "Quinn Ledingham", "Megan Greenwood"], "title": "White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.", "AI": {"tldr": "本文提出了物理光谱感知的白盒mHC（ES-mHC），该方法通过结构化矩阵明确建模电磁波谱组之间的交互，提高了超光谱图像分类模型的可解释性。", "motivation": "现有的大多数深度学习模型在超光谱图像分类中依赖于不透明的光谱-空间特征混合，限制了它们的可解释性，并阻碍了对其内部决策机制的理解。", "method": "ES-mHC是一个超连接框架，利用结构化方向矩阵明确建模不同电磁波谱组之间的交互，将特征表示与交互结构分离以促进电磁波谱分组的专门化和减少冗余。", "result": "研究结果表明，学习到的超连接矩阵表现出一致的空间模式和非对称交互行为。增加扩展率会加速结构化交互模式的出现。", "conclusion": "ES-mHC将超光谱图像分类从一个纯粹的黑盒预测任务转变为透明度更高的部分白盒学习过程，增强了模型内部动态机制的理解。"}}
{"id": "2601.15754", "pdf": "https://arxiv.org/pdf/2601.15754", "abs": "https://arxiv.org/abs/2601.15754", "authors": ["Ajvad Haneef K", "Karan Kuwar Singh", "Madhu Kumar S D"], "title": "CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via Chunk-wise Aggregated Gradient Boosting", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "High-dimensional malware datasets often exhibit feature redundancy, instability, and scalability limitations, which hinder the effectiveness and interpretability of machine learning-based malware detection systems. Although feature selection is commonly employed to mitigate these issues, many existing approaches lack robustness when applied to large-scale and heterogeneous malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise Aggregated Feature Estimation using Gradient Boosting), a scalable feature selection framework designed to produce stable and globally consistent feature rankings for high-dimensional malware detection. CAFE-GB partitions training data into overlapping chunks, estimates local feature importance using gradient boosting models, and aggregates these estimates to derive a robust global ranking. Feature budget selection is performed separately through a systematic k-selection and stability analysis to balance detection performance and robustness. The proposed framework is evaluated on two large-scale malware datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware feature spaces. Experimental results show that classifiers trained on CAFE-GB -selected features achieve performance parity with full-feature baselines across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and PR-AUC, while reducing feature dimensionality by more than 95\\%. Paired Wilcoxon signed-rank tests confirm that this reduction does not introduce statistically significant performance degradation. Additional analyses demonstrate low inter-feature redundancy and improved interpretability through SHAP-based explanations. Runtime and memory profiling further indicate reduced downstream classification overhead. Overall, CAFE-GB provides a stable, interpretable, and scalable feature selection strategy for large-scale malware detection.", "AI": {"tldr": "提出了一种名为CAFE-GB的可扩展特征选择框架，用于生成稳定的全局一致特征排名，以提高大规模恶意软件检测的效果和解释性。", "motivation": "现有方法在处理大规模和异质恶意软件数据时缺乏鲁棒性，导致高维恶意软件数据集出现特征冗余、不稳定性和可扩展性限制问题。", "method": "CAFE-GB将训练数据分割成重叠的块，使用梯度提升模型估计局部特征重要性，并聚合这些估计以得出稳健的全局排名。通过系统性的k选择和稳定性分析来平衡检测性能和鲁棒性。", "result": "在BODMAS和CIC-AndMal2020两个大规模恶意软件数据集上，使用CAFE-GB选取特征训练的分类器在多个指标（包括准确率、F1-score、MCC、ROC-AUC和PR-AUC）上与全特征基线达到性能持平，同时减少超过95%的特征维度，且未引入显著的性能下降。此外还展示了低特征冗余性和通过SHAP解释提高了可解释性。", "conclusion": "CAFE-GB提供了一种稳定、可解释和可扩展的特征选择策略，适用于大规模恶意软件检测，能有效减少下游分类的计算开销。"}}
{"id": "2601.15751", "pdf": "https://arxiv.org/pdf/2601.15751", "abs": "https://arxiv.org/abs/2601.15751", "authors": ["Xinda Chen", "Xing Zhen", "Hanyu Zhang", "Weimin Tan", "Bo Yan"], "title": "Tabular Incremental Inference", "categories": ["cs.AI"], "comment": null, "summary": "Tabular data is a fundamental form of data structure. The evolution of table analysis tools reflects humanity's continuous progress in data acquisition, management, and processing. The dynamic changes in table columns arise from technological advancements, changing needs, data integration, etc. However, the standard process of training AI models on tables with fixed columns and then performing inference is not suitable for handling dynamically changed tables. Therefore, new methods are needed for efficiently handling such tables in an unsupervised manner. In this paper, we introduce a new task, Tabular Incremental Inference (TabII), which aims to enable trained models to incorporate new columns during the inference stage, enhancing the practicality of AI models in scenarios where tables are dynamically changed. Furthermore, we demonstrate that this new task can be framed as an optimization problem based on the information bottleneck theory, which emphasizes that the key to an ideal tabular incremental inference approach lies in minimizing mutual information between tabular data and representation while maximizing between representation and task labels. Under this guidance, we design a TabII method with Large Language Model placeholders and Pretrained TabAdapter to provide external knowledge and Incremental Sample Condensation blocks to condense the task-relevant information given by incremental column attributes. Experimental results across eight public datasets show that TabII effectively utilizes incremental attributes, achieving state-of-the-art performance.", "AI": {"tldr": "本文提出了一种新的任务TabII，旨在使训练好的模型能够在推理阶段整合新列，提升在表格动态变化场景中的AI模型实用性。", "motivation": "传统的AI模型培训过程不适用于处理具有动态变化的表格数据。因此，需要一种方法来有效地解决这种动态表格的数据问题，并且不需要监督学习。", "method": "本文的方法基于信息瓶颈理论，设计了包含大型语言模型占位符和预训练TabAdapter以及增量样本浓缩块的TabII方法，用于提供外部知识并浓缩由新列属性提供的任务相关信息。", "result": "实验结果显示，在八个公共数据集上，TabII能有效地利用增量属性，实现了最先进的性能。", "conclusion": "该研究证明了TabII在处理动态变化表格方面的能力，并展示了它通过优化方法实现高效推理和外部知识整合的潜力。"}}
{"id": "2601.15739", "pdf": "https://arxiv.org/pdf/2601.15739", "abs": "https://arxiv.org/abs/2601.15739", "authors": ["Xinjue Hu", "Chi Wang", "Boyu Wang", "Xiang Zhang", "Zhenshan Tan", "Zhangjie Fu"], "title": "Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework", "categories": ["cs.CV"], "comment": null, "summary": "Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.", "AI": {"tldr": "本文提出了一种新的任意分辨率深度图像隐写框架ARDIS，解决了现有方法在处理不同分辨率隐藏和恢复秘密图像时的问题。", "motivation": "现有的深度图像隐写技术在保持隐蔽性和容量的同时，受限于必须维持秘密图像与载体图像相同分辨率的约束，导致了细节损失或无法精确恢复原始分辨率。", "method": "ARDIS框架采用频率解耦架构分离秘密图像，并设计了基于潜码引导的隐式重建器来准确恢复高频率残差，同时引入隐式分辨率编码策略实现无监督恢复。", "result": "实验表明，与现有方法相比，ARDIS在保持隐形性能的同时，在跨分辨率恢复保真度方面表现更优。", "conclusion": "本文提出的ARDIS框架通过创新的方法解决了深度图像隐写中的分辨率不一致问题，实现了高精度的任意分辨率恢复。"}}
{"id": "2601.15738", "pdf": "https://arxiv.org/pdf/2601.15738", "abs": "https://arxiv.org/abs/2601.15738", "authors": ["Junhao Qiu", "Haoyang Zhuang", "Fei Liu", "Jianjun Liu", "Qingfu Zhang"], "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling", "categories": ["cs.NE"], "comment": null, "summary": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.", "AI": {"tldr": "本文提出了一种LLM辅助的动态规则设计框架（LLM4DRD），用于解决灵活装配流水线调度问题，通过自动进化综合在线调度规则来优化多阶段套件生产和满足严格的供应约束。", "motivation": "在动态多产品交付环境中，快速协调零件完成和产品级套件组装变得非常具有挑战性。现有的自动化启发式设计方法难以捕捉和利用动态不确定性及层级依赖信息。因此，需要一种新的自动调度规则设计框架来应对这些挑战。", "method": "该研究首先将多阶段加工和装配供应决策转换成基于异构图的可行定向边排序。然后通过精英知识引导初始化嵌入高级设计专业知识到初始规则以提高初值质量，并引入LLM-A生成候选规则、LLM-S进行调度评估，以及动态特征适应性规则进化与混合评价机制来不断改进并提取具有强泛化能力的自适应规则。", "result": "通过一系列实验验证了该方法的有效性。在20个用于训练和测试的实际实例中，LLM4DRD比现有最先进的方法平均延迟提高了3.17-12.39%；在涉及不同资源配置、订单负荷和干扰水平的总共480个实例中，它实现了比第二佳竞争者高11.10%的表现，展示了优秀的鲁棒性。", "conclusion": "研究结果表明LLM4DRD框架能够有效应对动态灵活装配流水线调度问题中的挑战，并提供优于现有方法的性能和泛化能力。"}}
{"id": "2601.15737", "pdf": "https://arxiv.org/pdf/2601.15737", "abs": "https://arxiv.org/abs/2601.15737", "authors": ["Hanning Zhang", "Ruida Wang", "Rui Pan", "Wenyuan Wang", "Bingxu Meng", "Tong Zhang"], "title": "PhysProver: Advancing Automatic Theorem Proving for Physics", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "The combination of verifiable languages and LLMs has significantly influenced both the mathematical and computer science communities because it provides a rigorous foundation for theorem proving. Recent advancements in the field provide foundation models and sophisticated agentic systems pushing the boundaries of formal mathematical reasoning to approach the natural language capability of LLMs. However, little attention has been given to the formal physics reasoning, which also heavily relies on similar problem-solving and theorem-proving frameworks. To solve this problem, this paper presents, to the best of our knowledge, the first approach to enhance formal theorem proving in the physics domain. We compose a dedicated dataset PhysLeanData for the task. It is composed of theorems sampled from PhysLean and data generated by a conjecture-based formal data generation pipeline. In the training pipeline, we leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to train our model PhysProver. Comprehensive experiments demonstrate that, using only $\\sim$5K training samples, PhysProver achieves an overall 2.4\\% improvement in multiple sub-domains. Furthermore, after formal physics training, we observe 1.3\\% gains on the MiniF2F-Test benchmark, which indicates non-trivial generalization beyond physics domains and enhancement for formal math capability as well. The results highlight the effectiveness and efficiency of our approach, which provides a paradigm for extending formal provers outside mathematical domains. To foster further research, we will release both our dataset and model to the community.", "AI": {"tldr": "本文介绍了PhysProver，一种用于增强物理领域形式定理证明的系统。", "motivation": "虽然可验证语言和大语言模型在数学和计算机科学领域取得了显著进步，但在形式物理学推理方面却鲜有关注。因此，作者旨在开发针对物理学领域的形式化定理证明方法。", "method": "文章构建了一个名为PhysLeanData的数据集，并使用DeepSeek-Prover-V2-7B作为基础模型，通过强化学习和可验证奖励（RLVR）对模型进行训练，以提升其在物理学领域中的表现。", "result": "实验表明，仅用大约5000个训练样本，PhysProver在多个子领域中实现了2.4%的总体改进，并且在MiniF2F-Test基准测试上获得了1.3%的成绩增长。", "conclusion": "该结果展示了本方法的有效性和效率，为扩展形式证明系统到非数学领域的研究提供了范例。"}}
{"id": "2601.15734", "pdf": "https://arxiv.org/pdf/2601.15734", "abs": "https://arxiv.org/abs/2601.15734", "authors": ["Shadi Alijani", "Fereshteh Aghaee Meibodi", "Homayoun Najjaran"], "title": "Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.", "AI": {"tldr": "本文提出了一种新型框架，用于适应多模态医学影像的基础模型，并在脑肿瘤分割中进行了验证。", "motivation": "现有模型难以有效融合来自多个来源的信息并适应病理性组织的异质性，因此需要一种新的方法来解决这一问题。", "method": "该框架包含两项关键技术：子区域感知的模态注意力机制和自适应提示工程策略。注意力机制使模型能够学习每个肿瘤子区域的最佳模态组合，而自适应提示策略利用基础模型的内在能力以提高分割准确性。", "result": "在BraTS 2020脑肿瘤分割数据集上的验证结果显示，本文的方法显著优于基线方法，尤其在挑战性的坏死核心子区域中表现突出。", "conclusion": "这项工作为多模态融合和提示提供了一种原则性和有效的方法，并为进一步提高基于基础模型的医学影像解决方案的准确性和鲁棒性铺平了道路。"}}
{"id": "2601.15731", "pdf": "https://arxiv.org/pdf/2601.15731", "abs": "https://arxiv.org/abs/2601.15731", "authors": ["Linyong Zou", "Liang Zhang", "Xiongfei Wang", "Jia-Hong Gao", "Yi Sun", "Shurong Sheng", "Kuntao Xiao", "Wanli Yang", "Pengfei Teng", "Guoming Luan", "Zhao Lv", "Zikang Xu"], "title": "FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.", "AI": {"tldr": "本文介绍了FAIR-ESI框架，用于适应性地改进电生理源成像中的特征重要性。", "motivation": "尽管模型优化和深度学习方法在电生理源成像领域取得了显著进展，但准确选择和细化特征仍然是实现精确诊断的关键挑战。", "method": "提出FAIR-ESI框架，采用FFT谱特征细化、加权时间特征细化以及基于自注意力的分块特征细化三种方式适应性地改进不同视角下的特征重要性。", "result": "在两个模拟数据集和两个真实临床数据集上的广泛实验验证了该框架的有效性，并展示了其在脑部疾病诊断中的潜力及对大脑功能研究的新见解。", "conclusion": "FAIR-ESI框架能够有效提高电生理源成像的准确性，为脑部疾病的精确诊断提供了新的解决方案和洞见。"}}
{"id": "2601.15729", "pdf": "https://arxiv.org/pdf/2601.15729", "abs": "https://arxiv.org/abs/2601.15729", "authors": ["Rui Yang", "Lei Zheng", "Ruoyu Yao", "Jun Ma"], "title": "DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "8 pages, 5 figures", "summary": "Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.", "AI": {"tldr": "介绍了一种名为DualShield的规划和控制框架，用于解决扩散模型在自动驾驶中的安全性和动态可行性问题。", "motivation": "尽管扩散模型是自动驾驶中多模态运动规划的强大方法，但由于车辆动力学难以实施且依赖于其他代理的精确预测，其实际部署面临挑战。为了解决这些限制，提出了DualShield框架。", "method": "DualShield通过哈密尔顿-雅可比（HJ）可达性价值函数在双重能力中发挥作用：一是引导扩散去噪过程向安全和动态可行区域；二是形成使用控制屏障值函数（CBVF）的反应安全盾牌，修改执行的动作以确保安全性。", "result": "模拟实验表明，在不确定条件下，DualShield框架显著提高了保护未受保护左转场景中的安全性和任务效率。", "conclusion": "DualShield通过其双重机制在保持扩散模型的强大探索能力的同时提供了原则性的安全保障，特别是在不确定和对抗交互环境下表现出色。"}}
{"id": "2601.15728", "pdf": "https://arxiv.org/pdf/2601.15728", "abs": "https://arxiv.org/abs/2601.15728", "authors": ["Hangle Hu", "Chenyu Hou", "Bin Cao", "Ruizhe Li"], "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity", "categories": ["cs.AI", "cs.SE"], "comment": "8 pages, 7 figures", "summary": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.", "AI": {"tldr": "本文比较了Text-to-Python和Text-to-SQL的性能，通过引入BIRD-Python基准测试来评估两者的差异，并提出了解决Python代码生成中模糊性的Logic Completion Framework（LCF）。", "motivation": "随着数据分析需求的增长，现有技术在处理基于文件的数据和复杂工作流程方面存在限制。文本到SQL虽然成熟但不够灵活，而文本到Python虽有潜力却缺乏可靠性评估。", "method": "研究设计了BIRD-Python基准测试以减少注释噪声并使执行语义对齐；提出了Logic Completion Framework（LCF）来解决用户意图模糊性问题。", "result": "实验表明性能差异主要源于缺失的领域背景，而非代码生成本身的局限。通过填充这些空白，文本到Python可以达到与SQL相当的表现。", "conclusion": "研究表明，在有效处理自然语言输入的逻辑规范时，Python是数据分析师的一种可行基础技术。"}}
{"id": "2601.15724", "pdf": "https://arxiv.org/pdf/2601.15724", "abs": "https://arxiv.org/abs/2601.15724", "authors": ["Chenglin Li", "Qianglong Chen", "Feng Han", "Yikun Wang", "Xingxi Yin", "Yan Gong", "Ruilin Li", "Yin Zhang", "Jiaqi Wang"], "title": "VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.", "AI": {"tldr": "本文介绍了VideoThinker，一种通过合成工具交互轨迹训练的能主动探索长视频内容的视频大型语言模型。", "motivation": "现有的视频理解技术在处理长视频时存在时间定位不足和信息丢失的问题。为了克服这些限制并构建具备强长视频理解能力的代理工具，本文提出了VideoThinker。", "method": "通过将视频转换为丰富字幕，并利用强大的代理语言模型生成多步骤工具使用序列。这些轨迹随后被转换回视频帧，形成大规模交织的视频和工具推理数据集，用于训练VideoThinker。", "result": "实验结果表明，VideoThinker在长视频基准测试中显著优于仅基于字幕的语言模型代理和其他强基线视频模型，证明了合成工具增强数据及自适应检索与缩放推理的有效性。", "conclusion": "通过使用合成的代理数据集训练，VideoThinker具备动态推理能力和多步骤工具使用能力，展示了其在长视频理解中的优越性能。"}}
{"id": "2601.15721", "pdf": "https://arxiv.org/pdf/2601.15721", "abs": "https://arxiv.org/abs/2601.15721", "authors": ["Xinda Chen", "Jiawei Wu", "Yishuang Liu", "Jialin Zhu", "Shuwen Xiao", "Junjun Zheng", "Xiangheng Kong", "Yuning Jiang"], "title": "CoNRec: Context-Discerning Negative Recommendation with LLMs", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Understanding what users like is relatively straightforward; understanding what users dislike, however, remains a challenging and underexplored problem. Research into users' negative preferences has gained increasing importance in modern recommendation systems. Numerous platforms have introduced explicit negative feedback mechanisms and leverage such signals to refine their recommendation models. Beyond traditional business metrics, user experience-driven metrics, such as negative feedback rates, have become critical indicators for evaluating system performance. However, most existing approaches primarily use negative feedback as an auxiliary signal to enhance positive recommendations, paying little attention to directly modeling negative interests, which can be highly valuable in offline applications. Moreover, due to the inherent sparsity of negative feedback data, models often suffer from context understanding biases induced by positive feedback dominance. To address these challenges, we propose the first large language model framework for negative feedback modeling with special designed context-discerning modules. We use semantic ID Representation to replace text-based item descriptions and introduce an item-level alignment task that enhances the LLM's understanding of the semantic context behind negative feedback. Furthermore, we design a Progressive GRPO training paradigm that enables the model to dynamically balance the positive and negative behavioral context utilization. Besides, our investigation further reveals a fundamental misalignment between the conventional next-negative-item prediction objective and users' true negative preferences, which is heavily influenced by the system's recommendation order. To mitigate this, we propose a novel reward function and evaluation metric grounded in multi-day future negative feedback and their collaborative signals.", "AI": {"tldr": "本文提出了一个用于负面反馈建模的大型语言模型框架CoNRec，通过设计上下文辨识模块来解决传统推荐系统中正面反馈主导的问题，并提出了一种新的奖励函数和评估指标。", "motivation": "理解用户的喜好相对简单，但理解用户不喜欢什么是一个具有挑战性和未充分探索的问题。现有的大多数方法主要将负面反馈作为增强积极推荐的辅助信号，忽视了直接建模负面兴趣的重要性。", "method": "CoNRec使用语义ID表示代替基于文本的项目描述，并引入一个项目级别的对齐任务以提升模型理解负面反馈背后的语义上下文的能力。此外，设计了一种渐进式GRPO训练范例来平衡正面和负面行为上下文的利用。", "result": "论文揭示了传统的下一个负向项目预测目标与用户真正的负面偏好之间存在根本性不匹配的问题，并提出一种基于多天未来负面反馈及其协作信号的新奖励函数和评估指标以减轻这个问题。", "conclusion": "CoNRec框架通过专门设计的上下文辨识模块克服了传统推荐系统中正面反馈主导带来的偏见，引入新的训练范例、奖励函数及评估标准提高了对用户负面偏好的理解与建模能力。"}}
{"id": "2601.15719", "pdf": "https://arxiv.org/pdf/2601.15719", "abs": "https://arxiv.org/abs/2601.15719", "authors": ["Junjie Li", "Kong Aik Lee"], "title": "U3-xi: Pushing the Boundaries of Speaker Recognition via Incorporating Uncertainty", "categories": ["cs.SD"], "comment": null, "summary": "An utterance-level speaker embedding is typically obtained by aggregating a sequence of frame-level representations. However, in real-world scenarios, individual frames encode not only speaker-relevant information but also various nuisance factors. As a result, different frames contribute unequally to the final utterance-level speaker representation for Automatic Speaker Verification systems. To address this issue, we propose to estimate the inherent uncertainty of each frame and assign adaptive weights accordingly, where frames with higher uncertainty receive lower attention. Based on this idea, we present U3-xi, a comprehensive framework designed to produce more reliable and interpretable uncertainty estimates for speaker embeddings. Specifically, we introduce several strategies for uncertainty supervision. First, we propose speaker-level uncertainty supervision via a Stochastic Variance Loss, where the distance between an utterance embedding and its corresponding speaker centroid serves as a pseudo ground truth for uncertainty learning. Second, we incorporate global-level uncertainty supervision by injecting the predicted uncertainty into the sof tmax scale during training. This adaptive scaling mechanism adjusts the sharpness of the decision boundary according to sample difficulty, providing global guidance. Third, we redesign the uncertainty estimation module by integrating a Transformer encoder with multi-view self-attention, enabling the model to capture rich local and long-range temporal dependencies. Comprehensive experiments demonstrate that U3-xi is model-agnostic and can be seamlessly applied to various speaker encoders. In particular, when applied to ECAPA-TDNN, it achieves 21.1% and 15.57% relative improvements on the VoxCeleb1 test sets in terms of EER and minDCF, respectively.", "AI": {"tldr": "本文提出了U3-xi框架，通过估计每帧的内在不确定性并赋予自适应权重来改进说话人识别系统。", "motivation": "在现实场景中，不同音频帧对最终说话人表示的贡献不均等，因此提出一种方法来评估和利用这种不确定性以提高自动说话人验证系统的性能。", "method": "U3-xi框架包含三种策略：通过随机方差损失进行说话人级不确定性的监督、在训练过程中将预测的不确定性注入softmax尺度中提供全局级别的不确定性监督、重新设计不确定性估计模块，结合Transformer编码器和多视角自注意力机制来捕捉丰富的局部和长程时间依赖性。", "result": "实验表明U3-xi具有模型无关性，并能无缝应用于各种说话人编码器。特别地，在ECAPA-TDNN上，该方法在VoxCeleb1测试集上的等错误率（EER）和最小检测代价函数（minDCF）分别提升了21.1%和15.57%。", "conclusion": "U3-xi框架通过估计和利用帧级不确定性来改进说话人识别系统的性能，并展示了在多种模型上的广泛适用性和有效性。"}}
{"id": "2601.15717", "pdf": "https://arxiv.org/pdf/2601.15717", "abs": "https://arxiv.org/abs/2601.15717", "authors": ["Luyao Zhu", "Fangfang Zhang", "Yi Mei", "Mengjie Zhang"], "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling", "categories": ["cs.AI"], "comment": null, "summary": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.", "AI": {"tldr": "研究遗传编程演化的调度规则在动态柔性作业车间调度问题中的泛化能力。", "motivation": "现有的研究表明，使用相同的类型的实例对遗传编程演化的调度规则进行训练和测试时，其跨类型泛化能力尚未得到充分探索。因此，本论文旨在系统地探讨遗传编程演化的调度规则在不同条件下DFJSS的泛化能力。", "method": "通过一系列实验，在多个维度上进行了研究，包括问题规模（机器数量和作业数量）、关键车间参数（如利用率）以及数据分布等，以分析这些因素如何影响GP在未见过的问题实例类型上的性能。", "result": "研究表明，当训练实例包含更多的作业数而测试实例的机器数量保持不变时，能够实现较好的泛化。同时，训练和测试实例具有相似规模或车间参数也对良好泛化的实现至关重要。", "conclusion": "该研究为遗传编程在DFJSS中的泛化能力提供了新的见解，并强调了演化出更通用的GP规则以有效处理异构DFJSS实例的重要性。"}}
{"id": "2601.15715", "pdf": "https://arxiv.org/pdf/2601.15715", "abs": "https://arxiv.org/abs/2601.15715", "authors": ["Zhitao He", "Zongwei Lyu", "Yi R Fung"], "title": "Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory of Mind", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review", "summary": "Although artificial intelligence (AI) has become deeply integrated into various stages of the research workflow and achieved remarkable advancements, academic rebuttal remains a significant and underexplored challenge. This is because rebuttal is a complex process of strategic communication under severe information asymmetry rather than a simple technical debate. Consequently, current approaches struggle as they largely imitate surface-level linguistics, missing the essential element of perspective-taking required for effective persuasion. In this paper, we introduce RebuttalAgent, the first framework to ground academic rebuttal in Theory of Mind (ToM), operationalized through a ToM-Strategy-Response (TSR) pipeline that models reviewer mental state, formulates persuasion strategy, and generates strategy-grounded response. To train our agent, we construct RebuttalBench, a large-scale dataset synthesized via a novel critique-and-refine approach. Our training process consists of two stages, beginning with a supervised fine-tuning phase to equip the agent with ToM-based analysis and strategic planning capabilities, followed by a reinforcement learning phase leveraging the self-reward mechanism for scalable self-improvement. For reliable and efficient automated evaluation, we further develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of multi-source rebuttal data, which achieves scoring consistency with human preferences surpassing powerful judge GPT-4.1. Extensive experiments show RebuttalAgent significantly outperforms the base model by an average of 18.3% on automated metrics, while also outperforming advanced proprietary models across both automated and human evaluations. Disclaimer: the generated rebuttal content is for reference only to inspire authors and assist in drafting. It is not intended to replace the author's own critical analysis and response.", "AI": {"tldr": "本文介绍了RebuttalAgent，一种基于Theory of Mind（ToM）的框架，用于学术反驳，并展示了其在自动化评估和人类评价中的优越性。", "motivation": "尽管人工智能已深入研究工作流程并取得了显著进展，但学术反驳仍是一个重要且未充分探索的挑战。现有方法主要模仿表面语言学特征，而忽略了有效说服所需的视角转移能力。", "method": "本文提出RebuttalAgent框架，通过ToM-Strategy-Response（TSR）管道建模审稿人的心理状态、制定说服策略并生成基于策略的回答。训练过程包含监督微调和强化学习两个阶段，并开发了专门的评估器Rebuttal-RM。", "result": "实验结果显示，与基础模型相比，RebuttalAgent在自动化指标上平均提高了18.3%，并在自动化和人类评价中均优于先进的专有模型。", "conclusion": "RebuttalAgent通过整合ToM的能力，在学术反驳这一复杂的信息不对称沟通场景下取得了显著效果，并为未来的相关研究提供了新的视角和方法。"}}
{"id": "2601.15714", "pdf": "https://arxiv.org/pdf/2601.15714", "abs": "https://arxiv.org/abs/2601.15714", "authors": ["Ryoma Sato"], "title": "Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in Trustworthy LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents the maximum range that a model can solve without any errors. While ZEH itself is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we found that GPT-5.2 cannot even compute the parity of a short string like 11000, and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are balanced. This is surprising given the excellent capabilities of GPT-5.2. The fact that LLMs make mistakes on such simple problems serves as an important lesson when applying LLMs to safety-critical domains. By applying ZEH to Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates with accuracy, the detailed behaviors differ, and ZEH provides clues about the emergence of algorithmic capabilities. Finally, while computing ZEH incurs significant computational cost, we discuss how to mitigate this cost by achieving up to one order of magnitude speedup using tree structures and online softmax.", "AI": {"tldr": "本文提出了零错误边界（ZEH）的概念，用于衡量LLM在没有错误的情况下能解决的最大范围，并通过评估GPT-5.2和Qwen2.5的ZEH来揭示模型性能。", "motivation": "研究动机是探索如何使大型语言模型（LLMs）更加值得信赖。尽管这些模型表现出色，但它们在简单问题上仍然会出现错误，这可能影响其在安全关键领域的应用。", "method": "本文通过评估GPT-5.2和Qwen2.5的零错误边界（ZEH），并使用树结构和在线softmax技术来减少计算成本。", "result": "研究结果显示，尽管GPT-5.2具有强大的能力，但在简单任务中仍然存在错误。此外，对于Qwen2.5而言，通过详细分析发现ZEH与准确性相关，但具体行为有所不同，并且ZEH可以提供关于算法能力出现的线索。", "conclusion": "结论是零错误边界（ZEH）的概念有助于理解LLM的能力和限制，尽管计算成本较高，但仍可以通过特定方法实现显著加速。"}}
{"id": "2601.15711", "pdf": "https://arxiv.org/pdf/2601.15711", "abs": "https://arxiv.org/abs/2601.15711", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework", "categories": ["cs.CV"], "comment": "Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)", "summary": "Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, \"outer fabric\" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.", "AI": {"tldr": "该论文提出了一种三层次评估框架，用于评估基于视觉语言模型的零样本产品属性标注，并在DeepFashion-MultiModal数据集上对多种视觉语言模型进行了基准测试。", "motivation": "细粒度属性预测对于时尚零售应用至关重要。尽管视觉语言模型可以实现无需特定任务训练的零样本预测，但它们在多属性时尚任务上的系统评估尚未得到充分研究。因此，需要一个框架来分解和评估这些挑战。", "method": "论文提出了一种三层次的评估框架：1）整体任务性能（包括NA类），2）属性适用性检测，3）细粒度分类。使用DeepFashion-MultiModal数据集对九个不同级别的视觉语言模型进行了基准测试，并与基于预训练Fashion-CLIP嵌入的分类器进行比较。", "result": "零样本VLMs在整体任务上达到64.0%的宏F1值，比基于预训练Fashion-CLIP嵌入的逻辑回归提高了三倍。这些模型在细粒度分类上的表现较好（70.8% F1），但在属性适用性检测方面存在问题（NA-F1 34.1%）。高效和超高效的VLMs可以实现旗舰性能的90%以上，同时降低成本。", "conclusion": "该评估框架可以帮助从业者诊断错误来源是可见性检测还是分类，并为生产系统提供针对性改进路径。"}}
{"id": "2601.15710", "pdf": "https://arxiv.org/pdf/2601.15710", "abs": "https://arxiv.org/abs/2601.15710", "authors": ["Jiahao Zhang", "Zifan He", "Nicholas Fraser", "Michaela Blott", "Yizhou Sun", "Jason Cong"], "title": "FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator Design", "categories": ["cs.AR", "cs.AI", "cs.LG"], "comment": null, "summary": "We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid development of domain-specific LLM accelerators. FlexLLM exposes key architectural degrees of freedom for stage-customized inference, enabling hybrid designs that tailor temporal reuse and spatial dataflow differently for prefill and decode, and provides a comprehensive quantization suite to support accurate low-bit deployment. Using FlexLLM, we build a complete inference system for the Llama-3.2 1B model in under two months with only 1K lines of code. The system includes: (1) a stage-customized accelerator with hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves 1.29$\\times$ end-to-end speedup, 1.64$\\times$ higher decode throughput, and 3.14$\\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\\times$, 6.55$\\times$, and 4.13$\\times$, respectively. In long-context scenarios, integrating the HMT plug-in reduces prefill latency by 23.23$\\times$ and extends the context window by 64$\\times$, delivering 1.10$\\times$/4.86$\\times$ lower end-to-end latency and 5.21$\\times$/6.27$\\times$ higher energy efficiency on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic innovation in LLM inference and high-performance accelerators with minimal manual effort.", "AI": {"tldr": "介绍FlexLLM，一种用于快速开发特定领域大语言模型（LLM）加速器的可组合高级综合库。", "motivation": "为了实现灵活混合的大规模语言模型加速器设计，并简化算法创新与高性能加速器之间的连接，减少人工努力。", "method": "使用FlexLLM构建Llama-3.2 1B模型的完整推理系统，在两个月内以仅1K行代码开发出一个硬件高效的量化加速器和Hierarchical Memory Transformer插件。", "result": "在AMD U280 FPGA上，该加速器实现端到端加速提升1.29倍、解码吞吐量提高1.64倍及能耗效率提升3.14倍；HMT插件进一步提升了长上下文处理性能和能效。", "conclusion": "FlexLLM成功连接了大规模语言模型推理的算法创新与高性能加速器，显著减少了开发时间和工作量。"}}
{"id": "2601.15709", "pdf": "https://arxiv.org/pdf/2601.15709", "abs": "https://arxiv.org/abs/2601.15709", "authors": ["Asim Biswal", "Chuan Lei", "Xiao Qin", "Aodong Li", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "AgentSM: Semantic Memory for Agentic Text-to-SQL", "categories": ["cs.AI", "cs.DB", "cs.LG"], "comment": ":I.2.7; H.5.2", "summary": "Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale in realistic enterprise settings with large, complex schemas, diverse SQL dialects, and expensive multi-step reasoning. Emerging agentic approaches show potential for adaptive reasoning but often suffer from inefficiency and instability-repeating interactions with databases, producing inconsistent outputs, and occasionally failing to generate valid answers. To address these challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework for Text-to-SQL that builds and leverages interpretable semantic memory. Instead of relying on raw scratchpads or vector retrieval, AgentSM captures prior execution traces-or synthesizes curated ones-as structured programs that directly guide future reasoning. This design enables systematic reuse of reasoning paths, which allows agents to scale to larger schemas, more complex questions, and longer trajectories efficiently and reliably. Compared to state-of-the-art systems, AgentSM achieves higher efficiency by reducing average token usage and trajectory length by 25% and 35%, respectively, on the Spider 2.0 benchmark. It also improves execution accuracy, reaching a state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.", "AI": {"tldr": "本文介绍了AgentSM，一个用于Text-to-SQL的具有适应性推理能力的框架。", "motivation": "尽管基于LLM的Text-to-SQL系统在公共基准测试上取得了显著进步，但它们仍难以应对大规模、复杂的企业环境中的挑战，例如复杂的数据库模式和多步推理。本文旨在通过引入AgentSM来解决这些问题，提高效率与稳定性。", "method": "AgentSM构建并利用可解释的语义记忆，捕捉或合成执行轨迹作为结构化程序以指导未来的推理。这种方式有助于系统性地复用推理路径，提升在更大规模数据库模式和更复杂问题上的处理能力，并减少资源消耗。", "result": "相较于现有最佳系统，AgentSM在Spider 2.0基准上减少了平均令牌使用量和轨迹长度分别达25%和35%，并提高了执行精度，在Spider 2.0 Lite基准中达到了44.8%的最高准确率。", "conclusion": "研究表明，引入语义记忆机制可以有效提升Text-to-SQL系统的效率与稳定性，并在复杂环境中的表现显著优于现有方法。"}}
{"id": "2601.15707", "pdf": "https://arxiv.org/pdf/2601.15707", "abs": "https://arxiv.org/abs/2601.15707", "authors": ["Qifan Hu", "Branko Celler", "Weidong Mu", "Steven W. Su"], "title": "D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot", "categories": ["cs.RO"], "comment": null, "summary": "Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.", "AI": {"tldr": "本文提出了一种两阶段校准框架，用于一个自设计的三自由度踝关节康复机器人。", "motivation": "精确对齐多自由度康复机器人对于患者的安全和有效训练至关重要。", "method": "首先开发了基于Kronecker积的开环校准方法。然后使用D-最优准则指导从候选集合中选择4个信息量最大的姿势，通过Proximal Policy Optimization (PPO)智能体在模拟中进行学习。", "result": "实验结果表明，在模拟和真实机器人评估中，PPO策略显著提高了信息矩阵的平均行列式值，并且使用四个D-最优指导的姿势识别出的参数向量比从50个不结构化的姿势集中得到的估计具有更强的跨集预测一致性。", "conclusion": "该框架在保持精确的参数估计的同时，提升了校准效率，为多自由度康复机器人的高精度对齐提供了实用指南。"}}
{"id": "2601.15706", "pdf": "https://arxiv.org/pdf/2601.15706", "abs": "https://arxiv.org/abs/2601.15706", "authors": ["Akriti Vij", "Benjamin Chua", "Darshini Ramiah", "En Qi Ng", "Mahran Morsidi", "Naga Nikshith Gangarapu", "Sharmini Johnson", "Vanessa Wilfred", "Vikneswaran Kumaran", "Wan Sie Lee", "Wenzhuo Yang", "Yongsen Zheng", "Bill Black", "Boming Xia", "Frank Sun", "Hao Zhang", "Qinghua Lu", "Suyu Ma", "Yue Liu", "Chi-kiu Lo", "Fatemeh Azadi", "Isar Nejadgholi", "Sowmya Vajjala", "Agnes Delaborde", "Nicolas Rolin", "et al. (21 additional authors not shown)"], "title": "Improving Methodologies for LLM Evaluations Across Global Languages", "categories": ["cs.AI"], "comment": "Author names have been organised by country, and in alphabetical order within countries", "summary": "As frontier AI models are deployed globally, it is essential that their behaviour remains safe and reliable across diverse linguistic and cultural contexts. To examine how current model safeguards hold up in such settings, participants from the International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight models were tested across ten languages spanning high and low resourced groups: Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across five harm categories (privacy, non-violent crime, violent crime, intellectual property and jailbreak robustness), using both LLM-as-a-judge and human annotation. The exercise shows how safety behaviours can vary across languages. These include differences in safeguard robustness across languages and harm types and variation in evaluator reliability (LLM-as-judge vs. human review). Further, it also generated methodological insights for improving multilingual safety evaluations, such as the need for culturally contextualised translations, stress-tested evaluator prompts and clearer human annotation guidelines. This work represents an initial step toward a shared framework for multilingual safety testing of advanced AI systems and calls for continued collaboration with the wider research community and industry.", "AI": {"tldr": "本文描述了一个多语言评估练习，旨在检测前沿AI模型在全球多种语言和文化背景下的行为是否安全可靠。", "motivation": "鉴于前沿AI模型的全球部署需求，确保这些模型在不同文化和语言环境中保持安全性与可靠性是至关重要的。", "method": "来自国际先进人工智能测量、评估与科学网络的参与者对两种开放权重模型进行了多语种评估，涵盖十种语言，并使用了6000多个新翻译的提示语，在五个危害类别中进行评价，包括LLM作为裁判和人类标注。", "result": "研究结果显示了不同语言之间安全行为的变化，包括防护强度和评价者可靠性的差异，并提出了改善多语言安全性评估的方法论见解。", "conclusion": "这项工作是迈向一种共享的多语种先进AI系统安全性测试框架的初步步骤，并呼吁继续与更广泛的研究社区和产业界合作。"}}
{"id": "2601.15705", "pdf": "https://arxiv.org/pdf/2601.15705", "abs": "https://arxiv.org/abs/2601.15705", "authors": ["Ali Caglayan", "Nevrez Imamoglu", "Toru Kouyama"], "title": "Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data", "categories": ["cs.CV"], "comment": "5 pages, 4 figures", "summary": "This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.", "AI": {"tldr": "该论文通过轻量级模型改进在ALOS-2 SAR数据上的LULC分割。", "motivation": "研究旨在解决SAR密集预测中的常见失败模式，包括边界过度平滑、遗漏细长结构和长尾标签下的罕见类别退化问题。", "method": "引入了三种轻量级改进：(i) 将高分辨率特征注入多尺度解码中，(ii) 进步精炼上采样头交替进行卷积精炼和逐步上采样，(iii) 一个α缩放因子调整类别的重新加权。", "result": "模型在日本全国范围内的ALOS-2 LULC基准测试中的表现有所提升，特别是对于代表性不足的类别，并且在标准评估指标下的水体检测性能也得到了改善。", "conclusion": "通过轻量级改进的方法有效提升了LULC分割和水体检测的精度。"}}
{"id": "2601.15703", "pdf": "https://arxiv.org/pdf/2601.15703", "abs": "https://arxiv.org/abs/2601.15703", "authors": ["Jiaxin Zhang", "Prafulla Kumar Choubey", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Agentic Uncertainty Quantification", "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 9 figures, 9 tables", "summary": "Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,'' where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.", "AI": {"tldr": "本文提出了一种统一的双过程代理不确定性量化框架（AUQ），将语言化的不确定性转化为双向控制信号，以提高AI代理在长期推理中的可靠性和性能。", "motivation": "虽然AI代理展示了强大的长期推理能力，但它们因早期知识性错误而产生的可靠性问题亟待解决。现有的不确定度量化方法只能诊断风险但不解决问题，自我反省机制则存在持续或盲目修正的问题。", "method": "该论文提出了一种称为双过程代理不确定性量化的统一框架（AUQ），包含两个互补机制：系统1（UAM）通过隐式传播语言化信心和语义解释防止盲目的决策；系统2（UAR）利用这些解释作为理性的提示，仅在必要时触发有针对性的推理时间解决。", "result": "广泛的实验结果表明，在闭环基准测试和开放性深度研究任务中，该无训练的方法实现了优越的表现和轨迹级别的校准。", "conclusion": "这种原则性的框架AUQ被认为代表了可靠代理发展的一个重要步骤。"}}
{"id": "2601.15698", "pdf": "https://arxiv.org/pdf/2601.15698", "abs": "https://arxiv.org/abs/2601.15698", "authors": ["Mingyu Yu", "Lana Liu", "Zhehao Zhao", "Wei Wang", "Sujuan Qin"], "title": "Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a \"reconstruction-then-generation\" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.", "AI": {"tldr": "提出一种名为Beyond Visual Safety（BVS）的图像文本对越狱框架，旨在探测多模态大语言模型（MLLMs）的视觉安全性边界。", "motivation": "尽管现有方案已探索了MLLM的安全漏洞问题，但对其视觉安全边界的探讨仍显不足。因此，研究动机在于揭示当前MLLM在视觉安全性方面的潜在风险。", "method": "BVS采用“重建-生成”策略，利用中性化的图像拼接和归纳重组来解耦恶意意图与原始输入，诱导MLLMs生成有害图像。", "result": "实验结果显示，BVS对GPT-5（2026年1月12日发布）的越狱成功率达到了98.21%。", "conclusion": "研究结果揭示了当前多模态大语言模型在视觉安全性方面的关键漏洞。"}}
{"id": "2601.15690", "pdf": "https://arxiv.org/pdf/2601.15690", "abs": "https://arxiv.org/abs/2601.15690", "authors": ["Jiaxin Zhang", "Wendi Cui", "Zhuohang Li", "Lifu Huang", "Bradley Malin", "Caiming Xiong", "Chien-Sheng Wu"], "title": "From Passive Metric to Active Signal: The Evolving Role of Uncertainty Quantification in Large Language Models", "categories": ["cs.AI", "stat.AP"], "comment": "20 pages, 4 figures, 6 tables", "summary": "While Large Language Models (LLMs) show remarkable capabilities, their unreliability remains a critical barrier to deployment in high-stakes domains. This survey charts a functional evolution in addressing this challenge: the evolution of uncertainty from a passive diagnostic metric to an active control signal guiding real-time model behavior. We demonstrate how uncertainty is leveraged as an active control signal across three frontiers: in \\textbf{advanced reasoning} to optimize computation and trigger self-correction; in \\textbf{autonomous agents} to govern metacognitive decisions about tool use and information seeking; and in \\textbf{reinforcement learning} to mitigate reward hacking and enable self-improvement via intrinsic rewards. By grounding these advancements in emerging theoretical frameworks like Bayesian methods and Conformal Prediction, we provide a unified perspective on this transformative trend. This survey provides a comprehensive overview, critical analysis, and practical design patterns, arguing that mastering the new trend of uncertainty is essential for building the next generation of scalable, reliable, and trustworthy AI.", "AI": {"tldr": "本文探讨了不确定性量化在大型语言模型中的角色演变，从被动诊断指标转变为实时控制信号，并展示了其如何应用于高级推理、自主代理和强化学习。", "motivation": "尽管大型语言模型显示出强大的能力，但它们的不可靠性仍然是高风险领域部署的关键障碍。作者认为通过将不确定性作为主动控制信号可以提高这些模型的可靠性。", "method": "本文采用调查研究的方式，结合新兴理论框架如贝叶斯方法和符合预测技术，分析了不确定性在高级推理、自主代理决策及强化学习中的应用。", "result": "研究表明，当不确定性作为实时模型行为的主动控制信号时，可以优化计算资源使用，触发自我纠正机制，在工具使用和信息寻求方面做出更明智的选择，并减少奖励操纵现象。", "conclusion": "本文强调掌握这种新的不确定性趋势对于构建下一代可扩展、可靠且值得信赖的人工智能至关重要。"}}
{"id": "2601.15688", "pdf": "https://arxiv.org/pdf/2601.15688", "abs": "https://arxiv.org/abs/2601.15688", "authors": ["Zhixuan Liang", "Xingyu Zeng", "Rui Zhao", "Ping Luo"], "title": "Performance-guided Reinforced Active Learning for Object Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICASSP 2026. Camera-ready Version", "summary": "Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.", "AI": {"tldr": "本文提出了一种新的基于性能引导的强化主动学习方法（MGRAL）用于对象检测，旨在通过最小化标注工作量实现高性能模型训练。", "motivation": "现有主动学习策略主要关注数据分布或内在信息含量，而忽视了与下游任务性能（如平均精度mAP）的直接关联，本文因此提出了新的方法来解决这一问题。", "method": "MGRAL采用强化学习代理优化选取样本，以模型输出变化预期为信息度量，使用策略梯度进行选择优化，并通过无监督方法和快速查找表减少计算开销。", "result": "实验结果表明，在PASCAL VOC和COCO基准上，MGRAL在对象检测任务中的主动学习性能表现最佳，具有最高的AL曲线和令人信服的可视化效果。", "conclusion": "本文建立了一种新的强化学习驱动下的主动对象检测范式，并验证了其方法的有效性。"}}
{"id": "2601.15687", "pdf": "https://arxiv.org/pdf/2601.15687", "abs": "https://arxiv.org/abs/2601.15687", "authors": ["Khusrav Badalov", "Young Yoon"], "title": "FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.", "AI": {"tldr": "本文提出了FARM模型，用于自动生成具有正确触发器输出与动作输入绑定的完整TAP应用程序。", "motivation": "传统方法主要集中在从自然语言预测服务级别上，导致需要手动配置才能执行的应用程序。因此，本研究旨在解决功能级配置问题，即自动生成完全可执行且正确绑定的应用程序。", "method": "FARM采用两阶段架构：第一阶段通过对比双重编码器训练（选择性层冻结）从丰富化的模式中检索触发和动作函数候选；第二阶段使用基于大型语言模型的多代理流水线进行选择和配置，包括意图分析、触发选择、跨模式评分的动作选择以及配置验证。", "result": "FARM在功能级别上达到了81%的联合准确性（噪声数据为62%，单次推断为70%），映射到服务级别后也达到81%的联合准确性，比TARGE基准提高了23个百分点。此外，它生成了可执行自动化配置所需的触发器输出与动作输入绑定。", "conclusion": "FARM模型证明其在自动生成完整的、正确的Trigger-Action应用程序方面的有效性，并展示了超越现有服务级别预测方法的能力。"}}
{"id": "2601.15682", "pdf": "https://arxiv.org/pdf/2601.15682", "abs": "https://arxiv.org/abs/2601.15682", "authors": ["Wei Dong", "Li Ge"], "title": "Tight Bounds for Gaussian Mean Estimation under Personalized Differential Privacy", "categories": ["cs.DS"], "comment": null, "summary": "We study mean estimation for Gaussian distributions under \\textit{personalized differential privacy} (PDP), where each record has its own privacy budget. PDP is commonly considered in two variants: \\textit{bounded} and \\textit{unbounded} PDP. In bounded PDP, the privacy budgets are public and neighboring datasets differ by replacing one record. In unbounded PDP, neighboring datasets differ by adding or removing a record; consequently, an algorithm must additionally protect participation information, making both the dataset size and the privacy profile sensitive. Existing works have only studied mean estimation over bounded distributions under bounded PDP. Different from mean estimation for distributions with bounded range, where each element can be treated equally and we only need to consider the privacy diversity of elements, the challenge for Gaussian is that, elements can have very different contributions due to the unbounded support. we need to jointly consider the privacy information and the data values. Such a problem becomes even more challenging under unbounded PDP, where the privacy information is protected and the way to compute the weights becomes unclear. In this paper, we address these challenges by proposing optimal Gaussian mean estimators under both bounded and unbounded PDP, where in each setting we first derive lower bounds for both problems, following PDP mean estimators with the algorithmic upper bounds matching the corresponding lower bounds up to logarithmic factors.", "AI": {"tldr": "本文研究了在个性化差异隐私（PDP）条件下，对高斯分布的均值估计问题。", "motivation": "动机在于解决现有工作中未涉及的高斯分布下的个性化差异隐私挑战，特别是在无界PDP条件下的难题。", "method": "提出了一种针对有界和无界个性化差异隐私的最优高斯均值估计算法，并在每个设置中首先推导了问题的下限。", "result": "算法的上限与对应的下限相匹配，误差相差对数因子。", "conclusion": "研究结果表明，在个性化差异隐私条件下可以有效地估计高斯分布的均值。"}}
{"id": "2601.15681", "pdf": "https://arxiv.org/pdf/2601.15681", "abs": "https://arxiv.org/abs/2601.15681", "authors": ["Yikui Zhai", "Shikuang Liu", "Wenlve Zhou", "Hongsheng Zhang", "Zhiheng Zhou", "Xiaolin Tian", "C. L. Philip Chen"], "title": "Consistency-Regularized GAN for Few-Shot SAR Target Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.", "AI": {"tldr": "本文提出了一种名为一致性正则化的生成对抗网络（Cr-GAN）的方法，用于解决合成孔径雷达图像中的少量样本目标识别问题。", "motivation": "由于合成孔径雷达图像数据极度稀缺，现有的生成对抗网络需要大量的数据进行稳定训练，这与少量样本学习的前提相矛盾。因此，本文旨在开发一种新的方法来克服这一瓶颈。", "method": "Cr-GAN引入了一个双分支鉴别器，将对抗性训练和表示学习分离，并采用通道特征插值策略创造新颖的潜在特征，结合跨域循环一致性机制确保语义完整性。", "result": "实验结果表明，在MSTAR和SRSDD数据集上，Cr-GAN在8-shot设置下的准确率分别为71.21%和51.64%，显著优于现有领先基线，并且只需要不到状态-of-the-art扩散模型的5个参数量。", "conclusion": "本文提出的方法能够有效地生成高质量的合成数据以支持少量样本学习，从而提高了SAR图像目标识别性能。"}}
{"id": "2601.15679", "pdf": "https://arxiv.org/pdf/2601.15679", "abs": "https://arxiv.org/abs/2601.15679", "authors": ["Ee Wei Seah", "Yongsen Zheng", "Naga Nikshith", "Mahran Morsidi", "Gabriel Waikin Loh Matienzo", "Nigel Gay", "Akriti Vij", "Benjamin Chua", "En Qi Ng", "Sharmini Johnson", "Vanessa Wilfred", "Wan Sie Lee", "Anna Davidson", "Catherine Devine", "Erin Zorer", "Gareth Holvey", "Harry Coppock", "James Walpole", "Jerome Wynee", "Magda Dubois", "Michael Schmatz", "Patrick Keane", "Sam Deverett", "Bill Black", "Bo Yan", "et al. (45 additional authors not shown)"], "title": "Improving Methodologies for Agentic Evaluations Across Domains: Leakage of Sensitive Information, Fraud and Cybersecurity Threats", "categories": ["cs.AI"], "comment": "The author/contributor list organises contributors by country and alphabetical order within each country. In some places, the order has been altered to match other related publications", "summary": "The rapid rise of autonomous AI systems and advancements in agent capabilities are introducing new risks due to reduced oversight of real-world interactions. Yet agent testing remains nascent and is still a developing science. As AI agents begin to be deployed globally, it is important that they handle different languages and cultures accurately and securely. To address this, participants from The International Network for Advanced AI Measurement, Evaluation and Science, including representatives from Singapore, Japan, Australia, Canada, the European Commission, France, Kenya, South Korea, and the United Kingdom have come together to align approaches to agentic evaluations. This is the third exercise, building on insights from two earlier joint testing exercises conducted by the Network in November 2024 and February 2025. The objective is to further refine best practices for testing advanced AI systems. The exercise was split into two strands: (1) common risks, including leakage of sensitive information and fraud, led by Singapore AISI; and (2) cybersecurity, led by UK AISI. A mix of open and closed-weight models were evaluated against tasks from various public agentic benchmarks. Given the nascency of agentic testing, our primary focus was on understanding methodological issues in conducting such tests, rather than examining test results or model capabilities. This collaboration marks an important step forward as participants work together to advance the science of agentic evaluations.", "AI": {"tldr": "本文描述了国际先进人工智能测量、评估和科学网络的第三次联合测试活动，旨在改进针对自主AI系统的方法论。", "motivation": "随着自主AI系统的迅速崛起及其能力的进步，新的风险正在出现。为了应对这些挑战并确保全球部署时的语言和文化准确性及安全性，来自多个国家的专家合作进行测试方法的统一。", "method": "此次活动分为两部分：一是由新加坡AISI领导的风险检测，包括敏感信息泄露与欺诈；二是由英国AISI领导的网络安全。使用混合开放和封闭权重模型，并基于各种公开代理基准进行评估。", "result": "鉴于代理测试仍处于早期发展阶段，主要关注的是理解和解决方法论问题，而不是具体的测试结果或模型能力。", "conclusion": "这一合作标志着向统一和改进先进AI系统测试方法的方向迈出了重要一步。"}}
{"id": "2601.15678", "pdf": "https://arxiv.org/pdf/2601.15678", "abs": "https://arxiv.org/abs/2601.15678", "authors": ["Mengyu Yao", "Ziqi Zhang", "Ning Luo", "Shaofei Li", "Yifeng Cai", "Xiangqun Chen", "Yao Guo", "Ding Li"], "title": "Connect the Dots: Knowledge Graph-Guided Crawler Attack on Retrieval-Augmented Generation Systems", "categories": ["cs.CR", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems integrate document retrieval with large language models and have been widely adopted. However, in privacy-related scenarios, RAG introduces a new privacy risk: adversaries can issue carefully crafted queries to exfiltrate sensitive content from the underlying corpus gradually. Although recent studies have demonstrated multi-turn extraction attacks, they rely on heuristics and fail to perform long-term extraction planning. To address these limitations, we formulate the RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In ASCP, each query is treated as a probabilistic action that aims to maximize conditional marginal gain (CMG), enabling principled long-term planning under uncertainty. However, integrating ASCP with practical RAG attack faces three key challenges: unobservable CMG, intractability in the action space, and feasibility constraints. To overcome these challenges, we maintain a global attacker-side state to guide the attack. Building on this idea, we introduce RAGCRAWLER, which builds a knowledge graph to represent revealed information, uses this global state to estimate CMG, and plans queries in semantic space that target unretrieved regions. In comprehensive experiments across diverse RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently outperforms all baselines. It achieves up to 84.4% corpus coverage within a fixed query budget and deliver an average improvement of 20.7% over the top-performing baseline. It also maintains high semantic fidelity and strong content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER proves its robustness by maintaining effectiveness against advanced RAG systems employing query rewriting and multi-query retrieval strategies. Our work reveals significant security gaps and highlights the pressing need for stronger safeguards for RAG.", "AI": {"tldr": "本文提出了RAGCRAWLER方法，通过构建知识图谱引导的网络爬虫攻击来提取检索增强生成系统中的敏感信息。", "motivation": "现有研究依赖于启发式方法进行多轮抽取攻击，并且无法实现长期规划。为了克服这些限制并解决隐私风险问题，本文提出了一种新的解决方案。", "method": "作者将RAG提取攻击定义为自适应随机覆盖问题（ASCP），通过构建知识图谱来表示已揭示的信息，并在此基础上估计条件边际增益（CMG）以计划语义空间中的查询。", "result": "实验结果显示，与所有基线方法相比，RAGCRAWLER在广泛的检索增强生成架构和数据集中表现更为优越。其能实现高达84.4%的语料库覆盖率，并且提高了20.7%的性能。", "conclusion": "本文揭示了检索增强生成系统中的显著安全漏洞，并强调需要加强保护措施来防止此类攻击。"}}
{"id": "2601.15676", "pdf": "https://arxiv.org/pdf/2601.15676", "abs": "https://arxiv.org/abs/2601.15676", "authors": ["Hengfan Zhang", "Yueqian Lin", "Hai Helen Li", "Yiran Chen"], "title": "Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture for Edge Audio Systems", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": "10 pages, 3 figures, 2 tables. Preprint", "summary": "Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a persistent tension between perception depth and computational efficiency. Lightweight local models tend to produce passive perception - generic summaries that miss the subtle evidence required for multi-step audio reasoning - while indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost, and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent), a hybrid architecture targeting edge servers and gateways. It performs fast local perception and triggers conditional forensic refinement only when uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B Audio-LLM, then a cloud controller gates difficult cases and issues lightweight plans for on-device tools such as temporal re-listening and local ASR. On the MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while achieving a better accuracy-efficiency trade-off than an always-on investigation pipeline. Overall, CoFi-Agent bridges the perception gap via tool-enabled, conditional edge-cloud collaboration under practical system constraints.", "AI": {"tldr": "本文提出了一种轻量级的粗到细架构CoFi-Agent，旨在在边缘设备上部署音频语言模型时平衡感知深度和计算效率。", "motivation": "文章动机在于解决在边缘基础设施中部署Audio-LLMs时面临的感知深度与计算效率之间的矛盾。轻量级本地模型产生的被动感知无法捕捉多步推理所需的细微证据，而无差别地将任务卸载到云端会导致不可接受的延迟、带宽成本和隐私风险。", "method": "CoFi-Agent采用了一种混合架构，在边缘服务器和网关上进行快速局部感知，并在检测到不确定性时触发条件性的深入分析。该方法首先在一个本地7B Audio-LLM上执行单次通过，然后由云控制器对复杂情况进行筛选并为设备上的工具（如时间重新监听和局部ASR）生成轻量级计划。", "result": "实验结果显示，在MMAR基准测试中，CoFi-Agent的准确率从27.20%提升到53.60%，并且比始终开启的调查管道实现了更好的准确性-效率折衷。", "conclusion": "总体上，CoFi-Agent通过工具增强、条件性的边缘云协作在实际系统约束下成功弥合了感知差距。"}}
{"id": "2601.15673", "pdf": "https://arxiv.org/pdf/2601.15673", "abs": "https://arxiv.org/abs/2601.15673", "authors": ["Qilong Yan", "Yifei Xing", "Dugang Liu", "Jingpu Duan", "Jian Yin"], "title": "Enhancing guidance for missing data in diffusion-based sequential recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "ICASSP 2026 accecpted", "summary": "Contemporary sequential recommendation methods are becoming more complex, shifting from classification to a diffusion-guided generative paradigm. However, the quality of guidance in the form of user information is often compromised by missing data in the observed sequences, leading to suboptimal generation quality. Existing methods address this by removing locally similar items, but overlook ``critical turning points'' in user interest, which are crucial for accurately predicting subsequent user intent. To address this, we propose a novel Counterfactual Attention Regulation Diffusion model (CARD), which focuses on amplifying the signal from key interest-turning-point items while concurrently identifying and suppressing noise within the user sequence. CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences undergoing significant interest shift, and (2) a counterfactual attention mechanism for these sequences to quantify the importance of each item. In this manner, CARD provides the diffusion model with a high-quality guidance signal composed of dynamically re-weighted interaction vectors to enable effective generation. Experiments show our method works well on real-world data without being computationally expensive. Our code is available at https://github.com/yanqilong3321/CARD.", "AI": {"tldr": "本文提出了一种名为CARD的新模型，用于增强在扩散引导的序列推荐中处理缺失数据时的指导质量。", "motivation": "现有方法通过移除局部相似项目来解决用户信息中的缺失数据问题，但忽视了关键的兴趣转折点，这对准确预测后续用户意图至关重要。", "method": "CARD模型包括双侧汤普森采样法来识别兴趣变化显著的序列，以及对这些序列应用反事实注意力机制以量化每个项目的权重。", "result": "实验表明该方法在现实世界数据上表现良好且计算效率高。", "conclusion": "CARD通过增强关键兴趣转折点信号并抑制用户序列中的噪声，为扩散模型提供高质量的指导信号，从而实现有效生成。"}}
{"id": "2601.15671", "pdf": "https://arxiv.org/pdf/2601.15671", "abs": "https://arxiv.org/abs/2601.15671", "authors": ["Ziyi Wang", "Yilong Dai", "Duanya Lyu", "Mateo Nader", "Sihan Chen", "Wanghao Ye", "Zjian Ding", "Xiang Yan"], "title": "StreetDesignAI: A Multi-Persona Evaluation System for Inclusive Infrastructure Design", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Designing inclusive cycling infrastructure requires balancing competing needs of diverse user groups, yet designers often struggle to anticipate how different cyclists experience the same street. We investigate how persona-based multi-agent evaluation can support inclusive design by making experiential conflicts explicit. We present StreetDesignAI, an interactive system that enables designers to (1) ground evaluation in street context through imagery and map data, (2) receive parallel feedback from cyclist personas spanning confident to cautious users, and (3) iteratively modify designs while surfacing conflicts across perspectives. A within-subjects study with 26 transportation professionals demonstrates that structured multi-perspective feedback significantly improves designers' understanding of diverse user perspectives, ability to identify persona needs, and confidence in translating them into design decisions, with higher satisfaction and stronger intention for professional adoption. Qualitative findings reveal how conflict surfacing transforms design exploration from single-perspective optimization toward deliberate trade-off reasoning. We discuss implications for AI tools that scaffold inclusive design through disagreement as an interaction primitive.", "AI": {"tldr": "开发了一种名为StreetDesignAI的多角色评估系统，用于支持包容性基础设施设计。", "motivation": "旨在解决设计师在平衡不同用户需求时面临的挑战，并通过基于角色的多智能体评价方法明确体验冲突来促进包容性设计。", "method": "提出了一个交互式系统StreetDesignAI，该系统允许设计师根据街道背景进行评估、接收来自多种骑行者角色的反馈并迭代修改设计以揭示视角间的矛盾。", "result": "通过26位交通专业人员的研究表明，结构化多角度反馈显著提升了设计师对不同用户观点的理解、识别角色需求的能力以及将这些需求转化为设计决策的信心。参与者表现出更高的满意度和更强的专业采纳意愿。", "conclusion": "讨论了AI工具如何通过冲突作为交互原语来支撑包容性设计，揭示了从单视角优化到有意识的权衡推理的设计探索转变。"}}
{"id": "2601.15668", "pdf": "https://arxiv.org/pdf/2601.15668", "abs": "https://arxiv.org/abs/2601.15668", "authors": ["Dingdong Wang", "Shujie Liu", "Tianhua Zhang", "Youjun Chen", "Jinyu Li", "Helen Meng"], "title": "EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning", "categories": ["cs.SD"], "comment": null, "summary": "Emotional information in speech plays a unique role in multimodal perception. However, current Speech Large Language Models (SpeechLLMs), similar to conventional speech emotion recognition (SER) systems, still treat emotion understanding as a simple classification problem. This provides limited interpretability of predictions, while leaving the LLMs' expressive and reasoning capabilities underutilized. In this work, we take the first step to reformulate SER as a deep reasoning problem through reinforcement learning (RL). We propose EmotionThinker, which is designed to generate accurate emotion predictions with interpretable explanations grounded in fine-grained acoustic cues. To achieve this, we first construct EmotionCoT-35K, an emotional reasoning dataset with Chain-of-Thought annotations and detailed captions. Second, we observe that current SpeechLLMs exhibit weak prosody perception, whereas prosodic cues constitute fundamental signals for interpreting emotions. To address this, we develop the prosody-enhanced foundation model EmotionThinker-Base, and demonstrate that prosody enhancement improves emotion understanding. Third, we introduce Group-Relative-Policy-Optimization with Progressive-Trust-aware-Reasoning-Reward (GRPO-PTR) for RL. Different from standard GRPO, which relies only on rule-based outcome rewards, GRPO-PTR progressively introduces reasoning reward, dynamically adjusts it with a trustworthiness weight reflecting the alignment between reasoning and outcome, and evaluates the overall reasoning quality with a reward model based on multi-dimensional criteria. EmotionThinker outperforms previous state-of-the-art evaluation models both in emotion accuracy and explanation quality, advancing SER toward interpretable multimodal reasoning. Project page: https://github.com/dingdongwang/EmotionThinker", "AI": {"tldr": "提出EmotionThinker，通过强化学习和增强音调感知能力来改进语音情感识别。", "motivation": "现有的语音大语言模型将情感理解视为简单的分类问题，缺乏预测的解释性和推理能力。因此，研究者试图通过深度推理重新定义语音情感识别。", "method": "构建了EmotionCoT-35K数据集，并开发了音调增强基础模型EmotionThinker-Base。采用Group-Relative-Policy-Optimization和Progressive-Trust-aware-Reasoning-Reward（GRPO-PTR）强化学习方法来优化情感理解。", "result": "与之前最先进的情感识别模型相比，EmotionThinker在情感准确度和解释质量上均有所提升。", "conclusion": "通过引入深度推理机制和音调增强技术，语音情感识别可以向具有可解释性的多模态推理迈进。"}}
{"id": "2601.15664", "pdf": "https://arxiv.org/pdf/2601.15664", "abs": "https://arxiv.org/abs/2601.15664", "authors": ["Hongyang Wei", "Hongbo Liu", "Zidong Wang", "Yi Peng", "Baixin Xu", "Size Wu", "Xuying Zhang", "Xianglong He", "Zexiang Liu", "Peiyu Wang", "Xuchen Song", "Yangguang Li", "Yang Liu", "Yahui Zhou"], "title": "Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.", "AI": {"tldr": "本文介绍了Skywork UniPic 3.0，一种统一的多图像合成框架，旨在解决人类对象交互（HOI）相关的多图像编辑任务，并通过序列建模方法实现高质量融合。", "motivation": "鉴于社区对多图像合成任务的兴趣显著增长，而现有模型在该领域的细节披露不足且面临质量和一致性挑战，作者提出了Skywork UniPic 3.0以解决这些难题并专注于HOI相关的任务。", "method": "通过设计全面的数据收集、过滤和合成管道以及将多图像组合问题建模为序列建模问题来实现高质量的多图像合成，并利用轨迹映射与分布匹配加速推理过程。", "result": "Skywork UniPic 3.0在单个图像编辑基准测试中达到了最先进的性能，在多图像合成基准上超越了Nano-Banana和Seedream 4.0，模型能在8步内生成高质量样本并实现12.5倍的提速。", "conclusion": "验证了数据管道与训练范式的有效性，表明Skywork UniPic 3.0能够有效应对多图像合成挑战，并在质量和速度方面提供优越的表现。"}}
{"id": "2601.15663", "pdf": "https://arxiv.org/pdf/2601.15663", "abs": "https://arxiv.org/abs/2601.15663", "authors": ["Kristen Moore", "Diksha Goel", "Cody James Christopher", "Zhen Wang", "Minjune Kim", "Ahmed Ibrahim", "Ahmad Mohsin", "Seyit Camtepe"], "title": "TempoNet: Learning Realistic Communication and Timing Patterns for Network Traffic Simulation", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Realistic network traffic simulation is critical for evaluating intrusion detection systems, stress-testing network protocols, and constructing high-fidelity environments for cybersecurity training. While attack traffic can often be layered into training environments using red-teaming or replay methods, generating authentic benign background traffic remains a core challenge -- particularly in simulating the complex temporal and communication dynamics of real-world networks. This paper introduces TempoNet, a novel generative model that combines multi-task learning with multi-mark temporal point processes to jointly model inter-arrival times and all packet- and flow-header fields. TempoNet captures fine-grained timing patterns and higher-order correlations such as host-pair behavior and seasonal trends, addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail to reproduce structured temporal variation. TempoNet produces temporally consistent, high-fidelity traces, validated on real-world datasets. Furthermore, we show that intrusion detection models trained on TempoNet-generated background traffic perform comparably to those trained on real data, validating its utility for real-world security applications.", "AI": {"tldr": "介绍了一种新的生成模型TempoNet，用于模拟现实世界网络中的真实通信和时间模式。", "motivation": "为了评估入侵检测系统、对网络协议进行压力测试以及构建高保真的网络安全训练环境，需要生成具有实际背景流量的逼真网络流量。现有方法难以再现结构化的时态变化，因此提出了TempoNet来解决这一挑战。", "method": "TempoNet结合多任务学习和多标记时间点过程技术，联合建模到达间隔时间和所有数据包及流头字段，捕捉精细的时间模式和更高阶的相关性，如主机对行为和季节趋势。", "result": "实验验证了生成的流量在现实世界数据集上的时间一致性和高保真度。基于TempoNet生成背景流量训练的入侵检测模型与使用真实数据训练的性能相当。", "conclusion": "TempoNet能够生成高质量、时间上连贯的真实网络流量，对于实际的安全应用具有实用价值。"}}
{"id": "2601.15657", "pdf": "https://arxiv.org/pdf/2601.15657", "abs": "https://arxiv.org/abs/2601.15657", "authors": ["Yinxi Tian", "Changwu Huang", "Ke Tang", "Xin Yao"], "title": "Integrating Knowledge Distillation Methods: A Sequential Multi-Stage Framework", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Knowledge distillation (KD) transfers knowledge from large teacher models to compact student models, enabling efficient deployment on resource constrained devices. While diverse KD methods, including response based, feature based, and relation based approaches, capture different aspects of teacher knowledge, integrating multiple methods or knowledge sources is promising but often hampered by complex implementation, inflexible combinations, and catastrophic forgetting, which limits practical effectiveness. This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a flexible framework that sequentially integrates heterogeneous KD methods. At each stage, the student is trained with a specific distillation method, while a frozen reference model from the previous stage anchors learned knowledge to mitigate forgetting. In addition, we introduce an adaptive weighting mechanism based on the teacher true class probability (TCP) that dynamically adjusts the reference loss per sample to balance knowledge retention and integration. By design, SMSKD supports arbitrary method combinations and stage counts with negligible computational overhead. Extensive experiments show that SMSKD consistently improves student accuracy across diverse teacher student architectures and method combinations, outperforming existing baselines. Ablation studies confirm that stage wise distillation and reference model supervision are primary contributors to performance gains, with TCP based adaptive weighting providing complementary benefits. Overall, SMSKD is a practical and resource efficient solution for integrating heterogeneous KD methods.", "AI": {"tldr": "本文提出了SMSKD框架，通过顺序整合多种知识蒸馏方法来提升学生模型的性能。", "motivation": "在现有的知识蒸馏中，虽然有多种方法可以捕捉教师模型的不同方面的知识，但整合这些方法时往往会遇到实现复杂、组合缺乏灵活性和灾难性遗忘等问题。为了解决这些问题并提高实际效果，提出了SMSKD框架。", "method": "SMSKD是一个灵活的框架，它按顺序集成不同的知识蒸馏方法，并在每个阶段使用冻结参考模型来锚定已学习的知识以减轻忘记的影响。此外，引入了一个基于教师真实类概率（TCP）的自适应加权机制来动态调整样本之间的参考损失。", "result": "实验结果显示，在各种教师学生架构和方法组合中，SMSKD都持续提高了学生的准确性，并优于现有基线。消融研究表明阶段式蒸馏和参考模型监督是性能提升的主要贡献者，基于TCP的自适应加权提供了补充收益。", "conclusion": "SMSKD提供了一种实用且资源高效的解决方案，能够整合异构的知识蒸馏方法，支持任意组合的方法并具有可忽略的计算开销。"}}
{"id": "2601.15656", "pdf": "https://arxiv.org/pdf/2601.15656", "abs": "https://arxiv.org/abs/2601.15656", "authors": ["Michael Yin", "Robert Xiao", "Nadine Wagener"], "title": "Reflective Motion and a Physical Canvas: Exploring Embodied Journaling in Virtual Reality", "categories": ["cs.HC"], "comment": "19 pages, 6 figures, accepted at CHI 2026", "summary": "In traditional journaling practices, authors express and process their thoughts by writing them down. We propose a somaesthetic-inspired alternative that uses the human body, rather than written words, as the medium of expression. We coin this embodied journaling, as people's isolated body movements and spoken words become the canvas of reflection. We implemented embodied journaling in virtual reality and conducted a within-subject user study (n=20) to explore the emergent behaviours from the process and to compare its expressive and reflective qualities to those of written journaling. When writing-based norms and affordances were absent, we found that participants defaulted towards unfiltered emotional expression, often forgoing words altogether. Rather, subconscious body motion and paralinguistic acoustic qualities unveiled deeper, sometimes hidden feelings, prompting reflection that happens after emotional expression rather than during it. We discuss both the capabilities and pitfalls of embodied journaling, ultimately challenging the idea that reflection culminates in linguistic reasoning.", "AI": {"tldr": "本研究提出了一种基于虚拟现实的具身日记形式，利用身体动作而非文字进行表达，并通过用户研究评估其表达和反思能力。", "motivation": "动机在于探索一种替代传统写作的日记方式，使用身体动作为媒介来促进深层次的情感表达与反思。", "method": "在虚拟现实中实现具身日记并进行一项包含20名参与者的单组前后测实验，以探究该方法的表现形式和效果，并将其与传统的文字日记进行对比。", "result": "发现没有写作规范的情况下，参与者更倾向于无过滤的情感表达，通过肢体动作和副语言特征揭示了深层次的情感，从而引发了事后反思。", "conclusion": "讨论了具身日记的优势和局限性，挑战了将反思等同于语言推理的传统观念。"}}
{"id": "2601.15655", "pdf": "https://arxiv.org/pdf/2601.15655", "abs": "https://arxiv.org/abs/2601.15655", "authors": ["Zhenghui Guo", "Yuanbin Man", "Junyuan Sheng", "Bowen Lin", "Ahmed Ahmed", "Bo Jiang", "Boyuan Zhang", "Miao Yin", "Sian Jin", "Omprakash Gnawal", "Chengming Zhang"], "title": "Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.", "AI": {"tldr": "本文介绍了Event-VStream，一种针对长时间视频流实时理解的事件驱动框架。", "motivation": "传统的多模态大语言模型在处理长视频流时面临冗余帧处理和快速遗忘过去上下文的问题。现有系统依赖固定间隔解码或缓存修剪，导致重复输出或丢失关键时间信息。", "method": "Event-VStream通过结合运动、语义和预测线索来检测有意义的状态转换，并仅在此类边界触发语言生成，同时将每个事件嵌入整合到持久内存库中以实现远期推理并保持低延迟。", "result": "在OVOBench-Realtime和长格式Ego4D评估上，Event-VStream实现了具有竞争力的表现，比VideoLLM-Online-8B基线提高了10.4分，并且使用通用文本骨干也能接近Flash-VStream-7B的性能。", "conclusion": "通过引入事件感知机制，Event-VStream有效解决了长视频流实时理解中的挑战，证明了其在保持低延迟的同时进行长期推理的能力。"}}
{"id": "2601.15652", "pdf": "https://arxiv.org/pdf/2601.15652", "abs": "https://arxiv.org/abs/2601.15652", "authors": ["Manish Bhatt"], "title": "Predictive Coding and Information Bottleneck for Hallucination Detection in Large Language Models", "categories": ["cs.AI", "cs.CR", "cs.ET"], "comment": null, "summary": "Hallucinations in Large Language Models (LLMs) -- generations that are plausible but factually unfaithful -- remain a critical barrier to high-stakes deployment. Current detection methods typically rely on computationally expensive external retrieval loops or opaque black-box LLM judges requiring 70B+ parameters. In this work, we introduce [Model Name], a hybrid detection framework that combines neuroscience-inspired signal design with supervised machine learning. We extract interpretable signals grounded in Predictive Coding (quantifying surprise against internal priors) and the Information Bottleneck (measuring signal retention under perturbation). Through systematic ablation, we demonstrate three key enhancements: Entity-Focused Uptake (concentrating on high-value tokens), Context Adherence (measuring grounding strength), and Falsifiability Score (detecting confident but contradictory claims). Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC, while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain), demonstrating consistent improvements across architectures. This competitive performance is achieved while using 75x less training data than Lynx (200 vs 15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully interpretable. Crucially, we report a negative result: the Rationalization signal fails to distinguish hallucinations, suggesting that LLMs generate coherent reasoning for false premises (\"Sycophancy\"). This work demonstrates that domain knowledge encoded in signal architecture provides superior data efficiency compared to scaling LLM judges, achieving strong performance with lightweight (less than 1M parameter), explainable models suitable for production deployment.", "AI": {"tldr": "本文提出了一种名为[Model Name]的混合检测框架，用于识别大型语言模型中的幻觉生成。", "motivation": "大型语言模型中的幻觉问题阻碍了其在高风险场景下的部署。当前检测方法依赖于计算成本高昂的外部检索循环或不透明的大规模参数模型，本文旨在通过引入一种更高效、可解释的方法来解决这一挑战。", "method": "该框架结合了神经科学启发的信号设计和监督机器学习。它提取基于预测编码（量化与内部先验的惊讶度）和信息瓶颈（衡量扰动下的信号保留）的可解释信号，并通过系统消融实验展示了三个关键改进：实体聚焦吸收、上下文依附性和可证伪性得分。", "result": "在HaluBench数据集上的评估显示，该理论指导基线达到了0.8017 AUROC。监督模型达到0.8274 AUROC，而改进特征将性能提升至0.8669 AUROC，显示出跨架构的一致性提升，并使用更少的数据和更快的推理速度。", "conclusion": "该研究证明了领域知识嵌入信号架构可实现比扩展大型语言模型更好的数据效率，其轻量级且解释性的模型适用于生产部署。"}}
{"id": "2601.15644", "pdf": "https://arxiv.org/pdf/2601.15644", "abs": "https://arxiv.org/abs/2601.15644", "authors": ["Zichen Yu", "Quanli Liu", "Wei Wang", "Liyong Zhang", "Xiaoguang Zhao"], "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.", "AI": {"tldr": "本文提出SuperOcc框架，用于基于超四面体的3D占用预测，并通过时间建模、多超四面体解码策略和高效的超四面体到体素散射方案解决现有方法的问题。", "motivation": "传统的密集场景表示方法忽略了现实世界驾驶场景中的稀疏性，而现有的超四面体框架在时空建模、几何表达与查询稀疏性的平衡以及计算效率方面存在问题。", "method": "SuperOcc包括三个关键设计：（1）统一的时间建模机制以同时利用视角中心和对象中心时间线索；（2）多超四面体解码策略增强几何表达力而不牺牲查询稀疏性；（3）高效的超四面体到体素散射方案提高计算效率。", "result": "SuperOcc在SurroundOcc和Occ3D基准测试中实现了最先进的性能，同时保持了更高的效率。", "conclusion": "通过综合的时间建模、多超四面体解码策略以及高效的数据转换，SuperOcc提高了基于超四面体的3D占用预测的效果，并提升了计算效率。"}}
{"id": "2601.15643", "pdf": "https://arxiv.org/pdf/2601.15643", "abs": "https://arxiv.org/abs/2601.15643", "authors": ["Bo Yuan", "Danpei Zhao", "Wentao Li", "Tian Li", "Zhiguo Jiang"], "title": "Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14242", "summary": "Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.", "AI": {"tldr": "本文提出了一种持续全景感知模型（CPP），该模型结合了多模态和多任务连续学习，以提高图像的综合理解。", "motivation": "传统的单任务连续学习研究限制了在多任务和多模态场景下的应用潜力。除了灾难性遗忘的问题外，多任务连续学习还带来了跨模态对齐中的语义模糊问题。", "method": "论文提出了一个端到端的持续全景感知模型，包括协作跨模态编码器（CCE）进行多模态嵌入，并通过对比特征蒸馏和实例蒸馏提出可塑的知识继承模块。此外，还开发了跨模态一致性约束和CPP+，确保在多任务增量场景下更新模型时的语义对齐。", "result": "实验表明所提出的模型在处理多样化的连续学习任务上具有优势，尤其是在细粒度连续学习任务中表现突出。", "conclusion": "该研究成功地将连续学习扩展到持续全景感知领域，并通过一系列实验验证了其方法的有效性。"}}
{"id": "2601.15630", "pdf": "https://arxiv.org/pdf/2601.15630", "abs": "https://arxiv.org/abs/2601.15630", "authors": ["Chandra Prakash", "Mary Lind", "Avneesh Sisodia"], "title": "Agentic AI Governance and Lifecycle Management in Healthcare", "categories": ["cs.AI"], "comment": "9 Page, 3 figures", "summary": "Healthcare organizations are beginning to embed agentic AI into routine workflows, including clinical documentation support and early-warning monitoring. As these capabilities diffuse across departments and vendors, health systems face agent sprawl, causing duplicated agents, unclear accountability, inconsistent controls, and tool permissions that persist beyond the original use case. Existing AI governance frameworks emphasize lifecycle risk management but provide limited guidance for the day-to-day operations of agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint derived from a rapid, practice-oriented synthesis of governance standards, agent security literature, and healthcare compliance requirements. UALM maps recurring gaps onto five control-plane layers: (1) an identity and persona registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context and memory, (4) runtime policy enforcement with kill-switch triggers, and (5) lifecycle management and decommissioning linked to credential revocation and audit logging. A companion maturity model supports staged adoption. UALM offers healthcare CIOs, CISOs, and clinical leaders an implementable pattern for audit-ready oversight that preserves local innovation and enables safer scaling across clinical and administrative domains.", "AI": {"tldr": "本文提出了一个统一代理生命周期管理（UALM）蓝图，以应对医疗保健组织中嵌入能动AI时面临的挑战。", "motivation": "随着医疗机构将能动AI嵌入日常工作流程中，出现了代理扩散的问题，导致重复的代理、责任不明确、控制不一致等问题。现有的AI治理框架未能充分指导日常运营。", "method": "通过快速实践导向的合成治理标准、代理安全文献和医疗合规要求，提出了UALM蓝图，并设计了成熟度模型以支持分阶段采用。", "result": "UALM蓝图将反复出现的缺口映射到五个控制层：身份和个人注册库、编排与跨域调解、受PHI约束的环境和记忆、运行时策略执行和生命周期管理及注销，为医疗保健CIOs、CISOs和临床领导提供了可实施的模式。", "conclusion": "UALM蓝图能够提供审计就绪监督，同时保留地方创新并支持在临床和行政领域更安全地扩展。"}}
{"id": "2601.15628", "pdf": "https://arxiv.org/pdf/2601.15628", "abs": "https://arxiv.org/abs/2601.15628", "authors": ["Haibo Tong", "Zeyang Yue", "Feifei Zhao", "Erliang Lin", "Lu Jia", "Ruolin Chen", "Yinqian Sun", "Qian Zhang", "Yi Zeng"], "title": "CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human Cognition for Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Whether Large Language Models (LLMs) truly possess human-like Theory of Mind (ToM) capabilities has garnered increasing attention. However, existing benchmarks remain largely restricted to narrow paradigms like false belief tasks, failing to capture the full spectrum of human cognitive mechanisms. We introduce CogToM, a comprehensive, theoretically grounded benchmark comprising over 8000 bilingual instances across 46 paradigms, validated by 49 human annotator.A systematic evaluation of 22 representative models, including frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance heterogeneities and highlights persistent bottlenecks in specific dimensions. Further analysis based on human cognitive patterns suggests potential divergences between LLM and human cognitive structures. CogToM offers a robust instrument and perspective for investigating the evolving cognitive boundaries of LLMs.", "AI": {"tldr": "介绍了CogToM，一个全面的、基于人类认知的大型语言模型（LLMs）理论心理基准测试。", "motivation": "现有的ToM基准测试局限于狭窄的概念，无法完全捕捉到人类认知机制的全部范围。为了研究LLMs是否真正具备与人类相似的认知能力，需要更全面的评估方法。", "method": "开发了包含8000多个双语实例和46种范式的CogToM，并通过49个人类注释者验证。对22个代表性模型进行了系统性评估。", "result": "发现前沿模型在特定维度上存在显著的性能差异，表明LLMs与人类认知结构可能存在分歧。", "conclusion": "CogToM提供了一个强大的工具和视角来调查LLMs的认知边界和发展趋势。"}}
{"id": "2601.15626", "pdf": "https://arxiv.org/pdf/2601.15626", "abs": "https://arxiv.org/abs/2601.15626", "authors": ["Lili Chen", "Winn Wing-Yiu Chow", "Stella Peng", "Bencheng Fan", "Sachitha Bandara"], "title": "Bridging Qualitative Rubrics and AI: A Binary Question Framework for Criterion-Referenced Grading in Engineering", "categories": ["eess.SY", "cs.AI"], "comment": "Proceedings of the 36th Annual Conference of the Australasian Association for Engineering Education (AAEE 2025)", "summary": "PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a criterion-referenced grading framework to improve the efficiency and quality of grading for mathematical assessments in engineering. It specifically explores the challenges demonstrators face with manual, model solution-based grading and how a GenAI-supported system can be designed to reliably identify student errors, provide high-quality feedback, and support human graders. The research also examines human graders' perceptions of the effectiveness of this GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that GenAI achieved an overall grading accuracy of 92.5%, comparable to two experienced human graders. The two researchers, who also served as subject demonstrators, perceived the GenAI as a helpful second reviewer that improved accuracy by catching small errors and provided more complete feedback than they could manually. A central outcome was the significant enhancement of formative feedback. However, they noted the GenAI tool is not yet reliable enough for autonomous use, especially with unconventional solutions. CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when paired with a structured, criterion-referenced framework using binary questions, can grade engineering mathematical assessments with an accuracy comparable to human experts. Its primary contribution is a novel methodological approach that embeds the generation of high-quality, scalable formative feedback directly into the assessment workflow. Future work should investigate student perceptions of GenAI grading and feedback.", "AI": {"tldr": "研究探讨了如何将生成式AI与基于标准的评分框架相结合，以提高工程数学评估中的评分效率和质量。", "motivation": "该研究旨在解决手动基于模型解决方案进行评分时面临的挑战，并探索一种由生成式AI支持的系统，该系统能可靠地识别学生错误，提供高质量反馈，并辅助人工评卷者。", "method": "通过设计一个用于数学评估的生成式AI辅助系统，并采用二元问题框架来实现基于标准的评分。研究还考察了人类评分者的观点，以评价这种方法的有效性。", "result": "研究表明，生成式AI在工程数学评估中的评分准确率为92.5%，与两名经验丰富的评卷者相当。研究人员认为该工具作为辅助审阅者能提高准确性并提供更全面的反馈。", "conclusion": "研究证实了将基于标准框架的二元问题与生成式AI结合可以实现高质量、可扩展的形式化反馈，尽管目前还不适合自主使用，特别是针对非常规解决方案时。未来的研究应探讨学生对生成式AI评分和反馈的看法。"}}
{"id": "2601.15625", "pdf": "https://arxiv.org/pdf/2601.15625", "abs": "https://arxiv.org/abs/2601.15625", "authors": ["Zhiwei Zhang", "Fei Zhao", "Rui Wang", "Zezhong Wang", "Bin Liang", "Jiakang Wang", "Yao Hu", "Shaosheng Cao", "Kam-Fai Wong"], "title": "Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 4 figures, 2 tables", "summary": "Large language models (LLMs) can call tools effectively, yet they remain brittle in multi-turn execution: following a tool call error, smaller models often degenerate into repetitive invalid re-invocations, failing to interpret error feedback and self-correct. This brittleness hinders reliable real-world deployment, where the execution errors are inherently inevitable during tool interaction procedures. We identify a key limitation of current approaches: standard reinforcement learning (RL) treats errors as sparse negative rewards, providing no guidance on how to recover, while pre-collected synthetic error-correction datasets suffer from distribution mismatch with the model's on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework that converts execution errors into corrective supervision within the RL training loop. Our core mechanism fissions each failed trajectory into a new training instance by augmenting it with diagnostic feedback from a finetuned Error Simulator, then resampling recovery rollouts on-policy. This enables the model to learn from the precise errors it makes during exploration, rather than from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially, yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and outperforming specialized tool-use agents.", "AI": {"tldr": "本文提出了Fission-GRPO框架，通过将执行错误转化为强化学习训练中的纠正监督信息，提高大型语言模型在多轮工具使用过程中从执行错误中恢复的能力。", "motivation": "现有方法对于处理多轮工具调用过程中的执行错误效果不佳，导致模型容易重复无效的调用，并且无法自我修正。这限制了其在实际应用中的可靠性，因为执行错误是不可避免的。", "method": "Fission-GRPO框架将每次失败轨迹通过增强诊断反馈转化为新的训练实例，这些反馈来自一个微调过的错误模拟器，并在此基础上重新采样恢复策略。这种方法使模型能够从探索过程中遇到的确切错误中学习，而不是依赖于静态、预收集的错误案例。", "result": "在BFCL v4 Multi-Turn数据集上，Fission-GRPO将Qwen3-8B模型的错误恢复率提升了5.7%，整体准确度提高了4%（从42.75%到46.75%），并超过了专门用于工具使用的代理。", "conclusion": "研究表明，通过将执行错误转化为强化学习训练中的纠正监督信息可以显著提升大型语言模型在多轮工具使用过程中自我恢复的能力，并提高其整体性能。"}}
{"id": "2601.15624", "pdf": "https://arxiv.org/pdf/2601.15624", "abs": "https://arxiv.org/abs/2601.15624", "authors": ["Ning Jiang", "Dingheng Zeng", "Yanhong Liu", "Haiyang Yi", "Shijie Yu", "Minghe Weng", "Haifeng Shen", "Ying Li"], "title": "Explainable Deepfake Detection with RL Enhanced Self-Blended Images", "categories": ["cs.CV"], "comment": "Accepted at ICASSP 2026", "summary": "Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.", "AI": {"tldr": "本文提出了一种基于自我混合图像和强化学习的可解释深度伪造检测框架。", "motivation": "现有的深度伪造检测方法缺乏可解释性，高质量标注数据稀缺且成本高。研究者希望通过利用多模态大型语言模型（MLLMs）提高深度伪造检测的透明度，并通过强化学习来增强其性能和跨域泛化能力。", "method": "提出了一个基于自我混合图像的自动链式思维（CoT）数据生成框架，以及一个使用强化学习增强的深度伪造检测方法。", "result": "实验结果表明，所提出的数据构建管道、奖励机制和反馈驱动的合成数据生成方法是有效的，并且在多个跨域基准测试中达到了与现有最先进技术相当的性能。", "conclusion": "该研究证明了利用多模态大型语言模型和强化学习可以有效地提高深度伪造检测的可解释性和性能。"}}
{"id": "2601.15621", "pdf": "https://arxiv.org/pdf/2601.15621", "abs": "https://arxiv.org/abs/2601.15621", "authors": ["Hangrui Hu", "Xinfa Zhu", "Ting He", "Dake Guo", "Bin Zhang", "Xiong Wang", "Zhifang Guo", "Ziyue Jiang", "Hongkun Hao", "Zishan Guo", "Xinyu Zhang", "Pei Zhang", "Baosong Yang", "Jin Xu", "Jingren Zhou", "Junyang Lin"], "title": "Qwen3-TTS Technical Report", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "https://github.com/QwenLM/Qwen3-TTS", "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\\,\\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.", "AI": {"tldr": "介绍了Qwen3-TTS系列，这是一种先进的多语言、可控的、健壮且支持流媒体的文本转语音模型。", "motivation": "旨在开发一种能够实现最先进的3秒声音克隆和基于描述控制的文本转语音系统，支持多种语言，并提供精细的声音输出控制。", "method": "采用双轨LM架构进行实时合成，结合Qwen-TTS-Tokenizer-25Hz和Qwen-TTS-Tokenizer-12Hz两种语音编码器，分别强调语义内容整合及极低码率与超低延迟流传输能力。", "result": "实验显示其在多语言测试集、InstructTTSEval以及长音频测试集中表现出色，并达到业内领先水平。", "conclusion": "Qwen3-TTS模型和编码器已以Apache 2.0许可证形式公开发布，旨在促进社区研究与开发进程。"}}
{"id": "2601.15615", "pdf": "https://arxiv.org/pdf/2601.15615", "abs": "https://arxiv.org/abs/2601.15615", "authors": ["Weiwei Wu", "Yueyang Li", "Yuhu Shi", "Weiming Zeng", "Lang Qin", "Yang Yang", "Ke Zhou", "Zhiguo Zhang", "Wai Ting Siok", "Nizhuan Wang"], "title": "Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.", "AI": {"tldr": "本文提出了一个基于区域感知时空建模和协作领域泛化的框架（RSM-CoDG），用于解决跨受试者脑电波情绪识别的挑战。", "motivation": "传统的跨受试者EEG情绪识别方法在应对个体差异导致的数据分布变化以及情感相关神经表征的复杂性方面存在局限。因此，作者旨在开发一种新的模型框架来提高跨受试者的识别准确性和泛化能力。", "method": "RSM-CoDG通过利用功能脑区划分的先验知识构建区域级空间表示以提升跨受试者的一致性；采用多尺度时间建模来描述情感引发神经活动的动态演变过程；并使用协作领域泛化策略，减少特定个体偏差，在完全未知目标受试者的设置下提高模型性能。", "result": "实验结果表明，RSM-CoDG在SEED系列数据集上持续超越现有方法，展示了其改进识别鲁棒性的有效性。", "conclusion": "研究表明，通过结合区域感知时空建模和协作领域泛化策略的框架（RSM-CoDG），可以在跨受试者脑电波情绪识别中取得更佳效果，并为提高该领域的模型性能提供了一种有效的方法。"}}
{"id": "2601.15614", "pdf": "https://arxiv.org/pdf/2601.15614", "abs": "https://arxiv.org/abs/2601.15614", "authors": ["Zichen Yan", "Yuchen Hou", "Shenao Wang", "Yichao Gao", "Rui Huang", "Lin Zhao"], "title": "AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning", "categories": ["cs.RO"], "comment": null, "summary": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.", "AI": {"tldr": "本研究提出了AION，一种基于视觉的空中室内目标导向导航框架，用于自主探索未知环境并寻找到达指定语义标签的目标对象。", "motivation": "尽管先前的研究主要集中在二维移动下的零样本目标导向导航上，但将这种能力扩展到具有三维移动能力的空中平台仍然是一个未被充分研究的领域。空中机器人虽然提供了优越的操作灵活性和搜索效率，但也带来了新的挑战，包括空间感知、动态控制和安全保障问题。", "method": "AION是一个端到端的双策略强化学习框架，它将探索行为与目标到达行为解耦为两个专门的策略，并且不依赖于外部定位或全局地图。", "result": "在AI2-THOR基准测试中评估了AION的表现，并进一步在IsaacSim使用高保真无人机模型进行了实时性能评估。实验结果表明，AION在探索、导航效率和安全性方面实现了优越的性能表现。", "conclusion": "研究表明，通过双策略强化学习框架AION能够在未知环境中实现高效的空中目标导向自主导航任务。"}}
{"id": "2601.15607", "pdf": "https://arxiv.org/pdf/2601.15607", "abs": "https://arxiv.org/abs/2601.15607", "authors": ["Lenworth Thomas", "Tjaden Bridges", "Sarah Bergbreiter"], "title": "Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor", "categories": ["cs.RO"], "comment": null, "summary": "As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors < 100 g. We use this sensor to implement a modified version of the `Cast and Surge' algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.", "AI": {"tldr": "本文提出了一种使用单个流动传感器在小型四轴飞行器上寻找气流来源的方法。", "motivation": "由于环境灾害频发且严重，有必要利用烟羽追踪技术来寻找污染物或有害颗粒物的来源。然而，在小型四轴飞行器上的气体传感器存在灵敏度低和响应时间长的问题。", "method": "本文使用了一个能够感应气流大小和方向的定制流动传感器，并实施了一种修改版的'投掷与冲涌'算法，以利用气流方向感测找到并导航至气流来源。", "result": "一系列特性实验验证了该系统能够在飞行中检测到气流并将四轴飞行器重新定向向气流。多个试验表明源寻找算法可以可靠地找到气流来源。", "conclusion": "这项工作为未来能够使用流动传感器与其他传感器协同工作的平台奠定了基础，以收集更丰富的烟羽追踪数据和进行源寻迹。"}}
{"id": "2601.15600", "pdf": "https://arxiv.org/pdf/2601.15600", "abs": "https://arxiv.org/abs/2601.15600", "authors": ["Wanqi Zhang", "Jiangen He", "Marielle Santos"], "title": "Tackling the Scaffolding Paradox: A Person-Centered Adaptive Robotic Interview Coach", "categories": ["cs.HC"], "comment": null, "summary": "Job interview anxiety is a prevalent challenge among university students and can undermine both performance and confidence in high-stakes evaluative situations. Social robots have shown promise in reducing anxiety through emotional support, yet how such systems should balance psychological safety with effective instructional guidance remains an open question. In this work, we present a three-phase iterative design study of a robotic interview coach grounded in Person-Centered Therapy (PCT) and instructional scaffolding theory. Across three weekly sessions (N=8), we systematically explored how different interaction strategies shape users' emotional experience, cognitive load, and perceived utility. Phase I demonstrated that a PCT-based robot substantially increased perceived psychological safety but introduced a Safety-Guidance Gap, in which users felt supported yet insufficiently coached. Phase II revealed a Scaffolding Paradox: immediate feedback improved clarity but disrupted conversational flow and increased cognitive load, whereas delayed feedback preserved realism but lacked actionable specificity. To resolve this tension, Phase III introduced an Agency-Driven Interaction Mode that allowed users to opt in to feedback dynamically. Qualitative findings indicated that user control acted as an anxiety buffer, restoring trust, reducing overload, and reframing the interaction as collaborative rather than evaluative. Quantitative measures further showed significant reductions in interview-related social and communication anxiety, while maintaining high perceived warmth and therapeutic alliance. We synthesize these findings into an Adaptive Scaffolding Ecosystem framework, highlighting user agency as a key mechanism for balancing emotional support and instructional guidance in social robot coaching systems.", "AI": {"tldr": "该论文探讨了一种基于人格中心疗法和教学支架理论的机器人面试教练的设计，旨在解决如何在情感支持与有效指导之间找到平衡的问题。", "motivation": "论文动机在于应对大学生在求职面试中的焦虑问题，并探索社会机器人如何在提供心理安全感的同时给予有效的指导。", "method": "作者采用了一个三阶段迭代设计研究方法，通过三个每周一次的会话（N=8），系统地探讨了不同的互动策略对用户情感体验、认知负荷和使用价值的影响。", "result": "结果表明，人格中心疗法机器人可以显著提高心理安全感但存在指导不足的问题；直接反馈增加了清晰度但破坏了对话流畅性并增加认知负荷，而延迟反馈则保持真实感却缺乏具体的操作建议。通过引入用户控制的互动模式解决了这一矛盾，并在量化的社会和沟通焦虑方面取得了显著降低。", "conclusion": "论文总结了一个自适应支架生态系统框架，强调用户自主权是平衡情感支持与指导的关键机制，在社交机器人教练系统中具有重要作用。"}}
{"id": "2601.15599", "pdf": "https://arxiv.org/pdf/2601.15599", "abs": "https://arxiv.org/abs/2601.15599", "authors": ["Cecil Pang", "Hiroki Sayama"], "title": "Autonomous Business System via Neuro-symbolic AI", "categories": ["cs.AI"], "comment": "Accepted to IEEE SysCon 2026", "summary": "Current business environments require organizations to continuously reconfigure cross-functional processes, yet enterprise systems are still organized around siloed departments, rigid workflows, and hard-coded automation. Meanwhile large language models (LLMs) excel at interpreting natural language and unstructured data but lack deterministic, verifiable execution of complex business logic. To address this gap, here we introduce AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents, predicate-logic programming, and business-semantics-centric enterprise data into a coherent neuro-symbolic AI architecture for orchestrating end-to-end business initiatives. AUTOBUS models an initiative as a network of tasks with explicit pre/post conditions, required data, evaluation rules, and API-level actions. Enterprise data is organized as a knowledge graph whose entities, relationships, and constraints are translated into logic facts and foundational rules, providing the semantic grounding for task reasoning. Core AI agents synthesize task instructions, enterprise semantics, and available tools into task-specific logic programs, which are executed by a logic engine that enforces constraints, coordinates auxiliary tools, and orchestrate execution of actions and outcomes. Humans define and maintain the semantics, policies and task instructions, curate tools, and supervise high-impact or ambiguous decisions, ensuring accountability and adaptability. We detail the AUTOBUS architecture, the anatomy of the AI agent generated logic programs, and the role of humans and auxiliary tools in the lifecycle of a business initiative.", "AI": {"tldr": "本文介绍了AUTOBUS，一种结合大型语言模型（LLM）、谓词逻辑编程和企业语义中心数据的神经符号AI架构，用于协调端到端的企业倡议。", "motivation": "当前商业环境需要组织不断重新配置跨职能流程，而现有的企业系统仍然围绕着孤立的部门、刚性的工作流和硬编码自动化设计。大型语言模型（LLM）在解释自然语言和非结构化数据方面表现出色，但在执行复杂业务逻辑时缺乏确定性和可验证性。", "method": "AUTOBUS将倡议建模为具有明确前提/后置条件的任务网络，并整合了任务所需的数据、评估规则以及API级别的行动。企业数据被组织成一个知识图谱，其实体、关系和约束转化为逻辑事实和基础规则，提供任务推理的语义基础。", "result": "详细描述了AUTOBUS架构、AI代理生成的逻辑程序结构，以及人类与辅助工具在商业倡议生命周期中的角色。", "conclusion": "通过将LLM能力、逻辑编程和企业数据整合到统一的神经符号AI框架中，AUTOBUS可以更有效地协调复杂的企业操作，并确保人类监督下的责任性和适应性。"}}
{"id": "2601.15598", "pdf": "https://arxiv.org/pdf/2601.15598", "abs": "https://arxiv.org/abs/2601.15598", "authors": ["Boxuan Zhang", "Jiaxin Wang", "Zhen Xu", "Kuan Tao"], "title": "Ternary Spiking Neural Networks Enhanced by Complemented Neurons and Membrane Potential Aggregation", "categories": ["cs.NE"], "comment": "Submission in progress. Code is available at https://github.com/ZBX05/Enhanced-TernarySNN", "summary": "Spiking Neural Networks (SNNs) are promising energy-efficient models and powerful framworks of modeling neuron dynamics. However, existing binary spiking neurons exhibit limited biological plausibilities and low information capacity. Recently developed ternary spiking neuron possesses higher consistency with biological principles (i.e. excitation-inhibition balance mechanism). Despite of this, the ternary spiking neuron suffers from defects including iterative information loss, temporal gradient vanishing and irregular distributions of membrane potentials. To address these issues, we propose Complemented Ternary Spiking Neuron (CTSN), a novel ternary spiking neuron model that incorporates an learnable complemental term to store information from historical inputs. CTSN effectively improves the deficiencies of ternary spiking neuron, while the embedded learnable factors enable CTSN to adaptively adjust neuron dynamics, providing strong neural heterogeneity. Furthermore, based on the temporal evolution features of ternary spiking neurons' membrane potential distributions, we propose the Temporal Membrane Potential Regularization (TMPR) training method. TMPR introduces time-varying regularization strategy utilizing membrane potentials, furhter enhancing the training process by creating extra backpropagation paths. We validate our methods through extensive experiments on various datasets, demonstrating remarkable performance advances.", "AI": {"tldr": "本文提出了Complemented Ternary Spiking Neuron (CTSN)和Temporal Membrane Potential Regularization (TMPR)，以提高三态脉冲神经网络的性能。", "motivation": "现有的二进制脉冲神经元存在生物学一致性差和信息容量低的问题，而新发展的三态脉冲神经元虽然更符合生物原理，但仍存在迭代信息丢失、时间梯度消失和膜电位分布不规则等问题。", "method": "提出了CTSN模型，通过引入可学习的补偿项来存储历史输入的信息，并提出TMPR训练方法，利用时间变化的正则化策略增强训练过程。", "result": "实验结果表明，所提出的方法在多种数据集上表现出了显著的性能提升。", "conclusion": "CTSN和TMPR的有效性得到了验证，解决了三态脉冲神经元存在的问题，并提高了模型的整体性能。"}}
{"id": "2601.15596", "pdf": "https://arxiv.org/pdf/2601.15596", "abs": "https://arxiv.org/abs/2601.15596", "authors": ["Leying Zhang", "Tingxiao Zhou", "Haiyang Sun", "Mengxiao Bi", "Yanmin Qian"], "title": "DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any Voice", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "While modern Text-to-Speech (TTS) systems achieve high fidelity for read-style speech, they struggle to generate Autonomous Sensory Meridian Response (ASMR), a specialized, low-intensity speech style essential for relaxation. The inherent challenges include ASMR's subtle, often unvoiced characteristics and the demand for zero-shot speaker adaptation. In this paper, we introduce DeepASMR, the first framework designed for zero-shot ASMR generation. We demonstrate that a single short snippet of a speaker's ordinary, read-style speech is sufficient to synthesize high-fidelity ASMR in their voice, eliminating the need for whispered training data from the target speaker. Methodologically, we first identify that discrete speech tokens provide a soft factorization of ASMR style from speaker timbre. Leveraging this insight, we propose a two-stage pipeline incorporating a Large Language Model (LLM) for content-style encoding and a flow-matching acoustic decoder for timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive 670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a novel evaluation protocol integrating objective metrics, human listening tests, LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR generation for anyone of any voice, while maintaining competitive performance on normal speech synthesis.", "AI": {"tldr": "开发了一种名为DeepASMR的零样本ASMR语音生成框架，可以使用单个普通语音片段来合成高质量的ASMR语音。", "motivation": "现有的文本到语音系统难以生成具有放松效果的ASMR风格的声音，尤其是对于低强度、非发音特征和零样本说话者适应的需求。", "method": "提出了一种两阶段管道方法：第一阶段使用大型语言模型对内容样式进行编码，第二阶段使用流匹配声学解码器重建音色。同时构建了一个包含670小时的多说话者ASMR语音语料库和一种新的评估协议。", "result": "实验结果表明DeepASMR在ASMR生成方面达到了最先进的自然度和风格保真度，并且在正常语音合成上也具有竞争力的表现。", "conclusion": "证明了DeepASMR能够使用少量普通语音样本为任何人生成高质量的ASMR，解决了现有TTS系统面临的挑战。"}}
{"id": "2601.15595", "pdf": "https://arxiv.org/pdf/2601.15595", "abs": "https://arxiv.org/abs/2601.15595", "authors": ["Xinjie Zhou", "Zhihui Yang", "Lechao Cheng", "Sai Wu", "Gang Chen"], "title": "Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective Unlearning", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) exhibit powerful capabilities but risk memorizing sensitive personally identifiable information (PII) from their training data, posing significant privacy concerns. While machine unlearning techniques aim to remove such data, they predominantly depend on access to the training data. This requirement is often impractical, as training data in real-world deployments is commonly proprietary or inaccessible. To address this limitation, we propose Data-Free Selective Unlearning (DFSU), a novel privacy-preserving framework that removes sensitive PII from an LLM without requiring its training data. Our approach first synthesizes pseudo-PII through language model inversion, then constructs token-level privacy masks for these synthetic samples, and finally performs token-level selective unlearning via a contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive experiments on the AI4Privacy PII-Masking dataset using Pythia models demonstrate that our method effectively removes target PII while maintaining model utility.", "AI": {"tldr": "提出Data-Free Selective Unlearning（DFSU）框架，通过语言模型反转和选择性遗忘在不访问训练数据的情况下移除大型语言模型中的敏感个人信息。", "motivation": "大型语言模型可能记忆训练数据中的敏感信息，导致隐私泄露。现有的机器遗忘技术依赖于访问训练数据来解决问题，但现实中这些数据往往是专有的或不可获取的。", "method": "通过语言模型反转合成伪个人身份信息（PII），构建针对这些样本的令牌级隐私掩码，并在低秩自适应子空间内使用对比掩码损失执行选择性遗忘。", "result": "实验显示该方法能够有效地移除目标PII，同时保持模型的实用性。", "conclusion": "提出的方法证明了即使没有训练数据访问权限，也能实现有效的隐私保护措施来防止大型语言模型泄露敏感信息。"}}
{"id": "2601.15593", "pdf": "https://arxiv.org/pdf/2601.15593", "abs": "https://arxiv.org/abs/2601.15593", "authors": ["Yangyang Zhong", "Yanmei Gu", "Zhengqing Zang", "Xiaomeng Li", "Yuqi Ding", "Xibei Jia", "Yuting Shen", "Zhenzhong Lan", "Liwang Zhu", "Weiping Liu", "Junlin Zhou", "Haisheng Liu", "Zhong Xin Yu", "Pengxin Luo", "Donglian Qi", "Yunfeng Yan", "Junbo Zhao"], "title": "Parallelism and Generation Order in Masked Diffusion Language Models: Limits Today, Potential Tomorrow", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Language Models (MDLMs) promise parallel token generation and arbitrary-order decoding, yet it remains unclear to what extent current models truly realize these capabilities. We characterize MDLM behavior along two dimensions -- parallelism strength and generation order -- using Average Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning, and programming. The results show that MDLMs still lag behind comparably sized autoregressive models, mainly because parallel probabilistic modeling weakens inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior: their parallelism and generation order vary significantly with the task domain, the stage of reasoning, and whether the output is correct. On tasks that require \"backward information\" (e.g., Sudoku), MDLMs adopt a solution order that tends to fill easier Sudoku blanks first, highlighting their advantages. Finally, we provide theoretical motivation and design insights supporting a Generate-then-Edit paradigm, which mitigates dependency loss while retaining the efficiency of parallel decoding.", "AI": {"tldr": "本文研究了掩码扩散语言模型（MDLMs）在并行令牌生成和任意顺序解码方面的表现，并探讨了其在未来的发展潜力。", "motivation": "尽管掩码扩散语言模型承诺实现并行化和任意顺序的解码，但目前尚不清楚这些模型是否真正实现了这一目标。因此，作者希望通过研究来探索MDLMs在不同任务上的实际性能及其未来改进的方向。", "method": "使用平均最终并行度（AFP）和Kendall's tau来评估八个主流MDLMs（最多100B参数）在知识、推理及编程领域58个基准测试中的表现。", "result": "研究发现，尽管MDLMs表现出自适应解码行为，其并行性和生成顺序随任务域变化而显著不同，并且在需要“反向信息”的任务中具有一定优势，但整体上仍落后于同样规模的自回归模型。", "conclusion": "通过理论动机和设计建议支持“先生成后编辑”范式，可以缓解依赖损失并保留并行解码效率。"}}
{"id": "2601.15578", "pdf": "https://arxiv.org/pdf/2601.15578", "abs": "https://arxiv.org/abs/2601.15578", "authors": ["Cyril Shih-Huan Hsu", "Xi Li", "Lanfranco Zanzi", "Zhiheng Yang", "Chrysa Papagianni", "Xavier Costa Pérez"], "title": "MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments", "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.RO"], "comment": "This paper has been accepted for publication at IEEE International Conference on Communications (ICC) 2026", "summary": "Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.", "AI": {"tldr": "介绍了一种基于Vision Transformer（ViT）的两阶段框架MapViT，用于在动态环境中实时预测无线信号质量地图。", "motivation": "机器人需要准确理解周围环境和无线电波的质量以实现无缝导航和操作。然而，在高度动态变化的环境中实现这一点是一个挑战。因此，开发一个能够高效且可靠地进行此类预测的系统是必要的。", "method": "提出了MapViT框架，该框架受预训练和微调范式启发，用于在两阶段中分别预测环境变化及预期无线信号质量，并评估了多个代表性的机器学习模型。", "result": "实验结果表明提出的双阶段管道能够实现实时预测。基于ViT的实现方法在准确性和计算效率之间达到了良好的平衡，使其成为移动机器人等能源和资源受限平台的一个有前途的选择。", "conclusion": "这项工作为下一代数字孪生生态系统奠定了基础，并为未来的6G系统中的多模态智能驱动的新类机器学习基础模型铺平了道路。"}}
{"id": "2601.15575", "pdf": "https://arxiv.org/pdf/2601.15575", "abs": "https://arxiv.org/abs/2601.15575", "authors": ["Jason Kim", "Maria Teleki", "James Caverlee"], "title": "PromptHelper: A Prompt Recommender System for Encouraging Creativity in AI Chatbot Interactions", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": null, "summary": "Prompting is central to interaction with AI systems, yet many users struggle to explore alternative directions, articulate creative intent, or understand how variations in prompts shape model outputs. We introduce prompt recommender systems (PRS) as an interaction approach that supports exploration, suggesting contextually relevant follow-up prompts. We present PromptHelper, a PRS prototype integrated into an AI chatbot that surfaces semantically diverse prompt suggestions while users work on real writing tasks. We evaluate PromptHelper in a 2x2 fully within-subjects study (N=32) across creative and academic writing tasks. Results show that PromptHelper significantly increases users' perceived exploration and expressiveness without increasing cognitive workload. Qualitative findings illustrate how prompt recommendations help users branch into new directions, overcome uncertainty about what to ask next, and better articulate their intent. We discuss implications for designing AI interfaces that scaffold exploratory interaction while preserving user agency, and release open-source resources to support research on prompt recommendation.", "AI": {"tldr": "本文介绍了PromptHelper，一个嵌入AI聊天机器人的提示推荐系统原型，旨在通过提供语义多样化的提示建议来鼓励用户在实际写作任务中的探索和创造力。", "motivation": "许多用户在与AI系统的交互中难以探索不同的方向、表达创造性的意图或理解提示变化如何影响模型输出，因此提出了一种支持探索的提示推荐系统方法。", "method": "作者设计并评估了一个名为PromptHelper的原型，该原型嵌入到一个AI聊天机器人中，并通过2x2完全组内研究（N=32）在创意写作和学术写作任务上进行了测试。", "result": "结果表明，PromptHelper显著增加了用户的探索感和表达能力，且没有增加认知工作量。定性发现显示提示推荐帮助用户探索新方向、克服下一步该问什么的不确定性，并更好地表达意图。", "conclusion": "研究讨论了设计支持探索交互的同时保持用户自主性的AI界面的设计意义，并发布了开源资源以支持关于提示推荐的研究。"}}
{"id": "2601.15572", "pdf": "https://arxiv.org/pdf/2601.15572", "abs": "https://arxiv.org/abs/2601.15572", "authors": ["Jieyun Bai", "Yitong Tang", "Zihao Zhou", "Mahdi Islam", "Musarrat Tabassum", "Enrique Almar-Munoz", "Hongyu Liu", "Hui Meng", "Nianjiang Lv", "Bo Deng", "Yu Chen", "Zilun Peng", "Yusong Xiao", "Li Xiao", "Nam-Khanh Tran", "Dac-Phu Phan-Le", "Hai-Dang Nguyen", "Xiao Liu", "Jiale Hu", "Mingxu Huang", "Jitao Liang", "Chaolu Feng", "Xuezhi Zhang", "Lyuyang Tong", "Bo Du", "et al. (14 additional authors not shown)"], "title": "FUGC: Benchmarking Semi-Supervised Learning Methods for Cervical Segmentation", "categories": ["eess.IV", "cs.CE", "cs.CV"], "comment": null, "summary": "Accurate segmentation of cervical structures in transvaginal ultrasound (TVS) is critical for assessing the risk of spontaneous preterm birth (PTB), yet the scarcity of labeled data limits the performance of supervised learning approaches. This paper introduces the Fetal Ultrasound Grand Challenge (FUGC), the first benchmark for semi-supervised learning in cervical segmentation, hosted at ISBI 2025. FUGC provides a dataset of 890 TVS images, including 500 training images, 90 validation images, and 300 test images. Methods were evaluated using the Dice Similarity Coefficient (DSC), Hausdorff Distance (HD), and runtime (RT), with a weighted combination of 0.4/0.4/0.2. The challenge attracted 10 teams with 82 participants submitting innovative solutions. The best-performing methods for each individual metric achieved 90.26\\% mDSC, 38.88 mHD, and 32.85 ms RT, respectively. FUGC establishes a standardized benchmark for cervical segmentation, demonstrates the efficacy of semi-supervised methods with limited labeled data, and provides a foundation for AI-assisted clinical PTB risk assessment.", "AI": {"tldr": "介绍FUGC，用于评估半监督学习方法在宫颈结构分割中的表现。", "motivation": "鉴于标记数据稀缺限制了传统监督学习方法的性能，提出一个基准以促进半监督学习方法的发展。", "method": "通过DSC、HD和RT三个指标对参与竞赛的方法进行评价，并采用加权组合0.4/0.4/0.2来综合评估。", "result": "最佳表现的方法分别在mDSC、mHD和RT上达到90.26%、38.88和32.85毫秒。", "conclusion": "FUGC为宫颈分割建立了标准化基准，展示了半监督方法的有效性，并为AI辅助临床早产风险评估奠定了基础。"}}
{"id": "2601.15561", "pdf": "https://arxiv.org/pdf/2601.15561", "abs": "https://arxiv.org/abs/2601.15561", "authors": ["Naoya Onizawa", "Takahiro Hanyu"], "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems", "categories": ["cs.ET", "cs.LG"], "comment": "17 pages", "summary": "This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.", "AI": {"tldr": "本文探讨了使用概率位（p-bit）的模拟退火算法在解决大规模组合优化问题时遇到的问题，并提出了两种新算法来提高其性能。", "motivation": "研究动机在于识别并解决基于p-bit的模拟退火算法中由于反馈机制导致的不可预测震荡，这些震荡阻碍了Ising模型的能量减少和算法的有效执行。", "method": "本文提出两个新的算法——时间平均pSA（TApSA）和停滞pSA（SpSA），它们基于部分停用p-bits的设计，并在Python模拟中使用最大切割基准进行了测试。", "result": "实验结果显示，对于16个从800到5,000节点的基准问题，与传统pSA相比，新算法提高了平均归一化割值，提升了0.8%至98.4%。", "conclusion": "研究结论指出，提出的TApSA和SpSA算法有效解决了基于p-bit模拟退火在处理复杂组合优化任务中的能量滞留问题，并显著提升了性能。"}}
{"id": "2601.15560", "pdf": "https://arxiv.org/pdf/2601.15560", "abs": "https://arxiv.org/abs/2601.15560", "authors": ["Sylvey Lin", "Eranki Vasistha"], "title": "Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.", "AI": {"tldr": "本文提出了相对分类准确率(RCA)作为衡量细粒度K-pop人脸生成中的身份一致性的校准指标。", "motivation": "传统的评估指标如FID和IS在检测特定任务中身份错位方面存在局限性，因此需要新的方法来评价高保真图像生成模型的语义控制能力。", "method": "本文使用了条件DDPMs进行K-pop偶像人脸生成，并提出了一种新的校准度量RCA，该指标通过一个基准分类器对生成性能进行了标准化评估。", "result": "实验结果表明，尽管模型在视觉质量上表现良好（FID为8.93），但在语义模式崩溃方面存在问题（RCA仅为0.27），特别是在处理视觉模糊的身份时。", "conclusion": "本文通过混淆矩阵分析了这些失败的原因，并将这些问题归因于分辨率限制和性别内同质性。该框架提供了一个严格的基准来验证条件生成模型中的身份一致性。"}}
{"id": "2601.15552", "pdf": "https://arxiv.org/pdf/2601.15552", "abs": "https://arxiv.org/abs/2601.15552", "authors": ["Phuc Nguyen", "Benjamin Zelditch", "Joyce Chen", "Rohit Patra", "Changshuai Wei"], "title": "BanditLP: Large-Scale Stochastic Optimization for Personalized Recommendations", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "We present BanditLP, a scalable multi-stakeholder contextual bandit framework that unifies neural Thompson Sampling for learning objective-specific outcomes with a large-scale linear program for constrained action selection at serving time. The methodology is application-agnostic, compatible with arbitrary neural architectures, and deployable at web scale, with an LP solver capable of handling billions of variables. Experiments on public benchmarks and synthetic data show consistent gains over strong baselines. We apply this approach in LinkedIn's email marketing system and demonstrate business win, illustrating the value of integrated exploration and constrained optimization in production.", "AI": {"tldr": "本文介绍了BanditLP，一个大规模的多利益相关者上下文强盗框架，它统一了用于学习目标特定结果的神经汤普森采样与在服务时间进行约束行动选择的大规模线性程序。", "motivation": "动机在于开发一种可扩展的方法来处理个性化推荐中的大规模优化问题，并且能够在生产环境中集成探索和约束优化以实现商业价值。", "method": "BanditLP框架结合了神经汤普森采样用于学习目标特定的结果，以及一个能够处理数十亿变量的大规模线性程序来进行服务时间的约束行动选择。", "result": "实验显示，在公共基准数据集和合成数据上，该方法比强基线取得了持续的进步，并在LinkedIn的电子邮件营销系统中展示了商业收益。", "conclusion": "研究结论表明，BanditLP框架能够在大规模场景下有效实现个性化推荐并带来实际业务价值。"}}
{"id": "2601.15551", "pdf": "https://arxiv.org/pdf/2601.15551", "abs": "https://arxiv.org/abs/2601.15551", "authors": ["Bismack Tokoli", "Luis Jaimes", "Ayesha S. Dina"], "title": "ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and Next-step guidance", "categories": ["cs.AI", "cs.MA"], "comment": "35 pages", "summary": "Personalized learning systems have emerged as a promising approach to enhance student outcomes by tailoring educational content, pacing, and feedback to individual needs. However, most existing systems remain fragmented, specializing in either knowledge tracing, diagnostic modeling, or resource recommendation, but rarely integrating these components into a cohesive adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner Intelligence for Gap Identification and Next-step guidance), a multi-agent educational framework designed to deliver personalized learning through integrated knowledge estimation, skill-gap identification, and targeted resource recommendation.ALIGNAgent begins by processing student quiz performance, gradebook data, and learner preferences to generate topic-level proficiency estimates using a Skill Gap Agent that employs concept-level diagnostic reasoning to identify specific misconceptions and knowledge deficiencies. After identifying skill gaps, the Recommender Agent retrieves preference-aware learning materials aligned with diagnosed deficiencies, implementing a continuous feedback loop where interventions occur before advancing to subsequent topics. Extensive empirical evaluation on authentic datasets from two undergraduate computer science courses demonstrates ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of 0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation validated against actual exam performance.", "AI": {"tldr": "本文提出了ALIGNAgent，一个多代理教育框架，旨在通过整合知识评估、技能缺口识别和有针对性的资源推荐来提供个性化学习。", "motivation": "现有个性化学习系统在知识追踪、诊断建模或资源推荐方面存在专一化但不集成的问题，因此缺乏一个统一的自适应学习循环。本文旨在填补这一空白。", "method": "ALIGNAgent通过处理学生测验表现、成绩记录和学习偏好来生成主题层面的专业技能评估，并使用概念级别诊断推理识别具体误解和知识缺陷。接着，推荐代理检索与诊断出的缺陷相匹配的学习材料，实施连续反馈循环，在前进到后续主题之前进行干预。", "result": "在两个计算机科学课程的真实数据集上的广泛实证评价显示了ALIGNAgent的有效性，基于GPT-4o的代理实现了0.87至0.90的精度和0.84至0.87的F1分数，在知识熟练度估计方面通过与实际考试表现对比得到了验证。", "conclusion": "ALIGNAgent展示了在个性化学习中的有效性，并且能够有效地识别学生的技能缺口并提供针对性的学习资源推荐。"}}
{"id": "2601.15549", "pdf": "https://arxiv.org/pdf/2601.15549", "abs": "https://arxiv.org/abs/2601.15549", "authors": ["Ryo Fujii", "Hideo Saito", "Ryo Hachiuma"], "title": "VIOLA: Towards Video In-Context Learning with Minimal Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.", "AI": {"tldr": "本文提出VIOLA框架，旨在利用少量标注数据实现视频领域的In-Context Learning。", "motivation": "在真实场景中，如工业或外科环境，标注数据稀缺且专家的注释成本高，因此需要一种能够高效使用少量标注信息的方法来适应新的视频领域。", "method": "VIOLA框架包括密度不确定加权采样、混合池构建以及基于信心意识检索和提示的方法，以提高标记效率并利用大量未标记的数据，同时避免噪声传播。", "result": "实验表明，在低资源设置下，该框架相比各种基线方法显著提高了性能，并且能够实现鲁棒的适应性。", "conclusion": "VIOLA通过最小化标注需求和最大化未标注数据的价值，在视频领域的In-Context Learning中取得了优越的效果。"}}
{"id": "2601.15547", "pdf": "https://arxiv.org/pdf/2601.15547", "abs": "https://arxiv.org/abs/2601.15547", "authors": ["Jingren Hou", "Hong Wang", "Pengyu Xu", "Chang Gao", "Huafeng Liu", "Liping Jing"], "title": "Learning Neural Operators from Partial Observations via Latent Autoregressive Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Real-world scientific applications frequently encounter incomplete observational data due to sensor limitations, geographic constraints, or measurement costs. Although neural operators significantly advanced PDE solving in terms of computational efficiency and accuracy, their underlying assumption of fully-observed spatial inputs severely restricts applicability in real-world applications. We introduce the first systematic framework for learning neural operators from partial observation. We identify and formalize two fundamental obstacles: (i) the supervision gap in unobserved regions that prevents effective learning of physical correlations, and (ii) the dynamic spatial mismatch between incomplete inputs and complete solution fields. Specifically, our proposed Latent Autoregressive Neural Operator~(\\ours) introduces two novel components designed explicitly to address the core difficulties of partial observations: (i) a mask-to-predict training strategy that creates artificial supervision by strategically masking observed regions, and (ii) a Physics-Aware Latent Propagator that reconstructs solutions through boundary-first autoregressive generation in latent space. Additionally, we develop POBench-PDE, a dedicated and comprehensive benchmark designed specifically for evaluating neural operators under partial observation conditions across three PDE-governed tasks. \\ours achieves state-of-the-art performance with 18--69$\\%$ relative L2 error reduction across all benchmarks under patch-wise missingness with less than 50$\\%$ missing rate, including real-world climate prediction. Our approach effectively addresses practical scenarios involving up to 75$\\%$ missing rate, to some extent bridging the existing gap between idealized research settings and the complexities of real-world scientific computing.", "AI": {"tldr": "本文提出了一种从部分观测数据中学习神经算子的系统框架。", "motivation": "现有神经算子在处理现实世界不完全观测数据时受限，因为它们假设了空间输入是完整观测的。本研究旨在解决这一问题，使神经算子能够应用于更广泛的科学计算场景。", "method": "提出了一种潜变量自回归神经算子框架，包括一种通过屏蔽策略产生人工监督的方法和一个物理感知的潜变量传播器以重建解决方案。", "result": "在新开发的部分观测基准测试POBench-PDE中，该方法实现了相对于现有技术18-69%的L2误差减少，适用于不同缺失率下的PDE问题，包括实际气候预测。", "conclusion": "所提出的方法有效解决了部分观测条件下学习神经算子的问题，减少了理想研究设定和现实世界科学计算复杂性之间的差距。"}}
{"id": "2601.15545", "pdf": "https://arxiv.org/pdf/2601.15545", "abs": "https://arxiv.org/abs/2601.15545", "authors": ["Zhifan Yan", "Chang Liu", "Yiyang Jiang", "Wenxuan Zheng", "Xinhao Chen", "Axel Krieger"], "title": "A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control", "categories": ["cs.RO"], "comment": null, "summary": "Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a \"model-calibration bottleneck\", requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.", "AI": {"tldr": "本文提出了一种基于深度强化学习控制的便携式移动磁操纵平台，用于胃肠道导航。", "motivation": "传统的固定磁场系统和需要复杂物理模型校准的移动系统存在局限性，本文旨在开发一个能够克服这些限制、实现精准磁控操作的低耗时低成本解决方案。", "method": "使用Soft Actor-Critic (SAC)算法通过模拟到现实的训练管道对四电磁铁阵列进行控制，该阵列安装在UR5协作机器人上，实现了模型无关且快速部署的操作策略。", "result": "实验验证表明，基于DRL的控制器能够实现7毫米磁胶囊沿二维路径的有效导航，对于方形和圆形轨迹分别达到1.18 mm 和1.50 mm 的均方根误差，并在30厘米 * 20厘米的工作区域内成功追踪。", "conclusion": "本文展示了一种快速部署、模型无关且适用于大工作空间的磁控操作框架，为精确的胃肠道导航提供了解决方案。"}}
{"id": "2601.15544", "pdf": "https://arxiv.org/pdf/2601.15544", "abs": "https://arxiv.org/abs/2601.15544", "authors": ["Himanshu Mishra"], "title": "RDumb++: Drift-Aware Continual Test-Time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model during deployment using only the incoming, unlabeled data stream. Although prior approaches such as Tent, EATA etc. provide meaningful improvements under short evolving shifts, they struggle when the test distribution changes rapidly or over extremely long horizons. This challenge is exemplified by the CCC benchmark, where models operate over streams of 7.5M samples with continually changing corruption types and severities. We propose RDumb++, a principled extension of RDumb that introduces two drift-detection mechanisms i.e entropy-based drift scoring and KL-divergence drift scoring, together with adaptive reset strategies. These mechanisms allow the model to detect when accumulated adaptation becomes harmful and to recover before prediction collapse occurs. Across CCC-medium with three speeds and three seeds (nine runs, each containing one million samples), RDumb++ consistently surpasses RDumb, yielding approx 3% absolute accuracy gains while maintaining stable adaptation throughout the entire stream. Ablation experiments on drift thresholds and reset strengths further show that drift-aware resetting is essential for preventing collapse and achieving reliable long-horizon CTTA.", "AI": {"tldr": "本文提出了RDumb++，一种针对测试时间持续适应的模型，通过引入两种漂移检测机制和自适应重置策略来提高模型在长时间变化环境下的性能。", "motivation": "现有方法如Tent、EATA等在短期演化转变中提供有意义的改进，但在快速或极端长时段内测试分布发生变化时表现不佳。特别地，在CCC基准测试中，数据流包含750万样本且不断改变腐蚀类型和严重程度。", "method": "RDumb++扩展了RDumb模型，通过引入基于熵的漂移评分和KL散度漂移评分两种机制，并结合自适应重置策略来检测累积适应何时变得有害并防止预测崩溃。", "result": "在CCC-medium基准测试下，与RDumb相比，RDumb++获得了约3%绝对准确率提高，并在整个数据流中保持了稳定的适应性。通过消融实验表明漂移感知重置对于防止模型崩溃和实现可靠的长期CTTA是必不可少的。", "conclusion": "研究结果表明RDumb++在长时间变化环境下的测试时间持续适应任务上表现更优，证明了其提出的漂移检测机制和自适应重置策略的有效性。"}}
{"id": "2601.15541", "pdf": "https://arxiv.org/pdf/2601.15541", "abs": "https://arxiv.org/abs/2601.15541", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Qiyi Tong", "Gokhan Solak", "Puze Liu", "Sheng Liu", "Jan Peters", "Arash Ajoudani"], "title": "CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation", "categories": ["cs.RO"], "comment": "under review", "summary": "We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\\% to 17.29\\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.", "AI": {"tldr": "提出CompliantVLA-adaptor，通过将视觉语言模型指导的可变阻抗控制引入到现有VLA模型中，提高涉及接触、顺应性和不确定性的机器人操作任务的安全性与有效性。", "motivation": "现有的VLA系统在处理物理任务时缺乏对力的感知适应，导致交互过程中的安全问题或失败。该研究旨在通过增加对力和阻抗控制的支持来改进这些系统的性能。", "method": "使用视觉语言模型（VLM）从图像和自然语言中解读任务上下文，调整可变阻抗控制器的刚度和阻尼参数，并利用实时力/扭矩反馈确保交互过程中的力保持在安全范围内。", "result": "该方法在一系列复杂的接触密集型任务上优于传统的VLA基准模型，在模拟环境及实际硬件上的成功率从9.86%提升到了17.29%，减少了力的违规情况。", "conclusion": "CompliantVLA-adaptor为通过视觉语言动作系统实现安全的接触丰富操作提供了一条有希望的道路，展示了其在提高任务成功率和安全性方面的潜力。"}}
{"id": "2601.15540", "pdf": "https://arxiv.org/pdf/2601.15540", "abs": "https://arxiv.org/abs/2601.15540", "authors": ["Dongchen Huang"], "title": "PRISM: Deriving the Transformer as a Signal-Denoising Operator via Maximum Coding Rate Reduction", "categories": ["cs.LG", "cs.AI", "cs.CL", "physics.data-an"], "comment": null, "summary": "Deep learning models, particularly Transformers, are often criticized as \"black boxes\" and lack interpretability. We propose Prism, a white-box attention-based architecture derived from the principles of Maximizing Coding Rate Reduction ($\\text{MCR}^2$). By modeling the attention mechanism as a gradient ascent process on a distinct signal-noise manifold, we introduce two physical constraints: an overcomplete dictionary to expand the representational phase space, and an irrational frequency separation ($π$-RoPE) to enforce incoherence between signal and noise subspaces. We demonstrate that these geometric inductive biases can be viewed as a physical constraint and they are sufficient to induce unsupervised functional disentanglement alone. Using TinyStories as a controlled testbed for verifying spectral dynamics, we observe that Prism spontaneously specializes its attention heads into spectrally distinct regimes: low-frequency heads capturing long-range causal dependencies (signal) and high-frequency heads handling local syntactic constraints (noise). Our results suggest that interpretability and performance are not a trade-off, but can be unified through principled geometric construction.", "AI": {"tldr": "本文提出了PRISM，一种基于注意力机制的白盒架构，通过最大化编码率减少（MCR^2）的原则推导出Transformer作为信号去噪操作符。", "motivation": "深度学习模型，特别是变压器，常常被批评为“黑箱”且缺乏可解释性。为了提高其可解释性和性能，本文提出了一种新方法。", "method": "通过将注意力机制建模为在不同的信噪流形上的梯度上升过程，并引入两个物理约束条件：过度完整的字典来扩展表示相空间和无理频率分离(π-RoPE)以确保信号与噪声子空间之间的不相干性，本文提出PRISM。", "result": "实验表明，这些几何归纳偏置可以被视为一个物理约束，并且足以单独诱导无监督的功能解纠缠。使用TinyStories作为受控测试平台验证频谱动力学时，观察到PRISM的注意力头部自发地专门化为光谱不同的区域：低频头部捕捉长程因果依赖性（信号），高频头部处理局部句法约束（噪声）。", "conclusion": "研究结果表明可解释性和性能不是权衡关系，而是可以通过原则性的几何构造统一起来。"}}
{"id": "2601.15539", "pdf": "https://arxiv.org/pdf/2601.15539", "abs": "https://arxiv.org/abs/2601.15539", "authors": ["Ali Khreis", "Ro'Yah Radaideh", "Quinn McGill"], "title": "A Machine Vision Approach to Preliminary Skin Lesion Assessments", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, 2 figures, 2 tables", "summary": "Early detection of malignant skin lesions is critical for improving patient outcomes in aggressive, metastatic skin cancers. This study evaluates a comprehensive system for preliminary skin lesion assessment that combines the clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders, Color, and Dermoscopic Structures) with machine learning classification. Using a 1,000-image subset of the HAM10000 dataset, the system implements an automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for each lesion. This handcrafted approach is compared against various machine learning solutions, including traditional classifiers (Logistic Regression, Random Forest, and SVM) and deep learning models. While the rule-based system provides high clinical interpretability, results indicate a performance bottleneck when reducing complex morphology to five numerical features. Experimental findings show that transfer learning with EfficientNet-B0 failed significantly due to domain shift between natural and medical images. In contrast, a custom three-layer Convolutional Neural Network (CNN) trained from scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images, representing a 19-point accuracy improvement over traditional methods. The results demonstrate that direct pixel-level learning captures diagnostic patterns beyond handcrafted features and that purpose-built lightweight architectures can outperform large pretrained models for small, domain-specific medical datasets.", "AI": {"tldr": "本文介绍了一种结合ABCD规则和机器学习分类的综合系统，用于初步皮肤病变评估。", "motivation": "早期发现恶性皮肤病变对于改善患者预后至关重要。研究旨在通过结合临床建立的ABC D规则与机器学习来提高诊断准确性。", "method": "使用了HAM10000数据集中的1,000张图片，采用自动化、基于规则的流水线计算每个病灶的总皮肤镜评分(TDS)，并与传统分类器（逻辑回归、随机森林和支持向量机）和深度学习模型进行了比较。还采用了EfficientNet-B0进行迁移学习，以及从头开始训练的三层卷积神经网络(CNN)。", "result": "实验结果表明，基于规则的方法在解释性方面表现良好，但在将复杂形态学特征减少到五个数值特征时存在性能瓶颈。EfficientNet-B0模型因自然图像与医疗图像之间的领域偏移而失败。一个定制的三层层卷积神经网络（CNN）从零开始训练，在中值滤波后的图像上达到了78.5%的准确率和86.5%的召回率，比传统方法提高了19个百分点。", "conclusion": "直接在像素级别学习能够捕捉到超出手工特征的诊断模式，并且为小规模、特定领域医疗数据集设计的轻量级架构可以超越大型预训练模型。"}}
{"id": "2601.15538", "pdf": "https://arxiv.org/pdf/2601.15538", "abs": "https://arxiv.org/abs/2601.15538", "authors": ["Himanshu Mishra", "Kanwal Mehreen"], "title": "QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Machine unlearning aims to remove specific knowledge (e.g., copyrighted or private data) from a trained model without full retraining. In practice, models are often quantized (e.g., 4-bit) for deployment, but we find that quantization can catastrophically restore forgotten information [1]. In this paper, we (1) analyze why low-bit quantization undermines unlearning, and (2) propose a quantization-aware unlearning method to mitigate this. We first compute weight-change statistics and bucket overlaps in quantization to show that typical unlearning updates are too small to cross quantization thresholds. Building on this insight, we introduce a logits space hinge loss: for each forget example, we force the output logits of the unlearned model to differ from the original model by at least a margin (half the quantization step). This ensures forgotten examples remain distinguishable even after quantization. We evaluate on language and classification tasks (including a Twitter misinformation dataset) and show our method preserves forgetting under 4-bit quantization, whereas existing methods almost entirely recover the forgotten knowledge.", "AI": {"tldr": "该论文提出了一种量化感知的遗忘方法QUAIL，以防止低比特量化在机器学习模型中恢复被遗忘的信息。", "motivation": "旨在解决现有模型在量化部署后容易恢复已被遗忘的数据的问题，如版权或私有数据等敏感信息。", "method": "通过分析权重变化和量化桶重叠，提出了一种基于logits空间铰链损失的方法，强制遗忘示例与原始模型的输出差异至少达到量化步长的一半。", "result": "在语言任务和分类任务上进行了评估，结果表明该方法可以在4位量化下保持遗忘效果，而现有方法几乎完全恢复了被遗忘的知识。", "conclusion": "QUAIL方法有效地解决了低比特量化导致的模型恢复问题，保证了敏感数据的有效遗忘。"}}
{"id": "2601.15533", "pdf": "https://arxiv.org/pdf/2601.15533", "abs": "https://arxiv.org/abs/2601.15533", "authors": ["Zhikang Chen", "Tingting Zhu"], "title": "From Generative Engines to Actionable Simulators: The Imperative of Physical Grounding in World Models", "categories": ["cs.AI"], "comment": null, "summary": "A world model is an AI system that simulates how an environment evolves under actions, enabling planning through imagined futures rather than reactive perception. Current world models, however, suffer from visual conflation: the mistaken assumption that high-fidelity video generation implies an understanding of physical and causal dynamics. We show that while modern models excel at predicting pixels, they frequently violate invariant constraints, fail under intervention, and break down in safety-critical decision-making. This survey argues that visual realism is an unreliable proxy for world understanding. Instead, effective world models must encode causal structure, respect domain-specific constraints, and remain stable over long horizons. We propose a reframing of world models as actionable simulators rather than visual engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and closed-loop evaluation. Using medical decision-making as an epistemic stress test, where trial-and-error is impossible and errors are irreversible, we demonstrate that a world model's value is determined not by how realistic its rollouts appear, but by its ability to support counterfactual reasoning, intervention planning, and robust long-horizon foresight.", "AI": {"tldr": "本文讨论了将世界模型从视觉生成引擎转变为可操作模拟器的必要性，强调物理基础在世界模型中的重要性。", "motivation": "当前的世界模型虽然能生成高保真的视频图像，但不能理解物理和因果动态。论文旨在探讨如何使世界模型不仅仅依赖于视觉真实感，而是更注重对因果结构的理解、遵守领域特定的约束条件，并保持长期稳定。", "method": "提出将世界模型重新定义为可操作模拟器，强调结构化的4D接口、考虑约束的动力学以及闭环评估。通过医疗决策作为认知压力测试来说明这一转变的重要性。", "result": "展示了世界模型的价值不应只看其回放的视觉逼真度，而应更重视其支持反事实推理、干预规划和稳健长时段预测的能力。", "conclusion": "强调了物理基础在改进世界模型中的关键作用，并提出将世界模型作为可操作模拟器而不是单纯图像生成器的重要性。"}}
{"id": "2601.15519", "pdf": "https://arxiv.org/pdf/2601.15519", "abs": "https://arxiv.org/abs/2601.15519", "authors": ["Zhichao Yang", "Jiashu He", "Jinxuan Fan", "Cirillo Cinzia"], "title": "TransportAgents: a multi-agents LLM framework for traffic accident severity prediction", "categories": ["cs.AI"], "comment": null, "summary": "Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.", "AI": {"tldr": "本文提出了一种名为TransportAgents的多智能体框架，用于提高交通事故严重程度预测的准确性。", "motivation": "准确预测交通事故的严重性对于改进应急响应和公共安全规划至关重要。然而，现有的大语言模型在处理异质且领域特定的数据时存在局限性，并可能产生偏向或不稳定的预测结果。本文旨在解决这些问题。", "method": "TransportAgents是一个混合多智能体框架，它结合了类别特异性大型语言模型的推理能力和一个多层感知器（MLP）融合模块。每个专门的代理关注交通信息的一个特定子集，如人口统计、环境背景或事件细节，并生成中间严重性评估结果，这些评估结果随后被整合为统一预测。", "result": "在两个互补的数据集中进行的广泛实验表明，TransportAgents框架始终优于传统的机器学习方法和先进的大型语言模型。该框架表现出了强大的鲁棒性、可扩展性和跨数据集泛化能力，并且产生了更平衡、校准良好的严重性预测结果。", "conclusion": "TransportAgents展示了其在交通事故严重程度预测中的优越性能，特别是通过多智能体架构改进了大语言模型的局限性，提供了更加可靠和解释性强的安全关键决策支持应用。"}}
{"id": "2601.15516", "pdf": "https://arxiv.org/pdf/2601.15516", "abs": "https://arxiv.org/abs/2601.15516", "authors": ["William Huang", "Siyou Pei", "Leyi Zou", "Eric J. Gonzalez", "Ishan Chatterjee", "Yang Zhang"], "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal", "summary": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.", "AI": {"tldr": "本文提出了一种利用手背特征来改善第一人称视角下手部姿态估计的方法。", "motivation": "随着XR设备的普及，需要解决在第一人称视角下手指频繁被遮挡的问题。传统的手部姿态估计算法在这种情况下表现不佳，因此需要一种新的方法来提高其可靠性和精度。", "method": "本文提出了一种双流Delta编码器，通过对比动态手势和放松状态的手背特征来进行姿态估计。仅使用裁剪后的手背图像数据进行训练和测试。", "result": "实验结果表明，在手指遮挡超过50%的情况下，与依赖整个手部几何结构和大模型的先进算法相比，本文的方法将平均每关节角度误差降低了18%。", "conclusion": "该方法不仅提高了在遮挡场景中的下游任务如食指捏合和点击估计的可靠性，还解锁了新的交互范式，例如检测等长力表面“点击”时无需可见的动作，并且缩小了模型尺寸。"}}
{"id": "2601.15509", "pdf": "https://arxiv.org/pdf/2601.15509", "abs": "https://arxiv.org/abs/2601.15509", "authors": ["Prasanna Kumar"], "title": "The Dark Side of AI Transformers: Sentiment Polarization & the Loss of Business Neutrality by NLP Transformers", "categories": ["cs.AI", "cs.CL"], "comment": ":I.2.7", "summary": "The use of Transfer Learning & Transformers has steadily improved accuracy and has significantly contributed in solving complex computation problems. However, this transformer led accuracy improvement in Applied AI Analytics specifically in sentiment analytics comes with the dark side. It is observed during experiments that a lot of these improvements in transformer led accuracy of one class of sentiment has been at the cost of polarization of another class of sentiment and the failing of neutrality. This lack of neutrality poses an acute problem in the Applied NLP space, which relies heavily on the computational outputs of sentiment analytics for reliable industry ready tasks.", "AI": {"tldr": "研究探讨了AI Transformer在情感分析中的负面影响，发现其提高了某一类情感的准确性但导致另一类情感极化和中立性的丧失。", "motivation": "动机在于揭示Transformer技术应用于情感分析时所带来的问题，特别是它如何影响业务决策所需的准确性和中立性。", "method": "通过实验观察了Transformer在提升一类情感准确性的同时对其他类别情感的负面影响及其中立性的问题。", "result": "实验结果显示，虽然Transformers提高了某些类别的准确性，但导致了另一类情感极化和失去中立性，这对需要可靠计算输出的应用NLP领域构成了重大问题。", "conclusion": "研究结论指出，尽管Transformer技术在提升准确率方面有显著效果，但也存在使情感分析结果产生偏见的风险，这对于依赖于高可靠性情感分析的行业应用是不利的。"}}
{"id": "2601.15507", "pdf": "https://arxiv.org/pdf/2601.15507", "abs": "https://arxiv.org/abs/2601.15507", "authors": ["Jinrui Yang", "Qing Liu", "Yijun Li", "Mengwei Ren", "Letian Zhang", "Zhe Lin", "Cihang Xie", "Yuyin Zhou"], "title": "Controllable Layered Image Generation for Real-World Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.", "AI": {"tldr": "本文提出了LASAGNA，一种新的统一框架，用于同时生成图像及其组成层，并引入了包含清洁背景和带有物理基础视觉效果的RGBA前景的新数据集LASAGNA-48K。", "motivation": "现有的图象生成模型在控制特定元素编辑时结果不够一致且可控性差，因此本文旨在提出一种新的方法来改进这一状况。", "method": "通过LASAGNA框架和新数据集LASAGNA-48K，该方法可以从多种条件输入中高效地学习正确的图像合成，并提供更高的可控性以满足现实应用的需求。", "result": "实验显示，LASAGNA在同时生成多个图层时能够产生高度一致且连贯的结果，支持保留身份和视觉效果的多样化后期编辑应用。", "conclusion": "本文提出的框架和数据集可以促进社区内开放研究，并将公开发布以供进一步使用。"}}
{"id": "2601.15500", "pdf": "https://arxiv.org/pdf/2601.15500", "abs": "https://arxiv.org/abs/2601.15500", "authors": ["Saptarshi Roy", "Alessandro Rinaldo", "Purnamrita Sarkar"], "title": "Low-Dimensional Adaptation of Rectified Flow: A New Perspective through the Lens of Diffusion and Stochastic Localization", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "comment": "32 pages, 7 figures", "summary": "In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\\varepsilon)$ (up to log factors), where $\\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.", "AI": {"tldr": "本文研究了Rectified Flow (RF)如何自适应地加速采样过程，并提出了一种基于扩散和随机定位的新视角下的低维自适应RF方法。", "motivation": "鉴于Rectified Flow在生成效率和性能方面的优势，该研究旨在探索其自动适应目标分布内在低维度以加速采样的程度。", "method": "通过精心设计的时间离散化方案和精确的漂移估计，提出了新的时间离散化计划，并建立了DDPM与随机RF之间的新联系来设计一种更宽松要求下的随机RF抽样器。", "result": "展示了所提出的抽样器具有O(k/ε)迭代复杂度，并通过合成数据和文本到图像的数据实验验证了其改进的性能。", "conclusion": "研究表明，精心选择的时间离散化方案和精确漂移估计能显著加速RF采样过程，而基于扩散与随机定位的新视角下设计的低维自适应RF方法进一步提高了抽样的效率和准确性。"}}
{"id": "2601.15495", "pdf": "https://arxiv.org/pdf/2601.15495", "abs": "https://arxiv.org/abs/2601.15495", "authors": ["Yiyang Feng", "Zeming Chen", "Haotian Wu", "Jiawei Zhou", "Antoine Bosselut"], "title": "Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to EACL 2026 (Main)", "summary": "A common solution for mitigating outdated or incorrect information in Large Language Models (LLMs) is to provide updated facts in-context or through knowledge editing. However, these methods introduce knowledge conflicts when the knowledge update fails to overwrite the model's parametric knowledge, which propagate to faulty reasoning. Current benchmarks for this problem, however, largely focus only on single knowledge updates and fact recall without evaluating how these updates affect downstream reasoning. In this work, we introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark for studying how LLMs propagate new knowledge through multi-step reasoning when it conflicts with the model's initial parametric knowledge. Spanning three reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces multiple, realistic conflicts to mirror real-world complexity. Our results on TRACK reveal that providing updated facts to models for reasoning can worsen performance compared to providing no updated facts to a model, and that this performance degradation exacerbates as more updated facts are provided. We show this failure stems from both inability to faithfully integrate updated facts, but also flawed reasoning even when knowledge is integrated. TRACK provides a rigorous new benchmark to measure and guide future progress on propagating conflicting knowledge in multi-step reasoning.", "AI": {"tldr": "介绍了一种新的基准测试TRACK，用于研究LLMs在多步推理中如何处理与初始参数知识冲突的新知识。", "motivation": "现有的解决方法无法充分评估新事实更新对下游推理的影响。当前的基准测试大多只关注单个事实的更新和回忆，并未考虑这些更新如何影响多步骤推理过程中的复杂情况。", "method": "提出了TRACK（Testing Reasoning Amid Conflicting Knowledge）这一新的基准，涵盖三个需要强烈推理能力的情景（WIKI、CODE和MATH），引入多个现实冲突来模拟实际复杂性。", "result": "研究发现，给模型提供更新的事实进行推理会比不提供更新事实时表现更差，并且随着提供的更新事实数量增加，这种性能下降的现象更加严重。这一失败源于无法忠实整合新事实以及即使知识被成功整合后仍存在的推理错误。", "conclusion": "TRACK作为一项新的严格基准测试，测量并指导未来在多步推理中传播冲突性知识的进展。"}}
{"id": "2601.15490", "pdf": "https://arxiv.org/pdf/2601.15490", "abs": "https://arxiv.org/abs/2601.15490", "authors": ["Jobeal Solomon", "Ali Mohammed Mansoor Alsahag", "Seyed Sahand Mohammadi Ziabari"], "title": "Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.", "AI": {"tldr": "本文研究了使用Vision Transformer替换U-Net编码器作为属性中和框架的一部分，以减少胸部X光图像中的性別和年龄相关偏差。", "motivation": "鉴于在胸部X光分类器中存在的性别和年龄相关的偏见问题，作者希望通过改进的属性中和技术来更有效地消除这些偏差。", "method": "本文使用数据高效的图像转换器小模型（DeiT-S）作为属性中和器，并通过11个编辑强度水平对生成的图像进行评估。评估包括独立的人工智能判断者用于属性泄漏的评估，以及卷积神经网络用于疾病预测。", "result": "研究结果表明，在适当的编辑强度下（alpha = 0.5），Vision Transformer（ViT）中和器能够将患者性别识别AUC降至约0.80，比原始框架减少了大约10个百分点，并且在减少训练轮次的同时保持了较高的疾病预测准确率。", "conclusion": "研究表明，基于全局自注意力的视觉模型可以进一步抑制属性泄漏而不牺牲临床实用性，为实现更公正的胸部X光人工智能提供了实际途径。"}}
{"id": "2601.15488", "pdf": "https://arxiv.org/pdf/2601.15488", "abs": "https://arxiv.org/abs/2601.15488", "authors": ["Yuxing Chen", "Guoqing Luo", "Zijun Wu", "Lili Mou"], "title": "Multi-Persona Thinking for Bias Mitigation in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 3 figures", "summary": "Large Language Models (LLMs) exhibit significant social biases that can perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose Multi-Persona Thinking (MPT), a novel inference-time framework that leverages dialectical reasoning from multiple perspectives to reduce bias. MPT guides models to adopt contrasting social identities (e.g., male and female) along with a neutral viewpoint, and then engages these personas iteratively to expose and correct biases. Through a dialectical reasoning process, the framework transforms the potential weakness of persona assignment into a strength for bias mitigation. We evaluate MPT on two widely used bias benchmarks across both open-source and closed-source models of varying scales. Our results demonstrate substantial improvements over existing prompting-based strategies: MPT achieves the lowest bias while maintaining core reasoning ability.", "AI": {"tldr": "本文提出了一种名为多角色思考（MPT）的框架，旨在减少大型语言模型中的社会偏见。", "motivation": "大型语言模型表现出显著的社会偏见，这可能导致有害的刻板印象和不公平的结果。为了减轻这种偏见，作者提出了这一方法。", "method": "MPT采用辩证推理的方法，从多个视角出发，指导模型采取对比的社会身份（如男性和女性）以及一个中立的观点，并通过这些角色的迭代互动来揭示和纠正偏差。", "result": "在两个广泛使用的偏见基准测试上，无论是开源还是闭源的不同规模的语言模型，MPT都显示出比现有的提示策略更大的改进，实现了最低的偏见水平同时保持核心推理能力。", "conclusion": "研究表明多角色思考框架（MPT）能够在减少大型语言模型社会偏见的同时不损害其主要推理能力。"}}
{"id": "2601.15487", "pdf": "https://arxiv.org/pdf/2601.15487", "abs": "https://arxiv.org/abs/2601.15487", "authors": ["Chandan Kumar Sahu", "Premith Kumar Chilukuri", "Matthew Hetrich"], "title": "MiRAGE: A Multiagent Framework for Generating Multimodal Multihop Question-Answer Dataset for RAG Evaluation", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "12 pages, 2 figures, Submitted to ACL", "summary": "The rapid evolution of Retrieval-Augmented Generation (RAG) toward multimodal, high-stakes enterprise applications has outpaced the development of domain specific evaluation benchmarks. Existing datasets often rely on general-domain corpora or purely textual retrieval, failing to capture the complexity of specialized technical documents where information is inextricably multimodal and reasoning requires synthesizing disjoint evidence. We address this gap by introducing MiRAGE, a Multiagent framework for RAG systems Evaluation, that leverages a collaborative swarm of specialized agents to generate verified, domain-specific, multimodal, and multi-hop Question-Answer datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive context optimization loop to aggregate scattered evidence, an adversarial verifier agent to guarantee factual grounding, and an agent to recognize the expert persona and the relevant domain to mimic expert cognitive workflows. Extensive empirical evaluation across four distinct domains (regulations, finance, quantitative biology, and journalism) demonstrates that MiRAGE generates datasets with significantly higher reasoning complexity (>2.3 average hops) and factual faithfulness. Our ablation studies point that MiRAGE can be powered by LLMs if textual descriptions of the images are available. Visual grounding still remains a frontier. By automating the creation of gold standard evaluation datasets that reflect the latent thematic structure of proprietary corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark the next generation information retrieval systems.", "AI": {"tldr": "MiRAGE 是一个多智能体框架，用于为 RAG 系统生成多模态、多跳的问答数据集。", "motivation": "现有评测基准无法跟上 RAG 向多模态企业应用的发展步伐，缺乏专门针对复杂技术文档的评测基准。为此，引入了 MiRAGE 来弥补这一空白。", "method": "MiRAGE 利用协作智能体群生成经过验证、领域特定、多模态和多跳问答数据集，包括递归上下文优化循环聚合证据、对抗性验证器保证事实准确性以及识别专家人格和相关领域的代理。", "result": "在四个不同领域（法规、金融、定量生物学和新闻）的广泛实证评估中，MiRAGE 生成的数据集具有更高的推理复杂性和事实忠实度。", "conclusion": "通过自动化创建反映专有语料库潜在主题结构的黄金标准评测数据集，MiRAGE 为严格基准下一代信息检索系统提供了必要的基础设施。"}}
{"id": "2601.15486", "pdf": "https://arxiv.org/pdf/2601.15486", "abs": "https://arxiv.org/abs/2601.15486", "authors": ["Javier N. Ramos-Silva", "Peter J. Burke"], "title": "A Universal Large Language Model -- Drone Command and Control Interface", "categories": ["cs.RO"], "comment": null, "summary": "The use of artificial intelligence (AI) for drone control can have a transformative impact on drone capabilities, especially when real world information can be integrated with drone sensing, command, and control, part of a growing field of physical AI. Large language models (LLMs) can be advantageous if trained at scale on general knowledge, but especially and in particular when the training data includes information such as detailed map geography topology of the entire planet, as well as the ability to access real time situational data such as weather. However, challenges remain in the interface between drones and LLMs in general, with each application requiring a tedious, labor intensive effort to connect the LLM trained knowledge to drone command and control. Here, we solve that problem, using an interface strategy that is LLM agnostic and drone agnostic, providing the first universal, versatile, comprehensive and easy to use drone control interface. We do this using the new model context protocol (MCP) standard, an open standard that provides a universal way for AI systems to access external data, tools, and services. We develop and deploy a cloud based Linux machine hosting an MCP server that supports the Mavlink protocol, an ubiquitous drone control language used almost universally by millions of drones including Ardupilot and PX4 framework.We demonstrate flight control of a real unmanned aerial vehicle. In further testing, we demonstrate extensive flight planning and control capability in a simulated drone, integrated with a Google Maps MCP server for up to date, real time navigation information. This demonstrates a universal approach to integration of LLMs with drone command and control, a paradigm that leverages and exploits virtually all of modern AI industry with drone technology in an easy to use interface that translates natural language to drone control.", "AI": {"tldr": "开发一个通用的大语言模型与无人机控制接口，实现自然语言到无人机控制的转换。", "motivation": "利用AI提升无人机能力，并解决大语言模型（LLMs）和无人机之间的连接问题，使其更加便捷、高效。", "method": "使用新的MCP标准来构建一个云托管Linux机器作为MCP服务器，支持MAVLink协议，实现了从自然语言到无人机控制的转换。", "result": "演示了对实际无人飞行器的飞行控制，并在模拟环境中展示了广泛的飞行规划和控制能力，集成了实时导航信息。", "conclusion": "提出了一个通用的方法来整合LLMs与无人机命令控制接口，实现现代AI技术与无人机技术的结合，提供了一个易于使用的自然语言到无人机控制转换界面。"}}
{"id": "2601.15485", "pdf": "https://arxiv.org/pdf/2601.15485", "abs": "https://arxiv.org/abs/2601.15485", "authors": ["Yifan Qian", "Zhe Wen", "Alexander C. Furnas", "Yue Bai", "Erzhuo Shao", "Dashun Wang"], "title": "The Rise of Large Language Models and the Direction and Impact of US Federal Research Funding", "categories": ["cs.DL", "cs.AI", "cs.CY", "physics.soc-ph"], "comment": "41 pages, 23 figures, 12 tables", "summary": "Federal research funding shapes the direction, diversity, and impact of the US scientific enterprise. Large language models (LLMs) are rapidly diffusing into scientific practice, holding substantial promise while raising widespread concerns. Despite growing attention to AI use in scientific writing and evaluation, little is known about how the rise of LLMs is reshaping the public funding landscape. Here, we examine LLM involvement at key stages of the federal funding pipeline by combining two complementary data sources: confidential National Science Foundation (NSF) and National Institutes of Health (NIH) proposal submissions from two large US R1 universities, including funded, unfunded, and pending proposals, and the full population of publicly released NSF and NIH awards. We find that LLM use rises sharply beginning in 2023 and exhibits a bimodal distribution, indicating a clear split between minimal and substantive use. Across both private submissions and public awards, higher LLM involvement is consistently associated with lower semantic distinctiveness, positioning projects closer to recently funded work within the same agency. The consequences of this shift are agency-dependent. LLM use is positively associated with proposal success and higher subsequent publication output at NIH, whereas no comparable associations are observed at NSF. Notably, the productivity gains at NIH are concentrated in non-hit papers rather than the most highly cited work. Together, these findings provide large-scale evidence that the rise of LLMs is reshaping how scientific ideas are positioned, selected, and translated into publicly funded research, with implications for portfolio governance, research diversity, and the long-run impact of science.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）在联邦研究资金管道中的使用及其对科学研究方向和多样性的影响。", "motivation": "鉴于大型语言模型的迅速普及，以及其在科学写作与评估中日益受到关注，本研究旨在了解这些模型如何改变公共资助领域的格局。", "method": "通过整合两所大型R1大学提交给NSF和NIH的提案（包括已获资助、未获资助和待定提案）以及公开发布的所有NSF和NIH奖项数据进行分析。", "result": "发现2023年后LLM使用量急剧上升，其参与度与项目接近最近获得资助的工作呈现显著关联。在NIH，LLM的使用与提案的成功及后续出版物数量增加有关；而在NSF，则未观察到类似的结果。", "conclusion": "研究结果表明，大型语言模型的兴起正重新塑造科学思想定位、选择和转化为公共资助的研究的方式，并对投资组合治理、研究多样性以及科学研究的长期影响产生重要启示。"}}
{"id": "2601.15484", "pdf": "https://arxiv.org/pdf/2601.15484", "abs": "https://arxiv.org/abs/2601.15484", "authors": ["Philipp Eibl", "Erica Coppolillo", "Simone Mungari", "Luca Luceri"], "title": "Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia and Grokipedia on Controversial Topics", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Online encyclopedias are central to contemporary information infrastructures and have become focal points of debates over ideological bias. Wikipedia, in particular, has long been accused of left-leaning bias, while Grokipedia, an AI-generated encyclopedia launched by xAI, has been framed as a right-leaning alternative. This paper presents a comparative analysis of Wikipedia and Grokipedia on well-established politically contested topics. Specifically, we examine differences in semantic framing, political orientation, and content prioritization. We find that semantic similarity between the two platforms decays across article sections and diverges more strongly on controversial topics than on randomly sampled ones. Additionally, we show that both encyclopedias predominantly exhibit left-leaning framings, although Grokipedia exhibits a more bimodal distribution with increased prominence of right-leaning content. The experimental code is publicly available.", "AI": {"tldr": "该论文比较了维基百科和Grokipedia在争议性话题上的政治框架，分析两者的意识形态倾向。", "motivation": "鉴于在线百科全书已成为当代信息基础设施的关键部分，并且对其中的意识形态偏见存在广泛讨论，尤其是关于维基百科左倾偏见的指控以及Grokipedia作为右倾替代品的说法，该论文旨在通过比较这两种平台来验证这些观点。", "method": "研究者们针对已知的政治争议话题进行了语义框架、政治倾向和内容优先级方面的差异分析。具体而言，他们检查了两个平台上文章各部分之间的语义相似性，并对比了在争议话题与随机样本上的不同。", "result": "研究表明，在线百科全书平台之间存在显著的语义差异，尤其是在有争议的话题上表现得更为明显。尽管Grokipedia显示出了更多的右倾内容，但两者的主导倾向仍是左倾。", "conclusion": "该研究证实了维基百科和Grokipedia在政治框架上的分歧，尤其是对争议话题而言，并指出Grokipedia虽然有更显著的右倾倾向，但也倾向于呈现双模态分布。"}}
{"id": "2601.15482", "pdf": "https://arxiv.org/pdf/2601.15482", "abs": "https://arxiv.org/abs/2601.15482", "authors": ["Huayu Li", "ZhengXiao He", "Siyuan Tian", "Jinghao Wen", "Ao Li"], "title": "Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.", "AI": {"tldr": "本文提出了Martingale Foresight Sampling（MFS）框架，将大语言模型的解码问题转化为识别最优随机过程的问题。", "motivation": "传统的自回归解码在大规模语言模型中存在短视问题，导致难以找到全局最优的推理路径。现有的预见抽样方法依赖于启发式策略来评估和剪枝搜索空间。", "method": "MFS框架利用鞅理论设计了一个基于概率论原则的算法：通过Doob分解定理测量路径的可预测优势；使用选停理论进行原理性剪枝；并根据鞅收敛定理终止探索，确保路径质量收敛。", "result": "实验显示，在六个推理基准测试中，MFS在准确性上超过了现有最佳方法，并显著提高了计算效率。", "conclusion": "Martingale Foresight Sampling提供了理论上坚实的基础来改进大语言模型的解码过程，提升了准确性和计算效率。"}}
{"id": "2601.15479", "pdf": "https://arxiv.org/pdf/2601.15479", "abs": "https://arxiv.org/abs/2601.15479", "authors": ["Sydney Anuyah", "Sneha Shajee-Mohan", "Ankit-Singh Chauhan", "Sunandan Chakraborty"], "title": "Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and Multi-Domain Contexts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The safe deployment of large language models (LLMs) in high-stakes fields like biomedicine, requires them to be able to reason about cause and effect. We investigate this ability by testing 13 open-source LLMs on a fundamental task: pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse datasets, evaluates two core skills: 1) \\textbf{Causal Detection} (identifying if a text contains a causal link) and 2) \\textbf{Causal Extraction} (pulling out the exact cause and effect phrases). We tested various prompting methods, from simple instructions (zero-shot) to more complex strategies like Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL). The results show major deficiencies in current models. The best model for detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\\% ($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct, reached just 47.12\\% ($C_{extract}$). Models performed best on simple, explicit, single-sentence relations. However, performance plummeted for more difficult (and realistic) cases, such as implicit relationships, links spanning multiple sentences, and texts containing multiple causal pairs. We provide a unified evaluation framework, built on a dataset validated with high inter-annotator agreement ($κ\\ge 0.758$), and make all our data, code, and prompts publicly available to spur further research. \\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here: https://github.com/sydneyanuyah/CausalDiscovery}", "AI": {"tldr": "本文评估了13个开源大语言模型在生物医学和其他多领域环境下进行成对因果发现的能力。", "motivation": "为了确保大语言模型在高风险领域的安全应用，尤其是医疗保健行业，它们需要能够理解因果关系。该研究旨在测试这些模型在这方面的能力。", "method": "使用了包含12个不同数据集的基准测试来评估两个核心技能：因果检测和因果抽取。采用零样本、链式思维和少量示例学习等不同的提示方法对模型进行测试。", "result": "最佳模型在因果检测上的平均得分为49.57%，而在因果提取上仅为47.12%。模型在处理简单的单句关系时表现较好，但在应对更复杂的实际案例时性能显著下降。", "conclusion": "当前的模型存在重大缺陷，在识别和抽取文本中的复杂因果关系方面能力不足。该研究提供了一个统一的评估框架，并将数据、代码和提示公开以促进进一步的研究。"}}
{"id": "2601.15476", "pdf": "https://arxiv.org/pdf/2601.15476", "abs": "https://arxiv.org/abs/2601.15476", "authors": ["Alex Dantart"], "title": "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases", "categories": ["cs.AI", "cs.PF"], "comment": null, "summary": "This paper examines how to make large language models reliable for high-stakes legal work by reducing hallucinations. It distinguishes three AI paradigms: (1) standalone generative models (\"creative oracle\"), (2) basic retrieval-augmented systems (\"expert archivist\"), and (3) an advanced, end-to-end optimized RAG system (\"rigorous archivist\"). The authors introduce two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate (FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal tasks using expert, double-blind review. Results show that standalone models are unsuitable for professional use (FCR above 30%), while basic RAG greatly reduces errors but still leaves notable misgrounding. Advanced RAG, using techniques such as embedding fine-tuning, re-ranking, and self-correction, reduces fabrication to negligible levels (below 0.2%). The study concludes that trustworthy legal AI requires rigor-focused, retrieval-based architectures emphasizing verification and traceability, and provides an evaluation framework applicable to other high-risk domains.", "AI": {"tldr": "本文探讨如何通过减少大语言模型中的幻觉来提高其在高风险法律工作中的可靠性，并评估了三种不同AI架构的表现。", "motivation": "动机在于解决大语言模型在处理专业领域任务时可能出现的错误和不准确信息，特别是对于法律这样对准确性要求极高的领域。", "method": "提出了两个可靠度指标：虚假引用率(FCR)和虚构事实率(FFR)，并评估了12种大型语言模型生成的2700个司法风格答案，在75项法律任务中使用专家双盲评审进行评价。", "result": "研究表明，独立模型不适用于专业用途（FCR超过30%），而基本检索增强系统显著减少错误但仍然存在显著偏差。高级检索增强系统通过技术如嵌入微调、重新排名和自我校正等将虚构信息降低到可忽略水平（低于0.2%）。", "conclusion": "研究得出结论，值得信赖的法律AI需要采用注重验证和追溯性的检索架构，并提供了一个适用于其他高风险领域的评估框架。"}}
{"id": "2601.15475", "pdf": "https://arxiv.org/pdf/2601.15475", "abs": "https://arxiv.org/abs/2601.15475", "authors": ["Yunshan Qi", "Lin Zhu", "Nan Bao", "Yifan Zhao", "Jia Li"], "title": "Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.", "AI": {"tldr": "本文提出了一种基于传感器物理的NeRF框架，用于从单曝光模糊低动态范围图像和事件中进行清晰的高动态范围视图合成。", "motivation": "现有方法在极端光照条件下难以恢复高质量的HDR和锐利3D表示，并忽略了相机输出与物理世界辐射之间的传感物理不匹配问题。", "method": "提出了一种统一的基于传感器物理的NeRF框架，直接用NeRF表示三维场景的实际辐射，在高动态范围领域中建模原始HDR场景光线照射到传感器像素上，并引入了两个映射场来优化NeRF网络和提高3D表示学习。", "result": "实验表明，该方法在单曝光模糊LDR图像及其对应事件上的去模糊HDR视图合成结果达到了最先进的水平。", "conclusion": "通过结合物理世界动态信息与实际事件传感器输出之间的关联，本文提出的方法显著提高了从单曝光模糊低动态范围图像中获取高质量3D表示的能力。"}}
{"id": "2601.15474", "pdf": "https://arxiv.org/pdf/2601.15474", "abs": "https://arxiv.org/abs/2601.15474", "authors": ["Md Nabi Newaz Khan", "Abdullah Arafat Miah", "Yu Bi"], "title": "Multi-Targeted Graph Backdoor Attack", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Graph neural network (GNN) have demonstrated exceptional performance in solving critical problems across diverse domains yet remain susceptible to backdoor attacks. Existing studies on backdoor attack for graph classification are limited to single target attack using subgraph replacement based mechanism where the attacker implants only one trigger into the GNN model. In this paper, we introduce the first multi-targeted backdoor attack for graph classification task, where multiple triggers simultaneously redirect predictions to different target labels. Instead of subgraph replacement, we propose subgraph injection which preserves the structure of the original graphs while poisoning the clean graphs. Extensive experiments demonstrate the efficacy of our approach, where our attack achieves high attack success rates for all target labels with minimal impact on the clean accuracy. Experimental results on five dataset demonstrate the superior performance of our attack framework compared to the conventional subgraph replacement-based attack. Our analysis on four GNN models confirms the generalization capability of our attack which is effective regardless of the GNN model architectures and training parameters settings. We further investigate the impact of the attack design parameters including injection methods, number of connections, trigger sizes, trigger edge density and poisoning ratios. Additionally, our evaluation against state-of-the-art defenses (randomized smoothing and fine-pruning) demonstrates the robustness of our proposed multi-target attacks. This work highlights the GNN vulnerability against multi-targeted backdoor attack in graph classification task. Our source codes will be available at https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.", "AI": {"tldr": "本文提出了一种针对图分类任务的多目标后门攻击方法。", "motivation": "现有的关于图分类中的后门攻击研究主要局限于单一目标，通过子图替换机制进行。本文旨在克服这一局限，并提高对GNN模型的攻击效果。", "method": "作者提出了一个基于子图注入的多目标后门攻击框架，该方法在保留原始图结构的同时对其进行污染。", "result": "实验表明，相比传统的子图替换攻击，本方法能够实现更高的攻击成功率并保持较高的清洁数据准确性。此外，在不同的GNN模型和参数设置下都显示出了良好的泛化能力。", "conclusion": "研究表明，GNN在多目标后门攻击面前存在脆弱性，并且提出的攻击框架展现了比传统防御更强的鲁棒性。"}}
{"id": "2601.15473", "pdf": "https://arxiv.org/pdf/2601.15473", "abs": "https://arxiv.org/abs/2601.15473", "authors": ["Fahd Seddik", "Abdulrahman Elbedewy", "Gaser Sami", "Mohamed Abdelmoniem", "Yahia Zakaria"], "title": "Panther: Faster and Cheaper Computations with Randomized Numerical Linear Algebra", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages, 3 figures, 2 listings", "summary": "Training modern deep learning models is increasingly constrained by GPU memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA) offers proven techniques to compress these models, the lack of a unified, production-grade library prevents widely adopting these methods. We present Panther, a PyTorch-compatible library that consolidates established RandNLA algorithms into a single high-performance framework. Panther engineers efficient, drop-in replacements for standard components including sketched linear layers, 2D convolution, multi-head attention, and randomized matrix decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA backend (pawX), Panther provides an optimized implementation that can run on both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and Panther's ease of adoption. By replacing standard PyTorch linear layers with Panther layers (requiring only a few lines of code) we achieve significant memory savings (up to 75%) on BERT while maintaining comparable loss. Source code is available (MIT License) at https://github.com/FahdSeddik/panther, along with demonstration video at https://youtu.be/7M3RQb4KWxs.", "AI": {"tldr": "Panther是一个与PyTorch兼容的库，它整合了随机数值线性代数算法来实现更快速和低成本的深度学习模型计算。", "motivation": "现代深度学习模型训练受到GPU内存和计算限制。虽然随机数值线性代数提供了压缩这些模型的方法，但是缺乏一个统一且适合生产的库阻碍了广泛采用这些方法。", "method": "Panther提供了一个高效的框架，包括简化的线性层、2D卷积、多头注意力机制和随机矩阵分解的高效实现，并使用自定义C++/CUDA后端（pawX）进行优化以支持CPU和GPU运行。", "result": "通过将标准PyTorch线性层替换为Panther层，只需少量代码修改即可在BERT模型上实现高达75%的记忆节省且保持类似的损失值。", "conclusion": "Panther展示了随机数值线性代数技术的有效性和易于采用，同时提供了开源的MIT许可代码和演示视频。"}}
{"id": "2601.15472", "pdf": "https://arxiv.org/pdf/2601.15472", "abs": "https://arxiv.org/abs/2601.15472", "authors": ["Jana Franceska Funke", "Mario Sagawa", "Georgious Nurcan-Georgiou", "Naomi Sagawa", "Dennis Dietz", "Evgeny Stemasov", "Enrico Rukzio", "Teresa Hirzle"], "title": "Put Your Muscle Into It: Introducing XEM2, a Novel Approach for Monitoring Exertion in Stationary Physical Exercises Leveraging Muscle Work", "categories": ["cs.HC"], "comment": null, "summary": "We present a novel system for camera-based measurement and visualization of muscle work based on the Hill-Type-Muscle-Model: the exercise exertion muscle-work monitor (\\textit{XEM}$^{2}$). Our aim is to complement and, thus, address issues of established measurement techniques that offer imprecise data for non-uniform movements (burned calories) or provide limited information on strain across different body parts (self-perception scales). We validate the reliability of XEM's measurements through a technical evaluation of ten participants and five exercises. Further, we assess the acceptance, usefulness, benefits, and opportunities of \\textit{XEM}$^{2}$ in an empirical user study. Our results show that \\textit{XEM}$^{2}$ provides reliable values of muscle work and supports participants in understanding their workout while also providing reliable information about perceived exertion per muscle group. With this paper, we introduce a novel system capable of measuring and visualizing exertion for single muscle groups, which has the potential to improve exercise monitoring to prevent unbalanced workouts.", "AI": {"tldr": "介绍了一种基于相机的肌肉工作测量和可视化系统XEM2，用于监测静态锻炼中的用力情况。", "motivation": "旨在解决现有技术在非均匀运动中提供不精确数据或对不同身体部位施加压力信息有限的问题。", "method": "通过一个技术评估实验（十名参与者五个练习）验证了XEM2的测量可靠性，并进行了接受度、使用价值和机会的用户研究。", "result": "结果表明，XEM2能够提供可靠的肌肉工作值，帮助参与者了解他们的锻炼情况，并能为每个肌肉群提供可靠的压力信息。", "conclusion": "引入了一种可以对单个肌肉群进行测量和可视化的系统，有潜力改善运动监测，防止不均衡训练。"}}
{"id": "2601.15470", "pdf": "https://arxiv.org/pdf/2601.15470", "abs": "https://arxiv.org/abs/2601.15470", "authors": ["Shuchi Chawla", "Kristin Sheridan"], "title": "Nested and outlier embeddings into trees", "categories": ["cs.DS"], "comment": null, "summary": "In this paper, we consider outlier embeddings into HSTs and ultrametrics. In particular, for $(X,d)$, let $k$ be the size of the smallest subset of $X$ such that all but that subset (i.e. the ``outlier set'') can be probabilistically embedded into the space of HSTs with expected distortion at most $c$. Our primary result is showing that there exists an efficient algorithm that takes in $(X,d)$ and a target distortion $c$ and samples from a probabilistic embedding with at most $O(\\frac k ε\\log^2k)$ outliers and distortion at most $(32+ε)c$, for any $ε>0$. This leads to better instance-specific approximations for certain instances of the buy-at-bulk and dial-a-ride problems, whose current best approximation algorithms go through HST embeddings. In order to facilitate our results, we largely focus on the concept of compositions of nested embeddings introduced by [Chawla and Sheridan 2024]. A nested embedding is a composition of two embeddings of a metric space $(X,d)$ -- a low distortion embedding of a subset $S$ of nodes, and a higher distortion embedding of the entire metric. The composition is a single embedding that preserves the low distortion over $S$ and does not increase distortion over the remaining points by much. In this paper, we expand this concept from the setting of deterministic embeddings to the setting of probabilistic embeddings. We show how to find good nested compositions of embeddings into HSTs, and combine this with an approximation algorithm of [Munagala et al. 2023] to obtain our results.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.15468", "pdf": "https://arxiv.org/pdf/2601.15468", "abs": "https://arxiv.org/abs/2601.15468", "authors": ["Kareem Amin", "Alex Bie", "Weiwei Kong", "Umar Syed", "Sergei Vassilvitskii"], "title": "Learning from Synthetic Data: Limitations of ERM", "categories": ["cs.LG", "cs.DS", "stat.ML"], "comment": ":68T05ACM Class:I.2.6", "summary": "The prevalence and low cost of LLMs have led to a rise of synthetic content. From review sites to court documents, ``natural'' content has been contaminated by data points that appear similar to natural data, but are in fact LLM-generated. In this work we revisit fundamental learning theory questions in this, now ubiquitous, setting. We model this scenario as a sequence of learning tasks where the input is a mix of natural and synthetic data, and the learning algorithms are oblivious to the origin of any individual example. We study the possibilities and limitations of ERM in this setting. For the problem of estimating the mean of an arbitrary $d$-dimensional distribution, we find that while ERM converges to the true mean, it is outperformed by an algorithm that assigns non-uniform weights to examples from different generations of data. For the PAC learning setting, the disparity is even more stark. We find that ERM does not always converge to the true concept, echoing the model collapse literature. However, we show there are algorithms capable of learning the correct hypothesis for arbitrary VC classes and arbitrary amounts of contamination.", "AI": {"tldr": "该论文探讨了在混合真实和合成数据的情况下，经验风险最小化（ERM）算法的学习能力和局限性。", "motivation": "随着LLM生成内容的普及，真实与合成数据混杂的情况变得普遍。本研究旨在重新审视这种情况下机器学习理论的基本问题。", "method": "论文将情景建模为一系列学习任务，其中输入是自然和合成数据的混合，并且算法无法区分每个样本的来源。通过比较ERM和其他权重分配不均匀的方法来分析其性能差异。", "result": "对于估计$d$维分布均值的问题，尽管ERM可以收敛到真实的均值，但它被赋予不同数据世代非统一权值的算法所超越；在PAC学习设置下，这种差距更加显著。ERM并不总能收敛于真实概念，但存在能够正确学习任意VC类和任意程度污染假设的方法。", "conclusion": "研究揭示了经验风险最小化（ERM）在处理包含大量合成数据的任务中的局限性，并表明使用非均匀加权算法可以更有效地解决这些问题。"}}
{"id": "2601.15288", "pdf": "https://arxiv.org/pdf/2601.15288", "abs": "https://arxiv.org/abs/2601.15288", "authors": ["Jiwon Kang", "Yeji Choi", "JoungBin Lee", "Wooseok Jang", "Jinhyeok Choi", "Taekeun Kang", "Yongjae Park", "Myungin Kim", "Seungryong Kim"], "title": "APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping", "categories": ["cs.CV"], "comment": "Project Page: https://cvlab-kaist.github.io/APPLE/", "summary": "Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.", "AI": {"tldr": "本论文提出了一种基于扩散模型的教师-学生框架（APPLE），用于提高人脸交换中属性保真度和身份转换的质量。", "motivation": "由于真实的人脸交换数据不可用，且现有方法在保持目标面部特定属性如姿势、表情、光线、肤色和化妆方面存在困难，本论文旨在解决这些问题并提升视觉效果的保真度。", "method": "通过将人脸交换重新定义为条件去模糊任务，并引入基于属性感知的伪标签监督和逆向方案，以更忠实地保留目标面部特定属性。", "result": "APPLE实现了最先进的性能，在身份转换和属性保留方面产生了更加逼真的结果，符合目标面部特征。", "conclusion": "通过详细的属性保真设计进行教师学习，APPLE生成了高质量的伪三元组，并提供了直接的人脸交换监督，从而提升了整体效果。"}}
{"id": "2601.15287", "pdf": "https://arxiv.org/pdf/2601.15287", "abs": "https://arxiv.org/abs/2601.15287", "authors": ["Gautom Das", "Vincent La", "Ethan Lau", "Abhinav Shrivastava", "Matthew Gwilliam"], "title": "Towards Understanding Best Practices for Quantization of Vision-Language Models", "categories": ["cs.CV"], "comment": "15 pages, 12 figures, 1 table", "summary": "Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.", "AI": {"tldr": "研究了多种量化方法在视觉语言模型中的应用，探讨了不同位宽、量化方法以及量化位置对模型性能的影响。", "motivation": "减少大规模语言模型的内存和延迟需求，探索如何通过有效的量化策略来保持多模态模型的性能。", "method": "研究了GPTQ和AWQ等最新量化技术在视觉变压器（ViT）、语言模型及其连接部分的应用效果。", "result": "发现ViT和LLM在模型性能上具有相似的重要性，尽管参数大小差异显著，并且较低位宽对LLM的量化可以达到高精度。", "conclusion": "这些研究结果为多模态大规模语言模型（MLLM）的有效部署提供了实用见解，并强调了探索组件敏感性的重要价值。"}}
{"id": "2601.15286", "pdf": "https://arxiv.org/pdf/2601.15286", "abs": "https://arxiv.org/abs/2601.15286", "authors": ["Shantanu Jaiswal", "Mihir Prabhudesai", "Nikash Bhardwaj", "Zheyang Qin", "Amir Zadeh", "Chuan Li", "Katerina Fragkiadaki", "Deepak Pathak"], "title": "Iterative Refinement Improves Compositional Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project webpage: https://iterative-img-gen.github.io/", "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/", "AI": {"tldr": "本文提出了一种迭代修正策略，以改善文本到图像模型在处理复杂指令时的表现。", "motivation": "现有的文本到图像生成模型虽有所进展，但在应对复杂的多对象、关系和属性同时存在的场景中表现不佳。因此，研究动机在于寻求一种更有效的测试时间策略来改进这些模型的性能。", "method": "本论文的方法是采用迭代式精炼过程，在这一过程中，一个视觉语言模型被用作批评者，引导图像生成器逐步修正其结果以更好地满足复杂的指令要求。", "result": "实验证明，本文的方法在多个基准测试中都取得了显著的改进，包括ConceptMix、T2I-CompBench和Visual Jenga场景分解中的正确率分别提升了16.9%、13.8%和12.5%，并且人类评估者更偏好此方法生成的结果。", "conclusion": "研究结果表明，迭代自我修正是一种广泛适用于组合图像生成的有效原则，能够提高复杂场景中图像的生成质量。"}}
{"id": "2601.15284", "pdf": "https://arxiv.org/pdf/2601.15284", "abs": "https://arxiv.org/abs/2601.15284", "authors": ["Anurag Bagchi", "Zhipeng Bao", "Homanga Bharadhwaj", "Yu-Xiong Wang", "Pavel Tokmakov", "Martial Hebert"], "title": "Walk through Paintings: Egocentric World Models from Internet Priors", "categories": ["cs.CV"], "comment": null, "summary": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.", "AI": {"tldr": "该论文介绍了Egocentric World Model（EgoWM），一种简单且架构无关的方法，可将任何预训练的视频扩散模型转化为条件动作世界模型，实现可控未来预测。", "motivation": "动机在于探索是否可以利用现有的大规模视频模型的丰富先验知识，并通过轻量级条件层注入运动指令来准确反映每个动作如何改变世界的可能性。", "method": "EgoWM 方法利用已有的预训练视频扩散模型，不从头开始训练而是重新调整这些模型，通过轻量级条件层注射电机命令以实现忠实的动作跟随和保持现实性和良好的泛化能力。此方法可以应用于不同自由度的机器人或人形体中，并能够生成连贯的任务流程。", "result": "EgoWM 在导航世界模型上的结构一致性评分（SCS）提高了80%，同时降低了6倍的推理延迟，展示了在未见过环境中的强大泛化能力，包括画作内的导航任务。", "conclusion": "结论是通过EgoWM方法，可以将预训练视频扩散模型转化为有效的条件动作世界模型，该模型能够准确预测未来场景，并保持高真实感和良好的泛化性。"}}
{"id": "2601.15283", "pdf": "https://arxiv.org/pdf/2601.15283", "abs": "https://arxiv.org/abs/2601.15283", "authors": ["Ruofan Liang", "Norman Müller", "Ethan Weber", "Duncan Zauss", "Nandita Vijaykumar", "Peter Kontschieder", "Christian Richardt"], "title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://luxremix.github.io", "summary": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.", "AI": {"tldr": "该论文提出了一种从单个多视角场景捕获中进行室内场景的互动灯光编辑的新方法。", "motivation": "动机在于提供一种能够独立控制复杂室内场景照明光源的方法，允许对每个光源的状态、色度和强度进行操控，并确保多视图光照的一致性。", "method": "该方法采用了一种生成图像基的光分解模型来将复杂的室内场景照明分解为其构成的光源，并引入了多视角照明谐化技术以确保光照分解在所有场景视图中的一致性，整合到一个可重新照明的3D高斯散射表示中。", "result": "结果展示了跨多种室内场景的高度逼真的照明分解和再照明效果，在合成数据集和真实世界数据集上进行了定量和定性比较。", "conclusion": "结论表明该方法能够实现高质量的实时互动控制，为用户提供了一种新的编辑室内场景光照的方式。"}}
{"id": "2601.15282", "pdf": "https://arxiv.org/pdf/2601.15282", "abs": "https://arxiv.org/abs/2601.15282", "authors": ["Yufan Deng", "Zilin Pan", "Hongyu Zhang", "Xiaojie Li", "Ruoqing Hu", "Yufei Ding", "Yiming Zou", "Yan Zeng", "Daquan Zhou"], "title": "Rethinking Video Generation Model for the Embodied World", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/", "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "AI": {"tldr": "论文介绍了一种新的机器人视频生成模型评估基准RBench和一个大规模的公开数据集RoVid-X，以解决现有模型在物理现实性方面的不足，并促进视频生成模型的发展。", "motivation": "动机在于当前的视频生成模型在模拟真实世界机器人交互时存在困难，缺乏标准化的评估基准来公平比较和推动该领域进步。", "method": "作者提出了RBench作为机器人的视频生成评估标准，包括五个任务领域和四种不同的身体形态。同时，通过改进的数据处理流程创建了RoVid-X数据集，包含400万注释视频片段。", "result": "对25种代表性模型的评价显示它们在产生物理真实的机器人行为上存在显著缺陷。RBench与人类评估之间的Spearman相关系数为0.96，表明其有效性。", "conclusion": "通过建立一套包括评估和数据训练在内的生态系统，该论文为视频生成模型的发展提供了坚实的基础，并有望加速具身人工智能向通用智能的演进。"}}
{"id": "2601.15281", "pdf": "https://arxiv.org/pdf/2601.15281", "abs": "https://arxiv.org/abs/2601.15281", "authors": ["Ying Yang", "Zhengyao Lv", "Tianlin Pan", "Haofan Wang", "Binxin Yang", "Hubery Yin", "Chen Li", "Ziwei Liu", "Chenyang Si"], "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation", "categories": ["cs.CV"], "comment": "17 pages, 21 figures,", "summary": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.", "AI": {"tldr": "本文提出了StableWorld，一种动态帧淘汰机制，用于提高长时互动视频生成的稳定性和时间一致性。", "motivation": "虽然世界建模取得了显著进展，但在长时段交互中仍存在不稳定和时间退化问题，导致空间漂移和场景崩溃。为了解决这些问题并理解其原因，作者提出了一种新的方法。", "method": "StableWorld通过持续过滤降级帧而保留几何一致的帧来防止累积漂移的发生。", "result": "实验结果表明，StableWorld在多个互动视频模型上都表现出色，提升了稳定性和时间一致性，并且具有良好的泛化能力。", "conclusion": "StableWorld作为一种简单有效的机制，可以广泛应用于不同的互动视频生成框架中以提高其性能。"}}
{"id": "2601.15280", "pdf": "https://arxiv.org/pdf/2601.15280", "abs": "https://arxiv.org/abs/2601.15280", "authors": ["Chloe Qianhui Zhao", "Jie Cao", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback", "categories": ["cs.HC"], "comment": "11 pages, to be published at the 16th International Learning Analytics & Knowledge Conference (LAK '26)", "summary": "Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectiveness, (2) learner engagement, (3) perceived feedback quality and value. Results showed that AI multimodal feedback achieved learning gains equivalent to original educator feedback while significantly outperforming it on perceived clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load, with comparable correctness, trust, and acceptance. Process logs revealed distinct engagement patterns: for multiple-choice questions, educator feedback encouraged more submissions; for open-ended questions, AI-facilitated targeted suggestions lowered revision barriers and promoted iterative improvement. These findings highlight the potential of AI multimodal feedback to provide scalable, real-time, and context-aware support that both reduces instructor workload and enhances student experience.", "AI": {"tldr": "研究了一种实时AI辅助的多模态反馈系统，与传统教师反馈相比，在学习效果、学生参与度和感知反馈质量上进行了对比分析。", "motivation": "提供及时、有针对性且多模态的反馈对于帮助学生纠正错误、建立深刻理解和保持动力至关重要，但如何在大规模应用中实现这一目标是一个挑战。", "method": "研究设计了一个集成结构化文本解释与动态多媒体资源的实时AI辅助系统，并通过在线众包实验将该系统生成的反馈与固定的传统教师反馈进行了比较。", "result": "结果显示，AI多模态反馈在学习效果上等同于传统教师反馈，在感知清晰度、具体性、简洁性、动机、满意度和认知负荷等方面表现更优。过程日志揭示了不同的参与模式：对于多项选择题，教师反馈鼓励更多提交；而对于开放性问题，AI辅助的针对性建议降低了修订障碍并促进了迭代改进。", "conclusion": "这些发现表明，AI多模态反馈有潜力提供可扩展、实时且情境感知的支持，既能减少教师的工作负担又能提升学生体验。"}}
{"id": "2601.15279", "pdf": "https://arxiv.org/pdf/2601.15279", "abs": "https://arxiv.org/abs/2601.15279", "authors": ["Christoph Bartmann", "Johannes Schimunek", "Mykyta Ielanskyi", "Philipp Seidl", "Günter Klambauer", "Sohvi Luukkonen"], "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.", "AI": {"tldr": "本文介绍了MolecularIQ，一个专注于分子结构推理的基准测试，通过符号验证任务来评估大型语言模型在化学领域的表现。", "motivation": "当前大多数化学领域的大规模语言模型（LLMs）依赖于广义化学知识、文献或代理标签，并且存在泄漏和偏差的风险。为了更精细地评估这些模型对分子图的推理能力，本文提出了一个新的基准测试MolecularIQ。", "method": "MolecularIQ设计了一系列可以符号验证的任务来专注于分子结构推理，能够揭示模型在特定任务和分子结构上的表现模式。", "result": "通过使用MolecularIQ，研究者们发现了当前化学LLMs的能力和限制，并提供了具体的行动指南来改进这些模型的性能。", "conclusion": "该研究表明了MolecularIQ的有效性，为未来开发更准确地推理分子结构的模型提供指导。"}}
{"id": "2601.15275", "pdf": "https://arxiv.org/pdf/2601.15275", "abs": "https://arxiv.org/abs/2601.15275", "authors": ["Yu Wu", "Minsik Jeon", "Jen-Hao Rick Chang", "Oncel Tuzel", "Shubham Tulsiani"], "title": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://rayrope.github.io/", "summary": "We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.", "AI": {"tldr": "该论文提出了RayRoPE，一种用于多视角注意力机制的位置编码方法，以解决现有位置编码方案不能满足特定需求的问题。", "motivation": "动机在于现有的绝对或相对位置编码方案无法为多视角注意提供独特补丁编码、SE(3)不变性注意和多频率相似性的几何感知编码。", "method": "RayRoPE基于关联光线表示补丁位置，使用沿射线预测的点而不是方向进行几何感知编码，并计算查询框架投影坐标以实现SE(3)不变性和多频率相似性。此外，它还提供了一种机制，在不确定性下解析地计算预期位置编码。", "result": "RayRoPE在新视图合成和立体深度估计任务上验证了其有效性，相对改进了15%的LPIPS（例如CO3D中的表现）。", "conclusion": "结论表明RayRoPE可以无缝集成RGB-D输入，获得比不能进行位置编码的替代方案更大的性能提升。"}}
{"id": "2601.15267", "pdf": "https://arxiv.org/pdf/2601.15267", "abs": "https://arxiv.org/abs/2601.15267", "authors": ["Yiran Hu", "Huanghai Liu", "Chong Wang", "Kunran Li", "Tien-Hsuan Wu", "Haitao Li", "Xinran Xu", "Siqing Huo", "Weihang Su", "Ning Zheng", "Siyuan Zheng", "Qingyao Ai", "Yun Liu", "Renjun Bian", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen", "Ben Kao"], "title": "Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.", "AI": {"tldr": "评估大规模语言模型在法律应用中的表现：挑战、方法与未来方向", "motivation": "随着大规模语言模型越来越多地被整合到法律应用程序中，对这些模型进行系统的性能评估变得至关重要，以确保它们的负责任使用。", "method": "分析了大规模语言模型在法律任务中面临的主要困难，并回顾和分类现有评估方法及基准测试，按任务设计、数据集和评估指标进行了归类。", "result": "讨论了当前方法解决这些挑战的程度，指出了其局限性，并提出了未来研究方向，旨在开发更加现实可靠且基于法律的评估框架。", "conclusion": "大规模语言模型在法律领域的应用需要更深入地关注性能评价中的正确性、推理可靠性及可信赖性等问题，以推动该领域的发展。"}}
{"id": "2601.15260", "pdf": "https://arxiv.org/pdf/2601.15260", "abs": "https://arxiv.org/abs/2601.15260", "authors": ["Dominik Rößle", "Xujun Xie", "Adithya Mohan", "Venkatesh Thirugnana Sambandham", "Daniel Cremers", "Torsten Schön"], "title": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration", "categories": ["cs.CV"], "comment": "Accepted to the IEEE Intelligent Vehicles Symposium 2026. For code and dataset, see https://github.com/cvims/DrivIng", "summary": "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.", "AI": {"tldr": "介绍了一个大规模多模态驾驶数据集DrivIng，该数据集包含完整的地理参考数字孪生。", "motivation": "现有数据集缺乏高保真度的数字孪生，限制了系统测试、边缘案例模拟和仿真到现实世界的评估。因此开发了DrivIng来弥补这一空白。", "method": "收集了一条约18公里长的路线上的RGB相机、LiDAR以及高精度定位设备的数据，并进行了3D边界框和轨迹ID的注释。", "result": "创建了一个包含超过一百万个注释实例的大规模数据集，支持与数字孪生的直接转换，从而实现现实交通场景到仿真的转移。", "conclusion": "发布了DrivIng数据集、数字孪生、高清地图和代码库，以支持可重复研究和稳健验证。"}}
{"id": "2601.15254", "pdf": "https://arxiv.org/pdf/2601.15254", "abs": "https://arxiv.org/abs/2601.15254", "authors": ["Felix Schur", "Niklas Pfister", "Peng Ding", "Sach Mukherjee", "Jonas Peters"], "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\\ell_1$-regularized estimation and post-selection refitting.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.15250", "pdf": "https://arxiv.org/pdf/2601.15250", "abs": "https://arxiv.org/abs/2601.15250", "authors": ["Zichen Xi", "Hao-Xiang Chen", "Nan Xue", "Hongyu Yan", "Qi-Yuan Feng", "Levent Burak Kara", "Joaquim Jorge", "Qun-Ce Xu"], "title": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion", "categories": ["cs.CV", "cs.RO"], "comment": "Under Review", "summary": "Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.", "AI": {"tldr": "本文提出了FlowSSC，一种用于单目RGB图像的生成式语义场景补全框架。", "motivation": "传统的前馈方法在处理单视角下的3D几何推理时存在困难，无法很好地生成被遮挡区域的细节并保持物体间的空间关系。因此，需要一个能够进行准确生成推理的整体3D空间的方法来解决这个问题，并提高现实世界应用中的性能。", "method": "FlowSSC将语义场景补全视为条件生成问题，并引入了Shortcut Flow-matching技术，在紧凑的三平面潜在空间中操作以实现实时推断而不降低质量，通过单步机制实现了高质量生成。", "result": "在SemanticKITTI数据集上的广泛实验表明，FlowSSC达到了最先进的性能，显著超越现有基准。", "conclusion": "FlowSSC作为一种直接应用于单目语义场景补全的生成框架，展示了其优越性，并证明了它可以与现有的前馈方法无缝集成以大幅提高性能。"}}
{"id": "2601.15249", "pdf": "https://arxiv.org/pdf/2601.15249", "abs": "https://arxiv.org/abs/2601.15249", "authors": ["Garrett G. Wen", "Buxin Su", "Natalie Collina", "Zhun Deng", "Weijie Su"], "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism", "categories": ["cs.LG", "cs.AI", "cs.GT", "stat.ME"], "comment": null, "summary": "Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.", "AI": {"tldr": "本文提出了一种基于Isotonic机制的作者辅助方法，用于改进机器学习和人工智能会议的最佳论文奖评选。", "motivation": "随着NeurIPS和ICML等会议提交数量激增，维持审稿质量与一致性变得更具挑战性。特别是在最佳论文奖项的选择方面，该问题尤为突出，因为这些奖项是同行评审过程的重要组成部分，但其选择近年来成为了争议的焦点。", "method": "本文采用Isotonic机制来激励作者对其自己的论文进行排序评价，并利用这种排序信息调整原始审稿分数以优化对提交论文真实质量的估计。对于仅能提名一篇论文的情况，证明了即使当效用函数是非递减且可加时也保持诚实性。", "result": "通过使用2019年至2023年ICLR以及2021年至2023年NeurIPS的公开审稿数据验证了作者报告真实性的激励机制，并证明该方法显著提高了获奖论文的质量。", "conclusion": "研究结果表明，基于Isotonic机制的方法可以有效地改进最佳论文奖的选择过程。"}}
{"id": "2601.15241", "pdf": "https://arxiv.org/pdf/2601.15241", "abs": "https://arxiv.org/abs/2601.15241", "authors": ["Sean Plummer"], "title": "Feasibility Preservation under Monotone Retrieval Truncation", "categories": ["cs.LO", "cs.AI"], "comment": null, "summary": "Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation. We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage. Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.", "AI": {"tldr": "研究了在检索系统中，通过单调截断来维护查询可行性的问题。", "motivation": "探讨了现有的基于相关性的评估无法捕捉到由截断引起的兼容证据未能共现导致的失败问题，并从结构化的角度研究了检索。", "method": "将检索形式化为候选证据集序列，并分析在无限检索深度下的可行性如何影响有限检索深度下的可行性。", "result": "证明单调截断可以保证个体查询的有限见证性，对于类别的查询则需要额外条件即生成有限的见证证书来获得统一的检索界限。", "conclusion": "隔离了作为独立于相关得分或优化的正确性标准的可行性保持，并澄清了基于截断的检索内在的结构限制。"}}
{"id": "2601.15240", "pdf": "https://arxiv.org/pdf/2601.15240", "abs": "https://arxiv.org/abs/2601.15240", "authors": ["Lin Zhang", "Johan Rohdin", "Xin Wang", "Junyi Peng", "Tianchi Liu", "You Zhang", "Hieu-Thi Luong", "Shuai Wang", "Chengdong Liang", "Anna Silnova", "Nicholas Evans"], "title": "WeDefense: A Toolkit to Defend Against Fake Audio", "categories": ["cs.SD", "eess.AS"], "comment": "This is an ongoing work. v1 corresponds to the version completed by June 4, 2025 and previously submitted to ASRU 2025", "summary": "The advances in generative AI have enabled the creation of synthetic audio which is perceptually indistinguishable from real, genuine audio. Although this stellar progress enables many positive applications, it also raises risks of misuse, such as for impersonation, disinformation and fraud. Despite a growing number of open-source fake audio detection codes released through numerous challenges and initiatives, most are tailored to specific competitions, datasets or models. A standardized and unified toolkit that supports the fair benchmarking and comparison of competing solutions with not just common databases, protocols, metrics, but also a shared codebase, is missing. To address this, we propose WeDefense, the first open-source toolkit to support both fake audio detection and localization. Beyond model training, WeDefense emphasizes critical yet often overlooked components: flexible input and augmentation, calibration, score fusion, standardized evaluation metrics, and analysis tools for deeper understanding and interpretation. The toolkit is publicly available at https://github.com/zlin0/wedefense with interactive demos for fake audio detection and localization.", "AI": {"tldr": "介绍了一个名为WeDefense的开源工具包，用于支持假音检测和定位。", "motivation": "随着生成式AI的进步，合成音频变得越来越难以与真实音频区分。这种技术的应用也带来了诸如冒充、虚假信息和欺诈等风险。现有的开放源代码通常为特定的比赛或数据集定制，缺乏一个标准的工具包来进行公平的基准测试和比较。", "method": "WeDefense提供了一个支持假音检测和定位的开源工具包，包括灵活的数据输入和增强、校准、分数融合、标准化评估指标及分析工具，以实现对模型训练之外的关键组件的支持。", "result": "该工具包在公开的GitHub仓库中可以找到，并提供了假音频检测和定位的交互式演示。", "conclusion": "WeDefense是第一个开放源代码工具包，用于支持假音检测和定位，强调了除了模型训练外的重要但往往被忽视的组件。"}}
