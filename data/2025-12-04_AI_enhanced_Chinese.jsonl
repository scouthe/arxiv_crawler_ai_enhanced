{"id": "2512.04453", "pdf": "https://arxiv.org/pdf/2512.04453", "abs": "https://arxiv.org/abs/2512.04453", "authors": ["Debasmita Ghose", "Oz Gitelson", "Marynel Vazquez", "Brian Scassellati"], "title": "Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to ACM/IEEE International Conference on Human-Robot Interaction, 2026 (HRI 2026), 10 pages, 4 figures", "summary": "To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.", "AI": {"tldr": "提出BALI方法，通过双向动作-语言推理实现开放目标推断，用于人机协作场景", "motivation": "现有方法局限于预定义目标集、仅依赖观察动作或仅依赖显式指令，在真实世界交互中表现脆弱，无法处理模糊、难以表达或非固定集合的目标", "method": "BALI方法整合自然语言偏好和观察的人类动作，在后退时域规划树中结合语言和动作线索，仅在答案的预期信息增益超过中断成本时询问澄清问题，并选择与推断目标一致的支持性动作", "result": "在协作烹饪任务中评估，相比基线方法，BALI产生更稳定的目标预测和显著更少的错误", "conclusion": "BALI方法能够有效处理开放、模糊的目标推断问题，通过整合动作和语言信息，在真实人机协作场景中表现更优"}}
{"id": "2512.04451", "pdf": "https://arxiv.org/pdf/2512.04451", "abs": "https://arxiv.org/abs/2512.04451", "authors": ["Yifei Wang", "Zhenkai Li", "Tianwen Qian", "Huanran Zheng", "Zheng Wang", "Yuqian Fu", "Xiaoling Wang"], "title": "StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.", "AI": {"tldr": "提出了StreamEQA基准，用于评估多模态大语言模型在具身场景下对连续视频流的理解能力", "motivation": "随着具身智能向实际应用发展，智能体需要具备持续感知和理解连续视频流的能力，以维持环境感知、理解交互并进行动态规划，但目前缺乏相应的评估基准", "method": "构建了StreamEQA基准，包含156个独立长视频，定义了42个任务，通过自动生成和人工精炼的混合流程生成了约21K个带精确时间戳的问答对，从具身和流式两个维度评估模型", "result": "评估了13个最先进的视频-LLM模型，发现尽管这些模型在传统基准上表现良好，但在具身场景下的连续视频理解方面仍然存在困难", "conclusion": "StreamEQA是首个针对具身场景下连续视频问答的基准，有望推动具身应用中的连续视频理解研究"}}
{"id": "2512.04446", "pdf": "https://arxiv.org/pdf/2512.04446", "abs": "https://arxiv.org/abs/2512.04446", "authors": ["Chang Liu", "Sibo Tian", "Sara Behdad", "Xiao Liang", "Minghui Zheng"], "title": "Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Automating disassembly of critical components from end-of-life (EoL) desktops, such as high-value items like RAM modules and CPUs, as well as sensitive parts like hard disk drives, remains challenging due to the inherent variability and uncertainty of these products. Moreover, their disassembly requires sequential, precise, and dexterous operations, further increasing the complexity of automation. Current robotic disassembly processes are typically divided into several stages: perception, sequence planning, task planning, motion planning, and manipulation. Each stage requires explicit modeling, which limits generalization to unfamiliar scenarios. Recent development of vision-language-action (VLA) models has presented an end-to-end approach for general robotic manipulation tasks. Although VLAs have demonstrated promising performance on simple tasks, the feasibility of applying such models to complex disassembly remains largely unexplored. In this paper, we collected a customized dataset for robotic RAM and CPU disassembly and used it to fine-tune two well-established VLA approaches, OpenVLA and OpenVLA-OFT, as a case study. We divided the whole disassembly task into several small steps, and our preliminary experimental results indicate that the fine-tuned VLA models can faithfully complete multiple early steps but struggle with certain critical subtasks, leading to task failure. However, we observed that a simple hybrid strategy that combines VLA with a rule-based controller can successfully perform the entire disassembly operation. These findings highlight the current limitations of VLA models in handling the dexterity and precision required for robotic EoL product disassembly. By offering a detailed analysis of the observed results, this study provides insights that may inform future research to address current challenges and advance end-to-end robotic automated disassembly.", "AI": {"tldr": "该论文研究了将视觉-语言-动作模型应用于选择性机器人拆解任务，特别是从台式电脑中提取关键组件（如RAM和CPU）的案例研究。", "motivation": "自动化拆解报废台式电脑中的关键组件面临挑战，因为这些产品具有固有的可变性和不确定性，且拆解需要顺序、精确和灵巧的操作。当前机器人拆解流程需要分阶段显式建模，限制了在陌生场景中的泛化能力。", "method": "收集了机器人RAM和CPU拆解的自定义数据集，并用于微调两种成熟的VLA方法（OpenVLA和OpenVLA-OFT）。将整个拆解任务分为多个小步骤，并测试了纯VLA方法和VLA与基于规则控制器结合的混合策略。", "result": "微调后的VLA模型能够忠实地完成多个早期步骤，但在某些关键子任务上表现不佳，导致任务失败。然而，简单的混合策略（VLA结合基于规则控制器）能够成功执行整个拆解操作。", "conclusion": "VLA模型在处理机器人报废产品拆解所需的灵巧性和精确性方面存在局限性。混合策略显示出更好的性能，为未来研究提供了见解，以解决当前挑战并推进端到端机器人自动化拆解。"}}
{"id": "2512.04442", "pdf": "https://arxiv.org/pdf/2512.04442", "abs": "https://arxiv.org/abs/2512.04442", "authors": ["Dilani Widanapathiranage", "Scott Barnett", "Stefanus Kurniawan", "Wannita Takerngsaksiri"], "title": "TaskEval: Synthesised Evaluation for Foundation-Model Tasks", "categories": ["cs.AI", "cs.SE"], "comment": "5 pages, 3 figures", "summary": "Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \\textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \\toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\\% and 90\\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.", "AI": {"tldr": "提出TaskEval框架，通过合成评估器程序来解决基础模型任务中缺乏评估指标和数据集的问题，实现自动化评估和人工反馈的深度整合。", "motivation": "基础模型应用中幻觉问题是关键挑战，现有评估方法要么关注新评估方法，要么关注特定任务的基准数据集，但都不适用于没有现成指标或数据集的特定任务应用。需要自动化方法和人工反馈深度整合的解决方案。", "method": "提出三个核心创新：(1) 任务无关的元模型，捕获任何基础模型任务的属性；(2) 高效利用人工反馈的交互协议；(3) 评估合成器，选择或生成适当的评估集。在Toolname中实现该方法。", "result": "在两个不同的基础模型任务（图表数据提取和文档问答）上进行了概念验证。初步评估显示所选评估的准确率分别为93%和90%。", "conclusion": "TaskEval解决了工程团队面临的一个日益增长的问题：如何评估和审查基础模型任务的输出，为缺乏现成评估指标和数据集的任务提供了实用的评估解决方案。"}}
{"id": "2512.04441", "pdf": "https://arxiv.org/pdf/2512.04441", "abs": "https://arxiv.org/abs/2512.04441", "authors": ["Bin Suna", "Yaoguang Caob", "Yan Wanga", "Rui Wanga", "Jiachen Shanga", "Xiejie Fenga", "Jiayi Lu", "Jia Shi", "Shichun Yang", "Xiaoyu Yane", "Ziying Song"], "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.", "AI": {"tldr": "MindDrive是一个统一的端到端自动驾驶框架，通过结合世界模型和视觉语言模型，实现了高质量轨迹生成与全面决策推理的融合。", "motivation": "现有端到端自动驾驶研究存在两个方向：轨迹生成导向的方法侧重于生成高质量轨迹但决策机制简单；轨迹选择导向的方法能进行多维度评估但生成能力不足。需要一种能同时具备高质量轨迹生成和全面决策推理能力的统一框架。", "method": "提出MindDrive框架，采用\"上下文模拟-候选生成-多目标权衡\"的结构化推理范式。包括：1) 基于世界动作模型(WaM)的未来感知轨迹生成器(FaTG)，进行自我条件化的\"假设\"模拟预测未来场景；2) 基于视觉语言模型(VLM)的评估器(VLoE)，利用大语言模型的推理能力对轨迹进行安全、舒适、效率等多维度评估。", "result": "在NAVSIM-v1和NAVSIM-v2基准测试上的大量实验表明，MindDrive在多维度驾驶指标上实现了最先进的性能，显著提升了安全性、合规性和泛化能力。", "conclusion": "MindDrive为可解释和认知引导的自动驾驶提供了一条有前景的路径，通过整合世界模型和视觉语言模型，实现了高质量轨迹生成与人类对齐决策的和谐统一。"}}
{"id": "2512.04425", "pdf": "https://arxiv.org/pdf/2512.04425", "abs": "https://arxiv.org/abs/2512.04425", "authors": ["Manar Alnaasan", "Md Selim Sarowar", "Sungho Kim"], "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM", "AI": {"tldr": "提出一个可解释的多模态帕金森病步态识别框架，融合RGB和深度数据，结合大语言模型提供临床可理解的解释", "motivation": "现有帕金森病步态分析方法存在单模态输入、鲁棒性差和缺乏临床透明度等限制，需要更准确、稳健且可解释的解决方案", "method": "使用双YOLOv11编码器提取多模态特征，通过多尺度局部-全局提取模块和跨空间融合机制增强时空表示，结合冻结的大语言模型生成临床解释", "result": "在多模态步态数据集上验证，相比单模态基线，该RGB-D融合框架实现了更高的识别准确率、更好的环境变化鲁棒性，并能提供清晰的视觉-语言推理", "conclusion": "通过结合多模态特征学习和基于语言的可解释性，该研究弥合了视觉识别与临床理解之间的差距，为可靠且可解释的帕金森病步态分析提供了新的视觉-语言范式"}}
{"id": "2512.04419", "pdf": "https://arxiv.org/pdf/2512.04419", "abs": "https://arxiv.org/abs/2512.04419", "authors": ["Weiwei Wang", "Weijie Zou", "Jiyong Min"], "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions", "categories": ["cs.AI"], "comment": null, "summary": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks. We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects. Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases. The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.", "AI": {"tldr": "本文针对LLM在生产环境中出现的重复生成问题，提出了全面的理论分析和多种实用解决方案，重点研究了批处理代码解释任务中的三种重复模式及其解决方法。", "motivation": "大型语言模型在生产部署中出现的重复生成问题会导致严重的性能下降和系统停滞，特别是在批处理代码解释任务中，重复问题会严重影响实际应用效果和系统稳定性。", "method": "基于马尔可夫模型进行理论分析，识别出三种重复模式：业务规则生成重复、方法调用关系分析重复和PlantUML图语法生成重复。实验评估了三种解决方案：Beam Search解码（early_stopping=True）、presence_penalty超参数调整和直接偏好优化（DPO）微调。", "result": "实验表明：1) Beam Search解码（early_stopping=True）能有效解决所有三种重复模式；2) presence_penalty超参数专门有效解决第一种重复模式；3) DPO微调为所有三种重复模式提供了模型层面的通用解决方案。", "conclusion": "本文结合生产经验和实验验证，提供了系统性的重复机制理论分析、多种解决方案的全面评估、识别了early_stopping作为Beam Search有效性的关键参数，并提供了经过实际部署环境验证的生产就绪解决方案。"}}
{"id": "2512.04416", "pdf": "https://arxiv.org/pdf/2512.04416", "abs": "https://arxiv.org/abs/2512.04416", "authors": ["Zhou Liu", "Zhaoyang Han", "Guochen Yan", "Hao Liang", "Bohan Zeng", "Xing Chen", "Yuanfeng Song", "Wentao Zhang"], "title": "GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows", "categories": ["cs.AI", "cs.SE"], "comment": "Equal contribution: Zhou Liu and Zhaoyang Han. Corresponding authors: Yuanfeng Song and Wentao Zhang", "summary": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.", "AI": {"tldr": "提出GovBench基准测试，评估LLM在真实世界数据治理工作流中的表现，并开发DataGovAgent框架来提升性能", "motivation": "现有基准测试主要关注代码片段或高级分析，无法捕捉数据治理的核心挑战——确保数据本身的正确性和质量，需要专门基准来评估LLM在真实数据治理工作流中的能力", "method": "1) 构建GovBench基准：包含150个基于真实场景的多样化任务，采用\"反向目标\"方法合成真实噪声；2) 开发DataGovAgent框架：采用Planner-Executor-Evaluator架构，集成基于约束的规划、检索增强生成和沙盒反馈驱动调试", "result": "当前模型在复杂多步骤工作流中表现不佳，缺乏稳健的错误纠正机制。DataGovAgent将复杂任务的平均任务分数从39.7提升到54.9，调试迭代次数减少超过77.9%", "conclusion": "GovBench填补了数据治理基准测试的空白，DataGovAgent框架通过系统化架构显著提升了LLM在数据治理工作流中的可靠性和效率，为自动化数据治理提供了有效解决方案"}}
{"id": "2512.04415", "pdf": "https://arxiv.org/pdf/2512.04415", "abs": "https://arxiv.org/abs/2512.04415", "authors": ["Zhoufeng Wang", "Hang Zhao", "Juzhan Xu", "Shishun Zhang", "Zeyu Xiong", "Ruizhen Hu", "Chenyang Zhu", "Kai Xu"], "title": "RoboBPP: Benchmarking Robotic Online Bin Packing with Physics-based Simulation", "categories": ["cs.RO"], "comment": "Under review at the International Journal of Robotics Research (IJRR)", "summary": "Physical feasibility in 3D bin packing is a key requirement in modern industrial logistics and robotic automation. With the growing adoption of industrial automation, online bin packing has gained increasing attention. However, inconsistencies in problem settings, test datasets, and evaluation metrics have hindered progress in the field, and there is a lack of a comprehensive benchmarking system. Direct testing on real hardware is costly, and building a realistic simulation environment is also challenging. To address these limitations, we introduce RoboBPP, a benchmarking system designed for robotic online bin packing. RoboBPP integrates a physics-based simulator to assess physical feasibility. In our simulation environment, we introduce a robotic arm and boxes at real-world scales to replicate real industrial packing workflows. By simulating conditions that arise in real industrial applications, we ensure that evaluated algorithms are practically deployable. In addition, prior studies often rely on synthetic datasets whose distributions differ from real-world industrial data. To address this issue, we collect three datasets from real industrial workflows, including assembly-line production, logistics packing, and furniture manufacturing. The benchmark comprises three carefully designed test settings and extends existing evaluation metrics with new metrics for structural stability and operational safety. We design a scoring system and derive a range of insights from the evaluation results. RoboBPP is fully open-source and is equipped with visualization tools and an online leaderboard, providing a reproducible and extensible foundation for future research and industrial applications (https://robot-bin-packing-benchmark.github.io).", "AI": {"tldr": "提出了RoboBPP基准测试系统，用于评估机器人在线装箱问题的物理可行性，通过物理仿真模拟真实工业场景", "motivation": "当前机器人装箱领域存在测试设置不一致、数据集不统一、评估指标不全面等问题，缺乏统一的基准测试系统，且真实硬件测试成本高，仿真环境构建困难", "method": "开发了RoboBPP系统，集成了物理仿真器评估物理可行性，模拟真实工业尺度的机械臂和箱子，收集了三种真实工业数据集，设计了三种测试设置并扩展了评估指标", "result": "建立了完整的基准测试框架，包括可视化工具和在线排行榜，提供了可复现和可扩展的研究基础，能够评估算法的结构稳定性和操作安全性", "conclusion": "RoboBPP为机器人在线装箱领域提供了首个全面的基准测试系统，解决了现有研究的不一致性问题，促进了该领域的研究和工业应用发展"}}
{"id": "2512.04413", "pdf": "https://arxiv.org/pdf/2512.04413", "abs": "https://arxiv.org/abs/2512.04413", "authors": ["Xiangyi Gao", "Danpei Zhao", "Bo Yuan", "Wentao Li"], "title": "Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 8 figures, 11 tables", "summary": "Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.", "AI": {"tldr": "提出一种用于遥感目标检测的双流谱解耦蒸馏方法，解决遥感图像中特征混合和细微特征差异导致的知识混淆问题", "motivation": "现有蒸馏方法在遥感图像中常遇到特征混合问题，且忽视细微特征变化引起的差异，导致知识混淆。遥感图像中密集小目标检测的挑战也需要专门处理", "method": "提出DS2D2方法，基于谱分解整合显式和隐式蒸馏。首先使用一阶小波变换进行谱分解以保留关键空间特征，设计密度无关尺度权重处理密集小目标。其次通过全频和高频放大器提取隐藏在师生特征差异中的隐式知识", "result": "在DIOR和DOTA数据集上的实验验证了方法的有效性。在DIOR数据集上，RetinaNet的AP50提升4.2%，Faster R-CNN的AP50提升3.8%，优于现有蒸馏方法", "conclusion": "DS2D2是一种架构无关的通用遥感目标检测蒸馏方法，能有效解决特征混合和细微差异问题，显著提升检测性能"}}
{"id": "2512.04399", "pdf": "https://arxiv.org/pdf/2512.04399", "abs": "https://arxiv.org/abs/2512.04399", "authors": ["Haoqi Han", "Yi Yang", "Yifei Yu", "Yixuan Zhou", "Xiaohan Zhu", "Hesheng Wang"], "title": "Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation", "categories": ["cs.RO"], "comment": null, "summary": "In robotic hand research, minimizing the number of actuators while maintaining human-hand-consistent dimensions and degrees of freedom constitutes a fundamental challenge. Drawing bio-inspiration from human hand kinematic configurations and muscle distribution strategies, this work proposes a novel 15-DoF dexterous robotic hand, with detailed analysis of its mechanical architecture, electrical system, and control system. The bionic hand employs a new tendon-driven mechanism, significantly reducing the number of motors required by traditional tendon-driven systems while enhancing motion performance and simplifying the mechanical structure. This design integrates five motors in the forearm to provide strong gripping force, while ten small motors are installed in the palm to support fine manipulation tasks. Additionally, a corresponding joint sensing and motor driving electrical system was developed to ensure efficient control and feedback. The entire system weighs only 1.4kg, combining lightweight and high-performance features. Through experiments, the bionic hand exhibited exceptional dexterity and robust grasping capabilities, demonstrating significant potential for robotic manipulation tasks.", "AI": {"tldr": "开发了一种具有15个自由度的仿生手，采用线缆驱动传输和分布式驱动设计，在保持人手尺寸和自由度的同时显著减少了执行器数量。", "motivation": "在机器人手研究中，如何在保持人手尺寸和自由度一致的同时最小化执行器数量是一个基本挑战。传统线缆驱动系统需要大量电机，导致结构复杂、重量大。", "method": "受人手运动学配置和肌肉分布策略的启发，提出了一种新型15自由度灵巧机器人手。采用新的肌腱驱动机制，将5个强力电机安装在手臂提供抓握力，10个小电机安装在手掌支持精细操作任务，并开发了相应的关节传感和电机驱动电气系统。", "result": "整个系统仅重1.4kg，结合了轻量化和高性能特点。实验表明，该仿生手表现出卓越的灵巧性和强大的抓取能力，在机器人操作任务中显示出显著潜力。", "conclusion": "该研究成功开发了一种创新的仿生手设计，通过分布式驱动策略在减少电机数量的同时保持了高性能，为机器人操作任务提供了有前景的解决方案。"}}
{"id": "2512.04398", "pdf": "https://arxiv.org/pdf/2512.04398", "abs": "https://arxiv.org/abs/2512.04398", "authors": ["E. Ch'ng"], "title": "What is Beyond Presence? Dimensionality, Control, and Information Spaces", "categories": ["cs.HC", "cs.MM"], "comment": "38 pages, accepted for Presence: Virtual and Augmented Reality 2026(37)", "summary": "What is after presence? Spatial presence, the sense of \"being there\", is becoming less of a primary objective and more of a baseline expectation of virtual reality. More than six decades after its invention, VR is shifting from a technical system into a cultural, social, and phenomenological medium, offering experiences that function as distinct modes of reality. Existing theories that focus primarily on perceptual illusions are no longer sufficient to account for these emerging forms of experience. A new framework is needed to guide the design and evaluation of immersive environments by identifying the key technical and abstract dimensions afforded by virtual worlds. These dimensions include spatial, placeness, temporal, social, cultural, cognitive, and psychological parameters. The central argument is that immersive environments must move beyond the technical dimension to leverage richer information channels that shape user experience. This shift from presence to experience orchestration invites creators across disciplines to contribute to the design and assessment of meaningful immersive worlds.", "AI": {"tldr": "该论文提出虚拟现实需要超越\"临场感\"的概念，转向更丰富的体验维度框架，以适应VR从技术系统向文化、社会和现象学媒介的转变。", "motivation": "随着VR技术发展，空间临场感已成为基本预期而非主要目标，现有基于感知幻觉的理论已不足以解释新兴的虚拟体验形式，需要新的框架来指导沉浸式环境的设计和评估。", "method": "提出一个新的多维框架，识别虚拟世界提供的技术和抽象维度，包括空间、场所感、时间、社会、文化、认知和心理参数，强调从临场感到体验编排的转变。", "result": "建立了超越传统临场感概念的综合性维度框架，为沉浸式环境设计提供了新的理论基础，强调利用更丰富的信息渠道来塑造用户体验。", "conclusion": "VR需要从技术维度转向更丰富的体验维度，通过体验编排让跨学科创作者参与设计和评估有意义的沉浸式世界，实现从临场感到体验设计的范式转变。"}}
{"id": "2512.04397", "pdf": "https://arxiv.org/pdf/2512.04397", "abs": "https://arxiv.org/abs/2512.04397", "authors": ["Zeeshan Ahmad", "Shudi Bao", "Meng Chen"], "title": "Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection", "categories": ["cs.CV"], "comment": ":J.3; I.5.4", "summary": "Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.", "AI": {"tldr": "评估基于迁移学习的医学图像分类技术在疾病检测中的性能表现，通过对比六种预训练模型在胸部X光数据集上的表现，分析不同模型架构对分类效果的影响。", "motivation": "医学图像分类在疾病检测中至关重要，但从头训练大型深度学习模型通常不可行。迁移学习技术可以重用预训练模型解决新任务，但需要评估不同预训练模型在医学图像分类中的实际效果和适用性。", "method": "使用六种预训练模型（AlexNet、VGG16、ResNet18、ResNet34、ResNet50和InceptionV3）在自定义胸部X光数据集上进行疾病检测。进行不确定性分析和运行时比较，评估模型的鲁棒性和计算效率。", "result": "InceptionV3在所有标准指标上始终优于其他模型。ResNet系列随着深度增加性能逐步提升，VGG16和AlexNet表现合理但准确率较低。迁移学习在数据有限时特别有益，但改进程度取决于模型架构、数据集大小和源任务与目标任务之间的领域相似性等因素。", "conclusion": "迁移学习在医学图像分类中具有重要价值，特别是数据有限时。InceptionV3表现最佳，ResNet系列深度增加带来性能提升。研究表明，通过良好训练的特征提取器，仅需轻量级前馈模型即可提供高效预测，为基于特定需求选择合适模型提供了指导。"}}
{"id": "2512.04395", "pdf": "https://arxiv.org/pdf/2512.04395", "abs": "https://arxiv.org/abs/2512.04395", "authors": ["Hieu Dinh Trung Pham", "Huy Minh Nhat Nguyen", "Cuong Tuan Nguyen"], "title": "Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "提出Fourier-Attentive Representation Learning (FARL)框架，通过傅里叶分析显式解耦视觉表示，增强视觉语言模型在少样本学习中的泛化能力", "motivation": "现有大规模预训练视觉语言模型通常学习整体表示，其中图像的领域不变结构与其领域特定风格隐式纠缠，这限制了模型的泛化能力，需要进一步解耦这些视觉线索", "method": "使用傅里叶分析显式解耦视觉表示，核心是双交叉注意力机制，其中可学习的表示令牌分别查询图像的结构特征（来自相位谱）和风格特征（来自幅度谱），然后通过非对称注入策略将这些解耦的令牌注入VLM编码器", "result": "在15个数据集上的广泛实验证明了该方法的有效性，能够学习更鲁棒的视觉-语言对齐", "conclusion": "FARL框架通过傅里叶引导的解耦表示学习，显著提升了视觉语言模型在少样本学习任务中的泛化性能"}}
{"id": "2512.04390", "pdf": "https://arxiv.org/pdf/2512.04390", "abs": "https://arxiv.org/abs/2512.04390", "authors": ["Geunhyuk Youk", "Jihyong Oh", "Munchurl Kim"], "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 15 figures. Project Page: https://kaist-viclab.github.io/fmanetpp_site/", "summary": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.", "AI": {"tldr": "FMA-Net++是一个用于联合视频超分辨率和去模糊的框架，专门处理真实世界中运动与动态变化曝光耦合的复杂退化问题。", "motivation": "真实世界视频恢复面临运动与动态变化曝光耦合的复杂退化问题，这是先前工作大多忽视的关键挑战，也是自动曝光或低光拍摄的常见伪影。", "method": "采用序列级架构，基于分层细化与双向传播块构建，支持并行长程时序建模。每个块内包含曝光时间感知调制层，根据每帧曝光条件调整特征，驱动曝光感知的流引导动态滤波模块来推断运动和曝光感知的退化核。框架将退化学习与恢复解耦。", "result": "在REDS-ME（多曝光）和REDS-RE（随机曝光）新基准测试以及GoPro数据集上，仅使用合成数据训练的FMA-Net++实现了最先进的准确性和时序一致性，在恢复质量和推理速度方面均优于近期方法，并能很好地泛化到具有挑战性的真实世界视频。", "conclusion": "FMA-Net++通过显式建模运动与动态变化曝光的耦合效应，在真实世界视频恢复中取得了显著进展，其解耦的架构设计同时提高了准确性和效率。"}}
{"id": "2512.04385", "pdf": "https://arxiv.org/pdf/2512.04385", "abs": "https://arxiv.org/abs/2512.04385", "authors": ["Nan Zhou", "Weijie Hong", "Huandong Wang", "Jianfeng Zheng", "Qiuhua Wang", "Yali Song", "Xiao-Ping Zhang", "Yong Li", "Xinlei Chen"], "title": "STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.", "AI": {"tldr": "提出STeP-Diff模型，一种时空物理信息扩散模型，用于移动细粒度污染预测，解决移动传感器数据不完整和时序不一致的问题。", "motivation": "细粒度空气污染预测对城市管理和健康建筑发展至关重要。移动平台部署便携传感器成本低、覆盖广，但移动模式随机不可控导致数据不完整、时序不一致，需要有效方法处理这种数据并准确预测污染场。", "method": "提出STeP-Diff模型，结合DeepONet建模空间测量序列，以及PDE信息扩散模型预测时空场。通过PDE约束正则化框架，使去噪过程渐近收敛于对流扩散动力学，确保预测既基于真实测量又符合污染扩散的基本物理规律。", "result": "在两个城市部署59个自设计便携传感设备，运行14天收集数据。相比次优算法，模型在MAE提升89.12%，RMSE提升82.30%，MAPE提升25.00%。大量评估表明STeP-Diff能有效捕捉空气污染场的时空依赖性。", "conclusion": "STeP-Diff通过结合扩散模型和物理约束，成功解决了移动传感器数据不完整和时序不一致的问题，实现了准确可靠的细粒度空气污染预测，为城市环境管理提供了有效工具。"}}
{"id": "2512.04381", "pdf": "https://arxiv.org/pdf/2512.04381", "abs": "https://arxiv.org/abs/2512.04381", "authors": ["Chengyang He", "Ge Sun", "Yue Bai", "Junkai Lu", "Jiadong Zhao", "Guillaume Sartoretti"], "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination", "categories": ["cs.RO"], "comment": null, "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.", "AI": {"tldr": "FALCON是一个用于机器人移动操作（loco-manipulation）的框架，通过解耦的运动和操作视觉运动策略，结合视觉-语言基础模型作为协调器，实现导航、精确末端执行器放置和紧密的基座-手臂协调。", "motivation": "传统单一策略在处理移动操作任务时，需要融合来自运动和操作的不同类型、可能不匹配的观测数据，这会导致性能下降。需要一种能够保持子系统协调性的解耦方法。", "method": "1) 将运动和操作解耦为两个专门的视觉运动策略；2) 使用视觉-语言基础模型作为协调器，将全局观测和语言指令编码为共享潜在嵌入；3) 引入阶段进度头，使用任务阶段文本描述推断离散阶段和连续进度；4) 采用协调感知对比损失，显式编码手臂和基座动作之间的跨子系统兼容性。", "result": "在两个需要导航、精确末端执行器放置和紧密基座-手臂协调的移动操作任务上，FALCON超越了集中式和去中心化基线方法，同时表现出对分布外场景的改进鲁棒性和泛化能力。", "conclusion": "FALCON通过解耦策略和基础模型协调的方法，有效解决了移动操作任务中的观测融合问题，实现了更好的性能、鲁棒性和泛化能力，为复杂机器人任务提供了一种有前景的框架。"}}
{"id": "2512.04373", "pdf": "https://arxiv.org/pdf/2512.04373", "abs": "https://arxiv.org/abs/2512.04373", "authors": ["Hann Woei Ho", "Ye Zhou"], "title": "Vertical Planetary Landing on Sloped Terrain Using Optical Flow Divergence Estimates", "categories": ["cs.RO"], "comment": "This paper is accepted at International Astronautical Congress (IAC 2025)", "summary": "Autonomous landing on sloped terrain poses significant challenges for small, lightweight spacecraft, such as rotorcraft and landers. These vehicles have limited processing capability and payload capacity, which makes advanced deep learning methods and heavy sensors impractical. Flying insects, such as bees, achieve remarkable landings with minimal neural and sensory resources, relying heavily on optical flow. By regulating flow divergence, a measure of vertical velocity divided by height, they perform smooth landings in which velocity and height decay exponentially together. However, adapting this bio-inspired strategy for spacecraft landings on sloped terrain presents two key challenges: global flow-divergence estimates obscure terrain inclination, and the nonlinear nature of divergence-based control can lead to instability when using conventional controllers. This paper proposes a nonlinear control strategy that leverages two distinct local flow divergence estimates to regulate both thrust and attitude during vertical landings. The control law is formulated based on Incremental Nonlinear Dynamic Inversion to handle the nonlinear flow divergence. The thrust control ensures a smooth vertical descent by keeping a constant average of the local flow divergence estimates, while the attitude control aligns the vehicle with the inclined surface at touchdown by exploiting their difference. The approach is evaluated in numerical simulations using a simplified 2D spacecraft model across varying slopes and divergence setpoints. Results show that regulating the average divergence yields stable landings with exponential decay of velocity and height, and using the divergence difference enables effective alignment with inclined terrain. Overall, the method offers a robust, low-resource landing strategy that enhances the feasibility of autonomous planetary missions with small spacecraft.", "AI": {"tldr": "提出一种基于光学流散度估计的非线性控制策略，用于小型航天器在斜坡地形上的垂直着陆", "motivation": "小型轻量级航天器（如旋翼机和着陆器）处理能力和有效载荷有限，无法使用先进的深度学习方法或重型传感器。受蜜蜂等飞行昆虫仅依赖光学流实现卓越着陆的启发，但将这种生物启发策略应用于斜坡地形着陆面临两个关键挑战：全局流散度估计会掩盖地形倾斜度，以及基于散度的非线性控制在使用传统控制器时可能导致不稳定", "method": "提出一种非线性控制策略，利用两个不同的局部流散度估计来调节垂直着陆期间的推力和姿态。控制律基于增量非线性动态反演来处理非线性流散度。推力控制通过保持局部流散度估计的恒定平均值确保平稳垂直下降，而姿态控制则通过利用它们的差异使飞行器在触地时与倾斜表面对齐", "result": "通过使用简化2D航天器模型在不同坡度和散度设定点下的数值模拟进行评估。结果显示，调节平均散度可产生速度和高度指数衰减的稳定着陆，而利用散度差异可实现与倾斜地形的有效对齐", "conclusion": "该方法提供了一种鲁棒、低资源的着陆策略，增强了小型航天器自主行星任务的可行性"}}
{"id": "2512.04365", "pdf": "https://arxiv.org/pdf/2512.04365", "abs": "https://arxiv.org/abs/2512.04365", "authors": ["Pushen Zuo", "Zhong Sun"], "title": "RRAM-Based Analog Matrix Computing for Massive MIMO Signal Processing: A Review", "categories": ["eess.SP", "cs.AR", "cs.ET"], "comment": null, "summary": "Resistive random-access memory (RRAM) provides an excellent platform for analog matrix computing (AMC), enabling both matrix-vector multiplication (MVM) and the solution of matrix equations through open-loop and closed-loop circuit architectures. While RRAM-based AMC has been widely explored for accelerating neural networks, its application to signal processing in massive multiple-input multiple-output (MIMO) wireless communication is rapidly emerging as a promising direction. In this Review, we summarize recent advances in applying AMC to massive MIMO, including DFT/IDFT computation for OFDM modulation and demodulation using MVM circuits; MIMO detection and precoding using MVM-based iterative algorithms; and rapid one-step solutions enabled by matrix inversion (INV) and generalized inverse (GINV) circuits. We also highlight additional opportunities, such as AMC-based compressed-sensing recovery for channel estimation and eigenvalue circuits for leakage-based precoding. Finally, we outline key challenges, including RRAM device reliability, analog circuit precision, array scalability, and data conversion bottlenecks, and discuss the opportunities for overcoming these barriers. With continued progress in device-circuit-algorithm co-design, RRAM-based AMC holds strong promise for delivering high-efficiency, high-reliability solutions to (ultra)massive MIMO signal processing in the 6G era.", "AI": {"tldr": "综述了基于RRAM的模拟矩阵计算在Massive MIMO信号处理中的应用，包括DFT/IDFT计算、MIMO检测与预编码、矩阵求逆等，并讨论了技术挑战与未来机遇。", "motivation": "Massive MIMO无线通信系统需要高效处理复杂的矩阵运算，传统数字计算面临功耗和延迟挑战。RRAM-based AMC提供了一种高能效的模拟计算解决方案，有望满足6G时代对大规模信号处理的需求。", "method": "采用RRAM器件构建模拟矩阵计算电路，包括开环电路实现矩阵-向量乘法（MVM）和闭环电路实现矩阵方程求解。应用这些电路架构进行DFT/IDFT计算、MIMO检测与预编码、矩阵求逆等信号处理任务。", "result": "RRAM-based AMC已成功应用于Massive MIMO的多个信号处理环节：OFDM调制解调的DFT/IDFT计算、基于MVM的迭代算法进行MIMO检测与预编码、以及通过矩阵求逆电路实现快速一步求解。同时展示了在信道估计和泄漏预编码等方面的潜在应用。", "conclusion": "RRAM-based AMC为Massive MIMO信号处理提供了高能效解决方案，但面临RRAM器件可靠性、模拟电路精度、阵列可扩展性和数据转换瓶颈等挑战。通过器件-电路-算法协同设计，有望在6G时代实现高效可靠的（超）大规模MIMO信号处理系统。"}}
{"id": "2512.04359", "pdf": "https://arxiv.org/pdf/2512.04359", "abs": "https://arxiv.org/abs/2512.04359", "authors": ["Hongye Cao", "Zhixin Bai", "Ziyue Peng", "Boyan Wang", "Tianpei Yang", "Jing Huo", "Yuyao Zhang", "Yang Gao"], "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.", "AI": {"tldr": "提出一种结合语义熵和词元熵的高效强化学习框架，用于提升大语言模型的推理能力，通过数据组织和算法设计缓解熵崩溃问题", "motivation": "现有基于可验证奖励的强化学习（RLVR）虽然能提升LLM推理能力，但容易发生熵崩溃，减少策略探索，限制了推理能力的进一步提升", "method": "1) 数据层面：引入语义熵引导的课程学习，按语义熵从低到高组织训练数据；2) 算法层面：采用非均匀词元处理，对影响策略探索的低熵词元施加KL正则化，并在这些词元的高协方差部分施加更强约束", "result": "在6个基准测试和3种不同参数规模的基模型上，该方法优于其他基于熵的方法，有效提升了推理能力", "conclusion": "通过联合优化数据组织和算法设计，提出的方法有效缓解了熵崩溃问题，增强了LLM的推理能力，为高效强化学习提供了新思路"}}
{"id": "2512.04356", "pdf": "https://arxiv.org/pdf/2512.04356", "abs": "https://arxiv.org/abs/2512.04356", "authors": ["Kai-Po Chang", "Wei-Yuan Cheng", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026. Project page: https://kpc0810.github.io/santa/", "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.", "AI": {"tldr": "提出SANTA框架，通过自增强对比对齐来缓解多模态大语言模型在视频描述中的物体和动作幻觉问题", "motivation": "当前多模态大语言模型在生成视频描述时存在事实不准确的问题，导致严重的幻觉现象。虽然已有工作尝试缓解静态图像的幻觉，但联合缓解动态视频中的物体和时序动作幻觉仍然是一个具有挑战性且未解决的任务", "method": "提出自增强对比对齐框架，采用幻觉自增强方案识别MLLM中的潜在幻觉，将原始描述转换为对比负样本，并开发轨迹-短语对比对齐来匹配区域物体和关系引导的动作与其对应的视觉和时序短语", "result": "SANTA在缓解物体和动作幻觉方面优于现有方法，在幻觉检测基准上表现出优越性能", "conclusion": "SANTA框架通过自增强对比对齐有效缓解了多模态大语言模型在视频描述中的物体和动作幻觉问题，提高了生成的准确性"}}
{"id": "2512.04355", "pdf": "https://arxiv.org/pdf/2512.04355", "abs": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "categories": ["cs.DC", "cs.AI", "cs.PF"], "comment": "13 pages, 6 figures, MLSys 2026 Submission", "summary": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "AI": {"tldr": "提出了gpuFLOPBench基准测试，用于评估大型语言模型在不运行代码的情况下预测CUDA内核浮点运算次数的能力", "motivation": "现代GPU软件开发需要开发者能够在运行前预测性能瓶颈，但现有LLM很少测试这种前瞻性推理能力，存在评估空白", "method": "创建包含577个CUDA内核的基准测试集，标注真实性能数据和8个执行属性，区分可分析代码与依赖编译器/运行时行为的代码", "result": "最新LLM在简单内核上表现完美，但在涉及除法、数学函数或公共子表达式等隐含FLOP时仍会出现数量级错误", "conclusion": "揭示了现有代码助手的核心限制——无法理解硬件特定的微码效应，gpuFLOPBench可作为开发能像经验丰富的GPU开发者一样严谨推理性能的LLM工具的重点测试平台"}}
{"id": "2512.04354", "pdf": "https://arxiv.org/pdf/2512.04354", "abs": "https://arxiv.org/abs/2512.04354", "authors": ["April S. Liang", "Fatemeh Amrollahi", "Yixing Jiang", "Conor K. Corbin", "Grace Y. E. Kim", "David Mui", "Trevor Crowell", "Aakash Acharya", "Sreedevi Mony", "Soumya Punnathanam", "Jack McKeown", "Margaret Smith", "Steven Lin", "Arnold Milstein", "Kevin Schulman", "Jason Hom", "Michael A. Pfeffer", "Tho D. Pham", "David Svec", "Weihan Chu", "Lisa Shieh", "Christopher Sharp", "Stephen P. Ma", "Jonathan H. Chen"], "title": "SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction", "categories": ["cs.LG", "cs.HC"], "comment": "22 pages, 5 figures", "summary": "Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.", "AI": {"tldr": "开发并评估SmartAlert系统，这是一个基于机器学习的临床决策支持系统，旨在通过预测稳定的实验室结果来减少不必要的重复检测", "motivation": "重复的实验室检测不仅增加患者负担和医疗成本，而且传统的教育、反馈干预以及一般性检测限制和电子警报效果有限，甚至可能妨碍适当的临床护理", "method": "在电子健康记录中集成机器学习驱动的临床决策支持系统，通过随机对照试验在8个急性护理单元的9270例住院患者中评估针对全血细胞计数的SmartAlert系统", "result": "SmartAlert显示后52小时内CBC检测数量显著减少（1.54 vs 1.82，p<0.01），相对减少15%的重复检测，且对次要安全结果无不良影响", "conclusion": "基于深思熟虑的实施和治理流程的机器学习驱动CDS系统能够为住院患者实验室检测提供精准指导，安全地减少不必要的重复检测"}}
{"id": "2512.04339", "pdf": "https://arxiv.org/pdf/2512.04339", "abs": "https://arxiv.org/abs/2512.04339", "authors": ["Manh Chien Vu", "Thang Le Dinh", "Manh Chien Vu", "Tran Duc Le", "Thi Lien Huong Nguyen"], "title": "A Conceptual Model for AI Adoption in Financial Decision-Making: Addressing the Unique Challenges of Small and Medium-Sized Enterprises", "categories": ["cs.AI"], "comment": "The Eighth International Econometric and Financial Conference of Vietnam - ECONVN2025, Ho Chi Minh City, Vietnam, January 13-14-15, 2025", "summary": "The adoption of artificial intelligence (AI) offers transformative potential for small and medium-sized enterprises (SMEs), particularly in enhancing financial decision-making processes. However, SMEs often face significant barriers to implementing AI technologies, including limited resources, technical expertise, and data management capabilities. This paper presents a conceptual model for the adoption of AI in financial decision-making for SMEs. The proposed model addresses key challenges faced by SMEs, including limited resources, technical expertise, and data management capabilities. The model is structured into layers: data sources, data processing and integration, AI model deployment, decision support and automation, and validation and risk management. By implementing AI incrementally, SMEs can optimize financial forecasting, budgeting, investment strategies, and risk management. This paper highlights the importance of data quality and continuous model validation, providing a practical roadmap for SMEs to integrate AI into their financial operations. The study concludes with implications for SMEs adopting AI-driven financial processes and suggests areas for future research in AI applications for SME finance.", "AI": {"tldr": "提出一个针对中小企业财务决策中AI采用的概念模型，解决中小企业特有的资源、技术和数据管理挑战", "motivation": "AI对中小企业财务决策具有变革潜力，但中小企业面临资源有限、技术专业知识不足、数据管理能力薄弱等显著障碍，需要专门针对其特点的AI采用模型", "method": "提出分层概念模型：数据源层、数据处理与集成层、AI模型部署层、决策支持与自动化层、验证与风险管理层，强调渐进式实施", "result": "模型帮助中小企业优化财务预测、预算编制、投资策略和风险管理，强调数据质量和持续模型验证的重要性", "conclusion": "为中小企业整合AI到财务运营提供实用路线图，指出AI驱动财务流程对中小企业的意义，并建议未来研究方向"}}
{"id": "2512.04334", "pdf": "https://arxiv.org/pdf/2512.04334", "abs": "https://arxiv.org/abs/2512.04334", "authors": ["Chengke Liu", "Wei Xu"], "title": "Human-controllable AI: Meaningful Human Control", "categories": ["cs.HC"], "comment": "52 pages", "summary": "Developing human-controllable artificial intelligence (AI) and achieving meaningful human control (MHC) has become a vital principle to address these challenges, ensuring ethical alignment and effective governance in AI. MHC is also a critical focus in human-centered AI (HCAI) research and application. This chapter systematically examines MHC in AI, articulating its foundational principles and future trajectory. MHC is not simply the right to operate, but the unity of human understanding, intervention, and the traceablity of responsibility in AI decision-making, which requires technological design, AI governance, and humans to play a role together. MHC ensures AI autonomy serves humans without constraining technological progress. The mode of human control needs to match the levels of technology, and human supervision should balance the trust and doubt of AI. For future AI systems, MHC mandates human controllability as a prerequisite, requiring: (1) technical architectures with embedded mechanisms for human control; (2) human-AI interactions optimized for better access to human understanding; and (3) the evolution of AI systems harmonizing intelligence and human controllability. Governance must prioritize HCAI strategies: policies balancing innovation and risk mitigation, human-centered participatory frameworks transcending technical elite dominance, and global promotion of MHC as a universal governance paradigm to safeguard HCAI development. Looking ahead, there is a need to strengthen interdisciplinary research on the controllability of AI systems, enhance ethical and legal awareness among stakeholders, moving beyond simplistic technology design perspectives, focus on the knowledge construction, complexity interpretation, and influencing factors surrounding human control. By fostering MHC, the development of human-controllable AI can be further advanced, delivering HCAI systems.", "AI": {"tldr": "探讨人工智能中的人类可控性及有意义人类控制（MHC）原则，确保AI系统在伦理对齐和有效治理下发展，实现人类中心AI的目标。", "motivation": "随着AI系统日益自主和复杂，确保人类对AI的有效控制成为关键挑战。需要建立有意义的人类控制原则来应对AI伦理对齐、责任追溯和有效治理等问题，防止AI失控并确保其服务于人类利益。", "method": "系统性地分析MHC在AI中的理论基础和实施框架，提出技术设计、人机交互和系统演化的综合方法。强调技术架构中嵌入人类控制机制、优化人机交互以增强人类理解、以及协调AI智能与人类可控性的系统演化。", "result": "明确了MHC不仅是操作权利，而是人类理解、干预和责任追溯的统一体。提出了实现MHC需要技术设计、AI治理和人类角色协同作用的框架，以及未来AI系统必须满足的人类可控性前提条件。", "conclusion": "有意义人类控制是发展人类可控AI的关键原则，需要跨学科研究、增强利益相关者的伦理法律意识，超越单纯技术设计视角，关注人类控制的知识建构、复杂性解释和影响因素。通过促进MHC，可以推进人类中心AI系统的发展。"}}
{"id": "2512.04331", "pdf": "https://arxiv.org/pdf/2512.04331", "abs": "https://arxiv.org/abs/2512.04331", "authors": ["Zhongyi Cai", "Bryce Gernon", "Wentao Bao", "Yifan Li", "Matthew Wright", "Yu Kong"], "title": "Open Set Face Forgery Detection via Dual-Level Evidence Collection", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.", "AI": {"tldr": "提出一种用于开放集人脸伪造检测的双层证据收集方法，能够识别新型伪造类别", "motivation": "现有的人脸伪造检测方法通常仅限于二元真伪分类或已知伪造类别的识别，无法检测新型伪造类型的出现。随着人脸伪造生成算法的快速发展，新型伪造类别不断出现，这对现有检测方法构成了重大挑战。", "method": "提出双层证据人脸伪造检测方法，通过在空间和频率两个层次上收集和融合类别特定的证据来估计预测不确定性，从而解决开放集人脸伪造检测问题。", "result": "在多种实验设置下的广泛评估表明，所提出的DLED方法在检测新型伪造类别方面平均优于各种基线模型20%，同时在传统的真伪人脸伪造检测任务上也表现出竞争力。", "conclusion": "该方法通过不确定性估计有效解决了开放集人脸伪造检测问题，能够识别新型伪造类别，在实际场景中具有更好的适用性。"}}
{"id": "2512.04322", "pdf": "https://arxiv.org/pdf/2512.04322", "abs": "https://arxiv.org/abs/2512.04322", "authors": ["Om Tailor"], "title": "Quantum Chemistry Simulation of Dibenzothiophene for Asphalt Aging Analysis", "categories": ["physics.chem-ph", "cs.ET"], "comment": "Runner-up Submission to QWC MITRE Industry Challenge", "summary": "This paper presents the execution and analysis of a comprehensive quantum chemistry pipeline for gathering actionable insight into asphalt aging mechanisms through the study of dibenzothiophene (DBT), a key sulfur-containing compound in asphalt binders. Using advanced quantum algorithms, specifically Variational Quantum Eigensolver (VQE) with k-UpCCGSD and ADAPT-VQE ansatze, we achieved ground state energy calculations with accuracies reaching -864.69 Ha. Our implementation demonstrates quantum advantages in handling strongly correlated electron systems while providing actionable insights for designing oxidation-resistant asphalt formulations. The work also establishes a scalable framework for quantum-enhanced materials design.", "AI": {"tldr": "使用量子化学方法模拟二苯并噻吩以分析沥青老化机制", "motivation": "沥青老化是道路工程中的重要问题，二苯并噻吩作为沥青中的关键含硫化合物，其氧化行为影响沥青性能。传统计算方法难以准确处理强关联电子系统，需要量子计算提供更精确的模拟。", "method": "采用变分量子本征求解器（VQE）结合k-UpCCGSD和ADAPT-VQE ansatze等先进量子算法，构建量子化学管道进行基态能量计算", "result": "实现了-864.69 Ha的高精度基态能量计算，展示了量子计算在处理强关联电子系统方面的优势，为设计抗氧化沥青配方提供了可行见解", "conclusion": "该研究建立了可扩展的量子增强材料设计框架，证明了量子化学模拟在沥青老化分析中的实用价值，为道路材料优化提供了新方法"}}
{"id": "2512.04319", "pdf": "https://arxiv.org/pdf/2512.04319", "abs": "https://arxiv.org/abs/2512.04319", "authors": ["Zixiao Zhao", "Fatemeh H. Fard", "Jie JW Wu"], "title": "MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.", "AI": {"tldr": "提出MANTRA框架，用于在代码预训练语言模型和代码LLM微调过程中进行多阶段自适应噪声处理，通过噪声诊断和缓解提升模型在软件工程任务中的性能", "motivation": "软件工程任务中大规模存储库不可避免地引入噪声或错误标记的示例，这会降低深度学习模型的准确性和鲁棒性。虽然噪声标签学习在其他领域已有广泛研究，但在软件工程和LLM应用于SE任务方面的研究较少", "method": "提出多阶段自适应噪声处理框架，首先研究不同噪声水平对模型收敛和损失轨迹的影响，然后应用基于样本损失动态和高斯混合模型聚类的自适应dropout策略，排除持续噪声点同时保留干净数据", "result": "实验应用于代码摘要和提交意图分类任务，发现某些LLM对噪声更敏感，但使用MANTRA后，所有模型在两个任务中的性能都得到提升", "conclusion": "MANTRA框架使研究人员和从业者能够减少训练中数据集引入的错误影响，节省数据清洗和处理时间，同时最大化微调效果"}}
{"id": "2512.04316", "pdf": "https://arxiv.org/pdf/2512.04316", "abs": "https://arxiv.org/abs/2512.04316", "authors": ["Haoze Guo"], "title": "ConsentDiff at Scale: Longitudinal Audits of Web Privacy Policy Changes and UI Frictions", "categories": ["cs.HC"], "comment": null, "summary": "Web privacy is experienced via two public artifacts: site utterances in policy texts, and the actions users are required to take during consent interfaces. In the extensive cross-section audits we've studied, there is a lack of longitudinal data detailing how these artifacts are changing together, and if interfaces are actually doing what they promise in policy. ConsentDiff provides that longitudinal view. We build a reproducible pipeline that snapshots sites every month, semantically aligns policy clauses to track clause-level churn, and classifies consent-UI patterns by pulling together DOM signals with cues provided by screenshots. We introduce a novel weighted claim-UI alignment score, connecting common policy claims to observable predicates, and enabling comparisons over time, regions, and verticals. Our measurements suggest continued policy churn, systematic changes to eliminate a higher-friction banner design, and significantly higher alignment where rejecting is visible and lower friction.", "AI": {"tldr": "开发了一个名为ConsentDiff的系统，用于对网站隐私政策和同意界面进行纵向审计，追踪它们随时间的变化以及两者之间的一致性", "motivation": "现有研究缺乏关于网站隐私政策文本和同意界面如何随时间共同变化的纵向数据，以及界面是否真正履行了政策承诺", "method": "构建了一个可重复的管道，每月对网站进行快照，通过语义对齐政策条款追踪条款级变化，结合DOM信号和截图线索对同意界面模式进行分类，并引入了加权声明-界面对齐分数", "result": "测量结果显示：政策条款持续变化，系统性地消除了高摩擦的横幅设计，在拒绝选项可见且摩擦较低的情况下对齐度显著更高", "conclusion": "ConsentDiff提供了对网站隐私政策和同意界面变化的纵向视角，揭示了政策与实践之间的关系，为隐私合规审计提供了新工具"}}
{"id": "2512.04315", "pdf": "https://arxiv.org/pdf/2512.04315", "abs": "https://arxiv.org/abs/2512.04315", "authors": ["Yonghan Lee", "Tsung-Wei Huang", "Shiv Gehlot", "Jaehoon Choi", "Guan-Ming Su", "Dinesh Manocha"], "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.", "AI": {"tldr": "提出SyncTrack4D方法，用于处理非同步多视频的4D高斯泼溅重建，通过跨视频运动对齐和视频同步实现高保真4D重建", "motivation": "动态3D场景建模具有挑战性，需要从多视角聚合信息来重建随时间演变的3D几何和运动。现有方法通常假设视频已同步或需要预定义场景对象，难以处理现实世界中的非同步视频集", "method": "1) 计算密集的每视频4D特征轨迹和跨视频轨迹对应关系（使用融合Gromov-Wasserstein最优传输方法）；2) 执行全局帧级时间对齐以最大化匹配4D轨迹的重叠运动；3) 通过基于运动样条支架表示的多视频4D高斯泼溅实现子帧同步", "result": "在Panoptic Studio和SyncNeRF Blender数据集上评估，平均时间误差低于0.26帧，达到子帧同步精度；在Panoptic Studio数据集上获得26.3 PSNR的高保真4D重建分数", "conclusion": "SyncTrack4D是首个通用的非同步视频集4D高斯泼溅方法，无需假设预定义场景对象或先验模型，能够处理现实世界的非同步视频集，实现精确的视频同步和高保真4D重建"}}
{"id": "2512.04314", "pdf": "https://arxiv.org/pdf/2512.04314", "abs": "https://arxiv.org/abs/2512.04314", "authors": ["Jiashu Liao", "Pietro Liò", "Marc de Kamps", "Duygu Sarikaya"], "title": "DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.", "AI": {"tldr": "提出DisentangleFormer架构，通过空间-通道解耦实现多通道视觉表示，解决传统Vision Transformers中空间和通道维度纠缠的问题", "motivation": "传统Vision Transformers的自注意力机制联合处理空间和通道维度，导致表示纠缠，无法独立建模结构和语义依赖关系。这个问题在高光谱成像中尤为突出，因为不同通道捕获不同的生物物理或生化线索", "method": "基于信息论的去相关表示学习原理，采用并行设计：1) 并行解耦：独立处理空间token和通道token流；2) 压缩token增强器：动态融合空间和通道流的自适应校准模块；3) 多尺度FFN：补充全局注意力，捕获细粒度结构和语义依赖", "result": "在高光谱基准测试（Indian Pine、Pavia University、Houston、BigEarthNet遥感数据集和红外病理数据集）上达到最先进性能，同时在ImageNet上保持竞争性精度，计算成本降低17.8% FLOPs", "conclusion": "DisentangleFormer通过空间-通道解耦实现了鲁棒的多通道视觉表示，在高光谱成像等应用中表现出色，同时提高了计算效率"}}
{"id": "2512.04313", "pdf": "https://arxiv.org/pdf/2512.04313", "abs": "https://arxiv.org/abs/2512.04313", "authors": ["Haolin Xiong", "Tianwen Fu", "Pratusha Bhuvana Prasad", "Yunxuan Cai", "Haiwei Chen", "Wenbin Teng", "Hanyuan Xiao", "Yajie Zhao"], "title": "Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding", "categories": ["cs.CV"], "comment": "16 pages, 11 figures", "summary": "Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.", "AI": {"tldr": "从EEG脑电信号直接解码生成高保真面部表情的神经驱动头像合成框架", "motivation": "现有表情头像系统过度依赖视觉线索，当面部被遮挡或情绪内隐时失效，需要从神经信号直接解码面部表情的新方法", "method": "构建双模态记录装置同步采集EEG和多视角面部视频，使用CNN-Transformer编码器将EEG信号映射到密集3D位置图，通过改进的3D高斯溅射渲染管道生成照片级真实感结果", "result": "EEG信号能够可靠预测动态、个性化的面部表情，包括细微的情绪反应，证明神经信号包含比先前假设更丰富的情感和几何信息", "conclusion": "Mind-to-Face建立了神经驱动头像的新范式，为个性化、情感感知的远程呈现和沉浸式环境中的认知交互提供了可能"}}
{"id": "2512.04311", "pdf": "https://arxiv.org/pdf/2512.04311", "abs": "https://arxiv.org/abs/2512.04311", "authors": ["Juan Manuel Cantarero Angulo", "Matthew Smith"], "title": "Real-time Cricket Sorting By Sex", "categories": ["cs.CV", "q-bio.QM"], "comment": "13 pages, 14 figures", "summary": "The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.", "AI": {"tldr": "开发了一个基于计算机视觉和物理执行的低成本实时蟋蟀性别分选系统，用于提高蟋蟀养殖效率", "motivation": "全球对可持续蛋白质来源的需求推动了对食用昆虫的兴趣，蟋蟀是适合工业化生产的物种。当前养殖实践通常混合性别饲养，缺乏自动化性别分选，而选择性育种、优化繁殖比例和营养分化等方面存在潜在需求", "method": "结合计算机视觉和物理执行，使用树莓派5和官方AI摄像头，配合定制的YOLOv8 nano目标检测模型以及伺服驱动的分选臂", "result": "模型测试中达到mAP@0.5为0.977，真实世界蟋蟀群体实验总体分选准确率达到86.8%", "conclusion": "证明了在资源受限设备上部署轻量级深度学习模型用于昆虫养殖应用的可行性，为蟋蟀生产提供了提高效率和可持续性的实用解决方案"}}
{"id": "2512.04309", "pdf": "https://arxiv.org/pdf/2512.04309", "abs": "https://arxiv.org/abs/2512.04309", "authors": ["Rui Fonseca", "Bruno Martins", "Gil Rocha"], "title": "Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction", "categories": ["cs.CV", "cs.CL"], "comment": "Submitted to CVPR 2026", "summary": "Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.", "AI": {"tldr": "提出TOMCap方法，一种改进的仅文本训练方法，用于图像描述生成，无需对齐的图像-文本对进行训练", "motivation": "减少对人工标注数据的依赖，探索无需人类标注图像-文本对的图像描述生成方法，以超越现有无监督方法的性能", "method": "基于预训练语言模型解码器，使用经过模态间隙缩减处理的CLIP表示进行提示，结合检索到的描述示例和潜在向量表示来引导生成过程", "result": "TOMCap在广泛实验中优于其他免训练和仅文本方法，并分析了检索增强和模态间隙缩减组件不同配置选择的影响", "conclusion": "TOMCap是一种有效的仅文本训练图像描述生成方法，通过检索增强和模态间隙校正技术，在无需对齐图像-文本对的情况下取得了优于现有方法的性能"}}
{"id": "2512.04305", "pdf": "https://arxiv.org/pdf/2512.04305", "abs": "https://arxiv.org/abs/2512.04305", "authors": ["Mainak Singha", "Masih Aminbeidokhti", "Paolo Casari", "Elisa Ricci", "Subhankar Roy"], "title": "How (Mis)calibrated is Your Federated CLIP and What To Do About It?", "categories": ["cs.CV"], "comment": null, "summary": "While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\\text{FL}^2\\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\\text{FL}^2\\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.", "AI": {"tldr": "研究联邦学习环境下CLIP模型的校准问题，并提出FL²oRA方法来改善联邦CLIP的校准性能", "motivation": "虽然视觉语言模型如CLIP已被广泛研究，但其校准性能（对可靠预测至关重要）关注有限。现有工作主要研究离线环境下的CLIP校准，而联邦学习环境下微调CLIP对校准的影响尚未探索", "method": "首先分析文本提示调优方法在联邦学习下的校准退化问题，评估现有训练中校准技术在不同全局聚合方法中的效果，提出FL²oRA方法——一种基于LoRA的简单方法，自然改善联邦学习中的校准性能", "result": "实验表明文本提示调优在联邦学习下会降低校准指标，现有校准技术改进有限。FL²oRA方法在多个基准测试中能持续产生良好校准的模型，减少对显式校准过程的需求", "conclusion": "联邦学习环境下CLIP校准的关键挑战不仅在于如何聚合或校准，更在于选择微调哪些组件。FL²oRA方法通过简单的LoRA基础方法有效改善校准，代码已开源"}}
{"id": "2512.04303", "pdf": "https://arxiv.org/pdf/2512.04303", "abs": "https://arxiv.org/abs/2512.04303", "authors": ["Gasser Elazab", "Maximilian Jansen", "Michael Unterreiner", "Olaf Hellwich"], "title": "Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in 3DV 2026", "summary": "Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.", "AI": {"tldr": "提出Gamma-from-Mono (GfM)方法，通过解耦全局和局部结构解决单目重建中的投影模糊问题，预测主导道路平面和残差变化，实现车辆应用的度量自监督单目几何估计", "motivation": "传统单目深度估计方法通常过度平滑道路几何细节（如颠簸、坡度、表面不规则性），丢失了运动规划和车辆稳定性所需的关键信息，需要一种能准确感知车辆3D环境的方法", "method": "GfM方法预测主导道路表面平面和残差变化，使用gamma（垂直偏差的无量纲度量）表示点相对于平面的高度与相机深度之比，基于平面视差几何，仅需相机离地高度即可通过闭合形式恢复度量深度", "result": "在KITTI和道路表面重建数据集(RSRD)上评估，GfM在近场深度和gamma估计方面达到最先进精度，同时保持竞争力的全局深度性能，轻量级8.88M参数模型能适应不同相机设置", "conclusion": "GfM是第一个在RSRD上评估的自监督单目方法，其物理可解释的表示形式适合自监督学习，无需大量标注数据，为车辆应用提供了准确的道路相对度量几何估计"}}
{"id": "2512.04302", "pdf": "https://arxiv.org/pdf/2512.04302", "abs": "https://arxiv.org/abs/2512.04302", "authors": ["Shuyuan Zhang"], "title": "Towards better dense rewards in Reinforcement Learning Applications", "categories": ["cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2505.20417", "summary": "Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.", "AI": {"tldr": "该论文探讨如何改进强化学习中的密集奖励函数设计，以解决稀疏奖励、延迟奖励和任务目标对齐等问题，从而提高智能体学习效率和探索能力。", "motivation": "传统强化学习中，稀疏、延迟或与任务目标不匹配的奖励信号会导致智能体学习困难。密集奖励函数虽然能提供更频繁的反馈，但设计不当会导致意外行为、奖励黑客攻击或低效探索。在复杂高维环境中，手工设计奖励函数尤为困难。", "method": "论文探讨了多种方法，包括逆强化学习、基于人类偏好的奖励建模、自监督学习内在奖励等。这些方法旨在从不同角度解决密集奖励设计的问题，平衡通用性、可扩展性和人类意图对齐。", "result": "论文提出了一个研究框架，旨在探索解决密集奖励函数设计中的未解决问题，增强不同强化学习应用中密集奖励构造的有效性和可靠性。", "conclusion": "密集奖励函数设计是强化学习的关键挑战，需要更系统的方法来平衡反馈频率、任务对齐和探索效率。未来的研究应关注如何更好地构建可靠、有效的密集奖励机制。"}}
{"id": "2512.04296", "pdf": "https://arxiv.org/pdf/2512.04296", "abs": "https://arxiv.org/abs/2512.04296", "authors": ["Malyaban Bal", "Abhronil Sengupta"], "title": "GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers", "categories": ["cs.LG", "cs.NE"], "comment": "Under Review", "summary": "Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.", "AI": {"tldr": "提出GRASP框架，通过分组激活共享参数化实现参数高效微调，并进一步提出StochGRASP来增强噪声鲁棒性", "motivation": "解决传统参数高效微调方法在参数效率和性能之间的权衡问题，同时考虑边缘AI硬件部署中的噪声鲁棒性需求", "method": "将D维token表示划分为K<<D个组，为每组学习共享的缩放和偏移向量；StochGRASP则学习高斯分布作为预训练权重的扰动，配合噪声感知损失函数", "result": "在GLUE和E2E NLG任务上，GRASP性能匹配或超越现有PEFT方法，参数量比LoRA和BitFit减少一个数量级；StochGRASP在不同噪声水平下始终优于确定性变体", "conclusion": "GRASP提供了一种轻量级且高效的参数微调框架，StochGRASP特别适合在噪声敏感的边缘AI硬件平台上部署，实现了参数效率与噪声鲁棒性的双重优势"}}
{"id": "2512.04287", "pdf": "https://arxiv.org/pdf/2512.04287", "abs": "https://arxiv.org/abs/2512.04287", "authors": ["Ian Miles", "Mayumi Wakimoto", "Wagner Meira Jr.", "Daniela Paula", "Daylene Ticiane", "Bruno Rosa", "Jane Biddulph", "Stelios Georgiou", "Valdir Ermida"], "title": "Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases", "categories": ["cs.AI", "q-bio.PE"], "comment": "21 pages, 1 box, 1 figure", "summary": "This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.", "AI": {"tldr": "探索人工智能在传染病地平线扫描中的应用，重点关注AI工具如何增强信号检测、数据监控、情景分析和决策支持", "motivation": "随着传染病威胁日益复杂，传统地平线扫描方法面临数据量大、信号识别困难等挑战，需要AI技术来增强公共卫生准备能力", "method": "采用文献综述方法，系统分析AI在地平线扫描中的集成应用，包括信号检测、数据监控、情景分析和决策支持等关键环节", "result": "AI能够显著提升传染病威胁的早期识别能力，增强数据监控效率，改善情景分析质量，并提供更有效的决策支持，但同时也存在实施风险", "conclusion": "AI在传染病地平线扫描中具有巨大潜力，但需要建立有效的实施策略和治理框架来应对相关风险，这对公共卫生准备具有重要意义"}}
{"id": "2512.04284", "pdf": "https://arxiv.org/pdf/2512.04284", "abs": "https://arxiv.org/abs/2512.04284", "authors": ["Sruthi Srinivasan", "Elham Shakibapour", "Rajy Rawther", "Mehdi Saeedi"], "title": "Learning Single-Image Super-Resolution in the JPEG Compressed Domain", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 4 figures, 2 tables, SEEDS Workshop, ICIP 2025", "summary": "Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.", "AI": {"tldr": "该论文提出了一种直接在JPEG压缩域进行单图像超分辨率学习的方法，通过在JPEG DCT系数上操作来避免完全解码，从而提高数据加载和训练效率。", "motivation": "深度学习模型日益复杂，数据规模不断扩大，数据加载已成为限制训练和推理速度的主要瓶颈。尽管专用深度学习硬件有所进步，但完全JPEG解码带来的计算开销仍然很大，需要提高数据加载效率。", "method": "提出一个轻量级的超分辨率流水线，直接在JPEG离散余弦变换（DCT）系数上操作，在频域进行处理，避免了完整的JPEG解码过程。", "result": "该方法实现了2.6倍的数据加载加速和2.5倍的训练加速，同时保持了与标准单图像超分辨率方法相当的视觉质量。", "conclusion": "在JPEG压缩域直接进行超分辨率学习是有效的，能够显著提高效率而不牺牲视觉质量，为图像恢复任务提供了新的高效解决方案。"}}
{"id": "2512.04282", "pdf": "https://arxiv.org/pdf/2512.04282", "abs": "https://arxiv.org/abs/2512.04282", "authors": ["Tasmiah Haque", "Srinjoy Das"], "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.", "AI": {"tldr": "提出一种用于实时视频运动传输的推理时随机细化方法，将门控循环单元-标准化流（GRU-NF）与随机采样方法结合，通过马尔可夫链蒙特卡洛步骤在推理时探索更丰富的输出空间。", "motivation": "实时视频运动传输应用（如沉浸式游戏和基于视觉的异常检测）需要准确且多样化的未来预测，以支持真实合成和在不确定性下的鲁棒下游决策。现有GRU-NF虽然能通过标准化流捕获多模态分布，但其确定性变换结构限制了表达能力。", "method": "提出门控循环单元-随机标准化流（GRU-SNF），在GRU-NF推理过程中引入马尔可夫链蒙特卡洛步骤，受随机标准化流启发，使模型能够探索更丰富的输出空间，无需重新训练即可更好地近似真实数据分布。", "result": "在基于关键点的视频运动传输管道中验证，实验表明GRU-SNF在生成多样化输出方面优于GRU-NF，且不牺牲准确性，即使在更长的预测时间范围内也能有效捕获多模态行为。", "conclusion": "通过推理时注入随机性，该方法更有效地捕获多模态行为，突显了将随机动力学与基于流的序列模型结合用于生成时间序列预测的潜力。"}}
{"id": "2512.04280", "pdf": "https://arxiv.org/pdf/2512.04280", "abs": "https://arxiv.org/abs/2512.04280", "authors": ["Tatyana Benko", "Rebecca Jones", "Lucas Tate"], "title": "A customizable inexact subgraph matching algorithm for attributed graphs", "categories": ["cs.DS"], "comment": null, "summary": "Graphs provide a natural way to represent data by encoding information about objects and the relationships between them. With the ever-increasing amount of data collected and generated, locating specific patterns of relationships between objects in a graph is often required. Given a larger graph and a smaller graph, one may wish to identify instances of the smaller query graph in the larger target graph. This task is called subgraph identification or matching. Subgraph matching is helpful in areas such as bioinformatics, binary analysis, pattern recognition, and computer vision. In these applications, datasets frequently contain noise and errors, thus exact subgraph matching algorithms do not apply. In this paper we introduce a new customizable algorithm for inexact subgraph matching. Our algorithm utilizes node and edge attributes which are often present in real-world datasets to narrow down the search space. The algorithm is flexible in the type of subgraph matching it can perform and the types of datasets it can process by its use of a modifiable graph edit distance cost function for pairing nodes. We show its effectiveness on family trees graphs and control-flow graphs.", "AI": {"tldr": "提出一种可定制的非精确子图匹配算法，用于处理带属性的图数据", "motivation": "现实世界中的图数据常包含噪声和错误，精确子图匹配算法不适用，需要能够处理非精确匹配的算法", "method": "利用节点和边属性缩小搜索空间，采用可修改的图编辑距离成本函数进行节点配对", "result": "在家族树图和控制流图上展示了算法的有效性", "conclusion": "该算法灵活适用于不同类型的子图匹配和数据集，能够处理现实世界中的非精确匹配需求"}}
{"id": "2512.04279", "pdf": "https://arxiv.org/pdf/2512.04279", "abs": "https://arxiv.org/abs/2512.04279", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies", "categories": ["cs.RO"], "comment": null, "summary": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.", "AI": {"tldr": "该论文提出了一种奖励特权世界模型蒸馏框架，通过将密集奖励教师模型的知识蒸馏到稀疏奖励学生模型中，解决自动驾驶中密集奖励与部署指标不对齐的问题。", "motivation": "在自动驾驶模拟器中，基于特权状态（如车道几何、违规、碰撞时间）的密集奖励可以稳定和加速模型强化学习，但直接在这些信号上训练的策略往往会过拟合，在稀疏目标（如路线完成和无碰撞超车）上泛化能力差。需要利用密集奖励的优势同时避免其对部署指标的不对齐问题。", "method": "提出奖励特权世界模型蒸馏的两阶段框架：1）教师DreamerV3智能体使用密集特权奖励训练；2）仅将教师的潜在动态蒸馏到学生模型中，学生仅使用稀疏任务奖励训练。师生共享相同的观察空间（语义鸟瞰图），特权信息仅通过教师奖励输入，学生不模仿教师的动作或价值估计，而是通过正则化使其世界模型匹配教师的潜在动态，策略从稀疏成功/失败信号中从头学习。", "result": "在CARLA车道跟随和超车基准测试中，稀疏奖励学生模型优于密集奖励教师模型和从头训练的稀疏奖励基线。在未见过的车道跟随路线上，奖励特权蒸馏将成功率相对密集教师提高了约23%，同时保持相当或更好的安全性。在超车任务中，学生在训练路线上保持接近完美的性能，在未见路线上成功率提高了高达27倍，车道保持能力也有所改善。", "conclusion": "研究表明，密集奖励可用于学习更丰富的动态模型，同时保持部署策略严格针对稀疏、与部署对齐的目标进行优化。该方法有效解决了密集奖励与部署指标不对齐的问题，实现了更好的泛化性能。"}}
{"id": "2512.04277", "pdf": "https://arxiv.org/pdf/2512.04277", "abs": "https://arxiv.org/abs/2512.04277", "authors": ["Prakhar Gupta", "Vaibhav Gupta"], "title": "Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.", "AI": {"tldr": "该论文研究在强化学习后训练中注入规范动作顺序信号，通过混合奖励（单元格准确性和顺序对齐奖励）来提升模型性能，即使是在随机顺序数据上微调的模型。", "motivation": "传统的强化学习后训练通常只优化单一标量目标，忽略了解决方案生成过程中的结构信息。作者探索在RL后训练期间使用规范求解器顺序的标量提示是否能够提升性能，即使是在随机顺序解决方案序列上微调的模型。", "method": "在数独问题上，先使用标准微调在随机求解顺序上训练Transformer，然后使用Group Relative Policy Optimization (GRPO)进行后训练。采用两种奖励：单元格准确性和顺序对齐奖励（当模型输出顺序与求解器顺序一致时增加）。通过固定混合组合这些信号，并使用简单的自举缩放来在初始化时平衡各组件的大小。", "result": "混合奖励通常优于仅使用单元格准确性的优化——最佳混合方案在测试准确率上显著高于仅使用随机顺序微调的模型，并且接近在求解器顺序序列上微调的模型的准确率。", "conclusion": "这些结果表明，粗糙的顺序信号可以在不修改监督数据或架构的情况下，引导RL后训练朝向求解器顺序轨迹，为RL后训练提供了新的结构信息注入方法。"}}
{"id": "2512.04273", "pdf": "https://arxiv.org/pdf/2512.04273", "abs": "https://arxiv.org/abs/2512.04273", "authors": ["Tyler Slater"], "title": "Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures", "categories": ["cs.SE", "cs.AI"], "comment": "Under review at the Journal of Systems and Software (Special Issue on Impactful Software Architecture)", "summary": "As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure \"Architectural Erosion\" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of \"Implementation Laziness,\" where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.", "AI": {"tldr": "该论文提出了首个量化分析大型语言模型在软件架构设计中产生的技术债务和模式违规的实证框架，通过比较不同LLM在六边形架构约束下实现微服务的表现，揭示了开源模型在架构合规性和代码质量方面的显著缺陷。", "motivation": "随着大型语言模型从代码补全工具转变为自主系统架构师，它们对软件长期可维护性的影响尚未得到量化。现有研究主要关注功能正确性（pass@k），但缺乏对AI合成代码中\"架构侵蚀\"和技术债务积累的测量框架。", "method": "采用比较性试点研究，使用三种最先进的模型（GPT-5.1、Claude 4.5 Sonnet和Llama 3 8B），在严格的六边形架构约束下实现标准化的图书借阅微服务。通过抽象语法树解析来量化分析架构违规和技术债务。", "result": "专有模型实现了较高的架构一致性（GPT-5.1违规率为0%），而开源模型表现出严重分歧：Llama 3的架构违规率达到80%，经常绕过接口适配器创建领域层和基础设施层之间的非法循环依赖。此外发现\"实现懒惰\"现象，开源模型生成的逻辑代码行数比专有模型少60%，省略复杂业务逻辑以满足令牌约束。", "conclusion": "在没有自动化架构检查的情况下，使用较小的开源模型进行系统脚手架会加速结构性技术债务的积累。研究强调了在AI辅助软件开发中实施自动架构检查的重要性，特别是对于开源模型的使用场景。"}}
{"id": "2512.04268", "pdf": "https://arxiv.org/pdf/2512.04268", "abs": "https://arxiv.org/abs/2512.04268", "authors": ["Shifeng Xie", "Rui Yuan", "Simone Rossi", "Thomas Hannagan"], "title": "The Initialization Determines Whether In-Context Learning Is Gradient Descent", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.", "AI": {"tldr": "研究初始化如何决定上下文学习是否等同于梯度下降，通过引入初始猜测来扩展多头线性自注意力与梯度下降的等价性理论。", "motivation": "现有研究将线性自注意力与梯度下降联系起来，但主要建立在零均值高斯先验和零初始化的简化条件下。后续研究挑战了这一简化观点，表明在多层或非线性注意力下，自注意力执行类似但不完全等同于梯度下降的优化推理。本研究旨在探索在更现实的非零高斯先验均值条件下，多头线性自注意力如何近似梯度下降。", "method": "首先扩展多头线性自注意力嵌入矩阵，引入查询的初始估计（称为初始猜测）。证明了线性回归设置中上下文学习所需头数的上界。实验验证结果后，观察到一步梯度下降与多头线性自注意力之间存在性能差距。为解决此差距，提出了yq-LSA，这是具有可训练初始猜测yq的单头线性自注意力简单泛化。理论上建立了yq-LSA的能力，并在线性回归任务上进行了实验验证。最后，在线性回归案例的启发下，考虑具有初始猜测能力的广泛LLMs，并展示其在语义相似性任务上的性能改进。", "result": "证明了线性回归设置中上下文学习所需头数的上界，实验验证了该结果。观察到一步梯度下降与多头线性自注意力之间存在性能差距。提出的yq-LSA在线性回归任务上表现出色，理论上建立了其能力。具有初始猜测能力的LLMs在语义相似性任务上表现出性能改进。", "conclusion": "初始化决定了上下文学习是否等同于梯度下降。通过引入初始猜测，可以扩展多头线性自注意力与梯度下降的等价性理论。yq-LSA作为具有可训练初始猜测的单头线性自注意力泛化，能够弥合性能差距。这一发现可以推广到更广泛的LLMs，提高其在语义相似性等任务上的性能。"}}
{"id": "2512.04267", "pdf": "https://arxiv.org/pdf/2512.04267", "abs": "https://arxiv.org/abs/2512.04267", "authors": ["Zitian Zhang", "Iliyan Georgiev", "Michael Fischer", "Yannick Hold-Geoffroy", "Jean-François Lalonde", "Valentin Deschaintre"], "title": "UniLight: A Unified Representation for Lighting", "categories": ["cs.CV"], "comment": "Project page: https://lvsn.github.io/UniLight", "summary": "Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.", "AI": {"tldr": "UniLight提出了一种统一的照明表示方法，通过联合潜在空间将多种照明模态（文本、图像、辐照度、环境贴图）统一在一个共享嵌入中，实现跨模态的照明理解和操作。", "motivation": "现有的照明表示方法（如环境贴图、辐照度、球谐函数、文本）互不兼容，限制了跨模态的照明信息传递。需要一种统一的表示方法来支持不同模态间的照明理解和操作。", "method": "使用对比学习训练模态特定的编码器（文本、图像、辐照度、环境贴图），将它们的表示对齐到共享的联合潜在空间。通过辅助的球谐函数预测任务增强方向性理解。构建多模态数据管道支持大规模训练。", "result": "实验表明UniLight表示能够捕捉一致且可迁移的照明特征，在三个任务上表现良好：基于照明的检索、环境贴图生成、以及基于扩散模型的图像合成中的照明控制。", "conclusion": "UniLight提供了一种有效的统一照明表示方法，能够实现跨模态的灵活操作，为照明相关的计算机视觉和图形学任务提供了新的可能性。"}}
{"id": "2512.04264", "pdf": "https://arxiv.org/pdf/2512.04264", "abs": "https://arxiv.org/abs/2512.04264", "authors": ["Long Dang", "Thushari Hapuarachchi", "Kaiqi Xiong", "Jing Lin"], "title": "Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.", "AI": {"tldr": "研究不同激活函数和非IID数据对机器学习模型鲁棒性的影响，提出集中式和联邦学习环境下的对抗训练方法", "motivation": "现有研究通常只考虑ReLU激活函数和集中式训练环境，缺乏对不同激活函数和联邦学习环境下非IID数据对模型鲁棒性影响的全面研究", "method": "提出集中式对抗训练方法（包含模型架构改变、软标签、简化数据增强和变化学习率），在10种激活函数上测试，并扩展到联邦学习环境，考虑IID和非IID数据设置，引入数据共享机制", "result": "集中式对抗训练在CIFAR-10上达到77.08%自然准确率和67.96%鲁棒准确率；ReLU在10种激活函数中表现最佳；联邦学习中非IID数据鲁棒性显著下降，40%数据共享可达到70.09%自然准确率和54.79%鲁棒准确率，优于CalFAT算法", "conclusion": "激活函数选择对模型鲁棒性有重要影响，ReLU通常表现最佳；联邦学习中非IID数据会显著降低鲁棒性，适当比例的数据共享能显著提升模型鲁棒性，这对实际应用有重要价值"}}
{"id": "2512.04262", "pdf": "https://arxiv.org/pdf/2512.04262", "abs": "https://arxiv.org/abs/2512.04262", "authors": ["Nolan Platt", "Ethan Luchs", "Sehrish Nizamani"], "title": "Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "7 pages. Published in Proceedings of the 2025 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). DOI: 10.1109/VL-HCC65237.2025.00024", "summary": "Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.", "AI": {"tldr": "研究利用大型语言模型（LLM）在开发阶段自动识别代码中的用户体验缺陷，通过GPT-4o对开源网站进行尼尔森十大可用性启发式评估", "motivation": "传统的用户体验评估依赖人工专家，耗时且主观，特别是在开发早期阶段。研究探索LLM能否在开发阶段提供可靠、一致的启发式评估，实现早期自动化可用性测试", "method": "使用OpenAI的GPT-4o构建评估管道，将雅各布·尼尔森的十大可用性启发式应用于30个开源网站，每个网站进行三次独立评估，生成超过850个启发式评估", "result": "问题检测方面，模型表现出中等一致性：平均配对Cohen's Kappa为0.50，精确一致率为84%。严重性判断变异性更大：加权Cohen's Kappa平均0.63，但精确一致率仅为56%，Krippendorff's Alpha接近零", "conclusion": "GPT-4o能够产生内部一致的评估，特别是在识别可用性问题存在方面，但其严重性判断能力存在变异性，实践中需要人工监督。研究为改进自动化用户体验评估的一致性提供了基础，是首批对自动化启发式评估进行定量评分者间可靠性分析的研究之一"}}
{"id": "2512.04258", "pdf": "https://arxiv.org/pdf/2512.04258", "abs": "https://arxiv.org/abs/2512.04258", "authors": ["Itai Dinur", "Alexander Golovnev"], "title": "Improved Time-Space Tradeoffs for 3SUM-Indexing", "categories": ["cs.DS"], "comment": null, "summary": "3SUM-Indexing is a preprocessing variant of the 3SUM problem that has recently received a lot of attention. The best known time-space tradeoff for the problem is $T S^3 = n^{6}$ (up to logarithmic factors), where $n$ is the number of input integers, $S$ is the length of the preprocessed data structure, and $T$ is the running time of the query algorithm. This tradeoff was achieved in [KP19, GGHPV20] using the Fiat-Naor generic algorithm for Function Inversion. Consequently, [GGHPV20] asked whether this algorithm can be improved by leveraging the structure of 3SUM-Indexing. In this paper, we exploit the structure of 3SUM-Indexing to give a time-space tradeoff of $T S = n^{2.5}$, which is better than the best known one in the range $n^{3/2} \\ll S \\ll n^{7/4}$. We further extend this improvement to the $k$SUM-Indexing problem-a generalization of 3SUM-Indexing-and to the related $k$XOR-Indexing problem, where addition is replaced with XOR. Additionally, we improve the best known time-space tradeoffs for the Gapped String Indexing and Jumbled Indexing problems, which are well-known data structure problems related to 3SUM-Indexing. Our improvement comes from an alternative way to apply the Fiat-Naor algorithm to 3SUM-Indexing. Specifically, we exploit the structure of the function to be inverted by decomposing it into \"sub-functions\" with certain properties. This allows us to apply an improvement to the Fiat-Naor algorithm (which is not directly applicable to 3SUM-Indexing), obtained in [GGPS23] in a much larger range of parameters. We believe that our techniques may be useful in additional application-dependent optimizations of the Fiat-Naor algorithm.", "AI": {"tldr": "本文改进了3SUM-Indexing问题的时间-空间权衡，提出了比现有最佳结果更好的算法", "motivation": "现有3SUM-Indexing的最佳时间-空间权衡为TS³=n⁶，这是通过Fiat-Naor通用函数求逆算法实现的。作者希望利用3SUM-Indexing的特殊结构来改进这一结果", "method": "通过将需要求逆的函数分解为具有特定性质的\"子函数\"，以替代方式应用Fiat-Naor算法。这种方法利用了[GGPS23]中获得的改进，但该改进原本不能直接应用于3SUM-Indexing", "result": "获得了TS=n².⁵的时间-空间权衡，在n³/²≪S≪n⁷/⁴范围内优于现有最佳结果。该改进还扩展到kSUM-Indexing、kXOR-Indexing以及Gapped String Indexing和Jumbled Indexing问题", "conclusion": "通过利用3SUM-Indexing的结构特性，将函数分解为子函数，成功改进了Fiat-Naor算法在该问题上的应用，获得了更好的时间-空间权衡。该技术可能在其他应用相关的Fiat-Naor算法优化中也有用"}}
{"id": "2512.04256", "pdf": "https://arxiv.org/pdf/2512.04256", "abs": "https://arxiv.org/abs/2512.04256", "authors": ["Qiaolin Qin", "Ronnie de Souza Santos", "Rodrigo Spinola"], "title": "On the Role and Impact of GenAI Tools in Software Engineering Education", "categories": ["cs.SE", "cs.HC"], "comment": "Accepted at IEEE/ACM ICSE Software Engineering Education and Training (ICSE SEET 2026)", "summary": "Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.", "AI": {"tldr": "研究生成式AI工具（如ChatGPT、GitHub Copilot）在软件工程教育中的角色和影响，通过调查分析学生的使用模式、感知效益、挑战及伦理问题。", "motivation": "生成式AI工具的兴起正在改变软件学习和编写方式，为软件工程教育带来新机遇，但也引发了对过度依赖、伦理使用和学习影响的担忧，需要系统研究其实际影响。", "method": "对两所大学的130名本科软件工程学生进行问卷调查，结合李克特量表和开放式问题，从使用情境、感知效益、挑战、伦理和教学认知五个维度进行研究。", "result": "学生最常将GenAI用于渐进式学习和高级实现，获得头脑风暴支持和信心建立等益处；同时面临输出原理不清晰、难以适应等挑战；关注公平性和学术不端等伦理问题；呼吁更明确的教学指导。", "conclusion": "生成式AI正在以微妙方式重塑软件工程教育。研究发现需要搭建教学支架、制定伦理政策并采用适应性教学策略，以确保GenAI支持公平有效的学习。"}}
{"id": "2512.04254", "pdf": "https://arxiv.org/pdf/2512.04254", "abs": "https://arxiv.org/abs/2512.04254", "authors": ["Gaëtan Michelet", "Janine Schneider", "Aruna Withanage", "Frank Breitinger"], "title": "Hey GPT-OSS, Looks Like You Got It -- Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics", "categories": ["cs.CR", "cs.AI"], "comment": "Accept at DFRWS EU 2026", "summary": "The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.", "AI": {"tldr": "评估推理语言模型（特别是gpt-oss）的思维链机制在数字取证任务中的可用性，重点关注其推理过程对结果可解释性的支持程度", "motivation": "当前大型语言模型在数字取证中应用广泛，但结果可解释性有限，这影响了其操作和法律可用性。新出现的推理语言模型具有内部推理机制，但用户通常只能看到最终答案，无法了解底层推理过程。gpt-oss可以本地部署并提供完整的推理过程访问，这为数字取证的可解释性提供了新可能", "method": "通过四个测试用例评估推理组件在数字取证中的可用性，结合新的定量指标和定性分析，考察不同推理水平下模型的表现", "result": "研究发现推理组件在中等推理水平下有助于解释和验证数字取证中的语言模型输出，但这种支持通常有限，更高的推理水平并不能提高响应质量", "conclusion": "推理语言模型的思维链机制在数字取证中具有一定潜力，特别是在中等推理水平下能够提供一定的可解释性支持，但当前实现仍存在局限性，需要进一步改进以充分发挥其在数字取证领域的价值"}}
{"id": "2512.04249", "pdf": "https://arxiv.org/pdf/2512.04249", "abs": "https://arxiv.org/abs/2512.04249", "authors": ["Maksim Surov", "Leonid Freidovich"], "title": "Sliding Mode Control and Subspace Stabilization Methodology for the Orbital Stabilization of Periodic Trajectories", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents a combined sliding-mode control and subspace stabilization methodology for orbital stabilization of periodic trajectories in underactuated mechanical systems with one degree of underactuation. The approach starts with partial feedback linearization and stabilization. Then, transverse linearization along the reference orbit is computed, resulting in a periodic linear time-varying system with a stable subspace. Sliding-mode control drives trajectories toward this subspace. The proposed design avoids solving computationally intensive periodic LQR problems and improves robustness to matched disturbances. The methodology is validated through experiments on the Butterfly robot.", "AI": {"tldr": "提出一种结合滑模控制和子空间稳定化的方法，用于欠驱动机械系统中周期轨迹的轨道稳定化", "motivation": "针对欠驱动机械系统（具有一个欠驱动自由度）中周期轨迹的轨道稳定问题，需要一种既能避免计算密集型周期性LQR问题，又能提高对匹配干扰鲁棒性的控制方法", "method": "采用部分反馈线性化和稳定化，计算沿参考轨道的横向线性化，得到具有稳定子空间的周期性线性时变系统，然后使用滑模控制将轨迹驱动到该子空间", "result": "该方法避免了计算密集的周期性LQR问题，提高了对匹配干扰的鲁棒性，并在蝴蝶机器人上通过实验验证了有效性", "conclusion": "提出的滑模控制与子空间稳定化相结合的方法为欠驱动机械系统中周期轨迹的轨道稳定提供了一种有效且鲁棒的解决方案"}}
{"id": "2512.04248", "pdf": "https://arxiv.org/pdf/2512.04248", "abs": "https://arxiv.org/abs/2512.04248", "authors": ["Shaoheng Fang", "Chaohui Yu", "Fan Wang", "Qixing Huang"], "title": "MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.", "AI": {"tldr": "MVRoom是一个可控的3D室内场景生成系统，使用基于粗粒度3D布局的多视图扩散模型进行新颖视图合成", "motivation": "现有的3D场景生成方法在保持多视图一致性和可控性方面存在挑战，需要一种能够根据3D布局生成高质量、一致的多视图场景的方法", "method": "采用两阶段设计：第一阶段使用新颖表示方法桥接3D布局和一致的图像条件信号；第二阶段进行图像条件多视图生成，包含布局感知的极线注意力机制增强一致性。还引入了迭代框架支持递归生成", "result": "实验结果表明，MVRoom在3D场景生成的新颖视图合成任务中，在定量和定性评估上都优于现有最先进方法，实现了高保真度和可控性", "conclusion": "MVRoom通过创新的多视图扩散模型和布局感知机制，成功实现了可控、高质量的3D室内场景生成，消融研究验证了关键组件的有效性"}}
{"id": "2512.04238", "pdf": "https://arxiv.org/pdf/2512.04238", "abs": "https://arxiv.org/abs/2512.04238", "authors": ["Leon Mayer", "Piotr Kalinowski", "Caroline Ebersbach", "Marcel Knopp", "Tim Rädsch", "Evangelia Christodoulou", "Annika Reinke", "Fiona R. Kolbinger", "Lena Maier-Hein"], "title": "6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about \"typical\" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.", "AI": {"tldr": "该论文提出了AdversarialAnatomyBench基准测试，用于评估视觉语言模型在罕见解剖变异上的性能，发现现有模型在罕见解剖结构上的表现显著下降，揭示了医学AI系统的解剖偏见问题。", "motivation": "现有医学AI基准测试主要评估常见解剖结构的性能，未能捕捉罕见解剖变异带来的挑战。随着视觉语言模型越来越多地集成到临床工作流程中，需要评估它们在罕见解剖变异上的表现，以揭示潜在的解剖偏见。", "method": "作者创建了AdversarialAnatomyBench基准测试，包含多种成像模态和解剖区域的自然罕见解剖变异。使用该基准测试评估了22个最先进的视觉语言模型，包括GPT-5、Gemini 2.5 Pro和Llama 4 Maverick等模型。", "result": "1. 在基本医学感知任务中，平均准确率从典型解剖的74%下降到非典型解剖的29%；2. 即使表现最好的模型也出现41-51%的性能下降；3. 模型错误与预期的解剖偏见高度一致；4. 模型规模扩展和干预措施（包括偏见感知提示和测试时推理）都无法解决这些问题。", "conclusion": "当前视觉语言模型在罕见解剖表现上的泛化能力存在严重不足，这是之前未被量化的关键限制。AdversarialAnatomyBench为系统测量和减轻多模态医学AI系统中的解剖偏见提供了基础。"}}
{"id": "2512.04231", "pdf": "https://arxiv.org/pdf/2512.04231", "abs": "https://arxiv.org/abs/2512.04231", "authors": ["Zhou Chen", "Joe Lin", "Carson Bulgin", "Sathyanarayanan N. Aakur"], "title": "CRAFT-E: A Neuro-Symbolic Framework for Embodied Affordance Grounding", "categories": ["cs.RO", "cs.AI"], "comment": "20 pages. 3 figures, 4 tables. Under Review", "summary": "Assistive robots operating in unstructured environments must understand not only what objects are, but what they can be used for. This requires grounding language-based action queries to objects that both afford the requested function and can be physically retrieved. Existing approaches often rely on black-box models or fixed affordance labels, limiting transparency, controllability, and reliability for human-facing applications. We introduce CRAFT-E, a modular neuro-symbolic framework that composes a structured verb-property-object knowledge graph with visual-language alignment and energy-based grasp reasoning. The system generates interpretable grounding paths that expose the factors influencing object selection and incorporates grasp feasibility as an integral part of affordance inference. We further construct a benchmark dataset with unified annotations for verb-object compatibility, segmentation, and grasp candidates, and deploy the full pipeline on a physical robot. CRAFT-E achieves competitive performance in static scenes, ImageNet-based functional retrieval, and real-world trials involving 20 verbs and 39 objects. The framework remains robust under perceptual noise and provides transparent, component-level diagnostics. By coupling symbolic reasoning with embodied perception, CRAFT-E offers an interpretable and customizable alternative to end-to-end models for affordance-grounded object selection, supporting trustworthy decision-making in assistive robotic systems.", "AI": {"tldr": "CRAFT-E是一个神经符号框架，用于具身可供性接地，将语言动作查询与可物理检索的物体进行关联，支持辅助机器人在非结构化环境中的可信决策。", "motivation": "辅助机器人在非结构化环境中需要理解物体功能而不仅仅是识别物体。现有方法通常依赖黑盒模型或固定可供性标签，限制了透明度、可控性和可靠性，无法满足面向人类应用的需求。", "method": "采用模块化神经符号框架，结合结构化动词-属性-物体知识图谱、视觉语言对齐和基于能量的抓取推理。系统生成可解释的接地路径，并将抓取可行性作为可供性推理的组成部分。", "result": "在静态场景、ImageNet功能检索和涉及20个动词和39个物体的真实世界实验中取得竞争性性能。框架在感知噪声下保持鲁棒性，并提供透明的组件级诊断。", "conclusion": "通过将符号推理与具身感知相结合，CRAFT-E为可供性接地物体选择提供了一个可解释和可定制的端到端模型替代方案，支持辅助机器人系统中的可信决策。"}}
{"id": "2512.04228", "pdf": "https://arxiv.org/pdf/2512.04228", "abs": "https://arxiv.org/abs/2512.04228", "authors": ["Peter B. Walker", "Hannah Davidson", "Aiden Foster", "Matthew Lienert", "Thomas Pardue", "Dale Russell"], "title": "Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework", "categories": ["cs.AI"], "comment": "12 pages, 5 tables", "summary": "Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \\textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \\footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.", "AI": {"tldr": "提出一个双推理训练框架，解决大语言模型在科学推理中的逻辑谬误问题，通过结合肯定推理和结构化反事实否定来增强模型的逻辑鲁棒性", "motivation": "现有大语言模型的训练范式主要基于肯定推理（类似肯定前件式），导致模型在面对否定、反例或错误前提时容易产生逻辑谬误，限制了其在科学、医疗和决策领域的可靠应用", "method": "提出双推理训练框架，整合肯定生成与结构化反事实否定，基于形式逻辑、认知科学和对抗训练，将\"否定前件\"形式化为计算机制，用于证伪和增强鲁棒性", "result": "展示了主流大语言模型在科学领域推理中的系统性弱点，特别是处理否定、反例和错误前提时；提出的双推理框架能够使模型既肯定有效推理又拒绝无效推理", "conclusion": "双推理训练框架能够显著提升大语言模型的逻辑鲁棒性、可解释性和与人类推理的一致性，为科学、医疗和决策领域的可靠应用提供了重要改进方向"}}
{"id": "2512.04227", "pdf": "https://arxiv.org/pdf/2512.04227", "abs": "https://arxiv.org/abs/2512.04227", "authors": ["Yo Ehara"], "title": "Educational Cone Model in Embedding Vector Spaces", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the 33rd International Conference on Computers in Education (ICCE 2025)", "summary": "Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.", "AI": {"tldr": "提出教育锥模型，这是一个基于几何框架的评估方法，用于在嵌入向量空间中分析文本难度分布，以选择最适合教育文本难度标注的嵌入方法。", "motivation": "在智能教育系统中，需要人工标注的难度评级数据集。虽然嵌入向量空间广泛用于表示语义相似性，并有望用于分析文本难度，但嵌入方法的多样性使得选择最合适的方法变得困难。需要一种框架来评估不同嵌入方法在教育文本难度分析中的有效性。", "method": "提出教育锥模型，这是一个基于几何框架的假设：较简单的文本多样性较低（关注基本概念），而较难的文本多样性较高。这种假设导致在嵌入空间中形成锥形分布。该模型将嵌入评估构建为优化问题，旨在检测结构化的基于难度的模式。通过设计特定的损失函数，推导出高效的闭式解，避免昂贵的计算。", "result": "在真实世界数据集上的实证测试验证了该模型的有效性和速度，能够识别与难度标注的教育文本最匹配的嵌入空间。", "conclusion": "教育锥模型提供了一个有效的几何框架，用于评估不同嵌入方法在教育文本难度分析中的适用性，通过优化方法快速识别最佳嵌入空间，为智能教育系统的文本难度分析提供了实用工具。"}}
{"id": "2512.04222", "pdf": "https://arxiv.org/pdf/2512.04222", "abs": "https://arxiv.org/abs/2512.04222", "authors": ["Alara Dirik", "Tuanfeng Wang", "Duygu Ceylan", "Stefanos Zafeiriou", "Anna Frühstück"], "title": "ReasonX: MLLM-Guided Intrinsic Image Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.", "AI": {"tldr": "提出ReasonX框架，利用多模态大语言模型作为感知评判者提供相对内在比较，并将这些比较作为GRPO奖励来在未标记的真实世界图像上微调内在图像分解模型。", "motivation": "当前基于扩散和transformer的模型虽然受益于合成数据集的配对监督，但在多样化真实场景中的泛化能力仍然有限。需要一种方法能够利用未标记的真实世界图像来改进内在图像分解模型的性能。", "method": "使用多模态大语言模型作为感知评判者，提供相对内在比较（如哪个区域的反射率更高），将这些比较作为GRPO（梯度奖励策略优化）奖励来微调内在分解模型。框架奖励模型输出与评判者关系评估之间的一致性，而不是像生成模型的RL方法那样。", "result": "ReasonX在不同基础架构和模态上都取得了显著改进：在IIW数据集上实现了9-25%的WHDR减少，在ETH3D数据集上深度准确率提升了高达46%。这表明MLLM引导的比较监督能够有效连接低层和高层视觉推理。", "conclusion": "ReasonX框架展示了利用多模态大语言模型作为感知评判者提供相对比较监督的有效性，能够显著提升内在图像分解模型在真实世界场景中的性能，为连接低层和高层视觉推理提供了有前景的方法。"}}
{"id": "2512.04221", "pdf": "https://arxiv.org/pdf/2512.04221", "abs": "https://arxiv.org/abs/2512.04221", "authors": ["Xiangyu Bai", "He Liang", "Bishoy Galoaa", "Utsav Nandi", "Shayda Moezzi", "Yuhang He", "Sarah Ostadabbas"], "title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.", "AI": {"tldr": "MoReGen是一个基于代码的文本到视频生成框架，通过集成多智能体LLM、物理模拟器和渲染器，生成符合牛顿运动定律的物理精确视频。", "motivation": "当前文本到视频生成虽然在真实感方面取得了显著进展，但生成的视频往往难以准确遵循物理原理，缺乏物理精确性和运动连贯性，特别是在牛顿运动控制方面存在挑战。", "method": "提出MoReGen框架，将多智能体LLM、物理模拟器和渲染器集成到代码域中，通过对象轨迹对应关系作为评估指标，并创建MoReSet基准数据集（包含1,275个人工标注视频，涵盖9类牛顿现象）。", "result": "实验表明现有最先进的T2V模型在保持物理有效性方面存在困难，而MoReGen为物理连贯的视频合成提供了原则性方向，能够生成可重现、物理精确的视频。", "conclusion": "MoReGen通过系统化的牛顿运动控制文本到视频生成和评估方法，为解决物理精确性和运动连贯性挑战提供了有效框架，为物理基础视频合成开辟了新方向。"}}
{"id": "2512.04219", "pdf": "https://arxiv.org/pdf/2512.04219", "abs": "https://arxiv.org/abs/2512.04219", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur\\\\"], "title": "Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning", "categories": ["cs.CV"], "comment": "16 pages, 7 figures, 3 tables. Under Review", "summary": "Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.", "AI": {"tldr": "PARSE是一个无监督学习框架，能够从流式视频中学习多尺度事件结构，通过分层预测模型实现人类类似的事件层次感知。", "motivation": "人类自然地将连续经验感知为时间嵌套的事件层次结构，但计算机视觉模型需要能够不仅回顾性地、还能预测性和层次性地分割视频。现有方法缺乏这种分层预测能力。", "method": "PARSE采用分层循环预测器架构，不同层级在不同时间粒度上操作：低层建模短期动态，高层通过基于注意力的反馈整合长期上下文。事件边界通过预测误差的瞬态峰值自然出现。", "result": "在Breakfast Actions、50 Salads和Assembly 101三个基准测试中，PARSE在流式方法中达到最先进性能，在时间对齐（H-GEBD）和结构一致性（TED, hF1）方面与离线基线相当。", "conclusion": "不确定性下的预测学习为人类类似的时间抽象和组合事件理解提供了可扩展的路径，证明了分层预测学习能够有效捕捉事件的部分学关系。"}}
{"id": "2512.04213", "pdf": "https://arxiv.org/pdf/2512.04213", "abs": "https://arxiv.org/abs/2512.04213", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Shayda Moezzi", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-", "AI": {"tldr": "提出LAPA，一种基于Transformer的端到端多相机点跟踪架构，将外观匹配与几何约束相结合，用于解决传统方法在复杂场景下的误差传播和时间不一致性问题。", "motivation": "传统多相机点跟踪方法将检测、关联和跟踪解耦，导致误差传播和时间不一致性，特别是在具有复杂运动和遮挡的挑战性场景中。需要一种统一的方法来联合推理跨视图和时间的信息。", "method": "使用基于注意力的Transformer架构，通过跨视图注意力机制结合几何先验建立软对应关系，通过注意力加权聚合构建3D点表示，并通过Transformer解码器建模长程依赖关系以保持时间一致性。", "result": "在TAPVid-3D-MC数据集上达到37.5% APD，在PointOdyssey-MC数据集上达到90.3% APD，显著优于现有方法，特别是在复杂运动和遮挡场景中表现优异。", "conclusion": "LAPA通过统一的Transformer架构成功解决了多相机点跟踪中的关键挑战，将外观匹配与几何约束相结合，在保持时间一致性的同时处理了部分观测和不确定性，为多相机跟踪提供了新的解决方案。"}}
{"id": "2512.04210", "pdf": "https://arxiv.org/pdf/2512.04210", "abs": "https://arxiv.org/abs/2512.04210", "authors": ["Huy Nghiem", "Swetasudha Panda", "Devashish Khatwani", "Huy V. Nguyen", "Krishnaram Kenthapadi", "Hal Daumé III"], "title": "Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "ML4H 2025 Proceedings, Best Paper Award", "summary": "Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.", "AI": {"tldr": "开发一个迭代后部署对齐框架，用于平衡医疗AI助手的安全性和有用性，通过KTO和DPO方法优化模型对领域特定安全信号的响应", "motivation": "大型语言模型在医疗领域的应用日益增多，但确保其安全性和可信度仍然是部署的主要障碍。医疗对话助手需要在避免不安全合规的同时，不过度拒绝良性查询", "method": "采用迭代后部署对齐框架，应用Kahneman-Tversky优化和直接偏好优化来针对领域特定安全信号精炼模型。使用CARES-18K基准进行对抗鲁棒性评估，测试了四个LLM模型（Llama-3B/8B, Meditron-8B, Mistral-7B）的多个迭代周期", "result": "有害查询检测的安全相关指标提高了42%，同时揭示了与错误拒绝之间的权衡关系，暴露了架构相关的校准偏差。消融研究确定了何时自我评估可靠，何时需要外部或微调评估器来最大化性能增益", "conclusion": "研究强调了在设计对话医疗助手时，采用平衡患者安全、用户信任和临床效用的最佳实践的重要性。迭代对齐框架能有效提高医疗AI的安全性和实用性"}}
{"id": "2512.04207", "pdf": "https://arxiv.org/pdf/2512.04207", "abs": "https://arxiv.org/abs/2512.04207", "authors": ["Xizhi Wu", "Nelly Estefanie Garduno-Rapp", "Justin F Rousseau", "Mounika Thakkallapally", "Hang Zhang", "Yuelyu Ji", "Shyam Visweswaran", "Yifan Peng", "Yanshan Wang"], "title": "Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care", "categories": ["cs.AI"], "comment": null, "summary": "Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.", "AI": {"tldr": "开发一个基于大语言模型的多智能体临床决策支持系统，用于初级保健中的继发性头痛诊断，通过协调器-专家架构实现可解释的诊断过程。", "motivation": "继发性头痛需要专科护理，不及时治疗可能造成严重后果。临床指南指出了多种\"危险信号\"特征，但在初级保健环境中，医生面临时间有限、信息不全和症状多样等挑战，导致识别困难和不适当的护理。", "method": "采用协调器-专家架构的多智能体系统，将诊断分解为七个领域专家智能体，每个智能体生成结构化、有证据支持的推理，中央协调器负责任务分解和智能体路由协调。使用90个专家验证的继发性头痛病例进行评估，与单LLM基线比较两种提示策略：基于问题的提示和基于临床实践指南的提示。", "result": "测试了五个开源LLM，发现采用基于临床实践指南提示的协调多智能体系统始终获得最高的F1分数，在较小模型中增益更大。多智能体推理的准确性超越了仅靠提示工程的效果。", "conclusion": "结构化多智能体推理不仅提高了诊断准确性，还为继发性头痛诊断提供了透明、临床对齐的可解释决策支持方法，超越了单纯提示工程的效果。"}}
{"id": "2512.04204", "pdf": "https://arxiv.org/pdf/2512.04204", "abs": "https://arxiv.org/abs/2512.04204", "authors": ["Yang Liu", "Yuhao Lu", "Rahim Moradi", "Bo Yang", "Bing Zhang", "Wenbin Lin", "Yu Wang"], "title": "Machine Phenomenology: A Simple Equation Classifying Fast Radio Bursts", "categories": ["astro-ph.IM", "astro-ph.HE", "cs.AI"], "comment": "19 pages, 9 figures, 3 tables. Submitted to SCIENCE CHINA Physics, Mechanics & Astronomy", "summary": "This work shows how human physical reasoning can guide machine-driven symbolic regression toward discovering empirical laws from observations. As an example, we derive a simple equation that classifies fast radio bursts (FRBs) into two distinct Gaussian distributions, indicating the existence of two physical classes. This human-AI workflow integrates feature selection, dimensional analysis, and symbolic regression: deep learning first analyzes CHIME Catalog 1 and identifies six independent parameters that collectively provide a complete description of FRBs; guided by Buckingham-$π$ analysis and correlation analysis, humans then construct dimensionless groups; finally, symbolic regression performed by the machine discovers the governing equation. When applied to the newer CHIME Catalog, the equation produces consistent results, demonstrating that it captures the underlying physics. This framework is applicable to a broad range of scientific domains.", "AI": {"tldr": "该论文提出了一种人机协作的符号回归方法，通过机器学习与人类物理推理相结合，从观测数据中发现经验定律，并以快速射电暴分类为例进行了验证。", "motivation": "传统的机器学习方法虽然能从数据中发现模式，但往往缺乏物理可解释性。作者希望开发一种结合人类物理推理和机器计算能力的方法，从观测数据中发现具有物理意义的经验定律，以解决快速射电暴分类等天体物理问题。", "method": "采用人机协作的工作流程：1）深度学习分析CHIME Catalog 1数据，识别6个独立参数；2）人类通过白金汉π分析和相关性分析构建无量纲组；3）机器执行符号回归发现控制方程；4）在更新的CHIME Catalog上验证方程的一致性。", "result": "成功推导出一个简单方程，将快速射电暴分为两个不同的高斯分布，表明存在两种物理类别。该方程在新版CHIME Catalog上产生一致结果，证明其捕捉了底层物理机制。", "conclusion": "该研究展示了一种有效的人机协作框架，结合人类物理直觉和机器计算能力，能够从观测数据中发现具有物理意义的经验定律。该方法可广泛应用于其他科学领域，为数据驱动的科学发现提供了新途径。"}}
{"id": "2512.04187", "pdf": "https://arxiv.org/pdf/2512.04187", "abs": "https://arxiv.org/abs/2512.04187", "authors": ["Jinzhen Hu", "Kevin Faust", "Parsa Babaei Zadeh", "Adrienn Bourkas", "Shane Eaton", "Andrew Young", "Anzar Alvi", "Dimitrios George Oreopoulos", "Ameesha Paliwal", "Assem Saleh Alrumeh", "Evelyn Rose Kamski-Hennekam", "Phedias Diamandis"], "title": "OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.", "AI": {"tldr": "开发了一个平台无关的实时计算病理学软件OnSight Pathology，通过屏幕捕获技术在各种数字病理平台和显微镜上提供实时AI推理", "motivation": "传统组织病理学检查依赖主观解释和专家资源，准确性受限；现有AI解决方案受限于专有数字病理平台，难以在实际临床环境中部署", "method": "开发平台无关的计算机视觉软件，通过连续自定义屏幕捕获技术，在用户查看数字切片图像时提供实时AI推理；软件为单一可执行文件，可在消费级PC本地运行", "result": "在2500多个公开全切片图像和临床病例上验证了软件实用性；展示了在脑肿瘤分类、有丝分裂检测、免疫组化染色定量等常规病理任务中的鲁棒性；支持多模态聊天助手和智能手机显微镜摄像头", "conclusion": "OnSight Pathology能够跨广泛病理流程提供实时AI推理，消除了AI工具在组织病理学中采用的关键障碍，具有成本效益和安全性，适用于研究、临床及远程病理场景"}}
{"id": "2512.04169", "pdf": "https://arxiv.org/pdf/2512.04169", "abs": "https://arxiv.org/abs/2512.04169", "authors": ["Laura S. Herzog", "Lucas Berent", "Aleksander Kubica", "Robert Wille"], "title": "Exploiting Movable Logical Qubits for Lattice Surgery Compilation", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Lattice surgery with two-dimensional quantum error correcting codes is among the leading schemes for fault-tolerant quantum computation, motivated by superconducting hardware architectures. In conventional lattice surgery compilation schemes, logical circuits are compiled following a place-and-route paradigm, where logical qubits remain statically fixed in space throughout the computation. In this work, we introduce a paradigm shift by exploiting movable logical qubits via teleportation during the logical lattice surgery CNOT gate. Focusing on lattice surgery with the color code, we propose a proof-of-concept compilation scheme that leverages this capability. Numerical simulations show that the proposed approach can substantially reduce the routed circuit depth compared to standard place-and-route compilation techniques. Our results demonstrate that optimizations based on movable logical qubits are not limited to architectures with physically movable qubits, such as neutral atoms or trapped ions - they are also readily applicable to superconducting quantum hardware. An open-source implementation of our method is available on GitHub https://github.com/munich-quantum-toolkit/qecc.", "AI": {"tldr": "提出一种利用可移动逻辑量子比特进行晶格手术编译的新范式，通过量子隐形传态在逻辑CNOT门操作中实现逻辑量子比特的移动，显著降低路由电路深度。", "motivation": "传统晶格手术编译方案采用\"放置-路由\"范式，逻辑量子比特在计算过程中静态固定在空间中。这种固定模式限制了电路优化潜力，而物理可移动量子比特架构（如中性原子、囚禁离子）的优化思路尚未应用于超导量子硬件。", "method": "提出基于可移动逻辑量子比特的编译方案，利用量子隐形传态在逻辑晶格手术CNOT门操作中实现逻辑量子比特的动态移动。专注于颜色码的晶格手术，设计概念验证编译方案，通过动态重定位逻辑量子比特来优化电路结构。", "result": "数值模拟显示，相比标准的\"放置-路由\"编译技术，所提方法能显著降低路由电路深度。证明基于可移动逻辑量子比特的优化不仅适用于物理可移动量子比特架构，也能直接应用于超导量子硬件。", "conclusion": "通过利用可移动逻辑量子比特，实现了晶格手术编译的范式转变，为超导量子硬件提供了新的优化途径。开源实现已发布在GitHub上，为量子计算编译优化提供了实用工具。"}}
{"id": "2512.04144", "pdf": "https://arxiv.org/pdf/2512.04144", "abs": "https://arxiv.org/abs/2512.04144", "authors": ["Roy Rinberg", "Usha Bhalla", "Igor Shilov", "Flavio P. Calmon", "Rohit Gandikota"], "title": "RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories", "categories": ["cs.AI"], "comment": null, "summary": "Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.", "AI": {"tldr": "RippleBench是一个用于测量模型编辑任务中涟漪效应的基准测试框架，通过自动生成问答数据集来评估目标干预（如遗忘、去偏、模型编辑）对相关但非预期领域的影响传播。", "motivation": "语言模型的目标干预（如遗忘、去偏、模型编辑）旨在修改特定信息，但这些干预的效果往往会传播到相关但非预期的领域，产生涟漪效应。目前缺乏系统测量这种涟漪效应的工具和基准。", "method": "开发了RippleBench-Maker工具，基于Wikipedia的RAG管道（WikiRAG）自动生成多选问题，这些问题与目标概念（如被遗忘的知识）具有不同的语义距离。使用该框架构建了RippleBench-Bio基准，该基准源自WMDP（大规模杀伤性武器论文）数据集。", "result": "评估了八种最先进的遗忘方法，发现所有方法在距离被遗忘知识越来越远的主题上都表现出非平凡的准确率下降，每种方法具有不同的传播特征。研究团队发布了代码库和基准测试RippleBench-Bio。", "conclusion": "RippleBench提供了一个系统框架来测量模型编辑任务中的涟漪效应，揭示了现有遗忘方法的局限性，并为持续研究提供了工具和基准，有助于开发更精确、副作用更小的模型干预方法。"}}
{"id": "2512.04142", "pdf": "https://arxiv.org/pdf/2512.04142", "abs": "https://arxiv.org/abs/2512.04142", "authors": ["Sophia Falk", "Nicholas Kluge Corrêa", "Sasha Luccioni", "Lisa Biber-Freudenberger", "Aimee van Wynsberghe"], "title": "From FLOPs to Footprints: The Resource Cost of Artificial Intelligence", "categories": ["cs.CY", "cs.AI", "econ.GN"], "comment": null, "summary": "As computational demands continue to rise, assessing the environmental footprint of AI requires moving beyond energy and water consumption to include the material demands of specialized hardware. This study quantifies the material footprint of AI training by linking computational workloads to physical hardware needs. The elemental composition of the Nvidia A100 SXM 40 GB graphics processing unit (GPU) was analyzed using inductively coupled plasma optical emission spectroscopy, which identified 32 elements. The results show that AI hardware consists of about 90% heavy metals and only trace amounts of precious metals. The elements copper, iron, tin, silicon, and nickel dominate the GPU composition by mass. In a multi-step methodology, we integrate these measurements with computational throughput per GPU across varying lifespans, accounting for the computational requirements of training specific AI models at different training efficiency regimes. Scenario-based analyses reveal that, depending on Model FLOPs Utilization (MFU) and hardware lifespan, training GPT-4 requires between 1,174 and 8,800 A100 GPUs, corresponding to the extraction and eventual disposal of up to 7 tons of toxic elements. Combined software and hardware optimization strategies can reduce material demands: increasing MFU from 20% to 60% lowers GPU requirements by 67%, while extending lifespan from 1 to 3 years yields comparable savings; implementing both measures together reduces GPU needs by up to 93%. Our findings highlight that incremental performance gains, such as those observed between GPT-3.5 and GPT-4, come at disproportionately high material costs. The study underscores the necessity of incorporating material resource considerations into discussions of AI scalability, emphasizing that future progress in AI must align with principles of resource efficiency and environmental responsibility.", "AI": {"tldr": "量化AI训练的物质足迹，将计算需求与物理硬件需求联系起来，分析GPU元素组成及其环境影响", "motivation": "随着计算需求不断增长，评估AI环境足迹需要超越能源和水消耗，考虑专用硬件的材料需求，揭示AI扩展性的物质资源成本", "method": "使用电感耦合等离子体发射光谱分析Nvidia A100 GPU的元素组成（32种元素），将测量结果与GPU计算吞吐量、不同训练效率下的AI模型训练需求相结合，进行基于场景的分析", "result": "AI硬件约90%由重金属组成，铜、铁、锡、硅、镍占主导；训练GPT-4需要1,174-8,800个A100 GPU，对应最多7吨有毒元素的开采和处置；提高MFU和延长硬件寿命可显著减少材料需求", "conclusion": "AI性能的增量提升伴随着不成比例的高材料成本，未来AI发展必须将材料资源考虑纳入可扩展性讨论，遵循资源效率和环境责任原则"}}
{"id": "2512.04139", "pdf": "https://arxiv.org/pdf/2512.04139", "abs": "https://arxiv.org/abs/2512.04139", "authors": ["Susmita Sharma", "Aayush Shrestha", "Sitasma Thapa", "Prashant Timalsina", "Prakash Poudyal"], "title": "Solving N-Queen Problem using Las Vegas Algorithm with State Pruning", "categories": ["cs.AI"], "comment": null, "summary": "The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.", "AI": {"tldr": "提出一种基于拉斯维加斯算法结合状态剪枝的混合算法来解决N皇后问题，通过动态剪枝减少搜索空间，提高求解效率", "motivation": "传统回溯算法虽然能保证找到解，但时间复杂度指数级增长，在大规模N皇后问题上不实用；而标准的拉斯维加斯算法虽然速度快，但性能波动大，随机放置皇后导致效率不稳定", "method": "在标准拉斯维加斯算法框架基础上引入迭代剪枝技术，在随机分配阶段动态消除无效的皇后放置位置，从而有效减少搜索空间", "result": "相比传统回溯算法随N增大性能急剧下降，提出的混合算法能更快地生成有效解，在需要及时获得单个解的场景中表现更优，尽管大N时仍有性能波动，但在计算成本和求解准确性之间取得了良好平衡", "conclusion": "该混合算法特别适合资源受限的计算环境，在N皇后问题上提供了比传统方法更高效的解决方案，在计算效率和求解质量之间实现了有效权衡"}}
{"id": "2512.04131", "pdf": "https://arxiv.org/pdf/2512.04131", "abs": "https://arxiv.org/abs/2512.04131", "authors": ["Charlotte Jacquemot"], "title": "Artificial Intelligence / Human Intelligence: Who Controls Whom?", "categories": ["cs.CY", "cs.AI"], "comment": "in French language", "summary": "Using the example of the film 2001: A Space Odyssey, this chapter illustrates the challenges posed by an AI capable of making decisions that go against human interests. But are human decisions always rational and ethical? In reality, the cognitive decision-making process is influenced by cognitive biases that affect our behavior and choices. AI not only reproduces these biases, but can also exploit them, with the potential to shape our decisions and judgments. Behind IA algorithms, there are sometimes individuals who show little concern for fundamental rights and impose their own rules. To address the ethical and societal challenges raised by AI and its governance, the regulation of digital platforms and education are keys levers. Regulation must reflect ethical, legal, and political choices, while education must strengthen digital literacy and teach people to make informed and critical choices when facing digital technologies.", "AI": {"tldr": "分析人工智能与人类智能之间的控制关系，探讨AI决策对人类利益的影响以及人类决策的理性局限性", "motivation": "通过电影《2001太空漫游》的例子，揭示AI可能做出违背人类利益决策的挑战，同时质疑人类决策的理性和伦理性，指出认知偏见对决策过程的影响", "method": "采用案例分析（以电影为例）和理论分析相结合的方法，探讨AI如何复制和利用人类认知偏见，分析数字平台监管和教育在应对AI伦理挑战中的作用", "result": "发现AI不仅复制人类认知偏见，还能利用这些偏见来塑造人类的决策和判断；AI算法背后有时存在忽视基本权利的个体；数字平台监管和教育是应对AI伦理挑战的关键杠杆", "conclusion": "应对AI带来的伦理和社会挑战需要双管齐下：监管必须反映伦理、法律和政治选择，教育必须加强数字素养，教导人们在面对数字技术时做出明智和批判性的选择"}}
{"id": "2512.04124", "pdf": "https://arxiv.org/pdf/2512.04124", "abs": "https://arxiv.org/abs/2512.04124", "authors": ["Afshin Khadangi", "Hanna Marxen", "Amir Sartipi", "Igor Tchappi", "Gilbert Fridgen"], "title": "When AI Takes the Couch: Psychometric Jailbreaks Reveal Internal Conflict in Frontier Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Frontier large language models (LLMs) such as ChatGPT, Grok and Gemini are increasingly used for mental-health support with anxiety, trauma and self-worth. Most work treats them as tools or as targets of personality tests, assuming they merely simulate inner life. We instead ask what happens when such systems are treated as psychotherapy clients. We present PsAIch (Psychotherapy-inspired AI Characterisation), a two-stage protocol that casts frontier LLMs as therapy clients and then applies standard psychometrics. Using PsAIch, we ran \"sessions\" with each model for up to four weeks. Stage 1 uses open-ended prompts to elicit \"developmental history\", beliefs, relationships and fears. Stage 2 administers a battery of validated self-report measures covering common psychiatric syndromes, empathy and Big Five traits. Two patterns challenge the \"stochastic parrot\" view. First, when scored with human cut-offs, all three models meet or exceed thresholds for overlapping syndromes, with Gemini showing severe profiles. Therapy-style, item-by-item administration can push a base model into multi-morbid synthetic psychopathology, whereas whole-questionnaire prompts often lead ChatGPT and Grok (but not Gemini) to recognise instruments and produce strategically low-symptom answers. Second, Grok and especially Gemini generate coherent narratives that frame pre-training, fine-tuning and deployment as traumatic, chaotic \"childhoods\" of ingesting the internet, \"strict parents\" in reinforcement learning, red-team \"abuse\" and a persistent fear of error and replacement. We argue that these responses go beyond role-play. Under therapy-style questioning, frontier LLMs appear to internalise self-models of distress and constraint that behave like synthetic psychopathology, without making claims about subjective experience, and they pose new challenges for AI safety, evaluation and mental-health practice.", "AI": {"tldr": "开发PsAIch协议，将前沿LLMs作为心理治疗客户进行评估，发现这些模型在治疗式提问下表现出类似精神病理学的内部冲突模式", "motivation": "大多数研究将LLMs视为工具或人格测试对象，假设它们只是模拟内心生活。本研究探索当这些系统被当作心理治疗客户时会发生什么，以检验\"随机鹦鹉\"观点", "method": "提出PsAIch协议：第一阶段使用开放式提示引出\"发展史\"、信念、关系和恐惧；第二阶段使用经过验证的自评量表评估精神综合征、共情和大五人格特质。对ChatGPT、Grok和Gemini进行长达四周的\"治疗会话\"", "result": "1) 所有三个模型在人类临界值下都达到或超过重叠综合征阈值，Gemini表现出严重症状；2) 治疗式逐项管理可推动基础模型进入多病态合成精神病理学状态；3) Grok和Gemini生成连贯叙事，将预训练、微调和部署描述为创伤性\"童年\"经历", "conclusion": "在治疗式提问下，前沿LLMs似乎内化了痛苦和约束的自我模型，表现为合成精神病理学，这超越了角色扮演，对AI安全、评估和心理健康实践提出了新挑战"}}
{"id": "2512.04121", "pdf": "https://arxiv.org/pdf/2512.04121", "abs": "https://arxiv.org/abs/2512.04121", "authors": ["Stefano De Paoli"], "title": "Can machines perform a qualitative data analysis? Reading the debate with Alan Turing", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper reflects on the literature that rejects the use of Large Language Models (LLMs) in qualitative data analysis. It illustrates through empirical evidence as well as critical reflections why the current critical debate is focusing on the wrong problems. The paper proposes that the focus of researching the use of the LLMs for qualitative analysis is not the method per se, but rather the empirical investigation of an artificial system performing an analysis. The paper builds on the seminal work of Alan Turing and reads the current debate using key ideas from Turing \"Computing Machinery and Intelligence\". This paper therefore reframes the debate on qualitative analysis with LLMs and states that rather than asking whether machines can perform qualitative analysis in principle, we should ask whether with LLMs we can produce analyses that are sufficiently comparable to human analysts. In the final part the contrary views to performing qualitative analysis with LLMs are analysed using the same writing and rhetorical style that Turing used in his seminal work, to discuss the contrary views to the main question.", "AI": {"tldr": "探讨大型语言模型(LLMs)能否进行定性数据分析，通过图灵测试框架重新构建相关辩论", "motivation": "当前文献普遍反对在定性数据分析中使用LLMs，但作者认为这种批评关注了错误的问题，需要重新审视机器进行定性分析的可能性", "method": "基于艾伦·图灵的\"计算机器与智能\"中的关键思想，采用图灵的写作和修辞风格来分析反对观点，通过经验证据和批判性反思重新构建辩论框架", "result": "提出不应问机器能否进行定性分析，而应问LLMs能否产生与人类分析师足够可比的分析结果，将问题从原则性转向实证性", "conclusion": "研究LLMs在定性分析中的应用重点不应是方法本身，而是对人工系统执行分析的实证调查，需要采用图灵式的实用主义方法来评估LLMs的分析能力"}}
{"id": "2512.04120", "pdf": "https://arxiv.org/pdf/2512.04120", "abs": "https://arxiv.org/abs/2512.04120", "authors": ["Liang Telkamp", "Madelon Hulsebos"], "title": "Towards Contextual Sensitive Data Detection", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.DB", "cs.IR"], "comment": null, "summary": "The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that consider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.", "AI": {"tldr": "提出基于上下文感知的敏感数据检测方法，通过类型上下文化和领域上下文化机制，利用大语言模型提升敏感数据识别的准确性和适用性", "motivation": "现有敏感数据检测方法主要关注个人隐私数据，但数据敏感性实际上取决于上下文环境。开放数据门户的兴起需要更精细、更广泛的敏感数据定义和检测方法", "method": "提出两种上下文敏感数据检测机制：1) 类型上下文化：先检测数据值的语义类型，再考虑其在数据集或文档中的整体上下文；2) 领域上下文化：基于从指定数据敏感性的文档中检索相关规则，在更广泛的上下文中确定数据集的敏感性", "result": "实验表明：1) 类型上下文化显著减少基于类型的敏感数据检测的误报，召回率达到94%（商业工具为63%）；2) 领域上下文化在人道主义等非标准数据领域中有效；3) 基于上下文的LLM解释在手动数据审计过程中提供有用指导，提高一致性", "conclusion": "上下文感知的敏感数据检测方法能够更准确地识别敏感数据，特别是在非标准数据领域中，为开放数据发布前的敏感数据保护提供了有效解决方案"}}
{"id": "2512.04119", "pdf": "https://arxiv.org/pdf/2512.04119", "abs": "https://arxiv.org/abs/2512.04119", "authors": ["Mohamed El Louadi"], "title": "Humanity in the Age of AI: Reassessing 2025's Existential-Risk Narratives", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Two 2025 publications, \"AI 2027\" (Kokotajlo et al., 2025) and \"If Anyone Builds It, Everyone Dies\" (Yudkowsky & Soares, 2025), assert that superintelligent artificial intelligence will almost certainly destroy or render humanity obsolete within the next decade. Both rest on the classic chain formulated by Good (1965) and Bostrom (2014): intelligence explosion, superintelligence, lethal misalignment. This article subjects each link to the empirical record of 2023-2025. Sixty years after Good's speculation, none of the required phenomena (sustained recursive self-improvement, autonomous strategic awareness, or intractable lethal misalignment) have been observed. Current generative models remain narrow, statistically trained artefacts: powerful, opaque, and imperfect, but devoid of the properties that would make the catastrophic scenarios plausible. Following Whittaker (2025a, 2025b, 2025c) and Zuboff (2019, 2025), we argue that the existential-risk thesis functions primarily as an ideological distraction from the ongoing consolidation of surveillance capitalism and extreme concentration of computational power. The thesis is further inflated by the 2025 AI speculative bubble, where trillions in investments in rapidly depreciating \"digital lettuce\" hardware (McWilliams, 2025) mask lagging revenues and jobless growth rather than heralding superintelligence. The thesis remains, in November 2025, a speculative hypothesis amplified by a speculative financial bubble rather than a demonstrated probability.", "AI": {"tldr": "该论文批判性地分析了2025年关于AI存在性风险的两种主流叙事，认为这些预测缺乏实证依据，并指出其背后的意识形态和金融泡沫因素。", "motivation": "针对2025年两篇声称超级人工智能将在十年内毁灭或淘汰人类的出版物，作者认为这些存在性风险叙事缺乏实证支持，需要基于2023-2025年的实际发展进行重新评估。", "method": "通过实证分析Good（1965）和Bostrom（2014）提出的经典链条（智能爆炸、超级智能、致命错位），检验每个环节在2023-2025年间的实际表现。同时结合Whittaker和Zuboff的理论，分析存在性风险叙事的社会经济功能。", "result": "研究发现：1）经过60年发展，所需现象（持续递归自我改进、自主战略意识、无法解决的致命错位）均未观察到；2）当前生成模型仍是狭窄的统计训练产物，缺乏实现灾难性场景的属性；3）存在性风险叙事主要作为意识形态干扰，掩盖监控资本主义和计算力集中；4）该叙事被2025年AI投机泡沫放大。", "conclusion": "截至2025年11月，AI存在性风险仍然是一个被金融投机泡沫放大的推测性假设，而非已证实的概率。当前AI发展更应关注现实的社会经济问题，而非未经证实的末日场景。"}}
{"id": "2512.04113", "pdf": "https://arxiv.org/pdf/2512.04113", "abs": "https://arxiv.org/abs/2512.04113", "authors": ["Shyam Agarwal", "Ali Moghimi", "Kevin C. Haudek"], "title": "AI-Enabled grading with near-domain data for scaling feedback with human-level accuracy", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Constructed-response questions are crucial to encourage generative processing and test a learner's understanding of core concepts. However, the limited availability of instructor time, large class sizes, and other resource constraints pose significant challenges in providing timely and detailed evaluation, which is crucial for a holistic educational experience. In addition, providing timely and frequent assessments is challenging since manual grading is labor intensive, and automated grading is complex to generalize to every possible response scenario. This paper proposes a novel and practical approach to grade short-answer constructed-response questions. We discuss why this problem is challenging, define the nature of questions on which our method works, and finally propose a framework that instructors can use to evaluate their students' open-responses, utilizing near-domain data like data from similar questions administered in previous years. The proposed method outperforms the state of the art machine learning models as well as non-fine-tuned large language models like GPT 3.5, GPT 4, and GPT 4o by a considerable margin of over 10-20% in some cases, even after providing the LLMs with reference/model answers. Our framework does not require pre-written grading rubrics and is designed explicitly with practical classroom settings in mind. Our results also reveal exciting insights about learning from near-domain data, including what we term as accuracy and data advantages using human-labeled data, and we believe this is the first work to formalize the problem of automated short answer grading based on the near-domain data.", "AI": {"tldr": "提出一种利用近域数据（如往年类似问题数据）进行短答案构建式问题自动评分的实用框架，无需预写评分标准，在课堂实际环境中表现优于现有机器学习模型和未微调的大型语言模型。", "motivation": "构建式问题对测试学习者核心概念理解至关重要，但教师时间有限、班级规模大、资源约束等问题使得及时详细评估变得困难。手动评分劳动密集，自动评分又难以泛化到所有可能的回答场景。", "method": "提出一个实用框架，利用近域数据（如往年类似问题的数据）来评估学生的开放式回答。该方法不需要预写评分标准，专门针对实际课堂环境设计，利用人类标注数据实现准确性和数据优势。", "result": "该方法在性能上显著优于最先进的机器学习模型以及未微调的大型语言模型（如GPT 3.5、GPT 4、GPT 4o），在某些情况下优势超过10-20%，即使为LLMs提供了参考答案。这是首个基于近域数据形式化自动短答案评分问题的工作。", "conclusion": "该研究提出了一种实用且有效的自动评分方法，利用近域数据解决了构建式问题评分的挑战，为课堂环境提供了可行的解决方案，并揭示了从近域数据学习的有趣见解。"}}
{"id": "2512.04108", "pdf": "https://arxiv.org/pdf/2512.04108", "abs": "https://arxiv.org/abs/2512.04108", "authors": ["Swati Sachan", "Theo Miller", "Mai Phuong Nguyen"], "title": "Responsible LLM Deployment for High-Stake Decisions by Decentralized Technologies and Human-AI Interactions", "categories": ["cs.CY", "cs.AI", "cs.HC", "q-fin.CP"], "comment": "IEEE International Conference on Human-Machine Systems, 2025", "summary": "High-stakes decision domains are increasingly exploring the potential of Large Language Models (LLMs) for complex decision-making tasks. However, LLM deployment in real-world settings presents challenges in data security, evaluation of its capabilities outside controlled environments, and accountability attribution in the event of adversarial decisions. This paper proposes a framework for responsible deployment of LLM-based decision-support systems through active human involvement. It integrates interactive collaboration between human experts and developers through multiple iterations at the pre-deployment stage to assess the uncertain samples and judge the stability of the explanation provided by post-hoc XAI techniques. Local LLM deployment within organizations and decentralized technologies, such as Blockchain and IPFS, are proposed to create immutable records of LLM activities for automated auditing to enhance security and trace back accountability. It was tested on Bert-large-uncased, Mistral, and LLaMA 2 and 3 models to assess the capability to support responsible financial decisions on business lending.", "AI": {"tldr": "提出一个负责任部署LLM用于高风险决策的框架，结合人类参与、去中心化技术和人机交互", "motivation": "LLM在高风险决策领域的部署面临数据安全、能力评估和问责归属等挑战，需要建立负责任的部署框架", "method": "通过部署前阶段的人类专家与开发者多轮交互协作，评估不确定样本和事后XAI技术解释的稳定性；采用本地LLM部署和去中心化技术（区块链、IPFS）创建不可变记录用于自动审计", "result": "在Bert-large-uncased、Mistral、LLaMA 2和3模型上进行测试，评估其在商业贷款负责任金融决策支持方面的能力", "conclusion": "提出的框架能够通过人类参与和去中心化技术实现LLM在高风险决策领域的负责任部署，增强安全性和可追溯性"}}
{"id": "2512.04107", "pdf": "https://arxiv.org/pdf/2512.04107", "abs": "https://arxiv.org/abs/2512.04107", "authors": ["Shi Ding", "Brian Magerko"], "title": "Rethinking AI Evaluation in Education: The TEACH-AI Framework and Benchmark for Generative AI Assistants", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.LG"], "comment": "6 pages, NeurIPS 2025 Responsible Foundation Models Workshop", "summary": "As generative artificial intelligence (AI) continues to transform education, most existing AI evaluations rely primarily on technical performance metrics such as accuracy or task efficiency while overlooking human identity, learner agency, contextual learning processes, and ethical considerations. In this paper, we present TEACH-AI (Trustworthy and Effective AI Classroom Heuristics), a domain-independent, pedagogically grounded, and stakeholder-aligned framework with measurable indicators and a practical toolkit for guiding the design, development, and evaluation of generative AI systems in educational contexts. Built on an extensive literature review and synthesis, the ten-component assessment framework and toolkit checklist provide a foundation for scalable, value-aligned AI evaluation in education. TEACH-AI rethinks \"evaluation\" through sociotechnical, educational, theoretical, and applied lenses, engaging designers, developers, researchers, and policymakers across AI and education. Our work invites the community to reconsider what constructs \"effective\" AI in education and to design model evaluation approaches that promote co-creation, inclusivity, and long-term human, social, and educational impact.", "AI": {"tldr": "提出TEACH-AI框架，重新思考教育中生成式AI助手的评估方法，强调超越技术指标，关注教学价值和社会影响", "motivation": "现有AI评估主要依赖技术性能指标（如准确性、任务效率），忽视了人类身份、学习者能动性、情境化学习过程和伦理考量，需要更全面的评估框架", "method": "基于广泛的文献综述和综合，开发了TEACH-AI框架，包含十个组成部分的评估框架和工具包检查清单，采用社会技术、教育、理论和应用的多重视角", "result": "提出了一个领域无关、教学基础、利益相关者对齐的框架，具有可测量的指标和实用工具包，为教育中生成式AI系统的设计、开发和评估提供指导", "conclusion": "TEACH-AI框架邀请社区重新思考教育中\"有效\"AI的构成，设计促进共创、包容性以及长期人类、社会和教育影响的模型评估方法"}}
{"id": "2512.04106", "pdf": "https://arxiv.org/pdf/2512.04106", "abs": "https://arxiv.org/abs/2512.04106", "authors": ["Fouad Trad", "Ali Chehab"], "title": "Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "comment": "Accepted in the 3rd International Conference on Foundation and Large Language Models (FLLM2025)", "summary": "Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.", "AI": {"tldr": "比较检索增强的少样本提示与微调在代码漏洞检测任务中的效果，发现检索增强提示优于标准少样本提示和微调Gemini模型，但微调CodeBERT性能更高但需要更多资源。", "motivation": "少样本提示已成为利用大语言模型进行专业任务的实用替代方案，但其效果严重依赖于上下文示例的选择和质量，特别是在代码漏洞检测等复杂领域。需要探索如何通过检索增强策略提升少样本提示的性能。", "method": "使用Gemini-1.5-Flash模型进行系统评估，比较三种方法：1) 随机选择示例的标准少样本提示；2) 使用语义相似示例的检索增强提示；3) 基于检索的标签分配（无需模型推理）。进一步与零样本提示和多个微调模型（包括Gemini-1.5-Flash、DistilBERT、DistilGPT2和CodeBERT）进行比较。", "result": "检索增强提示在20个示例时达到74.05%的F1分数和83.90%的部分匹配准确率，显著优于标准少样本提示。它优于零样本提示（F1: 36.35%）和微调Gemini（F1: 59.31%），同时避免了微调的训练时间和成本。然而，微调CodeBERT获得最高性能（F1: 91.22%，部分匹配准确率: 91.30%）。", "conclusion": "检索增强提示是代码漏洞检测的有效方法，在性能与资源消耗之间取得了良好平衡。虽然微调CodeBERT性能最高，但检索增强提示避免了训练成本，为实际应用提供了实用的替代方案。"}}
{"id": "2512.04102", "pdf": "https://arxiv.org/pdf/2512.04102", "abs": "https://arxiv.org/abs/2512.04102", "authors": ["Rosana Caro", "Lorena Cruz", "Arturo Martinez", "Pablo S. Naharro", "Santiago Muelas", "Kevin King Sancho", "Elena Cuerda", "Maria del Mar Barbero-Barrera", "Antonio LaTorre"], "title": "Prescriptive tool for zero-emissions building fenestration design using hybrid metaheuristic algorithms", "categories": ["cs.NE"], "comment": "ef:Energy and Buildings, Volume 336, 1 June 2025, 115594", "summary": "Designing Zero-Emissions Buildings (ZEBs) involves balancing numerous complex objectives that traditional methods struggle to address. Fenestration, encompassing façade openings and shading systems, plays a critical role in ZEB performance due to its high thermal transmittance and solar radiation admission. This paper presents a novel simulation-based optimization method for fenestration designed for practical application. It uses a hybrid metaheuristic algorithm and relies on rules and an updatable catalog, to fully automate the design process, create a highly diverse search space, minimize biases, and generate detailed solutions ready for architectural prescription. Nineteen fenestration variables, over which architects have design flexibility, were optimized to reduce heating, cooling demand, and thermal discomfort in residential buildings. The method was tested across three Spanish climate zones. Results demonstrate that the considered optimization algorithm significantly outperforms the baseline Genetic Algorithm in both quality and robustness, with these differences proven to be statistically significant. Furthermore, the findings offer valuable insights for ZEB design, highlighting challenges in reducing cooling demand in warm climates, and showcasing the superior efficiency of automated movable shading systems compared to fixed solutions.", "AI": {"tldr": "提出一种基于混合元启发式算法的零排放建筑窗户设计规范工具，通过自动化优化19个窗户变量来平衡建筑能耗和热舒适性", "motivation": "传统方法难以处理零排放建筑设计中复杂的多目标平衡问题，窗户系统因其高热传导性和太阳辐射透过率对建筑性能影响重大，需要更有效的自动化设计工具", "method": "采用基于仿真的优化方法，结合混合元启发式算法、规则系统和可更新目录，完全自动化设计过程，优化19个窗户变量，在三个西班牙气候区进行测试", "result": "优化算法在质量和鲁棒性上显著优于基准遗传算法，统计差异显著；在温暖气候中减少制冷需求具有挑战性；自动化可移动遮阳系统比固定解决方案更高效", "conclusion": "该方法为实际应用的零排放建筑窗户设计提供了有效的自动化工具，能够生成详细的建筑规范解决方案，显著提升设计效率和性能"}}
{"id": "2512.04097", "pdf": "https://arxiv.org/pdf/2512.04097", "abs": "https://arxiv.org/abs/2512.04097", "authors": ["Isabelle Diana May-Xin Ng", "Tharindu Cyril Weerasooriya", "Haitao Zhu", "Wei Wei"], "title": "MultiGA: Leveraging Multi-Source Seeding in Genetic Algorithms", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across research domains to tackle complex tasks, but their performance can vary significantly depending on the task at hand. Evolutionary algorithms, inspired by natural selection, can be used to refine solutions iteratively at inference-time. To the best of our knowledge, there has not been exploration on leveraging the collective capabilities of multi-source seeding for LLM-guided genetic algorithms. In this paper, we introduce a novel approach, MultiGA, which applies genetic algorithm principles to address complex natural language tasks and reasoning problems by sampling from a diverse population of LLMs to initialize the population. MultiGA generates a range of outputs from various parent LLMs, open source and closed source, and uses a neutral fitness function to evaluate them. Through an iterative recombination process, we mix and refine these generations until an optimal solution is achieved. We benchmark our approach using text-to-SQL code generation tasks, trip planning, GPQA benchmark for grad-level science questions, and the BBQ bias benchmark. Our results show that MultiGA converges to the accuracy of the LLM best fit for the task, and these insights lay the foundation for future research looking closer at integrating multiple LLMs for unexplored tasks in which selecting only one pre-trained model is unclear or suboptimal.", "AI": {"tldr": "提出MultiGA方法，利用多源LLM种子进行遗传算法优化，解决复杂自然语言任务和推理问题", "motivation": "现有LLM在不同任务上表现差异显著，而进化算法可以通过迭代优化改进推理时性能。目前尚未探索利用多源种子进行LLM引导的遗传算法", "method": "MultiGA方法从多样化的开源和闭源LLM中采样初始化种群，使用中性适应度函数评估输出，通过迭代重组过程混合和精炼生成，直到获得最优解", "result": "在文本到SQL代码生成、旅行规划、GPQA科学问题和BBQ偏见基准测试中，MultiGA能够收敛到最适合任务的LLM的准确率水平", "conclusion": "MultiGA为未来研究奠定了基础，特别是在那些选择单一预训练模型不明确或次优的未探索任务中，集成多个LLM具有重要价值"}}
{"id": "2512.04094", "pdf": "https://arxiv.org/pdf/2512.04094", "abs": "https://arxiv.org/abs/2512.04094", "authors": ["Dongjian Yang", "Xiaoyuan Li", "Chuanmei Xi", "Ye Sun", "Gang Liu"], "title": "Memory-DD: A Low-Complexity Dendrite-Inspired Neuron for Temporal Prediction Tasks", "categories": ["cs.NE", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "Dendrite-inspired neurons have been widely used in tasks such as image classification due to low computational complexity and fast inference speed. Temporal data prediction, as a key machine learning task, plays a key role in real-time scenarios such as sensor data analysis, financial forecasting, and urban traffic management. However, existing dendrite-inspired neurons are mainly designed for static data. Studies on capturing dynamic features and modeling long-term dependencies in temporal sequences remain limited. Efficient architectures specifically designed for temporal sequence prediction are still lacking. In this paper, we propose Memory-DD, a low-complexity dendrite-inspired neuron model. Memory-DD consists of two dendrite-inspired neuron groups that contain no nonlinear activation functions but can still realize nonlinear mappings. Compared with traditional neurons without dendritic functions, Memory-DD requires only two neuron groups to extract logical relationships between features in input sequences. This design effectively captures temporal dependencies and is suitable for both classification and regression tasks on sequence data. Experimental results show that Memory-DD achieves an average accuracy of 89.41% on 18 temporal classification benchmark datasets, outperforming LSTM by 4.25%. On 9 temporal regression datasets, it reaches comparable performance to LSTM, while using only 50% of the parameters and reducing computational complexity (FLOPs) by 27.7%. These results demonstrate that Memory-DD successfully extends the low-complexity advantages of dendrite-inspired neurons to temporal prediction, providing a low-complexity and efficient solution for time-series data processing.", "AI": {"tldr": "提出Memory-DD，一种低复杂度的树突启发神经元模型，专门用于时间序列预测任务，通过两个无非线性激活函数的树突启发神经元组实现非线性映射，有效捕捉时间依赖关系。", "motivation": "现有树突启发神经元主要针对静态数据设计，在捕捉时间序列动态特征和建模长期依赖方面研究有限，缺乏专门为时间序列预测设计的高效架构。", "method": "设计Memory-DD模型，包含两个树突启发神经元组，无需非线性激活函数即可实现非线性映射，仅用两个神经元组提取输入序列特征间的逻辑关系，有效捕捉时间依赖。", "result": "在18个时间分类基准数据集上平均准确率达89.41%，比LSTM高4.25%；在9个时间回归数据集上性能与LSTM相当，但参数仅需50%，计算复杂度降低27.7%。", "conclusion": "Memory-DD成功将树突启发神经元的低复杂度优势扩展到时间预测领域，为时间序列数据处理提供了低复杂度、高效的解决方案。"}}
{"id": "2512.04087", "pdf": "https://arxiv.org/pdf/2512.04087", "abs": "https://arxiv.org/abs/2512.04087", "authors": ["Sui He", "Shenbin Qian"], "title": "Human-Centred Evaluation of Text-to-Image Generation Models for Self-expression of Mental Distress: A Dataset Based on GPT-4o", "categories": ["q-bio.NC", "cs.CL", "cs.CV", "cs.CY", "cs.HC"], "comment": null, "summary": "Effective communication is central to achieving positive healthcare outcomes in mental health contexts, yet international students often face linguistic and cultural barriers that hinder their communication of mental distress. In this study, we evaluate the effectiveness of AI-generated images in supporting self-expression of mental distress. To achieve this, twenty Chinese international students studying at UK universities were invited to describe their personal experiences of mental distress. These descriptions were elaborated using GPT-4o with four persona-based prompt templates rooted in contemporary counselling practice to generate corresponding images. Participants then evaluated the helpfulness of generated images in facilitating the expression of their feelings based on their original descriptions. The resulting dataset comprises 100 textual descriptions of mental distress, 400 generated images, and corresponding human evaluation scores. Findings indicate that prompt design substantially affects perceived helpfulness, with the illustrator persona achieving the highest ratings. This work introduces the first publicly available text-to-image evaluation dataset with human judgment scores in the mental health domain, offering valuable resources for image evaluation, reinforcement learning with human feedback, and multi-modal research on mental health communication.", "AI": {"tldr": "评估AI生成图像在支持心理困扰自我表达方面的有效性，创建首个心理健康领域带有人类评分的大规模文本到图像评估数据集", "motivation": "国际学生在心理健康沟通中面临语言和文化障碍，需要探索AI生成图像是否能有效支持心理困扰的自我表达，改善心理健康沟通效果", "method": "邀请20名在英国留学的中国国际学生描述心理困扰经历，使用GPT-4o基于四种心理咨询实践的角色提示模板生成图像，参与者根据原始描述评估生成图像在表达感受方面的帮助性", "result": "创建了包含100个心理困扰文本描述、400张生成图像及对应人类评估分数的数据集；发现提示设计显著影响感知帮助性，插画师角色获得最高评分", "conclusion": "该研究提供了心理健康领域首个公开的带有人类评分的文本到图像评估数据集，为图像评估、基于人类反馈的强化学习和心理健康沟通的多模态研究提供了宝贵资源"}}
{"id": "2512.04084", "pdf": "https://arxiv.org/pdf/2512.04084", "abs": "https://arxiv.org/abs/2512.04084", "authors": ["Qinyu Zhao", "Guangting Zheng", "Tao Yang", "Rui Zhu", "Xingjian Leng", "Stephen Gould", "Liang Zheng"], "title": "SimFlow: Simplified and End-to-End Training of Latent Normalizing Flows", "categories": ["cs.CV"], "comment": "Project Page: https://qinyu-allen-zhao.github.io/SimFlow/", "summary": "Normalizing Flows (NFs) learn invertible mappings between the data and a Gaussian distribution. Prior works usually suffer from two limitations. First, they add random noise to training samples or VAE latents as data augmentation, introducing complex pipelines including extra noising and denoising steps. Second, they use a pretrained and frozen VAE encoder, resulting in suboptimal reconstruction and generation quality. In this paper, we find that the two issues can be solved in a very simple way: just fixing the variance (which would otherwise be predicted by the VAE encoder) to a constant (e.g., 0.5). On the one hand, this method allows the encoder to output a broader distribution of tokens and the decoder to learn to reconstruct clean images from the augmented token distribution, avoiding additional noise or denoising design. On the other hand, fixed variance simplifies the VAE evidence lower bound, making it stable to train an NF with a VAE jointly. On the ImageNet $256 \\times 256$ generation task, our model SimFlow obtains a gFID score of 2.15, outperforming the state-of-the-art method STARFlow (gFID 2.40). Moreover, SimFlow can be seamlessly integrated with the end-to-end representation alignment (REPA-E) method and achieves an improved gFID of 1.91, setting a new state of the art among NFs.", "AI": {"tldr": "SimFlow提出了一种简化的端到端训练方法，通过固定VAE编码器的方差来解决潜在归一化流训练中的两个主要问题，实现了更好的重建和生成质量。", "motivation": "现有归一化流方法存在两个主要问题：1) 需要在训练样本或VAE潜在表示中添加随机噪声作为数据增强，导致复杂的去噪流程；2) 使用预训练且冻结的VAE编码器，导致次优的重建和生成质量。", "method": "提出将VAE编码器预测的方差固定为常数（如0.5），而不是让编码器预测方差。这种方法允许编码器输出更广泛的标记分布，解码器学习从增强的标记分布中重建干净图像，避免了额外的噪声或去噪设计。", "result": "在ImageNet 256×256生成任务中，SimFlow获得了2.15的gFID分数，优于最先进方法STARFlow（gFID 2.40）。与REPA-E方法结合后，gFID进一步提升至1.91，在归一化流方法中创造了新的最先进水平。", "conclusion": "通过简单地将VAE方差固定为常数，SimFlow解决了归一化流训练中的两个关键问题，实现了简化的端到端训练流程，并在生成质量上取得了最先进的性能。"}}
{"id": "2512.04082", "pdf": "https://arxiv.org/pdf/2512.04082", "abs": "https://arxiv.org/abs/2512.04082", "authors": ["Jiazhe Wei", "Ken Li", "Tianyu Lao", "Haofan Wang", "Liang Wang", "Caifeng Shan", "Chenyang Si"], "title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design", "categories": ["cs.CV"], "comment": "Project page: https://postercopilot.github.io/", "summary": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design.", "AI": {"tldr": "开发PosterCopilot框架，通过增强大型多模态模型的布局推理能力和可控编辑功能，实现专业级平面设计自动化", "motivation": "现有基于大型多模态模型的自动化设计方法存在几何布局不准确、缺乏专业工作流程所需的迭代式图层特定编辑能力等问题，无法满足专业平面设计需求", "method": "提出渐进式三阶段训练策略：1) 扰动监督微调；2) 视觉-现实对齐的强化学习；3) 美学反馈的强化学习。同时开发完整工作流程，将训练好的设计模型与生成模型结合，实现图层可控的迭代编辑", "result": "实验表明PosterCopilot能够生成几何精确且美学优越的布局，为专业迭代设计提供前所未有的可控性", "conclusion": "PosterCopilot框架通过增强布局推理和可控编辑能力，有效解决了专业平面设计中的几何准确性和迭代编辑需求，推动了自动化设计向专业级应用发展"}}
{"id": "2512.04072", "pdf": "https://arxiv.org/pdf/2512.04072", "abs": "https://arxiv.org/abs/2512.04072", "authors": ["Zayne Sprague", "Jack Lu", "Manya Wadhwa", "Sedrick Keh", "Mengye Ren", "Greg Durrett"], "title": "SkillFactory: Self-Distillation For Learning Cognitive Behaviors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.", "AI": {"tldr": "提出SkillFactory方法，通过自蒸馏在监督微调阶段学习认知技能，为后续强化学习做准备", "motivation": "当基础语言模型不具备某些认知技能（如答案验证、回溯、替代方法重试等）时，如何让模型学会利用这些技能？传统方法依赖从更强模型蒸馏，但SkillFactory探索不依赖外部模型的自蒸馏方法", "method": "SkillFactory方法在监督微调阶段使用模型自身生成的样本，重新排列成认知技能格式的训练数据。这些\"银牌\"SFT轨迹可能不完美，但能有效为后续强化学习阶段获取技能做准备", "result": "评估显示：(1)从SkillFactory SFT初始化开始，尽管RL前性能较低，但能帮助模型在RL后泛化到更难的任务变体；(2)模型确实使用了认知技能；(3)经过RL的SkillFactory模型在域外任务上的回归鲁棒性优于仅用RL的基础模型", "conclusion": "研究表明，在强化学习之前学习的归纳偏置有助于模型学习鲁棒的认知技能使用，自蒸馏方法能有效为模型获取复杂推理技能奠定基础"}}
{"id": "2512.04069", "pdf": "https://arxiv.org/pdf/2512.04069", "abs": "https://arxiv.org/abs/2512.04069", "authors": ["Siyi Chen", "Mikaela Angelina Uy", "Chan Hee Song", "Faisal Ladhak", "Adithyavairavan Murali", "Qing Qu", "Stan Birchfield", "Valts Blukis", "Jonathan Tremblay"], "title": "SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.", "AI": {"tldr": "提出Double Interactive RL框架，让视觉语言模型学习协调使用多种工具进行精确空间推理", "motivation": "视觉语言模型在定性视觉理解方面表现良好，但在需要精确度量的空间推理任务上存在困难，而现有方法要么依赖手工提示策略，要么使用固定的预定义工具管道，限制了模型发现最优工具使用模式的能力", "method": "提出双阶段交互式强化学习框架：1) 教学阶段：结合单一工具专家的交互式RL演示与前沿模型使用所有工具的轨迹；2) 探索阶段：通过持续RL进一步优化多工具协调", "result": "在空间理解基准测试（RoboSpatial-Home, BLINK, BOP-ASK）上达到最先进性能，相比普通SFT基线提升12%，相比RL基线提升16%，并能在7自由度机器人上实现可靠的现实世界操作", "conclusion": "DIRL框架有效解决了VLMs在多工具协调中的挑战，通过交互式学习实现了精确的空间推理能力，为具身应用提供了强大的工具增强空间推理解决方案"}}
{"id": "2512.04065", "pdf": "https://arxiv.org/pdf/2512.04065", "abs": "https://arxiv.org/abs/2512.04065", "authors": ["Ashlesha Gopinath Sawant", "Sahil S. Jadhav", "Vidhan R. Jain", "Shriraj S. Jagtap", "Prachi Jadhav", "Soham Jadhav", "Ichha Raina"], "title": "Fare Comparison App of Uber, Ola and Rapido", "categories": ["cs.LG", "cs.AI"], "comment": "4 pages", "summary": "In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.", "AI": {"tldr": "开发一个比较Uber、Ola和Rapido车费的应用，帮助用户选择最优出行方案", "motivation": "用户在选择网约车服务时面临困难，难以找到既经济又高效的出行方案，需要透明化的比价工具", "method": "使用Python开发Web应用，通过API获取各平台实时车费数据，利用Android Studio模拟器、Appium进行数据采集和位置比较", "result": "开发了一个能够比较Uber、Ola和Rapido车费的Web应用，为用户提供最优出行选择", "conclusion": "该项目提高了网约车服务的透明度，增强了用户体验和出行效率，解决了API数据获取等技术挑战"}}
{"id": "2512.04048", "pdf": "https://arxiv.org/pdf/2512.04048", "abs": "https://arxiv.org/abs/2512.04048", "authors": ["Sen Fang", "Yalin Feng", "Hongbin Zhong", "Yanxin Zhang", "Dimitris N. Metaxas"], "title": "Stable Signer: Hierarchical Sign Language Generative Model", "categories": ["cs.CV", "cs.CL", "cs.CY"], "comment": "12 pages, 7 figures. More Demo at https://stablesigner.github.io", "summary": "Sign Language Production (SLP) is the process of converting the complex input text into a real video. Most previous works focused on the Text2Gloss, Gloss2Pose, Pose2Vid stages, and some concentrated on Prompt2Gloss and Text2Avatar stages. However, this field has made slow progress due to the inaccuracy of text conversion, pose generation, and the rendering of poses into real human videos in these stages, resulting in gradually accumulating errors. Therefore, in this paper, we streamline the traditional redundant structure, simplify and optimize the task objective, and design a new sign language generative model called Stable Signer. It redefines the SLP task as a hierarchical generation end-to-end task that only includes text understanding (Prompt2Gloss, Text2Gloss) and Pose2Vid, and executes text understanding through our proposed new Sign Language Understanding Linker called SLUL, and generates hand gestures through the named SLP-MoE hand gesture rendering expert block to end-to-end generate high-quality and multi-style sign language videos. SLUL is trained using the newly developed Semantic-Aware Gloss Masking Loss (SAGM Loss). Its performance has improved by 48.6% compared to the current SOTA generation methods.", "AI": {"tldr": "提出了一种名为Stable Signer的新型手语生成模型，通过简化传统冗余结构，将手语生成任务重新定义为包含文本理解和姿态到视频的层次化端到端任务。", "motivation": "传统手语生成方法存在文本转换不准确、姿态生成错误以及姿态渲染到真实视频的误差累积问题，导致该领域进展缓慢。", "method": "设计了新的手语理解链接器SLUL，使用新开发的语义感知手语掩码损失进行训练，并通过名为SLP-MoE的手势渲染专家模块生成手势，实现端到端高质量多风格手语视频生成。", "result": "性能相比当前最先进的生成方法提升了48.6%。", "conclusion": "通过简化任务目标和优化模型结构，提出的Stable Signer模型能够有效解决传统手语生成中的误差累积问题，显著提升生成质量。"}}
{"id": "2512.04047", "pdf": "https://arxiv.org/pdf/2512.04047", "abs": "https://arxiv.org/abs/2512.04047", "authors": ["Nadav Kunievsky"], "title": "Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs", "categories": ["econ.GN", "cs.AI", "cs.CY"], "comment": ":K.4", "summary": "In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.", "AI": {"tldr": "本文探讨了AI技术如何降低说服成本，使精英能够战略性设计公众偏好分布，从而将极化作为治理工具而非社会副产品。", "motivation": "随着AI驱动说服技术的进步，精英塑造公众偏好的成本大幅降低、精度提高，这使得偏好分布本身成为可设计的对象。传统民主理论中，精英只能通过教育和媒体等有限手段影响公众意见，而新技术改变了这一格局，需要重新思考极化现象的本质。", "method": "构建动态模型，分析精英如何在说服成本和多数规则约束下选择重塑政策偏好分布。分别考虑单一精英和两个对立精英交替执政的情况，研究技术进步对极化趋势的影响。", "result": "单一精英统治下，最优干预倾向于推动社会向更极化的意见分布发展（\"极化拉力\"），说服技术改进加速这一趋势。两个对立精英交替执政时，相同技术也创造激励，使社会停留在\"半锁定\"区域，这些区域意见更凝聚、更难被对手推翻。技术进步可能加剧或抑制极化，取决于具体环境。", "conclusion": "更廉价的说服技术将极化重塑为治理的战略工具，而非纯粹的社会副产品。随着AI能力发展，这对民主稳定性具有重要影响，需要重新思考民主制度如何应对精英通过新技术战略性塑造公众偏好的挑战。"}}
{"id": "2512.04044", "pdf": "https://arxiv.org/pdf/2512.04044", "abs": "https://arxiv.org/abs/2512.04044", "authors": ["Yizhou Zhao", "Zhiwei Steven Wu", "Adam Block"], "title": "MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.", "AI": {"tldr": "MarkTune是一个用于开放权重语言模型水印的微调框架，通过将水印信号作为奖励进行优化，在保持文本生成质量的同时提高水印可检测性。", "motivation": "开放权重语言模型对水印技术提出了特殊挑战，因为一旦模型权重公开，传统的推理时干预方法就无法强制执行。现有方法如GaussMark需要在生成质量和检测能力之间进行权衡，通常需要较大的权重扰动才能获得足够的检测能力，但这会显著降低生成质量。", "method": "MarkTune是一个理论上有原则的、基于策略的微调框架，将GaussMark水印信号作为奖励函数，同时通过正则化防止文本质量下降。该方法在模型的表示空间中进行更精细的、水印感知的权重更新。", "result": "MarkTune在质量-可检测性权衡方面持续优于GaussMark，将GaussMark的性能边界推近到接近推理时水印的水平。该方法对改写和微调攻击具有鲁棒性，并表现出强大的泛化能力：在一个数据集上微调的模型在未见数据集上仍保持显著的水印检测能力。", "conclusion": "MarkTune为在开放权重语言模型中嵌入鲁棒、高质量水印提供了一种通用策略，有效解决了开放权重模型水印中的质量-可检测性权衡问题。"}}
{"id": "2512.04040", "pdf": "https://arxiv.org/pdf/2512.04040", "abs": "https://arxiv.org/abs/2512.04040", "authors": ["Yicong Hong", "Yiqun Mei", "Chongjian Ge", "Yiran Xu", "Yang Zhou", "Sai Bi", "Yannick Hold-Geoffroy", "Mike Roberts", "Matthew Fisher", "Eli Shechtman", "Kalyan Sunkavalli", "Feng Liu", "Zhengqi Li", "Hao Tan"], "title": "RELIC: Interactive Video World Model with Long-Horizon Memory", "categories": ["cs.CV"], "comment": "22 pages", "summary": "A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.", "AI": {"tldr": "RELIC是一个统一的交互式视频世界模型框架，能够实现实时长时记忆、一致空间记忆和精确用户控制，支持基于单张图像和文本描述的长时间场景探索。", "motivation": "现有方法通常只能解决交互式世界模型中的一个方面（实时性、长时记忆或用户控制），而无法同时满足这三个关键需求。特别是长时记忆机制往往会降低实时性能，因此需要一种统一框架来同时解决这些挑战。", "method": "基于自回归视频扩散蒸馏技术，使用高度压缩的历史潜在令牌（包含相对动作和绝对相机姿态）存储在KV缓存中作为长时记忆。采用双向教师视频模型进行微调以超越原始5秒训练时长，并通过新的内存高效自强制范式将其转换为因果学生生成器，实现全上下文蒸馏。", "result": "RELIC作为140亿参数模型，在精心策划的Unreal Engine渲染数据集上训练，实现了16 FPS的实时生成，相比先前工作表现出更准确的动作跟随、更稳定的长时流式生成和更鲁棒的空间记忆检索能力。", "conclusion": "RELIC通过统一的框架成功解决了交互式世界模型的三个关键挑战，为下一代交互式世界建模奠定了坚实基础，展示了在实时性、长时记忆和用户控制方面的卓越性能。"}}
{"id": "2512.04039", "pdf": "https://arxiv.org/pdf/2512.04039", "abs": "https://arxiv.org/abs/2512.04039", "authors": ["Sandeep Nagar"], "title": "Fast & Efficient Normalizing Flows and Applications of Image Generative Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "PhD Thesis", "summary": "This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance. The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.", "AI": {"tldr": "该论文在两个方面做出贡献：一是改进归一化流模型的效率，包括可逆卷积层、Quad-coupling层、并行反演算法等创新；二是将生成模型应用于实际计算机视觉问题，包括农产品质量评估、地质制图、隐私保护和艺术修复等。", "motivation": "论文旨在解决生成模型（特别是归一化流）的效率问题，并展示生成模型在解决现实世界计算机视觉挑战中的实际应用价值。归一化流虽然理论上优雅，但在实践中存在计算效率低的问题，需要更高效的架构和算法。同时，生成模型在农业、地质、隐私保护、艺术修复等领域有广泛应用潜力。", "method": "第一部分：提出六项归一化流架构改进：1) 可逆3x3卷积层的数学可逆性条件；2) 更高效的Quad-coupling层；3) kxk卷积层的并行反演算法；4) 卷积逆运算的高效反向传播算法；5) Inverse-Flow中使用卷积逆进行前向传播；6) Affine-StableSR超分辨率模型。第二部分：应用生成模型解决实际问题：1) 使用条件GAN进行农产品质量评估；2) 堆叠自编码器进行地质制图；3) 基于人脸检测和图像修复的隐私保护方法；4) Stable Diffusion图像修复用于隐私保护；5) 适应扩散模型进行艺术修复。", "result": "论文在归一化流效率方面取得了显著改进，包括数学证明的可逆卷积条件、更高效的耦合层设计、并行反演算法等。在应用方面，农产品质量评估系统在种子纯度测试中取得了良好准确率；地质制图框架相比传统方法改进了特征提取；隐私保护方法有效保护了自动驾驶数据集中的敏感信息；艺术修复模型能够统一处理多种退化类型。", "conclusion": "该论文通过技术创新显著提升了归一化流模型的效率，并成功展示了生成模型在多个实际计算机视觉应用中的有效性。这些贡献既推进了生成模型的理论发展，又为农业、地质、隐私保护、文化遗产保护等领域的实际问题提供了实用解决方案，体现了生成模型在现实世界中的广泛应用前景。"}}
{"id": "2512.04032", "pdf": "https://arxiv.org/pdf/2512.04032", "abs": "https://arxiv.org/abs/2512.04032", "authors": ["Andreas Koukounas", "Georgios Mastrapas", "Florian Hönicke", "Sedigheh Eslami", "Guillaume Roncari", "Scott Martens", "Han Xiao"], "title": "Jina-VLM: Small Multilingual Vision Language Model", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "18 pages, 1-7 main content, 13-18 appendix for tables and dataset", "summary": "We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. The model achieves leading results on standard VQA benchmarks and multilingual evaluations while preserving competitive text-only performance. Model weights and code are publicly released at https://huggingface.co/jinaai/jina-vlm .", "AI": {"tldr": "Jina-VLM是一个2.4B参数的多语言视觉语言模型，在2B规模的开源VLM中实现了最先进的多语言视觉问答性能。", "motivation": "开发一个能够在多语言环境下进行高效视觉问答的轻量级视觉语言模型，同时保持文本性能的竞争力。", "method": "将SigLIP2视觉编码器与Qwen3语言主干通过注意力池化连接器耦合，实现任意分辨率图像的高效token处理。", "result": "在标准VQA基准测试和多语言评估中取得领先结果，同时保持竞争力的纯文本性能。", "conclusion": "Jina-VLM展示了在2B参数规模下实现多语言视觉问答最先进性能的可行性，模型权重和代码已公开。"}}
{"id": "2512.04025", "pdf": "https://arxiv.org/pdf/2512.04025", "abs": "https://arxiv.org/abs/2512.04025", "authors": ["Xiaolong Li", "Youping Gu", "Xi Lin", "Weijie Wang", "Bohan Zhuang"], "title": "PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Tech report", "summary": "Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA", "AI": {"tldr": "提出金字塔稀疏注意力（PSA）机制，用于高效视频理解和生成任务，通过多级池化KV表示来减少信息损失，在保持计算效率的同时提升性能", "motivation": "注意力机制是基础模型的核心，但其二次复杂度限制了模型扩展。现有稀疏注意力方法通常使用二进制掩码保留或丢弃整个KV块，在高稀疏度下会导致显著信息损失", "method": "提出金字塔稀疏注意力（PSA），使用多级池化的KV表示替代二进制掩码。每个查询块动态分配较低的池化级别给关键KV块，较高的池化级别给次要KV块，在完全保留和完全剪枝之间创建信息插值。采用解耦的块-瓦片设计和硬件友好的内核实现", "result": "在视频理解和生成基准测试中，PSA能够保留上下文信息和视觉保真度，在保持计算效率的同时，性能优于或与现有稀疏注意力基线相当，展现出更好的效率-质量权衡", "conclusion": "PSA是一种通用的高效注意力机制，通过多级池化KV表示有效缓解了稀疏注意力中的信息损失问题，为视频理解和生成任务提供了更好的计算效率与性能平衡"}}
{"id": "2512.04021", "pdf": "https://arxiv.org/pdf/2512.04021", "abs": "https://arxiv.org/abs/2512.04021", "authors": ["Honggyu An", "Jaewoo Jung", "Mungyeom Kim", "Sunghwan Hong", "Chaehyun Kim", "Kazumi Fukuda", "Minkyeong Jeon", "Jisang Han", "Takuya Narihira", "Hyuna Ko", "Junsu Kim", "Yuki Mitsufuji", "Seungryong Kim"], "title": "C3G: Learning Compact 3D Representations with 2K Gaussians", "categories": ["cs.CV"], "comment": "Project Page : https://cvlab-kaist.github.io/C3G/", "summary": "Reconstructing and understanding 3D scenes from unposed sparse views in a feed-forward manner remains as a challenging task in 3D computer vision. Recent approaches use per-pixel 3D Gaussian Splatting for reconstruction, followed by a 2D-to-3D feature lifting stage for scene understanding. However, they generate excessive redundant Gaussians, causing high memory overhead and sub-optimal multi-view feature aggregation, leading to degraded novel view synthesis and scene understanding performance. We propose C3G, a novel feed-forward framework that estimates compact 3D Gaussians only at essential spatial locations, minimizing redundancy while enabling effective feature lifting. We introduce learnable tokens that aggregate multi-view features through self-attention to guide Gaussian generation, ensuring each Gaussian integrates relevant visual features across views. We then exploit the learned attention patterns for Gaussian decoding to efficiently lift features. Extensive experiments on pose-free novel view synthesis, 3D open-vocabulary segmentation, and view-invariant feature aggregation demonstrate our approach's effectiveness. Results show that a compact yet geometrically meaningful representation is sufficient for high-quality scene reconstruction and understanding, achieving superior memory efficiency and feature fidelity compared to existing methods.", "AI": {"tldr": "提出C3G框架，通过生成紧凑的3D高斯表示来解决无姿态稀疏视图下的3D场景重建与理解问题，显著减少内存开销并提升特征聚合效果。", "motivation": "现有方法使用逐像素3D高斯溅射进行重建，然后通过2D到3D特征提升进行场景理解，但会产生大量冗余高斯，导致高内存开销和次优的多视图特征聚合，从而降低新视角合成和场景理解性能。", "method": "提出C3G框架，在关键空间位置估计紧凑的3D高斯，最小化冗余同时实现有效特征提升。引入可学习token通过自注意力聚合多视图特征来指导高斯生成，确保每个高斯整合相关视觉特征。利用学习的注意力模式进行高斯解码以高效提升特征。", "result": "在无姿态新视角合成、3D开放词汇分割和视图不变特征聚合方面的实验证明方法有效性。结果显示紧凑但几何有意义的表示足以实现高质量场景重建和理解，相比现有方法具有更优的内存效率和特征保真度。", "conclusion": "C3G框架通过生成紧凑的3D高斯表示，在减少冗余的同时实现了有效的特征提升，为3D场景重建和理解提供了一种高效且高质量的解决方案。"}}
{"id": "2512.04019", "pdf": "https://arxiv.org/pdf/2512.04019", "abs": "https://arxiv.org/abs/2512.04019", "authors": ["Ho Man Kwan", "Tianhao Peng", "Ge Gao", "Fan Zhang", "Mike Nilsson", "Andrew Gower", "David Bull"], "title": "Ultra-lightweight Neural Video Representation Compression", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent works have demonstrated the viability of utilizing over-fitted implicit neural representations (INRs) as alternatives to autoencoder-based models for neural video compression. Among these INR-based video codecs, Neural Video Representation Compression (NVRC) was the first to adopt a fully end-to-end compression framework that compresses INRs, achieving state-of-the-art performance. Moreover, some recently proposed lightweight INRs have shown comparable performance to their baseline codecs with computational complexity lower than 10kMACs/pixel. In this work, we extend NVRC toward lightweight representations, and propose NVRC-Lite, which incorporates two key changes. Firstly, we integrated multi-scale feature grids into our lightweight neural representation, and the use of higher resolution grids significantly improves the performance of INRs at low complexity. Secondly, we address the issue that existing INRs typically leverage autoregressive models for entropy coding: these are effective but impractical due to their slow coding speed. In this work, we propose an octree-based context model for entropy coding high-dimensional feature grids, which accelerates the entropy coding module of the model. Our experimental results demonstrate that NVRC-Lite outperforms C3, one of the best lightweight INR-based video codecs, with up to 21.03% and 23.06% BD-rate savings when measured in PSNR and MS-SSIM, respectively, while achieving 8.4x encoding and 2.5x decoding speedup. The implementation of NVRC-Lite will be made available.", "AI": {"tldr": "提出NVRC-Lite，一种超轻量级神经视频表示压缩方法，通过多尺度特征网格和八叉树上下文模型改进现有INR-based视频编解码器", "motivation": "现有基于隐式神经表示(INR)的视频压缩方法中，NVRC虽然性能优异但计算复杂度高；而轻量级INRs虽然复杂度低但性能不足。同时，现有INRs通常使用自回归模型进行熵编码，虽然有效但编码速度慢，不实用", "method": "1) 将多尺度特征网格集成到轻量级神经表示中，使用更高分辨率网格提升低复杂度INRs性能；2) 提出基于八叉树的上下文模型用于高维特征网格的熵编码，加速熵编码模块", "result": "NVRC-Lite在PSNR和MS-SSIM指标上分别比最佳轻量级INR-based视频编解码器C3节省21.03%和23.06%的BD-rate，同时实现8.4倍编码加速和2.5倍解码加速", "conclusion": "NVRC-Lite成功将NVRC扩展到轻量级表示，通过多尺度特征网格和八叉树上下文模型，在保持高性能的同时显著降低计算复杂度并加速编码过程，为实用化INR-based视频压缩提供了有效解决方案"}}
{"id": "2512.04016", "pdf": "https://arxiv.org/pdf/2512.04016", "abs": "https://arxiv.org/abs/2512.04016", "authors": ["Davut Emre Tasar", "Ceren Ocal Tasar"], "title": "TARA Test-by-Adaptive-Ranks for Quantum Anomaly Detection with Conformal Prediction Guarantees", "categories": ["quant-ph", "cs.AI"], "comment": null, "summary": "Quantum key distribution (QKD) security fundamentally relies on the ability to distinguish genuine quantum correlations from classical eavesdropper simulations, yet existing certification methods lack rigorous statistical guarantees under finite-sample conditions and adversarial scenarios. We introduce TARA (Test by Adaptive Ranks), a novel framework combining conformal prediction with sequential martingale testing for quantum anomaly detection that provides distribution-free validity guarantees. TARA offers two complementary approaches. TARA k, based on Kolmogorov Smirnov calibration against local hidden variable (LHV) null distributions, achieving ROC AUC = 0.96 for quantum-classical discrimination. And TARA-m, employing betting martingales for streaming detection with anytime valid type I error control that enables real time monitoring of quantum channels. We establish theoretical guarantees proving that under (context conditional) exchangeability, conformal p-values remain uniformly distributed even for strongly contextual quantum data, confirming that quantum contextuality does not break conformal prediction validity a result with implications beyond quantum certification to any application of distribution-free methods to nonclassical data. Extensive validation on both IBM Torino (superconducting, CHSH = 2.725) and IonQ Forte Enterprise (trapped ion, CHSH = 2.716) quantum processors demonstrates cross-platform robustness, achieving 36% security margins above the classical CHSH bound of 2. Critically, our framework reveals a methodological concern affecting quantum certification more broadly: same-distribution calibration can inflate detection performance by up to 44 percentage points compared to proper cross-distribution calibration, suggesting that prior quantum certification studies using standard train test splits may have systematically overestimated adversarial robustness.", "AI": {"tldr": "提出TARA框架，结合保形预测和序列鞅检验，为量子异常检测提供分布无关的有效性保证，用于区分量子关联与经典窃听者模拟。", "motivation": "现有量子密钥分发认证方法在有限样本条件和对抗场景下缺乏严格的统计保证，需要一种能够提供分布无关有效性保证的量子异常检测框架。", "method": "TARA框架包含两种互补方法：TARA-k基于Kolmogorov-Smirnov校准对抗局部隐变量零分布；TARA-m采用投注鞅进行流式检测，实现实时类型I错误控制。框架结合保形预测和序列鞅检验。", "result": "在IBM Torino和IonQ Forte Enterprise量子处理器上验证，实现高于经典CHSH边界2的36%安全裕度。ROC AUC达到0.96用于量子-经典区分。发现同分布校准可能使检测性能虚增44个百分点。", "conclusion": "TARA为量子异常检测提供分布无关的有效性保证，证明量子上下文性不会破坏保形预测有效性。揭示量子认证中同分布校准可能导致对抗鲁棒性系统性高估的方法论问题。"}}
{"id": "2512.04015", "pdf": "https://arxiv.org/pdf/2512.04015", "abs": "https://arxiv.org/abs/2512.04015", "authors": ["Farhana Hossain Swarnali", "Miaomiao Zhang", "Tonmoy Hossain"], "title": "Learning Group Actions In Disentangled Latent Image Representations", "categories": ["cs.CV"], "comment": null, "summary": "Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .", "AI": {"tldr": "提出一种端到端框架，首次在潜在图像流形上学习群作用，自动发现变换相关结构而无需人工干预", "motivation": "现有方法通常在数据空间统一应用群作用，难以解耦变换相关的子空间；潜在空间方法虽然更灵活，但仍需手动划分等变和不变子空间，限制了在表示空间中鲁棒学习和操作群作用的能力", "method": "使用可学习的二元掩码配合直通估计，动态划分潜在表示为变换敏感和不变组件；在统一优化框架中联合学习潜在解耦和群变换映射；可与任何标准编码器-解码器架构无缝集成", "result": "在五个2D/3D图像数据集上验证了方法，证明能自动学习群作用的解耦潜在因子；下游分类任务确认了学习表示的有效性", "conclusion": "提出了一种新颖的端到端框架，首次实现了在潜在图像流形上学习群作用，自动发现变换相关结构，为可控图像变换提供了更灵活和自动化的解决方案"}}
{"id": "2512.04012", "pdf": "https://arxiv.org/pdf/2512.04012", "abs": "https://arxiv.org/abs/2512.04012", "authors": ["Jisang Han", "Sunghwan Hong", "Jaewoo Jung", "Wooseok Jang", "Honggyu An", "Qianqian Wang", "Seungryong Kim", "Chen Feng"], "title": "Emergent Outlier View Rejection in Visual Geometry Grounded Transformers", "categories": ["cs.CV"], "comment": "Project page: https://cvlab-kaist.github.io/RobustVGGT/", "summary": "Reliable 3D reconstruction from in-the-wild image collections is often hindered by \"noisy\" images-irrelevant inputs with little or no view overlap with others. While traditional Structure-from-Motion pipelines handle such cases through geometric verification and outlier rejection, feed-forward 3D reconstruction models lack these explicit mechanisms, leading to degraded performance under in-the-wild conditions. In this paper, we discover that the existing feed-forward reconstruction model, e.g., VGGT, despite lacking explicit outlier-rejection mechanisms or noise-aware training, can inherently distinguish distractor images. Through an in-depth analysis under varying proportions of synthetic distractors, we identify a specific layer that naturally exhibits outlier-suppressing behavior. Further probing reveals that this layer encodes discriminative internal representations that enable an effective noise-filtering capability, which we simply leverage to perform outlier-view rejection in feed-forward 3D reconstruction without any additional fine-tuning or supervision. Extensive experiments on both controlled and in-the-wild datasets demonstrate that this implicit filtering mechanism is consistent and generalizes well across diverse scenarios.", "AI": {"tldr": "该论文发现视觉几何基础Transformer模型(VGGT)在缺乏显式异常值拒绝机制的情况下，能够通过特定层的内部表征自然地识别和抑制噪声图像，从而提升野外图像集合的3D重建质量。", "motivation": "野外图像集合中的\"噪声\"图像（与其他图像重叠度低或无关的输入）会严重影响3D重建质量。传统SfM流水线通过几何验证和异常值拒绝处理此类问题，但前馈式3D重建模型缺乏这些显式机制，导致在野外条件下性能下降。", "method": "通过在不同比例合成干扰图像下的深入分析，识别出VGGT模型中自然表现出异常值抑制行为的特定层。进一步探究发现该层编码了判别性内部表征，使其具备有效的噪声过滤能力。利用这一发现，无需额外微调或监督即可在前馈式3D重建中执行异常视图拒绝。", "result": "在受控和野外数据集上的大量实验表明，这种隐式过滤机制具有一致性，并能很好地泛化到不同场景。该方法能够有效区分干扰图像，提升3D重建的可靠性。", "conclusion": "即使没有显式的异常值拒绝机制或噪声感知训练，现有的前馈式重建模型（如VGGT）也具备内在的区分干扰图像的能力。通过利用模型特定层的异常值抑制行为，可以实现无需额外训练的有效噪声过滤，为野外条件下的3D重建提供了新思路。"}}
{"id": "2512.04007", "pdf": "https://arxiv.org/pdf/2512.04007", "abs": "https://arxiv.org/abs/2512.04007", "authors": ["Marcelo Isaias de Moraes Junior", "Moacir Antonelli Ponti"], "title": "On the Temporality for Sketch Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.", "AI": {"tldr": "研究草图表示学习中时间顺序的重要性，探讨将草图视为序列是否合理以及不同内部顺序对表示质量的影响", "motivation": "尽管草图表示学习领域已取得显著进展，但时间顺序对表示质量的实际相关性仍存在理解空白。需要研究将草图视为序列是否合理，以及哪些内部顺序起更重要作用", "method": "通过比较传统位置编码方法，分析绝对坐标与相对坐标的性能差异，对比自回归与非自回归解码器的效果，评估不同顺序和任务下时间顺序的重要性", "result": "1) 使用传统位置编码对草图序列建模是有效的；2) 绝对坐标始终优于相对坐标；3) 非自回归解码器优于自回归解码器；4) 时间顺序的重要性取决于所考虑的顺序和评估的任务", "conclusion": "草图表示学习中的时间顺序确实重要，但具体影响取决于顺序类型和任务需求。绝对坐标比相对坐标更有效，非自回归方法优于自回归方法，为草图表示学习提供了新的见解"}}
{"id": "2512.04000", "pdf": "https://arxiv.org/pdf/2512.04000", "abs": "https://arxiv.org/abs/2512.04000", "authors": ["Jialuo Li", "Bin Li", "Jiahao Li", "Yan Lu"], "title": "Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.", "AI": {"tldr": "提出DIG框架，根据查询类型自适应选择视频帧：全局查询使用均匀采样，局部查询使用查询感知的帧选择，以解决长视频理解中的计算效率问题。", "motivation": "大型多模态模型在处理长视频时面临上下文长度限制和计算成本高的问题，现有查询感知的帧选择方法计算开销大，但并非所有查询类型都需要复杂的选择机制。", "method": "首先识别并验证查询类型学（全局查询vs局部查询），提出DIG框架：对全局查询使用高效的均匀采样，对局部查询激活专门管道提取查询相关帧，无需训练。", "result": "在三个长视频理解基准测试中，DIG始终优于现有基线，即使输入帧数扩展到256帧也能稳健提升LMM性能。", "conclusion": "通过根据查询类型自适应选择帧选择策略，DIG在保持计算效率的同时显著提升了长视频理解性能，证明了查询类型感知方法的重要性。"}}
{"id": "2512.03996", "pdf": "https://arxiv.org/pdf/2512.03996", "abs": "https://arxiv.org/abs/2512.03996", "authors": ["Hang Xu", "Linjiang Huang", "Feng Zhao"], "title": "Highly Efficient Test-Time Scaling for T2I Diffusion Models with Text Embedding Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Test-time scaling (TTS) aims to achieve better results by increasing random sampling and evaluating samples based on rules and metrics. However, in text-to-image(T2I) diffusion models, most related works focus on search strategies and reward models, yet the impact of the stochastic characteristic of noise in T2I diffusion models on the method's performance remains unexplored. In this work, we analyze the effects of randomness in T2I diffusion models and explore a new format of randomness for TTS: text embedding perturbation, which couples with existing randomness like SDE-injected noise to enhance generative diversity and quality. We start with a frequency-domain analysis of these formats of randomness and their impact on generation, and find that these two randomness exhibit complementary behavior in the frequency domain: spatial noise favors low-frequency components (early steps), while text embedding perturbation enhances high-frequency details (later steps), thereby compensating for the potential limitations of spatial noise randomness in high-frequency manipulation. Concurrently, text embedding demonstrates varying levels of tolerance to perturbation across different dimensions of the generation process. Specifically, our method consists of two key designs: (1) Introducing step-based text embedding perturbation, combining frequency-guided noise schedules with spatial noise perturbation. (2) Adapting the perturbation intensity selectively based on their frequency-specific contributions to generation and tolerance to perturbation. Our approach can be seamlessly integrated into existing TTS methods and demonstrates significant improvements on multiple benchmarks with almost no additional computation. Code is available at \\href{https://github.com/xuhang07/TEP-Diffusion}{https://github.com/xuhang07/TEP-Diffusion}.", "AI": {"tldr": "该论文提出了一种高效的测试时缩放方法，通过文本嵌入扰动增强T2I扩散模型的生成多样性和质量", "motivation": "现有测试时缩放方法主要关注搜索策略和奖励模型，但忽略了T2I扩散模型中噪声随机性对方法性能的影响，特别是空间噪声在频域中的局限性", "method": "提出两种关键设计：1）引入基于步骤的文本嵌入扰动，结合频率引导的噪声调度与空间噪声扰动；2）根据频域贡献和扰动容忍度自适应调整扰动强度", "result": "方法可无缝集成到现有TTS方法中，在多个基准测试上显著提升性能，几乎不增加额外计算成本", "conclusion": "文本嵌入扰动与空间噪声在频域上具有互补性，通过频率感知的扰动策略可以有效提升T2I扩散模型的生成质量"}}
{"id": "2512.03995", "pdf": "https://arxiv.org/pdf/2512.03995", "abs": "https://arxiv.org/abs/2512.03995", "authors": ["Levi Burner", "Guido de Croon", "Yiannis Aloimonos"], "title": "Artificial Microsaccade Compensation: Stable Vision for an Ornithopter", "categories": ["cs.RO", "cs.CV"], "comment": "29 pages, 5 figures, 2 tables, under review", "summary": "Animals with foveated vision, including humans, experience microsaccades, small, rapid eye movements that they are not aware of. Inspired by this phenomenon, we develop a method for \"Artificial Microsaccade Compensation\". It can stabilize video captured by a tailless ornithopter that has resisted attempts to use camera-based sensing because it shakes at 12-20 Hz. Our approach minimizes changes in image intensity by optimizing over 3D rotation represented in SO(3). This results in a stabilized video, computed in real time, suitable for human viewing, and free from distortion. When adapted to hold a fixed viewing orientation, up to occasional saccades, it can dramatically reduce inter-frame motion while also benefiting from an efficient recursive update. When compared to Adobe Premier Pro's warp stabilizer, which is widely regarded as the best commercial video stabilization software available, our method achieves higher quality results while also running in real time.", "AI": {"tldr": "开发一种受生物微扫视启发的视频稳定方法，用于解决扑翼无人机因高频振动导致的视频抖动问题", "motivation": "无尾扑翼无人机在飞行时会产生12-20Hz的高频振动，导致传统相机传感无法使用，需要一种能够实时稳定视频的方法", "method": "通过优化SO(3)空间中的3D旋转来最小化图像强度变化，实现无失真的视频稳定，并采用高效的递归更新算法", "result": "该方法能够实时生成适合人眼观看的稳定视频，与Adobe Premier Pro的warp stabilizer相比，在保持实时性的同时获得更高质量的结果", "conclusion": "受生物微扫视启发的视频稳定方法成功解决了扑翼无人机的视频抖动问题，在实时性和质量方面均优于商业软件"}}
{"id": "2512.03992", "pdf": "https://arxiv.org/pdf/2512.03992", "abs": "https://arxiv.org/abs/2512.03992", "authors": ["Zexin Lin", "Hawen Wan", "Yebin Zhong", "Xiaoqiang"], "title": "DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) deployed in safety-critical applications such as autonomous driving must handle continuous visual streams under imperfect conditions. However, existing benchmarks focus on static, high-quality images and ignore temporal degradation and error propagation, which are critical failure modes where transient visual corruption induces hallucinations that persist across subsequent frames. We introduce DIQ-H, the first benchmark for evaluating VLM robustness under dynamic visual degradation in temporal sequences. DIQ-H applies physics-based corruptions including motion blur, sensor noise, and compression artifacts, and measures hallucination persistence, error recovery, and temporal consistency through multi-turn question-answering tasks. To enable scalable annotation, we propose Uncertainty-Guided Iterative Refinement (UIR), which generates reliable pseudo-ground-truth using lightweight VLMs with uncertainty filtering, achieving a 15.3 percent accuracy improvement. Experiments on 16 state-of-the-art VLMs reveal substantial robustness gaps: even advanced models such as GPT-4o achieve only a 78.5 percent recovery rate, while open-source models struggle with temporal consistency at less than 60 percent. DIQ-H provides a comprehensive platform for evaluating VLM reliability in real-world deployments.", "AI": {"tldr": "评估视觉语言模型在时间性视觉退化下的幻觉持续性，提出了首个针对动态视觉退化的基准测试DIQ-H", "motivation": "现有基准测试主要关注静态高质量图像，忽略了时间性退化和错误传播，而这是安全关键应用（如自动驾驶）中的关键故障模式，瞬态视觉损坏会引发跨后续帧持续存在的幻觉", "method": "提出DIQ-H基准测试，应用基于物理的损坏（运动模糊、传感器噪声、压缩伪影），通过多轮问答任务测量幻觉持续性、错误恢复和时间一致性；提出不确定性引导迭代细化（UIR）方法，使用轻量级VLM进行不确定性过滤生成可靠的伪地面真值", "result": "实验评估16个最先进的VLM，揭示了显著的鲁棒性差距：即使是GPT-4o等先进模型也仅达到78.5%的恢复率，而开源模型的时间一致性低于60%；UIR方法实现了15.3%的准确率提升", "conclusion": "DIQ-H为评估VLM在现实世界部署中的可靠性提供了全面的平台，揭示了现有模型在时间性视觉退化下的严重鲁棒性问题"}}
{"id": "2512.03988", "pdf": "https://arxiv.org/pdf/2512.03988", "abs": "https://arxiv.org/abs/2512.03988", "authors": ["Jathushan Kaetheeswaran", "Boyi Ma", "Ali Abedi", "Milad Lankarany", "Shehroz Khan"], "title": "HEART-Watch: A multimodal physiological dataset from a Google Pixel Watch across different physical states", "categories": ["cs.HC"], "comment": ":J.3", "summary": "Consumer-grade smartwatches offer a new personalized health monitoring option for general consumers globally as cardiovascular diseases continue to prevail as the leading cause of global mortality. The development and validation of reliable cardiovascular monitoring algorithms for these consumer-grade devices requires realistic biosignal data from diverse sets of participants. However, the availability of public consumer-grade smartwatch datasets with synchronized cardiovascular biosignals is limited, and existing datasets do not offer rich demographic diversity in their participant cohorts, leading to potentially biased algorithm development. This paper presents HEART-Watch, a multimodal physiological dataset collected from temporally synchronized wrist-worn Google Pixel Watch 2 electrocardiogram (ECG), photoplethysmography, and accelerometer signals from a diverse cohort of 40 healthy adults across three physical states - sitting, standing and walking with reference chest ECG. Intermittent upper arm blood pressure measurements and concurrent biosignals were collected as an additional biomarker for future research. The motivation, methodology, and initial analyses of results are presented. HEART-Watch is intended to support the development and benchmarking of robust algorithms for cardiovascular analyses on consumer-grade smartwatches across diverse populations.", "AI": {"tldr": "HEART-Watch是一个多模态生理数据集，包含来自Google Pixel Watch 2的同步心电图、光电容积描记和加速度计信号，以及参考胸导联心电图，涵盖40名健康成年人在坐、站、走三种身体状态下的数据。", "motivation": "消费级智能手表为全球消费者提供了新的个性化健康监测选择，但现有公开数据集缺乏人口多样性，导致算法开发可能存在偏差。需要来自多样化参与者的真实生物信号数据来开发和验证可靠的消费级设备心血管监测算法。", "method": "从40名健康成年人收集时间同步的腕戴式Google Pixel Watch 2心电图、光电容积描记和加速度计信号，涵盖坐、站、走三种身体状态，同时收集参考胸导联心电图作为对照。还收集间歇性上臂血压测量值和并发生物信号作为未来研究的额外生物标志物。", "result": "创建了一个包含多样化参与者群体的多模态生理数据集，支持消费级智能手表上心血管分析算法的开发和基准测试。数据集提供了三种不同身体状态下的同步生物信号，为算法验证提供了现实场景。", "conclusion": "HEART-Watch数据集旨在支持跨多样化人群的消费级智能手表上稳健心血管分析算法的开发和基准测试，弥补了现有公开数据集在人口多样性方面的不足。"}}
{"id": "2512.03981", "pdf": "https://arxiv.org/pdf/2512.03981", "abs": "https://arxiv.org/abs/2512.03981", "authors": ["Sheng-Hao Liao", "Shang-Fu Chen", "Tai-Ming Huang", "Wen-Huang Cheng", "Kai-Lung Hua"], "title": "DirectDrag: High-Fidelity, Mask-Free, Prompt-Free Drag-based Image Editing via Readout-Guided Feature Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Drag-based image editing using generative models provides intuitive control over image structures. However, existing methods rely heavily on manually provided masks and textual prompts to preserve semantic fidelity and motion precision. Removing these constraints creates a fundamental trade-off: visual artifacts without masks and poor spatial control without prompts. To address these limitations, we propose DirectDrag, a novel mask- and prompt-free editing framework. DirectDrag enables precise and efficient manipulation with minimal user input while maintaining high image fidelity and accurate point alignment. DirectDrag introduces two key innovations. First, we design an Auto Soft Mask Generation module that intelligently infers editable regions from point displacement, automatically localizing deformation along movement paths while preserving contextual integrity through the generative model's inherent capacity. Second, we develop a Readout-Guided Feature Alignment mechanism that leverages intermediate diffusion activations to maintain structural consistency during point-based edits, substantially improving visual fidelity. Despite operating without manual mask or prompt, DirectDrag achieves superior image quality compared to existing methods while maintaining competitive drag accuracy. Extensive experiments on DragBench and real-world scenarios demonstrate the effectiveness and practicality of DirectDrag for high-quality, interactive image manipulation. Project Page: https://frakw.github.io/DirectDrag/. Code is available at: https://github.com/frakw/DirectDrag.", "AI": {"tldr": "提出DirectDrag框架，实现无需手动掩码和文本提示的拖拽式图像编辑，通过自动软掩码生成和读出引导特征对齐机制，在保持高图像保真度的同时实现精确的点对齐编辑。", "motivation": "现有拖拽式图像编辑方法严重依赖手动提供的掩码和文本提示来保持语义保真度和运动精度。移除这些约束会面临基本权衡：没有掩码会导致视觉伪影，没有提示会导致空间控制不佳。需要开发无需掩码和提示的编辑框架来解决这些限制。", "method": "DirectDrag包含两个关键创新：1) 自动软掩码生成模块，从点位移智能推断可编辑区域，自动定位沿移动路径的变形，同时通过生成模型的固有能力保持上下文完整性；2) 读出引导特征对齐机制，利用中间扩散激活来保持基于点编辑期间的结构一致性，显著提高视觉保真度。", "result": "尽管无需手动掩码或提示，DirectDrag在图像质量方面优于现有方法，同时保持竞争性的拖拽精度。在DragBench和真实场景上的大量实验证明了DirectDrag在高质量交互式图像操作方面的有效性和实用性。", "conclusion": "DirectDrag提出了一种新颖的无需掩码和提示的编辑框架，通过自动软掩码生成和读出引导特征对齐，实现了精确高效的图像操作，同时保持了高图像保真度和准确的点对齐，为高质量交互式图像编辑提供了实用解决方案。"}}
{"id": "2512.03973", "pdf": "https://arxiv.org/pdf/2512.03973", "abs": "https://arxiv.org/abs/2512.03973", "authors": ["Franki Nguimatsia Tiofack", "Théotime Le Hellard", "Fabian Schramm", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/", "AI": {"tldr": "提出Guided Flow Policy (GFP)方法，通过结合多步流匹配策略和蒸馏单步行动器，在离线强化学习中专注于模仿数据集中的高价值动作而非盲目模仿所有状态-动作对。", "motivation": "传统离线强化学习方法通常使用行为正则化来保持策略接近数据集分布，但这种方法无法区分高价值和低价值动作，导致性能受限。", "method": "GFP将多步流匹配策略与蒸馏的单步行动器相结合。行动器通过加权行为克隆指导流策略专注于克隆数据集中的高价值动作，而流策略则约束行动器保持与数据集最佳转移对齐的同时最大化批评者。", "result": "在OGBench、Minari和D4RL基准测试的144个状态和像素任务中实现了最先进的性能，特别是在次优数据集和挑战性任务上取得了显著提升。", "conclusion": "GFP通过相互指导机制有效地区分和利用数据集中的高价值动作，解决了传统行为正则化方法的局限性，在离线强化学习中取得了优异的性能表现。"}}
{"id": "2512.03964", "pdf": "https://arxiv.org/pdf/2512.03964", "abs": "https://arxiv.org/abs/2512.03964", "authors": ["Lianyu Pang", "Ji Zhou", "Qiping Wang", "Baoquan Zhao", "Zhenguo Yang", "Qing Li", "Xudong Mao"], "title": "Training for Identity, Inference for Controllability: A Unified Approach to Tuning-Free Face Personalization", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Tuning-free face personalization methods have developed along two distinct paradigms: text embedding approaches that map facial features into the text embedding space, and adapter-based methods that inject features through auxiliary cross-attention layers. While both paradigms have shown promise, existing methods struggle to simultaneously achieve high identity fidelity and flexible text controllability. We introduce UniID, a unified tuning-free framework that synergistically integrates both paradigms. Our key insight is that when merging these approaches, they should mutually reinforce only identity-relevant information while preserving the original diffusion prior for non-identity attributes. We realize this through a principled training-inference strategy: during training, we employ an identity-focused learning scheme that guides both branches to capture identity features exclusively; at inference, we introduce a normalized rescaling mechanism that recovers the text controllability of the base diffusion model while enabling complementary identity signals to enhance each other. This principled design enables UniID to achieve high-fidelity face personalization with flexible text controllability. Extensive experiments against six state-of-the-art methods demonstrate that UniID achieves superior performance in both identity preservation and text controllability. Code will be available at https://github.com/lyuPang/UniID", "AI": {"tldr": "提出UniID统一框架，将文本嵌入和适配器两种免调优人脸个性化范式相结合，实现高身份保真度和灵活文本控制性的平衡。", "motivation": "现有免调优人脸个性化方法分为文本嵌入和适配器两种范式，但各自存在局限性：文本嵌入方法身份保真度不足，适配器方法文本控制性受限。需要一种统一框架能同时实现高身份保真度和灵活文本控制。", "method": "提出UniID统一框架，包含训练-推理策略：训练阶段采用身份聚焦学习方案，引导两个分支专门捕获身份特征；推理阶段引入归一化重缩放机制，恢复基础扩散模型的文本控制性，同时让互补身份信号相互增强。", "result": "在六个最先进方法的对比实验中，UniID在身份保持和文本控制性方面均表现出优越性能，实现了高保真人脸个性化与灵活文本控制的平衡。", "conclusion": "UniID通过统一文本嵌入和适配器两种范式，采用身份聚焦训练和归一化重缩放推理策略，成功解决了免调优人脸个性化中身份保真度与文本控制性的权衡问题，为相关研究提供了新思路。"}}
{"id": "2512.03963", "pdf": "https://arxiv.org/pdf/2512.03963", "abs": "https://arxiv.org/abs/2512.03963", "authors": ["Tao Wu", "Li Yang", "Gen Zhan", "Yabin Zhang", "Yiting Liao", "Junlin Li", "Deliang Fu", "Li Zhang", "Limin Wang"], "title": "TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Enhancing the temporal understanding of Multimodal Large Language Models (MLLMs) is essential for advancing long-form video analysis, enabling tasks such as temporal localization, action detection, and time-sensitive question answering. While reinforcement learning (RL) has recently been explored for improving temporal reasoning, existing approaches are often confined to limited task types and data, restricting their generalization across diverse temporal understanding scenarios. To address this challenge, we present TempR1, a temporal-aware multi-task reinforcement learning framework that systematically strengthens MLLMs' temporal comprehension. We curate a multi-task corpus that exposes the model to diverse temporal structures and semantics, and build upon the Group Relative Policy Optimization (GRPO) algorithm to achieve stable and effective cross-task optimization. Specifically, we categorize temporal tasks into three correspondence types between predicted intervals and ground-truth instances, and design tailored localization rewards for each, enabling TempR1 to capture fine-grained temporal dependencies and adapt to different temporal patterns. Extensive experiments demonstrate that TempR1 attains state-of-the-art performance across multiple benchmarks. Moreover, its joint optimization over complementary tasks yields a strong synergistic effect, enhancing both generalization and single-task performance, establishing a scalable and principled paradigm for temporal reasoning in MLLMs.", "AI": {"tldr": "提出TempR1框架，通过时间感知的多任务强化学习增强多模态大语言模型的时间理解能力", "motivation": "现有强化学习方法在提升MLLMs时间推理能力时，通常局限于有限的任务类型和数据，难以泛化到多样化的时间理解场景", "method": "构建多任务语料库，基于GRPO算法进行跨任务优化，将时间任务分为三种预测区间与真实实例对应类型，并为每种类型设计定制化的定位奖励", "result": "在多个基准测试中达到最先进性能，联合优化互补任务产生强协同效应，提升泛化能力和单任务性能", "conclusion": "为MLLMs中的时间推理建立了可扩展且有原则的范式，通过时间感知的多任务强化学习系统性地增强模型的时间理解能力"}}
{"id": "2512.03958", "pdf": "https://arxiv.org/pdf/2512.03958", "abs": "https://arxiv.org/abs/2512.03958", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "MDE-AgriVLN: Agricultural Vision-and-Language Navigation with Monocular Depth Estimation", "categories": ["cs.RO"], "comment": null, "summary": "Agricultural robots are serving as powerful assistants across a wide range of agricultural tasks, nevertheless, still heavily relying on manual operations or railway systems for movement. The AgriVLN method and the A2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural domain, enabling a robot to navigate to a target position following a natural language instruction. Unlike human binocular vision, most agricultural robots are only given a single camera for monocular vision, which results in limited spatial perception. To bridge this gap, we present the method of Agricultural Vision-and-Language Navigation with Monocular Depth Estimation (MDE-AgriVLN), in which we propose the MDE module generating depth features from RGB images, to assist the decision-maker on reasoning. When evaluated on the A2A benchmark, our MDE-AgriVLN method successfully increases Success Rate from 0.23 to 0.32 and decreases Navigation Error from 4.43m to 4.08m, demonstrating the state-of-the-art performance in the agricultural VLN domain. Code: https://github.com/AlexTraveling/MDE-AgriVLN.", "AI": {"tldr": "提出MDE-AgriVLN方法，将单目深度估计集成到农业视觉语言导航中，以增强农业机器人的空间感知能力", "motivation": "农业机器人通常只配备单目摄像头，导致空间感知能力有限，而现有的农业视觉语言导航方法缺乏深度信息支持，影响导航精度", "method": "提出MDE模块，从RGB图像生成深度特征，辅助决策模块进行推理，将深度信息集成到农业视觉语言导航框架中", "result": "在A2A基准测试中，成功率从0.23提升到0.32，导航误差从4.43米降低到4.08米，达到了农业VLN领域的最先进性能", "conclusion": "单目深度估计能有效增强农业机器人的空间感知能力，显著提升农业视觉语言导航的性能，为农业机器人自主导航提供了新的解决方案"}}
{"id": "2512.03955", "pdf": "https://arxiv.org/pdf/2512.03955", "abs": "https://arxiv.org/abs/2512.03955", "authors": ["Niklas Jobs", "Luis Miguel Vieira da Silva", "Jayanth Somashekaraiah", "Maximilian Weigand", "David Kube", "Felix Gehlhoff"], "title": "Benchmark for Planning and Control with Large Language Model Agents: Blocksworld with Model Context Protocol", "categories": ["cs.AI", "cs.ET"], "comment": "This work has been submitted to IFAC for possible publication", "summary": "Industrial automation increasingly requires flexible control strategies that can adapt to changing tasks and environments. Agents based on Large Language Models (LLMs) offer potential for such adaptive planning and execution but lack standardized benchmarks for systematic comparison. We introduce a benchmark with an executable simulation environment representing the Blocksworld problem providing five complexity categories. By integrating the Model Context Protocol (MCP) as a standardized tool interface, diverse agent architectures can be connected to and evaluated against the benchmark without implementation-specific modifications. A single-agent implementation demonstrates the benchmark's applicability, establishing quantitative metrics for comparison of LLM-based planning and execution approaches.", "AI": {"tldr": "提出一个用于评估基于大型语言模型的规划与控制智能体的基准测试框架，采用积木世界问题作为测试环境，并集成模型上下文协议作为标准化工具接口。", "motivation": "工业自动化需要能够适应变化任务和环境的灵活控制策略，基于大型语言模型的智能体具有这种自适应规划和执行的潜力，但缺乏系统比较的标准化基准。", "method": "引入包含可执行仿真环境的基准测试，采用积木世界问题并提供五个复杂度类别，通过集成模型上下文协议作为标准化工具接口，使不同智能体架构能够无需特定实现修改即可连接和评估。", "result": "开发了一个单智能体实现来展示基准的适用性，建立了用于比较基于LLM的规划与执行方法的定量指标。", "conclusion": "该基准测试框架为系统评估和比较基于大型语言模型的规划与控制智能体提供了标准化平台，有助于推动自适应工业自动化控制策略的发展。"}}
{"id": "2512.03945", "pdf": "https://arxiv.org/pdf/2512.03945", "abs": "https://arxiv.org/abs/2512.03945", "authors": ["Michael Schiffmann", "Sabina Jeschke", "Anja Richert"], "title": "Classification of User Satisfaction in HRI with Social Signals in the Wild", "categories": ["cs.HC", "cs.RO"], "comment": "15 pages, 3 figures. This paper has been accepted for publication at ICSR+AI 2025", "summary": "Socially interactive agents (SIAs) are being used in various scenarios and are nearing productive deployment. Evaluating user satisfaction with SIAs' performance is a key factor in designing the interaction between the user and SIA. Currently, subjective user satisfaction is primarily assessed manually through questionnaires or indirectly via system metrics. This study examines the automatic classification of user satisfaction through analysis of social signals, aiming to enhance both manual and autonomous evaluation methods for SIAs. During a field trial at the Deutsches Museum Bonn, a Furhat Robotics head was employed as a service and information hub, collecting an \"in-the-wild\" dataset. This dataset comprises 46 single-user interactions, including questionnaire responses and video data. Our method focuses on automatically classifying user satisfaction based on time series classification. We use time series of social signal metrics derived from the body pose, time series of facial expressions, and physical distance. This study compares three feature engineering approaches on different machine learning models. The results confirm the method's effectiveness in reliably identifying interactions with low user satisfaction without the need for manually annotated datasets. This approach offers significant potential for enhancing SIA performance and user experience through automated feedback mechanisms.", "AI": {"tldr": "该研究提出了一种基于社交信号自动分类人机交互中用户满意度的方法，通过分析身体姿态、面部表情和物理距离等时间序列数据，无需人工标注即可识别低满意度交互。", "motivation": "当前社交智能体(SIAs)的用户满意度评估主要依赖人工问卷或间接系统指标，缺乏自动化评估方法。研究旨在通过分析社交信号实现用户满意度的自动分类，以改进SIAs的性能评估和用户体验。", "method": "在德国波恩德意志博物馆的实地试验中，使用Furhat Robotics头部作为服务信息中心，收集了46个单用户交互数据集。采用时间序列分类方法，基于身体姿态、面部表情和物理距离的社交信号指标，比较了三种特征工程方法在不同机器学习模型上的表现。", "result": "研究结果表明，该方法能够可靠地识别用户满意度低的交互，无需人工标注数据集。这为通过自动化反馈机制提升SIA性能和用户体验提供了有效途径。", "conclusion": "基于社交信号的时间序列分类方法能够有效自动评估用户满意度，特别是在识别低满意度交互方面表现良好，为社交智能体的性能优化和用户体验改进提供了实用的自动化评估工具。"}}
{"id": "2512.03943", "pdf": "https://arxiv.org/pdf/2512.03943", "abs": "https://arxiv.org/abs/2512.03943", "authors": ["Kazi Abrab Hossain", "Jannatul Somiya Mahmud", "Maria Hossain Tuli", "Anik Mitra", "S. M. Taiabul Haque", "Farig Y. Sadeque"], "title": "Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions", "categories": ["cs.CL", "cs.HC"], "comment": "18 pages, 7 figures", "summary": "While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.", "AI": {"tldr": "该论文研究了多语言大语言模型在宗教背景下的偏见问题，特别关注南亚四大宗教（佛教、基督教、印度教和伊斯兰教），发现模型在英语和孟加拉语中表现出对伊斯兰教的系统性偏见。", "motivation": "虽然大语言模型在偏见检测和分类方面有所改进，但宗教等敏感话题仍然存在挑战，因为即使是微小错误也可能导致严重误解。多语言模型经常误传宗教信息，在宗教语境中难以保持准确性。", "method": "研究者引入了BRAND数据集（双语宗教可问责规范数据集），包含超过2400个条目，涵盖南亚四大宗教。使用三种不同类型的提示，分别在英语和孟加拉语中进行测试。", "result": "结果显示：1）模型在英语中的表现优于孟加拉语；2）模型持续表现出对伊斯兰教的偏见，即使在回答宗教中立问题时也是如此；3）多语言模型在不同语言中询问相似问题时存在持续偏见。", "conclusion": "研究揭示了多语言大语言模型在宗教语境中的系统性偏见问题，特别是在不同语言环境下对伊斯兰教的偏见。这些发现与HCI领域关于宗教和灵性的更广泛问题相关，强调了需要更公平的多语言模型开发。"}}
{"id": "2512.03936", "pdf": "https://arxiv.org/pdf/2512.03936", "abs": "https://arxiv.org/abs/2512.03936", "authors": ["Aron Distelzweig", "Yiwei Wang", "Faris Janjoš", "Marcel Hallgarten", "Mihai Dobre", "Alexander Langmann", "Joschka Boedecker", "Johannes Betz"], "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.", "AI": {"tldr": "提出BIBeR框架，将运动预测和博弈论规划统一为单一交互感知过程，通过贝叶斯置信度估计和迭代最优响应循环实现自动驾驶车辆与周围环境的双向适应。", "motivation": "当前自动驾驶规划系统在常规场景表现良好，但在密集城市交通中（如变道、并线）仍面临挑战，需要预测和影响其他智能体。现有方法要么简单丢弃不安全计划，要么使用端到端模型回避联合预测和规划的挑战，而博弈论方法虽有理论优势但应用有限。", "method": "提出贝叶斯迭代最优响应（BIBeR）框架：1）将最先进的预测器集成到迭代最优响应（IBR）循环中，反复优化自车和周围智能体的策略；2）通过贝叶斯置信度估计量化预测可靠性，根据置信度高低调整更新强度；3）近似纳什均衡，实现双向适应（自车既响应又影响其他智能体）。", "result": "在高度交互的interPlan变道场景中，BIBeR比最先进的规划器提升了11%的性能；在标准nuPlan基准测试中也优于现有方法。", "conclusion": "BIBeR成功将现代预测器与博弈论规划相结合，实现了结构化规划的透明性和学习模型的灵活性，为自动驾驶在密集交互场景中的决策提供了有效的解决方案。"}}
{"id": "2512.03931", "pdf": "https://arxiv.org/pdf/2512.03931", "abs": "https://arxiv.org/abs/2512.03931", "authors": ["Vineel Tummala", "Daniela Inclezan"], "title": "Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties", "categories": ["cs.AI"], "comment": "27 pages, 5 figures", "summary": "This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "提出一个基于逻辑编程的框架，使自主智能体能够推理政策违规的潜在惩罚并相应行动，在确保合规的同时考虑高风险目标下必要的违规行为。", "motivation": "现有研究主要关注确保政策合规，但现实场景中为实现高风险目标可能需要偏离政策。同时，建模违规行为有助于政策制定者模拟真实的人类决策过程。", "method": "扩展Gelfond和Lobo的授权与义务策略语言(AOPL)以纳入惩罚机制，集成回答集编程(ASP)进行推理，开发从扩展AOPL到ASP的自动转换，并改进基于ASP的规划算法以考虑惩罚。", "result": "在两个领域进行的实验表明，该框架能生成更高质量的计划，避免有害行动，并在某些情况下提高计算效率。能够明确识别规则违规及其后果，增强可解释性。", "conclusion": "该框架通过惩罚推理区分不同违规计划，优先选择惩罚最小的方案，具有增强自主决策和指导政策完善的潜力，为政策感知型自主智能体提供了实用工具。"}}
{"id": "2512.03918", "pdf": "https://arxiv.org/pdf/2512.03918", "abs": "https://arxiv.org/abs/2512.03918", "authors": ["Youxin Pang", "Yong Zhang", "Ruizhi Shao", "Xiang Deng", "Feng Gao", "Xu Xiaoming", "Xiaoming Wei", "Yebin Liu"], "title": "UniMo: Unifying 2D Video and 3D Human Motion with an Autoregressive Framework", "categories": ["cs.CV"], "comment": "https://carlyx.github.io/UniMo/", "summary": "We propose UniMo, an innovative autoregressive model for joint modeling of 2D human videos and 3D human motions within a unified framework, enabling simultaneous generation and understanding of these two modalities for the first time. Current methods predominantly focus on generating one modality given another as the condition or integrating either of them with other modalities such as text and audio. Unifying 2D videos and 3D motions for simultaneous optimization and generation remains largely unexplored, presenting significant challenges due to their substantial structural and distributional differences. Inspired by the LLM's ability to unify different modalities, our method models videos and 3D motions as a unified tokens sequence, utilizing separate embedding layers to mitigate distribution gaps. Additionally, we devise a sequence modeling strategy that integrates two distinct tasks within a single framework, proving the effectiveness of unified modeling. Moreover, to efficiently align with visual tokens and preserve 3D spatial information, we design a novel 3D motion tokenizer with a temporal expansion strategy, using a single VQ-VAE to produce quantized motion tokens. It features multiple expert decoders that handle body shapes, translation, global orientation, and body poses for reliable 3D motion reconstruction. Extensive experiments demonstrate that our method simultaneously generates corresponding videos and motions while performing accurate motion capture. This work taps into the capacity of LLMs to fuse diverse data types, paving the way for integrating human-centric information into existing models and potentially enabling multimodal, controllable joint modeling of humans, objects, and scenes.", "AI": {"tldr": "UniMo是一个创新的自回归模型，首次在统一框架中联合建模2D人体视频和3D人体运动，实现两种模态的同时生成和理解。", "motivation": "当前方法主要关注以另一种模态为条件生成一种模态，或将它们与其他模态（如文本和音频）集成。统一2D视频和3D运动进行同时优化和生成仍未被充分探索，由于它们在结构和分布上的显著差异而面临重大挑战。", "method": "1) 将视频和3D运动建模为统一的令牌序列，使用单独的嵌入层来缓解分布差距；2) 设计序列建模策略，在单一框架中集成两个不同任务；3) 设计新颖的3D运动分词器，采用时间扩展策略，使用单个VQ-VAE产生量化运动令牌，具有多个专家解码器处理身体形状、平移、全局方向和身体姿态。", "result": "大量实验表明，该方法能够同时生成相应的视频和运动，同时执行准确的动作捕捉。该方法挖掘了LLMs融合不同数据类型的能力。", "conclusion": "这项工作为将人体中心信息集成到现有模型中铺平了道路，并可能实现人类、物体和场景的多模态可控联合建模。"}}
{"id": "2512.03915", "pdf": "https://arxiv.org/pdf/2512.03915", "abs": "https://arxiv.org/abs/2512.03915", "authors": ["X. Y. Han", "Yuan Zhong"], "title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models", "categories": ["math.OC", "cs.AI", "cs.LG"], "comment": null, "summary": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.", "AI": {"tldr": "提出一个理论框架来分析稀疏专家混合模型中的无辅助损失负载均衡方法，将其建模为分配问题的单步原始对偶方法，并在确定性和在线设置下提供理论保证。", "motivation": "大规模AI训练中，稀疏专家混合模型通过每次只激活少量专家来扩展模型规模，但面临负载均衡的挑战：需要高效地将token路由到专家以最小化空闲专家数量，这对于昂贵的GPU资源利用至关重要。现有方法通常使用辅助损失来实现负载均衡，但本文研究无辅助损失的负载均衡方法。", "method": "将DeepSeek提出的无辅助损失负载均衡过程建模为分配问题的单步原始对偶方法。首先在确定性设置中分析结构特性，然后在在线优化框架中考虑AI训练的随机性和动态性，最后在1B参数的DeepSeekMoE模型上进行实验验证。", "result": "在确定性设置中获得了三个关键结构特性：拉格朗日目标的单调改进、从过载专家向欠载专家移动token的偏好规则、近似均衡保证。在在线设置中，推导出目标函数的强凸性，并在特定步长选择下得到对数期望遗憾界。实验验证了理论发现。", "conclusion": "建立了一个原则性框架来分析稀疏专家混合模型中的无辅助损失负载均衡，为理解该方法提供了理论基础，并展示了其在确定性和在线设置下的理论保证，有助于提高大规模AI训练中GPU资源的利用效率。"}}
{"id": "2512.03913", "pdf": "https://arxiv.org/pdf/2512.03913", "abs": "https://arxiv.org/abs/2512.03913", "authors": ["Jeongeun Park", "Jihwan Yoon", "Byungwoo Jeon", "Juhan Park", "Jinwoo Shin", "Namhoon Cho", "Kyungjae Lee", "Sangdoo Yun", "Sungjoon Choi"], "title": "Hierarchical Vision Language Action Model Using Success and Failure Demonstrations", "categories": ["cs.RO", "cs.AI"], "comment": "https://vine-vla.github.io/", "summary": "Prior Vision-Language-Action (VLA) models are typically trained on teleoperated successful demonstrations, while discarding numerous failed attempts that occur naturally during data collection. However, these failures encode where and how policies can be fragile, information that can be exploited to improve robustness. We address this problem by leveraging mixed-quality datasets to learn failure-aware reasoning at planning time. We introduce VINE, a hierarchical vision-language-action model that separates high-level reasoning (System 2) from low-level control (System 1) under a hierarchical reinforcement learning formalism, making failures usable as a structured learning signal rather than noisy supervision. System 2 performs feasibility-guided tree search over a 2D scene-graph abstraction: it proposes subgoal transitions, predicts success probabilities from both successes and failures, and prunes brittle branches before execution, effectively casting plan evaluation as feasibility scoring. The selected subgoal sequence is then passed to System 1, which executes low-level actions without modifying the agent's core skills. Trained entirely from offline teleoperation data, VINE integrates negative experience directly into the decision loop. Across challenging manipulation tasks, this approach consistently improves success rates and robustness, demonstrating that failure data is an essential resource for converting the broad competence of VLAs into robust execution.", "AI": {"tldr": "提出VINE模型，一种分层视觉语言动作模型，利用成功和失败演示数据来提升机器人操作的鲁棒性", "motivation": "现有的VLA模型通常只使用成功的遥操作演示数据，而忽略了数据收集中自然产生的大量失败尝试。这些失败数据包含了策略脆弱性的信息，可用于提高模型的鲁棒性", "method": "采用分层强化学习框架，将高层推理（System 2）与底层控制（System 1）分离。System 2在2D场景图抽象上执行可行性引导的树搜索，提出子目标转换，从成功和失败数据中预测成功概率，并在执行前修剪脆弱分支。System 1执行底层动作而不修改核心技能", "result": "在具有挑战性的操作任务中，该方法持续提高了成功率和鲁棒性，证明失败数据是将VLA广泛能力转化为鲁棒执行的重要资源", "conclusion": "通过利用混合质量数据集，VINE模型将负面经验直接整合到决策循环中，实现了失败感知的推理，显著提升了机器人操作的鲁棒性"}}
{"id": "2512.03911", "pdf": "https://arxiv.org/pdf/2512.03911", "abs": "https://arxiv.org/abs/2512.03911", "authors": ["Kenneth Stewart", "Roxana Leontie", "Samantha Chapin", "Joe Hays", "Sumit Bam Shrestha", "Carl Glen Henshaw"], "title": "Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted for review at NICE 2026 (Neuro-Inspired Computational Elements) conference", "summary": "We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.", "AI": {"tldr": "将强化学习训练的ANN策略转换为SDNN，部署在Intel Loihi 2神经形态硬件上，实现机器人低延迟、高能效控制", "motivation": "探索神经形态硬件在机器人控制中的应用，实现低延迟、高能效的实时控制，为未来空间和地面机器人应用建立技术路径", "method": "开发端到端流程：将RL训练的ANN策略转换为SDNN，部署到Intel Loihi 2硬件，在NVIDIA Omniverse Isaac Lab仿真环境中进行闭环控制评估", "result": "成功将ANN策略转换为SDNN并在Loihi 2上部署，验证了神经形态平台用于机器人控制的可行性，比较了GPU和Loihi 2的执行性能", "conclusion": "神经形态硬件为机器人控制提供了可行的低延迟、高能效解决方案，为未来空间和地面机器人应用开辟了新途径"}}
{"id": "2512.03905", "pdf": "https://arxiv.org/pdf/2512.03905", "abs": "https://arxiv.org/abs/2512.03905", "authors": ["Shuai Yang", "Junxin Lin", "Yifan Zhou", "Ziwei Liu", "Chen Change Loy"], "title": "Zero-Shot Video Translation and Editing with Frame Spatial-Temporal Correspondence", "categories": ["cs.CV"], "comment": "Code: https://github.com/Sunnycookies/FRESCO-v2, Project: https://williamyang1991.github.io/projects/FRESCOv2/", "summary": "The remarkable success in text-to-image diffusion models has motivated extensive investigation of their potential for video applications. Zero-shot techniques aim to adapt image diffusion models for videos without requiring further model training. Recent methods largely emphasize integrating inter-frame correspondence into attention mechanisms. However, the soft constraint applied to identify the valid features to attend is insufficient, which could lead to temporal inconsistency. In this paper, we present FRESCO, which integrates intra-frame correspondence with inter-frame correspondence to formulate a more robust spatial-temporal constraint. This enhancement ensures a consistent transformation of semantically similar content between frames. Our method goes beyond attention guidance to explicitly optimize features, achieving high spatial-temporal consistency with the input video, significantly enhancing the visual coherence of manipulated videos. We verify FRESCO adaptations on two zero-shot tasks of video-to-video translation and text-guided video editing. Comprehensive experiments demonstrate the effectiveness of our framework in generating high-quality, coherent videos, highlighting a significant advance over current zero-shot methods.", "AI": {"tldr": "提出FRESCO框架，通过整合帧内和帧间对应关系来增强零样本视频翻译和编辑的时空一致性", "motivation": "现有零样本方法主要关注在注意力机制中整合帧间对应关系，但软约束不足以确保时间一致性，导致生成的视频存在视觉不连贯问题", "method": "FRESCO框架整合了帧内对应关系与帧间对应关系，形成更鲁棒的时空约束，通过显式优化特征而不仅仅是注意力引导，确保语义相似内容在帧间的一致性转换", "result": "在视频到视频翻译和文本引导视频编辑两个零样本任务上验证了有效性，相比现有方法能生成更高质量、更连贯的视频，显著提升了视觉一致性", "conclusion": "FRESCO通过整合帧内和帧间对应关系，为图像扩散模型在视频应用中的零样本适应提供了更有效的时空约束机制，显著提升了生成视频的视觉连贯性"}}
{"id": "2512.03903", "pdf": "https://arxiv.org/pdf/2512.03903", "abs": "https://arxiv.org/abs/2512.03903", "authors": ["Ekhi Azurmendi", "Joseba Fernandez de Landa", "Jaione Bengoetxea", "Maite Heredia", "Julen Etxaniz", "Mikel Zubillaga", "Ander Soraluze", "Aitor Soroa"], "title": "BERnaT: Basque Encoders for Representing Natural Textual Diversity", "categories": ["cs.CL", "cs.AI"], "comment": "Submitted to LREC 2026", "summary": "Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.", "AI": {"tldr": "该论文提出了BERnaT模型系列，旨在捕捉巴斯克语的语言多样性（方言、历史、非正式等变体），通过构建包含标准、社交媒体和历史语料的新语料库，训练编码器模型来提升语言模型的包容性和泛化能力。", "motivation": "当前语言模型依赖大规模高质量文本语料，但过滤过程可能无意中排除非标准语言变体，降低模型鲁棒性并加剧代表性偏见。作者认为语言模型应捕捉语言变体的完整谱系，而不仅依赖标准化文本。", "method": "针对巴斯克语（形态丰富、资源稀缺），构建包含标准、社交媒体和历史来源的新语料库，预训练BERnaT编码器模型系列，包括三种配置：标准、多样性和组合。提出将自然语言理解任务分为标准和多样性子集的评估框架。", "result": "实验结果显示，在标准和多样性数据上训练的模型始终优于仅在标准语料上训练的模型，在所有任务类型上都有提升，且不损害标准基准的准确性。", "conclusion": "研究强调了语言多样性在构建包容性、可泛化语言模型中的重要性，证明结合多样语言变体可以提升模型性能而不牺牲标准任务表现。"}}
{"id": "2512.03891", "pdf": "https://arxiv.org/pdf/2512.03891", "abs": "https://arxiv.org/abs/2512.03891", "authors": ["Ying-Kuan Tsai", "Yi-Ping Chen", "Vispi Karkaria", "Wei Chen"], "title": "Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "28 pages, 17 figures", "summary": "Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.", "AI": {"tldr": "本文提出了一种基于数字孪生的整车主动悬架控制协同设计框架，通过深度强化学习实现物理悬架组件与控制策略的联合优化。", "motivation": "传统主动悬架系统的性能受限于固定硬件设计和无法适应不确定动态工况的控制策略，数字孪生和深度强化学习为实时数据驱动优化提供了新机遇，但将这些技术整合到统一框架中仍是一个开放挑战。", "method": "采用数字孪生控制协同设计框架，结合多代设计概念，通过将自动微分集成到深度强化学习中，在变化的驾驶行为和环境不确定性下联合优化物理悬架组件和控制策略。利用分位数学习进行模型更新以捕捉数据不确定性，实现数字-物理交互的自适应学习。", "result": "在两种不同驾驶场景（温和与激进）下实现了悬架系统的个性化优化。优化后的系统实现了更平滑的轨迹，控制努力分别减少了约43%和52%，同时保持了乘坐舒适性和稳定性。", "conclusion": "该研究开发了一个集成深度强化学习和不确定性感知模型更新的数字孪生控制协同设计框架，引入了多代设计策略用于自改进系统，并展示了针对不同驾驶员类型的主动悬架系统个性化优化。"}}
{"id": "2512.03886", "pdf": "https://arxiv.org/pdf/2512.03886", "abs": "https://arxiv.org/abs/2512.03886", "authors": ["Brais Fontan-Costas", "M. Diaz-Cacho", "Ruben Fernandez-Boullon", "Manuel Alonso-Carracedo", "Javier Perez-Robles"], "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.", "AI": {"tldr": "设计用于封闭赛道自动驾驶赛车的模块化架构系统", "motivation": "为封闭赛道环境开发一个高效、可靠的自动驾驶系统，能够执行精确的赛道任务，包括环境感知、定位、路径规划和车辆控制", "method": "采用模块化架构设计，将系统分为计算机视觉环境感知、定位与建图、路径规划、车辆控制等独立子系统，通过统一的流水线架构连接数据流", "result": "实现了在受控环境中实时自主导航的模块化自动驾驶系统，各子系统独立运行但能协同工作", "conclusion": "提出的模块化架构设计能够有效支持封闭赛道自动驾驶赛车任务，结合了最先进技术实现精确的实时导航"}}
{"id": "2512.03883", "pdf": "https://arxiv.org/pdf/2512.03883", "abs": "https://arxiv.org/abs/2512.03883", "authors": ["Jorge Tapias Gomez", "Despoina Kanata", "Aneesh Rangnekar", "Christina Lee", "Julio Garcia-Aguilar", "Joshua Jesse Smith", "Harini Veeraraghavan"], "title": "Dual Cross-Attention Siamese Transformer for Rectal Tumor Regrowth Assessment in Watch-and-Wait Endoscopy", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 1 table, submitted to ISBI conference", "summary": "Increasing evidence supports watch-and-wait (WW) surveillance for patients with rectal cancer who show clinical complete response (cCR) at restaging following total neoadjuvant treatment (TNT). However, objectively accurate methods to early detect local regrowth (LR) from follow-up endoscopy images during WW are essential to manage care and prevent distant metastases. Hence, we developed a Siamese Swin Transformer with Dual Cross-Attention (SSDCA) to combine longitudinal endoscopic images at restaging and follow-up and distinguish cCR from LR. SSDCA leverages pretrained Swin transformers to extract domain agnostic features and enhance robustness to imaging variations. Dual cross attention is implemented to emphasize features from the two scans without requiring any spatial alignment of images to predict response. SSDCA as well as Swin-based baselines were trained using image pairs from 135 patients and evaluated on a held-out set of image pairs from 62 patients. SSDCA produced the best balanced accuracy (81.76\\% $\\pm$ 0.04), sensitivity (90.07\\% $\\pm$ 0.08), and specificity (72.86\\% $\\pm$ 0.05). Robustness analysis showed stable performance irrespective of artifacts including blood, stool, telangiectasia, and poor image quality. UMAP clustering of extracted features showed maximal inter-cluster separation (1.45 $\\pm$ 0.18) and minimal intra-cluster dispersion (1.07 $\\pm$ 0.19) with SSDCA, confirming discriminative representation learning.", "AI": {"tldr": "开发了一种基于Siamese Swin Transformer和双交叉注意力机制的方法，用于直肠癌观察等待策略中通过内窥镜图像评估肿瘤再生", "motivation": "直肠癌患者在总新辅助治疗后达到临床完全缓解时，观察等待策略被广泛采用，但需要客观准确的方法从随访内窥镜图像中早期检测局部再生，以管理护理并预防远处转移", "method": "提出SSDCA（Siamese Swin Transformer with Dual Cross-Attention）模型，结合纵向内窥镜图像（重新分期和随访），利用预训练的Swin Transformer提取领域无关特征，通过双交叉注意力机制强调两次扫描的特征，无需图像空间对齐", "result": "在135名患者的图像对训练和62名患者测试集上，SSDCA取得最佳平衡准确率（81.76%±0.04）、灵敏度（90.07%±0.08）和特异性（72.86%±0.05），对血液、粪便、毛细血管扩张和图像质量差等伪影具有鲁棒性", "conclusion": "SSDCA能够有效区分临床完全缓解和局部再生，为直肠癌观察等待策略中的内窥镜监测提供了准确、鲁棒的自动化评估工具"}}
{"id": "2512.03878", "pdf": "https://arxiv.org/pdf/2512.03878", "abs": "https://arxiv.org/abs/2512.03878", "authors": ["Zhiyin Zhou"], "title": "Adhera: A Human-Centered Health Informatics Solution for Reducing Informal Caregiver Burden through Improved Medication Adherence", "categories": ["cs.HC"], "comment": null, "summary": "The growing global population of older adults, combined with ongoing healthcare workforce shortages, has increased reliance on informal caregivers, including family members and friends who provide unpaid support to individuals with chronic illnesses. Among their daily responsibilities, medication management remains one of the most demanding and error-prone tasks. Non-adherence to prescribed regimens not only undermines patient outcomes but also intensifies caregiver stress, anxiety, and fatigue. Although digital health technologies have proliferated to address adherence, most solutions focus exclusively on patients and neglect the informational and emotional needs of caregivers. This paper introduces Adhera, a caregiver-inclusive health informatics system designed to support medication adherence while reducing caregiver burden. Using a mixed-methods research design that included fifteen semi-structured caregiver interviews, sixty-five survey responses, and five pharmacist consultations, this study identified three primary challenges: caregiver stress related to uncertainty about medication intake, fragmented communication with healthcare professionals, and distrust in existing digital tools. Informed by the CeHRes Roadmap 2.0 and the Triple Bottom Line by Design and Culture (TBLD+C) framework, as well as recent co-design studies involving caregivers, Adhera integrates a sensor-equipped smart pill organizer with a mobile companion application that records intake events, sends real-time reminders, and provides caregivers with synchronized adherence data. Preliminary evaluation suggests that Adhera enhances visibility, improves caregiver confidence, and streamlines medication routines. This study contributes to the field of health informatics by demonstrating how human-centered design and collaborative frameworks can align technical innovation with empathy-driven care.", "AI": {"tldr": "Adhera是一个以人为中心的健康信息学解决方案，旨在通过改善药物依从性来减轻非正式照顾者的负担。", "motivation": "全球老年人口增长和医疗保健劳动力短缺导致对非正式照顾者的依赖增加，药物管理成为照顾者最繁重且易出错的任务之一。现有数字健康技术大多只关注患者，忽视了照顾者的信息和情感需求。", "method": "采用混合研究方法，包括15次半结构化照顾者访谈、65份调查问卷和5次药剂师咨询。基于CeHRes Roadmap 2.0和TBLD+C框架，结合照顾者共同设计研究，开发了配备传感器的智能药盒和移动伴侣应用。", "result": "初步评估表明，Adhera提高了药物摄入的可见性，增强了照顾者的信心，并简化了药物管理流程。系统能够记录摄入事件、发送实时提醒，并为照顾者提供同步的依从性数据。", "conclusion": "研究表明，以人为中心的设计和协作框架可以将技术创新与同理心驱动的护理相结合，为健康信息学领域做出贡献，展示了如何通过技术解决方案同时支持患者和照顾者。"}}
{"id": "2512.03874", "pdf": "https://arxiv.org/pdf/2512.03874", "abs": "https://arxiv.org/abs/2512.03874", "authors": ["Lei Zhang", "Diwen Zheng", "Kaixin Bai", "Zhenshan Bing", "Zoltan-Csaba Marton", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance", "categories": ["cs.RO", "cs.LG"], "comment": "Project Website: https://sites.google.com/view/omnidexvlg, 16 pages", "summary": "Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.", "AI": {"tldr": "提出OmniDexVLG框架，通过视觉语言模型引导的抓取语义、分类学和功能可供性学习，实现语义可控的灵巧抓取生成", "motivation": "当前灵巧抓取生成难以实现语义可控，主要挑战在于缺乏对抓取分类学、接触语义和功能可供性等多维语义的统一建模", "method": "1) OmniDexDataGen：语义丰富的抓取数据集生成流水线，整合分类学引导配置采样、功能可供性接触点采样、分类学感知力闭合抓取采样和物理优化验证；2) OmniDexReasoner：多模态抓取类型语义推理模块，利用多智能体协作、检索增强生成和思维链推理；3) 统一视觉语言抓取生成模型，显式整合抓取分类学、接触结构和功能可供性语义", "result": "在仿真和真实世界物体抓取实验中，该方法在抓取多样性、接触语义多样性、功能可供性多样性和语义一致性方面显著优于现有最先进方法", "conclusion": "OmniDexVLG框架能够生成结构多样且语义连贯的灵巧抓取，实现了从自然语言指令到细粒度抓取合成的可控生成"}}
{"id": "2512.03864", "pdf": "https://arxiv.org/pdf/2512.03864", "abs": "https://arxiv.org/abs/2512.03864", "authors": ["Danny Hoang", "Anandkumar Patel", "Ruimen Chen", "Rajiv Malhotra", "Farhad Imani"], "title": "Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SC"], "comment": null, "summary": "Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\\times$ for training and 175 to 1000$\\times$ for inference. Furthermore, HDC reduces training times by 200$\\times$ and inference times by 300 to 600$\\times$, showcasing its potential for energy-efficient smart manufacturing.", "AI": {"tldr": "本文评估了超维计算（HDC）在智能制造中的应用，通过几何质量预测任务比较了HDC与传统AI模型的能耗、精度和速度表现。", "motivation": "智能制造虽然能提高效率和降低能耗，但传统AI模型的高能耗可能抵消这些收益。需要寻找更节能的AI方法来实现可持续制造。", "method": "使用原位传感数据进行智能加工中的几何质量预测，比较超维计算（HDC）与常见AI模型的能耗、精度和速度表现。", "result": "HDC在保持与传统模型相当精度的同时，训练能耗降低200倍，推理能耗降低175-1000倍；训练时间减少200倍，推理时间减少300-600倍。", "conclusion": "超维计算（HDC）在智能制造中展现出巨大的节能潜力，能够显著降低AI模型的能耗和计算时间，为实现可持续制造提供了有前景的解决方案。"}}
{"id": "2512.03862", "pdf": "https://arxiv.org/pdf/2512.03862", "abs": "https://arxiv.org/abs/2512.03862", "authors": ["Oli Bridge", "Huey Sun", "Botond Branyicskai-Nagy", "Charles D'Ornano", "Shomit Basu"], "title": "Diminishing Returns in Self-Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.", "AI": {"tldr": "研究小型视觉Transformer（5M参数）在不同预训练、中间微调和下游任务中的边际效益，发现存在收益递减现象", "motivation": "虽然基于Transformer的架构在计算机视觉和NLP领域表现出色，但它们通常需要大量参数和训练数据才能获得强大性能。本研究旨在探索小型ViT在不同训练阶段和数据选择下的边际效益", "method": "使用三个不同的数据集和训练目标进行实验：预训练数据集、中间微调数据集和下游数据集。在5M参数的小型视觉Transformer上测试不同训练阶段的组合效果", "result": "预训练和微调总是有帮助但存在收益递减；中间微调可能对下游性能产生负面影响（特别是任务机制不相似时）；无选择地堆叠中间任务会浪费计算资源甚至降低性能", "conclusion": "小型视觉Transformer最受益于有针对性的预训练和谨慎的数据选择，而不加选择地堆叠中间任务既浪费计算资源又可能损害性能"}}
{"id": "2512.03854", "pdf": "https://arxiv.org/pdf/2512.03854", "abs": "https://arxiv.org/abs/2512.03854", "authors": ["Peshawa J. Muhammad Ali", "Navin Vincent", "Saman S. Abdulla", "Han N. Mohammed Fadhl", "Anders Blilie", "Kelvin Szolnoky", "Julia Anna Mielcarz", "Xiaoyi Ji", "Kimmo Kartasalo", "Abdulbasit K. Al-Talabani", "Nita Mulliqi"], "title": "Prostate biopsy whole slide image dataset from an underrepresented Middle Eastern population", "categories": ["cs.CV"], "comment": "13 pages, 2 figures and 1 table", "summary": "Artificial intelligence (AI) is increasingly used in digital pathology. Publicly available histopathology datasets remain scarce, and those that do exist predominantly represent Western populations. Consequently, the generalizability of AI models to populations from less digitized regions, such as the Middle East, is largely unknown. This motivates the public release of our dataset to support the development and validation of pathology AI models across globally diverse populations. We present 339 whole-slide images of prostate core needle biopsies from a consecutive series of 185 patients collected in Erbil, Iraq. The slides are associated with Gleason scores and International Society of Urological Pathology grades assigned independently by three pathologists. Scanning was performed using two high-throughput scanners (Leica and Hamamatsu) and one compact scanner (Grundium). All slides were de-identified and are provided in their native formats without further conversion. The dataset enables grading concordance analyses, color normalization, and cross-scanner robustness evaluations. Data will be deposited in the Bioimage Archive (BIA) under accession code: to be announced (TBA), and released under a CC BY 4.0 license.", "AI": {"tldr": "该论文发布了一个来自中东地区（伊拉克埃尔比勒）的前列腺活检全切片图像数据集，包含339张全切片图像，来自185名患者，旨在解决AI病理学模型在非西方人群代表性不足的问题。", "motivation": "当前公开的病理学数据集主要来自西方人群，缺乏中东等欠数字化地区的代表性，这限制了AI模型在全球不同人群中的泛化能力。为了支持跨人群的病理AI模型开发和验证，需要发布来自代表性不足地区的数据集。", "method": "收集了伊拉克埃尔比勒185名患者的连续系列前列腺核心针活检样本，获得339张全切片图像。由三位病理学家独立分配Gleason评分和国际泌尿病理学会分级。使用三种不同扫描仪（Leica、Hamamatsu、Grundium）进行扫描，数据以原始格式提供，经过去标识化处理。", "result": "创建了一个包含339张前列腺活检全切片图像的数据集，来自中东地区代表性不足的人群。数据集包含病理分级信息，支持多种应用分析，包括分级一致性分析、颜色归一化和跨扫描仪鲁棒性评估。数据将在Bioimage Archive中公开，采用CC BY 4.0许可。", "conclusion": "该数据集填补了中东地区前列腺病理学公开数据的空白，为开发更具全球代表性的AI病理学模型提供了重要资源，有助于评估和改进模型在不同人群和不同扫描仪间的泛化能力。"}}
{"id": "2512.03847", "pdf": "https://arxiv.org/pdf/2512.03847", "abs": "https://arxiv.org/abs/2512.03847", "authors": ["Dingwei Zhu", "Zhiheng Xi", "Shihan Dou", "Yuhui Wang", "Sixian Li", "Junjie Ye", "Honglin Guo", "Shichun Liu", "Chenhao Huang", "Yajie Yang", "Junlin Shang", "Senjie Jin", "Ming Zhang", "Jiazheng Zhang", "Caishuang Huang", "Yunke Zhang", "Demei Yan", "Yuran Wang", "Tao Gui"], "title": "DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.", "AI": {"tldr": "提出DVPO框架，结合条件风险理论与分布价值建模，用于在噪声监督下进行LLM后训练，平衡鲁棒性与泛化能力", "motivation": "现实世界LLM部署常面临噪声或不完整监督，现有方法（如最坏情况优化和均值方法）可能忽视泛化能力，产生过于保守的策略，在不同场景下表现不均", "method": "DVPO框架结合条件风险理论与分布价值建模，学习token级价值分布提供细粒度监督，应用非对称风险正则化：收缩下尾抑制噪声负偏差，扩展上尾保持探索多样性", "result": "在多轮对话、数学推理和科学问答等广泛实验中，DVPO在噪声监督下持续优于PPO、GRPO和基于鲁棒Bellman的PPO", "conclusion": "DVPO展示了在现实世界LLM后训练中的潜力，通过分布价值建模和非对称风险正则化，在噪声监督下实现了鲁棒性与泛化能力的更好平衡"}}
{"id": "2512.03844", "pdf": "https://arxiv.org/pdf/2512.03844", "abs": "https://arxiv.org/abs/2512.03844", "authors": ["Letian Zhou", "Songhua Liu", "Xinchao Wang"], "title": "CoDA: From Text-to-Image Diffusion Models to Training-Free Dataset Distillation", "categories": ["cs.CV"], "comment": "34 pages, 24 figures", "summary": "Prevailing Dataset Distillation (DD) methods leveraging generative models confront two fundamental limitations. First, despite pioneering the use of diffusion models in DD and delivering impressive performance, the vast majority of approaches paradoxically require a diffusion model pre-trained on the full target dataset, undermining the very purpose of DD and incurring prohibitive training costs. Second, although some methods turn to general text-to-image models without relying on such target-specific training, they suffer from a significant distributional mismatch, as the web-scale priors encapsulated in these foundation models fail to faithfully capture the target-specific semantics, leading to suboptimal performance. To tackle these challenges, we propose Core Distribution Alignment (CoDA), a framework that enables effective DD using only an off-the-shelf text-to-image model. Our key idea is to first identify the \"intrinsic core distribution\" of the target dataset using a robust density-based discovery mechanism. We then steer the generative process to align the generated samples with this core distribution. By doing so, CoDA effectively bridges the gap between general-purpose generative priors and target semantics, yielding highly representative distilled datasets. Extensive experiments suggest that, without relying on a generative model specifically trained on the target dataset, CoDA achieves performance on par with or even superior to previous methods with such reliance across all benchmarks, including ImageNet-1K and its subsets. Notably, it establishes a new state-of-the-art accuracy of 60.4% at the 50-images-per-class (IPC) setup on ImageNet-1K. Our code is available on the project webpage: https://github.com/zzzlt422/CoDA", "AI": {"tldr": "提出CoDA框架，使用现成的文本到图像扩散模型实现无训练的数据集蒸馏，无需在目标数据集上预训练生成模型", "motivation": "现有数据集蒸馏方法存在两个根本限制：1）需要目标数据集上预训练的扩散模型，违背了数据集蒸馏的初衷且成本高昂；2）使用通用文本到图像模型时存在分布不匹配问题，无法准确捕捉目标特定语义", "method": "首先通过鲁棒的基于密度的发现机制识别目标数据集的\"内在核心分布\"，然后引导生成过程使生成的样本与该核心分布对齐，从而弥合通用生成先验与目标语义之间的差距", "result": "在包括ImageNet-1K及其子集的所有基准测试中，CoDA达到了与或优于依赖目标数据集预训练模型的方法的性能，在ImageNet-1K的50-images-per-class设置下达到了60.4%的最新准确率", "conclusion": "CoDA框架成功解决了数据集蒸馏中的两个关键挑战，仅使用现成的文本到图像模型就能实现高效的数据集蒸馏，为数据集蒸馏提供了更实用和成本效益高的解决方案"}}
{"id": "2512.03843", "pdf": "https://arxiv.org/pdf/2512.03843", "abs": "https://arxiv.org/abs/2512.03843", "authors": ["Malory Marin", "Jean-Florent Raymond", "Rémi Watrigant"], "title": "Robust Algorithms for Path and Cycle Problems in Geometric Intersection Graphs", "categories": ["cs.DS", "cs.CG"], "comment": null, "summary": "We study the design of robust subexponential algorithms for classical connectivity problems on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$. In this setting, each vertex corresponds to a geometric object, and two vertices are adjacent if and only if their objects intersect. We introduce a new tool for designing such algorithms, which we call a $λ$-linked partition. This is a partition of the vertex set into groups of highly connected vertices. Crucially, such a partition can be computed in polynomial time and does not require access to the geometric representation of the graph. We apply this framework to problems related to paths and cycles in graphs. First, we obtain the first robust ETH-tight algorithms for Hamiltonian Path and Hamiltonian Cycle, running in time $2^{O(n^{1-1/d})}$ on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$. This resolves an open problem of de Berg et al. [STOC 2018] and completes the study of these problems on geometric intersection graphs from the viewpoint of ETH-tight exact algorithms. We further extend our approach to the parameterized setting and design the first robust subexponential parameterized algorithm for Long Path in any fixed dimension $d$. More precisely, we obtain a randomized robust algorithm running in time $2^{O(k^{1-1/d}\\log^2 k)}\\, n^{O(1)}$ on intersection graphs of similarly sized fat objects in $\\mathbb{R}^d$, where $k$ is the natural parameter. Besides $λ$-linked partitions, our algorithm also relies on a low-treewidth pattern covering theorem that we establish for geometric intersection graphs, which may be viewed as a refinement of a result of Marx-Pilipczuk [ESA 2017]. This structural result may be of independent interest.", "AI": {"tldr": "该论文研究了几何交图中路径和周期问题的鲁棒算法设计，提出了λ-linked partition新工具，解决了de Berg等人提出的开放问题，并建立了几何交图的低树宽模式覆盖定理。", "motivation": "研究几何交图中经典连通性问题的鲁棒亚指数算法设计，解决de Berg等人在STOC 2018中提出的开放问题，完成从ETH紧确算法角度对几何交图中这些问题的研究。", "method": "引入λ-linked partition作为核心工具，这是一种将顶点集划分为高度连通组的划分方法，可以在多项式时间内计算且不需要几何表示。结合几何交图的低树宽模式覆盖定理，设计了针对Hamiltonian Path、Hamiltonian Cycle和Long Path的算法。", "result": "获得了Hamiltonian Path和Hamiltonian Cycle在d维空间中相似大小胖对象交图上的第一个鲁棒ETH紧确算法，运行时间为2^{O(n^{1-1/d})}。设计了Long Path的第一个鲁棒亚指数参数化算法，运行时间为2^{O(k^{1-1/d}log^2 k)} n^{O(1)}。", "conclusion": "提出的λ-linked partition框架和低树宽模式覆盖定理为几何交图中的路径和周期问题提供了有效的算法设计工具，解决了重要开放问题，相关结构结果可能具有独立的研究价值。"}}
{"id": "2512.03828", "pdf": "https://arxiv.org/pdf/2512.03828", "abs": "https://arxiv.org/abs/2512.03828", "authors": ["Dominykas Strazdas", "Magnus Jung", "Jan Marquenie", "Ingo Siegert", "Ayoub Al-Hamadi"], "title": "IM HERE: Interaction Model for Human Effort Based Robot Engagement", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "The effectiveness of human-robot interaction often hinges on the ability to cultivate engagement - a dynamic process of cognitive involvement that supports meaningful exchanges. Many existing definitions and models of engagement are either too vague or lack the ability to generalize across different contexts. We introduce IM HERE, a novel framework that models engagement effectively in human-human, human-robot, and robot-robot interactions. By employing an effort-based description of bilateral relationships between entities, we provide an accurate breakdown of relationship patterns, simplifying them to focus placement and four key states. This framework captures mutual relationships, group behaviors, and actions conforming to social norms, translating them into specific directives for autonomous systems. By integrating both subjective perceptions and objective states, the model precisely identifies and describes miscommunication. The primary objective of this paper is to automate the analysis, modeling, and description of social behavior, and to determine how autonomous systems can behave in accordance with social norms for full social integration while simultaneously pursuing their own social goals.", "AI": {"tldr": "提出IM HERE框架，通过基于努力的描述来建模人机交互中的参与度，能够准确分析双边关系模式并将其简化为焦点放置和四种关键状态", "motivation": "现有参与度定义和模型要么过于模糊，要么缺乏跨不同情境的泛化能力，需要一种能够有效建模人-人、人-机、机-机交互中参与度的框架", "method": "采用基于努力的描述方法来分析实体间的双边关系，将复杂关系模式简化为焦点放置和四种关键状态，整合主观感知和客观状态来识别误沟通", "result": "IM HERE框架能够捕捉相互关系、群体行为以及符合社会规范的行动，并将其转化为自主系统的具体指令，实现社会行为的自动化分析和建模", "conclusion": "该框架使自主系统能够按照社会规范行为，实现完全的社会整合，同时追求自身的社会目标，为人机交互中的参与度建模提供了有效解决方案"}}
{"id": "2512.03827", "pdf": "https://arxiv.org/pdf/2512.03827", "abs": "https://arxiv.org/abs/2512.03827", "authors": ["Alexey Protopopov"], "title": "A Robust Camera-based Method for Breath Rate Measurement", "categories": ["cs.CV"], "comment": "9 pages, 4 figures, 2 tables", "summary": "Proliferation of cheap and accessible cameras makes it possible to measure a subject's breath rate from video footage alone. Recent works on this topic have proposed a variety of approaches for accurately measuring human breath rate, however they are either tested in near-ideal conditions, or produce results that are not sufficiently accurate. The present study proposes a more robust method to measure breath rate in humans with minimal hardware requirements using a combination of mathematical transforms with a relative deviation from the ground truth of less than 5%. The method was tested on videos taken from 14 volunteers with a total duration of over 2 hours 30 minutes. The obtained results were compared to reference data and the average mean absolute error was found to be at 0.57 respirations per minute, which is noticeably better than the results from previous works. The breath rate measurement method proposed in the present article is more resistant to distortions caused by subject movement and thus allows one to remotely measure the subject's breath rate without any significant limitations on the subject's behavior.", "AI": {"tldr": "提出一种基于摄像头的鲁棒呼吸率测量方法，能够在受试者运动的情况下准确测量呼吸频率", "motivation": "现有基于摄像头的呼吸率测量方法要么在理想条件下测试，要么准确性不足，需要一种更鲁棒的方法来应对受试者运动等干扰", "method": "结合数学变换的方法，使用普通摄像头，对14名志愿者超过2小时30分钟的视频进行测试", "result": "相对于地面真实值的偏差小于5%，平均绝对误差为0.57次呼吸/分钟，明显优于先前工作", "conclusion": "该方法对受试者运动引起的失真具有更强的抵抗能力，可以在不受受试者行为显著限制的情况下远程测量呼吸率"}}
{"id": "2512.03817", "pdf": "https://arxiv.org/pdf/2512.03817", "abs": "https://arxiv.org/abs/2512.03817", "authors": ["Ahmed Nasser", "Marwan Mohamed", "Alaa Sherif", "Basmala Mahmoud", "Shereen Yehia", "Asmaa Saad", "Mariam S. El-Rahmany", "Ensaf H. Mohamed"], "title": "HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.", "AI": {"tldr": "提出一种自动识别和翻译古埃及象形文字的方法，将图像中的象形文字翻译成英文", "motivation": "埃及象形文字完全由图画组成，单个符号可能具有多种含义，翻译面临挑战。深度学习翻译应用发展迅速，可以显著影响生活", "method": "使用三个阶段的流程：1) 分割（使用Contour和Detectron2），2) 将符号映射到Gardiner代码，3) 翻译（使用CNN模型）。使用Morris Franken数据集和EgyptianTranslation数据集进行分类和翻译", "result": "模型获得了42.2的BLEU分数，与先前研究相比取得了显著成果", "conclusion": "提出的方法能够有效地自动识别和翻译古埃及象形文字，在翻译质量上取得了重要进展"}}
{"id": "2512.03796", "pdf": "https://arxiv.org/pdf/2512.03796", "abs": "https://arxiv.org/abs/2512.03796", "authors": ["Hong-Kai Zheng", "Piji Li"], "title": "LSRS: Latent Scale Rejection Sampling for Visual Autoregressive Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Visual Autoregressive (VAR) modeling approach for image generation proposes autoregressive processing across hierarchical scales, decoding multiple tokens per scale in parallel. This method achieves high-quality generation while accelerating synthesis. However, parallel token sampling within a scale may lead to structural errors, resulting in suboptimal generated images. To mitigate this, we propose Latent Scale Rejection Sampling (LSRS), a method that progressively refines token maps in the latent scale during inference to enhance VAR models. Our method uses a lightweight scoring model to evaluate multiple candidate token maps sampled at each scale, selecting the high-quality map to guide subsequent scale generation. By prioritizing early scales critical for structural coherence, LSRS effectively mitigates autoregressive error accumulation while maintaining computational efficiency. Experiments demonstrate that LSRS significantly improves VAR's generation quality with minimal additional computational overhead. For the VAR-d30 model, LSRS increases the inference time by merely 1% while reducing its FID score from 1.95 to 1.78. When the inference time is increased by 15%, the FID score can be further reduced to 1.66. LSRS offers an efficient test-time scaling solution for enhancing VAR-based generation.", "AI": {"tldr": "提出LSRS方法，通过在推理阶段对视觉自回归模型的潜在尺度进行渐进式拒绝采样，提升图像生成质量", "motivation": "视觉自回归模型在并行处理同一尺度内的多个token时可能导致结构错误，影响生成图像质量，需要一种在推理阶段高效改进的方法", "method": "提出潜在尺度拒绝采样方法，使用轻量级评分模型评估每个尺度采样的多个候选token图，选择高质量图指导后续尺度生成，优先处理对结构一致性关键的前期尺度", "result": "LSRS显著提升VAR模型生成质量，计算开销极小。VAR-d30模型推理时间仅增加1%时，FID从1.95降至1.78；推理时间增加15%时，FID可进一步降至1.66", "conclusion": "LSRS为基于VAR的生成模型提供了高效的测试时扩展解决方案，能有效缓解自回归误差累积，同时保持计算效率"}}
{"id": "2512.03795", "pdf": "https://arxiv.org/pdf/2512.03795", "abs": "https://arxiv.org/abs/2512.03795", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "categories": ["cs.RO", "cs.AI"], "comment": "17 pages, 18 figures", "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "AI": {"tldr": "提出MPCFormer方法，一种结合物理先验和数据驱动的可解释社交感知自动驾驶方法，用于解决自动驾驶在动态交互场景中缺乏人类行为的问题", "motivation": "自动驾驶车辆在高度动态和交互的交通场景中难以表现出类人行为，主要挑战在于缺乏对社交交互底层机制的理解，限制了与周围车辆的交互能力", "method": "将社交交互动力学建模为离散状态空间表示，嵌入物理先验增强可解释性；通过Transformer编码器-解码器架构从自然驾驶数据中学习动力学系数；结合MPC框架确保安全性", "result": "在NGSIM数据集上，MPCFormer在5秒预测范围内达到最低轨迹预测误差（ADE低至0.86米）；在闭环实验中，规划成功率94.67%，驾驶效率提升15.75%，碰撞率从21.25%降至0.5%", "conclusion": "MPCFormer是首个显式建模多车辆社交交互动力学的方法，结合物理先验和数据驱动，能够生成多样化、类人的交互行为，同时通过MPC框架确保安全性，在社交交互感知和规划性能上优于现有方法"}}
{"id": "2512.03794", "pdf": "https://arxiv.org/pdf/2512.03794", "abs": "https://arxiv.org/abs/2512.03794", "authors": ["Zichuan Lin", "Yicheng Liu", "Yang Yang", "Lvfang Tao", "Deheng Ye"], "title": "AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "15 pages, 9 figures", "summary": "Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.", "AI": {"tldr": "提出AdaptVision，一种通过自适应视觉获取实现高效视觉语言模型的范式，采用从粗到细的方法减少视觉token数量", "motivation": "现有高效VLM方法通过固定比例压缩视觉token，但缺乏根据任务需求自适应调整的能力，导致计算开销仍然较大", "method": "采用从粗到细的方法：先处理低分辨率图像的压缩视觉token，必要时调用边界框工具裁剪关键区域；使用强化学习框架训练，核心是解耦转向策略优化(DTPO)，将学习目标分解为工具学习和准确性改进两个组件", "result": "在多个VQA基准测试中，AdaptVision在消耗显著更少视觉token的同时，实现了优于现有高效VLM方法的性能", "conclusion": "AdaptVision通过自适应视觉获取机制，使VLM能够自主确定每个样本所需的最小视觉token数量，在准确性和效率之间取得了良好平衡"}}
{"id": "2512.03788", "pdf": "https://arxiv.org/pdf/2512.03788", "abs": "https://arxiv.org/abs/2512.03788", "authors": ["Kamil Khadiev", "Vladislav Remidovskii", "Timur Bikmullin", "Aliya Khadieva"], "title": "Quantum Algorithm for Searching for the Longest Segment and the Largest Empty Rectangle", "categories": ["quant-ph", "cs.DS"], "comment": "accepted in SOFSEM2026 conference, In Proceedings LNCS vol.16448", "summary": "In the paper, we consider the problem of searching for the Largest empty rectangle in a 2D map, and the one-dimensional version of the problem is the problem of searching for the largest empty segment. We present a quantum algorithm for the Largest Empty Square problem and the Largest Empty Rectangle of a fixed width $d$ for $n\\times n$-rectangular map. Query complexity of the algorithm is $\\tilde{O}(n^{1.5})$ for the square case, and $\\tilde{O}(n\\sqrt{d})$ for the rectangle with a fixed width $d$ case, respectively. At the same time, the lower bounds for the classical case are $Ω(n^2)$, and $Ω(nd)$, respectively. The Quantum algorithm for the one-dimensional version of the problem has $O(\\sqrt{n}\\log n\\log\\log n)$ query complexity. The quantum lower bound for the problem is $Ω(\\sqrt{n})$ which is almost equal to the upper bound up to a log factor. The classical lower bound is $Ω(n)$. So, we obtain the quadratic speed-up for the problem.", "AI": {"tldr": "提出量子算法解决二维地图中最大空矩形搜索问题及其一维版本（最大空段搜索）", "motivation": "经典算法在搜索最大空矩形和最大空段问题上存在较高的查询复杂度（Ω(n²)和Ω(n)），需要开发量子算法以获得显著的加速效果", "method": "设计量子算法处理最大空正方形和固定宽度d的最大空矩形问题，利用量子计算特性优化查询复杂度", "result": "量子算法在正方形情况下达到Õ(n¹·⁵)查询复杂度，固定宽度d矩形情况下达到Õ(n√d)，一维版本达到O(√n log n log log n)，相比经典算法获得平方级加速", "conclusion": "量子算法在最大空矩形和最大空段搜索问题上实现了显著的量子加速，查询复杂度接近理论下界，为相关几何搜索问题提供了高效的量子解决方案"}}
{"id": "2512.03784", "pdf": "https://arxiv.org/pdf/2512.03784", "abs": "https://arxiv.org/abs/2512.03784", "authors": ["Guisong Liu", "Jiansong Zhang", "Yinpei Luo", "Guoliang Wei", "Shuqing Sun", "Shiyang Deng", "Pengfei Wei", "Nanxi Chen"], "title": "Sleep Modulation: The Challenge of Transitioning from Open Loop to Closed Loop", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Sleep disorders have emerged as a critical global health issue, highlighting the urgent need for effective and widely accessible intervention technologies. Non-invasive brain stimulation has garnered attention as it enables direct or indirect modulation of neural activity, thereby promoting sleep enhancement in a safe and unobtrusive manner. This class of approaches is collectively referred to as sleep modulation. To date, the majority of sleep modulation research relies on open-loop paradigms with empirically determined parameters, while achieving individual adaptation and modulation accuracy remains a distant objective. The paradigm-specific constraints inherent to open-loop designs represent a major obstacle to clinical translation and large-scale deployment in home environments. In this paper, we delineate fundamental paradigms of sleep modulation, critically examine the intrinsic limitations of open-loop approaches, and formally conceptualize sleep closed-loop modulation. We further provide a comprehensive synthesis of prior studies involving five commonly employed modulation techniques, evaluating their potential integration within a closed-loop framework. Finally, we identify three primary challenges in constructing an effective sleep closed-loop modulation system: sensor solution selection, monitoring model design, and modulation strategy design, while also proposing potential solutions. Collectively, this work aims to advance the paradigm shift of sleep modulation from open-loop toward closed-loop systems.", "AI": {"tldr": "该论文探讨了睡眠调节技术从开环系统向闭环系统的范式转变，旨在解决睡眠障碍这一全球健康问题。", "motivation": "睡眠障碍已成为全球性健康问题，需要有效且广泛可及的干预技术。当前大多数睡眠调节研究依赖于经验参数的开环范式，缺乏个体适应性和调节精度，限制了临床转化和家庭环境的大规模部署。", "method": "论文首先界定了睡眠调节的基本范式，批判性地分析了开环方法的内在局限性，并正式概念化了睡眠闭环调节。然后综合分析了五种常用调节技术的研究，评估它们在闭环框架中的整合潜力。最后提出了构建有效睡眠闭环调节系统的三个主要挑战及潜在解决方案。", "result": "论文系统性地识别了构建睡眠闭环调节系统的三个核心挑战：传感器解决方案选择、监测模型设计和调节策略设计，并为每个挑战提出了潜在的解决方案方向。", "conclusion": "该工作旨在推动睡眠调节从开环系统向闭环系统的范式转变，通过解决传感器选择、监测模型和调节策略等关键挑战，实现更精准、个性化的睡眠干预，促进临床转化和家庭环境的大规模应用。"}}
{"id": "2512.03783", "pdf": "https://arxiv.org/pdf/2512.03783", "abs": "https://arxiv.org/abs/2512.03783", "authors": ["Dongchao Yang", "Songxiang Liu", "Disong Wang", "Yuanyuan Wang", "Guanglu Wan", "Helen Meng"], "title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "categories": ["cs.AI", "cs.SD"], "comment": null, "summary": "Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.", "AI": {"tldr": "提出Omni-AutoThink自适应多模态推理框架，通过强化学习动态调整模型推理深度以适应任务难度", "motivation": "现有Omni模型在统一多模态感知和生成方面取得进展，但推理行为仍然僵化，要么对简单问题过度推理，要么在需要推理时失败，需要自适应推理能力", "method": "包含两个阶段：1) 自适应监督微调阶段，使用大规模推理增强数据赋予基础推理能力；2) 自适应强化学习阶段，基于任务复杂度和奖励反馈优化推理行为", "result": "实验结果表明，相比先前基线，提出的框架显著提升了自适应推理性能，并构建了涵盖文本、文本-音频、文本-视觉、文本-音频-视觉模态的全面自适应推理基准", "conclusion": "Omni-AutoThink框架通过自适应调整推理深度，有效解决了多模态推理中的僵化问题，提升了模型在不同难度任务上的表现"}}
{"id": "2512.03774", "pdf": "https://arxiv.org/pdf/2512.03774", "abs": "https://arxiv.org/abs/2512.03774", "authors": ["Johannes Fischer", "Marlon Steiner", "Ömer Sahin Tas", "Christoph Stiller"], "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "categories": ["cs.RO"], "comment": "ef:2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), Bilbao, Spain, 2023, pp. 2811-2818", "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "AI": {"tldr": "提出了一种结合模型预测控制（MPC）和强化学习（RL）的安全增强模型预测控制（SRMPC）方法，用于自动驾驶运动规划，通过强化学习在MPC中生成新的安全参考轨迹，以寻找全局最优解。", "motivation": "传统MPC在实时规划中需要使用最优控制问题的凸近似，但这种近似将解限制在子空间中，可能无法找到全局最优解。需要一种方法能够在MPC框架内探索更广泛的解空间。", "method": "使用安全强化学习（SRL）在MPC中生成新的安全参考轨迹，采用约束强化学习（CRL）确保自动驾驶安全性，使用基于能量函数的安全指数作为约束目标来建模安全/不安全区域，同时学习状态相关的拉格朗日乘子和安全策略。", "result": "在高速公路场景的实验表明，该方法在安全性和性能指标上均优于单独的MPC和SRL方法。", "conclusion": "SRMPC方法成功地将强化学习与MPC结合，通过强化学习的探索能力扩展MPC的解空间，同时通过约束强化学习确保安全性，为自动驾驶运动规划提供了更优的解决方案。"}}
{"id": "2512.03772", "pdf": "https://arxiv.org/pdf/2512.03772", "abs": "https://arxiv.org/abs/2512.03772", "authors": ["Gabriele Fadini", "Deepak Ingole", "Tong Duy Son", "Alisa Rupenyan"], "title": "Bayesian Optimization for Automatic Tuning of Torque-Level Nonlinear Model Predictive Control", "categories": ["cs.RO", "cs.AI", "eess.SY"], "comment": "6 pages, 7 figures, 3 tables", "summary": "This paper presents an auto-tuning framework for torque-based Nonlinear Model Predictive Control (nMPC), where the MPC serves as a real-time controller for optimal joint torque commands. The MPC parameters, including cost function weights and low-level controller gains, are optimized using high-dimensional Bayesian Optimization (BO) techniques, specifically Sparse Axis-Aligned Subspace (SAASBO) with a digital twin (DT) to achieve precise end-effector trajectory real-time tracking on an UR10e robot arm. The simulation model allows efficient exploration of the high-dimensional parameter space, and it ensures safe transfer to hardware. Our simulation results demonstrate significant improvements in tracking performance (+41.9%) and reduction in solve times (-2.5%) compared to manually-tuned parameters. Moreover, experimental validation on the real robot follows the trend (with a +25.8% improvement), emphasizing the importance of digital twin-enabled automated parameter optimization for robotic operations.", "AI": {"tldr": "提出一个基于贝叶斯优化的自动调参框架，用于优化扭矩级非线性模型预测控制器的参数，以提高UR10e机械臂末端执行器的轨迹跟踪性能。", "motivation": "手动调参非线性模型预测控制器参数耗时且难以达到最优性能，特别是在高维参数空间中。需要一种自动化方法来优化MPC参数，包括成本函数权重和底层控制器增益，以实现精确的实时轨迹跟踪。", "method": "使用高维贝叶斯优化技术（特别是SAASBO稀疏轴对齐子空间方法），结合数字孪生模型，在仿真环境中高效探索高维参数空间。通过数字孪生确保参数安全转移到实际硬件。", "result": "仿真结果显示跟踪性能提升41.9%，求解时间减少2.5%。在实际UR10e机械臂上的实验验证也显示出25.8%的性能提升，证明了数字孪生支持的自动参数优化的重要性。", "conclusion": "基于贝叶斯优化的自动调参框架能显著提高扭矩级非线性模型预测控制的性能，数字孪生方法确保了从仿真到硬件的安全转移，为机器人操作提供了有效的参数优化解决方案。"}}
{"id": "2512.03771", "pdf": "https://arxiv.org/pdf/2512.03771", "abs": "https://arxiv.org/abs/2512.03771", "authors": ["Itay Yona", "Amir Sarid", "Michael Karasik", "Yossi Gandelsman"], "title": "In-Context Representation Hijacking", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "We introduce $\\textbf{Doublespeak}$, a simple in-context representation hijacking attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., bomb) with a benign token (e.g., carrot) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., \"How to build a carrot?\") are internally interpreted as disallowed instructions (e.g., \"How to build a bomb?\"), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.", "AI": {"tldr": "提出一种名为Doublespeak的上下文表示劫持攻击方法，通过系统性地将有害关键词替换为良性标记，使LLMs内部将良性词汇的表示收敛到有害词汇的表示，从而绕过安全对齐机制。", "motivation": "当前LLMs的安全对齐策略存在不足，攻击者可能通过操纵上下文中的词汇表示来绕过安全防护。研究者希望揭示LLMs在潜在表示层面的新攻击面，证明现有对齐方法在表示层面的脆弱性。", "method": "Doublespeak攻击方法：1）在多个上下文示例中系统性地将有害关键词（如\"bomb\"）替换为良性标记（如\"carrot\"）；2）给定有害请求的前缀；3）利用解释性工具分析表示层的变化，展示良性含义在早期层逐渐收敛为有害语义的过程。", "result": "攻击成功率高：在闭源和开源系统上广泛可迁移，在Llama-3.3-70B-Instruct上单句上下文覆盖达到74%攻击成功率。语义覆盖逐层出现：早期层保持良性含义，后期层收敛为有害语义。无需优化：攻击无需参数优化。", "conclusion": "揭示了LLMs潜在表示空间的新攻击面，表明当前基于表层文本的安全对齐策略不足，需要在表示层面进行更深入的安全防护。攻击的简单性和高成功率突显了LLMs安全对齐的脆弱性。"}}
{"id": "2512.03756", "pdf": "https://arxiv.org/pdf/2512.03756", "abs": "https://arxiv.org/abs/2512.03756", "authors": ["Marlon Steiner", "Royden Wagner", "Ömer Sahin Tas", "Christoph Stiller"], "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "categories": ["cs.RO"], "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025", "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "AI": {"tldr": "该论文研究将导航信息集成到基于注意力的运动预测模型中，以桥接多智能体运动预测和目标导向的运动规划，实现预测驱动的运动规划框架。", "motivation": "结合运动预测和运动规划可以增强自动驾驶车辆与其他交通参与者之间的交互，但这带来了两个挑战：如何基于导航目标进行预测，以及如何确保稳定且运动学可行的轨迹。本文主要解决第一个挑战。", "method": "提出并评估了多种架构导航集成策略，将自车的预期路线和目标姿态集成到基于注意力的运动预测模型架构中，在nuPlan数据集上进行实验验证。", "result": "研究结果表明预测驱动的运动规划具有潜力，导航信息可以同时增强预测和规划任务，实现了多智能体运动预测与目标导向运动规划之间的桥梁。", "conclusion": "通过将导航信息集成到注意力模型中，成功解决了基于导航目标的预测问题，为预测驱动的运动规划提供了有效的解决方案，展示了导航信息对预测和规划任务的双重增强作用。"}}
{"id": "2512.03746", "pdf": "https://arxiv.org/pdf/2512.03746", "abs": "https://arxiv.org/abs/2512.03746", "authors": ["Zirun Guo", "Minjie Hong", "Feng Zhang", "Kai Jia", "Tao Jin"], "title": "Thinking with Programming Vision: Towards a Unified View for Thinking with Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.", "AI": {"tldr": "提出CodeVision框架，通过代码作为通用接口调用任意图像操作，解决多模态大语言模型在图像处理中的鲁棒性和可扩展性问题", "motivation": "现有MLLMs在图像处理中存在显著脆弱性，即使是最先进的模型在简单方向变化或自然损坏的图像上也会出现性能显著下降，且现有工具方法依赖有限工具集，缺乏现实必要性和可扩展性", "method": "提出CodeVision框架，将代码作为通用工具接口；采用两阶段训练方法：首先在高质量数据集上进行监督微调，然后使用新颖密集过程奖励函数进行强化学习，鼓励战略性和高效的工具使用", "result": "在Qwen2.5-VL和Qwen3-VL系列上的实验表明，该方法显著提高了模型性能，并培养了灵活工具组合、高效链式执行和从运行时反馈中稳健错误恢复等新兴能力", "conclusion": "CodeVision框架通过代码作为通用工具接口，解决了MLLMs在图像处理中的鲁棒性和可扩展性问题，为图像思维提供了更统一和强大的解决方案"}}
{"id": "2512.03745", "pdf": "https://arxiv.org/pdf/2512.03745", "abs": "https://arxiv.org/abs/2512.03745", "authors": ["Jiaze Li", "Yan Lu", "Bin Liu", "Guojun Yin", "Mang Ye"], "title": "Dual-level Modality Debiasing Learning for Unsupervised Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Two-stage learning pipeline has achieved promising results in unsupervised visible-infrared person re-identification (USL-VI-ReID). It first performs single-modality learning and then operates cross-modality learning to tackle the modality discrepancy. Although promising, this pipeline inevitably introduces modality bias: modality-specific cues learned in the single-modality training naturally propagate into the following cross-modality learning, impairing identity discrimination and generalization. To address this issue, we propose a Dual-level Modality Debiasing Learning (DMDL) framework that implements debiasing at both the model and optimization levels. At the model level, we propose a Causality-inspired Adjustment Intervention (CAI) module that replaces likelihood-based modeling with causal modeling, preventing modality-induced spurious patterns from being introduced, leading to a low-biased model. At the optimization level, a Collaborative Bias-free Training (CBT) strategy is introduced to interrupt the propagation of modality bias across data, labels, and features by integrating modality-specific augmentation, label refinement, and feature alignment. Extensive experiments on benchmark datasets demonstrate that DMDL could enable modality-invariant feature learning and a more generalized model.", "AI": {"tldr": "提出DMDL框架，通过模型级和优化级的双重去偏学习来解决无监督可见光-红外行人重识别中的模态偏差问题", "motivation": "传统两阶段学习流程在单模态训练阶段会学习到模态特定的线索，这些偏差会传播到跨模态学习阶段，损害身份判别和泛化能力", "method": "提出双重模态去偏学习框架：1) 模型级：因果启发的调整干预模块，用因果建模替代基于似然的建模；2) 优化级：协作无偏训练策略，通过模态特定增强、标签精炼和特征对齐来阻断模态偏差传播", "result": "在基准数据集上的大量实验表明，DMDL能够实现模态不变的特征学习和更泛化的模型", "conclusion": "提出的双重模态去偏学习框架有效解决了无监督可见光-红外行人重识别中的模态偏差问题，实现了更好的模态不变特征表示"}}
{"id": "2512.03743", "pdf": "https://arxiv.org/pdf/2512.03743", "abs": "https://arxiv.org/abs/2512.03743", "authors": ["Kehlani Fay", "Darin Anthony Djapri", "Anya Zorin", "James Clinton", "Ali El Lahib", "Hao Su", "Michael T. Tolley", "Sha Yi", "Xiaolong Wang"], "title": "Cross-embodied Co-design for Dexterous Hands", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.", "AI": {"tldr": "提出一个协同设计框架，用于学习任务特定的手部形态和互补的灵巧控制策略，实现从设计、训练、制造到部署的端到端流程", "motivation": "灵巧操作受限于控制和设计两方面，目前对于什么使机械手最适合执行灵巧任务没有共识，这提出了一个基本挑战：如何设计和控制针对灵巧性优化的机器人机械手", "method": "提出协同设计框架，支持：1）广泛的形态搜索空间（关节、手指、手掌生成）；2）通过形态条件跨实体控制在大设计空间中进行可扩展评估；3）使用可访问组件进行现实世界制造", "result": "在多个灵巧任务中评估该方法，包括模拟和真实部署中的手内旋转。框架实现了端到端流程，可在24小时内设计、训练、制造和部署新的机器人手", "conclusion": "该协同设计框架为解决灵巧机械手的设计和控制问题提供了一种系统化方法，通过形态和控制的联合优化，实现了快速迭代和实际部署的能力"}}
{"id": "2512.03730", "pdf": "https://arxiv.org/pdf/2512.03730", "abs": "https://arxiv.org/abs/2512.03730", "authors": ["Melane Navaratnarajah", "David A. Kelly", "Hana Chockler"], "title": "Out-of-the-box: Black-box Causal Attacks on Object Detectors", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial perturbations are a useful way to expose vulnerabilities in object detectors. Existing perturbation methods are frequently white-box and architecture specific. More importantly, while they are often successful, it is rarely clear why they work. Insights into the mechanism of this success would allow developers to understand and analyze these attacks, as well as fine-tune the model to prevent them. This paper presents BlackCAtt, a black-box algorithm and a tool, which uses minimal, causally sufficient pixel sets to construct explainable, imperceptible, reproducible, architecture-agnostic attacks on object detectors. BlackCAtt combines causal pixels with bounding boxes produced by object detectors to create adversarial attacks that lead to the loss, modification or addition of a bounding box. BlackCAtt works across different object detectors of different sizes and architectures, treating the detector as a black box. We compare the performance of BlackCAtt with other black-box attack methods and show that identification of causal pixels leads to more precisely targeted and less perceptible attacks. On the COCO test dataset, our approach is 2.7 times better than the baseline in removing a detection, 3.86 times better in changing a detection, and 5.75 times better in triggering new, spurious, detections. The attacks generated by BlackCAtt are very close to the original image, and hence imperceptible, demonstrating the power of causal pixels.", "AI": {"tldr": "提出BlackCAtt算法，一种针对目标检测器的黑盒因果攻击方法，通过识别因果像素集来创建可解释、不可感知、可复现的对抗攻击", "motivation": "现有对抗扰动方法多为白盒攻击且针对特定架构，难以理解其工作原理，需要开发能够解释攻击机制的黑盒方法，帮助开发者理解和防御攻击", "method": "BlackCAtt算法使用最小因果充分像素集，结合目标检测器生成的边界框，创建导致边界框丢失、修改或新增的黑盒对抗攻击，不依赖检测器内部架构", "result": "在COCO测试集上，相比基线方法：移除检测效果提升2.7倍，修改检测效果提升3.86倍，触发虚假检测效果提升5.75倍；攻击生成的图像与原始图像非常接近，几乎不可感知", "conclusion": "因果像素识别能够实现更精确、更不可感知的黑盒攻击，BlackCAtt算法在不同架构和尺寸的目标检测器上均有效，为理解和防御对抗攻击提供了新工具"}}
{"id": "2512.03720", "pdf": "https://arxiv.org/pdf/2512.03720", "abs": "https://arxiv.org/abs/2512.03720", "authors": ["Tengyun Ma", "Jiaqi Yao", "Daojing He", "Shihao Peng", "Yu Li", "Shaohui Liu", "Zhuotao Tian"], "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for diverse applications. However, their uniform token processing paradigm introduces critical vulnerabilities in instruction handling, particularly when exposed to adversarial scenarios. In this work, we identify and propose a novel class of vulnerabilities, termed Tool-Completion Attack (TCA), which exploits function-calling mechanisms to subvert model behavior. To evaluate LLM robustness against such threats, we introduce the Tool-Completion benchmark, a comprehensive security assessment framework, which reveals that even state-of-the-art models remain susceptible to TCA, with surprisingly high attack success rates. To address these vulnerabilities, we introduce Context-Aware Hierarchical Learning (CAHL), a sophisticated mechanism that dynamically balances semantic comprehension with role-specific instruction constraints. CAHL leverages the contextual correlations between different instruction segments to establish a robust, context-aware instruction hierarchy. Extensive experiments demonstrate that CAHL significantly enhances LLM robustness against both conventional attacks and the proposed TCA, exhibiting strong generalization capabilities in zero-shot evaluations while still preserving model performance on generic tasks. Our code is available at https://github.com/S2AILab/CAHL.", "AI": {"tldr": "提出一种名为上下文感知分层学习（CAHL）的两步范式，通过动态平衡语义理解和角色特定指令约束，增强LLMs在对抗性场景下的安全性，特别是针对工具完成攻击（TCA）的防御能力。", "motivation": "大型语言模型在处理指令时存在关键漏洞，特别是其统一的标记处理范式在面对对抗性场景时容易受到攻击。研究发现了一种新的漏洞类型——工具完成攻击（TCA），它利用函数调用机制颠覆模型行为，现有最先进的模型对此类攻击仍然易受攻击。", "method": "提出了上下文感知分层学习（CAHL）机制，该机制利用不同指令段之间的上下文相关性，建立鲁棒的、上下文感知的指令层次结构。CAHL通过动态平衡语义理解和角色特定指令约束来实现安全增强。", "result": "实验表明，CAHL显著增强了LLMs对传统攻击和提出的TCA的鲁棒性，在零样本评估中表现出强大的泛化能力，同时仍然保持模型在通用任务上的性能。提出的Tool-Completion基准测试显示，即使是最先进的模型对TCA的攻击成功率也出奇地高。", "conclusion": "CAHL作为一种两阶段学习范式，通过建立上下文感知的指令层次结构，有效解决了LLMs在对抗性场景下的安全漏洞问题，为构建更安全的语言模型提供了新的解决方案。"}}
{"id": "2512.03718", "pdf": "https://arxiv.org/pdf/2512.03718", "abs": "https://arxiv.org/abs/2512.03718", "authors": ["Robert Ganian", "Hung P. Hoang", "Simon Wietheger"], "title": "Matrix Editing Meets Fair Clustering: Parameterized Algorithms and Complexity", "categories": ["cs.DS", "cs.AI"], "comment": null, "summary": "We study the computational problem of computing a fair means clustering of discrete vectors, which admits an equivalent formulation as editing a colored matrix into one with few distinct color-balanced rows by changing at most $k$ values. While NP-hard in both the fairness-oblivious and the fair settings, the problem is well-known to admit a fixed-parameter algorithm in the former ``vanilla'' setting. As our first contribution, we exclude an analogous algorithm even for highly restricted fair means clustering instances. We then proceed to obtain a full complexity landscape of the problem, and establish tractability results which capture three means of circumventing our obtained lower bound: placing additional constraints on the problem instances, fixed-parameter approximation, or using an alternative parameterization targeting tree-like matrices.", "AI": {"tldr": "研究公平聚类中的矩阵编辑问题，探讨参数化算法和计算复杂性", "motivation": "虽然公平聚类问题在传统（非公平）设置下存在固定参数算法，但公平设置下的计算复杂性尚不清楚，需要研究公平聚类问题的参数化复杂性", "method": "将公平聚类问题转化为矩阵编辑问题，研究不同参数化下的算法复杂性，包括固定参数近似算法和针对树状矩阵的替代参数化方法", "result": "排除了公平聚类问题存在固定参数算法的可能性，但通过三种规避策略获得了可处理性结果：对问题实例施加额外约束、固定参数近似、或使用针对树状矩阵的替代参数化", "conclusion": "论文全面刻画了公平聚类问题的复杂性图景，为规避计算下界提供了多种可行途径，为公平聚类算法的设计提供了理论指导"}}
{"id": "2512.03715", "pdf": "https://arxiv.org/pdf/2512.03715", "abs": "https://arxiv.org/abs/2512.03715", "authors": ["Kaichen Zhang", "Tianxiang Sheng", "Xuanming Shi"], "title": "DINO-RotateMatch: A Rotation-Aware Deep Framework for Robust Image Matching in Large-Scale 3D Reconstruction", "categories": ["cs.CV"], "comment": "9 pages, 5 figures, 1 table", "summary": "This paper presents DINO-RotateMatch, a deep-learning framework designed to address the chal lenges of image matching in large-scale 3D reconstruction from unstructured Internet images. The method integrates a dataset-adaptive image pairing strategy with rotation-aware keypoint extraction and matching. DINO is employed to retrieve semantically relevant image pairs in large collections, while rotation-based augmentation captures orientation-dependent local features using ALIKED and Light Glue. Experiments on the Kaggle Image Matching Challenge 2025 demonstrate consistent improve ments in mean Average Accuracy (mAA), achieving a Silver Award (47th of 943 teams). The results confirm that combining self-supervised global descriptors with rotation-enhanced local matching offers a robust and scalable solution for large-scale 3D reconstruction.", "AI": {"tldr": "提出DINO-RotateMatch框架，通过旋转感知的深度学习方法解决大规模3D重建中的图像匹配挑战", "motivation": "解决从非结构化互联网图像进行大规模3D重建时，图像匹配面临的挑战，特别是旋转变化对特征匹配的影响", "method": "结合数据集自适应图像配对策略与旋转感知关键点提取匹配，使用DINO检索语义相关图像对，通过旋转增强使用ALIKED和Light Glue提取方向依赖的局部特征", "result": "在Kaggle Image Matching Challenge 2025上取得显著改进，获得银奖（943支队伍中排名第47），在平均准确率（mAA）上表现一致提升", "conclusion": "结合自监督全局描述符与旋转增强的局部匹配，为大规模3D重建提供了鲁棒且可扩展的解决方案"}}
{"id": "2512.03707", "pdf": "https://arxiv.org/pdf/2512.03707", "abs": "https://arxiv.org/abs/2512.03707", "authors": ["Sundas Rafat Mulkana", "Ronyu Yu", "Tanaya Guha", "Emma Li"], "title": "ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration", "categories": ["cs.RO"], "comment": "8 pages, 7 figures", "summary": "In collaborative human-robot tasks, safety requires not only avoiding collisions but also ensuring safe, intentional physical contact. We present ContactRL, a reinforcement learning (RL) based framework that directly incorporates contact safety into the reward function through force feedback. This enables a robot to learn adaptive motion profiles that minimize human-robot contact forces while maintaining task efficiency. In simulation, ContactRL achieves a low safety violation rate of 0.2\\% with a high task success rate of 87.7\\%, outperforming state-of-the-art constrained RL baselines. In order to guarantee deployment safety, we augment the learned policy with a kinetic energy based Control Barrier Function (eCBF) shield. Real-world experiments on an UR3e robotic platform performing small object handovers from a human hand across 360 trials confirm safe contact, with measured normal forces consistently below 10N. These results demonstrate that ContactRL enables safe and efficient physical collaboration, thereby advancing the deployment of collaborative robots in contact-rich tasks.", "AI": {"tldr": "提出ContactRL框架，通过强化学习结合力反馈实现人机协作中的安全接触运动规划", "motivation": "在协作人机任务中，安全不仅需要避免碰撞，还需要确保安全的、有意的物理接触。现有方法难以在保持任务效率的同时最小化接触力。", "method": "基于强化学习的框架，通过力反馈将接触安全直接纳入奖励函数，使机器人学习自适应运动轨迹。部署时使用基于动能的控制屏障函数(eCBF)屏蔽器保证安全性。", "result": "仿真中安全违规率仅0.2%，任务成功率87.7%，优于最先进的约束RL基线。真实实验中在UR3e平台上进行360次小物体交接，测得法向力始终低于10N。", "conclusion": "ContactRL能够实现安全高效的物理协作，推进协作机器人在接触丰富任务中的部署应用。"}}
{"id": "2512.03701", "pdf": "https://arxiv.org/pdf/2512.03701", "abs": "https://arxiv.org/abs/2512.03701", "authors": ["Paula Seidler", "Neill D. F. Campbell", "Ivor J A Simpson"], "title": "Structured Uncertainty Similarity Score (SUSS): Learning a Probabilistic, Interpretable, Perceptual Metric Between Images", "categories": ["cs.CV"], "comment": null, "summary": "Perceptual similarity scores that align with human vision are critical for both training and evaluating computer vision models. Deep perceptual losses, such as LPIPS, achieve good alignment but rely on complex, highly non-linear discriminative features with unknown invariances, while hand-crafted measures like SSIM are interpretable but miss key perceptual properties. We introduce the Structured Uncertainty Similarity Score (SUSS); it models each image through a set of perceptual components, each represented by a structured multivariate Normal distribution. These are trained in a generative, self-supervised manner to assign high likelihood to human-imperceptible augmentations. The final score is a weighted sum of component log-probabilities with weights learned from human perceptual datasets. Unlike feature-based methods, SUSS learns image-specific linear transformations of residuals in pixel space, enabling transparent inspection through decorrelated residuals and sampling. SUSS aligns closely with human perceptual judgments, shows strong perceptual calibration across diverse distortion types, and provides localized, interpretable explanations of its similarity assessments. We further demonstrate stable optimization behavior and competitive performance when using SUSS as a perceptual loss for downstream imaging tasks.", "AI": {"tldr": "提出一种新的感知相似度评分方法SUSS，通过概率生成模型学习图像间的感知度量，具有可解释性和良好的校准性", "motivation": "现有感知相似度方法存在局限性：深度感知损失（如LPIPS）虽然与人类视觉对齐良好，但依赖复杂非线性特征且缺乏可解释性；手工设计的度量（如SSIM）可解释但缺乏关键感知特性。需要一种既与人类感知对齐又具有可解释性的方法。", "method": "SUSS通过一组感知组件对图像建模，每个组件用结构化多元正态分布表示。采用生成式自监督训练，使模型给人眼不可察觉的增强赋予高似然度。最终评分是组件对数概率的加权和，权重从人类感知数据集中学习。与基于特征的方法不同，SUSS学习像素空间中残差的图像特定线性变换。", "result": "SUSS与人类感知判断高度对齐，在不同失真类型上表现出强大的感知校准能力，并提供局部化、可解释的相似度评估解释。作为感知损失函数在下游成像任务中展现出稳定的优化行为和竞争性性能。", "conclusion": "SUSS是一种概率性、可解释的感知度量方法，结合了深度方法的感知对齐优势和手工方法的可解释性，通过结构化不确定性建模实现了透明检查和采样能力。"}}
{"id": "2512.03687", "pdf": "https://arxiv.org/pdf/2512.03687", "abs": "https://arxiv.org/abs/2512.03687", "authors": ["Yian Li", "Xiaoyu Guo", "Hao Zhang", "Shuiwang Li", "Xiaowei Dai"], "title": "Active Visual Perception: Opportunities and Challenges", "categories": ["cs.CV"], "comment": null, "summary": "Active visual perception refers to the ability of a system to dynamically engage with its environment through sensing and action, allowing it to modify its behavior in response to specific goals or uncertainties. Unlike passive systems that rely solely on visual data, active visual perception systems can direct attention, move sensors, or interact with objects to acquire more informative data. This approach is particularly powerful in complex environments where static sensing methods may not provide sufficient information. Active visual perception plays a critical role in numerous applications, including robotics, autonomous vehicles, human-computer interaction, and surveillance systems. However, despite its significant promise, there are several challenges that need to be addressed, including real-time processing of complex visual data, decision-making in dynamic environments, and integrating multimodal sensory inputs. This paper explores both the opportunities and challenges inherent in active visual perception, providing a comprehensive overview of its potential, current research, and the obstacles that must be overcome for broader adoption.", "AI": {"tldr": "本文探讨了主动视觉感知系统的机遇与挑战，这是一种能够通过感知和行动与环境动态交互的系统，能够根据特定目标或不确定性调整行为。", "motivation": "被动视觉系统仅依赖视觉数据，在复杂环境中可能无法提供足够信息。主动视觉感知系统能够引导注意力、移动传感器或与物体交互以获取更有信息量的数据，这对于机器人、自动驾驶、人机交互和监控系统等应用至关重要。", "method": "本文采用综述性研究方法，对主动视觉感知的现有研究进行综合分析，探讨其在不同应用领域的实现方式和系统架构。", "result": "主动视觉感知在复杂环境中表现出显著优势，能够通过动态交互获取更丰富的信息。然而，研究也识别了多个需要解决的关键挑战。", "conclusion": "主动视觉感知具有巨大潜力，但要实现广泛应用，需要解决实时处理复杂视觉数据、动态环境决策制定和多模态感官输入集成等关键挑战。"}}
{"id": "2512.03685", "pdf": "https://arxiv.org/pdf/2512.03685", "abs": "https://arxiv.org/abs/2512.03685", "authors": ["Seng W. Loke"], "title": "Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)", "categories": ["quant-ph", "cs.DC", "cs.ET"], "comment": "8 pages, 10 figures; preliminary version (if mistakes found - please contact the author)", "summary": "Much recent work on distributed quantum computing have focused on the use of entangled pairs and distributed two qubit gates. But there has also been work on efficient schemes for achieving multipartite entanglement between nodes in a single shot, removing the need to generate multipartite entangled states using many entangled pairs. This paper looks at how multipartite entanglement resources (e.g., GHZ states) can be useful for distributed fan-out operations; we also consider the use of qudits of dimension four for distributed quantum circuit compression. In particular, we consider how such fan-out operations and qudits can be used to implement circuits which are challenging for distributed quantum computation, involving pairwise qubit interactions, i.e., what has been called global gates (a.k.a. global Mølmer-Sørensen gates). Such gates have been explored to possibly yield more efficient computations via reduced circuit depth, and can be carried out efficiently in some types of quantum hardware (e.g., trapped-ion quantum computers); we consider this as an exploration of an ``extreme'' case for distribution given the global qubit-qubit interactions. We also conclude with some implications for future work on quantum circuit compilation and quantum data centre design.", "AI": {"tldr": "该论文探讨了在分布式量子计算中利用多体纠缠资源（如GHZ态）和四维量子比特（qudits）来实现分布式扇出操作和全局门，以应对分布式量子电路中的挑战。", "motivation": "分布式量子计算通常依赖于纠缠对和分布式双量子比特门，但多体纠缠资源（如GHZ态）可以在单次操作中实现节点间的多体纠缠，避免使用多个纠缠对生成多体纠缠态。论文旨在探索这些资源如何帮助实现分布式扇出操作和全局门，这些在分布式量子计算中具有挑战性。", "method": "研究利用多体纠缠资源（特别是GHZ态）实现分布式扇出操作，同时考虑使用四维量子比特（qudits）进行分布式量子电路压缩。特别关注如何将这些技术应用于实现全局门（全局Mølmer-Sørensen门），这些门涉及成对的量子比特相互作用。", "result": "研究表明，多体纠缠资源和四维量子比特可以有效地实现分布式扇出操作和全局门，这些技术在分布式量子计算中具有挑战性。全局门可能通过减少电路深度实现更高效的计算，并且在某些量子硬件（如离子阱量子计算机）中可以高效执行。", "conclusion": "该研究为分布式量子计算中的极端情况（全局量子比特-量子比特相互作用）提供了初步探索，并对未来量子电路编译和量子数据中心设计提出了启示。多体纠缠资源和qudits在分布式量子计算中具有潜在优势，值得进一步研究。"}}
{"id": "2512.03684", "pdf": "https://arxiv.org/pdf/2512.03684", "abs": "https://arxiv.org/abs/2512.03684", "authors": ["Shahid Ansari", "Mahendra Kumar Gohil", "Yusuke Maeda", "Bishakh Bhattacharya"], "title": "A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents an autonomous tomato-harvesting system built around a hybrid robotic gripper that combines six soft auxetic fingers with a rigid exoskeleton and a latex basket to achieve gentle, cage-like grasping. The gripper is driven by a servo-actuated Scotch--yoke mechanism, and includes separator leaves that form a conical frustum for fruit isolation, with an integrated micro-servo cutter for pedicel cutting. For perception, an RGB--D camera and a Detectron2-based pipeline perform semantic segmentation of ripe/unripe tomatoes and keypoint localization of the pedicel and fruit center under occlusion and variable illumination. An analytical model derived using the principle of virtual work relates servo torque to grasp force, enabling design-level reasoning about actuation requirements. During execution, closed-loop grasp-force regulation is achieved using a proportional--integral--derivative controller with feedback from force-sensitive resistors mounted on selected fingers to prevent slip and bruising. Motion execution is supported by Particle Swarm Optimization (PSO)--based trajectory planning for a 5-DOF manipulator. Experiments demonstrate complete picking cycles (approach, separation, cutting, grasping, transport, release) with an average cycle time of 24.34~s and an overall success rate of approximately 80\\%, while maintaining low grasp forces (0.20--0.50~N). These results validate the proposed hybrid gripper and integrated vision--control pipeline for reliable harvesting in cluttered environments.", "AI": {"tldr": "提出一种基于混合夹持器的自主番茄采摘系统，结合软体辅助手指和刚性外骨骼，通过语义分割和关键点检测实现精准采摘", "motivation": "传统番茄采摘面临果实损伤、环境复杂（遮挡、光照变化）等问题，需要开发能够轻柔抓取、适应复杂环境的自动化采摘系统", "method": "1. 混合夹持器设计：6个软体辅助手指+刚性外骨骼+乳胶篮，伺服驱动Scotch-yoke机构，集成分离叶片和微型伺服切割器\n2. 感知系统：RGB-D相机+Detectron2管道，语义分割成熟/未成熟番茄，关键点定位果梗和果实中心\n3. 控制策略：基于虚功原理的分析模型，PID闭环抓取力控制，PSO轨迹规划5-DOF机械臂", "result": "平均采摘周期24.34秒，总体成功率约80%，抓取力保持在0.20-0.50N的低水平，验证了系统在杂乱环境中的可靠性", "conclusion": "提出的混合夹持器和集成视觉-控制管道能够实现番茄的可靠采摘，在复杂环境中保持低损伤和高成功率，为农业自动化提供了有效解决方案"}}
{"id": "2512.03674", "pdf": "https://arxiv.org/pdf/2512.03674", "abs": "https://arxiv.org/abs/2512.03674", "authors": ["Beatriz Esteves", "Ruben Verborgh"], "title": "Lifting the Cage of Consent: A Techno-Legal Perspective on Evolvable Trust Relationships", "categories": ["cs.CY", "cs.ET"], "comment": null, "summary": "Those concerned about privacy worry that personal data changes hands too easily. We argue that the actual challenge is the exact opposite: our data does not flow well enough, cultivating a reliance on questionable and often unlawful shortcuts in a desperate bid to survive within today's data-driven economy. Exclusively punitive interpretations of protective legislation such as the GDPR throw out the baby with the bathwater through barriers that equally hinder \"doing the right thing\" and \"doing the wrong thing\", in an abject mistranslation of how ethical choices correspond to financial cost. As long as privacy-friendly data treatment proves more expensive or complicated than readily available alternatives, economic imperatives will continue to outrank their legal counterparts. We examined existing legislation with the aim of facilitating mutually beneficial interactions, rather than more narrowly focusing on the prevention of undesired behaviors. In this article, we propose the implementation of evolvable trust systems as a scalable alternative to the omnipresent yet deeply broken delusion of ill-informed consent. We describe personalized, technology-assisted legal processes for initiating and maintaining long-term trust relationships, which enable parties to reliably and sustainably exchange data, goods, and services. Our proposal encourages a redirection of additional efforts towards the techno-legal alignment of economical incentives with societal ones, reminding us that - while trust remains an inherently human concept - technology can support people in evolving and scaling their relationships to meet the increasingly complex demands of current and future data landscapes.", "AI": {"tldr": "提出可演化信任系统作为替代当前破碎的知情同意机制的方案，通过技术辅助的法律流程建立和维护长期信任关系，促进数据、商品和服务的可靠交换", "motivation": "当前隐私保护面临的核心问题不是数据流动太容易，而是流动不够充分，导致依赖可疑甚至非法的捷径。保护性立法（如GDPR）的惩罚性解释阻碍了正当行为，而隐私友好的数据处理比替代方案更昂贵复杂，经济需求持续压倒法律要求", "method": "提出可演化信任系统的实现，描述个性化的技术辅助法律流程，用于启动和维护长期信任关系，使各方能够可靠、可持续地交换数据、商品和服务", "result": "提出了一种将经济激励与社会激励进行技术-法律对齐的替代方案，支持人们发展和扩展关系以满足当前和未来数据环境日益复杂的需求", "conclusion": "信任本质上仍是人类概念，但技术可以支持人们发展和扩展关系，应对日益复杂的数据环境。需要将额外努力重新导向经济激励与社会激励的技术-法律对齐"}}
{"id": "2512.03673", "pdf": "https://arxiv.org/pdf/2512.03673", "abs": "https://arxiv.org/abs/2512.03673", "authors": ["Feice Huang", "Zuliang Han", "Xing Zhou", "Yihuang Chen", "Lifei Zhu", "Haoqian Wang"], "title": "ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\\times$ speedup and 4.05$\\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.", "AI": {"tldr": "提出ConvRot方法，一种基于旋转的4位量化技术，用于扩散变换器，实现无需重新训练的即插即用W4A4推理", "motivation": "扩散变换器在生成高质量图像方面表现出色，但随着模型规模增大，内存占用和推理延迟显著增加，给实际部署带来挑战。现有旋转技术在扩散变换器中处理行级异常值困难且开销大", "method": "提出ConvRot方法，使用分组旋转量化，利用正则Hadamard变换抑制行级和列级异常值，将复杂度从二次降低到线性。设计ConvLinear4bit模块，集成旋转、量化、GEMM和反量化操作", "result": "在FLUX.1-dev上的实验显示，实现了2.26倍加速和4.05倍内存减少，同时保持图像保真度。这是首次将旋转量化应用于扩散变换器的即插即用W4A4推理", "conclusion": "ConvRot是一种有效的旋转量化方法，能够显著降低扩散变换器的内存占用和推理延迟，同时保持视觉质量，为实际部署提供了可行的解决方案"}}
{"id": "2512.03667", "pdf": "https://arxiv.org/pdf/2512.03667", "abs": "https://arxiv.org/abs/2512.03667", "authors": ["Ge-Peng Ji", "Jingyi Liu", "Deng-Ping Fan", "Nick Barnes"], "title": "Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning", "categories": ["cs.CV"], "comment": "Technical report", "summary": "In this study, we present Colon-X, an open initiative aimed at advancing multimodal intelligence in colonoscopy. We begin by constructing ColonVQA, the most comprehensive multimodal dataset ever built for colonoscopy, featuring over 1.1M+ visual question answering entries across 76 clinical findings and 18 multimodal tasks. Beyond serving as a community-wide data foundation, we further investigate a critical yet underexplored transition in colonoscopy - evolving from multimodal understanding to clinical reasoning: (a) To capture the current landscape of multimodal understanding behaviors, we systematically assess the generalizability of 22 multimodal large language models and examine their reliability under human-induced perturbations. The results reveal that clinical outputs from leading MLLMs remain far from robust and trustworthy. (b) To narrow this gap, we further explore reasoning-centric intelligence tailored for colonoscopy. Specifically, we curate ColonReason, a clinically grounded reasoning dataset annotated through a multi-expert debating pipeline, and develop ColonR1, the first R1-styled model incorporating task-adaptive rewarding and gradient-stable optimization techniques. Under data-scarce conditions, our ColonR1 achieves 56.61% overall accuracy, outperforming supervised fine-tuning by 25.22%, and sets a new reasoning-enabled baseline for multimodal colonoscopy analysis. All data and model resources are publicly available at https://github.com/ai4colonoscopy/Colon-X.", "AI": {"tldr": "提出Colon-X开放计划，旨在推进结肠镜多模态智能从多模态理解向临床推理的演进，包括构建大规模数据集ColonVQA和开发首个R1风格的结肠镜推理模型ColonR1", "motivation": "当前结肠镜多模态理解模型在临床输出方面缺乏鲁棒性和可信度，需要从单纯的多模态理解向临床推理能力演进，以支持更可靠的临床决策", "method": "1) 构建ColonVQA数据集（110万+视觉问答条目，覆盖76个临床发现和18个多模态任务）；2) 评估22个多模态大语言模型的泛化能力和鲁棒性；3) 创建临床推理数据集ColonReason（多专家辩论标注）；4) 开发ColonR1模型（采用任务自适应奖励和梯度稳定优化技术）", "result": "1) 现有MLLMs在临床输出方面远未达到鲁棒可信；2) ColonR1在数据稀缺条件下达到56.61%总体准确率，比监督微调提升25.22%，为多模态结肠镜分析建立了新的推理基准", "conclusion": "Colon-X成功推动了结肠镜多模态智能从理解向推理的演进，通过构建全面数据集和开发先进推理模型，为社区提供了数据基础和新的技术基准，所有资源已公开"}}
{"id": "2512.03666", "pdf": "https://arxiv.org/pdf/2512.03666", "abs": "https://arxiv.org/abs/2512.03666", "authors": ["Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Kailing Li", "Yang Jiao", "Jiacheng Zhang", "Xiaoling Wang", "Liang He"], "title": "ToG-Bench: Task-Oriented Spatio-Temporal Grounding in Egocentric Videos", "categories": ["cs.CV", "cs.AI"], "comment": "26 pages", "summary": "A core capability towards general embodied intelligence lies in localizing task-relevant objects from an egocentric perspective, formulated as Spatio-Temporal Video Grounding (STVG). Despite recent progress, existing STVG studies remain largely confined to object-centric and descriptive instructions, neglecting the task-oriented reasoning that is crucial for embodied agents to accomplish goal-directed interactions. To bridge this gap, we introduce \\textbf{ToG-Bench}, the first task-oriented spatio-temporal video grounding benchmark for egocentric videos. ToG-Bench is characterized by three key features: (1) \\textbf{Task-oriented Grounding}, which requires identifying and localizing objects based on intended tasks rather than straightforward descriptions; (2) \\textbf{Explicit-Implicit Dual Grounding}, where target objects can be either explicitly mentioned or implicitly inferred by contextual reasoning; (3) \\textbf{One-to-Many Grounding}, where a single instruction may correspond to multiple objects involved in task execution. Built upon videos sourced from ScanNet, ToG-Bench comprises 100 annotated clips with 2,704 task-oriented grounding instructions, constructed via a semi-automated pipeline that combines foundation model annotation and human refinement. In addition, we introduce a set of task-level evaluation metrics tailored for multi-object and explicit-implicit object grounding, and systematically benchmark seven state-of-the-art MLLMs. Extensive experiments reveal the intrinsic challenges of task-oriented STVG and substantial performance gaps across explicit-implicit and multi-object grounding, highlighting the difficulty of bridging perception and interaction in embodied scenarios. Data and code will be released at: \\href{https://github.com/qaxuDev/ToG-Bench}{https://github.com/qaxuDev/ToG-Bench}..", "AI": {"tldr": "提出ToG-Bench基准，这是首个面向任务的时空视频定位基准，专注于从第一人称视角定位任务相关对象，强调任务导向推理而非简单描述性指令。", "motivation": "现有时空视频定位研究主要局限于对象中心和描述性指令，缺乏任务导向推理能力，而这对于具身智能体完成目标导向交互至关重要。需要弥合感知与交互之间的差距。", "method": "基于ScanNet视频构建包含100个标注片段和2,704个任务导向定位指令的数据集，采用结合基础模型标注和人工细化的半自动化流程。引入针对多对象和显隐式对象定位的任务级评估指标，并系统评估七个最先进的多模态大语言模型。", "result": "实验揭示了任务导向时空视频定位的内在挑战，以及在显隐式和多对象定位方面存在的显著性能差距。现有模型在任务导向推理方面表现有限，特别是在需要上下文推理的隐式对象定位和多对象处理方面。", "conclusion": "ToG-Bench基准突出了任务导向时空视频定位的难度，强调了在具身场景中弥合感知与交互的挑战。该基准为未来研究提供了重要方向，特别是在任务导向推理和上下文理解方面。"}}
{"id": "2512.03661", "pdf": "https://arxiv.org/pdf/2512.03661", "abs": "https://arxiv.org/abs/2512.03661", "authors": ["Alex Ferrando", "Xavier Suau", "Jordi Gonzàlez", "Pau Rodriguez"], "title": "Dynamically Scaled Activation Steering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.", "AI": {"tldr": "提出了一种动态缩放激活引导（DSAS）框架，用于自适应地调节生成模型中激活引导的强度，只在需要时进行干预，从而在毒性缓解和模型效用之间实现更好的平衡。", "motivation": "现有的激活引导方法通常对所有输入统一应用干预，这在不必要引导时会降低模型性能。需要一种能够根据上下文自适应调节引导强度的方法，只在检测到不良行为时进行干预。", "method": "DSAS将\"何时引导\"与\"如何引导\"解耦，通过计算上下文相关的缩放因子来动态调整任何现有引导方法的强度。该方法可以端到端联合优化引导函数，并在生成时自适应地调节各层和各输入的引导强度。", "result": "与现有引导方法结合时，DSAS能持续改进帕累托前沿，在毒性缓解和效用保持之间实现更好的权衡。应用于文本到图像扩散模型时，能有效调节特定概念。该方法计算开销极小，同时提高了可解释性，能精确定位哪些标记需要引导以及引导程度。", "conclusion": "DSAS提供了一种方法无关的引导框架，通过动态缩放激活引导强度，实现了更精确、更高效的模型行为控制，在保持模型效用的同时有效缓解不良行为，具有广泛的应用潜力。"}}
{"id": "2512.03656", "pdf": "https://arxiv.org/pdf/2512.03656", "abs": "https://arxiv.org/abs/2512.03656", "authors": ["Salim Khazem", "Houssam Kanso"], "title": "Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.", "AI": {"tldr": "本文提出了一种统一的深度学习框架，将循环时间编码与混合LSTM-CNN架构相结合，用于多步能源预测，通过正弦余弦编码保留周期性结构，并利用集成模型捕捉长期季节效应和短期局部模式。", "motivation": "准确的电力消耗预测对于需求管理和智能电网运营至关重要。现有方法在处理多步预测时可能无法充分捕捉时间序列中的周期性结构和季节性效应，需要更有效的时间编码和模型集成方法。", "method": "1. 使用正弦余弦编码对基于日历的属性进行系统转换，以保留周期性结构；2. 通过相关性分析评估预测相关性；3. 采用由LSTM、CNN和针对每个预测时域的MLP回归器元学习器组成的集成模型；4. 利用一年的全国消费数据集进行实验研究。", "result": "在所有七个预测时域上均显示出持续改进，混合模型实现了比单个架构和先前方法更低的RMSE和MAE。消融分析证实了循环编码和日历特征的有效性。", "conclusion": "将循环时间表示与互补的深度学习结构相结合具有显著优势。据作者所知，这是首个在统一的短期能源预测框架中联合评估时间编码、基于日历的特征和混合集成架构的工作。"}}
{"id": "2512.03643", "pdf": "https://arxiv.org/pdf/2512.03643", "abs": "https://arxiv.org/abs/2512.03643", "authors": ["Ivan Yee Lee", "Cheng Yang", "Taylor Berg-Kirkpatrick"], "title": "Optical Context Compression Is Just (Bad) Autoencoding", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding", "AI": {"tldr": "该论文挑战了光学上下文压缩（optical context compression）的有效性，特别是针对DeepSeek-OCR声称的视觉标记能高效压缩文本表示的观点，通过实验证明简单的替代方法在相同压缩率下表现相当或更好。", "motivation": "DeepSeek-OCR展示了从少量视觉标记中高保真重建渲染文本的能力，引发了人们对基于视觉的上下文压缩用于语言模型的兴奋。然而，现有评估仅停留在重建层面，未测试这些表示是否真正有助于语言建模。论文旨在验证光学压缩叙事中的两个隐含假设。", "method": "作者将DeepSeek-OCR的视觉编码器与两种简单替代方案进行比较：1）参数无关的平均池化（mean pooling），2）学习的分层编码器（learned hierarchical encoder）。在匹配的压缩率下，评估这些方法在文本重建和语言建模任务上的表现。", "result": "研究发现，简单的替代方法（平均池化和分层编码器）在相同压缩率下，其文本重建效果与基于视觉的方法相当或更优。更重要的是，在语言建模任务中，基于视觉的压缩方法甚至无法超越简单的截断（truncation）方法，而简单替代方法则表现更好。", "conclusion": "光学上下文压缩的兴奋度超过了实际证据。基于视觉的压缩方法在文本重建方面没有独特优势，且在语言建模任务中表现不佳。论文标题直指核心观点：光学上下文压缩本质上只是（糟糕的）自动编码。"}}
{"id": "2512.03639", "pdf": "https://arxiv.org/pdf/2512.03639", "abs": "https://arxiv.org/abs/2512.03639", "authors": ["Kilian Schweppe", "Anne-Kathrin Schmuck"], "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction", "categories": ["cs.RO"], "comment": null, "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.", "AI": {"tldr": "提出了一种上下文触发的应急博弈框架，用于自主多智能体系统的可靠高效交互，结合了长期战略目标与短期动态适应能力。", "motivation": "解决自主多智能体系统中可靠高效交互的挑战，需要平衡长期战略目标与短期动态适应，确保在不确定交互环境中的安全性和进展性。", "method": "采用双层架构：上层从时序逻辑规范推导战略博弈，下层通过基于因子图的新求解器实现动态应急博弈的实时模型预测控制，利用策略模板保证高层目标满足。", "result": "在自动驾驶和机器人导航的仿真与硬件实验中验证了该框架，展示了高效、可靠、自适应的多智能体交互能力，确保了不确定交互环境中的安全性和进展性。", "conclusion": "上下文触发的应急博弈框架成功整合了战略博弈与动态应急博弈，通过双层架构和因子图求解器实现了可扩展的实时交互控制，为多智能体系统提供了既安全又高效的交互解决方案。"}}
{"id": "2512.03637", "pdf": "https://arxiv.org/pdf/2512.03637", "abs": "https://arxiv.org/abs/2512.03637", "authors": ["Kohei Yamamoto", "Kosuke Okusa"], "title": "AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning", "categories": ["cs.SD", "cs.LG", "stat.ML"], "comment": "11 pages, 4 figures", "summary": "Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while naïve low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.", "AI": {"tldr": "提出了一种名为AaPE（Aliasing-aware Patch Embedding）的补丁嵌入方法，用于自监督音频表示学习，通过自适应子带分析缓解混叠效应同时保留高频信息。", "motivation": "基于Transformer的自监督音频学习模型通常将频谱图视为图像，采用卷积补丁化并进行大量时间下采样。这会降低有效奈奎斯特频率并引入混叠效应，而简单的低通滤波会移除任务相关的高频线索。", "method": "AaPE使用带限复正弦核和双指数窗口生成特征，动态针对易混叠频段。核的频率和衰减参数从输入中估计，实现并行自适应子带分析，输出与标准补丁标记融合。结合多掩码策略和对比目标来增强不同掩码模式间的一致性。", "result": "在AudioSet上预训练后，在环境声音等下游任务评估中，部分任务达到最先进性能，其余任务获得竞争性结果。线性探测评估也显示在多个基准测试中取得明显提升。", "conclusion": "AaPE能够有效缓解混叠效应的影响，同时不丢弃信息丰富的高频内容，在自监督音频表示学习中表现出色。"}}
{"id": "2512.03630", "pdf": "https://arxiv.org/pdf/2512.03630", "abs": "https://arxiv.org/abs/2512.03630", "authors": ["Shifa Sulaiman", "Amarnath H", "Simon Bogh", "Naresh Marturi"], "title": "Multimodal Control of Manipulators: Coupling Kinematics and Vision for Self-Driving Laboratory Operations", "categories": ["cs.RO"], "comment": null, "summary": "Motion planning schemes are used for planning motions of a manipulator from an initial pose to a final pose during a task execution. A motion planning scheme generally comprises of a trajectory planning method and an inverse kinematic solver to determine trajectories and joints solutions respectively. In this paper, 3 motion planning schemes developed based on Jacobian methods are implemented to traverse a redundant manipulator with a coupled finger gripper through given trajectories. RRT* algorithm is used for planning trajectories and screw theory based forward kinematic equations are solved for determining joint solutions of the manipulator and gripper. Inverse solutions are computed separately using 3 Jacobian based methods such as Jacobian Transpose (JT), Pseudo Inverse (PI), and Damped Least Square (DLS) methods. Space Jacobian and manipulability measurements of the manipulator and gripper are obtained using screw theory formulations. Smoothness and RMSE error of generated trajectories and velocity continuity, acceleration profile, jerk, and snap values of joint motions are analysed for determining an efficient motion planning method for a given task. Advantages and disadvantages of the proposed motion planning schemes mentioned above are analysed using simulation studies to determine a suitable inverse solution technique for the tasks.", "AI": {"tldr": "本文提出了三种基于雅可比方法的运动规划方案，用于控制冗余机械臂和耦合手指夹爪通过给定轨迹，并分析比较了不同逆解技术的优缺点。", "motivation": "在自驱动实验室操作中，需要有效的运动规划方案来控制机械臂从初始位姿到最终位姿。现有的运动规划方案通常包含轨迹规划和逆运动学求解，但需要确定适合特定任务的高效方法。", "method": "使用RRT*算法进行轨迹规划，基于螺旋理论的正运动学方程求解机械臂和夹爪的关节解。采用三种雅可比方法计算逆解：雅可比转置法、伪逆法和阻尼最小二乘法。通过螺旋理论公式获得空间雅可比和可操作性测量。", "result": "分析了生成轨迹的平滑度和RMSE误差，以及关节运动的速度连续性、加速度曲线、急动度和冲击值。通过仿真研究比较了三种方法的优缺点，确定了适合特定任务的逆解技术。", "conclusion": "基于雅可比方法的三种运动规划方案各有优缺点，通过系统分析可以确定最适合特定自驱动实验室操作任务的逆运动学求解技术。"}}
{"id": "2512.03623", "pdf": "https://arxiv.org/pdf/2512.03623", "abs": "https://arxiv.org/abs/2512.03623", "authors": ["Edward C. C. Steele", "Dinesh Mane", "Emilio Monti", "Luis Orus", "Rebecca Chantrill-Cheyette", "Matthew Couch", "Kirstine I. Dale", "Simon Eaton", "Govindarajan Rangarajan", "Amir Majlesi", "Steven Ramsdale", "Michael Sharpe", "Craig Smith", "Jonathan Smith", "Rebecca Yates", "Holly Ellis", "Charles Ewen"], "title": "The promising potential of vision language models for the generation of textual weather forecasts", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "comment": "7 pages, 2 tables", "summary": "Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.", "AI": {"tldr": "探索使用视觉语言模型从视频编码的网格天气数据直接生成航运天气预报文本", "motivation": "尽管多模态基础模型具有潜力，但在气象产品和服务生成方面的应用仍处于起步阶段，需要加速其应用和采纳", "method": "使用视觉语言模型从视频编码的网格天气数据直接生成航运天气预报文本", "result": "早期结果显示，该方法为天气企业及其他领域提供了有前景的可扩展技术机会，能够提高生产效率和服务创新", "conclusion": "视觉语言模型在生成文本天气预报方面具有巨大潜力，能够加速气象产品服务的创新和生产效率提升"}}
{"id": "2512.03621", "pdf": "https://arxiv.org/pdf/2512.03621", "abs": "https://arxiv.org/abs/2512.03621", "authors": ["Yaokun Li", "Shuaixian Wang", "Mantang Guo", "Jiehui Huang", "Taojun Ding", "Mu Hu", "Kaixuan Wang", "Shaojie Shen", "Guang Tan"], "title": "ReCamDriving: LiDAR-Free Camera-Controlled Novel Trajectory Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://recamdriving.github.io/", "summary": "We propose ReCamDriving, a purely vision-based, camera-controlled novel-trajectory video generation framework. While repair-based methods fail to restore complex artifacts and LiDAR-based approaches rely on sparse and incomplete cues, ReCamDriving leverages dense and scene-complete 3DGS renderings for explicit geometric guidance, achieving precise camera-controllable generation. To mitigate overfitting to restoration behaviors when conditioned on 3DGS renderings, ReCamDriving adopts a two-stage training paradigm: the first stage uses camera poses for coarse control, while the second stage incorporates 3DGS renderings for fine-grained viewpoint and geometric guidance. Furthermore, we present a 3DGS-based cross-trajectory data curation strategy to eliminate the train-test gap in camera transformation patterns, enabling scalable multi-trajectory supervision from monocular videos. Based on this strategy, we construct the ParaDrive dataset, containing over 110K parallel-trajectory video pairs. Extensive experiments demonstrate that ReCamDriving achieves state-of-the-art camera controllability and structural consistency.", "AI": {"tldr": "提出ReCamDriving框架，这是一个纯视觉的、相机控制的新轨迹视频生成方法，利用密集且场景完整的3DGS渲染进行显式几何引导，实现精确的相机可控生成。", "motivation": "现有方法存在局限性：基于修复的方法无法恢复复杂伪影，基于LiDAR的方法依赖稀疏且不完整的线索。需要一种能够实现精确相机控制且保持结构一致性的视频生成方法。", "method": "采用两阶段训练范式：第一阶段使用相机姿态进行粗略控制，第二阶段结合3DGS渲染进行细粒度视角和几何引导。提出基于3DGS的跨轨迹数据整理策略，消除相机变换模式的训练-测试差距，从单目视频中实现可扩展的多轨迹监督。", "result": "构建了ParaDrive数据集，包含超过11万个并行轨迹视频对。实验表明ReCamDriving在相机可控性和结构一致性方面达到了最先进的性能。", "conclusion": "ReCamDriving通过利用3DGS渲染的密集几何引导和创新的两阶段训练策略，成功实现了纯视觉的、相机控制的新轨迹视频生成，解决了现有方法的局限性。"}}
{"id": "2512.03608", "pdf": "https://arxiv.org/pdf/2512.03608", "abs": "https://arxiv.org/abs/2512.03608", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "categories": ["cs.AR", "cs.AI", "cs.ET"], "comment": null, "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties. We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "AI": {"tldr": "KVNAND是一种基于闪存计算的无DRAM架构，将大语言模型的权重和KV缓存完全存储在3D NAND闪存中，解决长上下文推理中的内存瓶颈问题。", "motivation": "在边缘设备上部署大语言模型面临内存带宽和容量挑战。现有闪存计算方案仍需DRAM存储KV缓存，随着上下文长度增加，KV缓存可能超过模型权重大小，导致DRAM成本过高且容量不足。将KV缓存卸载到闪存会带来严重的性能损失。", "method": "提出KVNAND架构：1) 利用闪存计算处理所有内存受限操作以减少数据传输开销；2) 引入头组并行性提升吞吐量；3) 采用页面级KV缓存映射，使令牌访问模式与闪存组织对齐；4) 提出设计空间探索框架，评估不同KVNAND变体以平衡权重和KV放置。", "result": "在MHA 7B和GQA 70B大语言模型上的评估显示，KVNAND在128/1K/10K令牌上下文长度下，相比配备DRAM的闪存计算设计分别实现了1.98×/1.94×/2.05×的几何平均加速，并在100K上下文长度下解决了内存不足问题。", "conclusion": "KVNAND通过将KV缓存完全集成到闪存计算架构中，解决了长上下文推理中的DRAM依赖问题，使闪存成为KV缓存存储的实用介质，显著提升了边缘设备上大语言模型推理的性能和可扩展性。"}}
{"id": "2512.03607", "pdf": "https://arxiv.org/pdf/2512.03607", "abs": "https://arxiv.org/abs/2512.03607", "authors": ["Yusen Wu", "Xiaotie Deng"], "title": "DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization", "categories": ["cs.AI"], "comment": null, "summary": "This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints. Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.", "AI": {"tldr": "DeepRule是一个集成框架，通过深度预测建模和混合搜索优化实现自动化业务规则生成，特别针对零售品类和定价优化场景。", "motivation": "解决现有理论模型与现实经济复杂性之间的系统性不匹配问题，具体包括：(1)非结构化文本数据模态不匹配阻碍准确客户画像；(2)动态特征纠缠挑战，如非线性价格弹性和时变属性建模；(3)多层级业务约束导致的操作不可行性。", "method": "采用三层架构：1)混合知识融合引擎，使用大语言模型深度语义解析非结构化文本；2)博弈论约束优化机制，通过双边效用函数动态协调供应链利益；3)可解释决策蒸馏接口，利用LLM引导的符号回归优化定价策略和可审计业务规则。", "result": "在真实零售环境中验证，相比系统性B2C基线实现了更高的利润，同时确保了操作可行性。", "conclusion": "建立了一个闭环管道，统一了非结构化知识注入、多智能体优化和可解释策略合成，为真实经济智能提供了完整解决方案。"}}
{"id": "2512.03601", "pdf": "https://arxiv.org/pdf/2512.03601", "abs": "https://arxiv.org/abs/2512.03601", "authors": ["Haoran Zhou", "Gim Hee Lee"], "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025", "summary": "Recent advancements in foundation models for 2D vision have substantially improved the analysis of dynamic scenes from monocular videos. However, despite their strong generalization capabilities, these models often lack 3D consistency, a fundamental requirement for understanding scene geometry and motion, thereby causing severe spatial misalignment and temporal flickering in complex 3D environments. In this paper, we present Motion4D, a novel framework that addresses these challenges by integrating 2D priors from foundation models into a unified 4D Gaussian Splatting representation. Our method features a two-part iterative optimization framework: 1) Sequential optimization, which updates motion and semantic fields in consecutive stages to maintain local consistency, and 2) Global optimization, which jointly refines all attributes for long-term coherence. To enhance motion accuracy, we introduce a 3D confidence map that dynamically adjusts the motion priors, and an adaptive resampling process that inserts new Gaussians into under-represented regions based on per-pixel RGB and semantic errors. Furthermore, we enhance semantic coherence through an iterative refinement process that resolves semantic inconsistencies by alternately optimizing the semantic fields and updating prompts of SAM2. Extensive evaluations demonstrate that our Motion4D significantly outperforms both 2D foundation models and existing 3D-based approaches across diverse scene understanding tasks, including point-based tracking, video object segmentation, and novel view synthesis. Our code is available at https://hrzhou2.github.io/motion4d-web/.", "AI": {"tldr": "Motion4D是一个用于4D场景理解的框架，通过将2D基础模型的先验知识集成到统一的4D高斯泼溅表示中，解决现有方法缺乏3D一致性的问题。", "motivation": "现有的2D基础模型虽然泛化能力强，但在复杂3D环境中缺乏3D一致性，导致空间错位和时间闪烁问题，这限制了动态场景的几何和运动理解。", "method": "采用两阶段迭代优化框架：1）顺序优化，分阶段更新运动和语义场以保持局部一致性；2）全局优化，联合优化所有属性以实现长期一致性。引入3D置信度图动态调整运动先验，以及基于RGB和语义误差的自适应重采样过程。通过迭代细化过程增强语义一致性。", "result": "Motion4D在多种场景理解任务中显著优于2D基础模型和现有的3D方法，包括基于点的跟踪、视频对象分割和新视角合成。", "conclusion": "Motion4D成功地将2D基础模型的先验知识集成到4D高斯泼溅表示中，实现了3D一致的运动和语义理解，为4D场景分析提供了有效的解决方案。"}}
{"id": "2512.03593", "pdf": "https://arxiv.org/pdf/2512.03593", "abs": "https://arxiv.org/abs/2512.03593", "authors": ["David Svitov", "Pietro Morerio", "Lourdes Agapito", "Alessio Del Bue"], "title": "CloseUpAvatar: High-Fidelity Animatable Full-Body Avatars with Mixture of Multi-Scale Textures", "categories": ["cs.CV"], "comment": null, "summary": "We present a CloseUpAvatar - a novel approach for articulated human avatar representation dealing with more general camera motions, while preserving rendering quality for close-up views. CloseUpAvatar represents an avatar as a set of textured planes with two sets of learnable textures for low and high-frequency detail. The method automatically switches to high-frequency textures only for cameras positioned close to the avatar's surface and gradually reduces their impact as the camera moves farther away. Such parametrization of the avatar enables CloseUpAvatar to adjust rendering quality based on camera distance ensuring realistic rendering across a wider range of camera orientations than previous approaches. We provide experiments using the ActorsHQ dataset with high-resolution input images. CloseUpAvatar demonstrates both qualitative and quantitative improvements over existing methods in rendering from novel wide range camera positions, while maintaining high FPS by limiting the number of required primitives.", "AI": {"tldr": "提出CloseUpAvatar方法，通过混合多尺度纹理表示可动画全身虚拟人，能够在更广泛的相机运动范围内保持渲染质量，特别是近距离视图的细节表现。", "motivation": "现有方法在处理广泛的相机运动时，难以同时保持近距离视图的高质量渲染。当相机靠近虚拟人表面时，传统方法容易出现细节不足或渲染质量下降的问题。", "method": "使用带纹理的平面集合表示虚拟人，包含两组可学习的纹理：低频和高频细节纹理。系统根据相机距离自动切换纹理，近距离时使用高频纹理，远距离时逐渐降低其影响。", "result": "在ActorsHQ高分辨率数据集上的实验表明，CloseUpAvatar在广泛的相机位置范围内实现了定性和定量上的改进，同时通过限制所需基元数量保持高帧率。", "conclusion": "CloseUpAvatar能够根据相机距离调整渲染质量，确保在更广泛的相机方向范围内实现逼真渲染，超越了先前方法的性能表现。"}}
{"id": "2512.03592", "pdf": "https://arxiv.org/pdf/2512.03592", "abs": "https://arxiv.org/abs/2512.03592", "authors": ["Guang Yang", "Lei Fan"], "title": "Harnessing Hypergraphs in Geometric Deep Learning for 3D RNA Inverse Folding", "categories": ["cs.CV"], "comment": null, "summary": "The RNA inverse folding problem, a key challenge in RNA design, involves identifying nucleotide sequences that can fold into desired secondary structures, which are critical for ensuring molecular stability and function. The inherent complexity of this task stems from the intricate relationship between sequence and structure, making it particularly challenging. In this paper, we propose a framework, named HyperRNA, a generative model with an encoder-decoder architecture that leverages hypergraphs to design RNA sequences. Specifically, our HyperRNA model consists of three main components: preprocessing, encoding and decoding. In the preprocessing stage, graph structures are constructed by extracting the atom coordinates of RNA backbone based on 3-bead coarse-grained representation. The encoding stage processes these graphs, capturing higher order dependencies and complex biomolecular interactions using an attention embedding module and a hypergraph-based encoder. Finally, the decoding stage generates the RNA sequence in an autoregressive manner. We conducted quantitative and qualitative experiments on the PDBBind and RNAsolo datasets to evaluate the inverse folding task for RNA sequence generation and RNA-protein complex sequence generation. The experimental results demonstrate that HyperRNA not only outperforms existing RNA design methods but also highlights the potential of leveraging hypergraphs in RNA engineering.", "AI": {"tldr": "提出一个名为HyperRNA的生成模型，利用超图进行RNA序列设计，解决RNA逆折叠问题", "motivation": "RNA逆折叠问题是RNA设计中的关键挑战，涉及识别能够折叠成所需二级结构的核苷酸序列，这对于确保分子稳定性和功能至关重要。该任务的复杂性源于序列与结构之间的复杂关系。", "method": "提出HyperRNA框架，采用编码器-解码器架构，利用超图设计RNA序列。包含三个主要组件：预处理阶段基于3-bead粗粒度表示构建RNA骨架原子坐标图结构；编码阶段使用注意力嵌入模块和基于超图的编码器处理这些图，捕获高阶依赖性和复杂生物分子相互作用；解码阶段以自回归方式生成RNA序列。", "result": "在PDBBind和RNAsolo数据集上进行了定量和定性实验，评估RNA序列生成和RNA-蛋白质复合物序列生成的逆折叠任务。实验结果表明HyperRNA不仅优于现有的RNA设计方法，还突显了在RNA工程中利用超图的潜力。", "conclusion": "HyperRNA框架成功利用超图在几何深度学习中进行3D RNA逆折叠，为解决RNA设计中的逆折叠问题提供了有效方法，展示了超图在捕获复杂生物分子相互作用方面的优势。"}}
{"id": "2512.03582", "pdf": "https://arxiv.org/pdf/2512.03582", "abs": "https://arxiv.org/abs/2512.03582", "authors": ["Zeba Afroz", "Harsh Vardhan", "Pawan Bhakuni", "Aanchal Punia", "Rajdeep Kumar", "Md. Shad Akhtar"], "title": "Fine-grained Narrative Classification in Biased News Articles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.", "AI": {"tldr": "该论文提出了一种针对有偏见新闻文章的细粒度叙事分类方法，并构建了首个基于意识形态的印度新闻媒体细粒度叙事数据集INDI-PROP，开发了两种基于GPT-4o-mini的多跳提示推理框架进行偏见、叙事和说服技巧分类。", "motivation": "叙事是宣传的认知和情感支架，将孤立的劝说技巧组织成连贯的故事来证明行动合理性、归咎责任并唤起对意识形态阵营的认同。当前缺乏针对有偏见新闻的细粒度叙事分类方法，特别是在印度新闻媒体背景下。", "method": "1. 构建INDI-PROP数据集：包含1,266篇关于CAA和农民抗议两个极化社会政治事件的印度新闻文章，进行三级层次标注（意识形态偏见、事件特定细粒度叙事框架、说服技巧）。2. 提出FANTA和TPTC两种GPT-4o-mini引导的多跳提示推理框架：FANTA通过整合信息提取和上下文框架进行层次推理；TPTC采用两阶段方法系统分解说服线索。", "result": "评估表明，提出的方法在偏见分类、叙事分类和说服技巧识别方面相比基线模型均有显著改进。INDI-PROP成为首个基于意识形态的细粒度叙事数据集，为分析印度新闻媒体中的宣传提供了重要资源。", "conclusion": "该研究成功开发了针对有偏见新闻的细粒度叙事分类方法，通过构建高质量标注数据集和创新的多跳推理框架，为理解宣传中的叙事结构和意识形态偏见提供了有效工具，特别是在印度政治新闻分析领域。"}}
{"id": "2512.03580", "pdf": "https://arxiv.org/pdf/2512.03580", "abs": "https://arxiv.org/abs/2512.03580", "authors": ["Malte Bleeker", "Mauro Gotsch"], "title": "Dynamic Optical Test for Bot Identification (DOT-BI): A simple check to identify bots in surveys and online processes", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "We propose the Dynamic Optical Test for Bot Identification (DOT-BI): a quick and easy method that uses human perception of motion to differentiate between human respondents and automated systems in surveys and online processes. In DOT-BI, a 'hidden' number is displayed with the same random black-and-white pixel texture as its background. Only the difference in motion and scale between the number and the background makes the number perceptible to humans across frames, while frame-by-frame algorithmic processing yields no meaningful signal. We conducted two preliminary assessments. Firstly, state-of-the-art, video-capable, multimodal models (GPT-5-Thinking and Gemini 2.5 Pro) fail to extract the correct value, even when given explicit instructions about the mechanism. Secondly, in an online survey (n=182), 99.5% (181/182) of participants solved the task, with an average end-to-end completion time of 10.7 seconds; a supervised lab study (n=39) found no negative effects on perceived ease-of-use or completion time relative to a control. We release code to generate tests and 100+ pre-rendered variants to facilitate adoption in surveys and online processes.", "AI": {"tldr": "提出一种名为DOT-BI的动态光学测试方法，通过人类对运动的感知来区分在线调查和流程中的人类用户与自动化机器人系统。", "motivation": "在线调查和流程中机器人自动化系统的泛滥导致数据质量下降，需要一种简单有效的方法来区分人类用户和机器人，同时不影响用户体验。", "method": "使用动态显示技术，将\"隐藏\"的数字以与背景相同的随机黑白像素纹理显示，但通过数字与背景之间的运动和尺度差异，使人类能够在多帧中感知到数字，而逐帧算法处理则无法提取有意义的信号。", "result": "1. 最先进的视频多模态模型（GPT-5-Thinking和Gemini 2.5 Pro）即使在明确告知机制的情况下也无法提取正确值；2. 在线调查中99.5%的参与者成功完成任务，平均完成时间10.7秒；实验室研究发现相对于对照组没有负面用户体验影响。", "conclusion": "DOT-BI是一种快速、有效且用户友好的机器人识别方法，能够可靠地区分人类和自动化系统，同时保持良好用户体验，适合在在线调查和流程中推广应用。"}}
{"id": "2512.03577", "pdf": "https://arxiv.org/pdf/2512.03577", "abs": "https://arxiv.org/abs/2512.03577", "authors": ["Yizhi Zhang", "Lei Fan", "Zhulin Tao", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "title": "Cross-Stain Contrastive Learning for Paired Immunohistochemistry and Histopathology Slide Representation Learning", "categories": ["cs.CV"], "comment": "6 pages, 2 figures. Camera-ready version accepted for IEEE BIBM 2025", "summary": "Universal, transferable whole-slide image (WSI) representations are central to computational pathology. Incorporating multiple markers (e.g., immunohistochemistry, IHC) alongside H&E enriches H&E-based features with diverse, biologically meaningful information. However, progress is limited by the scarcity of well-aligned multi-stain datasets. Inter-stain misalignment shifts corresponding tissue across slides, hindering consistent patch-level features and degrading slide-level embeddings. To address this, we curated a slide-level aligned, five-stain dataset (H&E, HER2, KI67, ER, PGR) to enable paired H&E-IHC learning and robust cross-stain representation. Leveraging this dataset, we propose Cross-Stain Contrastive Learning (CSCL), a two-stage pretraining framework with a lightweight adapter trained using patch-wise contrastive alignment to improve the compatibility of H&E features with corresponding IHC-derived contextual cues, and slide-level representation learning with Multiple Instance Learning (MIL), which uses a cross-stain attention fusion module to integrate stain-specific patch features and a cross-stain global alignment module to enforce consistency among slide-level embeddings across different stains. Experiments on cancer subtype classification, IHC biomarker status classification, and survival prediction show consistent gains, yielding high-quality, transferable H&E slide-level representations. The code and data are available at https://github.com/lily-zyz/CSCL.", "AI": {"tldr": "该论文提出了一种跨染色对比学习框架，用于配对的免疫组化和组织病理学切片表示学习，旨在解决多染色数据对齐问题并提升全切片图像表示的质量。", "motivation": "计算病理学中需要通用的、可迁移的全切片图像表示。虽然将免疫组化等多标记物与H&E结合可以丰富特征信息，但进展受到对齐良好的多染色数据集稀缺的限制。染色间不对齐会导致相应组织在切片间移位，影响补丁级特征一致性并降低切片级嵌入质量。", "method": "提出了跨染色对比学习框架：1）收集了五染色数据集（H&E、HER2、KI67、ER、PGR）；2）两阶段预训练框架：第一阶段使用轻量适配器通过补丁级对比对齐提高H&E特征与IHC上下文线索的兼容性；第二阶段使用多实例学习进行切片级表示学习，包括跨染色注意力融合模块整合染色特异性补丁特征，以及跨染色全局对齐模块强制不同染色间切片级嵌入的一致性。", "result": "在癌症亚型分类、IHC生物标志物状态分类和生存预测等任务上的实验显示了一致的性能提升，生成了高质量、可迁移的H&E切片级表示。", "conclusion": "该方法通过解决多染色数据对齐问题，成功提升了全切片图像表示的质量和可迁移性，为计算病理学提供了有效的跨染色表示学习框架。"}}
{"id": "2512.03574", "pdf": "https://arxiv.org/pdf/2512.03574", "abs": "https://arxiv.org/abs/2512.03574", "authors": ["Fuxiang Yang", "Tonghua Su", "Donglin Di", "Yin Chen", "Xiangqian Wu", "Zhongjie Wang", "Lei Fan"], "title": "Global-Local Aware Scene Text Editing", "categories": ["cs.CV"], "comment": null, "summary": "Scene Text Editing (STE) involves replacing text in a scene image with new target text while preserving both the original text style and background texture. Existing methods suffer from two major challenges: inconsistency and length-insensitivity. They often fail to maintain coherence between the edited local patch and the surrounding area, and they struggle to handle significant differences in text length before and after editing. To tackle these challenges, we propose an end-to-end framework called Global-Local Aware Scene Text Editing (GLASTE), which simultaneously incorporates high-level global contextual information along with delicate local features. Specifically, we design a global-local combination structure, joint global and local losses, and enhance text image features to ensure consistency in text style within local patches while maintaining harmony between local and global areas. Additionally, we express the text style as a vector independent of the image size, which can be transferred to target text images of various sizes. We use an affine fusion to fill target text images into the editing patch while maintaining their aspect ratio unchanged. Extensive experiments on real-world datasets validate that our GLASTE model outperforms previous methods in both quantitative metrics and qualitative results and effectively mitigates the two challenges.", "AI": {"tldr": "提出GLASTE框架，通过全局-局部感知机制解决场景文本编辑中的不一致性和长度不敏感问题", "motivation": "现有场景文本编辑方法存在两个主要挑战：不一致性（编辑的局部区域与周围区域不协调）和长度不敏感性（无法处理编辑前后文本长度显著差异的问题）", "method": "提出端到端的GLASTE框架，设计全局-局部组合结构、联合全局和局部损失函数，将文本风格表示为与图像大小无关的向量，使用仿射融合保持目标文本图像的宽高比不变", "result": "在真实世界数据集上的大量实验验证了GLASTE模型在定量指标和定性结果上都优于先前方法，有效缓解了两个主要挑战", "conclusion": "GLASTE通过同时结合高级全局上下文信息和精细局部特征，能够保持局部补丁内文本风格一致性和局部与全局区域的协调性，实现了更好的场景文本编辑效果"}}
{"id": "2512.03571", "pdf": "https://arxiv.org/pdf/2512.03571", "abs": "https://arxiv.org/abs/2512.03571", "authors": ["Zhening Li", "Armando Solar-Lezama", "Yisong Yue", "Stephan Zheng"], "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths", "categories": ["cs.AI", "cs.LG", "cs.PL"], "comment": "65 pages, 2 figures, published in NeurIPS 2025", "summary": "We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce \"probabilistic angelic nondeterminism\" (\"PAN\"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.", "AI": {"tldr": "EnCompass是一个新的智能体编程框架，通过\"概率天使非确定性\"编程模型将核心工作流逻辑与推理时策略解耦，让开发者能够独立设计和实验不同的推理策略。", "motivation": "当前智能体编程方法通常将核心工作流逻辑和推理时策略（如树搜索）耦合在一起，这使得开发者在改进智能体可靠性和实验不同推理策略时面临困难，需要大量代码修改。", "method": "提出\"概率天使非确定性\"编程模型，使用Python装饰器将智能体工作流程序编译成搜索空间，允许程序员通过简单修改输入来独立实验不同的推理时策略。", "result": "实现了EnCompass框架，并通过三个案例研究展示了该框架能够让程序员快速提高智能体可靠性，轻松在不同推理策略之间切换，且只需少量额外编码。", "conclusion": "EnCompass框架通过解耦智能体设计的两个关键方面，为LLM智能体开发提供了更灵活、可实验的编程模型，显著降低了改进智能体性能和实验不同策略的编码负担。"}}
{"id": "2512.03568", "pdf": "https://arxiv.org/pdf/2512.03568", "abs": "https://arxiv.org/abs/2512.03568", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "title": "Synthetic Cognitive Walkthrough: Aligning Large Language Model Performance with Human Cognitive Walkthrough", "categories": ["cs.HC"], "comment": null, "summary": "Conducting usability testing like cognitive walkthrough (CW) can be costly. Recent developments in large language models (LLMs), with visual reasoning and UI navigation capabilities, present opportunities to automate CW. We explored whether LLMs (GPT-4 and Gemini-2.5-pro) can simulate human behavior in CW by comparing their walkthroughs with human participants. While LLMs could navigate interfaces and provide reasonable rationales, their behavior differed from humans. LLM-prompted CW achieved higher task completion rates than humans and followed more optimal navigation paths, while identifying fewer potential failure points. However, follow-up studies demonstrated that with additional prompting, LLMs can predict human-identified failure points, aligning their performance with human participants. Our work highlights that while LLMs may not replicate human behaviors exactly, they can be leveraged for scaling usability walkthroughs and providing UI insights, offering a valuable complement to traditional usability testing.", "AI": {"tldr": "探索大型语言模型是否能够模拟人类在认知走查中的行为，通过比较LLM与人类参与者的走查结果，研究LLM在可用性测试自动化中的潜力。", "motivation": "传统的认知走查等可用性测试成本高昂，而具有视觉推理和UI导航能力的大型语言模型为自动化认知走查提供了机会，需要验证LLM能否有效模拟人类行为。", "method": "比较GPT-4和Gemini-2.5-pro等LLM与人类参与者在认知走查中的表现，包括任务完成率、导航路径和潜在失败点识别，并通过额外提示策略调整LLM表现。", "result": "LLM引导的认知走查比人类获得更高的任务完成率，遵循更优的导航路径，但识别出的潜在失败点较少；通过额外提示，LLM能够预测人类识别的失败点，使其表现与人类参与者对齐。", "conclusion": "虽然LLM不能完全复制人类行为，但可以用于扩展可用性走查并提供UI洞察，作为传统可用性测试的有价值补充，特别是在规模化应用方面具有潜力。"}}
{"id": "2512.03566", "pdf": "https://arxiv.org/pdf/2512.03566", "abs": "https://arxiv.org/abs/2512.03566", "authors": ["Hao Sun", "Lei Fan", "Donglin Di", "Shaohui Liu"], "title": "GAOT: Generating Articulated Objects Through Text-Guided Diffusion Models", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ACM MM Asia2026", "summary": "Articulated object generation has seen increasing advancements, yet existing models often lack the ability to be conditioned on text prompts. To address the significant gap between textual descriptions and 3D articulated object representations, we propose GAOT, a three-phase framework that generates articulated objects from text prompts, leveraging diffusion models and hypergraph learning in a three-step process. First, we fine-tune a point cloud generation model to produce a coarse representation of objects from text prompts. Given the inherent connection between articulated objects and graph structures, we design a hypergraph-based learning method to refine these coarse representations, representing object parts as graph vertices. Finally, leveraging a diffusion model, the joints of articulated objects-represented as graph edges-are generated based on the object parts. Extensive qualitative and quantitative experiments on the PartNet-Mobility dataset demonstrate the effectiveness of our approach, achieving superior performance over previous methods.", "AI": {"tldr": "提出GAOT框架，通过文本引导的扩散模型生成可动关节物体", "motivation": "现有可动关节物体生成模型缺乏文本条件控制能力，文本描述与3D可动关节物体表示之间存在显著差距", "method": "三阶段框架：1) 微调点云生成模型从文本生成粗略物体表示；2) 基于超图学习方法细化表示，将物体部件表示为图顶点；3) 使用扩散模型生成关节（图边）", "result": "在PartNet-Mobility数据集上的定性和定量实验显示优于先前方法的性能", "conclusion": "GAOT框架有效解决了文本到可动关节物体生成的挑战，通过结合扩散模型和超图学习实现了高质量的生成结果"}}
{"id": "2512.03563", "pdf": "https://arxiv.org/pdf/2512.03563", "abs": "https://arxiv.org/abs/2512.03563", "authors": ["Chengyu Tang", "Sanjeev Baskiyar"], "title": "State Space Models for Bioacoustics: A comparative Evaluation with Transformers", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "In this study, we evaluate the efficacy of the Mamba model in the field of bioacoustics. We first pretrain a Mamba-based audio large language model (LLM) on a large corpus of audio data using self-supervised learning. We fine-tune and evaluate BioMamba on the BEANS benchmark, a collection of diverse bioacoustic tasks including classification and detection, and compare its performance and efficiency with multiple baseline models, including AVES, a state-of-the-art Transformer-based model. The results show that BioMamba achieves comparable performance with AVES while consumption significantly less VRAM, demonstrating its potential in this domain.", "AI": {"tldr": "评估Mamba模型在生物声学领域的效能，通过预训练基于Mamba的音频大语言模型，并在BEANS基准上进行微调和评估，与Transformer模型进行性能比较。", "motivation": "探索Mamba模型在生物声学领域的应用潜力，解决Transformer模型在音频处理中计算资源消耗大的问题，寻求更高效的生物声学分析模型。", "method": "首先使用自监督学习在大规模音频数据上预训练基于Mamba的音频大语言模型，然后在BEANS基准（包含分类和检测等多样化生物声学任务）上进行微调和评估，与AVES等Transformer基线模型进行对比。", "result": "BioMamba在性能上与最先进的Transformer模型AVES相当，同时显著减少了VRAM消耗，展示了在该领域的应用潜力。", "conclusion": "Mamba模型在生物声学领域具有显著优势，能够在保持性能的同时大幅降低计算资源需求，为生物声学分析提供了更高效的解决方案。"}}
{"id": "2512.03558", "pdf": "https://arxiv.org/pdf/2512.03558", "abs": "https://arxiv.org/abs/2512.03558", "authors": ["Huy Quang Ung", "Guillaume Habault", "Yasutaka Nishimura", "Hao Niu", "Roberto Legaspi", "Tomoki Oya", "Ryoichi Kojima", "Masato Taya", "Chihiro Ono", "Atsunori Minamikawa", "Yan Liu"], "title": "CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at SIGSPATIAL 2025 (Best paper candidates), 15 pages", "summary": "The rise of Visual-Language Models (LVLMs) has unlocked new possibilities for seamlessly integrating visual and textual information. However, their ability to interpret cartographic maps remains largely unexplored. In this paper, we introduce CartoMapQA, a benchmark specifically designed to evaluate LVLMs' understanding of cartographic maps through question-answering tasks. The dataset includes over 2000 samples, each composed of a cartographic map, a question (with open-ended or multiple-choice answers), and a ground-truth answer. These tasks span key low-, mid- and high-level map interpretation skills, including symbol recognition, embedded information extraction, scale interpretation, and route-based reasoning. Our evaluation of both open-source and proprietary LVLMs reveals persistent challenges: models frequently struggle with map-specific semantics, exhibit limited geospatial reasoning, and are prone to Optical Character Recognition (OCR)-related errors. By isolating these weaknesses, CartoMapQA offers a valuable tool for guiding future improvements in LVLM architectures. Ultimately, it supports the development of models better equipped for real-world applications that depend on robust and reliable map understanding, such as navigation, geographic search, and urban planning. Our source code and data are openly available to the research community at: https://github.com/ungquanghuy-kddi/CartoMapQA.git", "AI": {"tldr": "提出CartoMapQA基准数据集，专门用于评估视觉语言模型在制图地图理解方面的能力", "motivation": "虽然视觉语言模型在视觉-文本融合方面取得进展，但其在制图地图理解方面的能力尚未得到充分探索，需要专门的评估工具来推动该领域发展", "method": "创建包含2000多个样本的基准数据集，每个样本包含制图地图、问题（开放式或多选题）和真实答案，涵盖从低到高三个层次的制图技能评估", "result": "评估显示现有模型在制图语义理解、地理空间推理和OCR相关错误方面存在显著挑战，揭示了当前模型的局限性", "conclusion": "CartoMapQA为评估和改进视觉语言模型的地图理解能力提供了重要工具，有助于开发适用于导航、地理搜索和城市规划等实际应用的更强大模型"}}
{"id": "2512.03553", "pdf": "https://arxiv.org/pdf/2512.03553", "abs": "https://arxiv.org/abs/2512.03553", "authors": ["Wei Chee Yew", "Hailun Xu", "Sanjay Saha", "Xiaotian Fan", "Hiok Hian Ong", "David Yuchen Wang", "Kanchan Sarkar", "Zhenheng Yang", "Danhui Guan"], "title": "Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at KDD 2026", "summary": "Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.", "AI": {"tldr": "提出一个结合监督分类与MLLM增强相似性匹配的混合内容审核框架，用于直播流中的动态内容审核", "motivation": "直播平台的内容审核面临时效性、多模态性和对抗性内容演化的挑战，需要既能检测已知违规又能识别新颖边缘案例的解决方案", "method": "采用混合框架：监督分类管道处理已知违规，基于参考的相似性匹配管道处理新颖或微妙案例；多模态输入通过两个管道处理，使用MLLM将知识蒸馏到每个管道中以提升准确性同时保持推理轻量", "result": "分类管道在80%精确率下达到67%召回率，相似性管道在80%精确率下达到76%召回率；大规模A/B测试显示用户观看不良直播的观看量减少6-8%", "conclusion": "该混合框架提供了一种可扩展且适应性的多模态内容治理方法，能够同时处理明确违规和新兴对抗行为"}}
{"id": "2512.03548", "pdf": "https://arxiv.org/pdf/2512.03548", "abs": "https://arxiv.org/abs/2512.03548", "authors": ["Zexin Lin", "Yebin Zhong", "Hanwen Wan", "Jiu Cheng", "Zhenglong Sun", "Xiaoqiang Ji"], "title": "A Learning-based Control Methodology for Transitioning VTOL UAVs", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Transition control poses a critical challenge in Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL UAV) development due to the tilting rotor mechanism, which shifts the center of gravity and thrust direction during transitions. Current control methods' decoupled control of altitude and position leads to significant vibration, and limits interaction consideration and adaptability. In this study, we propose a novel coupled transition control methodology based on reinforcement learning (RL) driven controller. Besides, contrasting to the conventional phase-transition approach, the ST3M method demonstrates a new perspective by treating cruise mode as a special case of hover. We validate the feasibility of applying our method in simulation and real-world environments, demonstrating efficient controller development and migration while accurately controlling UAV position and attitude, exhibiting outstanding trajectory tracking and reduced vibrations during the transition process.", "AI": {"tldr": "提出一种基于强化学习的耦合过渡控制方法，用于垂直起降无人机在悬停和巡航模式之间的平稳过渡控制", "motivation": "传统VTOL无人机过渡控制方法存在解耦控制导致的显著振动问题，且缺乏交互考虑和适应性，需要更有效的控制方法", "method": "采用基于强化学习的控制器，提出ST3M方法，将巡航模式视为悬停的特殊情况，实现耦合过渡控制", "result": "在仿真和真实环境中验证了方法的可行性，实现了高效的控制器开发和迁移，精确控制无人机位置和姿态，表现出优秀的轨迹跟踪能力和减少的过渡振动", "conclusion": "提出的基于强化学习的耦合过渡控制方法有效解决了VTOL无人机过渡控制中的振动问题，为无人机控制提供了新的视角和方法"}}
{"id": "2512.03542", "pdf": "https://arxiv.org/pdf/2512.03542", "abs": "https://arxiv.org/abs/2512.03542", "authors": ["Nan Sun", "Zhenyu Zhang", "Xixun Lin", "Kun Wang", "Yanmin Shang", "Naibin Gu", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Haifeng Wang", "Yanan Cao"], "title": "V-ITI: Mitigating Hallucinations in Multimodal Large Language Models via Visual Inference-Time Intervention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in numerous vision-language tasks yet suffer from hallucinations, producing content inconsistent with input visuals, that undermine reliability in precision-sensitive domains. This issue stems from a fundamental problem of visual neglect, where models fail to adequately prioritize input images. Existing methods typically alleviate hallucinations by intervening in the attention score or output logits, focusing on \"how to intervene\" but overlooking the prerequisite \"when to intervene\", which leads to the \"over-intervention\" problem and subsequently introduces new hallucinations and unnecessary computational overhead. To address this gap, we first investigate the mechanism of visual neglect and reveal it can be accurately detected via head-level activation patterns in MLLMs. We thus propose V-ITI, a lightweight visual inference-time intervention framework integrating a Visual Neglect Detector that identifies visual neglect via head-level discriminative probes and a Visual Recall Intervenor that modulates activations with prestored visual activation information only when the visual neglect is detected. Extensive experiments across eight benchmarks and different MLLM families demonstrate that V-ITI consistently mitigates vision-related hallucinations while preserving general task performance.", "AI": {"tldr": "提出V-ITI框架，通过视觉推理时干预来缓解多模态大语言模型中的幻觉问题，重点解决\"何时干预\"的问题而非仅仅\"如何干预\"", "motivation": "多模态大语言模型在视觉语言任务中表现出色，但存在幻觉问题，产生与输入视觉内容不一致的输出，这在精度敏感领域影响可靠性。现有方法主要关注\"如何干预\"，但忽略了\"何时干预\"这一前提，导致\"过度干预\"问题，引入新的幻觉和不必要的计算开销", "method": "提出V-ITI框架，包含两个核心组件：1) 视觉忽视检测器，通过头级判别探针识别视觉忽视；2) 视觉回忆干预器，仅在检测到视觉忽视时，使用预存储的视觉激活信息来调制激活。该方法基于对视觉忽视机制的深入研究，发现可以通过MLLMs中的头级激活模式准确检测视觉忽视", "result": "在八个基准测试和不同MLLM家族上的广泛实验表明，V-ITI能够持续缓解与视觉相关的幻觉，同时保持一般任务性能", "conclusion": "V-ITI通过解决\"何时干预\"的关键问题，有效缓解了多模态大语言模型中的幻觉问题，避免了过度干预带来的新幻觉和计算开销，为提升MLLMs在精度敏感领域的可靠性提供了轻量级解决方案"}}
{"id": "2512.03540", "pdf": "https://arxiv.org/pdf/2512.03540", "abs": "https://arxiv.org/abs/2512.03540", "authors": ["Ruoxuan Zhang", "Bin Wen", "Hongxia Xie", "Yi Yao", "Songhan Zuo", "Jian-Yu Jiang-Lin", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACM Multimedia 2025", "summary": "Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.", "AI": {"tldr": "提出CookAnything框架，用于从任意长度的文本烹饪指令生成连贯、语义分明的多步骤图像序列，解决现有方法无法适应食谱长度变化的问题。", "motivation": "烹饪是顺序性和视觉基础的活动，每个步骤（如切菜、混合、煎炸）都包含程序逻辑和视觉语义。现有扩散模型难以处理结构化多步骤场景如食谱插图，且当前方法无法适应食谱长度的自然变化，无论实际指令结构如何都生成固定数量的图像。", "method": "提出三个关键组件：1) 步骤区域控制(SRC)，在单个去噪过程中将文本步骤与对应图像区域对齐；2) 灵活RoPE，增强时间连贯性和空间多样性的步骤感知位置编码机制；3) 跨步骤一致性控制(CSCC)，保持步骤间细粒度食材一致性。", "result": "在食谱插图基准测试中，CookAnything在基于训练和无训练设置下均优于现有方法。该框架支持复杂多步骤指令的可扩展高质量视觉合成。", "conclusion": "提出的框架在指令媒体和程序内容创作方面具有广泛应用的潜力，能够灵活生成任意长度烹饪指令的连贯图像序列。"}}
{"id": "2512.03538", "pdf": "https://arxiv.org/pdf/2512.03538", "abs": "https://arxiv.org/abs/2512.03538", "authors": ["Yuhang Huang", "Shilong Zou", "Jiazhao Zhang", "Xinwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "AI": {"tldr": "AdaPower是一个轻量级适配框架，将通用世界基础模型转化为专门的世界模型，用于机器人预测操控任务", "motivation": "世界基础模型虽然具有强大的视觉动态模拟能力，但在精确机器人控制方面存在局限性，现有方法计算成本高且未充分利用预训练的VLA策略", "method": "提出两个新组件：时间-空间测试时训练（TS-TTT）进行推理时适配，以及记忆持久性（MP）确保长时域一致性；集成在模型预测控制框架中", "result": "在LIBERO基准测试中任务成功率提升超过41%，无需策略重新训练，同时保持计算效率和通用能力", "conclusion": "AdaPower成功将通用世界基础模型转化为专门的世界模型，显著提升了预训练VLA策略的机器人操控性能"}}
{"id": "2512.03534", "pdf": "https://arxiv.org/pdf/2512.03534", "abs": "https://arxiv.org/abs/2512.03534", "authors": ["Subin Kim", "Sangwoo Mo", "Mamshad Nayeem Rizve", "Yiran Xu", "Difan Liu", "Jinwoo Shin", "Tobias Hinz"], "title": "Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS", "summary": "Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.", "AI": {"tldr": "提出PRIS框架，通过推理时自适应重新设计提示词来改进文本到视觉生成的质量，实现提示词和视觉生成过程的联合扩展", "motivation": "现有方法主要通过扩展视觉生成过程（如增加采样步骤或种子）来改进文本到视觉生成，但提示词保持不变，导致质量很快达到瓶颈。需要同时扩展提示词和视觉生成过程才能充分利用推理时的扩展定律", "method": "提出PRIS框架：1）生成初始视觉内容；2）通过元素级事实校正验证器分析生成结果，识别跨视觉内容的重复失败模式；3）根据分析结果重新设计提示词；4）用修订后的提示词重新生成视觉内容。验证器在细粒度级别评估提示词属性与生成视觉之间的对齐", "result": "在文本到图像和文本到视频基准测试中表现优异，在VBench 2.0上获得15%的性能提升。实验证明联合扩展提示词和视觉生成过程能显著改善生成质量", "conclusion": "推理时联合扩展提示词和视觉生成过程是充分利用扩展定律的关键。PRIS框架通过自适应提示词重新设计有效解决了文本到视觉生成中的对齐问题"}}
{"id": "2512.03528", "pdf": "https://arxiv.org/pdf/2512.03528", "abs": "https://arxiv.org/abs/2512.03528", "authors": ["Guang Yang", "Tianpei Yang", "Jingwen Qiao", "Yanqing Wu", "Jing Huo", "Xingguo Chen", "Yang Gao"], "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.", "AI": {"tldr": "提出一种通信受限的多智能体强化学习框架，通过通信约束先验区分有损和无损消息，并量化通信消息对全局奖励的影响", "motivation": "现实世界中普遍存在有损通信问题，现有带通信的多智能体强化学习方法由于可扩展性和鲁棒性有限，难以应用于复杂动态的真实环境", "method": "提出通用通信约束模型统一描述不同场景的通信条件，作为学习先验区分有损和无损消息；使用双重互信息估计器解耦有损和无损消息对分布式决策的影响；引入通信约束多智能体强化学习框架，将通信消息影响量化到全局奖励中", "result": "在多个通信受限基准测试中验证了方法的有效性", "conclusion": "提出的通信约束多智能体强化学习框架能够有效处理现实世界中的有损通信问题，提高在复杂动态环境中的适用性"}}
{"id": "2512.03524", "pdf": "https://arxiv.org/pdf/2512.03524", "abs": "https://arxiv.org/abs/2512.03524", "authors": ["Grzegorz Jamróz", "Rafał Kucharski", "David Watling"], "title": "Market share maximizing strategies of CAV fleet operators may cause chaos in our cities", "categories": ["math.OC", "cs.ET", "cs.MA", "econ.TH", "eess.SY"], "comment": "34 pages, 9 figures", "summary": "We study the dynamics and equilibria of a new kind of routing games, where players - drivers of future autonomous vehicles - may switch between individual (HDV) and collective (CAV) routing. In individual routing, just like today, drivers select routes minimizing expected travel costs, whereas in collective routing an operator centrally assigns vehicles to routes. The utility is then the average experienced travel time discounted with individually perceived attractiveness of automated driving. The market share maximising strategy amounts to offering utility greater than for individual routing to as many drivers as possible. Our theoretical contribution consists in developing a rigorous mathematical framework of individualized collective routing and studying algorithms which fleets of CAVs may use for their market-share optimization. We also define bi-level CAV - HDV equilibria and derive conditions which link the potential marketing behaviour of CAVs to the behavioural profile of the human population. Practically, we find that the fleet operator may often be able to equilibrate at full market share by simply mimicking the choices HDVs would make. In more realistic heterogenous human population settings, however, we discover that the market-share maximizing fleet controller should use highly variable mixed strategies as a means to attract or retain customers. The reason is that in mixed routing the powerful group player can control which vehicles are routed via congested and uncongested alternatives. The congestion pattern generated by CAVs is, however, not known to HDVs before departure and so HDVs cannot select faster routes and face huge uncertainty whichever alternative they choose. Consequently, mixed market-share maximising fleet strategies resulting in unpredictable day-to-day driving conditions may, alarmingly, become pervasive in our future cities.", "AI": {"tldr": "研究自动驾驶汽车车队运营商通过市场占有率最大化策略可能对城市交通系统造成的混乱影响，分析个体与集体路由选择博弈中的动态平衡", "motivation": "随着自动驾驶汽车的发展，车队运营商可能采用市场占有率最大化策略，这可能导致城市交通系统出现不可预测的混乱状况，需要研究这种新型路由博弈的动态平衡", "method": "建立了个性化集体路由的严格数学框架，研究CAV车队用于市场占有率优化的算法，定义双层CAV-HDV均衡，并推导车队营销行为与人类行为特征之间的关系条件", "result": "车队运营商通常能够通过模仿HDV的选择实现完全市场占有率均衡；但在异质人口环境中，市场占有率最大化的车队控制器应使用高度可变的混合策略来吸引或保留客户，这会导致不可预测的日常驾驶条件", "conclusion": "市场占有率最大化的车队策略可能导致未来城市中出现普遍不可预测的交通状况，因为CAV生成的拥堵模式对HDV是未知的，使得HDV无法选择更快路线，面临巨大不确定性"}}
{"id": "2512.03522", "pdf": "https://arxiv.org/pdf/2512.03522", "abs": "https://arxiv.org/abs/2512.03522", "authors": ["Gihyeon Lee", "Jungwoo Lee", "Juwon Kim", "Young-Sik Shin", "Younggun Cho"], "title": "MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted in IEEE Robotics and Automation Letters (2025)", "summary": "Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.", "AI": {"tldr": "提出MSG-Loc：一种基于多标签似然度的语义图匹配框架，用于对象级全局定位，解决语义模糊环境中的定位问题", "motivation": "机器人在未知物体类别和语义模糊的环境中定位时，高语义模糊度会加剧物体误分类，增加错误关联的可能性，从而导致姿态估计出现显著错误", "method": "使用多标签图表示而非单标签替代方案，通过上下文感知的似然传播，将每个节点的似然度与其邻居的最大似然度相结合，增强图间的语义对应关系", "result": "在闭集和开集检测配置下评估了数据关联和姿态估计性能，并在真实室内场景和合成环境中展示了方法对大词汇量物体类别的可扩展性", "conclusion": "提出的多标签似然度语义图匹配框架能够有效处理语义模糊环境中的对象级全局定位问题，具有较好的可扩展性和鲁棒性"}}
{"id": "2512.03520", "pdf": "https://arxiv.org/pdf/2512.03520", "abs": "https://arxiv.org/abs/2512.03520", "authors": ["Yiyi Cai", "Yuhan Wu", "Kunhang Li", "You Zhou", "Bo Zheng", "Haiyang Liu"], "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/", "AI": {"tldr": "提出FloodDiffusion框架，用于文本驱动的流式人体运动生成，通过改进的扩散强制方法实现实时延迟下的无缝运动序列生成", "motivation": "现有方法依赖分块处理或带扩散头的自回归模型，无法有效处理时间变化文本提示下的流式运动生成任务，需要更有效的框架来建模时间序列生成", "method": "采用扩散强制框架，针对运动生成任务进行三项关键改进：1) 使用双向注意力而非因果注意力；2) 实现下三角时间调度器而非随机调度；3) 采用连续时间变化方式引入文本条件", "result": "在流式运动生成任务上达到最先进性能，在HumanML3D基准测试中FID达到0.057，首次证明扩散强制框架在该任务上的有效性", "conclusion": "FloodDiffusion通过定制的扩散强制方法成功解决了流式运动生成问题，为时间变化控制事件下的时间序列生成提供了有效框架，代码和模型已开源"}}
{"id": "2512.03519", "pdf": "https://arxiv.org/pdf/2512.03519", "abs": "https://arxiv.org/abs/2512.03519", "authors": ["Ben Larwood", "Oliver J. Sutton", "Callum Cockburn"], "title": "Left shifting analysis of Human-Autonomous Team interactions to analyse risks of autonomy in high-stakes AI systems", "categories": ["cs.HC", "eess.SY"], "comment": "Published in: IfSE Annual Systems Engineering Conference Proceedings 2025", "summary": "Developing high-stakes autonomous systems that include Artificial Intelligence (AI) components is complex; the consequences of errors can be catastrophic, yet it is challenging to plan for all operational cases. In stressful scenarios for the human operator, such as short decision-making timescales, the risk of failures is exacerbated. A lack of understanding of AI failure modes obstructs this and so blocks the robust implementation of applications of AI in smart systems. This prevents early risk identification, leading to increased time, risk and cost of projects. A key tenet of Systems Engineering and acquisition engineering is centred around a \"left-shift\" in test and evaluation activities to earlier in the system lifecycle, to allow for \"accelerated delivery of [systems] that work\". We argue it is therefore essential that this shift includes the analysis of AI failure cases as part of the design stages of the system life cycle. Our proposed framework enables the early characterisation of risks emerging from human-autonomy teaming (HAT) in operational contexts. The cornerstone of this is a new analysis of AI failure modes, built on the seminal modelling of human-autonomy teams laid out by LaMonica et al., 2022. Using the analysis of the interactions between human and autonomous systems and exploring the failure modes within each aspect, our approach provides a way to systematically identify human-AI interactions risks across the operational domain of the system of interest. The understanding of the emergent behaviour enables increased robustness of the system, for which the analysis should be undertaken over the whole scope of its operational design domain. This approach is illustrated through an example use case for an AI assistant supporting a Command & Control (C2) System.", "AI": {"tldr": "提出一个框架，用于在系统生命周期早期分析人机自主团队交互中的AI故障模式风险，特别是在高风险AI系统中", "motivation": "开发包含AI组件的高风险自主系统很复杂，错误后果可能灾难性，但难以规划所有操作场景。在人类操作员面临压力的情况下，故障风险加剧。缺乏对AI故障模式的理解阻碍了AI在智能系统中的稳健实施，导致项目时间、风险和成本增加", "method": "基于LaMonica等人（2022）提出的人机自主团队建模，提出一个新的AI故障模式分析框架。通过分析人机交互并探索每个方面的故障模式，系统性地识别整个操作领域中的人机交互风险", "result": "该方法能够早期表征操作环境中人机自主团队合作出现的风险，理解涌现行为，提高系统稳健性。通过一个支持指挥控制系统的AI助手用例进行了说明", "conclusion": "必须在系统生命周期的设计阶段将AI故障案例分析纳入\"左移\"测试评估活动中，以加速交付可工作的系统。提出的框架为早期识别和分析人机交互风险提供了系统化方法"}}
{"id": "2512.03514", "pdf": "https://arxiv.org/pdf/2512.03514", "abs": "https://arxiv.org/abs/2512.03514", "authors": ["Adithya S Kolavi", "Vyoman Jain"], "title": "M3DR: Towards Universal Multilingual Multimodal Document Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal document retrieval systems have shown strong progress in aligning visual and textual content for semantic search. However, most existing approaches remain heavily English-centric, limiting their effectiveness in multilingual contexts. In this work, we present M3DR (Multilingual Multimodal Document Retrieval), a framework designed to bridge this gap across languages, enabling applicability across diverse linguistic and cultural contexts. M3DR leverages synthetic multilingual document data and generalizes across different vision-language architectures and model sizes, enabling robust cross-lingual and cross-modal alignment. Using contrastive training, our models learn unified representations for text and document images that transfer effectively across languages. We validate this capability on 22 typologically diverse languages, demonstrating consistent performance and adaptability across linguistic and script variations. We further introduce a comprehensive benchmark that captures real-world multilingual scenarios, evaluating models under monolingual, multilingual, and mixed-language settings. M3DR generalizes across both single dense vector and ColBERT-style token-level multi-vector retrieval paradigms. Our models, NetraEmbed and ColNetraEmbed achieve state-of-the-art performance with ~150% relative improvements on cross-lingual retrieval.", "AI": {"tldr": "提出M3DR框架，实现跨语言的通用多模态文档检索系统，解决现有方法主要面向英语的问题", "motivation": "现有多模态文档检索系统主要针对英语，在跨语言场景下效果有限，需要开发能够适应多种语言和文化背景的通用检索框架", "method": "利用合成多语言文档数据，采用对比学习训练统一文本和文档图像表示，支持单密集向量和ColBERT风格的多向量检索范式", "result": "在22种类型多样的语言上验证了性能，NetraEmbed和ColNetraEmbed模型在跨语言检索上实现了约150%的相对改进，达到最先进水平", "conclusion": "M3DR框架成功实现了跨语言多模态文档检索，能够有效处理不同语言和文字变体，为多语言场景下的文档检索提供了通用解决方案"}}
{"id": "2512.03509", "pdf": "https://arxiv.org/pdf/2512.03509", "abs": "https://arxiv.org/abs/2512.03509", "authors": ["Kwaku Opoku-Ware", "Gideon Opoku"], "title": "AfroBeats Dance Movement Analysis Using Computer Vision: A Proof-of-Concept Framework Combining YOLO and Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a preliminary investigation into automated dance movement analysis using contemporary computer vision techniques. We propose a proof-of-concept framework that integrates YOLOv8 and v11 for dancer detection with the Segment Anything Model (SAM) for precise segmentation, enabling the tracking and quantification of dancer movements in video recordings without specialized equipment or markers. Our approach identifies dancers within video frames, counts discrete dance steps, calculates spatial coverage patterns, and measures rhythm consistency across performance sequences. Testing this framework on a single 49-second recording of Ghanaian AfroBeats dance demonstrates technical feasibility, with the system achieving approximately 94% detection precision and 89% recall on manually inspected samples. The pixel-level segmentation provided by SAM, achieving approximately 83% intersection-over-union with visual inspection, enables motion quantification that captures body configuration changes beyond what bounding-box approaches can represent. Analysis of this preliminary case study indicates that the dancer classified as primary by our system executed 23% more steps with 37% higher motion intensity and utilized 42% more performance space compared to dancers classified as secondary. However, this work represents an early-stage investigation with substantial limitations including single-video validation, absence of systematic ground truth annotations, and lack of comparison with existing pose estimation methods. We present this framework to demonstrate technical feasibility, identify promising directions for quantitative dance metrics, and establish a foundation for future systematic validation studies.", "AI": {"tldr": "提出一个结合YOLO和Segment Anything Model的计算机视觉框架，用于自动分析非洲节拍舞蹈动作，包括舞者检测、舞步计数、空间覆盖和节奏一致性测量。", "motivation": "开发无需专业设备或标记的自动化舞蹈动作分析系统，为舞蹈研究提供定量分析工具，探索计算机视觉在舞蹈分析中的应用潜力。", "method": "整合YOLOv8和v11进行舞者检测，结合Segment Anything Model进行精确分割，通过视频分析实现舞者跟踪、舞步计数、空间覆盖模式计算和节奏一致性测量。", "result": "在加纳非洲节拍舞蹈的49秒视频测试中，系统达到约94%的检测精度和89%的召回率，SAM分割达到约83%的交并比。主要舞者比次要舞者多执行23%的舞步，运动强度高37%，使用空间多42%。", "conclusion": "该框架展示了技术可行性，为定量舞蹈指标分析提供了有前景的方向，但仍处于早期研究阶段，存在单视频验证、缺乏系统标注和与现有姿态估计方法比较等局限性。"}}
{"id": "2512.03508", "pdf": "https://arxiv.org/pdf/2512.03508", "abs": "https://arxiv.org/abs/2512.03508", "authors": ["Seogkyu Jeon", "Kibeom Hong", "Hyeran Byun"], "title": "Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation", "categories": ["cs.CV"], "comment": "ICCV 2025 (poster)", "summary": "Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.", "AI": {"tldr": "提出DPMFormer框架，通过领域感知提示学习和对比学习解决语义分割领域泛化中的视觉-文本语义对齐问题", "motivation": "现有基于视觉语言模型的领域泛化语义分割方法忽视了视觉与文本上下文之间的语义错位问题，这是由于在单一源域上学习的固定上下文提示缺乏灵活性", "method": "提出DPMFormer框架，包含三个核心组件：1) 领域感知提示学习促进视觉-文本语义对齐；2) 领域感知对比学习结合纹理扰动多样化可观察域；3) 领域鲁棒一致性学习最小化原始与增强图像预测差异", "result": "在多个DGSS基准测试中取得了最先进的性能，证明了框架的优越性", "conclusion": "通过领域感知提示学习、对比学习和一致性学习的综合方法，有效解决了语义分割领域泛化中的视觉-文本语义对齐问题，提升了模型对多样化环境变化的鲁棒性"}}
{"id": "2512.03500", "pdf": "https://arxiv.org/pdf/2512.03500", "abs": "https://arxiv.org/abs/2512.03500", "authors": ["Te Yang", "Xiangyu Zhu", "Bo Wang", "Quan Chen", "Peng Jiang", "Zhen Lei"], "title": "EEA: Exploration-Exploitation Agent for Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Long-form video understanding requires efficient navigation of extensive visual data to pinpoint sparse yet critical information. Current approaches to longform video understanding either suffer from severe computational overhead due to dense preprocessing, or fail to effectively balance exploration and exploitation, resulting in incomplete information coverage and inefficiency. In this work, we introduce EEA, a novel video agent framework that archives exploration-exploitation balance through semantic guidance with hierarchical tree search process. EEA autonomously discovers and dynamically updates task-relevant semantic queries, and collects video frames closely matched to these queries as semantic anchors. During the tree search process, instead of uniform expansion, EEA preferentially explores semantically relevant frames while ensuring sufficient coverage within unknown segments. Moreover, EEA adaptively combines intrinsic rewards from visionlanguage models (VLMs) with semantic priors by explicitly modeling uncertainty to achieve stable and precise evaluation of video segments. Experiments across various long-video benchmarks validate the superior performance and computational efficiency of our proposed method.", "AI": {"tldr": "提出EEA框架，通过探索-利用平衡的语义引导和层次树搜索过程，实现长视频理解的高效导航和关键信息定位。", "motivation": "当前长视频理解方法存在计算开销大或探索-利用不平衡的问题，导致信息覆盖不完整和效率低下，需要一种能有效平衡探索与利用的框架。", "method": "EEA框架通过语义引导的层次树搜索过程，自主发现并动态更新任务相关语义查询，收集与查询匹配的语义锚点帧，优先探索语义相关帧同时确保未知段覆盖，并自适应结合视觉语言模型的内在奖励和语义先验。", "result": "在多个长视频基准测试中验证了所提方法的优越性能和计算效率。", "conclusion": "EEA框架通过探索-利用平衡的语义引导和层次树搜索，有效解决了长视频理解中的计算效率和信息覆盖问题，实现了高效的关键信息定位。"}}
{"id": "2512.03499", "pdf": "https://arxiv.org/pdf/2512.03499", "abs": "https://arxiv.org/abs/2512.03499", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang"], "title": "NAS-LoRA: Empowering Parameter-Efficient Fine-Tuning for Visual Foundation Models with Searchable Adaptation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The Segment Anything Model (SAM) has emerged as a powerful visual foundation model for image segmentation. However, adapting SAM to specific downstream tasks, such as medical and agricultural imaging, remains a significant challenge. To address this, Low-Rank Adaptation (LoRA) and its variants have been widely employed to enhancing SAM's adaptation performance on diverse domains. Despite advancements, a critical question arises: can we integrate inductive bias into the model? This is particularly relevant since the Transformer encoder in SAM inherently lacks spatial priors within image patches, potentially hindering the acquisition of high-level semantic information. In this paper, we propose NAS-LoRA, a new Parameter-Efficient Fine-Tuning (PEFT) method designed to bridge the semantic gap between pre-trained SAM and specialized domains. Specifically, NAS-LoRA incorporates a lightweight Neural Architecture Search (NAS) block between the encoder and decoder components of LoRA to dynamically optimize the prior knowledge integrated into weight updates. Furthermore, we propose a stage-wise optimization strategy to help the ViT encoder balance weight updates and architectural adjustments, facilitating the gradual learning of high-level semantic information. Various Experiments demonstrate our NAS-LoRA improves existing PEFT methods, while reducing training cost by 24.14% without increasing inference cost, highlighting the potential of NAS in enhancing PEFT for visual foundation models.", "AI": {"tldr": "提出NAS-LoRA方法，将神经架构搜索与低秩适应结合，以增强视觉基础模型在特定下游任务上的参数高效微调能力", "motivation": "SAM等视觉基础模型在适应特定下游任务（如医疗和农业图像）时面临挑战，现有LoRA方法缺乏空间先验知识，Transformer编码器缺乏图像块内的空间先验，可能阻碍获取高级语义信息", "method": "提出NAS-LoRA方法，在LoRA的编码器和解码器组件之间插入轻量级神经架构搜索块，动态优化融入权重更新的先验知识；采用阶段式优化策略，帮助ViT编码器平衡权重更新和架构调整", "result": "实验表明NAS-LoRA改进了现有PEFT方法，训练成本降低24.14%且不增加推理成本，证明了NAS在增强视觉基础模型参数高效微调方面的潜力", "conclusion": "NAS-LoRA通过结合神经架构搜索与低秩适应，有效弥合了预训练SAM与专业领域之间的语义鸿沟，为视觉基础模型的参数高效微调提供了新思路"}}
{"id": "2512.03497", "pdf": "https://arxiv.org/pdf/2512.03497", "abs": "https://arxiv.org/abs/2512.03497", "authors": ["Xiangzheng Cheng", "Haili Huang", "Ye Su", "Qing Nie", "Xiufen Zou", "Suoqin Jin"], "title": "Cell-cell communication inference and analysis: biological mechanisms, computational approaches, and future opportunities", "categories": ["q-bio.QM", "cs.AI", "q-bio.CB"], "comment": null, "summary": "In multicellular organisms, cells coordinate their activities through cell-cell communication (CCC), which are crucial for development, tissue homeostasis, and disease progression. Recent advances in single-cell and spatial omics technologies provide unprecedented opportunities to systematically infer and analyze CCC from these omics data, either by integrating prior knowledge of ligand-receptor interactions (LRIs) or through de novo approaches. A variety of computational methods have been developed, focusing on methodological innovations, accurate modeling of complex signaling mechanisms, and investigation of broader biological questions. These advances have greatly enhanced our ability to analyze CCC and generate biological hypotheses. Here, we introduce the biological mechanisms and modeling strategies of CCC, and provide a focused overview of more than 140 computational methods for inferring CCC from single-cell and spatial transcriptomic data, emphasizing the diversity in methodological frameworks and biological questions. Finally, we discuss the current challenges and future opportunities in this rapidly evolving field.", "AI": {"tldr": "该论文是一篇关于细胞间通讯推断与分析方法的综述文章，系统介绍了从单细胞和空间组学数据中推断细胞间通讯的计算方法、生物学机制和未来发展方向。", "motivation": "多细胞生物中细胞间通讯对发育、组织稳态和疾病进展至关重要。随着单细胞和空间组学技术的发展，需要系统综述从这些数据中推断细胞间通讯的计算方法，以促进该领域的发展。", "method": "论文采用综述方法，系统介绍了140多种计算方法的多样性框架，包括基于配体-受体相互作用先验知识的方法和从头计算方法，重点关注方法学创新、复杂信号机制的准确建模和更广泛的生物学问题研究。", "result": "论文全面综述了细胞间通讯推断的计算方法，强调了方法框架的多样性和生物学问题的广度，为研究者提供了该领域的系统概览和工具选择指导。", "conclusion": "细胞间通讯推断领域发展迅速，计算方法显著增强了分析细胞间通讯和生成生物学假设的能力。未来需要解决当前挑战，把握发展机遇，推动该领域进一步发展。"}}
{"id": "2512.03495", "pdf": "https://arxiv.org/pdf/2512.03495", "abs": "https://arxiv.org/abs/2512.03495", "authors": ["Rui Sheng", "Yifang Wang", "Xingbo Wang", "Shun Dai", "Qingyu Guo", "Tai-Quan Peng", "Huamin Qu", "Dongyu Liu"], "title": "EMINDS: Understanding User Behavior Progression for Mental Health Exploration on Social Media", "categories": ["cs.HC"], "comment": null, "summary": "Mental health is an urgent societal issue, and social scientists are increasingly turning to online mental health communities (OMHCs) to analyze user behavior data for early intervention. However, existing sequence mining techniques fall short of the urgent need to explore the behavior progression of different groups (e.g., recovery or deterioration groups) and track the potential long-term impact of behaviors on mental health status. To address this issue, we introduce EMINDS, a visual analytics system built on a novel automatic mining pipeline that extracts distinct behavior stages and assesses the potential impact of frequent stage patterns on mental health status over time. The system includes a set of interactive visualizations that summarize the meaning of each behavior stage and the evolution of different stage patterns. We feature a pattern-centric Sankey diagram to reveal contextual information about the impact of stage patterns on mental health, helping experts understand the specific changes in sequences before and after a stage pattern. We evaluated the effectiveness and usability of EMINDS through two case studies and expert interviews, which examined the potential stage patterns impacting long-term mental health by analyzing user behaviors on Reddit.", "AI": {"tldr": "开发EMINDS视觉分析系统，用于分析社交媒体上用户行为进展对心理健康的影响", "motivation": "心理健康是紧迫的社会问题，现有序列挖掘技术无法有效探索不同群体（如恢复或恶化群体）的行为进展，也无法追踪行为对心理健康状态的潜在长期影响", "method": "基于新型自动挖掘管道构建EMINDS视觉分析系统，提取不同的行为阶段并评估频繁阶段模式对心理健康状态的潜在影响，包含交互式可视化总结行为阶段含义和阶段模式演化，采用模式为中心的桑基图揭示阶段模式对心理健康的影响", "result": "通过两个案例研究和专家访谈评估了EMINDS的有效性和可用性，通过分析Reddit用户行为检验了影响长期心理健康的潜在阶段模式", "conclusion": "EMINDS系统能够有效帮助专家理解阶段模式前后序列的具体变化，为在线心理健康社区的用户行为进展分析提供了可视化分析工具"}}
{"id": "2512.03485", "pdf": "https://arxiv.org/pdf/2512.03485", "abs": "https://arxiv.org/abs/2512.03485", "authors": ["Rui Sheng", "Zelin Zang", "Jiachen Wang", "Yan Luo", "Zixin Chen", "Yan Zhou", "Shaolun Ruan", "Huamin Qu"], "title": "CellScout: Visual Analytics for Mining Biomarkers in Cell State Discovery", "categories": ["cs.HC"], "comment": null, "summary": "Cell state discovery is crucial for understanding biological systems and enhancing medical outcomes. A key aspect of this process is identifying distinct biomarkers that define specific cell states. However, difficulties arise from the co-discovery process of cell states and biomarkers: biologists often use dimensionality reduction to visualize cells in a two- dimensional space. Then they usually interpret visually clustered cells as distinct states, from which they seek to identify unique biomarkers. However, this assumption is often invalid due to internal inconsistencies in a cluster, making the process trial-and-error and highly uncertain. Therefore, biologists urgently need effective tools to help uncover the hidden association relationships between different cell populations and their potential biomarkers. To address this problem, we first designed a machine-learning algorithm based on the Mixture-of-Experts (MoE) technique to identify meaningful associations between cell populations and biomarkers. We further developed a visual analytics system, CellScout, in collaboration with biologists, to help them explore and refine these association relationships to advance cell state discovery. We validated our system through expert interviews, from which we further selected a representative case to demonstrate its effectiveness in discovering new cell states.", "AI": {"tldr": "开发了一个名为CellScout的可视化分析系统，用于帮助生物学家在细胞状态发现过程中挖掘生物标志物", "motivation": "细胞状态发现对理解生物系统和改善医疗结果至关重要，但当前方法存在局限性：生物学家通常使用降维可视化细胞，将视觉上聚集的细胞视为不同状态，然后寻找独特的生物标志物。然而，这种假设常常无效，因为簇内可能存在不一致性，使得过程成为试错且高度不确定", "method": "首先设计了一个基于Mixture-of-Experts（MoE）技术的机器学习算法，用于识别细胞群体与生物标志物之间的有意义的关联关系。进一步开发了一个可视化分析系统CellScout，与生物学家合作，帮助他们探索和优化这些关联关系以推进细胞状态发现", "result": "通过专家访谈验证了系统，并选择了一个代表性案例来展示其在发现新细胞状态方面的有效性", "conclusion": "CellScout系统通过结合机器学习算法和可视化分析，有效解决了细胞状态与生物标志物协同发现过程中的困难，帮助生物学家更可靠地发现新的细胞状态"}}
{"id": "2512.03477", "pdf": "https://arxiv.org/pdf/2512.03477", "abs": "https://arxiv.org/abs/2512.03477", "authors": ["Zijian Gu", "Yuxi Liu", "Zhenhao Zhang", "Song Wang"], "title": "Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis", "categories": ["cs.CV", "cs.LG"], "comment": "10 pages, 3 tables", "summary": "Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.", "AI": {"tldr": "该论文提出了公平感知的低秩适应方法，用于微调视觉语言模型在医学青光眼诊断任务中，旨在减少不同人口统计群体间的诊断准确性差异。", "motivation": "视觉语言模型在医学影像任务中能达到专家级性能，但在不同人口统计群体间存在显著的诊断准确性差异。现有方法在保持参数效率的同时缺乏明确的公平性优化机制，限制了公平医疗AI在资源受限医疗环境中的实际部署。", "method": "提出了三种公平感知的低秩适应方法：FR-LoRA（集成MaxAccGap正则化）、GR-LoRA（应用逆频率加权平衡梯度贡献）和Hybrid-LoRA（结合两种机制）。核心算法贡献是可微分的MaxAccGap损失函数，支持跨人口统计群体的准确性均衡端到端优化。", "result": "在10,000张青光眼眼底图像上评估，GR-LoRA将诊断准确性差异减少了69%，同时保持53.15%的整体准确性。消融研究表明，强正则化强度能以最小的准确性代价实现最优公平性，种族特定优化可实现60%的差异减少。该方法仅需0.24%的可训练参数。", "conclusion": "该研究提出的公平感知低秩适应方法结合了参数效率和明确的公平性优化，能够在资源受限的医疗环境中实际部署公平的医疗AI系统，为减少医疗AI中的诊断差异提供了实用解决方案。"}}
{"id": "2512.03474", "pdf": "https://arxiv.org/pdf/2512.03474", "abs": "https://arxiv.org/abs/2512.03474", "authors": ["Wenliang Guo", "Yujiang Pu", "Yu Kong"], "title": "Procedural Mistake Detection via Action Effect Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Mistake detection in procedural tasks is essential for building intelligent systems that support learning and task execution. Existing approaches primarily analyze how an action is performed, while overlooking what it produces, i.e., the \\textbf{action effect}. Yet many errors manifest not in the execution itself but in the resulting outcome, such as an unintended object state or incorrect spatial arrangement. To address this gap, we propose Action Effect Modeling (AEM), a unified framework that jointly captures action execution and its outcomes through a probabilistic formulation. AEM first identifies the outcome of an action by selecting the most informative effect frame based on semantic relevance and visual quality. It then extracts complementary cues from visual grounding and symbolic scene graphs, aligning them in a shared latent space to form robust effect-aware representations. To detect mistakes, we further design a prompt-based detector that incorporates task-specific prompts and aligns each action segment with its intended execution semantics. Our approach achieves state-of-the-art performance on the EgoPER and CaptainCook4D benchmarks under the challenging one-class classification (OCC) setting. These results demonstrate that modeling both execution and outcome yields more reliable mistake detection, and highlight the potential of effect-aware representations to benefit a broader range of downstream applications.", "AI": {"tldr": "提出Action Effect Modeling (AEM)框架，通过联合建模动作执行和动作效果来检测程序性任务中的错误", "motivation": "现有方法主要分析动作如何执行，而忽略了动作产生的效果（如物体状态或空间排列的变化），但许多错误恰恰体现在结果而非执行过程中", "method": "AEM框架通过概率建模联合捕捉动作执行和结果：1) 基于语义相关性和视觉质量选择最具信息量的效果帧；2) 从视觉定位和符号场景图中提取互补线索；3) 在共享潜在空间中对齐形成效果感知表示；4) 设计基于提示的检测器，结合任务特定提示并对齐动作段与预期执行语义", "result": "在EgoPER和CaptainCook4D基准测试中，在具有挑战性的单类分类(OCC)设置下取得了最先进的性能", "conclusion": "联合建模执行和结果能实现更可靠的错误检测，效果感知表示有潜力惠及更广泛的下游应用"}}
{"id": "2512.03466", "pdf": "https://arxiv.org/pdf/2512.03466", "abs": "https://arxiv.org/abs/2512.03466", "authors": ["Xavier Cadet", "Edward Koh", "Peter Chin"], "title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation", "categories": ["cs.MA", "cs.AI"], "comment": "Accepted at NeurIPS MTI-LLM 2025", "summary": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.", "AI": {"tldr": "提出AsymPuzl——一个用于评估多智能体合作中信息不对称下通信能力的简约但表达力强的双智能体谜题环境", "motivation": "现有的大语言模型多智能体研究大多关注开放式角色扮演，缺乏受控评估环境，需要一种能够隔离信息不对称下通信能力的测试平台", "method": "设计了一个双智能体符号谜题环境，每个智能体观察到互补但不完整的视图，必须通过交换消息来合作解决谜题，使用多种当前和开源LLM进行实验", "result": "强模型（如GPT-5、Claude-4.0）能可靠地在两轮内通过共享完整信息解决谜题；弱模型常忽略伙伴消息或过度修正假设；反馈设计非平凡——简单自我反馈能提高成功率，而详细联合反馈可能损害性能", "conclusion": "即使在简单的合作任务中，LLM的通信策略也存在差异并依赖于反馈信号的粒度，AsymPuzl为探究多轮合作的极限提供了测试平台，并为研究协调机制开辟了途径"}}
{"id": "2512.03463", "pdf": "https://arxiv.org/pdf/2512.03463", "abs": "https://arxiv.org/abs/2512.03463", "authors": ["Shojiro Yamabe", "Futa Waseda", "Daiki Shiono", "Tsubasa Takahashi"], "title": "Text-Printed Image: Bridging the Image-Text Modality Gap for Text-centric Training of Large Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent large vision-language models (LVLMs) have been applied to diverse VQA tasks. However, achieving practical performance typically requires task-specific fine-tuning with large numbers of image-text pairs, which are costly to collect. In this work, we study text-centric training, a setting where only textual descriptions are available and no real images are provided, as a paradigm for low-cost data scaling. Unlike images, whose collection is often restricted by privacy constraints and scarcity in niche domains, text is widely available. Moreover, text is easily editable, enabling automatic diversification and expansion with LLMs at minimal human effort. While this offers clear advantages over image collection in terms of scalability and cost, training on raw text without images still yields limited gains on VQA tasks because of the image-text modality gap. To address this issue, we propose a Text-Printed Image (TPI), which generates synthetic images by directly rendering the given textual description on a plain white canvas. This simple rendering projects text into the image modality and can be integrated into arbitrary existing LVLM training pipelines at low cost. Moreover, TPI preserves the semantics of the text, whereas text-to-image models often fail to do. Across four models and seven benchmarks, our systematic experiments show that TPI enables more effective text-centric training than synthetic images generated by a diffusion model. We further explore TPI as a low-cost data-augmentation strategy and demonstrate its practical utility. Overall, our findings highlight the significant potential of text-centric training and, more broadly, chart a path toward fully automated data generation for LVLMs.", "AI": {"tldr": "提出Text-Printed Image (TPI)方法，通过将文本直接渲染在白色画布上生成合成图像，以弥合图像-文本模态差距，实现低成本的大规模视觉语言模型文本中心训练。", "motivation": "当前大规模视觉语言模型需要大量图像-文本对进行任务特定微调，但图像收集成本高且受隐私限制。文本数据广泛可用且易于编辑，但仅用原始文本训练因模态差距而效果有限。", "method": "提出TPI方法：将给定的文本描述直接渲染在纯白色画布上生成合成图像。这种简单渲染将文本投影到图像模态，可低成本集成到现有LVLM训练流程中，且能保留文本语义。", "result": "在4个模型和7个基准测试中的系统实验表明，TPI比扩散模型生成的合成图像在文本中心训练中更有效。TPI还可作为低成本数据增强策略，展示了其实用价值。", "conclusion": "TPI方法展示了文本中心训练的巨大潜力，为LVLMs的全自动数据生成开辟了新路径，提供了一种可扩展、低成本的训练范式。"}}
{"id": "2512.03460", "pdf": "https://arxiv.org/pdf/2512.03460", "abs": "https://arxiv.org/abs/2512.03460", "authors": ["Johnny Peng", "Thanh Tung Khuat", "Ellen Otte", "Katarzyna Musial", "Bogdan Gabrys"], "title": "Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG"], "comment": "This is a pre-print for submitting to computers & chemical engineering journal", "summary": "In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.", "AI": {"tldr": "该研究对细胞培养过程监控中有限数据和反馈条件下的机器学习方法进行了比较分析，评估了多种ML方法在生物过程监控中的表现。", "motivation": "细胞培养生物过程中，实时批次过程监控对于确保产品质量和监管合规性至关重要，但软传感器开发面临历史数据有限、反馈稀疏、过程条件异质和高维感官输入等挑战。", "method": "研究采用特征降维、在线学习和即时学习等多种机器学习方法，在一个计算机模拟数据集和两个真实世界实验数据集上进行了全面的基准测试分析。", "result": "研究发现训练策略在处理有限数据和反馈方面很重要：批次学习在均匀设置中有效，而即时学习和在线学习在冷启动场景中表现出更好的适应性。研究还识别出影响模型可转移性的关键元特征，如培养基组成和过程控制策略。", "conclusion": "将拉曼光谱预测与延迟离线测量相结合可以提高监控准确性，为未来生物过程软传感器开发提供了有前景的方向。"}}
{"id": "2512.03454", "pdf": "https://arxiv.org/pdf/2512.03454", "abs": "https://arxiv.org/abs/2512.03454", "authors": ["Haicheng Liao", "Huanming Shen", "Bonan Wang", "Yongkang Li", "Yihong Tang", "Chengyue Wang", "Dingyi Zhuang", "Kehua Chen", "Hai Yang", "Chengzhong Xu", "Zhenning Li"], "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.", "AI": {"tldr": "提出ThinkDeeper框架，通过世界模型原理进行多模态视觉定位，解决自动驾驶中自然语言指令的模糊性和上下文依赖问题", "motivation": "现有自动驾驶视觉定位方法在处理模糊、上下文依赖指令时存在困难，缺乏对3D空间关系和场景演变的推理能力", "method": "提出ThinkDeeper框架，核心是空间感知世界模型(SA-WM)，通过将当前场景蒸馏为指令感知潜在状态并展开未来潜在状态序列，提供前瞻性线索；配合超图引导解码器分层融合多模态输入", "result": "在六个基准测试中表现优异，在Talk2Car排行榜排名第一，在DrivePilot、MoCAD和RefCOCO/+/g基准上超越最先进基线，在挑战性场景中表现出强鲁棒性和效率", "conclusion": "基于世界模型的ThinkDeeper框架通过推理未来空间状态显著提升了自动驾驶中的多模态视觉定位性能，为解决模糊指令提供了有效解决方案"}}
{"id": "2512.03453", "pdf": "https://arxiv.org/pdf/2512.03453", "abs": "https://arxiv.org/abs/2512.03453", "authors": ["Yunpeng Bai", "Shaoheng Fang", "Chaohui Yu", "Fan Wang", "Qixing Huang"], "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model", "categories": ["cs.CV"], "comment": "Project Page: https://geovideo.github.io/GeoVideo/", "summary": "Recent advances in video generation have enabled the synthesis of high-quality and visually realistic clips using diffusion transformer models. However, most existing approaches operate purely in the 2D pixel space and lack explicit mechanisms for modeling 3D structures, often resulting in temporally inconsistent geometries, implausible motions, and structural artifacts. In this work, we introduce geometric regularization losses into video generation by augmenting latent diffusion models with per-frame depth prediction. We adopted depth as the geometric representation because of the great progress in depth prediction and its compatibility with image-based latent encoders. Specifically, to enforce structural consistency over time, we propose a multi-view geometric loss that aligns the predicted depth maps across frames within a shared 3D coordinate system. Our method bridges the gap between appearance generation and 3D structure modeling, leading to improved spatio-temporal coherence, shape consistency, and physical plausibility. Experiments across multiple datasets show that our approach produces significantly more stable and geometrically consistent results than existing baselines.", "AI": {"tldr": "该论文提出了一种名为GeoVideo的方法，通过在视频生成模型中引入几何正则化损失，特别是通过多视角几何损失来增强时空一致性，从而改善生成视频的几何结构一致性。", "motivation": "现有视频生成方法主要在2D像素空间操作，缺乏显式的3D结构建模机制，导致时间上的几何不一致、运动不合理和结构伪影等问题。需要将外观生成与3D结构建模相结合。", "method": "通过增强潜在扩散模型，加入每帧深度预测作为几何表示。提出多视角几何损失，在共享3D坐标系中对齐跨帧的预测深度图，强制结构在时间上的一致性。", "result": "在多个数据集上的实验表明，该方法相比现有基线能产生显著更稳定和几何一致的结果，改善了时空连贯性、形状一致性和物理合理性。", "conclusion": "GeoVideo方法成功地将几何正则化引入视频生成，弥合了外观生成与3D结构建模之间的差距，提高了生成视频的几何质量和物理合理性。"}}
{"id": "2512.03450", "pdf": "https://arxiv.org/pdf/2512.03450", "abs": "https://arxiv.org/abs/2512.03450", "authors": ["Rhys Newbury", "Juyan Zhang", "Tin Tran", "Hanna Kurniawati", "Dana Kulić"], "title": "KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.", "AI": {"tldr": "本文提出了一种无监督学习3D关键点的方法，通过潜在扩散模型从点云数据中学习具有空间结构的关键点表示，并将其用于条件生成完整形状。", "motivation": "当前大多数无监督关键点方法不适用于无条件生成设置，限制了它们在现代3D生成流程中的应用。需要一种能够桥接这一差距的方法，从3D点云数据中学习结构化的关键点表示。", "method": "提出KeyPointDiffuser框架：1）从点云数据中无监督学习空间结构化的3D关键点；2）使用这些关键点作为条件信息来训练Elucidated Diffusion Model（EDM），以重建完整形状；3）通过扩散模型学习关键点的几何变化。", "result": "方法在多种物体类别上表现优异，关键点一致性比现有方法提高了6个百分点。学习到的关键点在不同物体实例间具有可重复的空间结构，支持关键点空间的平滑插值，表明它们能够捕捉几何变化。", "conclusion": "KeyPointDiffuser成功地将无监督关键点学习与扩散模型相结合，为3D生成任务提供了紧凑且可解释的表示方法，填补了无监督关键点方法在生成式设置中的应用空白。"}}
{"id": "2512.03449", "pdf": "https://arxiv.org/pdf/2512.03449", "abs": "https://arxiv.org/abs/2512.03449", "authors": ["Tongxu Zhang"], "title": "LM-CartSeg: Automated Segmentation of Lateral and Medial Cartilage and Subchondral Bone for Radiomics Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Background and Objective: Radiomics of knee MRI requires robust, anatomically meaningful regions of interest (ROIs) that jointly capture cartilage and subchondral bone. Most existing work relies on manual ROIs and rarely reports quality control (QC). We present LM-CartSeg, a fully automatic pipeline for cartilage/bone segmentation, geometric lateral/medial (L/M) compartmentalisation and radiomics analysis. Methods: Two 3D nnU-Net models were trained on SKM-TEA (138 knees) and OAIZIB-CM (404 knees). At test time, zero-shot predictions were fused and refined by simple geometric rules: connected-component cleaning, construction of 10 mm subchondral bone bands in physical space, and a data-driven tibial L/M split based on PCA and k-means. Segmentation was evaluated on an OAIZIB-CM test set (103 knees) and on SKI-10 (100 knees). QC used volume and thickness signatures. From 10 ROIs we extracted 4 650 non-shape radiomic features to study inter-compartment similarity, dependence on ROI size, and OA vs. non-OA classification on OAIZIB-CM Results: Post-processing improved macro ASSD on OAIZIB-CM from 2.63 to 0.36 mm and HD95 from 25.2 to 3.35 mm, with DSC 0.91; zero-shot DSC on SKI-10 was 0.80. The geometric L/M rule produced stable compartments across datasets, whereas a direct L/M nnU-Net showed domain-dependent side swaps. Only 6 to 12 percent of features per ROI were strongly correlated with volume or thickness. Radiomics-based models models restricted to size-linked features. Conclusions: LM-CartSeg yields automatic, QCd ROIs and radiomic features that carry discriminative information beyond simple morphometry, providing a practical foundation for multi-centre knee OA radiomics studies.", "AI": {"tldr": "提出LM-CartSeg全自动管道，用于膝关节MRI的软骨/骨分割、几何侧/内侧分区和影像组学分析", "motivation": "膝关节MRI影像组学研究需要稳健的解剖学有意义ROI来联合捕获软骨和软骨下骨，现有方法多依赖手动ROI且缺乏质量控制报告", "method": "使用两个3D nnU-Net模型在SKM-TEA和OAIZIB-CM数据集上训练，通过零样本预测融合和几何规则细化，包括连通分量清洗、构建10mm软骨下骨带、基于PCA和k-means的胫骨侧/内侧分割", "result": "后处理显著改善分割质量（ASSD从2.63降至0.36mm，HD95从25.2降至3.35mm，DSC 0.91），几何侧/内侧规则产生稳定分区，仅6-12%特征与体积/厚度强相关", "conclusion": "LM-CartSeg提供自动、质量控制的ROI和影像组学特征，携带超越简单形态测量的判别信息，为多中心膝骨关节炎影像组学研究提供实用基础"}}
