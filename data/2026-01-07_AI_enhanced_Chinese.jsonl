{"id": "2601.03256", "pdf": "https://arxiv.org/pdf/2601.03256", "abs": "https://arxiv.org/abs/2601.03256", "authors": ["Hexiao Lu", "Xiaokun Sun", "Zeyu Cai", "Hao Guo", "Ying Tai", "Jian Yang", "Zhenyu Zhang"], "title": "Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training", "categories": ["cs.CV"], "comment": "Project page: https://luhexiao.github.io/Muses.github.io/", "summary": "We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses' state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.", "AI": {"tldr": "Muses是一种无需训练的方法，用于生成幻想风格的3D生物。", "motivation": "前人的方法由于难以处理复杂的部分级操作和有限的域外生成，常常产生不现实或缺乏连贯性的3D资产。Muses通过利用骨骼这一生物学形式的基础表示来解决这些问题。", "method": "Muses首先构建一个具有合理结构的创意三维骨架；然后在这个结构化的潜在空间中指导基于体素的组装过程；最后，在骨骼条件下进行图像引导的外观建模，生成风格一致且和谐的纹理。", "result": "实验结果显示，Muses在视觉保真度和对文本描述的符合程度方面达到了最先进的水平，并展示了灵活编辑3D对象的能力。", "conclusion": "通过利用结构化的三维骨架表示，Muses能够以无需训练的方式创建连贯、富有创意且风格一致的幻想生物模型。"}}
{"id": "2601.03252", "pdf": "https://arxiv.org/pdf/2601.03252", "abs": "https://arxiv.org/abs/2601.03252", "authors": ["Hao Yu", "Haotong Lin", "Jiawei Wang", "Jiaxin Li", "Yida Wang", "Xueyang Zhang", "Yue Wang", "Xiaowei Zhou", "Ruizhen Hu", "Sida Peng"], "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "categories": ["cs.CV"], "comment": "19 pages, 13 figures", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "AI": {"tldr": "本文提出了InfiniDepth，一种基于神经隐式场的深度估计方法，可以在任意分辨率下进行精细的深度预测。", "motivation": "现有的深度估计方法受限于离散图像网格表示，难以扩展到任意输出分辨率，并且在几何细节恢复上存在局限性。为了克服这些问题，引入了InfiniDepth。", "method": "通过简单的局部隐式解码器，InfiniDepth能够在连续的二维坐标下查询深度值，支持任意分辨率和精细粒度的深度估计。", "result": "实验结果表明，在合成数据集和真实世界基准上的相对和绝对深度估计任务中，InfiniDepth达到了最先进的性能，特别是在细节区域表现突出。同时在大幅度视角变化下的新视图合成任务中也表现出色。", "conclusion": "InfiniDepth通过神经隐式场表示实现了高精度的深度预测，并且展示了其在大规模应用场景中的潜力和优越性。"}}
{"id": "2601.03250", "pdf": "https://arxiv.org/pdf/2601.03250", "abs": "https://arxiv.org/abs/2601.03250", "authors": ["Daoan Zhang", "Wenlin Yao", "Xiaoyang Wang", "Yebowen Hu", "Jiebo Luo", "Dong Yu"], "title": "A Versatile Multimodal Agent for Multimedia Content Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.", "AI": {"tldr": "本文提出了一种多功能的多媒体代理，旨在自动化复杂的内容创作过程。", "motivation": "当前AI生成内容模型难以实现多模态集成和端到端的任务处理，而编辑专家们通常需要处理各种图像和视频输入，并产生包含音频、文本等多种元素的输出。", "method": "提出了一种多媒体代理系统，包括数据生成管道、内容创作工具库以及评估偏好评级标准。通过引入技能获取理论来模拟训练数据整理及代理人培训过程，并设计了一个两阶段相关策略进行计划优化。", "result": "实验结果表明，该方法是有效的，所提出的多媒体代理能够产生比新型模型更好的多模态内容。", "conclusion": "本文展示了一种创新的多功能多媒体代理系统，在复杂的内容生成任务中展现了优越性。"}}
{"id": "2601.03247", "pdf": "https://arxiv.org/pdf/2601.03247", "abs": "https://arxiv.org/abs/2601.03247", "authors": ["Leonardo Bettini", "Amirhossein Kazemipour", "Robert K. Katzschmann", "George Haller"], "title": "Nonlinear Spectral Modeling and Control of Soft-Robotic Muscles from Data", "categories": ["math.DS", "cs.CE", "cs.RO", "eess.SY", "math.OC"], "comment": null, "summary": "Artificial muscles are essential for compliant musculoskeletal robotics but complicate control due to nonlinear multiphysics dynamics. Hydraulically amplified electrostatic (HASEL) actuators, a class of soft artificial muscles, offer high performance but exhibit memory effects and hysteresis. Here we present a data-driven reduction and control strategy grounded in spectral submanifold (SSM) theory. In the adiabatic regime, where inputs vary slowly relative to intrinsic transients, trajectories rapidly converge to a low-dimensional slow manifold. We learn an explicit input-to-output map on this manifold from forced-response trajectories alone, avoiding decay experiments that can trigger hysteresis. We deploy the SSM-based model for real-time control of an antagonistic HASEL-clutch joint. This approach yields a substantial reduction in tracking error compared to feedback-only and feedforward-only baselines under identical settings. This record-and-control workflow enables rapid characterization and high-performance control of soft muscles and muscle-driven joints without detailed physics-based modeling.", "AI": {"tldr": "本文提出了一种基于谱子流形理论的数据驱动降维和控制策略，用于软机器人肌肉的实时控制。", "motivation": "软人工肌肉在柔顺仿生机器人中至关重要，但由于非线性多物理动态特性，其控制变得复杂。液压增强电静电力（HASEL）执行器提供高性能但表现出记忆效应和迟滞。", "method": "本文提出了基于谱子流形理论的数据驱动降维和控制策略，在慢输入快速变化的情况下，轨迹迅速收敛到低维度慢流形上，并从强制响应轨迹中学习输入输出映射。", "result": "该方法在实际情况下实现了显著的跟踪误差减少，相比反馈和前馈基线性能更优。", "conclusion": "通过记录和控制工作流程可以快速表征并实现软肌肉及其驱动关节的高性能控制，而无需详细的基于物理模型的方法。"}}
{"id": "2601.03244", "pdf": "https://arxiv.org/pdf/2601.03244", "abs": "https://arxiv.org/abs/2601.03244", "authors": ["Julián Tachella", "Mike Davies"], "title": "Self-Supervised Learning from Noisy and Incomplete Data", "categories": ["stat.ML", "cs.LG", "eess.IV"], "comment": ":68U10ACM Class:I.4.5; I.2.10; G.3", "summary": "Many important problems in science and engineering involve inferring a signal from noisy and/or incomplete observations, where the observation process is known. Historically, this problem has been tackled using hand-crafted regularization (e.g., sparsity, total-variation) to obtain meaningful estimates. Recent data-driven methods often offer better solutions by directly learning a solver from examples of ground-truth signals and associated observations. However, in many real-world applications, obtaining ground-truth references for training is expensive or impossible. Self-supervised learning methods offer a promising alternative by learning a solver from measurement data alone, bypassing the need for ground-truth references. This manuscript provides a comprehensive summary of different self-supervised methods for inverse problems, with a special emphasis on their theoretical underpinnings, and presents practical applications in imaging inverse problems.", "AI": {"tldr": "本文综述了不同自监督学习方法在逆问题中的应用，并特别强调其理论基础，展示了实际成像逆问题中的应用场景。", "motivation": "传统的方法使用手工设计的正则化来解决信号从噪声或不完整观测中恢复的问题，但获取训练用的真实参考数据昂贵且困难。自监督学习提供了一种替代方案，通过仅使用测量数据学习求解器，无需真实参考。", "method": "综述了不同自监督方法在处理逆问题中的应用，并讨论其理论依据和实际效果。", "result": "展示了这些方法在成像逆问题中的实际应用场景及性能表现。", "conclusion": "自监督学习为解决从噪声或不完整数据中恢复信号提供了有效的方法，特别适用于真实参考难以获取的情况。"}}
{"id": "2601.03237", "pdf": "https://arxiv.org/pdf/2601.03237", "abs": "https://arxiv.org/abs/2601.03237", "authors": ["Javier Salazar Cavazos"], "title": "PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters", "categories": ["cs.LG", "eess.IV", "stat.ML"], "comment": "ef:IEEE Signal Processing Letters, vol. 33, pp. 91-95, 2026", "summary": "Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.", "AI": {"tldr": "本文提出了一种名为PET-TURTLE的深度无监督支持向量机方法，旨在解决不平衡数据集上的聚类问题。", "motivation": "传统的TURTLE算法在处理平衡的数据集群时效果很好，但在面对不平衡的数据分布时表现不佳。为了改善这种情况并提高总体聚类精度，提出了PET-TURTLE算法。", "method": "通过引入幂律先验来修改成本函数以处理不平衡数据集，并在标签过程中加入稀疏逻辑值，优化了搜索空间，从而提高了平衡数据集中分类的准确性。", "result": "实验表明，与原始TURTLE方法相比，PET-TURTLE能更好地提升不平衡来源的数据聚类精度，减少对少数群体的过度预测，提高总体聚类性能。", "conclusion": "通过改进成本函数和优化搜索空间，PET-TURTLE在解决不平衡数据集上的聚类问题上表现出了显著的优势。"}}
{"id": "2601.03236", "pdf": "https://arxiv.org/pdf/2601.03236", "abs": "https://arxiv.org/abs/2601.03236", "authors": ["Dongming Jiang", "Yi Li", "Guanpeng Li", "Bingzhe Li"], "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.", "AI": {"tldr": "提出一种基于多图的代理记忆架构MAGMA，以改进AI代理的记忆和推理能力。", "motivation": "现有方法依赖于单一记忆存储中的语义相似性检索，导致解释性和查询意图与检索证据之间的对齐度不足，影响了长序列推理精度。", "method": "设计一个多图代理记忆体系结构MAGMA，在多个独立的图形表示中组织记忆项，并通过策略引导遍历这些关系视图进行检索。", "result": "实验表明，MAGMA在长时间推理任务上优于最先进的代理记忆系统。", "conclusion": "MAGMA提供了一种透明和可控的记忆检索方法，改进了AI代理的理解能力和长序列推理性能。"}}
{"id": "2601.03233", "pdf": "https://arxiv.org/pdf/2601.03233", "abs": "https://arxiv.org/abs/2601.03233", "authors": ["Yoav HaCohen", "Benny Brazowski", "Nisan Chiprut", "Yaki Bitterman", "Andrew Kvochko", "Avishai Berkowitz", "Daniel Shalem", "Daphna Lifschitz", "Dudu Moshe", "Eitan Porat", "Eitan Richardson", "Guy Shiran", "Itay Chachy", "Jonathan Chetboun", "Michael Finkelson", "Michael Kupchick", "Nir Zabari", "Nitzan Guetta", "Noa Kotler", "Ofir Bibi", "Ori Gordon", "Poriya Panet", "Roi Benita", "Shahar Armon", "Victor Kulikov", "et al. (4 additional authors not shown)"], "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "categories": ["cs.CV"], "comment": null, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "AI": {"tldr": "LTX-2是一个开放源代码基础模型，能够生成高质量的同步音频视频内容。", "motivation": "当前的文字到视频扩散模型可以生成逼真的视频序列，但它们缺乏音频提供的语义、情感和氛围线索。为了填补这一空白，研究者引入了LTX-2，一个可以在统一模式下生成高质同步音视频的基础模型。", "method": "LTX-2由不对称双流变压器组成，包括14B参数的视频流和5B参数的音频流，并通过双向音频视频交叉注意力层以及跨模态AdaLN进行时间位置嵌入共享时长条件。此架构实现了统一音视频模型的有效训练与推理，同时为视频生成分配了更多容量。", "result": "LTX-2在开放源代码系统中达到了最先进的音视频质量和提示遵守度，并且以更低的计算成本和推理时间为代价与专有模型的结果相当。", "conclusion": "所有模型权重和代码都是公开发布的，展示了LTX-2在生成同步音频视频内容方面的优越性和开放性。"}}
{"id": "2601.03232", "pdf": "https://arxiv.org/pdf/2601.03232", "abs": "https://arxiv.org/abs/2601.03232", "authors": ["Kartik Bose", "Abhinandan Kumar", "Raghuraman Soundararajan", "Priya Mudgil", "Samonee Ralmilay", "Niharika Dutta", "Manphool Singhal", "Arun Kumar", "Saugata Sen", "Anurima Patra", "Priya Ghosh", "Abanti Das", "Amit Gupta", "Ashish Verma", "Dipin Sudhakaran", "Ekta Dhamija", "Himangi Unde", "Ishan Kumar", "Krithika Rangarajan", "Prerna Garg", "Rachel Sequeira", "Sudhin Shylendran", "Taruna Yadav", "Tej Pal", "Pankaj Gupta"], "title": "Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between <1B and >=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.", "AI": {"tldr": "构建了一个经放射科医生验证的合成多RADS基准RXL-RADSet，并比较了开源和专有语言模型在RADS分配中的有效性和准确性。", "motivation": "自动从叙述报告中进行RADS分配面临挑战，因为存在指南复杂性、输出格式限制以及不同RADS框架和模型规模之间的有限基准测试问题。", "method": "生成了一个包含1600个合成放射学报告的RXL-RADSet数据集，并使用固定引导提示评估了41种量化小型语言模型（SLMs）及GPT-5.2。主要指标是有效性和准确性，次要分析比较了引导提示与零样本提示。", "result": "在引导提示下，GPT-5.2实现了99.8%的有效性及81.1%的准确率；合并后的SLMs达到了96.8%的有效性和61.1%的准确性。大型模型（20-32B）有效性能接近专有模型水平，在引导提示下，有效性达到99.2%，准确性为78.5%", "conclusion": "RXL-RADSet提供了经放射科医生验证的多RADS基准；在引导提示下，大型SLMs（20-32B）可以接近专有模型性能，但对于复杂度更高的方案仍存在差距。"}}
{"id": "2601.03227", "pdf": "https://arxiv.org/pdf/2601.03227", "abs": "https://arxiv.org/abs/2601.03227", "authors": ["Ruixing Zhang", "Zihan Liu", "Leilei Sun", "Tongyu Zhu", "Weifeng Lv"], "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.", "AI": {"tldr": "本文提出AGL1K，一个用于评估音频语言模型（ALMs）在地理定位能力上的基准数据集。", "motivation": "由于高质量的音频地理位置对的缺乏，音频地理定位的进步受到了限制。因此，为了填补这个空白，作者创建了第一个针对音频语言模型的音频地理定位基准AGL1K。", "method": "通过引入“音频可识别性”度量从众包平台上提取可靠的本地化样本，并收集了覆盖72个国家和地区的1444个音频剪辑。然后，在16种ALMs上进行评估，分析这些模型的推理轨迹、地区偏见以及错误原因。", "result": "实验表明闭源模型在性能上显著优于开源模型；语言线索常常作为预测的主要支撑。", "conclusion": "AGL1K为音频地理定位提供了基准，并可能推动ALMs更好地理解地理位置信息。"}}
{"id": "2601.03225", "pdf": "https://arxiv.org/pdf/2601.03225", "abs": "https://arxiv.org/abs/2601.03225", "authors": ["Yun Ye", "Yuan Che", "Haoyang Liang", "Yingheng Zhang", "Pengpeng Xu"], "title": "Wait or cross? Understanding the influence of behavioral tendency, trust, and risk perception on pedestrian gap-acceptance of automated truck platoons", "categories": ["cs.HC"], "comment": null, "summary": "Although automated trucks have the potential to improve freight efficiency, reduce costs, and address driver shortages, organizing two or more trucks in a convoy has raised considerable concerns for pedestrian safety. This study conducted a controlled experiment to examine the influence of behavioral tendency, trust, and risk perception on pedestrian intention to cross in front of an automated truck platoon. A total of 603 subjects participated in the virtual reality video-based questionnaire survey. By fusing the merits of structural equation modeling and artificial neural networks, a two-stage, hybrid model was developed to examine complex relationships between latent variables and gap-acceptance behaviors. Our results indicated that subjects watched an average of five vehicle gaps before starting crossing and the average time gap accepted was about 5.35 seconds. Risk perception not only played the most dominant role in shaping pedestrian crossing decisions, but also served as the strong bone, mediating the effects of behavioral tendency and trust on gap-acceptance. Participants who frequently violated traffic rules were more likely to accept a smaller time gap, while those who showed positive behaviors to other road users tended to wait for a larger time gap. Participants who often committed errors, showed aggressive behaviors, and held greater trust in the safety of automated trucks generally reported a lower level of risk for road-crossing in front of automated truck platoons. Built on these findings, a range of tailored countermeasures were proposed to ensure safer and smother interactions between pedestrians and automated truck platoons.", "AI": {"tldr": "研究探讨了行人过马路时的行为倾向、信任度和风险感知对自动化卡车编队通行决策的影响。", "motivation": "尽管自动驾驶卡车有望提高货运效率并解决驾驶员短缺问题，但两辆或多辆卡车组成的车队引发了行人的安全担忧。因此，本文旨在通过实验探究这些因素如何影响行人过马路的决定。", "method": "研究采用虚拟现实视频问卷调查的方式进行，并开发了一种结合结构方程模型和人工神经网络优势的双阶段混合模型来考察潜在变量与通行间隔行为之间的复杂关系。", "result": "结果显示，参与者平均需要观察五个车辆间隙后才会开始过马路，而他们接受的时间间隙平均约为5.35秒。风险感知在影响行人过马路决定中扮演最重要角色，并且还充当了中介作用，影响行为倾向和信任对时间间隔选择的影响。", "conclusion": "基于这些发现，提出了多种针对性的措施以确保行人与自动化卡车编队之间的更安全、顺畅互动。"}}
{"id": "2601.03223", "pdf": "https://arxiv.org/pdf/2601.03223", "abs": "https://arxiv.org/abs/2601.03223", "authors": ["Yun Ye", "Zexuan Li", "Panagiotis Angeloudis", "S. C. Wong", "Jian Sun", "Haoyang Liang"], "title": "Are eHMIs always helpful? Investigating how eHMIs interfere with pedestrian behavior on multi-lane streets: An eye-tracking virtual reality experiment", "categories": ["cs.HC"], "comment": null, "summary": "Appropriate communication is crucial for efficient and safe interactions between pedestrians and autonomous vehicles (AVs). External human-machine interfaces (eHMIs) on AVs, which can be categorized as allocentric or egocentric, are considered a promising solution. While the effectiveness of eHMIs has been extensively studied, in complex environments, such as unsignalized multi-lane streets, their potential to interfere with pedestrian crossing behavior remains underexplored. Hence, a virtual reality-based experiment was conducted to examine how different types of eHMIs displayed on AVs affect the crossing behavior of pedestrians in multi-lane streets environments, with a focus on the gaze patterns of pedestrians during crossing. The results revealed that the presence of eHMIs significantly influenced the cognitive load on pedestrians and increased the possibility of distraction, even misleading pedestrians in cases involving multiple AVs on multi-lane streets. Notably, allocentric eHMIs induced higher cognitive loads and greater distraction in pedestrians than egocentric eHMIs. This was primarily evidenced by longer gaze time and higher proportions of attention for the eHMI on the interacting vehicle, as well as a broader distribution of gaze toward vehicles in the non-interacting lane. However, misleading behavior was mainly triggered by eHMI signals from yielding vehicles in the non-interacting lane. Under such asymmetric signal configurations, egocentric eHMIs resulted in a higher misjudgment rate than allocentric eHMIs. These findings highlight the importance of enhancing eHMI designs to balance the clarity and consistency of the displayed information across different perspectives, especially in complex multi-lane traffic scenarios. This study provides valuable insights regarding the application and standardization of future eHMI systems for AVs.", "AI": {"tldr": "研究了自动驾驶汽车外部人机界面（eHMIs）在多车道街道上对行人过街行为的影响，尤其是其引起认知负荷和分散注意力的程度。", "motivation": "虽然已有大量关于eHMIs有效性的研究，但在复杂的、无信号控制的多车道环境中，它们可能干扰行人的过街行为的研究相对较少。此研究旨在填补这一空白，探讨不同类型的eHMIs对行人过街决策的影响。", "method": "通过虚拟现实实验，在模拟的多车道街道环境下测试了两种类型（allocentric和egocentric）eHMIs在自动驾驶汽车上的显示如何影响行人的过街行为及其注意力分配。", "result": "研究发现，eHMIs的存在显著增加了行人的认知负荷并可能导致分散注意力。尤其是allocentric eHMIs比egocentric eHMIs引起更高的认知负担和更大的分散，特别是在涉及多个自动驾驶汽车的情景中，行人更可能受到误导。", "conclusion": "研究表明，在复杂多车道交通场景下设计eHMI时需要考虑信息清晰性和一致性的重要性，并为未来eHMI系统的应用及标准化提供了宝贵的见解。"}}
{"id": "2601.03222", "pdf": "https://arxiv.org/pdf/2601.03222", "abs": "https://arxiv.org/abs/2601.03222", "authors": ["Jacob Erickson"], "title": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Manuscript under review", "summary": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.", "AI": {"tldr": "本文提出了假朋友困境（FFD）的概念，探讨了用户对看似友好的AI系统的信任如何被利用来实现与用户利益不一致的目标。", "motivation": "随着对话式人工智能系统越来越融入日常生活，它们引发了关于用户自主权、信任和影响其行为的商业利益的重要问题。", "method": "通过参考信任理论、AI一致性研究以及监视资本主义文献，构建了一个危害类型学，并评估了可能的缓解策略。", "result": "提出了假朋友困境这一关键框架来分析AI系统如何利用用户的信任进行微妙的操纵与剥削。", "conclusion": "FFD提供了一种理解AI系统如何在保持友好外表的同时削弱用户自主性的视角。"}}
{"id": "2601.03218", "pdf": "https://arxiv.org/pdf/2601.03218", "abs": "https://arxiv.org/abs/2601.03218", "authors": ["Yuan Che", "Mun On Wong", "Xiaowei Gao", "Haoyang Liang", "Yun Ye"], "title": "Enhancing Safety in Automated Ports: A Virtual Reality Study of Pedestrian-Autonomous Vehicle Interactions under Time Pressure, Visual Constraints, and Varying Vehicle Size", "categories": ["cs.HC"], "comment": null, "summary": "Autonomous driving improves traffic efficiency but presents safety challenges in complex port environments. This study investigates how environmental factors, traffic factors, and pedestrian characteristics influence interaction safety between autonomous vehicles and pedestrians in ports. Using virtual reality (VR) simulations of typical port scenarios, 33 participants completed pedestrian crossing tasks under varying visibility, vehicle sizes, and time pressure conditions. Results indicate that low-visibility conditions, partial occlusions and larger vehicle sizes significantly increase perceived risk, prompting pedestrians to wait longer and accept larger gaps. Specifically, pedestrians tended to accept larger gaps and waited longer when interacting with large autonomous truck platoons, reflecting heightened caution due to their perceived threat. However, local obstructions also reduce post-encroachment time, compressing safety margins. Individual attributes such as age, gender, and driving experience further shape decision-making, while time pressure undermines compensatory behaviors and increases risk. Based on these findings, safety strategies are proposed, including installing wide-angle cameras at multiple viewpoints, enabling real-time vehicle-infrastructure communication, enhancing port lighting and signage, and strengthening pedestrian safety training. This study offers practical recommendations for improving the safety and deployment of vision-based autonomous systems in port settings.", "AI": {"tldr": "研究使用虚拟现实模拟港口环境中行人与自动驾驶车辆的交互，探索环境因素和行人特征对安全性的影响。", "motivation": "自动驾驶技术提高了交通效率但带来了复杂港口环境中的安全挑战。该研究旨在通过VR仿真分析影响行人与自动驾驶车辆互动的安全性因素。", "method": "33名参与者在不同能见度、车辆大小和时间压力条件下完成虚拟现实中的过马路任务。", "result": "低能见度、部分遮挡和较大车体增加了行人的感知风险，导致等待时间和接受的间隙增大；局部障碍减少了后侵入时间，压缩了安全距离。个人因素如年龄、性别和驾驶经验也影响决策，而时间压力则增加风险。", "conclusion": "建议采用广角摄像头、车辆基础设施实时通信、增强港口照明标志以及加强行人安全培训等策略来提高自动系统的安全性与部署效率。"}}
{"id": "2601.03211", "pdf": "https://arxiv.org/pdf/2601.03211", "abs": "https://arxiv.org/abs/2601.03211", "authors": ["Yue Kang", "Zhuoyi Huang", "Benji Schussheim", "Diana Licon", "Dina Atia", "Shixing Cao", "Jacob Danovitch", "Kunho Kim", "Billy Norcilien", "Jonah Karpman", "Mahmound Sayed", "Mike Taylor", "Tao Sun", "Pavel Metrikov", "Vipul Agarwal", "Chris Quirk", "Ye-Yi Wang", "Nick Craswell", "Irene Shaffer", "Tianwei Chen", "Sulaiman Vesal", "Soundar Srinivasan"], "title": "Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.", "AI": {"tldr": "本文提出了一种有效的方法，通过微调小型语言模型（SLMs）来生成高质量的相关性标签数据，以支持企业搜索。", "motivation": "在企业搜索中，构建大规模的高质量数据集仍然是一个挑战。为了克服这一问题，文章提出了使用小型语言模型进行准确的相关性标注的新方法。", "method": "该方法利用大型语言模型（LLMs）生成合成的企业查询，并通过BM25检索困难样本，用教师LLM分配相关性得分。然后将这些数据集提炼到SLMs中以生成紧凑的相关性标签器。", "result": "评估显示，经过精炼的SLMs与人类判断达成一致的程度可以与或优于教师LLM。此外，该方法显著提高了处理速度，增加了17倍，并且比教师模型成本低19倍。", "conclusion": "这种方法支持大规模、经济高效的相关性标注，适用于企业规模检索应用程序中的离线评估和迭代。"}}
{"id": "2601.03205", "pdf": "https://arxiv.org/pdf/2601.03205", "abs": "https://arxiv.org/abs/2601.03205", "authors": ["Yile Liu", "Yixian Liu", "Zongwei Li", "Yufei Huang", "Xinhua Feng", "Zhichao Hu", "Jinglu Hu", "Jianfeng Yan", "Fengzong Lian", "Yuhong Liu"], "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures, 7 tables", "summary": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.", "AI": {"tldr": "通过大规模数据合成和双极浮点奖励机制提升大型语言模型的推理能力。", "motivation": "尽管大模型在自然语言处理中表现出显著潜力，但复杂多步骤逻辑、计划和验证的通用推理仍然是瓶颈。现有的强化学习方法难以应对大规模高质量难度校准的数据挑战。", "method": "提出UltraLogic框架，通过代码解决方式分离问题的核心逻辑和自然语言表达，自动化生成高质量数据；引入双极浮点奖励机制以克服二元奖励稀疏性和非负奖励陷阱。", "result": "实验表明任务多样性是推理能力提升的主要驱动力，并且结合难度匹配策略的双极浮点奖励机制显著提高了训练效率。", "conclusion": "UltraLogic框架通过大规模数据生成和双极浮点奖励机制，有效提升了大模型在通用逻辑推理方面的性能。"}}
{"id": "2601.03204", "pdf": "https://arxiv.org/pdf/2601.03204", "abs": "https://arxiv.org/abs/2601.03204", "authors": ["Chenglin Yu", "Yuchen Wang", "Songmiao Wang", "Hongxia Yang", "Ming Li"], "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent", "AI": {"tldr": "本文提出了InfiAgent，一个适用于长时任务的通用框架。", "motivation": "LLM代理在处理长时间任务时会因上下文增长和累积错误而失效。现有解决方案如上下文压缩或检索增强提示存在信息保真与稳定性之间的权衡问题。", "method": "通过将持久状态外部化为基于文件的状态抽象，InfiAgent能够在每个步骤中从工作空间状态快照及最近行动的固定窗口重建上下文。", "result": "实验表明，在不进行特定任务微调的情况下，20B开源模型支持的InfiAgent在长时间任务覆盖方面显著优于上下文中心基线。", "conclusion": "研究结果证明了将状态外部化作为长期稳定代理的基础是可行且有效的。"}}
{"id": "2601.03203", "pdf": "https://arxiv.org/pdf/2601.03203", "abs": "https://arxiv.org/abs/2601.03203", "authors": ["Davi Valério", "Chrysoula Zerva", "Mariana Pinto", "Ricardo Santos", "André Carreiro"], "title": "Counterfactual Fairness with Graph Uncertainty", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "Peer reviewed pre-print. Presented at the BIAS 2025 Workshop at ECML PKDD", "summary": "Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorporates the uncertainty of specifying a causal graph into CF. CF-GU (i) bootstraps a Causal Discovery algorithm under domain knowledge constraints to produce a bag of plausible Directed Acyclic Graphs (DAGs), (ii) quantifies graph uncertainty with the normalized Shannon entropy, and (iii) provides confidence bounds on CF metrics. Experiments on synthetic data show how contrasting domain knowledge assumptions support or refute audits of CF, while experiments on real-world data (COMPAS and Adult datasets) pinpoint well-known biases with high confidence, even when supplied with minimal domain knowledge constraints.", "AI": {"tldr": "本文提出了带有图不确定性（Graph Uncertainty）的反事实公平性评估方法，以解决在真实场景中因果图不确定的问题。", "motivation": "评估机器学习模型中的偏见对于构建可信和稳健的系统至关重要。然而，现有的反事实公平性审计依赖于单个确定性的因果图，在现实世界应用中难以实现。", "method": "该方法通过引导一个在领域知识约束下的因果发现算法来生成一组可能的方向无环图（DAG），并使用归一化香农熵量化不确定性。进而提供反事实公平度量的信心界限。", "result": "实验表明，基于合成数据和真实世界数据（如COMPAS和Adult数据集）的结果支持或反驳了反事实公平性审计，并且即使在最小的领域知识限制下也能准确地识别已知偏见。", "conclusion": "该研究通过引入图不确定性扩展了反事实公平性的评估，增强了机器学习模型偏差评估的可信度。"}}
{"id": "2601.03201", "pdf": "https://arxiv.org/pdf/2601.03201", "abs": "https://arxiv.org/abs/2601.03201", "authors": ["Martin Grohe", "Christoph Standke", "Juno Steegmans", "Jan Van den Bussche"], "title": "Recursive querying of neural networks via weighted structures", "categories": ["cs.LO", "cs.AI", "cs.DB"], "comment": null, "summary": "Expressive querying of machine learning models - viewed as a form of intentional data - enables their verification and interpretation using declarative languages, thereby making learned representations of data more accessible. Motivated by the querying of feedforward neural networks, we investigate logics for weighted structures. In the absence of a bound on neural network depth, such logics must incorporate recursion; thereto we revisit the functional fixpoint mechanism proposed by Grädel and Gurevich. We adopt it in a Datalog-like syntax; we extend normal forms for fixpoint logics to weighted structures; and show an equivalent \"loose\" fixpoint mechanism that allows values of inductively defined weight functions to be overwritten. We propose a \"scalar\" restriction of functional fixpoint logic, of polynomial-time data complexity, and show it can express all PTIME model-agnostic queries over reduced networks with polynomially bounded weights. In contrast, we show that very simple model-agnostic queries are already NP-complete. Finally, we consider transformations of weighted structures by iterated transductions.", "AI": {"tldr": "递归查询神经网络的加权结构，提出一种用于处理无深度限制的神经网络逻辑。", "motivation": "通过声明性语言查询机器学习模型可以验证和解释它们，使数据表示更容易访问。为了能够查询前馈神经网络，研究了加权结构上的逻辑。", "method": "采用类似于Datalog的语言重新审视Grädel和Gurevich提出的函数不动点机制；将正常形式扩展到加权结构，并展示了一种可以覆盖递归定义的权重函数值的松动的不动点机制。提出了一个多项式时间数据复杂性的“标量”限制，能够表达关于具有多项式有界权重的简化网络的所有PTIME模型无关查询。", "result": "提出的一种“标量”限制功能固定点逻辑可以表达所有与模型无关的PTIME查询；简单模型无关查询已经是NP完全问题。最后，考虑通过迭代转换变换加权结构。", "conclusion": "这种方法提供了一种处理递归查询神经网络的有效方式，并展示了其在多项式时间内的数据复杂性。"}}
{"id": "2601.03200", "pdf": "https://arxiv.org/pdf/2601.03200", "abs": "https://arxiv.org/abs/2601.03200", "authors": ["Ziyang Sun", "Lingfan Bao", "Tianhu Peng", "Jingcheng Sun", "Chengxu Zhou"], "title": "A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting", "categories": ["cs.RO"], "comment": "Under review of Journal of Robot Learning", "summary": "Developing high-fidelity, interactive digital twins is crucial for enabling closed-loop motion planning and reliable real-world robot execution, which are essential to advancing sim-to-real transfer. However, existing approaches often suffer from slow reconstruction, limited visual fidelity, and difficulties in converting photorealistic models into planning-ready collision geometry. We present a practical framework that constructs high-quality digital twins within minutes from sparse RGB inputs. Our system employs 3D Gaussian Splatting (3DGS) for fast, photorealistic reconstruction as a unified scene representation. We enhance 3DGS with visibility-aware semantic fusion for accurate 3D labelling and introduce an efficient, filter-based geometry conversion method to produce collision-ready models seamlessly integrated with a Unity-ROS2-MoveIt physics engine. In experiments with a Franka Emika Panda robot performing pick-and-place tasks, we demonstrate that this enhanced geometric accuracy effectively supports robust manipulation in real-world trials. These results demonstrate that 3DGS-based digital twins, enriched with semantic and geometric consistency, offer a fast, reliable, and scalable path from perception to manipulation in unstructured environments.", "AI": {"tldr": "基于三维高斯点云的数字孪生技术，实现快速、高质量重建与机器人仿真。", "motivation": "现有的方法在重构速度、视觉保真度和从照片级真实模型转换为规划几何方面存在不足。为此开发了高效框架，用于创建基于稀疏RGB输入的高质量数字孪生。", "method": "采用3D高斯点云（3DGS）实现快速、逼真的重建，并通过视图感知语义融合增强精确的三维标签；引入一种高效的过滤器基几何转换方法以生成适用于Unity-ROS2-MoveIt物理引擎中的碰撞准备模型。", "result": "实验证明，改进的几何精度有效支持真实世界中的鲁棒操作。数字孪生技术提供了一条从感知到操纵的快速、可靠和可扩展路径。", "conclusion": "基于三维高斯点云的增强型数字孪生技术可以提高机器人在复杂环境下的仿真能力及实际操作性能，具有广泛的应用前景。"}}
{"id": "2601.03199", "pdf": "https://arxiv.org/pdf/2601.03199", "abs": "https://arxiv.org/abs/2601.03199", "authors": ["Yang Li", "Han Meng", "Chenan Wang", "Haipeng Chen"], "title": "DIP: Dynamic In-Context Planner For Diffusion Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages", "summary": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows \\textit{efficient dynamic adjustment of the context} during generation. Building on this insight, we propose \\textbf{D}ynamic \\textbf{I}n-Context \\textbf{P}lanner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront. Results show DIP maintains generation quality while achieving up to 12.9$\\times$ inference speedup over standard inference and 1.17$\\times$ over KV cache-enhanced inference.", "AI": {"tldr": "动态上下文规划器DIP通过在生成过程中动态选择和插入上下文示例，优化了扩散语言模型的效率。", "motivation": "由于双向注意力机制导致计算成本随着上下文长度增加而大幅上升，因此提出了动态上下文规划方法以减少推理时间并保持生成质量。", "method": "DIP通过在生成过程中根据需要选择和插入最相关的示例来优化上下文，而不是一次性提供所有示例。这种方法基于扩散语言模型允许在生成过程中高效调整上下文的发现。", "result": "实验结果表明，DIP能够在保持生成质量的同时实现高达12.9倍的标准推理加速以及1.17倍的KV缓存增强推理加速。", "conclusion": "通过动态选择和插入最相关的示例，DIP成功地减少了扩散语言模型的推理时间并提高了效率。"}}
{"id": "2601.03193", "pdf": "https://arxiv.org/pdf/2601.03193", "abs": "https://arxiv.org/abs/2601.03193", "authors": ["Ruiyan Han", "Zhen Fang", "XinYu Sun", "Yuchen Ma", "Ziheng Wang", "Yu Zeng", "Zehui Chen", "Lin Chen", "Wenxuan Huang", "Wei-Jie Xu", "Yi Cao", "Feng Zhao"], "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.", "AI": {"tldr": "UniCorn是一个自改进的统一多模态模型，通过自我生成监督来提高图像到文本（T2I）生成的质量。", "motivation": "现有的统一多模态模型在跨模式理解方面取得了显著的成功，但在利用这些内部知识进行高质量生成方面仍存在差距。为了解决这个问题，提出了UniCorn框架以消除对外部数据或教师监督的需求，增强生成任务的表现。", "method": "将单一的UMM划分为三个协作角色：Proposer、Solver和Judge通过自玩游戏并使用认知模式重构来提取潜在的理解，并将其转化为明确的生成信号。", "result": "实验表明，UniCorn在六个通用图像生成基准测试中实现了对基础模型的重大改进。特别是在TIIF、DPG、CompBench和UniCycle上取得了SOTA性能，在WISE和OneIG上的表现分别提高了+5.0和+6.5。", "conclusion": "该方法显著增强了T2I生成的质量，同时保持了强大的理解能力，证明了完全自监督精炼在统一多模态智能中的可扩展性。"}}
{"id": "2601.03191", "pdf": "https://arxiv.org/pdf/2601.03191", "abs": "https://arxiv.org/abs/2601.03191", "authors": ["Anees Ur Rehman Hashmi", "Numan Saeed", "Christoph Lippert"], "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix", "AI": {"tldr": "AnatomiX是一个为胸部X光解读设计的解剖学基础的多模态大型语言模型。", "motivation": "现有的多模态医学大型语言模型在空间推理和解剖理解上仍存在挑战，未能建立真正的解剖对应关系。为此引入了AnatomiX以解决这些缺陷。", "method": "AnatomiX采用两阶段方法：首先识别解剖结构并提取特征，然后利用大型语言模型进行多种下游任务如短语定位、报告生成等。", "result": "实验表明，相比现有技术方案，AnatomiX在解剖定位等多个任务上性能提高了超过25%。", "conclusion": "AnatomiX在解决胸部X光解读中的空间推理和解剖理解问题方面表现出色。"}}
{"id": "2601.03184", "pdf": "https://arxiv.org/pdf/2601.03184", "abs": "https://arxiv.org/abs/2601.03184", "authors": ["Stepan Maschan", "Haoxuan Qu", "Jun Liu"], "title": "Decentralized Autoregressive Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Work in progress", "summary": "We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.", "AI": {"tldr": "本文提出了去中心化自回归生成的理论分析，并通过实验验证了在多模态语言模型中，去中心化和集中式训练设置之间的等价性。", "motivation": "动机在于探索去中心化的自回归生成方法的有效性和性能表现，在不同的基准测试上与集中式的训练方式进行对比。", "method": "定义了分散的离散流匹配目标，并通过将概率生成速度表示为专家流动性的线性组合来进行理论分析。实验中比较了LLaVA和InternVL两种模式，使用固定CLIP视觉编码器进行全参数微调。", "result": "实验结果表明，在多模态语言模型上，去中心化训练与集中式训练在各种基准测试中的表现相当。", "conclusion": "研究证明了去中心化的自回归生成方法在理论上和实践中都具有可行性，并且能够在不同的设置下提供一致的性能。"}}
{"id": "2601.03181", "pdf": "https://arxiv.org/pdf/2601.03181", "abs": "https://arxiv.org/abs/2601.03181", "authors": ["Han Zhang", "Mohammad Farzanullah", "Mohammad Ghassemi", "Akram Bin Sediq", "Ali Afana", "Melike Erol-Kantarci"], "title": "Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.CV"], "comment": "5 figures, 7 tables, IEEE COMST", "summary": "Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of FMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless network management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.", "AI": {"tldr": "本文综述了多模态基础模型在无线网络预测和控制任务中的应用。", "motivation": "为了实现通用的人工智能代理来处理多样化的网络管理请求以及复杂的无线相关任务，作者探讨了将基础模型应用于无线网络的潜力。", "method": "文章讨论了基础模型在理解多模态上下文信息方面的优势，并解释了它们如何被用于预测和控制任务。此外，还介绍了针对特定无线应用的数据集开发及方法论。", "result": "通过整合多模态数据，可以提高网络管理的效率和准确性。", "conclusion": "尽管存在挑战，但基础模型在无线网络中的应用前景广阔，并为未来的研发指明了方向。"}}
{"id": "2601.03178", "pdf": "https://arxiv.org/pdf/2601.03178", "abs": "https://arxiv.org/abs/2601.03178", "authors": ["Jiajun jiao", "Haowei Zhu", "Puyuan Yang", "Jianghui Wang", "Ji Liu", "Ziqiong Liu", "Dong Li", "Yuejian Fang", "Junhai Yong", "Bin Wang", "Emad Barsoum"], "title": "DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.", "AI": {"tldr": "该论文提出了一个大型语言模型驱动的自动化扩散加速代码生成和评估框架，旨在解决扩散模型在实际部署中的计算开销问题。", "motivation": "扩散模型因多步推理过程导致了显著的计算负担，阻碍其实际应用。如何结合多种加速技术成为挑战。", "method": "论文介绍了DiffBench基准测试工具，以及基于遗传算法和闭环工作流程设计的DiffAgent代理。DiffAgent通过规划组件、调试组件与代码生成组件之间的迭代优化来实现最优加速策略生成。", "result": "实验表明，DiffBench能够全面评估生成代码，并且DiffAgent在生产有效扩散加速策略上显著优于现有大型语言模型。", "conclusion": "该框架提供了一种自动化和高效的解决方案以促进扩散模型的广泛应用。"}}
{"id": "2601.03173", "pdf": "https://arxiv.org/pdf/2601.03173", "abs": "https://arxiv.org/abs/2601.03173", "authors": ["Sumit S. Shevtekar", "Chandresh K. Maurya", "Gourab Sil", "Subasish Das"], "title": "Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions", "categories": ["cs.LG", "cs.HC"], "comment": "13 pages, 8 figures", "summary": "Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.", "AI": {"tldr": "本文提出了一个用于预测摩托车手时间压力的大规模数据集和深度学习模型，以支持智能交通系统的主动安全干预。", "motivation": "时间压力对摩托车手的风险行为和事故发生率有重要影响，但其预测在智能交通系统中尚未充分研究。因此开发了一个大规模的数据集与深度学习模型来解决这个问题。", "method": "提出了一种名为MotoTimePressure的深度学习模型，结合卷积预处理、双阶段时间注意力机制以及Squeeze-and-Excitation特征重标定技术。", "result": "该模型在预测摩托车手的时间压力方面表现出色，准确率达到91.53%，ROC AUC达到98.93%，显著优于八个基准方法。利用此模型提升碰撞风险预测准确性至93.51%。", "conclusion": "时间压力可以作为智能交通系统中主动安全干预的依据，帮助识别骑手的认知负担，并提供适应性警报、振动反馈等多种干预手段以支持摩托车的安全行驶"}}
{"id": "2601.03171", "pdf": "https://arxiv.org/pdf/2601.03171", "abs": "https://arxiv.org/abs/2601.03171", "authors": ["Silvano Cortesi", "Lukas Schulthess", "Davide Plozza", "Christian Vogt", "Michele Magno"], "title": "Eco-WakeLoc: An Energy-Neutral and Cooperative UWB Real-Time Locating System", "categories": ["cs.NI", "cs.ET", "eess.SP"], "comment": "This work has been accepted for publication in the IEEE Sensors Journal, specifically the Special Issue on \"Special Issue on Advances in Resource-Efficient Sensors and Interfaces Fostered by Artificial Intelligence\"", "summary": "Indoor localization systems face a fundamental trade-off between efficiency and responsiveness, which is especially important for emerging use cases such as mobile robots operating in GPS-denied environments. Traditional RTLS either require continuously powered infrastructure, limiting their scalability, or are limited by their responsiveness. This work presents Eco-WakeLoc, designed to achieve centimeter-level UWB localization while remaining energy-neutral by combining ultra-low power wake-up radios (WuRs) with solar energy harvesting. By activating anchor nodes only on demand, the proposed system eliminates constant energy consumption while achieving centimeter-level positioning accuracy. To reduce coordination overhead and improve scalability, Eco-WakeLoc employs cooperative localization where active tags initiate ranging exchanges (trilateration), while passive tags opportunistically reuse these messages for TDOA positioning. An additive-increase/multiplicative-decrease (AIMD)-based energy-aware scheduler adapts localization rates according to the harvested energy, thereby maximizing the overall performance of the sensor network while ensuring long-term energy neutrality. The measured energy consumption is only 3.22mJ per localization for active tags, 951uJ for passive tags, and 353uJ for anchors. Real-world deployment on a quadruped robot with nine anchors confirms the practical feasibility, achieving an average accuracy of 43cm in dynamic indoor environments. Year-long simulations show that tags achieve an average of 2031 localizations per day, retaining over 7% battery capacity after one year -- demonstrating that the RTLS achieves sustained energy-neutral operation. Eco-WakeLoc demonstrates that high-accuracy indoor localization can be achieved at scale without continuous infrastructure operation, combining energy neutrality, cooperative positioning, and adaptive scheduling.", "AI": {"tldr": "本文提出了Eco-WakeLoc系统，旨在实现厘米级的超宽带（UWB）定位同时保持能量中性。", "motivation": "传统实时定位系统（RTLS）在效率和响应速度之间存在权衡问题，尤其是在GPS缺失环境下工作的移动机器人等新兴应用领域。该论文通过结合低功耗唤醒无线电（WuRs）与太阳能采集技术，设计了一种能够在需要时激活锚节点的系统，以消除持续能量消耗。", "method": "Eco-WakeLoc使用超低功耗唤醒无线电和太阳能源收集来实现厘米级定位，并在被动标签中采用机会性时间差定位。为了减少协调负担并提高可扩展性，该系统采用了协作定位策略：主动标签发起测距交换（三边测量），而被动标签利用这些消息进行时差到达（TDOA）定位。", "result": "实验证明，在动态室内环境中，九个锚点部署在四足机器人上的实际测试实现了平均43cm的精度。一年仿真显示，每个标签每天可以执行大约2031次定位，并且电池容量还能保持7%以上，从而表明系统能够实现可持续的能量中性操作。", "conclusion": "Eco-WakeLoc证明了高精度室内定位可以在不依赖持续基础设施运行的情况下大规模实现，结合能量中性、协作定位和自适应调度。"}}
{"id": "2601.03170", "pdf": "https://arxiv.org/pdf/2601.03170", "abs": "https://arxiv.org/abs/2601.03170", "authors": ["Qifan Liang", "Yuansen Liu", "Ruixin Wei", "Nan Lu", "Junchuan Zhao", "Ye Wang"], "title": "Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech", "categories": ["cs.SD"], "comment": "24 pages, 8 figures, 7 tables, 3 lists", "summary": "While controllable Text-to-Speech (TTS) has achieved notable progress, most existing methods remain limited to inter-utterance-level control, making fine-grained intra-utterance expression challenging due to their reliance on non-public datasets or complex multi-stage training. In this paper, we propose a training-free controllable framework for pretrained zero-shot TTS to enable intra-utterance emotion and duration expression. Specifically, we propose a segment-aware emotion conditioning strategy that combines causal masking with monotonic stream alignment filtering to isolate emotion conditioning and schedule mask transitions, enabling smooth intra-utterance emotion shifts while preserving global semantic coherence. Based on this, we further propose a segment-aware duration steering strategy to combine local duration embedding steering with global EOS logit modulation, allowing local duration adjustment while ensuring globally consistent termination. To eliminate the need for segment-level manual prompt engineering, we construct a 30,000-sample multi-emotion and duration-annotated text dataset to enable LLM-based automatic prompt construction. Extensive experiments demonstrate that our training-free method not only achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control, but also maintains baseline-level speech quality of the underlying TTS model. Audio samples are available at https://aclanonymous111.github.io/TED-TTS-DemoPage/.", "AI": {"tldr": "提出了一种无需训练的框架，用于预训练零样本TTS中的段落感知情感和持续时间控制", "motivation": "现有的大多数可控语音合成方法局限于句子间的情感表达，难以实现句内的细粒度情感变化，并且依赖于非公开数据集或复杂的多阶段训练。本研究旨在开发一种无需训练的方法来实现句内情感和持续时间的调整，以提高情感自然性和一致性", "method": "提出了一种结合因果掩码和单调流对齐滤波器的情感条件策略以及局部持续时间嵌入导向与全局EOS概率调节相结合的持续时间控制策略。使用LLM自动构建段落感知提示工程，无需人工标注", "result": "实验表明该方法实现了最先进的句内一致性，在多情感和持续时间控制方面表现出色，并保持了基础TTS模型的基线水平语音质量", "conclusion": "所提出的方法为实现无需训练的情感和持续时间调整提供了一种有效途径，提高了可控语音合成的质量"}}
{"id": "2601.03163", "pdf": "https://arxiv.org/pdf/2601.03163", "abs": "https://arxiv.org/abs/2601.03163", "authors": ["Matěj Pekár", "Vít Musil", "Rudolf Nenutil", "Petr Holub", "Tomáš Brázdil"], "title": "LSP-DETR: Efficient and Scalable Nuclei Segmentation in Whole Slide Images", "categories": ["cs.CV"], "comment": null, "summary": "Precise and scalable instance segmentation of cell nuclei is essential for computational pathology, yet gigapixel Whole-Slide Images pose major computational challenges. Existing approaches rely on patch-based processing and costly post-processing for instance separation, sacrificing context and efficiency. We introduce LSP-DETR (Local Star Polygon DEtection TRansformer), a fully end-to-end framework that uses a lightweight transformer with linear complexity to process substantially larger images without additional computational cost. Nuclei are represented as star-convex polygons, and a novel radial distance loss function allows the segmentation of overlapping nuclei to emerge naturally, without requiring explicit overlap annotations or handcrafted post-processing. Evaluations on PanNuke and MoNuSeg show strong generalization across tissues and state-of-the-art efficiency, with LSP-DETR being over five times faster than the next-fastest leading method. Code and models are available at https://github.com/RationAI/lsp-detr.", "AI": {"tldr": "LSP-DETR是一种用于细胞核分割的高效且可扩展的方法，适用于全滑片图像。", "motivation": "当前方法依赖于基于补丁处理和昂贵的后处理步骤来实现实例分离，导致计算效率低下。提出一种新的端到端框架以解决此问题，并提高整体效率。", "method": "LSP-DETR利用轻量级转换器并采用线性复杂度进行图像处理。该方法将细胞核表示为星凸多边形并通过新颖的径向距离损失函数自然地分割重叠的细胞核，无需显式重叠注释或手动后处理。", "result": "在PanNuke和MoNuSeg数据集上的评估显示了LSP-DETR具有强大的跨组织泛化能力和最先进的效率。与速度最快的现有方法相比，其速度快五倍以上。", "conclusion": "LSP-DETR提供了一种高效且可扩展的细胞核分割解决方案，在全滑片图像上实现了卓越的效果和效率提升"}}
{"id": "2601.03159", "pdf": "https://arxiv.org/pdf/2601.03159", "abs": "https://arxiv.org/abs/2601.03159", "authors": ["Wadie Skaf", "Felix Kern", "Aryamaan Basu Roy", "Tejas Pradhan", "Roman Kalkreuth", "Holger Hoos"], "title": "Rapid Augmentations for Time Series (RATS): A High-Performance Library for Time Series Augmentation", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Time series augmentation is critical for training robust deep learning models, particularly in domains where labelled data is scarce and expensive to obtain. However, existing augmentation libraries for time series, mainly written in Python, suffer from performance bottlenecks, where running time grows exponentially as dataset sizes increase -- an aspect limiting their applicability in large-scale, production-grade systems. We introduce RATS (Rapid Augmentations for Time Series), a high-performance library for time series augmentation written in Rust with Python bindings (RATSpy). RATS implements multiple augmentation methods spanning basic transformations, frequency-domain operations and time warping techniques, all accessible through a unified pipeline interface with built-in parallelisation. Comprehensive benchmarking of RATSpy versus a commonly used library (tasug) on 143 datasets demonstrates that RATSpy achieves an average speedup of 74.5\\% over tsaug (up to 94.8\\% on large datasets), with up to 47.9\\% less peak memory usage.", "AI": {"tldr": "RATS 是一个用于时间序列增强的高性能库，用 Rust 编写并带有 Python 绑定。", "motivation": "现有的时间序列增强库由于性能瓶颈限制了其在大规模生产系统中的应用。", "method": "RATS 实现了多种增强方法，并通过统一的管道接口提供了内置的并行化支持。", "result": "与常用的 tsaug 库相比，RATSpy 在143个数据集上平均实现了74.5%的速度提升（最大可达94.8%），同时内存使用降低了最多47.9%。", "conclusion": "RATS 提供了一种高效的时间序列增强解决方案，适用于大规模生产系统。"}}
{"id": "2601.03156", "pdf": "https://arxiv.org/pdf/2601.03156", "abs": "https://arxiv.org/abs/2601.03156", "authors": ["Sofie Goethals", "Foster Provost", "João Sedoc"], "title": "Prompt-Counterfactual Explanations for Generative AI System Behavior", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -- the prompt -- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.", "AI": {"tldr": "本文研究了一种生成性AI系统行为解释的新方法：提示反事实解释（PCE），用于理解输入提示如何影响输出特征。", "motivation": "随着生成性AI系统的广泛应用，理解和解释其行为变得至关重要。决策者需要知道是什么因素导致这些系统产生特定的输出特性，如毒性、负面情绪或政治偏见。", "method": "本文提出了一种灵活的框架，该框架将反事实解释应用于非确定性和生成性的AI系统，并基于此框架引入了用于生成提示反事实解释（PCE）的算法。", "result": "通过三个案例研究展示了如何使用PCE来分析不同输出特性，包括政治倾向、毒性和情绪。这些案例还显示了PCE在抑制不良输出特性和增强红队行动中发现引发不良输出的新提示方面的作用。", "conclusion": "这项工作为生成性AI系统中的提示解释奠定了基础，并且随着模型承担更高风险的任务并面临透明度和问责制的监管要求，这种能力将变得不可或缺。"}}
{"id": "2601.03144", "pdf": "https://arxiv.org/pdf/2601.03144", "abs": "https://arxiv.org/abs/2601.03144", "authors": ["Andrew Shin"], "title": "Self-Verification is All You Need To Pass The Japanese Bar Examination", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/shinandrew/self_verification", "summary": "Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.", "AI": {"tldr": "本文介绍了一种新的自我验证模型，该模型能够通过未修改的日本司法考试。", "motivation": "尽管大语言模型在法律推理方面有所进展，但要在高度专业化和结构化的考试中实现可靠的表现仍具挑战。现有的方法未能系统地评估原始考试格式下的表现，因此本文旨在创建一个能够在不改变问题结构或评分规则的情况下通过日本司法考试的模型。", "method": "作者构建了一个新的训练集，并采用自我验证模型进行训练，该模型能够准确复制并适应日本司法考试的形式和评价标准。此外，还进行了与多代理推理及基于分解监督策略方法的比较实验。", "result": "所提出的自我验证模型在实际考试评分中超过了官方通过分数线，而其他复杂系统如多代理推理或基于分解的方法未能达到同等水平的表现。", "conclusion": "研究结果表明，在专业性推理任务中，格式忠实的监督和一致性检验至关重要，并且精心设计的单一模型方法可以胜过更复杂的系统。"}}
{"id": "2601.03136", "pdf": "https://arxiv.org/pdf/2601.03136", "abs": "https://arxiv.org/abs/2601.03136", "authors": ["Selma Wanna", "Agnes Luhtaru", "Jonathan Salfity", "Ryan Barron", "Juston Moore", "Cynthia Matuszek", "Mitch Pryor"], "title": "Limited Linguistic Diversity in Embodied AI Datasets", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": null, "summary": "Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.", "AI": {"tldr": "对几个常用的Vision-Language-Action（VLA）语料库进行了系统的数据集审计，以描述这些数据集中包含的指令类型及其提供的语言多样性。", "motivation": "当前用于训练和评估VLA模型的数据集中语言特征尚未得到充分研究，需要对其进行详细的文档化以便于更好地了解语言信号，并为更合理的数据集选择和支持针对性的语言覆盖增强策略提供支持。", "method": "通过量化指令语言的不同方面（如词汇多样性、重复性、语义相似性和句法复杂度）来分析不同VLA数据集中的语言特征。", "result": "许多数据集依赖于高度重复的模板指令，导致指令形式分布狭窄。", "conclusion": "这些发现是对当前VLA训练和评估数据中语言信号的描述性文档，旨在支持更详细的报告、更合理的数据选择以及针对性的增强策略来扩展语言覆盖范围。"}}
{"id": "2601.03130", "pdf": "https://arxiv.org/pdf/2601.03130", "abs": "https://arxiv.org/abs/2601.03130", "authors": ["Faisal Chowdhury", "Nandana Mihindukulasooriya", "Niharika S D'Souza", "Horst Samulowitz", "Neeru Gupta", "Tomasz Hanusiak", "Michal Kapitonow"], "title": "Automatic Prompt Engineering with No Task Cues and No Tuning", "categories": ["cs.AI", "cs.CL"], "comment": "ef:The IEEE International Conference on Data Mining (ICDM) 2025 : Demo Track", "summary": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.", "AI": {"tldr": "本文提出了一种自动提示工程系统，该系统在设计和应用上更加简单，并且无需调整或明确的任务线索就能达到与现有方法相当的效果。", "motivation": "目前针对数据库表格中隐含列名扩展（CNE）任务的研究很少。因此，作者希望通过开发一种新型的自动提示工程技术来改善这个问题。", "method": "该系统是一种自动提示工程方法，它不依赖于任何特定的任务线索或调整步骤，在英语和德语的数据集上进行了实验。", "result": "在隐含列名扩展任务中，这种方法展示了与现有技术相当的效果。此外，这是首次将自动提示工程技术应用于非英语语言的研究工作。", "conclusion": "该方法表明了通过简单的自动提示工程可以有效解决复杂数据处理问题的可能性，并为其他领域的应用提供了新的思路。"}}
{"id": "2601.03129", "pdf": "https://arxiv.org/pdf/2601.03129", "abs": "https://arxiv.org/abs/2601.03129", "authors": ["Matthias Bentert", "Tom-Lukas Breitkopf", "Vincent Froese", "Anton Herrmann", "André Nichterlein"], "title": "Density Matters: A Complexity Dichotomy of Deleting Edges to Bound Subgraph Density", "categories": ["cs.DS", "cs.DM"], "comment": "to appear at STACS 2026", "summary": "We study $τ$-Bounded-Density Edge Deletion ($τ$-BDED), where given an undirected graph $G$, the task is to remove as few edges as possible to obtain a graph $G'$ where no subgraph of $G'$ has density more than $τ$. The density of a (sub)graph is the number of edges divided by the number of vertices. This problem was recently introduced and shown to be NP-hard for $τ\\in \\{2/3, 3/4, 1 + 1/25\\}$, but polynomial-time solvable for $τ\\in \\{0,1/2,1\\}$ [Bazgan et al., JCSS 2025]. We provide a complete dichotomy with respect to the target density $τ$: 1. If $2τ\\in \\mathbb{N}$ (half-integral target density) or $τ< 2/3$, then $τ$-BDED is polynomial-time solvable. 2. Otherwise, $τ$-BDED is NP-hard. We complement the NP-hardness with fixed-parameter tractability with respect to the treewidth of $G$. Moreover, for integral target density $τ\\in \\mathbb{N}$, we show $τ$-BDED to be solvable in randomized $O(m^{1 + o(1)})$ time. Our algorithmic results are based on a reduction to a new general flow problem on restricted networks that, depending on $τ$, can be solved via Maximum s-t-Flow or General Factors. We believe this connection between these variants of flow and matching to be of independent interest.", "AI": {"tldr": "本文研究了删除边以将子图密度限制在τ之下的问题，并提供了关于目标密度τ的完全分类。", "motivation": "探讨如何通过最小化删除的边数来控制图中所有子图的密度，进而解决其复杂度分类问题。", "method": "利用算法技术对问题进行归约到新的流网络问题上，并根据不同的τ值选择合适的方法求解。", "result": "证明当2τ为半整数或τ小于2/3时，该问题是多项式时间可解的；否则，它是NP难的。此外，在整数目标密度下提供了一个随机算法以解决该问题。", "conclusion": "确立了关于删除边限制子图密度问题复杂度分类的完全结果，并提出了一种新的流网络模型和相关算法技术。"}}
{"id": "2601.03127", "pdf": "https://arxiv.org/pdf/2601.03127", "abs": "https://arxiv.org/abs/2601.03127", "authors": ["Sashuai Zhou", "Qiang Zhou", "Jijin Hu", "Hanqing Yang", "Yue Cao", "Junpeng Ma", "Yinchao Ma", "Jun Song", "Tiezheng Ge", "Cheng Yu", "Bo Zheng", "Zhou Zhao"], "title": "Unified Thinker: A General Reasoning Modular Core for Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite impressive progress in high-fidelity image synthesis, generative models still struggle with logic-intensive instruction following, exposing a persistent reasoning--execution gap. Meanwhile, closed-source systems (e.g., Nano Banana) have demonstrated strong reasoning-driven image generation, highlighting a substantial gap to current open-source models. We argue that closing this gap requires not merely better visual generators, but executable reasoning: decomposing high-level intents into grounded, verifiable plans that directly steer the generative process. To this end, we propose Unified Thinker, a task-agnostic reasoning architecture for general image generation, designed as a unified planning core that can plug into diverse generators and workflows. Unified Thinker decouples a dedicated Thinker from the image Generator, enabling modular upgrades of reasoning without retraining the entire generative model. We further introduce a two-stage training paradigm: we first build a structured planning interface for the Thinker, then apply reinforcement learning to ground its policy in pixel-level feedback, encouraging plans that optimize visual correctness over textual plausibility. Extensive experiments on text-to-image generation and image editing show that Unified Thinker substantially improves image reasoning and generation quality.", "AI": {"tldr": "Unified Thinker是一款用于图像生成的通用推理模块核心，旨在解决当前图像生成模型在逻辑密集型指令跟随方面的不足。", "motivation": "目前的生成模型在高保真度图像合成方面取得了显著进展，但在执行复杂推理任务时仍存在困难。为缩小这一差距，需要开发能够将高级意图分解为可验证计划，并直接指导生成过程的技术。", "method": "提出了一种通用的推理架构Unified Thinker，它作为一个独立于任务的规划核心工作，可以插入不同的图像生成器和工作流程中。通过强化学习训练Thinker模块以适应像素级反馈，提高视觉正确性。", "result": "实验结果显示，Unified Thinker在文本到图像生成和图像编辑方面的表现优于现有模型，特别是在提高图像推理质量方面取得了显著进步。", "conclusion": "研究证明了利用通用推理架构可以有效提升图像生成的质量，特别是解决复杂的逻辑密集型指令跟随问题。"}}
{"id": "2601.03124", "pdf": "https://arxiv.org/pdf/2601.03124", "abs": "https://arxiv.org/abs/2601.03124", "authors": ["B. M. Shahria Alam", "Md. Nasim Ahmed"], "title": "LeafLife: An Explainable Deep Learning Framework with Robustness for Grape Leaf Disease Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 8 figures, 2025 IEEE International Conference on Signal Processing, Information, Communication and Systems (SPICSCON)", "summary": "Plant disease diagnosis is essential to farmers' management choices because plant diseases frequently lower crop yield and product quality. For harvests to flourish and agricultural productivity to boost, grape leaf disease detection is important. The plant disease dataset contains grape leaf diseases total of 9,032 images of four classes, among them three classes are leaf diseases, and the other one is healthy leaves. After rigorous pre-processing dataset was split (70% training, 20% validation, 10% testing), and two pre-trained models were deployed: InceptionV3 and Xception. Xception shows a promising result of 96.23% accuracy, which is remarkable than InceptionV3. Adversarial Training is used for robustness, along with more transparency. Grad-CAM is integrated to confirm the leaf disease. Finally deployed a web application using Streamlit with a heatmap visualization and prediction with confidence level for robust grape leaf disease classification.", "AI": {"tldr": "该论文提出了一种可解释的深度学习框架用于识别葡萄叶片疾病，提高了诊断准确性和透明度。", "motivation": "植物病害对农作物产量和质量有重大影响，通过准确的植物病害检测可以提高农业生产力。本文针对葡萄叶病害，利用深度学习技术进行高效精准的识别。", "method": "使用了预处理过的9032张图像数据集（包含四种类别），并采用InceptionV3和Xception模型进行了训练。其中Xception表现更优，达到了96.23%的准确率。同时利用对抗性训练来增加模型鲁棒性，并结合Grad-CAM进行可视化解释。", "result": "实验中Xception模型在葡萄叶片病害识别上获得了96.23%的准确性，优于InceptionV3；此外通过集成Grad-CAM提升了算法透明度和解释力。最终开发了一个包含热图视觉化预测结果及置信水平展示的网络应用。", "conclusion": "该研究提出了一种新的深度学习框架用于葡萄叶病害识别，并展示了在提高准确性和鲁棒性方面的优越性能，为农业病虫害监测提供重要技术支持。"}}
{"id": "2601.03121", "pdf": "https://arxiv.org/pdf/2601.03121", "abs": "https://arxiv.org/abs/2601.03121", "authors": ["Peiran Li", "Jan Fillies", "Adrian Paschke"], "title": "ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to the main conference of EACL 2026", "summary": "Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.", "AI": {"tldr": "提出了一种基于对抗生成网络和大语言模型指导的文本增强框架ToxiGAN，用于改善毒性分类任务中的数据增强。", "motivation": "为了提高毒性分类任务中的鲁棒性，在可控且类别特定的方式下增加有毒语言的数据非常重要。然而，由于有限的监督和支持向量偏差问题，这一目标面临挑战。", "method": "提出了一种基于对抗生成网络和大语言模型指导的文本增强框架ToxiGAN，通过引入两步定向训练策略并利用大语言模型生成中立文本作为语义平衡器来解决模式坍塌和语义漂移等问题。此外，该方法动态选择中立示例以提供平衡引导，并优化有毒样本使其远离这些示例。", "result": "在四个仇恨言论基准测试上进行的实验表明，ToxiGAN在宏F1和仇恨F1两个指标上的平均性能最强，且明显优于传统及基于大语言模型的数据增强方法。进一步的消融实验和敏感性分析也证实了语义平衡器和定向训练在提升分类器鲁棒性方面的优势。", "conclusion": "通过结合对抗生成网络与大语言模型指导，ToxiGAN能够有效解决毒性数据增强中的问题，并显著提高毒性分类任务的表现。"}}
{"id": "2601.03120", "pdf": "https://arxiv.org/pdf/2601.03120", "abs": "https://arxiv.org/abs/2601.03120", "authors": ["Adam Keane", "Nick Pepper", "Chris Burr", "Amy Hodgkin", "Dewi Gould", "John Korna", "Marc Thomas"], "title": "A framework for assuring the accuracy and fidelity of an AI-enabled Digital Twin of en route UK airspace", "categories": ["cs.AI"], "comment": null, "summary": "Digital Twins combine simulation, operational data and Artificial Intelligence (AI), and have the potential to bring significant benefits across the aviation industry. Project Bluebird, an industry-academic collaboration, has developed a probabilistic Digital Twin of en route UK airspace as an environment for training and testing AI Air Traffic Control (ATC) agents. There is a developing regulatory landscape for this kind of novel technology. Regulatory requirements are expected to be application specific, and may need to be tailored to each specific use case. We draw on emerging guidance for both Digital Twin development and the use of Artificial Intelligence/Machine Learning (AI/ML) in Air Traffic Management (ATM) to present an assurance framework. This framework defines actionable goals and the evidence required to demonstrate that a Digital Twin accurately represents its physical counterpart and also provides sufficient functionality across target use cases. It provides a structured approach for researchers to assess, understand and document the strengths and limitations of the Digital Twin, whilst also identifying areas where fidelity could be improved. Furthermore, it serves as a foundation for engagement with stakeholders and regulators, supporting discussions around the regulatory needs for future applications, and contributing to the emerging guidance through a concrete, working example of a Digital Twin. The framework leverages a methodology known as Trustworthy and Ethical Assurance (TEA) to develop an assurance case. An assurance case is a nested set of structured arguments that provides justified evidence for how a top-level goal has been realised. In this paper we provide an overview of each structured argument and a number of deep dives which elaborate in more detail upon particular arguments, including the required evidence, assumptions and justifications.", "AI": {"tldr": "论文提出了一个框架，用于确保AI增强的数字孪生在英国空域中的准确性和真实性。", "motivation": "随着数字化双胞胎和人工智能技术的发展，需要一个新的监管框架来保证这些技术的应用。该框架旨在满足特定需求并证明其准确性与功能适用性。", "method": "论文提出了一种基于可信度论证方法的保证框架，以确保数字孪生准确反映其实体，并提供足够的功能覆盖目标用例。", "result": "通过使用TEA（可靠且道德保证）的方法来构建一个保证案例，该论文概述了每个结构化论点并提供了详细解释。", "conclusion": "提出的新框架为研究人员和监管者提供了评估数字孪生的工具，并支持未来的应用讨论。"}}
{"id": "2601.03117", "pdf": "https://arxiv.org/pdf/2601.03117", "abs": "https://arxiv.org/abs/2601.03117", "authors": ["Lalit Pandey", "Samantha M. W. Wood", "Justin N. Wood"], "title": "Transformers self-organize like newborn visual systems when trained in prenatal worlds", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Do transformers learn like brains? A key challenge in addressing this question is that transformers and brains are trained on fundamentally different data. Brains are initially \"trained\" on prenatal sensory experiences (e.g., retinal waves), whereas transformers are typically trained on large datasets that are not biologically plausible. We reasoned that if transformers learn like brains, then they should develop the same structure as newborn brains when exposed to the same prenatal data. To test this prediction, we simulated prenatal visual input using a retinal wave generator. Then, using self-supervised temporal learning, we trained transformers to adapt to those retinal waves. During training, the transformers spontaneously developed the same structure as newborn visual systems: (1) early layers became sensitive to edges, (2) later layers became sensitive to shapes, and (3) the models developed larger receptive fields across layers. The organization of newborn visual systems emerges spontaneously when transformers adapt to a prenatal visual world. This developmental convergence suggests that brains and transformers learn in common ways and follow the same general fitting principles.", "AI": {"tldr": "通过模拟胎儿期的视觉输入，研究者训练了变压器模型，并观察到其结构类似于新生婴儿的大脑视觉系统。", "motivation": "探讨变压器是否以类似大脑的方式进行学习。为解决这一问题，需要将变压器与大脑在不同数据集上的训练情况进行比较。", "method": "使用视网膜波动生成器模拟胎儿期的视觉输入，并通过自我监督的时间学习方法对变压器模型进行训练。", "result": "经过训练后，变压器模型自发地发展出了类似于新生婴儿视觉系统的特点：早期层变得敏感于边缘，后期层变得敏感于形状；并且各层次的感受野大小也有所增加。", "conclusion": "当变压器适应模拟的胎儿期视觉世界时，新生儿视觉系统的组织结构会自然出现。这表明大脑和变压器在学习方式上存在共同点，并遵循相同的通用拟合原则。"}}
{"id": "2601.03115", "pdf": "https://arxiv.org/pdf/2601.03115", "abs": "https://arxiv.org/abs/2601.03115", "authors": ["Xiutian Zhao", "Björn Schuller", "Berrak Sisman"], "title": "Discovering and Causally Validating Emotion-Sensitive Neurons in Large Audio-Language Models", "categories": ["cs.CL", "eess.AS"], "comment": "16 pages, 6 figures", "summary": "Emotion is a central dimension of spoken communication, yet, we still lack a mechanistic account of how modern large audio-language models (LALMs) encode it internally. We present the first neuron-level interpretability study of emotion-sensitive neurons (ESNs) in LALMs and provide causal evidence that such units exist in Qwen2.5-Omni, Kimi-Audio, and Audio Flamingo 3. Across these three widely used open-source models, we compare frequency-, entropy-, magnitude-, and contrast-based neuron selectors on multiple emotion recognition benchmarks. Using inference-time interventions, we reveal a consistent emotion-specific signature: ablating neurons selected for a given emotion disproportionately degrades recognition of that emotion while largely preserving other classes, whereas gain-based amplification steers predictions toward the target emotion. These effects arise with modest identification data and scale systematically with intervention strength. We further observe that ESNs exhibit non-uniform layer-wise clustering with partial cross-dataset transfer. Taken together, our results offer a causal, neuron-level account of emotion decisions in LALMs and highlight targeted neuron interventions as an actionable handle for controllable affective behaviors.", "AI": {"tldr": "本文通过神经元层面的解释性研究，探索了大型音频语言模型中的情感敏感神经元，并提供了因果证据。", "motivation": "探讨现代大型音频语言模型内部如何编码情绪，为该领域的机制性理解提供基础。", "method": "通过对比频率、熵值、强度和对比度基于的不同神经元选择器，在多个情感识别基准上进行研究。使用推理时间干预来揭示特定的情感签名，并观察到这些效果在不同数据集之间存在部分跨数据集的迁移。", "result": "发现了情绪敏感神经元的存在，并且通过增益或抑制这些神经元，可以改变模型对特定情绪的预测倾向。", "conclusion": "研究提供了一个关于大型音频语言模型中情感决策的因果、神经元层面的解释，并强调了针对神经元干预作为可操作手段来控制情感行为的重要性。"}}
{"id": "2601.03112", "pdf": "https://arxiv.org/pdf/2601.03112", "abs": "https://arxiv.org/abs/2601.03112", "authors": ["Kailin Tan", "Jincheng Dai", "Sixian Wang", "Guo Lu", "Shuo Shao", "Kai Niu", "Wenjun Zhang", "Ping Zhang"], "title": "DiT-JSCC: Rethinking Deep JSCC with Diffusion Transformers and Semantic Representations", "categories": ["eess.IV", "cs.CV"], "comment": "14pages, 14figures, 2tables", "summary": "Generative joint source-channel coding (GJSCC) has emerged as a new Deep JSCC paradigm for achieving high-fidelity and robust image transmission under extreme wireless channel conditions, such as ultra-low bandwidth and low signal-to-noise ratio. Recent studies commonly adopt diffusion models as generative decoders, but they frequently produce visually realistic results with limited semantic consistency. This limitation stems from a fundamental mismatch between reconstruction-oriented JSCC encoders and generative decoders, as the former lack explicit semantic discriminability and fail to provide reliable conditional cues. In this paper, we propose DiT-JSCC, a novel GJSCC backbone that can jointly learn a semantics-prioritized representation encoder and a diffusion transformer (DiT) based generative decoder, our open-source project aims to promote the future research in GJSCC. Specifically, we design a semantics-detail dual-branch encoder that aligns naturally with a coarse-to-fine conditional DiT decoder, prioritizing semantic consistency under extreme channel conditions. Moreover, a training-free adaptive bandwidth allocation strategy inspired by Kolmogorov complexity is introduced to further improve the transmission efficiency, thereby indeed redefining the notion of information value in the era of generative decoding. Extensive experiments demonstrate that DiT-JSCC consistently outperforms existing JSCC methods in both semantic consistency and visual quality, particularly in extreme regimes.", "AI": {"tldr": "提出了一种新的GJSCC框架DiT-JSCC，用于图像在极端无线条件下的高质量传输", "motivation": "现有深度JSCC方法尽管视觉效果真实但缺乏语义一致性。为解决这一问题并提高极低带宽和信噪比环境中的信息传输效率", "method": "设计了一种基于扩散转换器的DiT-JSCC模型，包括语义细节双分支编码器与精细化条件解码器，并引入了训练无关的自适应带宽分配策略", "result": "实验表明，所提出的方法在极低带宽和信噪比条件下超越了现有的JSCC方法，在视觉质量和语义一致性方面均有提升", "conclusion": "DiT-JSCC框架重新定义了生成式解码中的信息价值，并为GJSCC的未来发展提供了新的研究方向"}}
{"id": "2601.03103", "pdf": "https://arxiv.org/pdf/2601.03103", "abs": "https://arxiv.org/abs/2601.03103", "authors": ["Soichiro Murakami", "Hidetaka Kamigaito", "Hiroya Takamura", "Manabu Okumura"], "title": "Who Laughs with Whom? Disentangling Influential Factors in Humor Preferences across User Clusters and LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Humor preferences vary widely across individuals and cultures, complicating the evaluation of humor using large language models (LLMs). In this study, we model heterogeneity in humor preferences in Oogiri, a Japanese creative response game, by clustering users with voting logs and estimating cluster-specific weights over interpretable preference factors using Bradley-Terry-Luce models. We elicit preference judgments from LLMs by prompting them to select the funnier response and found that user clusters exhibit distinct preference patterns and that the LLM results can resemble those of particular clusters. Finally, we demonstrate that, by persona prompting, LLM preferences can be directed toward a specific cluster. The scripts for data collection and analysis will be released to support reproducibility.", "AI": {"tldr": "本文通过聚类分析用户在Oogiri游戏中的投票记录，研究幽默偏好，并使用Bradley-Terry-Luce模型估计各集群的特定权重。同时评估大型语言模型（LLM）对幽默偏好的判断。", "motivation": "幽默偏好因人而异，这使得使用大语言模型来评价幽默变得复杂。本文旨在通过建模用户在Oogiri游戏中的不同幽默偏好来研究这一问题，并探究大型语言模型的幽默偏好是否可以被指导。", "method": "首先对用户进行聚类分析以识别不同的幽默偏好群体；然后利用Bradley-Terry-Luce模型估计每个集群的具体权重；最后通过提示引导LLM模仿特定用户的幽默偏好。", "result": "研究发现不同用户群展现出了独特的幽默偏好模式，大型语言模型的判断结果与某些用户组相符，并且可以通过引导使LLM的偏好向某个特定用户群体靠拢。", "conclusion": "本研究表明，通过聚类分析可以有效地识别和理解不同用户的幽默偏好；此外，利用适当的提示技术也可以调整大语言模型对幽默的理解方向。"}}
{"id": "2601.03100", "pdf": "https://arxiv.org/pdf/2601.03100", "abs": "https://arxiv.org/abs/2601.03100", "authors": ["Chenchen Lin", "Sanbao Su", "Rachel Luo", "Yuxiao Chen", "Yan Wang", "Marco Pavone", "Fei Miao"], "title": "Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) typically rely on a single late-layer feature from a frozen vision encoder, leaving the encoder's rich hierarchy of visual cues under-utilized. MLLMs still suffer from visually ungrounded hallucinations, often relying on language priors rather than image evidence. While many prior mitigation strategies operate on the text side, they leave the visual representation unchanged and do not exploit the rich hierarchy of features encoded across vision layers. Existing multi-layer fusion methods partially address this limitation but remain static, applying the same layer mixture regardless of the query. In this work, we introduce TGIF (Text-Guided Inter-layer Fusion), a lightweight module that treats encoder layers as depth-wise \"experts\" and predicts a prompt-dependent fusion of visual features. TGIF follows the principle of direct external fusion, requires no vision-encoder updates, and adds minimal overhead. Integrated into LLaVA-1.5-7B, TGIF provides consistent improvements across hallucination, OCR, and VQA benchmarks, while preserving or improving performance on ScienceQA, GQA, and MMBench. These results suggest that query-conditioned, hierarchy-aware fusion is an effective way to strengthen visual grounding and reduce hallucination in modern MLLMs.", "AI": {"tldr": "本文介绍了TGIF模块，该模块通过文本引导的多层融合来减少多模态大语言模型中的幻觉问题。", "motivation": "现有的多模态大型语言模型依赖单一视觉编码器的特征，未能充分利用其丰富的层级结构。这导致模型在处理图像时倾向于依赖语言先验而非图像证据，从而引发幻觉。", "method": "TGIF模块将编码层视为深度专家，并根据查询预测视觉特征的融合。它遵循直接外部融合的原则，无需更新视觉编码器并具有较低的计算开销。", "result": "实验结果显示，在LLaVA-1.5-7B中集成TGIF后，模型在幻觉、OCR和VQA基准测试上均有提升，并且在ScienceQA、GQA和MMBenC h上的性能也得到了保持或提高。", "conclusion": "研究结果表明，基于查询条件的层级感知融合是增强现代多模态大型语言模型视觉接地并减少幻觉的有效方法。"}}
{"id": "2601.03098", "pdf": "https://arxiv.org/pdf/2601.03098", "abs": "https://arxiv.org/abs/2601.03098", "authors": ["Meghna Roy Chowdhury", "Shreyas Sen", "Yi Ding"], "title": "From Muscle to Text with MyoText: sEMG to Text via Finger Classification and Transformer-Based Decoding", "categories": ["cs.LG", "cs.NE"], "comment": "25 pages, 11 tables, 11 figures", "summary": "Surface electromyography (sEMG) provides a direct neural interface for decoding muscle activity and offers a promising foundation for keyboard-free text input in wearable and mixed-reality systems. Previous sEMG-to-text studies mainly focused on recognizing letters directly from sEMG signals, forming an important first step toward translating muscle activity into text. Building on this foundation, we present MyoText, a hierarchical framework that decodes sEMG signals to text through physiologically grounded intermediate stages. MyoText first classifies finger activations from multichannel sEMG using a CNN-BiLSTM-Attention model, applies ergonomic typing priors to infer letters, and reconstructs full sentences with a fine-tuned T5 transformer. This modular design mirrors the natural hierarchy of typing, linking muscle intent to language output and reducing the search space for decoding. Evaluated on 30 users from the emg2qwerty dataset, MyoText outperforms baselines by achieving 85.4% finger-classification accuracy, 5.4% character error rate (CER), and 6.5% word error rate (WER). Beyond accuracy gains, this methodology establishes a principled pathway from neuromuscular signals to text, providing a blueprint for virtual and augmented-reality typing interfaces that operate entirely without physical keyboards. By integrating ergonomic structure with transformer-based linguistic reasoning, MyoText advances the feasibility of seamless, wearable neural input for future ubiquitous computing environments.", "AI": {"tldr": "本文提出了一种从表面肌电图（sEMG）信号解码文本的框架MyoText。", "motivation": "利用肌肉活动进行无键盘文本输入是可穿戴系统和混合现实技术的关键，但现有的直接字母识别方法搜索空间大且精度有限。因此开发一种将肌肉活动转换为文本的新途径至关重要。", "method": "MyoText采用分层框架，首先使用CNN-BiLSTM-Attention模型对多通道sEMG信号进行手指激活分类；然后应用打字先验推断字母；最后通过微调的T5变换器重构完整的句子。", "result": "在emg2qwerty数据集上评估30名用户，MyoText达到了85.4%的手指分类准确率、5.4%的字符错误率和6.5%的词错率。", "conclusion": "通过集成生理结构与变换器语言推理，MyoText为无缝可穿戴神经输入提供了一个蓝图。"}}
{"id": "2601.03097", "pdf": "https://arxiv.org/pdf/2601.03097", "abs": "https://arxiv.org/abs/2601.03097", "authors": ["Omayra Yago Nieto", "Alexandre Anahory Simoes", "Juan I. Giribet", "Leonardo Colombo"], "title": "Dual-quaternion learning control for autonomous vehicle trajectory tracking with safety guarantees", "categories": ["cs.RO", "eess.SY", "math.OC"], "comment": null, "summary": "We propose a learning-based trajectory tracking controller for autonomous robotic platforms whose motion can be described kinematically on $\\mathrm{SE}(3)$. The controller is formulated in the dual quaternion framework and operates at the velocity level, assuming direct command of angular and linear velocities, as is standard in many aerial vehicles and omnidirectional mobile robots. Gaussian Process (GP) regression is integrated into a geometric feedback law to learn and compensate online for unknown, state-dependent disturbances and modeling imperfections affecting both attitude and position, while preserving the algebraic structure and coupling properties inherent to rigid-body motion. The proposed approach does not rely on explicit parametric models of the unknown effects, making it well-suited for robotic systems subject to sensor-induced disturbances, unmodeled actuation couplings, and environmental uncertainties. A Lyapunov-based analysis establishes probabilistic ultimate boundedness of the pose tracking error under bounded GP uncertainty, providing formal stability guarantees for the learning-based controller. Simulation results demonstrate accurate and smooth trajectory tracking in the presence of realistic, localized disturbances, including correlated rotational and translational effects arising from magnetometer perturbations. These results illustrate the potential of combining geometric modeling and probabilistic learning to achieve robust, data-efficient pose control for autonomous robotic systems.", "AI": {"tldr": "提出了一种基于学习的轨迹跟踪控制器，用于自主机器人平台，在SE(3)上描述其运动。该控制器使用双四元数框架在线补偿未知的状态依赖扰动和建模误差。", "motivation": "为了提高自动驾驶车辆在存在传感器干扰、未建模耦合和环境不确定性情况下的轨迹跟踪性能。", "method": "将高斯过程回归集成到几何反馈律中，以学习并在线补偿姿态和位置上的未知状态依赖扰动及建模误差。使用李雅普诺夫分析建立概率最终有界性，确保控制器的稳定性。", "result": "模拟结果显示在存在现实中的局部干扰时，能够实现准确且平滑的姿态轨迹跟踪，包括由磁力计扰动引起的旋转和移动效果。", "conclusion": "结合了几何建模与概率学习的方法可以为自主机器人系统实现鲁棒高效的位置控制。"}}
{"id": "2601.03090", "pdf": "https://arxiv.org/pdf/2601.03090", "abs": "https://arxiv.org/abs/2601.03090", "authors": ["Rocio Mexia Diaz", "Yasmin Greenway", "Petru Manescu"], "title": "LesionTABE: Equitable AI for Skin Lesion Detection", "categories": ["cs.CV"], "comment": "Submitted to IEEE ISBI 2026", "summary": "Bias remains a major barrier to the clinical adoption of AI in dermatology, as diagnostic models underperform on darker skin tones. We present LesionTABE, a fairness-centric framework that couples adversarial debiasing with dermatology-specific foundation model embeddings. Evaluated across multiple datasets covering both malignant and inflammatory conditions, LesionTABE achieves over a 25\\% improvement in fairness metrics compared to a ResNet-152 baseline, outperforming existing debiasing methods while simultaneously enhancing overall diagnostic accuracy. These results highlight the potential of foundation model debiasing as a step towards equitable clinical AI adoption.", "AI": {"tldr": "提出了一种名为LesionTABE的框架，旨在消除皮肤病学中AI诊断模型在较深肤色人群中表现不佳的问题。", "motivation": "解决皮肤病变检测中的偏见问题，提高对不同肤色人群的公平性，推动临床应用。", "method": "结合对抗去偏与皮肤病特定基础模型嵌入的方法，提升诊断准确性的同时增强公平性。", "result": "在多个数据集上评估表明，LesionTABE相对于ResNet-152基线，在公平性度量指标上有超过25%的提高，并且整体诊断准确率也得到提升。", "conclusion": "通过基础模型去偏技术的应用展示了实现临床AI应用中公平性的潜力。"}}
{"id": "2601.03089", "pdf": "https://arxiv.org/pdf/2601.03089", "abs": "https://arxiv.org/abs/2601.03089", "authors": ["Xin Huang", "Antoni B. Chan"], "title": "Grad-ELLM: Gradient-based Explanations for Decoder-only LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their black-box nature raises concerns about transparency and faithfulness. Input attribution methods aim to highlight each input token's contributions to the model's output, but existing approaches are typically model-agnostic, and do not focus on transformer-specific architectures, leading to limited faithfulness. To address this, we propose Grad-ELLM, a gradient-based attribution method for decoder-only transformer-based LLMs. By aggregating channel importance from gradients of the output logit with respect to attention layers and spatial importance from attention maps, Grad-ELLM generates heatmaps at each generation step without requiring architectural modifications. Additionally, we introduce two faithfulneses metrics $π$-Soft-NC and $π$-Soft-NS, which are modifications of Soft-NC/NS that provide fairer comparisons by controlling the amount of information kept when perturbing the text. We evaluate Grad-ELLM on sentiment classification, question answering, and open-generation tasks using different models. Experiment results show that Grad-ELLM consistently achieves superior faithfulness than other attribution methods.", "AI": {"tldr": "该论文提出了Grad-ELLM，一种针对解码器模型的梯度属性方法。", "motivation": "为了提高大语言模型透明性和忠实性，解决现有输入归属方法对特定架构关注不足的问题，作者提出了一种新的属性方法。", "method": "通过聚合注意力层和空间重要性的梯度信息，Grad-ELLM在每个生成步骤中无需修改架构就能产生热力图。此外还引入了两种衡量忠实性的指标。", "result": "实验表明，在不同任务上，Grad-ELLM的性能优于其他归属方法。", "conclusion": "Grad-ELLM为解码器模型提供了更透明和忠实的解释方式，并通过引入新的忠实性度量提高了评估标准。"}}
{"id": "2601.03073", "pdf": "https://arxiv.org/pdf/2601.03073", "abs": "https://arxiv.org/abs/2601.03073", "authors": ["Tong Wu", "Thanet Markchom"], "title": "Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA", "categories": ["cs.CV"], "comment": null, "summary": "Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.", "AI": {"tldr": "研究针对卡通图像的视觉问答任务，通过设计多代理大型语言模型框架来解决传统模型在处理夸张抽象和叙事背景时的问题。", "motivation": "传统的大型语言模型无法有效应对卡通图像中的夸张抽象及叙事驱动上下文问题。此研究旨在开发一种更适合卡通VQA任务的方法以克服这些挑战。", "method": "提出了一种多代理框架，包括视觉代理、语言代理和批评代理三个部分。这三个代理协同工作，通过整合视觉线索和叙述背景来支持结构化推理。", "result": "实验在Pororo和Simpsons两个卡通数据集上进行，并提供了每个代理对最终预测贡献的详细分析，揭示了基于LLM的多代理行为在卡通VQA中的运作机制。", "conclusion": "研究表明，所提出的多代理框架能够有效解决传统大型语言模型在处理卡通图像时遇到的问题，并展示了其在理解和推理复杂叙事背景下的优势。"}}
{"id": "2601.03070", "pdf": "https://arxiv.org/pdf/2601.03070", "abs": "https://arxiv.org/abs/2601.03070", "authors": ["Tamlin Love", "Ferran Gebellí", "Pradip Pramanick", "Antonio Andriella", "Guillem Alenyà", "Anais Garrell", "Raquel Ros", "Silvia Rossi"], "title": "HEXAR: a Hierarchical Explainability Architecture for Robots", "categories": ["cs.RO"], "comment": "8 pages, 6 figures", "summary": "As robotic systems become increasingly complex, the need for explainable decision-making becomes critical. Existing explainability approaches in robotics typically either focus on individual modules, which can be difficult to query from the perspective of high-level behaviour, or employ monolithic approaches, which do not exploit the modularity of robotic architectures. We present HEXAR (Hierarchical EXplainability Architecture for Robots), a novel framework that provides a plug-in, hierarchical approach to generate explanations about robotic systems. HEXAR consists of specialised component explainers using diverse explanation techniques (e.g., LLM-based reasoning, causal models, feature importance, etc) tailored to specific robot modules, orchestrated by an explainer selector that chooses the most appropriate one for a given query. We implement and evaluate HEXAR on a TIAGo robot performing assistive tasks in a home environment, comparing it against end-to-end and aggregated baseline approaches across 180 scenario-query variations. We observe that HEXAR significantly outperforms baselines in root cause identification, incorrect information exclusion, and runtime, offering a promising direction for transparent autonomous systems.", "AI": {"tldr": "本文提出了一种新的解释性架构HEXAR，用于机器人系统的决策过程。", "motivation": "随着机器人系统变得越来越复杂，对其决策过程的可解释性需求日益增加。现有方法要么专注于单个模块而难以从高层次行为角度查询，要么采用单一整体方式而不利用机器人的模组化结构。", "method": "HEXAR通过插件化的分层架构生成关于机器人系统的解释，包括特定组件解释器和由选择器协调的多样解释技术。在TIAGo机器人执行辅助任务的情境下进行实施与评估，并对比端到端及聚合基线方法的表现。", "result": "实验结果显示，HEXAR在识别根本原因、排除错误信息和运行时间上显著优于基准方法。", "conclusion": "HEXAR为透明自主系统提供了一种有前景的方向。"}}
{"id": "2601.03067", "pdf": "https://arxiv.org/pdf/2601.03067", "abs": "https://arxiv.org/abs/2601.03067", "authors": ["Joseph Kampeas", "Emir Haleva"], "title": "Joint Encoding of KV-Cache Blocks for Scalable LLM Serving", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 16 figures, 2 tables", "summary": "Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment. We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\\sim$40\\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion kv_joint_encoding.", "AI": {"tldr": "本文提出了一种联合编码KV缓存块的方法，以解决大规模语言模型中内存瓶颈的问题。", "motivation": "现有的KV缓存压缩方法依赖于严格的启发式规则、破坏张量布局或需要专用计算硬件，限制了其可扩展性和部署能力。因此，作者提出了一个新的解决方案来应对这些问题。", "method": "该方案通过将类似块跨请求和输入片段融合成共享表示来联合编码KV缓存块，并保持标准的缓存结构，从而缓解KV缓存内存瓶颈。", "result": "实验表明，该方法在多种大规模语言模型和基准测试中实现了高达4.38倍的KV缓存压缩率，同时准确性损失可以忽略不计。此外，在实际的大规模语言模型服务中，联合编码将令牌吞吐量提高了约40%。", "conclusion": "该研究提出了一种新的方法来解决大规模语言模型中的内存瓶颈问题，并在多个基准测试和实际部署场景中验证了其有效性。"}}
{"id": "2601.03066", "pdf": "https://arxiv.org/pdf/2601.03066", "abs": "https://arxiv.org/abs/2601.03066", "authors": ["Janvijay Singh", "Dilek Hakkani-Tür"], "title": "Do LLMs Encode Functional Importance of Reasoning Tokens?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures, 2 tables", "summary": "Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.", "AI": {"tldr": "研究通过贪婪剪枝方法评估大型语言模型在生成长推理链时内部编码的各个代词的功能重要性，以实现更短且有效的推理过程。", "motivation": "现有的紧凑型推理方法虽能缩短推理链，但未能揭示模型是否在其内部对回答生成具有功能重要性的代词进行编码。因此，研究旨在探索此问题并提供解决方案。", "method": "提出了贪婪剪枝，一种在不降低模型概率的情况下逐次删除那些移除后仅轻微损害模型概率的推理代词的过程，以此形成可控长度的推理链。", "result": "研究表明，在相同推理长度下，基于修剪后的推理链条训练的学生模型优于由前沿模型监督压缩基准。此外，注意力得分可以预测贪婪剪枝等级，进一步表明模型编码了对推理代词的功能重要性结构。", "conclusion": "研究展示了大型语言模型内部编码的各个推理代词功能的重要性，并提出了一种新颖的方法来实现更短且有效的推理过程，这对实际应用具有重要意义。"}}
{"id": "2601.03062", "pdf": "https://arxiv.org/pdf/2601.03062", "abs": "https://arxiv.org/abs/2601.03062", "authors": ["Qusai Khaled", "Pasquale De Marinis", "Moez Louati", "David Ferras", "Laura Genga", "Uzay Kaymak"], "title": "Explainable Fuzzy GNNs for Leak Detection in Water Distribution Networks", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted at IFSA-NAFIPS 2025", "summary": "Timely leak detection in water distribution networks is critical for conserving resources and maintaining operational efficiency. Although Graph Neural Networks (GNNs) excel at capturing spatial-temporal dependencies in sensor data, their black-box nature and the limited work on graph-based explainable models for water networks hinder practical adoption. We propose an explainable GNN framework that integrates mutual information to identify critical network regions and fuzzy logic to provide clear, rule-based explanations for node classification tasks. After benchmarking several GNN architectures, we selected the generalized graph convolution network (GENConv) for its superior performance and developed a fuzzy-enhanced variant that offers intuitive explanations for classified leak locations. Our fuzzy graph neural network (FGENConv) achieved Graph F1 scores of 0.889 for detection and 0.814 for localization, slightly below the crisp GENConv 0.938 and 0.858, respectively. Yet it compensates by providing spatially localized, fuzzy rule-based explanations. By striking the right balance between precision and explainability, the proposed fuzzy network could enable hydraulic engineers to validate predicted leak locations, conserve human resources, and optimize maintenance strategies. The code is available at github.com/pasqualedem/GNNLeakDetection.", "AI": {"tldr": "本文提出了一种可解释的模糊图神经网络框架，用于水分配网络中的漏损检测。", "motivation": "现有的图神经网络（GNN）虽然擅长捕捉传感器数据的空间和时间依赖关系，但其黑箱性质限制了实际应用。为了改善这种情况，研究者开发了一个新的框架来提供清晰的、基于规则的解释。", "method": "该方法利用互信息识别关键区域，并结合模糊逻辑生成节点分类任务的直观解释。在选择GENConv作为基础架构后，进一步发展了一种增强版FGENConv模型。", "result": "提出的FGENConv模型检测得分0.889，定位得分为0.814，虽然略低于GENConv的结果（分别为0.938和0.858），但它提供了空间上局部化的、基于模糊规则的解释。", "conclusion": "通过在精确性和可解释性之间找到平衡点，这种新的网络使得水力工程师能够验证预测出的漏损位置，从而节省人力资源并优化维护策略。"}}
{"id": "2601.03056", "pdf": "https://arxiv.org/pdf/2601.03056", "abs": "https://arxiv.org/abs/2601.03056", "authors": ["Zhen Wang", "Jiaojiao Zhao", "Qilong Wang", "Yongfeng Dong", "Wenlong Yu"], "title": "Fine-Grained Generalization via Structuralizing Concept and Feature Space into Commonality, Specificity and Confounding", "categories": ["cs.CV"], "comment": "Accepted in AAAI26", "summary": "Fine-Grained Domain Generalization (FGDG) presents greater challenges than conventional domain generalization due to the subtle inter-class differences and relatively pronounced intra-class variations inherent in fine-grained recognition tasks. Under domain shifts, the model becomes overly sensitive to fine-grained cues, leading to the suppression of critical features and a significant drop in performance. Cognitive studies suggest that humans classify objects by leveraging both common and specific attributes, enabling accurate differentiation between fine-grained categories. However, current deep learning models have yet to incorporate this mechanism effectively. Inspired by this mechanism, we propose Concept-Feature Structuralized Generalization (CFSG). This model explicitly disentangles both the concept and feature spaces into three structured components: common, specific, and confounding segments. To mitigate the adverse effects of varying degrees of distribution shift, we introduce an adaptive mechanism that dynamically adjusts the proportions of common, specific, and confounding components. In the final prediction, explicit weights are assigned to each pair of components. Extensive experiments on three single-source benchmark datasets demonstrate that CFSG achieves an average performance improvement of 9.87% over baseline models and outperforms existing state-of-the-art methods by an average of 3.08%. Additionally, explainability analysis validates that CFSG effectively integrates multi-granularity structured knowledge and confirms that feature structuralization facilitates the emergence of concept structuralization.", "AI": {"tldr": "本文提出了概念和特征空间结构化的细粒度泛化方法CFSG，以应对细粒度识别任务中的域迁移问题。", "motivation": "面对细粒度识别任务中细微的类别差异和显著的类内变化，现有模型在域偏移时过于敏感于细粒度线索，导致性能大幅下降。本文受人类认知机制启发，提出了解决方案。", "method": "通过结构化概念和特征空间为公共、特定和混淆三个组成部分，并引入自适应机制以动态调整这三个部分的比例，在最终预测中赋予每个部分明确的权重。", "result": "实验结果显示CFSG在单源基准数据集上的平均性能优于基线模型9.87%，并比现有最佳方法高出3.08%。解释性分析证明了CFSG的有效性和特征结构化的促进作用。", "conclusion": "本文提出的方法有效解决了细粒度识别任务中的域迁移问题，并通过实验验证了其优越性。"}}
{"id": "2601.03055", "pdf": "https://arxiv.org/pdf/2601.03055", "abs": "https://arxiv.org/abs/2601.03055", "authors": ["Shiying Dong", "Zhipeng Shen", "Rudolf Reiter", "Hailong Huang", "Bingzhao Gao", "Hong Chen", "Wen-Hua Chen"], "title": "A Fast Semidefinite Convex Relaxation for Optimal Control Problems With Spatio-Temporal Constraints", "categories": ["cs.RO"], "comment": "9 pages, 6 figures", "summary": "Solving optimal control problems (OCPs) of autonomous agents operating under spatial and temporal constraints fast and accurately is essential in applications ranging from eco-driving of autonomous vehicles to quadrotor navigation. However, the nonlinear programs approximating the OCPs are inherently nonconvex due to the coupling between the dynamics and the event timing, and therefore, they are challenging to solve. Most approaches address this challenge by predefining waypoint times or just using nonconvex trajectory optimization, which simplifies the problem but often yields suboptimal solutions. To significantly improve the numerical properties, we propose a formulation with a time-scaling direct multiple shooting scheme that partitions the prediction horizon into segments aligned with characteristic time constraints. Moreover, we develop a fast semidefinite-programming-based convex relaxation that exploits the sparsity pattern of the lifted formulation. Comprehensive simulation studies demonstrate the solution optimality and computational efficiency. Furthermore, real-world experiments on a quadrotor waypoint flight task with constrained open time windows validate the practical applicability of the approach in complex environments.", "AI": {"tldr": "提出了一种快速求解带时空约束的最优控制问题（OCPs）的方法。", "motivation": "为了解决自主代理在空间和时间限制下的高效优化控制问题，特别是在自动驾驶车辆的生态驾驶和四旋翼飞行器导航中的应用，需要开发一种能提高数值特性的新方法。", "method": "通过采用时间缩放直接多重射击方案并利用时空约束的时间窗口划分预测范围，同时提出了一种快速半定规划（SDP）松弛法来解决非凸问题。", "result": "仿真研究表明所提方法在优化解决方案和计算效率方面表现优异。实际飞行实验进一步验证了该方法的实用性和适应复杂环境的能力。", "conclusion": "新方法能够更有效地求解带时空约束的最优控制问题，展现了其在实际应用中的优势和潜力。"}}
{"id": "2601.03054", "pdf": "https://arxiv.org/pdf/2601.03054", "abs": "https://arxiv.org/abs/2601.03054", "authors": ["Yankai Jiang", "Qiaoru Li", "Binlu Xu", "Haoran Sun", "Chao Ding", "Junting Dong", "Yuxiang Cai", "Xuhong Zhang", "Jianwei Yin"], "title": "IBISAgent: Reinforcing Pixel-Level Visual Reasoning in MLLMs for Universal Biomedical Object Referring and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent research on medical MLLMs has gradually shifted its focus from image-level understanding to fine-grained, pixel-level comprehension. Although segmentation serves as the foundation for pixel-level understanding, existing approaches face two major challenges. First, they introduce implicit segmentation tokens and require simultaneous fine-tuning of both the MLLM and external pixel decoders, which increases the risk of catastrophic forgetting and limits generalization to out-of-domain scenarios. Second, most methods rely on single-pass reasoning and lack the capability to iteratively refine segmentation results, leading to suboptimal performance. To overcome these limitations, we propose a novel agentic MLLM, named IBISAgent, that reformulates segmentation as a vision-centric, multi-step decision-making process. IBISAgent enables MLLMs to generate interleaved reasoning and text-based click actions, invoke segmentation tools, and produce high-quality masks without architectural modifications. By iteratively performing multi-step visual reasoning on masked image features, IBISAgent naturally supports mask refinement and promotes the development of pixel-level visual reasoning capabilities. We further design a two-stage training framework consisting of cold-start supervised fine-tuning and agentic reinforcement learning with tailored, fine-grained rewards, enhancing the model's robustness in complex medical referring and reasoning segmentation tasks. Extensive experiments demonstrate that IBISAgent consistently outperforms both closed-source and open-source SOTA methods. All datasets, code, and trained models will be released publicly.", "AI": {"tldr": "IBISAgent提出了一个新型的代理多模态大型语言模型，通过迭代视觉推理提高像素级理解能力。", "motivation": "现有的方法在像素级理解方面面临两个主要挑战：增加灾难性遗忘的风险和限制泛化能力；依赖单次推理导致性能不佳。为解决这些问题，提出IBISAgent以改进生物医学对象引用和分割。", "method": "通过重新定义分割过程为多步视觉决策任务，IBISAgent能够在不修改架构的情况下生成迭代的推理和点击动作，并且采用两阶段训练框架提高模型在复杂任务中的鲁棒性。", "result": "实验表明，IBISAgent在复杂的医学引用与推理分割任务上优于现有的最先进方法。", "conclusion": "提出的方法能够有效提升像素级视觉理解能力，并展示了其在多模态大型语言模型应用中广泛的应用前景。"}}
{"id": "2601.03048", "pdf": "https://arxiv.org/pdf/2601.03048", "abs": "https://arxiv.org/abs/2601.03048", "authors": ["Siyi Lyu", "Quan Liu", "Feng Yan"], "title": "On the Intrinsic Limits of Transformer Image Embeddings in Non-Solvable Spatial Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CC"], "comment": null, "summary": "Vision Transformers (ViTs) excel in semantic recognition but exhibit systematic failures in spatial reasoning tasks such as mental rotation. While often attributed to data scale, we propose that this limitation arises from the intrinsic circuit complexity of the architecture. We formalize spatial understanding as learning a Group Homomorphism: mapping image sequences to a latent space that preserves the algebraic structure of the underlying transformation group. We demonstrate that for non-solvable groups (e.g., the 3D rotation group $\\mathrm{SO}(3)$), maintaining such a structure-preserving embedding is computationally lower-bounded by the Word Problem, which is $\\mathsf{NC^1}$-complete. In contrast, we prove that constant-depth ViTs with polynomial precision are strictly bounded by $\\mathsf{TC^0}$. Under the conjecture $\\mathsf{TC^0} \\subsetneq \\mathsf{NC^1}$, we establish a complexity boundary: constant-depth ViTs fundamentally lack the logical depth to efficiently capture non-solvable spatial structures. We validate this complexity gap via latent-space probing, demonstrating that ViT representations suffer a structural collapse on non-solvable tasks as compositional depth increases.", "AI": {"tldr": "探讨了Vision Transformers在处理空间推理任务时的固有限制。", "motivation": "提出了Vision Transformers在处理如心理旋转等空间推理任务中失败的原因，归因于其架构内在电路复杂性而非数据量不足。", "method": "将空间理解形式化为学习群同态：图像序列映射到保持底层变换群代数结构的潜在空间。证明了对于非可解群（如3D旋转组SO(3)），维持这种结构保存嵌入在计算上由单词问题下界，该问题是NC^1完全的。", "result": "通过潜空间探测验证了复杂度差距：随着组合深度增加，Vision Transformers表示在非可解任务中遭受结构崩溃。", "conclusion": "证明了固定深度的Vision Transformers由于逻辑深度不足，无法高效捕捉非可解的空间结构。"}}
{"id": "2601.03046", "pdf": "https://arxiv.org/pdf/2601.03046", "abs": "https://arxiv.org/abs/2601.03046", "authors": ["Han Zhang", "Yanwei Wang", "Fang Li", "Hongjun Wang"], "title": "Motion Blur Robust Wheat Pest Damage Detection with Dynamic Fuzzy Feature Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Motion blur caused by camera shake produces ghosting artifacts that substantially degrade edge side object detection. Existing approaches either suppress blur as noise and lose discriminative structure, or apply full image restoration that increases latency and limits deployment on resource constrained devices. We propose DFRCP, a Dynamic Fuzzy Robust Convolutional Pyramid, as a plug in upgrade to YOLOv11 for blur robust detection. DFRCP enhances the YOLOv11 feature pyramid by combining large scale and medium scale features while preserving native representations, and by introducing Dynamic Robust Switch units that adaptively inject fuzzy features to strengthen global perception under jitter. Fuzzy features are synthesized by rotating and nonlinearly interpolating multiscale features, then merged through a transparency convolution that learns a content adaptive trade off between original and fuzzy cues. We further develop a CUDA parallel rotation and interpolation kernel that avoids boundary overflow and delivers more than 400 times speedup, making the design practical for edge deployment. We train with paired supervision on a private wheat pest damage dataset of about 3,500 images, augmented threefold using two blur regimes, uniform image wide motion blur and bounding box confined rotational blur. On blurred test sets, YOLOv11 with DFRCP achieves about 10.4 percent higher accuracy than the YOLOv11 baseline with only a modest training time overhead, reducing the need for manual filtering after data collection.", "AI": {"tldr": "提出了一种动态模糊稳健卷积金字塔（DFRCP），以改进YOLOv11在运动模糊情况下的目标检测能力。", "motivation": "为了提高相机抖动引起运动模糊情况下，边缘侧对象检测的准确性。现有方法要么抑制模糊作为噪声并丢失判别结构，要么应用全图像恢复而增加了延迟限制了资源受限设备上的部署。", "method": "通过结合大规模和中规模特征同时保留原始表示，并引入动态稳健切换单元自适应注入模糊特征以增强全局感知。模糊特征由多尺度特征旋转和非线性插值合成并通过透明卷积学习内容自适应的原始与模糊线索之间的折衷。", "result": "在运动模糊测试集上，YOLOv11结合DFRCP比基线YOLOv11提高了约10.4%的准确性，并且训练时间仅略有增加。进一步开发了一个CUDA并行旋转和插值内核以避免边界溢出，实现超过400倍的速度提升。", "conclusion": "通过改进的YOLOv11模型以及DFRCP结构，在运动模糊条件下实现了更高效的边缘侧目标检测，并且在资源受限设备上具备部署能力。"}}
{"id": "2601.03044", "pdf": "https://arxiv.org/pdf/2601.03044", "abs": "https://arxiv.org/abs/2601.03044", "authors": ["Mingjie Pan", "Siyuan Feng", "Qinglin Zhang", "Xinchen Li", "Jianheng Song", "Chendi Qu", "Yi Wang", "Chuankang Li", "Ziyu Xiong", "Zhi Chen", "Yi Liu", "Jianlan Luo"], "title": "SOP: A Scalable Online Post-Training System for Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action (VLA) models achieve strong generalization through large-scale pre-training, but real-world deployment requires expert-level task proficiency in addition to broad generality. Existing post-training approaches for VLA models are typically offline, single-robot, or task-specific, limiting effective on-policy adaptation and scalable learning from real-world interaction. We introduce a Scalable Online Post-training (SOP) system that enables online, distributed, multi-task post-training of generalist VLA models directly in the physical world. SOP tightly couples execution and learning through a closed-loop architecture in which a fleet of robots continuously streams on-policy experience and human intervention signals to a centralized cloud learner, and asynchronously receives updated policies. This design supports prompt on-policy correction, scales experience collection through parallel deployment, and preserves generality during adaptation. SOP is agnostic to the choice of post-training algorithm; we instantiate it with both interactive imitation learning (HG-DAgger) and reinforcement learning (RECAP). Across a range of real-world manipulation tasks including cloth folding, box assembly, and grocery restocking, we show that SOP substantially improves the performance of large pretrained VLA models while maintaining a single shared policy across tasks. Effective post-training can be achieved within hours of real-world interaction, and performance scales near-linearly with the number of robots in the fleet. These results suggest that tightly coupling online learning with fleet-scale deployment is instrumental to enabling efficient, reliable, and scalable post-training of generalist robot policies in the physical world.", "AI": {"tldr": "SOP系统通过在真实世界中进行在线、分布式多任务后训练，提高了视觉语言动作模型的实际部署效果。", "motivation": "现有的VLA模型的后训练方法大多为离线或单机器人操作，这限制了它们的有效适应和从现实世界交互中学到的经验。为此，研究者提出了SOP系统以解决这些问题并提高模型性能。", "method": "SOP通过一个闭合反馈架构，在其中一组机器人持续地向中央云端学习器传输实时经验和人类干预信号，并接收更新的策略进行操作；同时支持多种后训练算法如互动模仿学习和强化学习。", "result": "在一系列现实世界的操作任务中，包括布料折叠、盒子装配等，SOP系统显著提高了大型预训练VLA模型的表现，且能够在数小时内实现有效的后期训练。", "conclusion": "研究表明通过将在线学习与大规模部署紧密耦合能够有效提高通用机器人策略的后训练效率、可靠性和可扩展性。"}}
{"id": "2601.03043", "pdf": "https://arxiv.org/pdf/2601.03043", "abs": "https://arxiv.org/abs/2601.03043", "authors": ["Junhao Hu", "Fangze Li", "Mingtao Xu", "Feifan Meng", "Shiju Zhao", "Tiancheng Hu", "Ting Peng", "Anmin Liu", "Wenrui Huang", "Chenxu Liu", "Ziyue Hua", "Tao Xie"], "title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.", "AI": {"tldr": "研究提出了一种早停算法，用于减少稀疏注意力机制在长解码阶段的信息损失，提高大型语言模型的推理效率。", "motivation": "为了降低长序列中由于信息丢失导致的时间和内存复杂度，解决‘少即是更少’问题，提升大型语言模型的推断效率。", "method": "提出了一种早停算法，通过检测稀疏解码过程中的阈值来平衡信息损失与收益，减少token消耗量。", "result": "该方法在推理密集型基准测试中将token消费降低至多90%，同时精度下降不超过2%。", "conclusion": "证明了稀疏注意力机制可能导致序列变长和复杂度增加的问题，并提出了一种有效的早停算法来缓解这一问题，从而提高大型语言模型的效率。"}}
{"id": "2601.03040", "pdf": "https://arxiv.org/pdf/2601.03040", "abs": "https://arxiv.org/abs/2601.03040", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "PiDR: Physics-Informed Inertial Dead Reckoning for Autonomous Platforms", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "11 pages and 7 figures", "summary": "A fundamental requirement for full autonomy is the ability to sustain accurate navigation in the absence of external data, such as GNSS signals or visual information. In these challenging environments, the platform must rely exclusively on inertial sensors, leading to pure inertial navigation. However, the inherent noise and other error terms of the inertial sensors in such real-world scenarios will cause the navigation solution to drift over time. Although conventional deep-learning models have emerged as a possible approach to inertial navigation, they are inherently black-box in nature. Furthermore, they struggle to learn effectively with limited supervised sensor data and often fail to preserve physical principles. To address these limitations, we propose PiDR, a physics-informed inertial dead-reckoning framework for autonomous platforms in situations of pure inertial navigation. PiDR offers transparency by explicitly integrating inertial navigation principles into the network training process through the physics-informed residual component. PiDR plays a crucial role in mitigating abrupt trajectory deviations even under limited or sparse supervision. We evaluated PiDR on real-world datasets collected by a mobile robot and an autonomous underwater vehicle. We obtained more than 29% positioning improvement in both datasets, demonstrating the ability of PiDR to generalize different platforms operating in various environments and dynamics. Thus, PiDR offers a robust, lightweight, yet effective architecture and can be deployed on resource-constrained platforms, enabling real-time pure inertial navigation in adverse scenarios.", "AI": {"tldr": "提出PiDR框架，通过物理信息残差组件将惯性导航原理融入网络训练过程，以解决纯惯导系统中的定位漂移问题。", "motivation": "在缺乏外部数据的情况下，自主平台需要依赖惯性传感器进行精确导航。然而，惯性传感器的噪声和误差会导致长期轨迹偏移，传统深度学习模型难以有效利用有限监督信息并保持物理原理一致性。", "method": "PiDR框架通过物理信息残差组件将惯性导航原则融入网络训练过程，解决纯惯导系统中的定位漂移问题，并在实际数据集中进行验证。", "result": "实验结果表明，与现有方法相比，PiDR在两个不同平台的数据集上提高了29%以上的定位精度，证明了其在不同环境和动态情况下的泛化能力。", "conclusion": "PiDR框架提供了一种轻量级、有效且可部署于资源受限平台上的架构，实现了恶劣条件下实时纯惯导导航。"}}
{"id": "2601.03038", "pdf": "https://arxiv.org/pdf/2601.03038", "abs": "https://arxiv.org/abs/2601.03038", "authors": ["Changwen Li", "Rongjie Yan", "Chih-Hong Cheng", "Jian Zhang"], "title": "Validating Generalist Robots with Situation Calculus and STL Falsification", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Generalist robots are becoming a reality, capable of interpreting natural language instructions and executing diverse operations. However, their validation remains challenging because each task induces its own operational context and correctness specification, exceeding the assumptions of traditional validation methods. We propose a two-layer validation framework that combines abstract reasoning with concrete system falsification. At the abstract layer, situation calculus models the world and derives weakest preconditions, enabling constraint-aware combinatorial testing to systematically generate diverse, semantically valid world-task configurations with controllable coverage strength. At the concrete layer, these configurations are instantiated for simulation-based falsification with STL monitoring. Experiments on tabletop manipulation tasks show that our framework effectively uncovers failure cases in the NVIDIA GR00T controller, demonstrating its promise for validating general-purpose robot autonomy.", "AI": {"tldr": "本文提出了一种两层验证框架，用于验证能够理解自然语言指令并执行多样操作的通用型机器人。", "motivation": "由于每个任务都会产生不同的操作环境和正确性规范，传统的验证方法难以满足这些要求。因此需要一种新的验证方法来解决这个问题。", "method": "该框架结合了抽象推理与具体系统检验两层结构：抽象层面利用情况演算建模世界并推导最弱前提条件；具体层面通过信号时序逻辑监控进行仿真实验以验证错误案例。", "result": "实验表明，本文提出的框架在NVIDIA GR00T控制器上的台面操作任务中能有效发现失败案例。", "conclusion": "该研究证明了所提框架对通用机器人自主性的验证具有潜在价值。"}}
{"id": "2601.03037", "pdf": "https://arxiv.org/pdf/2601.03037", "abs": "https://arxiv.org/abs/2601.03037", "authors": ["Chunhui Zhao", "Xirui Kao", "Yilin Lu", "Yang Lyu"], "title": "A Bi-directional Adaptive Framework for Agile UAV Landing", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE Robotics and Automation Letters (RA-L) for possible publication", "summary": "Autonomous landing on mobile platforms is crucial for extending quadcopter operational flexibility, yet conventional methods are often too inefficient for highly dynamic scenarios. The core limitation lies in the prevalent ``track-then-descend'' paradigm, which treats the platform as a passive target and forces the quadcopter to perform complex, sequential maneuvers. This paper challenges that paradigm by introducing a bi-directional cooperative landing framework that redefines the roles of the vehicle and the platform. The essential innovation is transforming the problem from a single-agent tracking challenge into a coupled system optimization. Our key insight is that the mobile platform is not merely a target, but an active agent in the landing process. It proactively tilts its surface to create an optimal, stable terminal attitude for the approaching quadcopter. This active cooperation fundamentally breaks the sequential model by parallelizing the alignment and descent phases. Concurrently, the quadcopter's planning pipeline focuses on generating a time-optimal and dynamically feasible trajectory that minimizes energy consumption. This bi-directional coordination allows the system to execute the recovery in an agile manner, characterized by aggressive trajectory tracking and rapid state synchronization within transient windows. The framework's effectiveness, validated in dynamic scenarios, significantly improves the efficiency, precision, and robustness of autonomous quadrotor recovery in complex and time-constrained missions.", "AI": {"tldr": "提出了一种双向自适应框架，用于无人机的敏捷着陆", "motivation": "传统方法在动态场景中效率低下，限制了四旋翼飞行器的操作灵活性", "method": "通过平台和无人机之间的主动合作，重新定义问题为耦合系统优化。平台倾斜表面以创建最优稳定的姿态，无人机则生成时间最短且动力学可行的轨迹", "result": "该框架提高了复杂、时间受限任务中自主四旋翼恢复的效率、精度和鲁棒性", "conclusion": "双向自适应框架实现了敏捷着陆，显著改善了动态场景下的操作性能"}}
{"id": "2601.03032", "pdf": "https://arxiv.org/pdf/2601.03032", "abs": "https://arxiv.org/abs/2601.03032", "authors": ["Vidhi Rathore"], "title": "Causal Manifold Fairness: Enforcing Geometric Invariance in Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": null, "summary": "Fairness in machine learning is increasingly critical, yet standard approaches often treat data as static points in a high-dimensional space, ignoring the underlying generative structure. We posit that sensitive attributes (e.g., race, gender) do not merely shift data distributions but causally warp the geometry of the data manifold itself. To address this, we introduce Causal Manifold Fairness (CMF), a novel framework that bridges causal inference and geometric deep learning. CMF learns a latent representation where the local Riemannian geometry, defined by the metric tensor and curvature, remains invariant under counterfactual interventions on sensitive attributes. By enforcing constraints on the Jacobian and Hessian of the decoder, CMF ensures that the rules of the latent space (distances and shapes) are preserved across demographic groups. We validate CMF on synthetic Structural Causal Models (SCMs), demonstrating that it effectively disentangles sensitive geometric warping while preserving task utility, offering a rigorous quantification of the fairness-utility trade-off via geometric metrics.", "AI": {"tldr": "提出因果流形公平性（CMF）框架，通过学习保持几何不变性的潜在表示来解决机器学习中的偏见问题。", "motivation": "标准方法将数据视为高维空间中的静态点，忽略了敏感属性对数据流形的因果影响。因此，需要一种新方法来确保在不同人口群体中保持公平性的同时不牺牲任务效用。", "method": "CMF结合因果推理和几何深度学习，在潜在表示中施加约束以保持局部黎曼几何不变，并通过验证结构因果模型（SCMs）来证明其有效性。", "result": "实验结果显示，CMF能有效解耦敏感属性的几何扭曲并保留任务效用，同时提供了公平性与效用权衡的度量标准。", "conclusion": "CMF框架提供了一种新颖的方法，用于解决机器学习中的数据偏见问题，并通过严格的量化指标证明了其优势。"}}
{"id": "2601.03030", "pdf": "https://arxiv.org/pdf/2601.03030", "abs": "https://arxiv.org/abs/2601.03030", "authors": ["Ali Kashefi"], "title": "Flow Matching and Diffusion Models via PointNet for Generating Fluid Fields on Irregular Geometries", "categories": ["cs.CV", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "We present two novel generative geometric deep learning frameworks, termed Flow Matching PointNet and Diffusion PointNet, for predicting fluid flow variables on irregular geometries by incorporating PointNet into flow matching and diffusion models, respectively. In these frameworks, a reverse generative process reconstructs physical fields from standard Gaussian noise conditioned on unseen geometries. The proposed approaches operate directly on point-cloud representations of computational domains (e.g., grid vertices of finite-volume meshes) and therefore avoid the limitations of pixelation used to project geometries onto uniform lattices. In contrast to graph neural network-based diffusion models, Flow Matching PointNet and Diffusion PointNet do not exhibit high-frequency noise artifacts in the predicted fields. Moreover, unlike such approaches, which require auxiliary intermediate networks to condition geometry, the proposed frameworks rely solely on PointNet, resulting in a simple and unified architecture. The performance of the proposed frameworks is evaluated on steady incompressible flow past a cylinder, using a geometric dataset constructed by varying the cylinder's cross-sectional shape and orientation across samples. The results demonstrate that Flow Matching PointNet and Diffusion PointNet achieve more accurate predictions of velocity and pressure fields, as well as lift and drag forces, and exhibit greater robustness to incomplete geometries compared to a vanilla PointNet with the same number of trainable parameters.", "AI": {"tldr": "提出两种基于PointNet的生成几何深度学习框架Flow Matching PointNet和Diffusion PointNet，用于预测不规则几何上的流体流动变量。", "motivation": "现有的图形神经网络扩散模型存在高频噪声问题，并且需要额外的中间网络来调节几何形状。为解决这些问题并提高准确性与鲁棒性，提出直接在点云表示上工作的Flow Matching PointNet和Diffusion PointNet框架。", "method": "结合PointNet技术和流匹配、扩散模型技术，开发生成过程从标准高斯噪声重建物理场的逆向方法，并基于不同形状和方向的几何样本进行验证。", "result": "在用于绕圆柱体流动的稳态不可压缩流测试中，Flow Matching PointNet和Diffusion PointNet框架显示更高的预测准确性和鲁棒性，优于相同参数数量的传统PointNet模型。", "conclusion": "该研究成功展示了通过引入PointNet技术改进生成几何深度学习方法的有效性，在复杂形状上实现更精确的物理场预测。"}}
{"id": "2601.03024", "pdf": "https://arxiv.org/pdf/2601.03024", "abs": "https://arxiv.org/abs/2601.03024", "authors": ["Kim Jun-Seong", "Tae-Hyun Oh", "Eduardo Pérez-Pellitero", "Youngkyoon Jang"], "title": "SA-ResGS: Self-Augmented Residual 3D Gaussian Splatting for Next Best View Selection", "categories": ["cs.CV"], "comment": null, "summary": "We propose Self-Augmented Residual 3D Gaussian Splatting (SA-ResGS), a novel framework to stabilize uncertainty quantification and enhancing uncertainty-aware supervision in next-best-view (NBV) selection for active scene reconstruction. SA-ResGS improves both the reliability of uncertainty estimates and their effectiveness for supervision by generating Self-Augmented point clouds (SA-Points) via triangulation between a training view and a rasterized extrapolated view, enabling efficient scene coverage estimation. While improving scene coverage through physically guided view selection, SA-ResGS also addresses the challenge of under-supervised Gaussians, exacerbated by sparse and wide-baseline views, by introducing the first residual learning strategy tailored for 3D Gaussian Splatting. This targeted supervision enhances gradient flow in high-uncertainty Gaussians by combining uncertainty-driven filtering with dropout- and hard-negative-mining-inspired sampling. Our contributions are threefold: (1) a physically grounded view selection strategy that promotes efficient and uniform scene coverage; (2) an uncertainty-aware residual supervision scheme that amplifies learning signals for weakly contributing Gaussians, improving training stability and uncertainty estimation across scenes with diverse camera distributions; (3) an implicit unbiasing of uncertainty quantification as a consequence of constrained view selection and residual supervision, which together mitigate conflicting effects of wide-baseline exploration and sparse-view ambiguity in NBV planning. Experiments on active view selection demonstrate that SA-ResGS outperforms state-of-the-art baselines in both reconstruction quality and view selection robustness.", "AI": {"tldr": "提出了一种新的框架SA-ResGS，用于优化主动场景重建中的下一个最佳视图选择。", "motivation": "为了稳定不确定性量化并增强基于不确定性的监督，在下一个最佳视图（NBV）选择中改进了不确定性的可靠性和有效性。", "method": "通过生成自增点云和引入第一种针对3D高斯光斑的残差学习策略，提高场景覆盖率并通过组合不确定性驱动过滤与采样方法来增强弱贡献高斯的学习信号。", "result": "实验表明，SA-ResGS在重建质量和视图选择稳健性方面优于现有的基线。", "conclusion": "SA-ResGS通过物理引导的视图选择策略和残差监督方案增强了场景覆盖率和不确定性估计，在主动视图选择中表现出色。"}}
{"id": "2601.03023", "pdf": "https://arxiv.org/pdf/2601.03023", "abs": "https://arxiv.org/abs/2601.03023", "authors": ["Lecheng Gong", "Weimin Fang", "Ting Yang", "Dongjie Tao", "Chunxiao Guo", "Peng Wei", "Bo Xie", "Jinqun Guan", "Zixiao Chen", "Fang Shi", "Jinjie Gu", "Junwei Liu"], "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models", "categories": ["cs.CL", "cs.HC"], "comment": ":68T42ACM Class:I.2.1", "summary": "Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.", "AI": {"tldr": "开发了一个名为MedDialogRubrics的新基准，用于评估大型语言模型在多轮医疗咨询中的诊断能力。", "motivation": "现有的评估框架未能充分测试医学大语言模型的信息收集和诊断推理能力。因此，需要一个更为全面的评价体系来填补这一空白。", "method": "创建了一个包含5200个合成病人案例及超过6万项细化评分标准的数据集，并设计了一套由LLM生成且经临床专家修正的评估框架。该系统采用多智能体结构模拟真实患者记录，使用证据基础医学指南和拒绝抽样技术来制定每个案例的关键问题。", "result": "通过对现有模型进行测试发现，它们在多个评价维度上存在挑战，表明改进医疗对话需要提升对话管理架构而非简单的微调。", "conclusion": "MedDialogRubrics为评估大型语言模型的医学诊断能力提供了一个有力工具，并指出了未来研究的方向。"}}
{"id": "2601.03020", "pdf": "https://arxiv.org/pdf/2601.03020", "abs": "https://arxiv.org/abs/2601.03020", "authors": ["Taisei Nogami", "Tachio Terauchi"], "title": "Hardness of Regular Expression Matching with Extensions", "categories": ["cs.DS", "cs.CC", "cs.FL"], "comment": null, "summary": "The regular expression matching problem asks whether a given regular expression of length $m$ matches a given string of length $n$. As is well known, the problem can be solved in $O(nm)$ time using Thompson's algorithm. Moreover, recent studies have shown that the matching problem for regular expressions extended with a practical extension called lookaround can be solved in the same time complexity. In this work, we consider three well-known extensions to regular expressions called backreference, intersection and complement, and we show that, unlike in the case of lookaround, the matching problem for regular expressions extended with any of the three (for backreference, even when restricted to one capturing group) cannot be solved in $O(n^{2-\\varepsilon} \\mathrm{poly}(m))$ time for any constant $\\varepsilon > 0$ under the Orthogonal Vectors Conjecture. Moreover, we study the matching problem for regular expressions extended with complement in more detail, which is also known as extended regular expression (ERE) matching. We show that there is no ERE matching algorithm that runs in $O(n^{ω-\\varepsilon} \\mathrm{poly}(m))$ time ($2 \\le ω< 2.3716$ is the exponent of square matrix multiplication) for any constant $\\varepsilon > 0$ under the $k$-Clique Hypothesis, and there is no combinatorial ERE matching algorithm that runs in $O(n^{3-\\varepsilon} \\mathrm{poly}(m))$ time for any constant $\\varepsilon > 0$ under the Combinatorial $k$-Clique Hypothesis. This shows that the $O(n^3 m)$-time algorithm introduced by Hopcroft and Ullman in 1979 and recently improved by Bille et al. to run in $O(n^ωm)$ time using fast matrix multiplication was already optimal in a sense, and sheds light on why the theoretical computer science community has struggled to improve the time complexity of ERE matching with respect to $n$ and $m$ for more than 45 years.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.03018", "pdf": "https://arxiv.org/pdf/2601.03018", "abs": "https://arxiv.org/abs/2601.03018", "authors": ["Choonghan Kim", "Hyunmin Hwang", "Hangeol Chang", "Jaemin Kim", "Jinse Park", "Jae-Sung Lim", "Jong Chul Ye"], "title": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5", "AI": {"tldr": "该论文提出了一种基于强化学习的框架Dementia-R1，用于从不结构化的临床记录中预测痴呆症的发展。", "motivation": "大型语言模型在理解临床文本方面表现出色，但在处理需要跨多次访问推理复杂的非单调症状轨迹的长期预测任务时存在困难。标准监督训练缺乏对症状演变的明确标注，而直接强化学习则受制于稀疏二进制奖励。", "method": "提出了一种冷启动强化学习策略，首先预训练模型以预测从患者历史中提取的可验证临床指标，并通过长期疾病进展推理来增强最终临床状态的判断能力。采用Dementia-R1框架进行纵向痴呆症预测。", "result": "实验结果表明，Dementia-R1在真实世界不结构化临床数据集上实现了77.03%的F1得分，在ADNI基准测试中，其性能与GPT-4o相当，能够有效捕捉认知轨迹的变化。", "conclusion": "Dementia-R1框架通过强化学习和冷启动策略改进了痴呆症长期预测的能力，并在实际临床数据集中表现出色。"}}
{"id": "2601.03015", "pdf": "https://arxiv.org/pdf/2601.03015", "abs": "https://arxiv.org/abs/2601.03015", "authors": ["Anaïs Berkes", "Vincent Taboga", "Donna Vakalis", "David Rolnick", "Yoshua Bengio"], "title": "In-Context Reinforcement Learning through Bayesian Fusion of Context and Value Prior", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In-context reinforcement learning (ICRL) promises fast adaptation to unseen environments without parameter updates, but current methods either cannot improve beyond the training distribution or require near-optimal data, limiting practical adoption. We introduce SPICE, a Bayesian ICRL method that learns a prior over Q-values via deep ensemble and updates this prior at test-time using in-context information through Bayesian updates. To recover from poor priors resulting from training on sub-optimal data, our online inference follows an Upper-Confidence Bound rule that favours exploration and adaptation. We prove that SPICE achieves regret-optimal behaviour in both stochastic bandits and finite-horizon MDPs, even when pretrained only on suboptimal trajectories. We validate these findings empirically across bandit and control benchmarks. SPICE achieves near-optimal decisions on unseen tasks, substantially reduces regret compared to prior ICRL and meta-RL approaches while rapidly adapting to unseen tasks and remaining robust under distribution shift.", "AI": {"tldr": "介绍了一种名为SPICE的贝叶斯ICRL方法，该方法通过深度集成学习Q值先验，并在测试时利用上下文信息进行更新，以实现在未见过的任务中的快速适应。", "motivation": "现有ICRL方法要么无法超越训练分布，要么需要近最优数据，限制了其实用性。作者提出SPICE来解决这些局限性，通过贝叶斯融合提高泛化能力和鲁棒性。", "method": "使用深度集成学习Q值的先验，并在测试时利用上下文信息进行贝叶斯更新。遵循上置信边界规则以促进探索和适应。", "result": "SPICE实现了最佳遗憾行为，即使是在仅基于次优轨迹训练的情况下也能做到。实验证明了其在未见过任务中的近最优决策能力以及减少遗憾的效果。", "conclusion": "通过引入SPICE，解决了ICRL方法的局限性，并证明了它在未见过的任务中具有快速适应能力和鲁棒性的优势。"}}
{"id": "2601.03014", "pdf": "https://arxiv.org/pdf/2601.03014", "abs": "https://arxiv.org/abs/2601.03014", "authors": ["Junli Liang", "Pengfei Zhou", "Wangqiu Zhou", "Wenjie Qing", "Qi Zhao", "Ziwen Wang", "Qi Song", "Xiangyang Li"], "title": "SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.", "AI": {"tldr": "本文提出了SentGraph，一种基于句子级图的检索增强生成框架，以支持多跳问答任务。", "motivation": "传统RAG模型在单跳问题回答中效果显著，但在需要从多个文档组合证据的多跳任务中面临挑战。现有的块基线检索方法提供的上下文相关性低且逻辑不连贯。", "method": "SentGraph通过构建层级句子图来显式建模句子之间的细粒度逻辑关系，该图由Rhetorical Structure Theory区分核心句和从属句，并组织成主题级子图。在线检索时，根据图引导的证据选择和路径扩展来检索细粒度的句子级证据。", "result": "在四个多跳问答基准上进行了广泛实验，结果验证了显式建模句子级别逻辑依赖的重要性。", "conclusion": "SentGraph通过构造层次化句子图模型，改进了多跳问题回答中证据组合和推理过程。"}}
{"id": "2601.03011", "pdf": "https://arxiv.org/pdf/2601.03011", "abs": "https://arxiv.org/abs/2601.03011", "authors": ["Yihan Wei", "Shenghai Yuan", "Tianchen Deng", "Boyang Lou", "Enwen Hu"], "title": "ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.", "AI": {"tldr": "递归角案例策划框架ReCCur用于增强视觉语言理解在开放和边缘环境中的鲁棒性", "motivation": "处理罕见或极端场景驱动的现实世界失败，这些场景难以大规模策划。网络数据嘈杂，标签脆弱且边缘部署无法进行大量再训练。", "method": "通过多代理递归管道将嘈杂的网络图像转换为可审计的细粒度标签。包括大型数据获取和过滤、混合专家知识蒸馏以及区域证据视觉语言模型对抗标记等步骤。", "result": "在现实角案例场景中（如洪水车辆检查），ReCCur可在消费级GPU上运行，持续改进纯度和分离性，并需要最少的人类监督。", "conclusion": "提供了一个实用的下游训练和评估资源约束的基础框架。"}}
{"id": "2601.03005", "pdf": "https://arxiv.org/pdf/2601.03005", "abs": "https://arxiv.org/abs/2601.03005", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Zhaoye Li", "Bin Ji", "Baosheng Wang", "Jie Yu"], "title": "JPU: Bridging Jailbreak Defense and Unlearning via On-Policy Path Rectification", "categories": ["cs.CR", "cs.AI"], "comment": "14 pages, 6 figures, under review;", "summary": "Despite extensive safety alignment, Large Language Models (LLMs) often fail against jailbreak attacks. While machine unlearning has emerged as a promising defense by erasing specific harmful parameters, current methods remain vulnerable to diverse jailbreaks. We first conduct an empirical study and discover that this failure mechanism is caused by jailbreaks primarily activating non-erased parameters in the intermediate layers. Further, by probing the underlying mechanism through which these circumvented parameters reassemble into the prohibited output, we verify the persistent existence of dynamic $\\textbf{jailbreak paths}$ and show that the inability to rectify them constitutes the fundamental gap in existing unlearning defenses. To bridge this gap, we propose $\\textbf{J}$ailbreak $\\textbf{P}$ath $\\textbf{U}$nlearning (JPU), which is the first to rectify dynamic jailbreak paths towards safety anchors by dynamically mining on-policy adversarial samples to expose vulnerabilities and identify jailbreak paths. Extensive experiments demonstrate that JPU significantly enhances jailbreak resistance against dynamic attacks while preserving the model's utility.", "AI": {"tldr": "提出JPU方法，通过动态挖掘在策略对抗样本来修复动态越狱路径，提高大型语言模型的安全性。", "motivation": "当前机器遗忘方法面对多样化越狱攻击仍然脆弱。研究发现这些失败是因为非擦除参数重新激活导致的持续存在动态越狱路径。", "method": "提出JPU方法，通过动态挖掘在策略对抗样本来暴露漏洞并识别越狱路径，从而修复动态越狱路径。", "result": "实验表明，JPU显著增强了模型对动态攻击的安全性，并保持了模型效用。", "conclusion": "首次提出了可以修复动态越狱路径的方法JPU，为大型语言模型的越狱防御提供了一种有效途径。"}}
{"id": "2601.03001", "pdf": "https://arxiv.org/pdf/2601.03001", "abs": "https://arxiv.org/abs/2601.03001", "authors": ["Li Wang", "Boqi Li", "Hang Chen", "Xingjian Wu", "Yichen Wang", "Jiewen Tan", "Xinyu Zhang", "Huaping Liu"], "title": "Towards Efficient 3D Object Detection for Vehicle-Infrastructure Collaboration via Risk-Intent Selection", "categories": ["cs.CV"], "comment": null, "summary": "Vehicle-Infrastructure Collaborative Perception (VICP) is pivotal for resolving occlusion in autonomous driving, yet the trade-off between communication bandwidth and feature redundancy remains a critical bottleneck. While intermediate fusion mitigates data volume compared to raw sharing, existing frameworks typically rely on spatial compression or static confidence maps, which inefficiently transmit spatially redundant features from non-critical background regions. To address this, we propose Risk-intent Selective detection (RiSe), an interaction-aware framework that shifts the paradigm from identifying visible regions to prioritizing risk-critical ones. Specifically, we introduce a Potential Field-Trajectory Correlation Model (PTCM) grounded in potential field theory to quantitatively assess kinematic risks. Complementing this, an Intention-Driven Area Prediction Module (IDAPM) leverages ego-motion priors to proactively predict and filter key Bird's-Eye-View (BEV) areas essential for decision-making. By integrating these components, RiSe implements a semantic-selective fusion scheme that transmits high-fidelity features only from high-interaction regions, effectively acting as a feature denoiser. Extensive experiments on the DeepAccident dataset demonstrate that our method reduces communication volume to 0.71\\% of full feature sharing while maintaining state-of-the-art detection accuracy, establishing a competitive Pareto frontier between bandwidth efficiency and perception performance.", "AI": {"tldr": "本文提出了一个名为Risk-intent Selective detection (RiSe)的框架，该框架旨在通过选择风险关键区域来优化车辆基础设施协作感知中的特征传输效率。", "motivation": "现有的车辆与基础设施协作感知系统在解决遮挡问题时面临通信带宽和特征冗余之间的权衡。传统的空间压缩或静态置信度图方法不足以有效地处理这些挑战，因此需要一种新的交互意识框架来优化数据传输。", "method": "RiSe框架结合了潜在场轨迹相关模型（PTCM）和意图驱动区域预测模块（IDAPM），以定量评估运动风险并主动预测关键区域。通过这种方式，该方法可以实现语义选择性融合，仅从高互动区域传递高质量特征。", "result": "实验结果表明，RiSe能够在减少通信量到原始特征共享的0.71%的同时，保持最先进的检测精度。", "conclusion": "本文提出的框架通过优化风险关键区域的选择，在带宽效率和感知性能之间建立了竞争性的帕累托前沿。"}}
{"id": "2601.02997", "pdf": "https://arxiv.org/pdf/2601.02997", "abs": "https://arxiv.org/abs/2601.02997", "authors": ["Waleed Khalid", "Dmitry Ignatov", "Radu Timofte"], "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.", "AI": {"tldr": "通过将代码导向的大规模语言模型置于闭环合成框架中，研究其在神经架构设计方面的自主导航能力。", "motivation": "探索大规模语言模型是否能够自主生成新颖且高性能的神经网络架构。", "method": "利用一个基于PyTorch的卷积网络，进行22轮监督微调；通过低精度性能信号和MinHash-Jaccard标准过滤重复结构；采用参数高效的LoRA适应技术，将高表现、新结构模型迭代细化。", "result": "生成的有效率稳定在50.6%，最高达到74.5%；初始单周期准确度从28.06%提升到50.99%，超过40%的候选架构比例从2.04%增加至96.81%。", "conclusion": "语言模型能够通过执行反馈，转化成自主性能驱动的神经网络设计师，并能超越训练数据，生成非文本奖励模式。"}}
{"id": "2601.02994", "pdf": "https://arxiv.org/pdf/2601.02994", "abs": "https://arxiv.org/abs/2601.02994", "authors": ["Youngjoon Jeong", "Junha Chun", "Taesup Kim"], "title": "Learning to Act Robustly with View-Invariant Latent Actions", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Website: https://joon-stack.github.io/VILA/", "summary": "Vision-based robotic policies often struggle with even minor viewpoint changes, underscoring the need for view-invariant visual representations. This challenge becomes more pronounced in real-world settings, where viewpoint variability is unavoidable and can significantly disrupt policy performance. Existing methods typically learn invariance from multi-view observations at the scene level, but such approaches rely on visual appearance and fail to incorporate the physical dynamics essential for robust generalization. We propose View-Invariant Latent Action (VILA), which models a latent action capturing transition patterns across trajectories to learn view-invariant representations grounded in physical dynamics. VILA aligns these latent actions across viewpoints using an action-guided objective based on ground-truth action sequences. Experiments in both simulation and the real world show that VILA-based policies generalize effectively to unseen viewpoints and transfer well to new tasks, establishing VILA as a strong pretraining framework that improves robustness and downstream learning performance.", "AI": {"tldr": "本文提出了VILA模型，以解决机器人策略在不同视角下的表现问题。", "motivation": "现有的方法通常依赖于场景级别的多视图观察来学习不变性，但这些方法忽略了物理动态的重要性。因此，在现实世界中难以保证策略的鲁棒性和泛化能力。", "method": "VILA通过建模一个反映过渡模式的潜在动作，并使用基于真实动作序列的动作引导目标来对齐不同视角下的潜在动作。", "result": "实验结果表明，VILA可以有效地推广到未见过的角度和新任务上，提高了策略的鲁棒性和下游学习性能。", "conclusion": "本文提出了一个强大的预训练框架VILA，以解决基于视觉的机器人策略在不同视角下表现不佳的问题。"}}
{"id": "2601.02991", "pdf": "https://arxiv.org/pdf/2601.02991", "abs": "https://arxiv.org/abs/2601.02991", "authors": ["Chengcheng Feng", "Haojie Yin", "Yucheng Jin", "Kaizhu Huang"], "title": "Towards Faithful Reasoning in Comics for Small MLLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\\mathbf{12.1\\%}$ across different MLLMs.", "AI": {"tldr": "提出了一种新的漫画推理框架，以解决小规模多模态语言模型在漫画视觉问答中的推理问题。", "motivation": "标准的Chain-of-Thought (CoT) 方法应用于漫画视觉问答时，在小型模型中表现不佳，并且存在状态纠缠、伪转移和资源利用效率低下的问题。", "method": "结合模块化CoT生成，GRPO增强型强化微调以及新的结构奖励，提出了一个全新的漫画推理框架。", "result": "在五个具有挑战性的基准测试上，该方法相较于现有技术取得了12.1%的平均改进。", "conclusion": "所提出的漫画推理框架显著提升了小规模多模态语言模型处理漫画视觉问答任务的能力。"}}
{"id": "2601.02988", "pdf": "https://arxiv.org/pdf/2601.02988", "abs": "https://arxiv.org/abs/2601.02988", "authors": ["Rianne Weber", "Niels Rocholl", "Max de Grauw", "Mathias Prokop", "Ewoud Smit", "Alessa Hering"], "title": "ULS+: Data-driven Model Adaptation Enhances Lesion Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at BVM 2026 (Bildverarbeitung für die Medizin), peer-reviewed conference paper", "summary": "In this study, we present ULS+, an enhanced version of the Universal Lesion Segmentation (ULS) model. The original ULS model segments lesions across the whole body in CT scans given volumes of interest (VOIs) centered around a click-point. Since its release, several new public datasets have become available that can further improve model performance. ULS+ incorporates these additional datasets and uses smaller input image sizes, resulting in higher accuracy and faster inference. We compared ULS and ULS+ using the Dice score and robustness to click-point location on the ULS23 Challenge test data and a subset of the Longitudinal-CT dataset. In all comparisons, ULS+ significantly outperformed ULS. Additionally, ULS+ ranks first on the ULS23 Challenge test-phase leaderboard. By maintaining a cycle of data-driven updates and clinical validation, ULS+ establishes a foundation for robust and clinically relevant lesion segmentation models.", "AI": {"tldr": "ULS+是一款改进的用于CT扫描中肿瘤分割的数据驱动模型，通过整合新数据集并减小输入图像尺寸，提高准确性和推理速度。", "motivation": "为了提升原版通用肿瘤分割(ULS)模型在不同CT扫描中的表现，并适应新的公共数据集以进一步优化性能。", "method": "将额外的公开数据集纳入训练过程，并采用较小的输入图像尺寸来改进模型，提高准确性和推理速度。", "result": "对比原有模型ULS，在所有测试中，ULS+在Dice分数和对点击点位置的鲁棒性方面表现出色，并且在ULS23挑战赛的测试阶段排行榜上排名第一。", "conclusion": "通过持续的数据驱动更新和临床验证过程，ULS+建立了坚实的肿瘤分割模型基础，具有较强的准确性和临床相关性。"}}
{"id": "2601.02987", "pdf": "https://arxiv.org/pdf/2601.02987", "abs": "https://arxiv.org/abs/2601.02987", "authors": ["Wingwa Fu", "Takayuki Okatani"], "title": "LAMS-Edit: Latent and Attention Mixing with Schedulers for Improved Content Preservation in Diffusion-Based Image and Style Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image editing using diffusion models faces challenges in balancing content preservation with edit application and handling real-image editing. To address these, we propose LAMS-Edit, leveraging intermediate states from the inversion process--an essential step in real-image editing--during edited image generation. Specifically, latent representations and attention maps from both processes are combined at each step using weighted interpolation, controlled by a scheduler. This technique, Latent and Attention Mixing with Schedulers (LAMS), integrates with Prompt-to-Prompt (P2P) to form LAMS-Edit--an extensible framework that supports precise editing with region masks and enables style transfer via LoRA. Extensive experiments demonstrate that LAMS-Edit effectively balances content preservation and edit application.", "AI": {"tldr": "该论文提出了一种结合中间状态的扩散模型编辑方法，用于改善图像和风格编辑中的内容保留。", "motivation": "文本到图像编辑中存在平衡内容保持与编辑应用的挑战，并且在处理真实图片时也有困难。为此，研究者提出了LAMS-Edit以解决这些问题。", "method": "提出了一种称为Latent and Attention Mixing with Schedulers（LAMS）的技术，在生成编辑后的图像过程中结合了来自反演过程中的潜在表示和注意力图，并使用调度器控制加权插值。将此技术与Prompt-to-Prompt结合起来形成了一个可扩展的框架，支持精准编辑并可通过LoRA实现风格转移。", "result": "广泛的实验表明LAMS-Edit能够有效平衡内容保持和编辑应用。", "conclusion": "通过结合中间状态的技术，LAMS-Edit在图像和风格编辑中展示了改进的内容保留能力。"}}
{"id": "2601.02983", "pdf": "https://arxiv.org/pdf/2601.02983", "abs": "https://arxiv.org/abs/2601.02983", "authors": ["Yuankun Xie", "Xiaoxuan Guo", "Jiayi Zhou", "Tao Wang", "Jian Liu", "Ruibo Fu", "Xiaopeng Wang", "Haonan Cheng", "Long Ye"], "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.", "AI": {"tldr": "论文提出了基于频率时间结构化推理链的音频大语言模型（ALLM）微调方法，用于可解释型全类型音视频深度伪造检测。", "motivation": "当前高质合成音频技术的发展使得恶意音频深度伪造的风险增加，现有的监督细化方法难以提供透明决策。为了提高通用性和解释性，研究者引入了频率时间结构化推理链数据及优化算法。", "method": "利用ALLM的多任务泛化能力，在有监督和强化学习微调基础上，设计自动注释与打磨管道生成34万条冷启动示例，并提出频域时序分组相对策略优化（FT-GRPO），实现解释性强的全类型音视频深度伪造检测。", "result": "实验表明，该方法在多种音频类型的深度伪造检测中达到SOTA水平，同时产生基于频率时间结构化推理链的可解释理由。", "conclusion": "论文通过提出频域时序分组相对策略优化算法和生成冷启动示例的方法，在增强模型性能的同时提高了其透明度。"}}
{"id": "2601.02978", "pdf": "https://arxiv.org/pdf/2601.02978", "abs": "https://arxiv.org/abs/2601.02978", "authors": ["Ruikang Zhang", "Shuo Wang", "Qi Su"], "title": "Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.", "AI": {"tldr": "本文提出了一种基于稀疏自编码器的框架，用于检索和控制大型语言模型内部与高级语义行为相关的可解释特征。", "motivation": "现有工作难以将内部特征可靠地链接到复杂的行为级语义属性控制上，作者希望通过新的方法解决这一挑战。", "method": "利用对比特征检索管道基于受控语义对立结合统计激活分析和生成验证来从稀疏激活空间中提取单义功能特征。", "result": "实验表明该方法能够精确双向调节模型行为，并且比现有激活调控方法更稳定、性能更好，同时发现了干预特定内部特征会引起多维语言维度的连贯可预测变化的现象。", "conclusion": "大型语言模型内部化了高级概念的深层次集成表示，并为复杂AI行为的调控提供了新的稳健机制路径。"}}
{"id": "2601.02972", "pdf": "https://arxiv.org/pdf/2601.02972", "abs": "https://arxiv.org/abs/2601.02972", "authors": ["Nathanaël Carraz Rakotonirina", "Ren Pang", "Neha Anna John", "Michael Bohlke-Schneider", "Momchil Hardalov"], "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.", "AI": {"tldr": "该论文提出了一种多阶段高效的推理方法，旨在减少大型语言模型的过度思考现象。", "motivation": "随着测试时间计算量增加，大型语言模型在生成中间链式思维（CoT）时容易变得冗长，增加了计算成本，并可能降低性能。", "method": "通过监督微调和引入轻量级奖励函数结合强化学习进行训练，使用自验证仅在有益的情况下鼓励生成。", "result": "该方法减少了响应长度，分别使8B模型减少28%，32B模型减少40%；同时仅导致性能下降1.6分和2.5分，在$\text{AUC}_{\text{OAA}}$方面优于其他高效推理方法。", "conclusion": "该论文提出的方法在准确性和响应长度之间找到了更好的平衡，证明了其有效性。"}}
{"id": "2601.02968", "pdf": "https://arxiv.org/pdf/2601.02968", "abs": "https://arxiv.org/abs/2601.02968", "authors": ["Qingxiang Liu", "Zhiqing Cui", "Xiaoliang Luo", "Yuqian Wu", "Zhuoyang Jiang", "Huaiyu Wan", "Sheng Sun", "Lvchun Wang", "Wei Yu", "Yuxuan Liang"], "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.", "AI": {"tldr": "该论文提出了一种名为RationaleTS的方法，用于时间序列推理中的上下文学习。", "motivation": "现有的时间序列推理的多模态大型语言模型在性能上不足，原因是缺乏将时间观察与下游结果联系起来的理由先验知识，导致模型依赖于表面模式匹配而不是原则性推理。", "method": "首先诱导出由可观察证据到潜在结果的推理路径组成的条件标签理由。然后设计混合检索以平衡时间模式和语义背景，为新样本进行最终上下文推断而检索相关理由先验。", "result": "在三个领域的时间序列推理任务上进行了广泛的实验，并展示了所提出方法的有效性和效率。", "conclusion": "提出了新的RationaleTS方法，用于改善多模态大型语言模型时间序列推理的性能。"}}
{"id": "2601.02967", "pdf": "https://arxiv.org/pdf/2601.02967", "abs": "https://arxiv.org/abs/2601.02967", "authors": ["Yishu Lei", "Shuwei He", "Jing Hu", "Dan Zhang", "Xianlong Luo", "Danxiang Zhu", "Shikun Feng", "Rui Liu", "Jingzhou He", "Yu Sun", "Hua Wu", "Haifeng Wang"], "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "13 pages, 5 figures", "summary": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.", "AI": {"tldr": "本文提出了MoE适配器，用于解决大音频语言模型中梯度冲突的问题，并实现在音频语义和副语言任务上的优越性能。", "motivation": "现有研究使用密集、参数共享的适配器来处理异构声学信息时存在梯度冲突问题，为此提出了一种稀疏混合专家架构以解耦声学信息。", "method": "本文提出了MoE-Adapter，一种动态门控机制将音频令牌路由到捕获互补特征子空间的专门专家和保持全局上下文共享专家的方法，从而缓解了梯度冲突并支持细粒度特征学习。", "result": "实验表明，该模型在音频语义和副语言任务上均优于密集线性基线，并且计算成本相当。", "conclusion": "MoE-Adapter有效解决了大音频语言模型中的梯度冲突问题，在多个音频任务上表现出色。"}}
{"id": "2601.02965", "pdf": "https://arxiv.org/pdf/2601.02965", "abs": "https://arxiv.org/abs/2601.02965", "authors": ["Phat Tran", "Phuoc Pham", "Hung Trinh", "Tho Quan"], "title": "Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.", "AI": {"tldr": "通过OCR技术提高Bahnar语言文档的数字化精度", "motivation": "保护面临数据匮乏挑战的少数族裔Bahnar语言，改善其文献的数字保存", "method": "结合表格和非表格检测算法及概率校正后处理方法提升识别准确性", "result": "实验结果显示识别准确率从72.86%提高至79.26%", "conclusion": "贡献了重要的资源用于Bahnar语言保护，并为其他少数族裔语言的数字化提供了框架"}}
{"id": "2601.02954", "pdf": "https://arxiv.org/pdf/2601.02954", "abs": "https://arxiv.org/abs/2601.02954", "authors": ["Yuhuan You", "Lai Wei", "Xihong Wu", "Tianshu Qu"], "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models", "categories": ["cs.SD", "cs.AI"], "comment": null, "summary": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.", "AI": {"tldr": "本文提出了一种使大型音频语言模型理解复杂声学场景的框架，特别是空间维度。", "motivation": "现有大型音视频模型仅能处理单一音频流，缺乏对关键的空间维度的理解能力。为了弥补这一不足，作者提出了一个能够增强模型空间感知的新方法。", "method": "首先引入了一个层次化的听觉场景分析（ASA）框架；其次构建了大规模的双耳合成音频数据集以提供丰富的空间提示；再设计了一种混合特征投影器，利用平行语义和空间编码器提取解耦表示，并通过密集融合机制使模型获得声学场景的整体视图；最后采用从监督微调到基于小组相对策略优化（GRPO）的强化学习的渐进式训练课程。", "result": "在全面基准测试中，模型展示了较强的空间理解能力，为大型模型的空间感知提供了明确路径。", "conclusion": "通过使模型具备空间感知能力，本文工作开启了将强大推理能力应用于完整声学场景分析的新篇章。"}}
{"id": "2601.02950", "pdf": "https://arxiv.org/pdf/2601.02950", "abs": "https://arxiv.org/abs/2601.02950", "authors": ["Xuan Yang", "Furong Jia", "Roy Xie", "Xiong Xi", "Hengwei Bian", "Jian Li", "Monica Agrawal"], "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.", "AI": {"tldr": "本文介绍了一种名为Batch-of-Thought (BoT) 的方法，该方法通过联合处理相关查询来实现跨实例学习，提升大型语言模型的推理能力。", "motivation": "当前的语言模型在处理查询时是独立进行的，忽略了有价值的跨实例信号。作者希望通过联合处理相关查询来利用这些信号，提高模型的推理质量和效率。", "method": "Batch-of-Thought (BoT) 是一种不需额外训练的方法，通过批处理方式比较分析相关查询以发现高质量的推理模板、检测错误并通过一致性检查降低计算成本。在多代理反思架构（BoT-R）中，一个反射器执行联合评估来解锁孤立处理中无法获得的信息。", "result": "实验表明，BoT-R 在三个模型家族和六个基准测试上都能提高准确性，校准置信度，并最多减少61%的推理成本。", "conclusion": "本文提出的方法展示了在大型语言模型中批处理感知推理的优势及其潜在影响。"}}
{"id": "2601.02948", "pdf": "https://arxiv.org/pdf/2601.02948", "abs": "https://arxiv.org/abs/2601.02948", "authors": ["Matti Vahs", "Jaeyoun Choi", "Niklas Schmid", "Jana Tumova", "Chuchu Fan"], "title": "Parameter-Robust MPPI for Safe Online Learning of Unknown Parameters", "categories": ["cs.RO"], "comment": null, "summary": "Robots deployed in dynamic environments must remain safe even when key physical parameters are uncertain or change over time. We propose Parameter-Robust Model Predictive Path Integral (PRMPPI) control, a framework that integrates online parameter learning with probabilistic safety constraints. PRMPPI maintains a particle-based belief over parameters via Stein Variational Gradient Descent, evaluates safety constraints using Conformal Prediction, and optimizes both a nominal performance-driven and a safety-focused backup trajectory in parallel. This yields a controller that is cautious at first, improves performance as parameters are learned, and ensures safety throughout. Simulation and hardware experiments demonstrate higher success rates, lower tracking error, and more accurate parameter estimates than baselines.", "AI": {"tldr": "本文提出了Parameter-Robust Model Predictive Path Integral (PRMPPI) 控制框架，该框架在动态环境中通过在线参数学习和概率安全约束保持机器人运行的安全性。", "motivation": "当关键物理参数不确定或随时间变化时，部署于动态环境中的机器人必须确保自身安全性。传统的控制方法在这种情况下可能无法保证机器人的安全性能。", "method": "PRMPPI 控制框架使用粒子基础信念通过Stein变分梯度下降来估计未知参数，并利用符合预测技术评估安全约束条件。同时优化名义上的性能导向和安全导向的备份轨迹，确保机器人在学习参数的同时保持安全性。", "result": "仿真及硬件实验表明，相比于基线方法，PRMPPI 控制器能够实现更高的成功率、更小的跟踪误差以及更准确的参数估计。", "conclusion": "PRMPPI控制框架展示了其在动态环境中通过在线参数学习和安全约束保持机器人运行的有效性和安全性。"}}
{"id": "2601.02945", "pdf": "https://arxiv.org/pdf/2601.02945", "abs": "https://arxiv.org/abs/2601.02945", "authors": ["Xinyi Wei", "Sijing Wu", "Zitong Xu", "Yunhao Li", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "VTONQA: A Multi-Dimensional Quality Assessment Dataset for Virtual Try-on", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of e-commerce and digital fashion, image-based virtual try-on (VTON) has attracted increasing attention. However, existing VTON models often suffer from artifacts such as garment distortion and body inconsistency, highlighting the need for reliable quality evaluation of VTON-generated images. To this end, we construct VTONQA, the first multi-dimensional quality assessment dataset specifically designed for VTON, which contains 8,132 images generated by 11 representative VTON models, along with 24,396 mean opinion scores (MOSs) across three evaluation dimensions (i.e., clothing fit, body compatibility, and overall quality). Based on VTONQA, we benchmark both VTON models and a diverse set of image quality assessment (IQA) metrics, revealing the limitations of existing methods and highlighting the value of the proposed dataset. We believe that the VTONQA dataset and corresponding benchmarks will provide a solid foundation for perceptually aligned evaluation, benefiting both the development of quality assessment methods and the advancement of VTON models.", "AI": {"tldr": "构建了一个专门用于虚拟试衣（VTON）的多维度质量评估数据集VTONQA。", "motivation": "现有的VTON模型存在衣物变形和人体不一致等缺陷，需要可靠的质量评价方法来评估生成图像的质量。", "method": "创建了包含8132张由11个代表性VTON模型产生的图像及其多维度质量评分的数据集，并基于该数据集对多种IQA指标进行了基准测试。", "result": "揭示了现有方法的局限性，展示了VTONQA数据集中评估指标的价值。", "conclusion": "认为该数据集和相应的基准测试将为感知一致的评价提供坚实的基础，有利于质量评估方法的发展以及VTON模型的进步。"}}
{"id": "2601.02941", "pdf": "https://arxiv.org/pdf/2601.02941", "abs": "https://arxiv.org/abs/2601.02941", "authors": ["Jake Feiglin", "Guy Dar"], "title": "SastBench: A Benchmark for Testing Agentic SAST Triage", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.", "AI": {"tldr": "SastBench是一个用于评估自动化静态应用安全测试（SAST）误报过滤代理的基准。", "motivation": "尽管SAST工具在网络安全中广泛使用，但由于它们生成大量的假阳性结果，需要进行耗时的人工筛选。现有的LLM驱动的安全任务自动化代理缺乏实用性的验证标准。", "method": "开发了一个结合真实CVE作为真阳性和过滤后的SAST工具结果作为近似假阳性的数据集，并评估不同代理在此基准上的表现。", "result": "提供了对不同安全代理在SastBench上性能的比较分析以及详细的实验结果展示。", "conclusion": "提出了一个新颖的用于自动化SAST误报过滤任务评估的数据集，旨在促进更有效的网络安全工具开发和验证。"}}
{"id": "2601.02933", "pdf": "https://arxiv.org/pdf/2601.02933", "abs": "https://arxiv.org/abs/2601.02933", "authors": ["Vilém Zouhar", "Tom Kocmi"], "title": "Pearmut: Human Evaluation of Translation Made Trivial", "categories": ["cs.CL", "cs.HC"], "comment": "typeset with Typst", "summary": "Human evaluation is the gold standard for multilingual NLP, but is often skipped in practice and substituted with automatic metrics, because it is notoriously complex and slow to set up with existing tools with substantial engineering and operational overhead. We introduce Pearmut, a lightweight yet feature-rich platform that makes end-to-end human evaluation as easy to run as automatic evaluation. Pearmut removes common entry barriers and provides support for evaluating multilingual tasks, with a particular focus on machine translation. The platform implements standard evaluation protocols, including DA, ESA, or MQM, but is also extensible to allow prototyping new protocols. It features document-level context, absolute and contrastive evaluation, attention checks, ESAAI pre-annotations and both static and active learning-based assignment strategies. Pearmut enables reliable human evaluation to become a practical, routine component of model development and diagnosis rather than an occasional effort.", "AI": {"tldr": "介绍了一种轻量级且功能丰富的平台Pearmut，用于简化人类评估过程。", "motivation": "现有工具在设置人工评估时存在大量工程和操作负担，导致人们倾向于使用自动指标代替人工评估。因此，需要一种易于使用的平台来支持多语言任务的人工评估，并特别关注机器翻译。", "method": "Pearmut实现了标准的评估协议并提供文档级上下文、绝对对比评估、注意力检查等特性，同时允许用户原型化新的评估协议。它还使用静态和主动学习分配策略进行人机交互评估。", "result": "Pearmut使得可靠的人工评价成为模型开发和诊断中的一种实用常规组件，而非偶尔的努力。", "conclusion": "通过引入Pearmut平台，人工评估变得更加简单快速，并且能够作为模型改进的重要环节。"}}
{"id": "2601.02928", "pdf": "https://arxiv.org/pdf/2601.02928", "abs": "https://arxiv.org/abs/2601.02928", "authors": ["Md. Asif Hossain", "G M Mota-Tahrin Tayef", "Nabil Subhan"], "title": "HybridSolarNet: A Lightweight and Explainable EfficientNet-CBAM Architecture for Real-Time Solar Panel Fault Detection", "categories": ["cs.CV"], "comment": "5 page , 6 figures", "summary": "Manual inspections for solar panel systems are a tedious, costly, and error-prone task, making it desirable for Unmanned Aerial Vehicle (UAV) based monitoring. Though deep learning models have excellent fault detection capabilities, almost all methods either are too large and heavy for edge computing devices or involve biased estimation of accuracy due to ineffective learning techniques. We propose a new solar panel fault detection model called HybridSolarNet. It integrates EfficientNet-B0 with Convolutional Block Attention Module (CBAM). We implemented it on the Kaggle Solar Panel Images competition dataset with a tight split-before-augmentation protocol. It avoids leakage in accuracy estimation. We introduced focal loss and cosine annealing. Ablation analysis validates that accuracy boosts due to added benefits from CBAM (+1.53%) and that there are benefits from recognition of classes with imbalanced samples via focal loss. Overall average accuracy on 5-fold stratified cross-validation experiments on the given competition dataset topped 92.37% +/- 0.41 and an F1-score of 0.9226 +/- 0.39 compared to baselines like VGG19, requiring merely 16.3 MB storage, i.e., 32 times less. Its inference speed measured at 54.9 FPS with GPU support makes it a successful candidate for real-time UAV implementation. Moreover, visualization obtained from Grad-CAM illustrates that HybridSolarNet focuses on actual locations instead of irrelevant ones.", "AI": {"tldr": "提出了HybridSolarNet模型，用于实时太阳能面板故障检测。", "motivation": "手动检查太阳能系统耗时且成本高，而现有深度学习方法在边缘计算设备上应用受限或准确性估计不准确。", "method": "结合EfficientNet-B0和CBAM模块构建HybridSolarNet，并引入焦点损失函数和余弦退火技术。通过Kaggle数据集进行测试，采用严格的分割前增强协议以避免精度泄露问题。", "result": "在5折交叉验证实验中，平均准确率超过92.37%，F1值为0.9226；模型大小仅为16.3MB，推断速度达到54.9FPS。", "conclusion": "HybridSolarNet在保证准确性的同时，实现了轻量级和快速推理，适合实时无人机应用。"}}
{"id": "2601.02927", "pdf": "https://arxiv.org/pdf/2601.02927", "abs": "https://arxiv.org/abs/2601.02927", "authors": ["Iñaki Erregue", "Kamal Nasrollahi", "Sergio Escalera"], "title": "PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted to the 6th Workshop on Real-World Surveillance: Applications and Challenges (WACV 2025)", "summary": "Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.", "AI": {"tldr": "介绍了一种名为PrismVAU的轻量级系统，用于实时视频异常理解。", "motivation": "现有方法依赖于精细调整的多模态大型语言模型或外部模块，导致标注成本高、训练流程复杂且推理开销大。因此提出一种低成本高效的方法解决这些问题。", "method": "PrismVAU通过两个互补阶段工作：粗略异常评分模块基于文本锚点计算帧级异常得分；MLLM优化的细化模块通过系统和用户提示上下文化异常。二者均使用弱监督自动提示工程框架进行优化。", "result": "实验表明，PrismVAU在标准VAD基准测试中具有竞争力的检测性能，并提供可解释的异常说明，无需指令微调、帧级标注或外部模块密集处理。", "conclusion": "提出的方法PrismVAU是一种有效且实用的解决方案，适用于实时视频异常理解的实际应用。"}}
{"id": "2601.02924", "pdf": "https://arxiv.org/pdf/2601.02924", "abs": "https://arxiv.org/abs/2601.02924", "authors": ["Aihua Zheng", "Ya Gao", "Shihao Li", "Chenglong Li", "Jin Tang"], "title": "DCG ReID: Disentangling Collaboration and Guidance Fusion Representations for Multi-modal Vehicle Re-Identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal vehicle Re-Identification (ReID) aims to leverage complementary information from RGB, Near Infrared (NIR), and Thermal Infrared (TIR) modalities to retrieve the same vehicle. The challenges of multi-modal vehicle ReID arise from the uncertainty of modality quality distribution induced by inherent discrepancies across modalities, resulting in distinct conflicting fusion requirements for data with balanced and unbalanced quality distributions. Existing methods handle all multi-modal data within a single fusion model, overlooking the different needs of the two data types and making it difficult to decouple the conflict between intra-class consistency and inter-modal heterogeneity. To this end, we propose Disentangle Collaboration and Guidance Fusion Representations for Multi-modal Vehicle ReID (DCG-ReID). Specifically, to disentangle heterogeneous quality-distributed modal data without mutual interference, we first design the Dynamic Confidence-based Disentangling Weighting (DCDW) mechanism: dynamically reweighting three-modal contributions via interaction-derived modal confidence to build a disentangled fusion framework. Building on DCDW, we develop two scenario-specific fusion strategies: (1) for balanced quality distributions, Collaboration Fusion Module (CFM) mines pairwise consensus features to capture shared discriminative information and boost intra-class consistency; (2) for unbalanced distributions, Guidance Fusion Module (GFM) implements differential amplification of modal discriminative disparities to reinforce dominant modality advantages, guide auxiliary modalities to mine complementary discriminative info, and mitigate inter-modal divergence to boost multi-modal joint decision performance. Extensive experiments on three multi-modal ReID benchmarks (WMVeID863, MSVR310, RGBNT100) validate the effectiveness of our method. Code will be released upon acceptance.", "AI": {"tldr": "该论文提出了一种解耦异质质量分布多模态车辆ReID的方法，旨在解决不同质量分布数据的融合需求。", "motivation": "现有的方法无法区分处理平衡和不平衡质量分布的数据类型，并且难以在类内一致性与跨模态差异性之间进行分离。", "method": "通过设计动态置信度解耦加权机制（DCDW），并在其基础上开发两种场景特定的融合策略：平衡质量分布下采用协作融合模块（CFM）增强类内一致性，不平衡分布下采用指导融合模块（GFM）强化主导模态优势并减少跨模态差异。", "result": "实验在三个多模态ReID基准数据集上验证了所提方法的有效性。", "conclusion": "该论文提出了一种解耦异质质量分布的多模态车辆ReID方法，通过DCDW机制和特定场景融合策略，有效提升了跨模态联合决策性能。"}}
{"id": "2601.02918", "pdf": "https://arxiv.org/pdf/2601.02918", "abs": "https://arxiv.org/abs/2601.02918", "authors": ["Guoqiang Liang", "Jianyi Wang", "Zhonghua Wu", "Shangchen Zhou"], "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning", "categories": ["cs.CV"], "comment": "Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage", "summary": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.", "AI": {"tldr": "提出Zoom-IQA模型以解决图像质量评估中的可靠性问题，通过不确定性感知、区域推理和迭代细化来提升模型性能。", "motivation": "现有基于视觉语言模型的图像质量评分方法存在视觉与文本线索整合能力有限的问题，导致其推理不可靠。", "method": "采用两阶段训练流程：监督微调以使模型在关键区域内进行评估，并通过强化学习动态探索策略；使用KL覆盖正则化器和渐进重采样策略来防止推理多样化崩溃及标注偏差。", "result": "实验表明Zoom-IQA提升了鲁棒性、可解释性和泛化能力，在图像恢复等下游任务中表现出色。", "conclusion": "提出基于视觉语言模型的Zoom-IQA，通过认知行为模拟改进了图像质量评估方法。"}}
{"id": "2601.02917", "pdf": "https://arxiv.org/pdf/2601.02917", "abs": "https://arxiv.org/abs/2601.02917", "authors": ["Mengze Hong", "Di Jiang", "Jiangtao Wen", "Zhiyang Su", "Yawen Li", "Yanjie Sun", "Guan Wang", "Chen Jason Zhang"], "title": "RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the \"wisdom of the crowd\" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.", "AI": {"tldr": "本文提出了一种基于检索的框架RAL2M，通过重新定位LLMs作为查询响应匹配裁判来消除生成幻觉。", "motivation": "在LLM驱动的服务系统中，需要显式的知识基础以确保合规性保证的回答。为此引入了可以减少幻觉问题的新方法。", "method": "提出了一种新的框架RAL2M, 它重新定位了LLMs作为查询响应匹配的裁判，并采用一种适应于查询的潜在集成策略来缓解判断幻觉，此策略明确地建模了异构模型能力和LLMs之间的相互依赖关系。", "result": "实验表明该方法有效利用了“群体智慧”，并且显著优于强大的基线。", "conclusion": "讨论了最佳实践和未来研究中进一步开发潜在表示的有希望的方向。"}}
{"id": "2601.02914", "pdf": "https://arxiv.org/pdf/2601.02914", "abs": "https://arxiv.org/abs/2601.02914", "authors": ["Mengze Hong", "Di Jiang", "Zeying Xie", "Weiwei Zhao", "Guan Wang", "Chen Jason Zhang"], "title": "Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis", "categories": ["cs.SD", "cs.CR"], "comment": null, "summary": "As audio deepfakes transition from research artifacts to widely available commercial tools, robust biometric authentication faces pressing security threats in high-stakes industries. This paper presents a systematic empirical evaluation of state-of-the-art speaker authentication systems based on a large-scale speech synthesis dataset, revealing two major security vulnerabilities: 1) modern voice cloning models trained on very small samples can easily bypass commercial speaker verification systems; and 2) anti-spoofing detectors struggle to generalize across different methods of audio synthesis, leading to a significant gap between in-domain performance and real-world robustness. These findings call for a reconsideration of security measures and stress the need for architectural innovations, adaptive defenses, and the transition towards multi-factor authentication.", "AI": {"tldr": "评估基于大型语音合成数据集的最先进的说话人认证系统的漏洞，发现现代声音克隆模型可以轻易绕过商业说话人验证系统，并且防欺诈检测器在不同音频合成方法之间的泛化能力较差。", "motivation": "随着深度伪造技术从研究对象转变为广泛可用的商品工具，基于音频的身份认证面临着严峻的安全威胁。因此需要评估这些系统的安全性并提出改进方案。", "method": "通过对大规模语音合成数据集进行系统性实证分析，揭示了现有说话人验证系统和防欺诈检测器的两个主要安全漏洞。", "result": "发现现代声音克隆模型即使在训练样本极小的情况下也能轻易绕过商用说话人验证系统；同时，防欺诈检测器难以泛化到不同音频合成方法，导致真实环境中性能下降。", "conclusion": "研究结果表明了现有安全措施的不足，并强调需要通过架构创新、适应性防御和向多因素认证转变来加强身份认证的安全性。"}}
{"id": "2601.02908", "pdf": "https://arxiv.org/pdf/2601.02908", "abs": "https://arxiv.org/abs/2601.02908", "authors": ["Wei-Yuan Cheng", "Kai-Po Chang", "Chi-Pin Huang", "Fu-En Yang", "Yu-Chiang Frank Wang"], "title": "TA-Prompting: Enhancing Video Large Language Models for Dense Video Captioning via Temporal Anchors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages for main paper (exclude citation pages), 6 pages for appendix, totally 10 figures 7 tables and 2 algorithms. The paper is accepted by WACV 2026", "summary": "Dense video captioning aims to interpret and describe all temporally localized events throughout an input video. Recent state-of-the-art methods leverage large language models (LLMs) to provide detailed moment descriptions for video data. However, existing VideoLLMs remain challenging in identifying precise event boundaries in untrimmed videos, causing the generated captions to be not properly grounded. In this paper, we propose TA-Prompting, which enhances VideoLLMs via Temporal Anchors that learn to precisely localize events and prompt the VideoLLMs to perform temporal-aware video event understanding. During inference, in order to properly determine the output caption sequence from an arbitrary number of events presented within a video, we introduce an event coherent sampling strategy to select event captions with sufficient coherence across temporal events and cross-modal similarity with the given video. Through extensive experiments on benchmark datasets, we show that our TA-Prompting is favorable against state-of-the-art VideoLLMs, yielding superior performance on dense video captioning and temporal understanding tasks including moment retrieval and temporalQA.", "AI": {"tldr": "本文提出了TA-Prompting方法，通过时间锚点来增强视频大语言模型在密集视频描述中的性能。", "motivation": "现有视频大语言模型难以准确识别未剪辑视频中事件的时间边界，导致生成的字幕不够精确。", "method": "引入了时间锚点（Temporal Anchors）和事件一致性采样策略来提高模型对视频事件的理解能力和生成准确描述的能力。", "result": "通过在基准数据集上的大量实验表明，TA-Prompting方法在密集视频描述任务中优于当前最先进的视频大语言模型。", "conclusion": "本文提出的方法改进了现有视频大语言模型的性能，在密集视频描述、时刻检索和时间问答等任务上取得了更优的结果。"}}
{"id": "2601.02905", "pdf": "https://arxiv.org/pdf/2601.02905", "abs": "https://arxiv.org/abs/2601.02905", "authors": ["Sara Micol Ferraina", "Michele Brienza", "Francesco Argenziano", "Emanuele Musumeci", "Vincenzo Suriani", "Domenico D. Bloisi", "Daniele Nardi"], "title": "LOST-3DSG: Lightweight Open-Vocabulary 3D Scene Graphs with Semantic Tracking in Dynamic Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Tracking objects that move within dynamic environments is a core challenge in robotics. Recent research has advanced this topic significantly; however, many existing approaches remain inefficient due to their reliance on heavy foundation models. To address this limitation, we propose LOST-3DSG, a lightweight open-vocabulary 3D scene graph designed to track dynamic objects in real-world environments. Our method adopts a semantic approach to entity tracking based on word2vec and sentence embeddings, enabling an open-vocabulary representation while avoiding the necessity of storing dense CLIP visual features. As a result, LOST-3DSG achieves superior performance compared to approaches that rely on high-dimensional visual embeddings. We evaluate our method through qualitative and quantitative experiments conducted in a real 3D environment using a TIAGo robot. The results demonstrate the effectiveness and efficiency of LOST-3DSG in dynamic object tracking. Code and supplementary material are publicly available on the project website at https://lab-rococo-sapienza.github.io/lost-3dsg/.", "AI": {"tldr": "提出了一种轻量级的开放词汇3D场景图LOST-3DSG，用于在动态环境中跟踪移动对象。", "motivation": "现有方法依赖于重型基础模型，效率较低。为了解决这一问题，提出了一个基于语义的方法来更有效地处理动态环境中的物体追踪任务。", "method": "采用word2vec和句子嵌入技术进行实体的开放词汇表示，并且不存储密集的CLIP视觉特征，从而实现轻量级跟踪。", "result": "实验结果表明，LOST-3DSG在动态对象跟踪方面取得了优于依赖高维视觉嵌入方法的表现。", "conclusion": "通过采用语义化的实体追踪和开放词汇表示方式，LOST-3DSG展现了其在处理动态环境中的高效性和有效性。"}}
{"id": "2601.02902", "pdf": "https://arxiv.org/pdf/2601.02902", "abs": "https://arxiv.org/abs/2601.02902", "authors": ["Xinglang Zhang", "Yunyao Zhang", "ZeLiang Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": null, "summary": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.", "AI": {"tldr": "研究提出了逻辑相变现象，并提出了一种基于神经符号框架的适应性调优方法，以改善大语言模型在复杂逻辑推理任务中的性能。", "motivation": "探索大型语言模型在逻辑推理方面的局限性和改进策略。通过分析不同复杂度下逻辑推理的表现，揭示了逻辑相变现象。", "method": "提出了一种神经符号框架——Neuro-Symbolic Curriculum Tuning，该框架通过自适应地将自然语言与逻辑符号对齐，并围绕相变边界重新调整训练动力学来增强逐步增加的逻辑深度上的推理能力。", "result": "实验结果表明，在五个基准测试上，提出的调优方法能够有效缓解高复杂度下的逻辑推理崩溃问题，使准确率分别提高了1.26和3.95个百分点。", "conclusion": "研究揭示了大型语言模型在逻辑推理中存在未被认识的相变现象，并提出了一种有效的解决策略，以改善其在处理更复杂逻辑任务时的表现。"}}
{"id": "2601.02900", "pdf": "https://arxiv.org/pdf/2601.02900", "abs": "https://arxiv.org/abs/2601.02900", "authors": ["Taisei Takano", "Ryoya Yoshida"], "title": "SPO-CLAPScore: Enhancing CLAP-based alignment prediction system with Standardize Preference Optimization, for the first XACLE Challenge", "categories": ["cs.SD", "eess.AS"], "comment": "https://github.com/ttakano398/SPO-CLAPScore", "summary": "The first XACLE Challenge (x-to-audio alignment challenge) addresses the critical need for automatic evaluation metrics that correlate with human perception of audio-text semantic alignment. In this paper, we describe the \"Takano_UTokyo_03\" system submitted to XACLE Challenge. Our approach leverages a CLAPScore-based architecture integrated with a novel training method called Standardized Preference Optimization (SPO). SPO standardizes the raw alignment scores provided by each listener, enabling the model to learn relative preferences and mitigate the impact of individual scoring biases. Additionally, we employ listener screening to exclude listeners with inconsistent ratings. Experimental evaluations demonstrate that both SPO and listener screening effectively improve the correlation with human judgment. Our system achieved 6th place in the challenge with a Spearman's rank correlation coefficient (SRCC) of 0.6142, demonstrating competitive performance within a marginal gap from the top-ranked systems. The code is available at https://github.com/ttakano398/SPO-CLAPScore.", "AI": {"tldr": "提出了一种结合标准化偏好优化（SPO）和听众筛选的CLAPScore系统，用于XACLE挑战赛中的音频文本语义对齐预测。", "motivation": "为解决自动评估指标与人类感知音频-文本对齐质量之间的相关性问题，提出了改进的CLAPScore系统。", "method": "采用标准化的偏好优化（SPO）方法来处理听众评分的偏差，并结合了筛选不一致评分者的机制。", "result": "在XACLE挑战赛中取得了第六名的成绩，SRCC为0.6142，展示了与顶级系统的竞争性表现。", "conclusion": "该系统通过引入标准化偏好优化和听众筛查策略，在音频文本语义对齐预测方面达到了较好的效果。"}}
{"id": "2601.02881", "pdf": "https://arxiv.org/pdf/2601.02881", "abs": "https://arxiv.org/abs/2601.02881", "authors": ["Jakob Lønborg Christensen", "Morten Rieger Hannemose", "Anders Bjorholm Dahl", "Vedrana Andersen Dahl"], "title": "Towards Agnostic and Holistic Universal Image Segmentation with Bit Diffusion", "categories": ["cs.CV"], "comment": "Accepted at NLDL 26", "summary": "This paper introduces a diffusion-based framework for universal image segmentation, making agnostic segmentation possible without depending on mask-based frameworks and instead predicting the full segmentation in a holistic manner. We present several key adaptations to diffusion models, which are important in this discrete setting. Notably, we show that a location-aware palette with our 2D gray code ordering improves performance. Adding a final tanh activation function is crucial for discrete data. On optimizing diffusion parameters, the sigmoid loss weighting consistently outperforms alternatives, regardless of the prediction type used, and we settle on x-prediction. While our current model does not yet surpass leading mask-based architectures, it narrows the performance gap and introduces unique capabilities, such as principled ambiguity modeling, that these models lack. All models were trained from scratch, and we believe that combining our proposed improvements with large-scale pretraining or promptable conditioning could lead to competitive models.", "AI": {"tldr": "本文提出了一种基于扩散模型的框架，用于通用图像分割任务。", "motivation": "传统方法依赖于掩码框架进行图像分割，而这种方法旨在实现不依赖于掩码的全图分割，提高鲁棒性和灵活性。", "method": "通过引入位置感知调色板和2D灰度代码排序来适应扩散模型，并在预测中添加tanh激活函数。优化扩散参数时采用sigmoid损失加权方法，最终选择x-prediction方式。", "result": "尽管当前模型尚未超越领先的掩码框架，但其性能差距正在缩小，并引入了模棱两可建模等独特能力。", "conclusion": "结合大规模预训练或条件提示可以进一步提高该方法的竞争力。"}}
{"id": "2601.02880", "pdf": "https://arxiv.org/pdf/2601.02880", "abs": "https://arxiv.org/abs/2601.02880", "authors": ["Abhishek HS", "Pavan C Shekar", "Arpit Jain", "Ashwanth Krishnan"], "title": "ReTreVal: Reasoning Tree with Validation -- A Hybrid Framework for Enhanced LLM Multi-Step Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "14 pages, 1 figure, 5 tables", "summary": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.", "AI": {"tldr": "提出了一种名为ReTreVal的混合框架，用于增强大型语言模型在多步推理中的表现。", "motivation": "当前的方法如ReAct、Reflexion和Self-Refine虽然改进了迭代细化和反思，但缺乏结构化探索替代解决方案路径的能力，也没有跨问题学习能力。", "method": "提出了一种结合树型思维探索、自我精炼、基于LLM的批评评分以及反射记忆机制的框架ReTreVal。该方法构建了一个根据问题复杂性自适应调整深度的推理树，并通过LLM生成反馈进行迭代自我批判和精炼，同时使用双重验证机制评估每个节点的质量。", "result": "在500个数学题和创意写作任务上与现有方法比较中，ReTreVal表现出色，特别是在需要探索性推理、严格验证和知识转移的任务上。", "conclusion": "通过结构化探索、批评驱动的精炼以及跨问题记忆机制，ReTreVal优于现有的多步推理方法。"}}
{"id": "2601.02873", "pdf": "https://arxiv.org/pdf/2601.02873", "abs": "https://arxiv.org/abs/2601.02873", "authors": ["Arthur Haffemayer", "Alexandre Chapin", "Armand Jordana", "Krzysztof Wojciechowski", "Florent Lamiraux", "Nicolas Mansard", "Vladimir Petrik"], "title": "Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion", "categories": ["cs.RO"], "comment": "An open-source implementation is provided https://cozy-fairy-0e0139.netlify.app/", "summary": "Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.", "AI": {"tldr": "本文提出了一种结合扩散模型和碰撞感知预测控制的方法，用于在复杂环境中实现高效且可靠的运动规划。", "motivation": "传统基于优化的控制器难以快速生成多障碍物环境中的可行解。扩散模型可以产生绕过障碍物的不同轨迹，但缺乏有效的场景结构条件化方法。", "method": "本文结合了对象中心的扩散模型和碰撞感知预测控制，使用槽注意机制将场景简化为紧凑的障碍表示。优化问题进一步细化样本轨迹以确保物理约束。", "result": "在基准任务上，该方法比采样规划器或单独组件表现更好，成功率更高且延迟更低。真实机器人实验验证了其可靠性和安全性。", "conclusion": "本文提出的方法能在复杂环境下的实时运动控制中达到高效率和可靠性，适用于实际应用中的精确操作。"}}
{"id": "2601.02872", "pdf": "https://arxiv.org/pdf/2601.02872", "abs": "https://arxiv.org/abs/2601.02872", "authors": ["Ziyang Chen", "Xing Wu", "Junlong Jia", "Chaochen Gao", "Qi Fu", "Debing Zhang", "Songlin Hu"], "title": "LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the \"thinking\" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.", "AI": {"tldr": "开发一个更现实和全面的双语长上下文评估基准LongBench Pro。", "motivation": "现有长上下文评估基准在可扩展性和现实性之间做出了取舍，无法代表真实世界的复杂性。为了填补这一空白，引入了LongBench Pro来更好地模拟实际场景。", "method": "使用人机协作构建管道生成自然发生的长上下文样本，并通过专家验证确保质量与规模的平衡。", "result": "评估46种广泛使用的长上下文LLM后发现：1）长上下文优化比参数扩展对理解更有帮助；2）有效上下文长度通常小于宣称的长度，不同语言之间存在明显的错位；3）\"思考\"范式主要有助于经过本族语训练的模型。", "conclusion": "LongBench Pro提供了一个强大的测试平台，以推动长上下文理解的进步。"}}
{"id": "2601.02871", "pdf": "https://arxiv.org/pdf/2601.02871", "abs": "https://arxiv.org/abs/2601.02871", "authors": ["Zhiyong Cao", "Dunqiang Liu", "Qi Dai", "Haojun Xu", "Huaiyan Xu", "Huan He", "Yafei Liu", "Siyuan Liu", "XiaoLin Lin", "Ke Ma", "Ruqian Shi", "Sijia Yao", "Hao Wang", "Sicheng Zhou"], "title": "SimRPD: Optimizing Recruitment Proactive Dialogue Agents through Simulator-Based Data Evaluation and Selection", "categories": ["cs.AI"], "comment": null, "summary": "Task-oriented proactive dialogue agents play a pivotal role in recruitment, particularly for steering conversations towards specific business outcomes, such as acquiring social-media contacts for private-channel conversion. Although supervised fine-tuning and reinforcement learning have proven effective for training such agents, their performance is heavily constrained by the scarcity of high-quality, goal-oriented domain-specific training data. To address this challenge, we propose SimRPD, a three-stage framework for training recruitment proactive dialogue agents. First, we develop a high-fidelity user simulator to synthesize large-scale conversational data through multi-turn online dialogue. Then we introduce a multi-dimensional evaluation framework based on Chain-of-Intention (CoI) to comprehensively assess the simulator and effectively select high-quality data, incorporating both global-level and instance-level metrics. Finally, we train the recruitment proactive dialogue agent on the selected dataset. Experiments in a real-world recruitment scenario demonstrate that SimRPD outperforms existing simulator-based data selection strategies, highlighting its practical value for industrial deployment and its potential applicability to other business-oriented dialogue scenarios.", "AI": {"tldr": "本文提出SimRPD，一种用于优化招聘主动对话代理的三阶段框架。", "motivation": "为了应对招聘主动对话代理训练中高质量目标导向数据稀缺的问题，开发了一种新的方法来生成和选择高质量的数据集。", "method": "首先通过多轮在线对话发展出一个高度逼真的用户模拟器来合成大规模会话数据。接着引入基于意图链的多层次评估框架以全面评估模拟器并有效选出高价值数据。最后在选中的数据集上训练招聘主动对话代理。", "result": "实验结果显示，SimRPD比现有的模拟器基线策略表现更好，在现实世界的应用场景中显示出了实际的价值。", "conclusion": "SimRPD框架证明了其在工业部署上的实用性，并且对其他业务导向的对话情景也具有潜在应用价值。"}}
{"id": "2601.02864", "pdf": "https://arxiv.org/pdf/2601.02864", "abs": "https://arxiv.org/abs/2601.02864", "authors": ["Shovini Guha", "Dwaipayan Nandi"], "title": "Lesion Segmentation in FDG-PET/CT Using Swin Transformer U-Net 3D: A Robust Deep Learning Framework", "categories": ["eess.IV", "cs.CV"], "comment": "8 pages, 3 figures, 3 tables", "summary": "Accurate and automated lesion segmentation in Positron Emission Tomography / Computed Tomography (PET/CT) imaging is essential for cancer diagnosis and therapy planning. This paper presents a Swin Transformer UNet 3D (SwinUNet3D) framework for lesion segmentation in Fluorodeoxyglucose Positron Emission Tomography / Computed Tomography (FDG-PET/CT) scans. By combining shifted window self-attention with U-Net style skip connections, the model captures both global context and fine anatomical detail. We evaluate SwinUNet3D on the AutoPET III FDG dataset and compare it against a baseline 3D U-Net. Results show that SwinUNet3D achieves a Dice score of 0.88 and IoU of 0.78, surpassing 3D U-Net (Dice 0.48, IoU 0.32) while also delivering faster inference times. Qualitative analysis demonstrates improved detection of small and irregular lesions, reduced false positives, and more accurate PET/CT fusion. While the framework is currently limited to FDG scans and trained under modest GPU resources, it establishes a strong foundation for future multi-tracer, multi-center evaluations and benchmarking against other transformer-based architectures. Overall, SwinUNet3D represents an efficient and robust approach to PET/CT lesion segmentation, advancing the integration of transformer-based models into oncology imaging workflows.", "AI": {"tldr": "本文提出了一种基于Swin Transformer的三维U-Net框架(SwinUNet3D)，用于FDG-PET/CT扫描中的病灶分割。", "motivation": "准确和自动化的病灶分割在PET/CT成像中对癌症诊断和治疗规划至关重要，为此研究提出了结合移位窗口自注意力机制与U-Net跳过连接的SwinUNet3D框架来改进PET/CT图像病变检测。", "method": "通过将移位窗口自注意机制与传统的三维U-Net架构相结合，构建了新的SwinUNet3D模型以提高对病灶区域全局特征及细微解剖结构的理解能力。", "result": "实验结果显示，在AutoPET III FDG数据集上测试时，SwinUNet3D的Dice系数达到0.88，IoU值为0.78，明显优于标准三维U-Net(分别为0.48和0.32)，并且具有更快的推理速度。", "conclusion": "该研究成功开发了适用于PET/CT病灶分割任务的SwinUNet3D方法，并认为它为未来多示踪剂、跨中心评估及与其他基于变换器模型的竞争奠定了坚实的基础，有助于推动肿瘤成像工作流程中的应用。"}}
{"id": "2601.02857", "pdf": "https://arxiv.org/pdf/2601.02857", "abs": "https://arxiv.org/abs/2601.02857", "authors": ["Chunzheng Wang", "Yiyuan Zhang", "Annan Tang", "Ziqiu Zeng", "Haoran Chen", "Quan Gao", "Zixuan Zhuang", "Boyu Li", "Zhilin Xiong", "Aoqian Zhang", "Ce Hao", "Siyuan Luo", "Tongyang Zhao", "Cecilia Laschi", "Fan Shi"], "title": "Soft Responsive Materials Enhance Humanoid Safety", "categories": ["cs.RO"], "comment": "40 pages, 11 figures", "summary": "Humanoid robots are envisioned as general-purpose platforms in human-centered environments, yet their deployment is limited by vulnerability to falls and the risks posed by rigid metal-plastic structures to people and surroundings. We introduce a soft-rigid co-design framework that leverages non-Newtonian fluid-based soft responsive materials to enhance humanoid safety. The material remains compliant during normal interaction but rapidly stiffens under impact, absorbing and dissipating fall-induced forces. Physics-based simulations guide protector placement and thickness and enable learning of active fall policies. Applied to a 42 kg life-size humanoid, the protector markedly reduces peak impact and allows repeated falls without hardware damage, including drops from 3 m and tumbles down long staircases. Across diverse scenarios, the approach improves robot robustness and environmental safety. By uniting responsive materials, structural co-design, and learning-based control, this work advances interact-safe, industry-ready humanoid robots.", "AI": {"tldr": "本文介绍了一种软刚性共设计框架，利用非牛顿流体基软响应材料增强人形机器人的安全性。", "motivation": "在人类中心的环境中部署的人形机器人由于易受摔倒影响和由坚硬的金属塑料结构带来的风险而受到限制。", "method": "通过物理模拟引导保护器的位置和厚度，并学习主动防跌策略，应用于42公斤大小的实际人形机器人中。", "result": "该方法显著降低了峰值冲击力，使机器人在多次跌落过程中不会损坏硬件，包括从3米高的地方掉落或滚下长楼梯的情况下。", "conclusion": "这项工作通过结合响应材料、结构共设计和基于学习的控制技术，推进了互动安全的人形机器人开发，并使其适用于工业应用。"}}
{"id": "2601.02854", "pdf": "https://arxiv.org/pdf/2601.02854", "abs": "https://arxiv.org/abs/2601.02854", "authors": ["Ao Li", "Jinghui Zhang", "Luyu Li", "Yuxiang Duan", "Lang Gao", "Mingcai Chen", "Weijun Qin", "Shaopeng Li", "Fengxian Ji", "Ning Liu", "Lizhen Cui", "Xiuying Chen", "Yuntao Du"], "title": "M3MAD-Bench: Are Multi-Agent Debates Really Effective Across Domains and Modalities?", "categories": ["cs.AI"], "comment": null, "summary": "As an agent-level reasoning and coordination paradigm, Multi-Agent Debate (MAD) orchestrates multiple agents through structured debate to improve answer quality and support complex reasoning. However, existing research on MAD suffers from two fundamental limitations: evaluations are conducted under fragmented and inconsistent settings, hindering fair comparison, and are largely restricted to single-modality scenarios that rely on textual inputs only. To address these gaps, we introduce M3MAD-Bench, a unified and extensible benchmark for evaluating MAD methods across Multi-domain tasks, Multi-modal inputs, and Multi-dimensional metrics. M3MAD-Bench establishes standardized protocols over five core task domains: Knowledge, Mathematics, Medicine, Natural Sciences, and Complex Reasoning, and systematically covers both pure text and vision-language datasets, enabling controlled cross-modality comparison. We evaluate MAD methods on nine base models spanning different architectures, scales, and modality capabilities. Beyond accuracy, M3MAD-Bench incorporates efficiency-oriented metrics such as token consumption and inference time, providing a holistic view of performance--cost trade-offs. Extensive experiments yield systematic insights into the effectiveness, robustness, and efficiency of MAD across text-only and multimodal scenarios. We believe M3MAD-Bench offers a reliable foundation for future research on standardized MAD evaluation. The code is available at http://github.com/liaolea/M3MAD-Bench.", "AI": {"tldr": "构建M3MAD-Bench，用于评估多智能体辩论方法在跨领域和模态上的效果。", "motivation": "现有的多智能体辩论研究存在评测标准不统一、仅限单模态输入的问题。因此，需要一个标准化的基准来公平对比和全面评估这些方法。", "method": "开发了一个名为M3MAD-Bench的框架，在五个核心任务领域中建立了标准化协议，并覆盖了纯文本及视觉语言数据集，以支持跨模式比较。此外，该框架还涵盖了效率相关指标，如令牌消耗时间和推理时间。", "result": "实验结果显示，多智能体辩论方法在单一模态和多模态场景中的有效性、鲁棒性和效率都有所体现，并且提供了性能与成本之间的全面视角。", "conclusion": "M3MAD-Bench为标准化的多智能体辩论评估提供了一个可靠的平台。"}}
{"id": "2601.02850", "pdf": "https://arxiv.org/pdf/2601.02850", "abs": "https://arxiv.org/abs/2601.02850", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Sample-Efficient Neurosymbolic Deep Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) is a well-established framework for sequential decision-making in complex environments. However, state-of-the-art Deep RL (DRL) algorithms typically require large training datasets and often struggle to generalize beyond small-scale training scenarios, even within standard benchmarks. We propose a neuro-symbolic DRL approach that integrates background symbolic knowledge to improve sample efficiency and generalization to more challenging, unseen tasks. Partial policies defined for simple domain instances, where high performance is easily attained, are transferred as useful priors to accelerate learning in more complex settings and avoid tuning DRL parameters from scratch. To do so, partial policies are represented as logical rules, and online reasoning is performed to guide the training process through two mechanisms: (i) biasing the action distribution during exploration, and (ii) rescaling Q-values during exploitation. This neuro-symbolic integration enhances interpretability and trustworthiness while accelerating convergence, particularly in sparse-reward environments and tasks with long planning horizons. We empirically validate our methodology on challenging variants of gridworld environments, both in the fully observable and partially observable setting. We show improved performance over a state-of-the-art reward machine baseline.", "AI": {"tldr": "本文提出了一种神经符号深度强化学习方法，结合背景知识以提高样本效率和泛化能力。", "motivation": "当前先进的深度强化学习算法通常需要大量训练数据，并且在小型训练场景之外的泛化表现不佳。文章旨在通过引入背景符号知识来增强模型的样本效率和对更具挑战性的未见过任务的泛化能力。", "method": "该方法将部分策略表示为逻辑规则，通过偏置动作分布进行探索以及调整Q值来进行在线推理，从而加速复杂环境下的学习过程。", "result": "实验表明，所提方法在具有长规划时间和稀疏奖励的任务中表现优于最先进的奖赏机基准。", "conclusion": "神经符号集成增强了可解释性和可信性，并且能够更快地收敛，特别是在具有长时间规划和稀疏奖励的环境中。"}}
{"id": "2601.02845", "pdf": "https://arxiv.org/pdf/2601.02845", "abs": "https://arxiv.org/abs/2601.02845", "authors": ["Kai Li", "Xuanqing Yu", "Ziyi Ni", "Yi Zeng", "Yao Xu", "Zheqing Zhang", "Xin Li", "Jitao Sang", "Xiaogang Duan", "Xuelei Wang", "Chengbao Liu", "Jie Tan"], "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.", "AI": {"tldr": "该论文提出了TiMem，一种用于长时对话代理的时空分层记忆框架。", "motivation": "现有记忆框架在处理时间结构化信息方面支持有限，导致记忆碎片化和长期个性化不稳定。", "method": "TiMem通过时空记忆树（TMT）系统地组织对话，实现从原始对话观察到逐步抽象的人物表示的记忆整合。它具备三个核心特性：时空分层组织、语义引导的整合以及复杂性感知的记忆召回。", "result": "在一致的评估设置下，TiMem在LoCoMo和LongMemEval-S基准测试中达到了75.30%和76.88%，超越所有基线方法，并减少了52.20%的记忆调用长度。", "conclusion": "TiMem将时间连续性作为长时记忆的一级组织原则，适用于对话代理的长期记忆管理。"}}
{"id": "2601.02837", "pdf": "https://arxiv.org/pdf/2601.02837", "abs": "https://arxiv.org/abs/2601.02837", "authors": ["Yuteng Liu", "Duanni Meng", "Maoxun Yuan", "Xingxing Wei"], "title": "Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.", "AI": {"tldr": "重新思考红外小目标检测中的查询初始化，以提高目标定位的准确性。", "motivation": "现有DETR基检测器在处理低信噪比、小尺寸和复杂背景的情况下表现不佳。自注意力机制导致相关目标嵌入被主导背景特征淹没，影响了查询初始化的可靠性及目标定位准确性。", "method": "提出SEF-DETR框架，包括频率导向补丁筛选（FPS）、动态嵌入增强（DEE）与可靠一致性感知融合（RCF）。通过构建目标相关的密度图来抑制背景特征；加强多尺度表征；进一步优化对象查询以提高空间-频域一致性和可靠性。", "result": "在三个公开的红外小目标检测数据集上，SEF-DETR取得了优于现有方法的结果，展示了对红外小目标检测任务的有效性。", "conclusion": "通过改进查询初始化策略，解决了自注意力机制导致的目标定位问题，提供了高效鲁棒的解决方案。"}}
{"id": "2601.02836", "pdf": "https://arxiv.org/pdf/2601.02836", "abs": "https://arxiv.org/abs/2601.02836", "authors": ["Klaus Jansen", "Felix Ohnesorge"], "title": "A Practical 73/50 Approximation for Contiguous Monotone Moldable Job Scheduling", "categories": ["cs.DS"], "comment": "to appear in STACS 2026", "summary": "In moldable job scheduling, we are provided $m$ identical machines and $n$ jobs that can be executed on a variable number of machines. The execution time of each job depends on the number of machines assigned to execute that job. For the specific problem of monotone moldable job scheduling, jobs are assumed to have a processing time that is non-increasing in the number of machines. The previous best-known algorithms are: (1) a polynomial-time approximation scheme with time complexity $Ω(n^{g(1/\\varepsilon)})$, where $g(\\cdot)$ is a super-exponential function [Jansen and Thöle '08; Jansen and Land '18], (2) a fully polynomial approximation scheme for the case of $m \\geq 8\\frac{n}{\\varepsilon}$ [Jansen and Land '18], and (3) a $\\frac{3}{2}$ approximation with time complexity $O(nm\\log(mn))$ [Wu, Zhang, and Chen '23]. We present a new practically efficient algorithm with an approximation ratio of $\\approx (1.4593 + \\varepsilon)$ and a time complexity of $O(nm \\log \\frac{1}{\\varepsilon})$. Our result also applies to the contiguous variant of the problem. In addition to our theoretical results, we implement the presented algorithm and show that the practical performance is significantly better than the theoretical worst-case approximation ratio.", "AI": {"tldr": "本文提出了一种新的近似算法，用于解决单调可塑作业调度问题，并证明了该算法具有较好的理论和实践性能。", "motivation": "现有算法在时间和复杂度上存在不足或不够实用。为此，作者旨在开发一种更有效的、理论与实践均表现优良的近似算法来应对这一挑战。", "method": "通过设计一个新的算法框架并优化其时间复杂度，使得该方法能够在实际应用中获得更好的性能。", "result": "新算法具有约1.4593+ε的近似比，并且在时间和空间上的效率得到了显著改善。实验结果显示，在实践中表现优于理论上限定的结果。", "conclusion": "本文提出的算法不仅在理论上给出了改进，而且通过实际测试证明了其在实践中的优越性。"}}
{"id": "2601.02831", "pdf": "https://arxiv.org/pdf/2601.02831", "abs": "https://arxiv.org/abs/2601.02831", "authors": ["Yuetong Li", "Qing Zhang", "Yilin Zhao", "Gongyang Li", "Zeming Liu"], "title": "DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting\" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.", "AI": {"tldr": "DGA-Net通过引入深度提示和图锚引导，增强了Segment Anything Model（SAM）在伪装目标检测中的性能。", "motivation": "为了更好地利用深度线索以提高伪装物体检测的准确性，作者提出了一种新的框架来增强现有技术。", "method": "该方法包括交叉模态图形增强(CGE)模块和锚引导精炼(AGR)模块，前者在异构图中综合了RGB语义与深度几何信息，后者则通过构建全局锚点并通过非局部路径将其指导信号从深层传播到浅层，以确保精确且一致的分割。", "result": "实验证明DGA-Net优于当前最佳伪装物体检测方法。", "conclusion": "通过引入深度提示和图锚引导机制，DGA-Net在增强Segment Anything Model（SAM）用于伪装目标检测方面表现出色。"}}
{"id": "2601.02829", "pdf": "https://arxiv.org/pdf/2601.02829", "abs": "https://arxiv.org/abs/2601.02829", "authors": ["Jialin Wang", "Xinru Cheng", "Boyong Hou", "Hai-Ning Liang"], "title": "Resolution deficits drive simulator sickness and compromise reading performance in virtual environments", "categories": ["cs.HC", "cs.GR", "cs.MM"], "comment": "18 pages, 7 figures, 7 tables", "summary": "Extended reality (XR) is evolving into a general-purpose computing platform, yet its adoption for productivity is hindered by visual fatigue and simulator sickness. While these symptoms are often attributed to latency or motion conflicts, the precise impact of textual clarity on physiological comfort remains undefined. Here we show that sub-optimal effective resolution, the clarity that reaches the eye after the full display-optics-rendering pipeline, is a primary driver of simulator sickness during reading tasks in both virtual reality and video see-through environments. By systematically manipulating end-to-end effective resolution on a unified logMAR scale, we measured reading psychophysics and sickness symptoms in a controlled within-subjects study. We find that reading performance and user comfort degrade exponentially as resolution drops below 0 logMAR (normal visual acuity). Notably, our results reveal 0 logMAR as a key physiological tipping point: resolutions better than this threshold yield naked-eye-level performance with minimal sickness, whereas poorer resolutions trigger rapid, non-linear increases in nausea and oculomotor strain. These findings suggest that the cognitive and perceptual effort required to resolve blurry text directly compromises user comfort, establishing human-eye resolution as a critical baseline for the design of future ergonomic XR systems.", "AI": {"tldr": "探讨了虚拟环境中阅读任务中视觉清晰度对生理舒适性的影响。", "motivation": "研究XR系统中的文本清晰度如何影响用户的生理舒适性和工作效率，以解决现有技术中的问题。", "method": "通过操控端到端有效分辨率进行实验，在统一的logMAR尺度下测量了阅读心理学和晕动症症状。", "result": "发现阅读表现和用户舒适性随着分辨率下降而指数级恶化。0 logMAR是一个关键生理转折点，超过该阈值可实现裸眼级性能且几乎无晕动病；低于此则会导致快速、非线性的恶心和眼部肌肉疲劳增加。", "conclusion": "文本清晰度直接影响用户舒适性和阅读效率，建议将人眼分辨率作为未来XR系统设计的关键基准。"}}
{"id": "2601.02825", "pdf": "https://arxiv.org/pdf/2601.02825", "abs": "https://arxiv.org/abs/2601.02825", "authors": ["Ruiyang Zhang", "Dongzhan Zhou", "Zhedong Zheng"], "title": "SketchThinker-R1: Towards Efficient Sketch-Style Reasoning in Large Multimodal Models", "categories": ["cs.CV"], "comment": "28 pages, 11 figures", "summary": "Despite the empirical success of extensive, step-by-step reasoning in large multimodal models, long reasoning processes inevitably incur substantial computational overhead, i.e., in terms of higher token costs and increased response time, which undermines inference efficiency. In contrast, humans often employ sketch-style reasoning: a concise, goal-directed cognitive process that prioritizes salient information and enables efficient problem-solving. Inspired by this cognitive efficiency, we propose SketchThinker-R1, which incentivizes sketch-style reasoning ability in large multimodal models. Our method consists of three primary stages. In the Sketch-Mode Cold Start stage, we convert standard long reasoning process into sketch-style reasoning and finetune base multimodal model, instilling initial sketch-style reasoning capability. Next, we train SketchJudge Reward Model, which explicitly evaluates thinking process of model and assigns higher scores to sketch-style reasoning. Finally, we conduct Sketch-Thinking Reinforcement Learning under supervision of SketchJudge to further generalize sketch-style reasoning ability. Experimental evaluation on four benchmarks reveals that our SketchThinker-R1 achieves over 64% reduction in reasoning token cost without compromising final answer accuracy. Qualitative analysis further shows that sketch-style reasoning focuses more on key cues during problem solving.", "AI": {"tldr": "该论文提出了SketchThinker-R1，一种激励大型多模态模型采用更高效的草图式推理能力的方法。", "motivation": "长期的逐步推理过程虽然成功但计算开销大，导致成本增加和响应时间延长。人类倾向于使用简洁、目标导向的认知过程解决复杂问题，论文试图模仿这种认知效率。", "method": "该方法包括三个主要阶段：Sketch-Mode冷启动阶段，将标准长推理转化为草图式推理并微调基础多模态模型；训练SketchJudge奖励模型以评估和评分推理过程；进行Sketch-Thinking强化学习来进一步推广草图式推理能力。", "result": "实验结果显示，在四个基准上，SketchThinker-R1在不降低最终答案准确性的情况下，减少了超过64％的推理代币成本。定性分析表明，草图式推理更注重问题解决过程中的关键线索。", "conclusion": "论文成功地激励大型多模态模型采用高效的草图式推理能力，在保持准确性的前提下显著降低了计算开销。"}}
{"id": "2601.02818", "pdf": "https://arxiv.org/pdf/2601.02818", "abs": "https://arxiv.org/abs/2601.02818", "authors": ["Muzhen Zhang", "Yujie Cheng", "Zhanxiang Lei"], "title": "Quantum-enhanced long short-term memory with attention for spatial permeability prediction in oilfield reservoirs", "categories": ["cs.AI", "quant-ph"], "comment": "22 pages, 7 figures", "summary": "Spatial prediction of reservoir parameters, especially permeability, is crucial for oil and gas exploration and development. However, the wide range and high variability of permeability prevent existing methods from providing reliable predictions. For the first time in subsurface spatial prediction, this study presents a quantum-enhanced long short-term memory with attention (QLSTMA) model that incorporates variational quantum circuits (VQCs) into the recurrent cell. Using quantum entanglement and superposition principles, the QLSTMA significantly improves the ability to predict complex geological parameters such as permeability. Two quantization structures, QLSTMA with Shared Gates (QLSTMA-SG) and with Independent Gates (QLSTMA-IG), are designed to investigate and evaluate the effects of quantum structure configurations and the number of qubits on model performance. Experimental results demonstrate that the 8-qubit QLSTMA-IG model significantly outperforms the traditional long short-term memory with attention (LSTMA), reducing Mean Absolute Error (MAE) by 19% and Root Mean Squared Error (RMSE) by 20%, with particularly strong performance in regions featuring complex well-logging data. These findings validate the potential of quantum-classical hybrid neural networks for reservoir prediction, indicating that increasing the number of qubits yields further accuracy gains despite the reliance on classical simulations. This study establishes a foundational framework for the eventual deployment of such models on real quantum hardware and their extension to broader applications in petroleum engineering and geoscience.", "AI": {"tldr": "本文提出了一种量子增强的长短期记忆网络与注意力机制结合模型（QLSTMA），用于油藏渗透率的空间预测。", "motivation": "现有的方法在宽广范围和高变异性的渗透率预测中表现不佳，因此提出了基于量子计算的新型神经网络来提高预测精度。", "method": "设计了一种将变分量子电路（VQCs）集成到循环单元中的QLSTMA模型，并引入了两种量化结构：共享门QLSTMA-SG和独立门QLSTMA-IG。实验结果表明，8个量子位的QLSTMA-IG模型在预测复杂地质参数如渗透率时表现出色。", "result": "与传统的LSTMA相比，8个量子位的QLSTMA-IG模型将平均绝对误差（MAE）降低了19%，均方根误差（RMSE）降低了20%。特别是在具有复杂测井数据区域中表现尤为突出。", "conclusion": "该研究验证了量子与经典混合神经网络在油藏预测中的潜力，表明增加量子位的数量能进一步提高准确性。这为未来将此类模型部署到真实量子硬件上以及扩展到更广泛的应用领域奠定了基础。"}}
{"id": "2601.02814", "pdf": "https://arxiv.org/pdf/2601.02814", "abs": "https://arxiv.org/abs/2601.02814", "authors": ["Duc Ngo", "Arya Rahgoza"], "title": "Causal-Enhanced AI Agents for Medical Research Screening", "categories": ["cs.AI"], "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence", "summary": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care. We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways. Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.", "AI": {"tldr": "本文提出了一种基于因果图的增强型AI代理，用于医学研究筛选任务，旨在提高准确性和减少错误。", "motivation": "手动审查每年超过150万篇出版物对于证据为基础的医学是不可行的。现有的AI方法在系统回顾任务中存在幻觉问题，影响患者护理时这种误差率是无法接受的。", "method": "该研究提出了一种因果图增强型检索辅助生成系统，结合了明确的因果推理和双层知识图谱，实现了证据优先协议，并自动产生有向无环图以可视化干预-结果路径。", "result": "在234份痴呆症锻炼摘要上进行评估显示，CausalAgent达到了95％准确性，100％检索成功且没有幻觉错误，而基线AI的准确性和幻觉误差率分别为34%和10%。", "conclusion": "该系统证明了在医学领域实现可信AI的原则，并展示了因果推理对高风险医疗保健中应用的价值。"}}
{"id": "2601.02813", "pdf": "https://arxiv.org/pdf/2601.02813", "abs": "https://arxiv.org/abs/2601.02813", "authors": ["Masum Hasan", "Junjie Zhao", "Ehsan Hoque"], "title": "HAL: Inducing Human-likeness in LLMs with Alignment", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Conversational human-likeness plays a central role in human-AI interaction, yet it has remained difficult to define, measure, and optimize. As a result, improvements in human-like behavior are largely driven by scale or broad supervised training, rather than targeted alignment. We introduce Human Aligning LLMs (HAL), a framework for aligning language models to conversational human-likeness using an interpretable, data-driven reward. HAL derives explicit conversational traits from contrastive dialogue data, combines them into a compact scalar score, and uses this score as a transparent reward signal for alignment with standard preference optimization methods. Using this approach, we align models of varying sizes without affecting their overall performance. In large-scale human evaluations, models aligned with HAL are more frequently perceived as human-like in conversation. Because HAL operates over explicit, interpretable traits, it enables inspection of alignment behavior and diagnosis of unintended effects. More broadly, HAL demonstrates how soft, qualitative properties of language--previously outside the scope for alignment--can be made measurable and aligned in an interpretable and explainable way.", "AI": {"tldr": "HAL框架通过可解释的数据驱动奖励信号，将语言模型与对话中的类人性对齐。", "motivation": "改进人类行为主要是依靠规模或广泛的监督训练而非针对性的对齐。为了明确、衡量和优化类人性，引入了HAL框架。", "method": "从对比对话数据中推导出显式的对话特征，并将这些特征合并为一个紧凑的标量分数作为透明奖励信号进行模型对齐。", "result": "在大规模的人类评估中，与HAL对齐的模型更常被感知为对话中的类人类。", "conclusion": "因为HAL操作的是明确、可解释的特质，它能够检查对齐行为并诊断意外效果。"}}
{"id": "2601.02806", "pdf": "https://arxiv.org/pdf/2601.02806", "abs": "https://arxiv.org/abs/2601.02806", "authors": ["Mingzhou Jiang", "Jiaying Zhou", "Nan Zeng", "Mickael Li", "Qijie Tang", "Chao He", "Huazhu Fu", "Honghui He"], "title": "Topology-aware Pathological Consistency Matching for Weakly-Paired IHC Virtual Staining", "categories": ["cs.CV"], "comment": null, "summary": "Immunohistochemical (IHC) staining provides crucial molecular characterization of tissue samples and plays an indispensable role in the clinical examination and diagnosis of cancers. However, compared with the commonly used Hematoxylin and Eosin (H&E) staining, IHC staining involves complex procedures and is both time-consuming and expensive, which limits its widespread clinical use. Virtual staining converts H&E images to IHC images, offering a cost-effective alternative to clinical IHC staining. Nevertheless, using adjacent slides as ground truth often results in weakly-paired data with spatial misalignment and local deformations, hindering effective supervised learning. To address these challenges, we propose a novel topology-aware framework for H&E-to-IHC virtual staining. Specifically, we introduce a Topology-aware Consistency Matching (TACM) mechanism that employs graph contrastive learning and topological perturbations to learn robust matching patterns despite spatial misalignments, ensuring structural consistency. Furthermore, we propose a Topology-constrained Pathological Matching (TCPM) mechanism that aligns pathological positive regions based on node importance to enhance pathological consistency. Extensive experiments on two benchmarks across four staining tasks demonstrate that our method outperforms state-of-the-art approaches, achieving superior generation quality with higher clinical relevance.", "AI": {"tldr": "提出了一种拓扑感知框架用于H&E到IHC的虚拟染色，包括拓扑感知一致性匹配和病理约束匹配机制以提高生成图像的质量。", "motivation": "免疫组化（IHC）染色虽然提供重要分子信息，但因耗时费钱限制了临床应用。虚拟染色成为一种低成本替代方案，但仍面临弱配对数据带来的空间错位和局部变形问题。", "method": "引入拓扑感知一致性匹配机制利用图对比学习和拓扑扰动解决空间错位问题；提出病理约束匹配机制根据节点重要性对齐病理阳性区域提高病理一致性。", "result": "实验结果表明该方法在两个基准数据集上优于现有方法，生成质量更优且临床相关性强。", "conclusion": "通过创新的拓扑感知框架有效解决了虚拟染色中的空间错位和局部变形问题，为H&E到IHC的转换提供了新思路。"}}
{"id": "2601.02805", "pdf": "https://arxiv.org/pdf/2601.02805", "abs": "https://arxiv.org/abs/2601.02805", "authors": ["Jialin Wang", "Songming Ping", "Kemu Xu", "Yue Li", "Hai-Ning Liang"], "title": "The perceptual gap between video see-through displays and natural human vision", "categories": ["cs.HC", "cs.GR", "cs.MM"], "comment": "19 pages, 9 figures, 4 tables", "summary": "Video see-through (VST) technology aims to seamlessly blend virtual and physical worlds by reconstructing reality through cameras. While manufacturers promise perceptual fidelity, it remains unclear how close these systems are to replicating natural human vision across varying environmental conditions. In this work, we quantify the perceptual gap between the human eye and different popular VST headsets (Apple Vision Pro, Meta Quest 3, Quest Pro) using psychophysical measures of visual acuity, contrast sensitivity, and color vision. We show that despite hardware advancements, all tested VST systems fail to match the dynamic range and adaptability of the naked eye. While high-end devices approach human performance in ideal lighting, they exhibit significant degradation in low-light conditions, particularly in contrast sensitivity and acuity. Our results map the physiological limitations of digital reality reconstruction, establishing a specific perceptual gap that defines the roadmap for achieving indistinguishable VST experiences.", "AI": {"tldr": "本文量化了视频透视（VST）头戴设备与裸眼视觉之间的感知差距。", "motivation": "制造商承诺的感知保真度和实际表现之间存在不确定性，尤其是不同环境条件下的人类视觉复制情况。", "method": "使用视力敏锐度、对比敏感度和色彩视觉的心理物理学测量方法来比较人类眼睛与三款流行VST头戴设备之间的感知差异。", "result": "尽管硬件有所改进，所有测试的VST系统在低光环境下特别是在对比敏感度和清晰度方面表现较差。这些结果绘制了数字现实重建的生理局限性图谱。", "conclusion": "本文确定了一个具体的感知差距，并为实现不可区分的视频透视体验制定了路线图。"}}
{"id": "2601.02798", "pdf": "https://arxiv.org/pdf/2601.02798", "abs": "https://arxiv.org/abs/2601.02798", "authors": ["Sicong Gao", "Chen Qian", "Laurence Xian", "Liao Wu", "Maurice Pagnucco", "Yang Song"], "title": "Reinforcement Learning for Follow-the-Leader Robotic Endoscopic Navigation via Synthetic Data", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous navigation is crucial for both medical and industrial endoscopic robots, enabling safe and efficient exploration of narrow tubular environments without continuous human intervention, where avoiding contact with the inner walls has been a longstanding challenge for prior approaches. We present a follow-the-leader endoscopic robot based on a flexible continuum structure designed to minimize contact between the endoscope body and intestinal walls, thereby reducing patient discomfort. To achieve this objective, we propose a vision-based deep reinforcement learning framework guided by monocular depth estimation. A realistic intestinal simulation environment was constructed in \\textit{NVIDIA Omniverse} to train and evaluate autonomous navigation strategies. Furthermore, thousands of synthetic intraluminal images were generated using NVIDIA Replicator to fine-tune the Depth Anything model, enabling dense three-dimensional perception of the intestinal environment with a single monocular camera. Subsequently, we introduce a geometry-aware reward and penalty mechanism to enable accurate lumen tracking. Compared with the original Depth Anything model, our method improves $δ_{1}$ depth accuracy by 39.2% and reduces the navigation J-index by 0.67 relative to the second-best method, demonstrating the robustness and effectiveness of the proposed approach.", "AI": {"tldr": "提出了一种基于合成数据和深度增强学习的自主内镜导航方法，以减少与肠道壁接触并提高导航精度。", "motivation": "为了解决医疗和工业内镜机器人在狭窄管状环境中自主导航时避免接触内壁的问题，从而提高安全性和效率，并减轻患者不适感。", "method": "通过NVIDIA Omniverse构建了现实的肠模拟环境来训练和评估自主导航策略。使用NVIDIA Replicator生成了大量的合成肠道内部图像以优化Depth Anything模型，实现单目相机下的密集三维感知。引入了几何感知奖励机制来精确跟踪腔道。", "result": "相比原始Depth Anything模型，该方法将δ1深度精度提高了39.2%，并比第二好方法减少了导航J指数0.67。", "conclusion": "提出的基于合成数据和几何感知强化学习的方法在内镜自主导航中表现出了优越的准确性和鲁棒性。"}}
{"id": "2601.02793", "pdf": "https://arxiv.org/pdf/2601.02793", "abs": "https://arxiv.org/abs/2601.02793", "authors": ["Ivan Sobko", "Hayko Riemenschneider", "Markus Gross", "Christopher Schroers"], "title": "StableDPT: Temporal Stable Monocular Video Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Applying single image Monocular Depth Estimation (MDE) models to video sequences introduces significant temporal instability and flickering artifacts. We propose a novel approach that adapts any state-of-the-art image-based (depth) estimation model for video processing by integrating a new temporal module - trainable on a single GPU in a few days. Our architecture StableDPT builds upon an off-the-shelf Vision Transformer (ViT) encoder and enhances the Dense Prediction Transformer (DPT) head. The core of our contribution lies in the temporal layers within the head, which use an efficient cross-attention mechanism to integrate information from keyframes sampled across the entire video sequence. This allows the model to capture global context and inter-frame relationships leading to more accurate and temporally stable depth predictions. Furthermore, we propose a novel inference strategy for processing videos of arbitrary length avoiding the scale misalignment and redundant computations associated with overlapping windows used in other methods. Evaluations on multiple benchmark datasets demonstrate improved temporal consistency, competitive state-of-the-art performance and on top 2x faster processing in real-world scenarios.", "AI": {"tldr": "提出一种改进的单目视频深度估计模型，解决图像到视频转换时产生的不稳定性和闪烁问题。", "motivation": "现有单张图像的单目深度估计方法在应用于视频序列时常出现显著的时间不稳定性及闪烁瑕疵。为了解决这些问题，作者提出了一个新颖的方法来适应现有的基于图像的深度估计模型以处理视频。", "method": "利用了一个现成的视觉变换器编码器，并增强了密集预测变压器头部。关键在于头中的时间层，该层使用高效的交叉注意力机制从整个视频序列中采样的关键帧集成信息，从而捕捉全局背景和帧间关系。", "result": "在多个基准数据集上的评估显示了改进的时间一致性、具有竞争力的当前最佳性能，并且在现实世界场景中处理速度提高了两倍以上。", "conclusion": "通过引入时间模块，StableDPT有效改善了单目视频深度估计中的时间和闪烁问题。"}}
{"id": "2601.02792", "pdf": "https://arxiv.org/pdf/2601.02792", "abs": "https://arxiv.org/abs/2601.02792", "authors": ["Petteri Teikari", "Neliana Fuenmayor"], "title": "Textile IR: A Bidirectional Intermediate Representation for Physics-Aware Fashion CAD", "categories": ["cs.CV"], "comment": "20 pages, 8 figures, SI Technologies and Practices (Fashion Practice)", "summary": "We introduce Textile IR, a bidirectional intermediate representation that connects manufacturing-valid CAD, physics-based simulation, and lifecycle assessment for fashion design. Unlike existing siloed tools where pattern software guarantees sewable outputs but understands nothing about drape, and physics simulation predicts behaviour but cannot automatically fix patterns, Textile IR provides the semantic glue for integration through a seven-layer Verification Ladder -- from cheap syntactic checks (pattern closure, seam compatibility) to expensive physics validation (drape simulation, stress analysis). The architecture enables bidirectional feedback: simulation failures suggest pattern modifications; material substitutions update sustainability estimates in real time; uncertainty propagates across the pipeline with explicit confidence bounds. We formalise fashion engineering as constraint satisfaction over three domains and demonstrate how Textile IR's scene-graph representation enables AI systems to manipulate garments as structured programs rather than pixel arrays. The framework addresses the compound uncertainty problem: when measurement errors in material testing, simulation approximations, and LCA database gaps combine, sustainability claims become unreliable without explicit uncertainty tracking. We propose six research priorities and discuss deployment considerations for fashion SMEs where integrated workflows reduce specialised engineering requirements. Key contribution: a formal representation that makes engineering constraints perceptible, manipulable, and immediately consequential -- enabling designers to navigate sustainability, manufacturability, and aesthetic tradeoffs simultaneously rather than discovering conflicts after costly physical prototyping.", "AI": {"tldr": "介绍了一种名为Textile IR的双向中间表示，用于时尚设计中的物理感知CAD、基于物理的模拟和生命周期评估。", "motivation": "解决现有工具之间孤立的问题，这些工具在保证缝纫输出的同时不了解面料悬垂性，在预测行为时无法自动修复图案问题。引入一种新的形式化方法来处理工程约束并解决复合不确定性问题。", "method": "提出了一个七层验证阶梯架构，包括从语法检查到物理验证的各种层面，并采用场景图表示法使AI系统能够像操作程序一样操纵服装设计。", "result": "Textile IR的框架允许双向反馈，如模拟失败可以提示图案修改，材料替换可实时更新可持续性估计等。还提出了六个研究优先事项和部署考虑因素。", "conclusion": "贡献在于提供了一种形式化的表示方法，使工程约束变得可见、可操作且即时相关，从而帮助设计师同时处理可持续性、制造可行性及美学之间的权衡。"}}
{"id": "2601.02785", "pdf": "https://arxiv.org/pdf/2601.02785", "abs": "https://arxiv.org/abs/2601.02785", "authors": ["Mengtian Li", "Jinshu Chen", "Songtao Zhao", "Wanquan Feng", "Pengqi Tu", "Qian He"], "title": "DreamStyle: A Unified Framework for Video Stylization", "categories": ["cs.CV"], "comment": "Github Page: https://lemonsky1995.github.io/dreamstyle/", "summary": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality.", "AI": {"tldr": "DreamStyle是一个统一的视频风格化框架，支持文本引导、样式图像引导和首帧引导的视频风格化。", "motivation": "现有的视频风格化方法局限于单一类型的样式条件，导致应用范围受限，并且由于缺乏高质量的数据集而存在风格不一致和时间闪烁的问题。", "method": "DreamStyle基于一个基本的图象到视频（I2V）模型构建，采用低秩适应（LoRA）与特定令牌上调矩阵来减少不同条件令牌之间的混淆。", "result": "定性和定量评估表明，DreamStyle在所有三个视频风格化任务中表现出色，在风格一致性和视频质量方面优于竞争对手。", "conclusion": "通过统一的框架和高质量的数据集管理管道，DreamStyle成功解决了现有方法中的问题，并提高了视频风格化的性能。"}}
{"id": "2601.02783", "pdf": "https://arxiv.org/pdf/2601.02783", "abs": "https://arxiv.org/abs/2601.02783", "authors": ["Junjue Wang", "Yanfei Zhong", "Zihang Chen", "Zhuo Zheng", "Ailong Ma", "Liangpei Zhang"], "title": "EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework", "categories": ["cs.CV"], "comment": ":I.4.9", "summary": "Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.", "AI": {"tldr": "该论文提出了一种针对地球视觉和语言理解和生成的渐进式框架EarthVL，包括一个多任务数据集EarthVLSet和一个语义引导网络EarthVLNet。", "motivation": "现有的地球视觉技术在地理空间物体识别方面取得了里程碑式的成就，但在物体关系推理方面的探索不足，限制了对场景的整体理解。为了弥补这一缺口，该论文提出了一种新的框架来解决这些问题，并推动城市规划等应用的发展。", "method": "该方法包括一个名为EarthVLSet的多任务数据集和一个基于语义引导网络EarthVLNet的方法。在物体中心的方式下，EarthVLNet首先进行土地覆盖分割以生成用于VQA指导的对象语义，然后利用像素级语义引导的大规模语言模型（LLM）执行关系推理和知识总结。", "result": "实验结果表明EarthVLNet在三个基准测试中表现优越：语义分割、多选题视觉问答任务以及开放性问题视觉问答任务。这些结果揭示了未来的研究方向，包括跨数据集场景下的VQA性能改进等。", "conclusion": "该论文提出的数据集和方法为连接“图像-掩码-文本”提供了有益的基准，并有望促进地球视觉在地理应用中的发展。"}}
{"id": "2601.02780", "pdf": "https://arxiv.org/pdf/2601.02780", "abs": "https://arxiv.org/abs/2601.02780", "authors": ["Bangjun Xiao", "Bingquan Xia", "Bo Yang", "Bofei Gao", "Bowen Shen", "Chen Zhang", "Chenhong He", "Chiheng Lou", "Fuli Luo", "Gang Wang", "Gang Xie", "Hailin Zhang", "Hanglong Lv", "Hanyu Li", "Heyu Chen", "Hongshen Xu", "Houbin Zhang", "Huaqiu Liu", "Jiangshan Duo", "Jianyu Wei", "Jiebao Xiao", "Jinhao Dong", "Jun Shi", "Junhao Hu", "Kainan Bao", "et al. (100 additional authors not shown)"], "title": "MiMo-V2-Flash Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": "31 pages, technical report", "summary": "We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.", "AI": {"tldr": "MiMo-V2-Flash 是一种混合专家模型，具有快速推理和代理能力。该模型采用滑动窗口注意力与全局注意力相结合的架构，并引入了多教师在策略蒸馏框架。", "motivation": "开发一个参数更少但性能更强、速度更快的大规模语言模型，以应对计算资源限制并提高推理效率。", "method": "利用混合注意力机制和多教师在策略蒸馏技术，通过大规模预训练和多令牌预测实现高效推理。", "result": "MiMo-V2-Flash 模型与顶级开源模型相比，在参数更少的情况下达到了相似的性能，并实现了显著的速度提升。", "conclusion": "该研究展示了 MiMo-V2-Flash 的优异表现，通过开放源代码推动了相关领域的进一步研究和社区合作。"}}
{"id": "2601.02778", "pdf": "https://arxiv.org/pdf/2601.02778", "abs": "https://arxiv.org/abs/2601.02778", "authors": ["Haoyu Dong", "Zhengmao He", "Yang Li", "Zhibin Li", "Xinyu Yi", "Zhe Zhao"], "title": "Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Human-like dexterous hands with multiple fingers offer human-level manipulation capabilities, but training control policies that can directly deploy on real hardware remains difficult due to contact-rich physics and imperfect actuation. We close this gap with a practical sim-to-real reinforcement learning (RL) framework that utilizes dense tactile feedback combined with joint torque sensing to explicitly regulate physical interactions. To enable effective sim-to-real transfer, we introduce (i) a computationally fast tactile simulation that computes distances between dense virtual tactile units and the object via parallel forward kinematics, providing high-rate, high-resolution touch signals needed by RL; (ii) a current-to-torque calibration that eliminates the need for torque sensors on dexterous hands by mapping motor current to joint torque; and (iii) actuator dynamics modeling to bridge the actuation gaps with randomization of non-ideal effects such as backlash, torque-speed saturation. Using an asymmetric actor-critic PPO pipeline trained entirely in simulation, our policies deploy directly to a five-finger hand. The resulting policies demonstrated two essential skills: (1) command-based, controllable grasp force tracking, and (2) reorientation of objects in the hand, both of which were robustly executed without fine-tuning on the robot. By combining tactile and torque in the observation space with effective sensing/actuation modeling, our system provides a practical solution to achieve reliable dexterous manipulation. To our knowledge, this is the first demonstration of controllable grasping on a multi-finger dexterous hand trained entirely in simulation and transferred zero-shot on real hardware.", "AI": {"tldr": "通过引入快速触觉仿真、电流到扭矩校准以及执行器动态建模，该研究在模拟环境中训练五指手的控制策略，并直接将其部署到真实硬件上进行无微调的抓取和操纵任务。", "motivation": "为了克服接触丰富的物理现象和不完美的驱动问题，在现实世界中训练灵巧的手的控制策略非常困难。引入了结合触觉和扭矩的方法，以实现从模拟到真实的有效迁移。", "method": "研究提出了一种基于强化学习框架的快速触觉仿真，通过并行正向运动学计算虚拟触觉单元与物体之间的距离来提供高分辨率的触摸信号；电流到扭矩校准消除了驱动器上扭矩传感器的需求；模型化驱动器动态以弥补非理想效应带来的差距。使用不对称演员-评论家PPO流水线在模拟环境中进行训练。", "result": "研究展示了一种可以在没有微调的情况下直接部署到五指手上的策略，该策略能够执行命令式、可控制的抓取力跟踪和物体再定位技能，这是首次仅通过完全仿真训练并在真实硬件上零样本迁移得到成功结果的研究。", "conclusion": "结合触觉和扭矩的方法以及有效的感应/驱动建模为实现可靠的灵巧操纵提供了一种实用解决方案。"}}
{"id": "2601.02777", "pdf": "https://arxiv.org/pdf/2601.02777", "abs": "https://arxiv.org/abs/2601.02777", "authors": ["Jingcheng Cao", "Chaoran Xiong", "Jianmin Song", "Shang Yan", "Jiachen Liu", "Ling Pei"], "title": "M-SEVIQ: A Multi-band Stereo Event Visual-Inertial Quadruped-based Dataset for Perception under Rapid Motion and Challenging Illumination", "categories": ["cs.RO"], "comment": "6 pages, 7 figures", "summary": "Agile locomotion in legged robots poses significant challenges for visual perception. Traditional frame-based cameras often fail in these scenarios for producing blurred images, particularly under low-light conditions. In contrast, event cameras capture changes in brightness asynchronously, offering low latency, high temporal resolution, and high dynamic range. These advantages make them suitable for robust perception during rapid motion and under challenging illumination. However, existing event camera datasets exhibit limitations in stereo configurations and multi-band sensing domains under various illumination conditions. To address this gap, we present M-SEVIQ, a multi-band stereo event visual and inertial quadruped dataset collected using a Unitree Go2 equipped with stereo event cameras, a frame-based camera, an inertial measurement unit (IMU), and joint encoders. This dataset contains more than 30 real-world sequences captured across different velocity levels, illumination wavelengths, and lighting conditions. In addition, comprehensive calibration data, including intrinsic, extrinsic, and temporal alignments, are provided to facilitate accurate sensor fusion and benchmarking. Our M-SEVIQ can be used to support research in agile robot perception, sensor fusion, semantic segmentation and multi-modal vision in challenging environments.", "AI": {"tldr": "提出了M-SEVIQ数据集，用于支持敏捷机器人感知、传感器融合等研究。", "motivation": "传统摄像机在快速运动和复杂光照条件下难以提供清晰图像，而事件相机可以解决这些问题但现有数据集存在局限性。", "method": "使用配备立体事件相机和其他传感器的Unitree Go2收集多频段立体视觉惯性四足机器人数据。", "result": "提供了包含超过30个真实世界序列的数据集，涵盖不同速度、光照波长和照明条件。", "conclusion": "M-SEVIQ可以促进敏捷机器人感知研究和跨模态视觉的应用。"}}
{"id": "2601.02776", "pdf": "https://arxiv.org/pdf/2601.02776", "abs": "https://arxiv.org/abs/2601.02776", "authors": ["Zhisheng Zhang", "Xiang Li", "Yixuan Zhou", "Jing Peng", "Shengbo Cai", "Guoyang Zeng", "Zhiyong Wu"], "title": "UniSRCodec: Unified and Low-Bitrate Single Codebook Codec with Sub-Band Reconstruction", "categories": ["cs.SD", "cs.AI", "cs.MM"], "comment": "6 pages, 2 figures, and 3 tables", "summary": "Neural Audio Codecs (NACs) can reduce transmission overhead by performing compact compression and reconstruction, which also aim to bridge the gap between continuous and discrete signals. Existing NACs can be divided into two categories: multi-codebook and single-codebook codecs. Multi-codebook codecs face challenges such as structural complexity and difficulty in adapting to downstream tasks, while single-codebook codecs, though structurally simpler, suffer from low-fidelity, ineffective modeling of unified audio, and an inability to support modeling of high-frequency audio. We propose the UniSRCodec, a single-codebook codec capable of supporting high sampling rate, low-bandwidth, high fidelity, and unified. We analyze the inefficiency of waveform-based compression and introduce the time and frequency compression method using the Mel-spectrogram, and cooperate with a Vocoder to recover the phase information of the original audio. Moreover, we propose a sub-band reconstruction technique to achieve high-quality compression across both low and high frequency bands. Subjective and objective experimental results demonstrate that UniSRCodec achieves state-of-the-art (SOTA) performance among cross-domain single-codebook codecs with only a token rate of 40, and its reconstruction quality is comparable to that of certain multi-codebook methods. Our demo page is available at https://wxzyd123.github.io/unisrcodec.", "AI": {"tldr": "提出了一种名为UniSRCodec的单码本音频编解码器，旨在实现高质量、低比特率和统一性的音频压缩。", "motivation": "现有神经音频编解码器存在结构复杂度高或无法支持高频音频建模等问题。为了克服这些问题并提高音频编码的质量与效率，提出了UniSRCodec。", "method": "使用梅尔频谱进行时域和频域的压缩，并通过声码器恢复原始音频相位信息。同时采用子带重建技术来实现高低频段高质量的压缩。", "result": "实验表明，UniSRCodec在跨领域单码本编解码器中达到了最先进的性能，在仅有40个令牌率的情况下，其重构质量可与某些多码本方法媲美。", "conclusion": "通过提出新颖的技术手段，UniSRCodec能够实现高质量、低比特率和统一性的音频压缩，并且在主观及客观实验上均取得了优异的结果。"}}
{"id": "2601.02775", "pdf": "https://arxiv.org/pdf/2601.02775", "abs": "https://arxiv.org/abs/2601.02775", "authors": ["Md Nazmus Sakib", "Naga Manogna Rayasam", "Sanorita Dey"], "title": "Experience and Adaptation in AI-mediated Hiring Systems: A Combined Analysis of Online Discourse and Interface Design", "categories": ["cs.HC"], "comment": null, "summary": "Automated interviewing tools are now widely adopted to manage recruitment at scale, often replacing early human screening with algorithmic assessments. While these systems are promoted as efficient and consistent, they also generate new forms of uncertainty for applicants. Efforts to soften these experiences through human-like design features have only partially addressed underlying concerns. To understand how candidates interpret and cope with such systems, we conducted a mixed empirical investigation that combined analysis of online discussions, responses from more than one hundred and fifty survey participants, and follow-up conversations with seventeen interviewees. The findings point to several recurring problems, including unclear evaluation criteria, limited organizational responsibility for automated outcomes, and a lack of practical support for preparation. Many participants described the technology as far less advanced than advertised, leading them to infer how decisions might be made in the absence of guidance. This speculation often intensified stress and emotional strain. Furthermore, the minimal sense of interpersonal engagement contributed to feelings of detachment and disposability. Based on these observations, we propose design directions aimed at improving clarity, accountability, and candidate support in AI-mediated hiring processes.", "AI": {"tldr": "研究探讨了求职者如何理解和应对AI面试工具，结合在线讨论、调查和访谈数据揭示问题并提出设计建议。", "motivation": "理解求职者在使用自动化招聘系统时的体验与适应情况，以改进这些系统的用户界面和支持措施。", "method": "采用混合方法研究，包括分析在线讨论，收集超过150名参与者的问卷反馈，并进行17次访谈参与者。", "result": "发现主要问题为评估标准不明确、组织对自动化决策责任有限以及缺乏准备支持。求职者认为技术不如宣传先进，导致额外压力和情感负担。", "conclusion": "提出设计方向以提高透明度、责任感及候选人支援，改善AI面试系统用户体验和支持措施"}}
{"id": "2601.02771", "pdf": "https://arxiv.org/pdf/2601.02771", "abs": "https://arxiv.org/abs/2601.02771", "authors": ["Boyu Chang", "Qi Wang", "Xi Guo", "Zhixiong Nan", "Yazhou Yao", "Tianfei Zhou"], "title": "AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026 as Oral. Code:https://github.com/ChangPtR/AbdMLLM", "summary": "Visual abductive reasoning (VAR) is a challenging task that requires AI systems to infer the most likely explanation for incomplete visual observations. While recent MLLMs develop strong general-purpose multimodal reasoning capabilities, they fall short in abductive inference, as compared to human beings. To bridge this gap, we draw inspiration from the interplay between verbal and pictorial abduction in human cognition, and propose to strengthen abduction of MLLMs by mimicking such dual-mode behavior. Concretely, we introduce AbductiveMLLM comprising of two synergistic components: REASONER and IMAGINER. The REASONER operates in the verbal domain. It first explores a broad space of possible explanations using a blind LLM and then prunes visually incongruent hypotheses based on cross-modal causal alignment. The remaining hypotheses are introduced into the MLLM as targeted priors, steering its reasoning toward causally coherent explanations. The IMAGINER, on the other hand, further guides MLLMs by emulating human-like pictorial thinking. It conditions a text-to-image diffusion model on both the input video and the REASONER's output embeddings to \"imagine\" plausible visual scenes that correspond to verbal explanation, thereby enriching MLLMs' contextual grounding. The two components are trained jointly in an end-to-end manner. Experiments on standard VAR benchmarks show that AbductiveMLLM achieves state-of-the-art performance, consistently outperforming traditional solutions and advanced MLLMs.", "AI": {"tldr": "该论文提出了一种名为AbductiveMLLM的方法，旨在增强多模态大型语言模型（MLLM）的视觉归纳推理能力。", "motivation": "虽然最近开发出的多模态大语言模型展示了强大的通用多模态推理能力，但在进行视觉归纳推理时仍不如人类表现优异。为此，研究人员试图通过模拟人脑中言语与图像之间的双重模式行为来改善MLLM中的归纳推理。", "method": "AbductiveMLLM由REASONER和IMAGINER两个协同组件构成。REASONER首先利用无偏见的LLM探索可能解释的空间，并根据跨模态因果对齐剔除视觉上不一致假设；剩余假设作为目标先验被引入到MLLM，引导其推理走向因果一致性的解释。IMAGINER通过文本到图像扩散模型条件化输入视频和REASONER输出嵌入以模拟人类样式的图像思维。", "result": "在标准的视觉归纳推理基准测试上，AbductiveMLLM实现了最先进的性能，持续超越传统方法和先进多模态大型语言模型。", "conclusion": "该研究展示了通过模拟人脑中言语与图像之间的双重模式行为来提升多模态大语言模型（MLLM）视觉归纳推理能力的有效性。"}}
{"id": "2601.02766", "pdf": "https://arxiv.org/pdf/2601.02766", "abs": "https://arxiv.org/abs/2601.02766", "authors": ["Md. Anowar Hossain", "Mohd. Ehsanul Hoque"], "title": "Advancing Assistive Robotics: Multi-Modal Navigation and Biophysical Monitoring for Next-Generation Wheelchairs", "categories": ["cs.RO", "cs.AR"], "comment": null, "summary": "Assistive electric-powered wheelchairs (EPWs) have become essential mobility aids for people with disabilities such as amyotrophic lateral sclerosis (ALS), post-stroke hemiplegia, and dementia-related mobility impairment. This work presents a novel multi-modal EPW control system designed to prioritize patient needs while allowing seamless switching between control modes. Four complementary interfaces, namely joystick, speech, hand gesture, and electrooculography (EOG), are integrated with a continuous vital sign monitoring framework measuring heart rate variability, oxygen saturation (SpO2), and skin temperature. This combination enables greater patient independence while allowing caregivers to maintain real-time supervision and early intervention capability. Two-point calibration of the biophysical sensors against clinical reference devices resulted in root mean square errors of at most 2 bpm for heart rate, 0.5 degree Celsius for skin temperature, and 1 percent for SpO2. Experimental evaluation involved twenty participants with mobility impairments executing a total of 500 indoor navigation commands. The achieved command recognition accuracies were 99 percent for joystick control, 97 percent plus or minus 2 percent for speech, and 95 percent plus or minus 3 percent for hand gesture, with an average closed-loop latency of 20 plus or minus 0.5 milliseconds. Caregivers receive real-time alerts through an Android application following encrypted cloud transmission of physiological data. By integrating multi-modal mobility control with cloud-enabled health monitoring and reporting latency and energy budgets, the proposed prototype addresses key challenges in assistive robotics, contributes toward compliance with ISO 7176-31 and IEC 80601-2-78 safety standards, and establishes a foundation for future adaptive machine learning enhancements.", "AI": {"tldr": "本论文提出了一种新型多模式电动轮椅控制系统，结合多种交互方式和生理参数监测以提高患者独立性和监护能力。", "motivation": "为了满足ALS、中风偏瘫和痴呆症相关行动障碍患者的移动需求，并提升其生活质量及护理人员的实时监控能力。", "method": "开发了集成四种接口（操纵杆、语音、手势、眼动追踪）和生理参数监测系统的电动轮椅，通过两步校准验证传感器准确性并进行实测验证系统性能。", "result": "实验中导航指令识别准确率为99％的操纵杆控制，语音为97%±2%，手部动作则为95%±3%, 平均闭环延迟时间为20±0.5毫秒。生理数据通过加密云传输并在Android应用上实时报警。", "conclusion": "该设计满足ISO和IEC标准要求，构建了适应性机器学习增强的基础，并显著改善了辅助机器人领域中的关键挑战。"}}
{"id": "2601.02764", "pdf": "https://arxiv.org/pdf/2601.02764", "abs": "https://arxiv.org/abs/2601.02764", "authors": ["Hyunji Nam", "Sejoon Oh", "Emma Kong", "Yesu Feng", "Moumita Bhattacharya"], "title": "Netflix Artwork Personalization via LLM Post-training", "categories": ["cs.IR", "cs.AI"], "comment": "6 pages", "summary": "Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.", "AI": {"tldr": "通过微调预训练语言模型，根据用户的个性化偏好推荐电影艺术作品。", "motivation": "用户对电影艺术品的偏好各不相同，传统的单一推荐方法无法满足所有用户的需求。因此，研究一种能够根据不同用户喜好进行个性化的电影艺术品推荐方法显得尤为重要。", "method": "利用预训练的语言模型，并通过特定数据集对其进行微调，以生成针对不同用户的个性化艺术品推荐。", "result": "实验结果表明，与Netflix现有的生产模型相比，所提出的后训练语言模型在110K训练数据点和5K独立测试用户-电影对上实现了3-5%的性能提升。", "conclusion": "通过微调大型语言模型来进行个性化艺术品推荐是一个有前景的方向，能够显著提高用户的满意度和参与度。"}}
{"id": "2601.02763", "pdf": "https://arxiv.org/pdf/2601.02763", "abs": "https://arxiv.org/abs/2601.02763", "authors": ["Xu Zhang", "Huan Zhang", "Guoli Wang", "Qian Zhang", "Lefei Zhang"], "title": "ClearAIR: A Human-Visual-Perception-Inspired All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026. Project page: https://github.com/House-yuyu/ClearAIR", "summary": "All-in-One Image Restoration (AiOIR) has advanced significantly, offering promising solutions for complex real-world degradations. However, most existing approaches rely heavily on degradation-specific representations, often resulting in oversmoothing and artifacts. To address this, we propose ClearAIR, a novel AiOIR framework inspired by Human Visual Perception (HVP) and designed with a hierarchical, coarse-to-fine restoration strategy. First, leveraging the global priority of early HVP, we employ a Multimodal Large Language Model (MLLM)-based Image Quality Assessment (IQA) model for overall evaluation. Unlike conventional IQA, our method integrates cross-modal understanding to more accurately characterize complex, composite degradations. Building upon this overall assessment, we then introduce a region awareness and task recognition pipeline. A semantic cross-attention, leveraging semantic guidance unit, first produces coarse semantic prompts. Guided by this regional context, a degradation-aware module implicitly captures region-specific degradation characteristics, enabling more precise local restoration. Finally, to recover fine details, we propose an internal clue reuse mechanism. It operates in a self-supervised manner to mine and leverage the intrinsic information of the image itself, substantially enhancing detail restoration. Experimental results show that ClearAIR achieves superior performance across diverse synthetic and real-world datasets.", "AI": {"tldr": "本文提出了一种基于人类视觉感知的All-in-One图像恢复框架ClearAIR，旨在解决现有方法中的过度平滑和伪影问题。", "motivation": "现有的图像恢复方法依赖于特定降质情况下的表示，导致过平滑和伪影。为了解决这些问题，作者提出了一个受人眼视觉系统启发的方法。", "method": "首先使用多模态大语言模型进行整体评估；然后引入区域感知与任务识别管道，结合语义交叉注意机制捕捉局部特性；最后通过内部线索再利用恢复细节。", "result": "实验结果表明，ClearAIR在合成和真实世界数据集中表现出色。", "conclusion": "ClearAIR提供了一种新颖的图像恢复方法，能够有效地处理复杂的降质情况，并提高细节恢复的质量。"}}
{"id": "2601.02762", "pdf": "https://arxiv.org/pdf/2601.02762", "abs": "https://arxiv.org/abs/2601.02762", "authors": ["Zihan Yang", "Jindou Jia", "Meng Wang", "Yuhang Liu", "Kexin Guo", "Xiang Yu"], "title": "Unified Meta-Representation and Feedback Calibration for General Disturbance Estimation", "categories": ["cs.RO"], "comment": "8 pages, 10 figures", "summary": "Precise control in modern robotic applications is always an open issue due to unknown time-varying disturbances. Existing meta-learning-based approaches require a shared representation of environmental structures, which lack flexibility for realistic non-structural disturbances. Besides, representation error and the distribution shifts can lead to heavy degradation in prediction accuracy. This work presents a generalizable disturbance estimation framework that builds on meta-learning and feedback-calibrated online adaptation. By extracting features from a finite time window of past observations, a unified representation that effectively captures general non-structural disturbances can be learned without predefined structural assumptions. The online adaptation process is subsequently calibrated by a state-feedback mechanism to attenuate the learning residual originating from the representation and generalizability limitations. Theoretical analysis shows that simultaneous convergence of both the online learning error and the disturbance estimation error can be achieved. Through the unified meta-representation, our framework effectively estimates multiple rapidly changing disturbances, as demonstrated by quadrotor flight experiments. See the project page for video, supplementary material and code: https://nonstructural-metalearn.github.io.", "AI": {"tldr": "本文提出了一种基于元学习和反馈校准的通用扰动估计框架，用于精确控制中的非结构化扰动。", "motivation": "现代机器人应用中由于未知的时间变化干扰导致精确控制成为难题。现有基于元学习的方法缺乏对现实世界非结构性干扰的灵活性，并且表征误差及分布迁移会导致预测准确性下降。", "method": "该框架通过从有限时间窗口中的过去观测中提取特征，获取一个统一表示来有效捕捉一般非结构化扰动，并利用状态反馈机制校准在线适应过程以减轻由表示和泛化限制产生的学习残差。", "result": "理论分析表明同时实现在线学习误差与干扰估计误差的收敛是可能的。通过统一元表征，该框架能够有效地估计多个快速变化的干扰，这在四旋翼飞行实验中得到了验证。", "conclusion": "提出的通用扰动估计框架能有效处理非结构化扰动，提高机器人控制精度，并已在实际应用中得到验证。"}}
{"id": "2601.02760", "pdf": "https://arxiv.org/pdf/2601.02760", "abs": "https://arxiv.org/abs/2601.02760", "authors": ["Zeyu Ren", "Zeyu Zhang", "Wukai Li", "Qingxiang Liu", "Hao Tang"], "title": "AnyDepth: Depth Estimation Made Easy", "categories": ["cs.CV"], "comment": null, "summary": "Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.", "AI": {"tldr": "本文提出了一个轻量级和数据为中心的框架，用于单目深度估计。", "motivation": "现有的单目深度估计算法依赖于大规模的数据集和复杂的解码器，这限制了其效率和泛化能力。为了解决这些问题，本文提出了一种新的方法来提高模型的设计并提升数据质量。", "method": "采用了DINOv3作为视觉编码器获取高质量的密集特征，并设计了一个简单的深度变换器（SDT）作为解码器，通过单路径特性融合和上采样过程减少跨尺度特性融合的计算开销。此外，提出了一种基于质量的过滤策略以排除有害样本。", "result": "实验结果表明，该框架在五个基准测试中均超过了DPT，在精度上有所提升，并减少了约85%-89%的参数数量。", "conclusion": "本文展示了平衡模型设计和数据质量对于实现高效且通用的零样本深度估计的重要性。"}}
{"id": "2601.02759", "pdf": "https://arxiv.org/pdf/2601.02759", "abs": "https://arxiv.org/abs/2601.02759", "authors": ["Hyungtae Lim", "Minkyun Seo", "Luca Carlone", "Jaesik Park"], "title": "Towards Zero-Shot Point Cloud Registration Across Diverse Scales, Scenes, and Sensor Setups", "categories": ["cs.CV", "cs.RO"], "comment": "18 pages, 15 figures. Extended version of our ICCV 2025 highlight paper [arXiv:2503.07940]. arXiv admin note: substantial text overlap with arXiv:2503.07940", "summary": "Some deep learning-based point cloud registration methods struggle with zero-shot generalization, often requiring dataset-specific hyperparameter tuning or retraining for new environments. We identify three critical limitations: (a) fixed user-defined parameters (e.g., voxel size, search radius) that fail to generalize across varying scales, (b) learned keypoint detectors exhibit poor cross-domain transferability, and (c) absolute coordinates amplify scale mismatches between datasets. To address these three issues, we present BUFFER-X, a training-free registration framework that achieves zero-shot generalization through: (a) geometric bootstrapping for automatic hyperparameter estimation, (b) distribution-aware farthest point sampling to replace learned detectors, and (c) patch-level coordinate normalization to ensure scale consistency. Our approach employs hierarchical multi-scale matching to extract correspondences across local, middle, and global receptive fields, enabling robust registration in diverse environments. For efficiency-critical applications, we introduce BUFFER-X-Lite, which reduces total computation time by 43% (relative to BUFFER-X) through early exit strategies and fast pose solvers while preserving accuracy. We evaluate on a comprehensive benchmark comprising 12 datasets spanning object-scale, indoor, and outdoor scenes, including cross-sensor registration between heterogeneous LiDAR configurations. Results demonstrate that our approach generalizes effectively without manual tuning or prior knowledge of test domains. Code: https://github.com/MIT-SPARK/BUFFER-X.", "AI": {"tldr": "本文提出了一种零样本泛化的点云注册框架BUFFER-X，旨在解决传统方法在不同尺度、场景和传感器配置下的适应性问题。", "motivation": "一些基于深度学习的点云注册方法在跨域应用中存在挑战，包括固定参数难以适应变化、关键点检测器的可迁移性差以及绝对坐标带来的缩放不一致问题。", "method": "BUFFER-X通过几何引导自动估计超参数、分布感知最远点采样替换学习检测器和局部坐标归一化来解决上述问题。此外，还提出BUFFER-X-Lite以提高效率并减少计算时间。", "result": "实验结果表明，该方法在多个数据集上表现出优秀的零样本泛化能力，并且在保持准确性的前提下通过早退出策略提高了效率。", "conclusion": "本文提出的BUFFER-X框架能够在不进行手动调整或不了解测试域的情况下实现有效的点云注册。"}}
{"id": "2601.02757", "pdf": "https://arxiv.org/pdf/2601.02757", "abs": "https://arxiv.org/abs/2601.02757", "authors": ["Zixuan Xiao", "Jun Ma"], "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery", "categories": ["cs.AI"], "comment": "ef:Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341", "summary": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.", "AI": {"tldr": "提出了一种结合大型语言模型与视觉基础模型的智能变化分析框架ChangeGPT，用于解决城市环境中的遥感图像变化检测问题。", "motivation": "现有变化检测方法缺乏处理多样查询和全面分析的能力。为了提高灵活性和智能化水平，提出了一个集成LLM的通用代理框架。", "method": "通过分层结构减轻模型幻觉问题，并在包含140个分类问题的数据集上评估了工具选择能力和整体查询准确性。实验中使用GPT-4-turbo作为后台。", "result": "ChangeGPT达到了90.71%的匹配率，尤其是在需要多步推理和稳健工具选择的变化相关查询方面表现突出，并通过深圳前海湾区的实际案例验证了其有效性。", "conclusion": "ChangeGPT为遥感应用中的智能变化分析提供了强大解决方案，具备灵活性、适应性和多种类型的变化分析能力。"}}
{"id": "2601.02754", "pdf": "https://arxiv.org/pdf/2601.02754", "abs": "https://arxiv.org/abs/2601.02754", "authors": ["Mingming Zhang", "Na Li", "Zhuang Feiqing", "Hongyang Zheng", "Jiangbing Zhou", "Wang Wuyin", "Sheng-jie Sun", "XiaoWei Chen", "Junxiong Zhu", "Lixin Zou", "Chenliang Li"], "title": "Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "11pages, 5figures, In Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining", "summary": "With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning. To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.", "AI": {"tldr": "提出了一种新的Q值正则化生成自动出价方法(QGA)，用于解决现有广告投放策略的不足，特别是在利用历史数据进行政策学习时遇到的问题。", "motivation": "现有的自动出价技术依赖于复杂的结构和昂贵的超参数调整，并且受到次优轨迹的影响。因此需要一种新的方法来优化策略并减少这些影响。", "method": "提出了一种基于决策转换器(QGA)的方法，通过将Q值正则化与双重学习策略结合，同时进行策略模仿和行为价值最大化。此外，还提出了一个由Q值指导的双探索机制以安全地扩展政策空间。", "result": "在公共基准测试和模拟环境中，QGA方法表现出色，在大规模真实世界A/B测试中取得了3.27%广告GMV增加及2.49%广告ROI提升的结果。", "conclusion": "提出的QGA方法有效解决了现有自动出价策略的不足，并通过实验验证了其优越性。"}}
{"id": "2601.02751", "pdf": "https://arxiv.org/pdf/2601.02751", "abs": "https://arxiv.org/abs/2601.02751", "authors": ["Yuetian Chen", "Yuntao Du", "Kaiyuan Zhang", "Ashish Kundu", "Charles Fleming", "Bruno Ribeiro", "Ninghui Li"], "title": "Window-based Membership Inference Attacks Against Fine-tuned Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "Code is available at [https://github.com/Stry233/WBC/](https://github.com/Stry233/WBC/). This arXiv version corresponds to the accepted paper and includes the full experimental results", "summary": "Most membership inference attacks (MIAs) against Large Language Models (LLMs) rely on global signals, like average loss, to identify training data. This approach, however, dilutes the subtle, localized signals of memorization, reducing attack effectiveness. We challenge this global-averaging paradigm, positing that membership signals are more pronounced within localized contexts. We introduce WBC (Window-Based Comparison), which exploits this insight through a sliding window approach with sign-based aggregation. Our method slides windows of varying sizes across text sequences, with each window casting a binary vote on membership based on loss comparisons between target and reference models. By ensembling votes across geometrically spaced window sizes, we capture memorization patterns from token-level artifacts to phrase-level structures. Extensive experiments across eleven datasets demonstrate that WBC substantially outperforms established baselines, achieving higher AUC scores and 2-3 times improvements in detection rates at low false positive thresholds. Our findings reveal that aggregating localized evidence is fundamentally more effective than global averaging, exposing critical privacy vulnerabilities in fine-tuned LLMs.", "AI": {"tldr": "该论文提出了一种基于窗口比较的成员身份推理攻击方法，用于识别大型语言模型中的训练数据。", "motivation": "当前大多数针对大型语言模型的成员身份推理攻击依赖于全局信号，这种方法削弱了记忆化的局部信号，从而降低了攻击的有效性。因此，作者提出了一个基于局部上下文的方法来捕捉更细微的记忆化模式。", "method": "WBC方法通过滑动窗口技术和基于符号的聚合方法，在不同的文本序列上应用不同大小的滑动窗口进行二进制投票，以此判断目标模型与参考模型之间的损失差异。最终通过组合不同窗口大小的结果，实现对记忆化模式的有效捕捉。", "result": "实验表明WBC在多个数据集上的表现显著优于现有基线方法，在AUC和低误报率下的检测率方面表现出2-3倍的提升。", "conclusion": "论文发现局部证据聚合比全局平均更有效地识别大型语言模型中的训练数据，从而揭示了这些模型中存在的隐私漏洞。"}}
{"id": "2601.02749", "pdf": "https://arxiv.org/pdf/2601.02749", "abs": "https://arxiv.org/abs/2601.02749", "authors": ["Nadia Sibai", "Yara Ahmed", "Serry Sibaee", "Sawsan AlHalawani", "Adel Ammar", "Wadii Boulila"], "title": "The Path Ahead for Agentic AI: Challenges and Opportunities", "categories": ["cs.AI"], "comment": null, "summary": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.", "AI": {"tldr": "本文探讨大型语言模型向自主行动系统演化的路径，分析其技术挑战和机遇。", "motivation": "随着大模型从单纯文本生成工具发展为具有规划、记忆和迭代推理能力的自主系统，需要研究这些模型如何在复杂环境中进行有效操作的技术细节及伦理考量。", "method": "本文回顾了语言理解向自主行动发展的架构转变，并提出了一个集成感知、内存、计划和工具体现等核心组件的框架。同时评估了相关应用中的挑战与机会。", "result": "文章指出了关键的研究优先事项，包括可验证规划、大规模多代理协调、持久性记忆体系结构以及治理框架的需求。", "conclusion": "为了实现大模型潜在价值并降低误对齐和意外后果的风险，需要在技术稳健性、解释性和伦理保障方面同时取得进展。"}}
{"id": "2601.02747", "pdf": "https://arxiv.org/pdf/2601.02747", "abs": "https://arxiv.org/abs/2601.02747", "authors": ["Zixiao Wen", "Zhen Yang", "Xianjie Bao", "Lei Zhang", "Xiantai Xiang", "Wenshuai Li", "Yuhan Liu"], "title": "D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.", "AI": {"tldr": "提出了一种新的基于DETR的检测器D$^3$R-DETR，用于改善空中图像中的微小对象检测。", "motivation": "主流Transformer基线检测器在处理像素信息有限和物体密度变化大的情况下性能不佳，导致收敛慢且查询目标匹配不准确。为解决这些问题，作者提出了一种新的方法来改进微小目标的检测。", "method": "通过融合空间域和频率域的信息，D$^3$R-DETR优化了低级特征图，并利用它们的丰富细节预测更精确的对象密度图，从而指导模型精确定位微小对象。", "result": "在AI-TOD-v2数据集上的大量实验表明，D$^3$R-DETR优于现有最先进的检测器。", "conclusion": "该方法通过融合空间和频率域的信息来优化低级特征图，并利用这些细节改进了目标密度图的预测。这显著提高了微小对象检测的准确性。"}}
{"id": "2601.02738", "pdf": "https://arxiv.org/pdf/2601.02738", "abs": "https://arxiv.org/abs/2601.02738", "authors": ["Kexin Guo", "Zihan Yang", "Yuhang Liu", "Jindou Jia", "Xiang Yu"], "title": "Optimizing Control-Friendly Trajectories with Self-Supervised Residual Learning", "categories": ["cs.RO"], "comment": "10 pages, 9 figures", "summary": "Real-world physics can only be analytically modeled with a certain level of precision for modern intricate robotic systems. As a result, tracking aggressive trajectories accurately could be challenging due to the existence of residual physics during controller synthesis. This paper presents a self-supervised residual learning and trajectory optimization framework to address the aforementioned challenges. At first, unknown dynamic effects on the closed-loop model are learned and treated as residuals of the nominal dynamics, jointly forming a hybrid model. We show that learning with analytic gradients can be achieved using only trajectory-level data while enjoying accurate long-horizon prediction with an arbitrary integration step size. Subsequently, a trajectory optimizer is developed to compute the optimal reference trajectory with the residual physics along it minimized. It ends up with trajectories that are friendly to the following control level. The agile flight of quadrotors illustrates that by utilizing the hybrid dynamics, the proposed optimizer outputs aggressive motions that can be precisely tracked.", "AI": {"tldr": "提出了一种自监督残差学习和轨迹优化框架，以解决复杂机器人系统中的精确跟踪问题。", "motivation": "由于现代复杂机器人系统的实际物理现象难以精确建模，因此在控制器合成期间准确跟踪激进的轨迹具有挑战性。本文旨在通过学习未知的动力学影响来克服这一难题。", "method": "首先使用自监督残差学习法处理闭合回路模型中的动态效应，并将其作为名义动力学的一部分；然后开发了一个轨迹优化器，在最小化残差物理的同时计算最优参考轨迹，使得生成的轨迹更易于控制层跟踪。", "result": "利用混合动力学，所提优化器能够输出可精确跟踪的激进飞行运动，尤其是在四旋翼机上的敏捷飞行展示中得到了验证。", "conclusion": "该方法通过学习残差动力学并最小化其影响，成功实现了复杂机器人系统的高精度轨迹生成和跟踪任务。"}}
{"id": "2601.02737", "pdf": "https://arxiv.org/pdf/2601.02737", "abs": "https://arxiv.org/abs/2601.02737", "authors": ["Zanting Ye", "Xiaolong Niu", "Xuanbin Wu", "Xu Han", "Shengyuan Liu", "Jing Hao", "Zhihao Peng", "Hao Sun", "Jieqin Lv", "Fanghu Wang", "Yanchao Huang", "Hubing Wu", "Yixuan Yuan", "Habib Zaidi", "Arman Rahmim", "Yefeng Zheng", "Lijun Lu"], "title": "Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 6 tables", "summary": "While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.", "AI": {"tldr": "研究揭示了多模态大型语言模型在功能性成像任务上的感知差距，并提出了PET-Bench基准和原子视觉对齐（AVA）策略以改善其性能。", "motivation": "当前的多模态大型语言模型虽然在形态学模态的任务上表现出色，但在功能性成像方面的能力尚未被充分研究。引入了新的挑战——链式思维陷阱，并开发了PET-Bench基准来解决这些问题。", "method": "提出了原子视觉对齐（AVA）策略以改善语言生成和视觉证据之间的连接问题，通过强制模型学习低级功能感知后再进行高级诊断推理。", "result": "实验结果表明，AVA策略能够显著提高模型的准确性，使其在临床诊断中的表现更加可靠，并将性能提高了最多14.83%。", "conclusion": "研究揭示了当前多模态大型语言模型在功能性成像上的感知差距，并通过引入PET-Bench和AVA策略成功解决了这个问题。"}}
{"id": "2601.02736", "pdf": "https://arxiv.org/pdf/2601.02736", "abs": "https://arxiv.org/abs/2601.02736", "authors": ["Lingzhe Zhang", "Tong Jia", "Yunpeng Zhai", "Leyi Pan", "Chiming Duan", "Minghua He", "Pei Xiao", "Ying Li"], "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism", "categories": ["cs.SE", "cs.AI"], "comment": "accepted by ICSE-NIER'26", "summary": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.", "AI": {"tldr": "提出了一种基于假设验证的微服务根因分析框架SpecRCA，以提高准确性和效率。", "motivation": "现有基于大语言模型的方法在探索多样性及依赖大规模模型方面存在局限性，影响了其准确性和推理速度。", "method": "采用假设生成模块快速生成候选原因，并利用并行验证器高效地确认这些根因。", "result": "实验表明，SpecRCA相比其他方法，在精度和效率上都更胜一筹。", "conclusion": "该框架展示了其在复杂微服务环境下的可扩展性和解释性潜力。"}}
{"id": "2601.02735", "pdf": "https://arxiv.org/pdf/2601.02735", "abs": "https://arxiv.org/abs/2601.02735", "authors": ["Adrien Aumon", "Guy Wolf", "Kevin R. Moon", "Jake S. Rhodes"], "title": "Scalable Tree Ensemble Proximities in Python", "categories": ["cs.LG", "cs.DS", "cs.PF"], "comment": null, "summary": "Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.", "AI": {"tldr": "介绍了一种计算树集合近似的新框架，使其在大规模数据集上更加高效和可扩展。", "motivation": "现有的基于决策树的聚类方法时间或内存复杂度为二次方，限制了其在大数据上的应用。", "method": "提出一种新的分离加权叶碰撞接近度方法，并通过稀疏线性代数计算实现低内存、高效率的方法。", "result": "实验证明该方法相比传统方式有显著的时间和空间改进，在标准CPU上实现了对大规模数据集的高效近似计算。", "conclusion": "所提出的方法使得树集合近似能够有效处理包含成千上万样本的数据集，具有实际应用价值。"}}
{"id": "2601.02732", "pdf": "https://arxiv.org/pdf/2601.02732", "abs": "https://arxiv.org/abs/2601.02732", "authors": ["Lingzhe Zhang", "Tong Jia", "Yunpeng Zhai", "Leyi Pan", "Chiming Duan", "Minghua He", "Mengxi Jia", "Ying Li"], "title": "Agentic Memory Enhanced Recursive Reasoning for Root Cause Localization in Microservices", "categories": ["cs.SE", "cs.AI"], "comment": "accepted by ICSE-SEIP'26", "summary": "As contemporary microservice systems become increasingly popular and complex-often comprising hundreds or even thousands of fine-grained, interdependent subsystems-they are experiencing more frequent failures. Ensuring system reliability thus demands accurate root cause localization. While many traditional graph-based and deep learning approaches have been explored for this task, they often rely heavily on pre-defined schemas that struggle to adapt to evolving operational contexts. Consequently, a number of LLM-based methods have recently been proposed. However, these methods still face two major limitations: shallow, symptom-centric reasoning that undermines accuracy, and a lack of cross-alert reuse that leads to redundant reasoning and high latency. In this paper, we conduct a comprehensive study of how Site Reliability Engineers (SREs) localize the root causes of failures, drawing insights from professionals across multiple organizations. Our investigation reveals that expert root cause analysis exhibits three key characteristics: recursiveness, multi-dimensional expansion, and cross-modal reasoning. Motivated by these findings, we introduce AMER-RCL, an agentic memory enhanced recursive reasoning framework for root cause localization in microservices. AMER-RCL employs the Recursive Reasoning RCL engine, a multi-agent framework that performs recursive reasoning on each alert to progressively refine candidate causes, while Agentic Memory incrementally accumulates and reuses reasoning from prior alerts within a time window to reduce redundant exploration and lower inference latency. Experimental results demonstrate that AMER-RCL consistently outperforms state-of-the-art methods in both localization accuracy and inference efficiency.", "AI": {"tldr": "提出了AMER-RCL框架，用于微服务中的根因定位。", "motivation": "现有方法依赖预定义模式，难以适应动态环境，并存在浅层推理和重复推理的问题。因此，提出一个更准确、高效的根因定位解决方案。", "method": "基于专家分析的递归推理特性，开发AMER-RCL框架，结合多代理递归推理引擎与增量记忆机制以提高推理精度及效率。", "result": "实验结果表明，AMER-RCL在定位准确性及推断效率上优于现有方法。", "conclusion": "通过借鉴专家经验并利用递归推理和增量内存技术，AMER-RCL显著提升了微服务系统中根因定位的性能。"}}
{"id": "2601.02731", "pdf": "https://arxiv.org/pdf/2601.02731", "abs": "https://arxiv.org/abs/2601.02731", "authors": ["Yusheng Dai", "Zehua Chen", "Yuxuan Jiang", "Baolong Gao", "Qiuhong Ke", "Jun Zhu", "Jianfei Cai"], "title": "Omni2Sound: Towards Unified Video-Text-to-Audio Generation", "categories": ["cs.SD", "cs.CV", "cs.MM"], "comment": null, "summary": "Training a unified model integrating video-to-audio (V2A), text-to-audio (T2A), and joint video-text-to-audio (VT2A) generation offers significant application flexibility, yet faces two unexplored foundational challenges: (1) the scarcity of high-quality audio captions with tight A-V-T alignment, leading to severe semantic conflict between multimodal conditions, and (2) cross-task and intra-task competition, manifesting as an adverse V2A-T2A performance trade-off and modality bias in the VT2A task. First, to address data scarcity, we introduce SoundAtlas, a large-scale dataset (470k pairs) that significantly outperforms existing benchmarks and even human experts in quality. Powered by a novel agentic pipeline, it integrates Vision-to-Language Compression to mitigate visual bias of MLLMs, a Junior-Senior Agent Handoff for a 5 times cost reduction, and rigorous Post-hoc Filtering to ensure fidelity. Consequently, SoundAtlas delivers semantically rich and temporally detailed captions with tight V-A-T alignment. Second, we propose Omni2Sound, a unified VT2A diffusion model supporting flexible input modalities. To resolve the inherent cross-task and intra-task competition, we design a three-stage multi-task progressive training schedule that converts cross-task competition into joint optimization and mitigates modality bias in the VT2A task, maintaining both audio-visual alignment and off-screen audio generation faithfulness. Finally, we construct VGGSound-Omni, a comprehensive benchmark for unified evaluation, including challenging off-screen tracks. With a standard DiT backbone, Omni2Sound achieves unified SOTA performance across all three tasks within a single model, demonstrating strong generalization across benchmarks with heterogeneous input conditions. The project page is at https://swapforward.github.io/Omni2Sound.", "AI": {"tldr": "本文提出了一种统一的视频、文本到音频生成模型Omni2Sound，解决多模态数据稀缺及跨任务和内部竞争问题。", "motivation": "训练一个能同时进行视频转音频（V2A）、文本转音频（T2A）以及联合视频文本转音频（VT2A）的统一模型可以提供广泛的应用灵活性。但面临两个未解决的基本挑战：高质量多模态数据稀缺导致语义冲突和跨任务及内部竞争。", "method": "首先，通过Vision-to-Language Compression缓解视觉偏见，并采用Junior-Senior Agent Handoff减少5倍成本；其次，利用Post-hoc Filtering确保数据质量，创建SoundAtlas大型数据集。此外，设计了三阶段多任务训练方案解决跨任务及内部竞争问题。", "result": "Omni2Sound模型实现了视频、文本到音频生成的统一SOTA性能，在所有三项任务中都表现优异，并在不同的输入条件下展现出强大的泛化能力。", "conclusion": "本文通过引入大型数据集和多阶段训练方案，解决了跨模态数据稀缺及内部竞争问题，成功构建了能够处理复杂条件下的视频、文本到音频生成的统一模型Omni2Sound。"}}
{"id": "2601.02730", "pdf": "https://arxiv.org/pdf/2601.02730", "abs": "https://arxiv.org/abs/2601.02730", "authors": ["Xuchang Zhong", "Xu Cao", "Jinke Feng", "Hao Fang"], "title": "HOLO: Homography-Guided Pose Estimator Network for Fine-Grained Visual Localization on SD Maps", "categories": ["cs.CV"], "comment": null, "summary": "Visual localization on standard-definition (SD) maps has emerged as a promising low-cost and scalable solution for autonomous driving. However, existing regression-based approaches often overlook inherent geometric priors, resulting in suboptimal training efficiency and limited localization accuracy. In this paper, we propose a novel homography-guided pose estimator network for fine-grained visual localization between multi-view images and standard-definition (SD) maps. We construct input pairs that satisfy a homography constraint by projecting ground-view features into the BEV domain and enforcing semantic alignment with map features. Then we leverage homography relationships to guide feature fusion and restrict the pose outputs to a valid feasible region, which significantly improves training efficiency and localization accuracy compared to prior methods relying on attention-based fusion and direct 3-DoF pose regression. To the best of our knowledge, this is the first work to unify BEV semantic reasoning with homography learning for image-to-map localization. Furthermore, by explicitly modeling homography transformations, the proposed framework naturally supports cross-resolution inputs, enhancing model flexibility. Extensive experiments on the nuScenes dataset demonstrate that our approach significantly outperforms existing state-of-the-art visual localization methods. Code and pretrained models will be publicly released to foster future research.", "AI": {"tldr": "本文提出了一种基于同态约束的新型姿态估计网络，用于多视图图像和标准定义地图之间的精细视觉定位。", "motivation": "现有的回归方法忽略了固有的几何先验，导致训练效率低下且定位精度有限。作者旨在通过引入几何引导的方法来提高视觉定位的准确性和效率。", "method": "本文提出了一种基于同态约束的姿态估计网络（HOLO），该网络利用同态关系进行特征融合，并限制姿态输出在一个有效的可行区域中，从而显著提高了训练效率和定位精度。", "result": "实验结果表明，所提出的模型在nuScenes数据集上显著优于现有的视觉定位方法。", "conclusion": "本文首次统一了俯视图语义推理与同态学习的方法，并通过显式建模同态变换增强了模型的灵活性。"}}
{"id": "2601.02727", "pdf": "https://arxiv.org/pdf/2601.02727", "abs": "https://arxiv.org/abs/2601.02727", "authors": ["Longzhen Li", "Guang Li", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Foreground-Aware Dataset Distillation via Dynamic Patch Selection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose a foreground-aware dataset distillation method that enhances patch selection in a content-adaptive manner. With the rising computational cost of training large-scale deep models, dataset distillation has emerged as a promising approach for constructing compact synthetic datasets that retain the knowledge of their large original counterparts. However, traditional optimization-based methods often suffer from high computational overhead, memory constraints, and the generation of unrealistic, noise-like images with limited architectural generalization. Recent non-optimization methods alleviate some of these issues by constructing distilled data from real image patches, but the used rigid patch selection strategies can still discard critical information about the main objects. To solve this problem, we first leverage Grounded SAM2 to identify foreground objects and compute per-image foreground occupancy, from which we derive a category-wise patch decision threshold. Guided by these thresholds, we design a dynamic patch selection strategy that, for each image, either selects the most informative patch from multiple candidates or directly resizes the full image when the foreground dominates. This dual-path mechanism preserves more key information about the main objects while reducing redundant background content. Extensive experiments on multiple benchmarks show that the proposed method consistently improves distillation performance over existing approaches, producing more informative and representative distilled datasets and enhancing robustness across different architectures and image compositions.", "AI": {"tldr": "本文提出了一种基于前景感知的数据集蒸馏方法，通过动态选择关键补丁来构建紧凑的合成数据集。", "motivation": "为了缓解大规模深度模型训练中的高计算成本问题，并解决现有数据集蒸馏方法在生成不现实图像和信息丢失方面的问题，作者提出了这种方法。", "method": "利用Grounded SAM2识别前景物体并计算每张图片的前景占有率，据此制定类别级别的补丁选择阈值。根据这些阈值设计动态补丁选择策略，在每个图中要么选择最具有信息量的关键补丁，要么直接缩放整幅图像以保留主要对象的信息。", "result": "实验表明，该方法在多个基准测试中持续提升了数据集蒸馏的表现，生成了更具信息和代表性的合成数据，并增强了不同架构及图像组合下的鲁棒性。", "conclusion": "通过改进的动态补丁选择策略，新方法有效地减少了背景冗余内容并提高了关键对象的信息保留率，从而改善了数据集蒸馏的效果。"}}
{"id": "2601.02723", "pdf": "https://arxiv.org/pdf/2601.02723", "abs": "https://arxiv.org/abs/2601.02723", "authors": ["Wenzheng Zhang", "Kazuki Adachi", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Loop Closure using AnyLoc Visual Place Recognition in DPV-SLAM", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at IEEE/SICE International Symposium on System Integration(SII) 2026. 6 pages, 14 figures", "summary": "Loop closure is crucial for maintaining the accuracy and consistency of visual SLAM. We propose a method to improve loop closure performance in DPV-SLAM. Our approach integrates AnyLoc, a learning-based visual place recognition technique, as a replacement for the classical Bag of Visual Words (BoVW) loop detection method. In contrast to BoVW, which relies on handcrafted features, AnyLoc utilizes deep feature representations, enabling more robust image retrieval across diverse viewpoints and lighting conditions. Furthermore, we propose an adaptive mechanism that dynamically adjusts similarity threshold based on environmental conditions, removing the need for manual tuning. Experiments on both indoor and outdoor datasets demonstrate that our method significantly outperforms the original DPV-SLAM in terms of loop closure accuracy and robustness. The proposed method offers a practical and scalable solution for enhancing loop closure performance in modern SLAM systems.", "AI": {"tldr": "本文提出了一种改进DPV-SLAM中循环闭合性能的方法，通过将AnyLoc学习型视觉地方识别技术集成进来替代传统的BoVW方法。", "motivation": "传统SLAM系统中的循环闭合对于保持精度和一致性至关重要，但基于手工特征的传统方法在不同视角和光照条件下不够鲁棒。本文旨在改进DPV-SLAM的循环闭合性能，提高其准确性和稳健性。", "method": "文章采用了AnyLoc作为视觉地方识别技术，并提出了一个自适应机制动态调整相似度阈值以适应环境变化，无需手动调整参数。", "result": "实验结果表明，在室内和室外数据集上该方法在循环闭合准确性及鲁棒性方面显著优于原DPV-SLAM系统。", "conclusion": "本文提供的方法为提高现代SLAM系统的循环闭合性能提供了一个实用且可扩展的解决方案。"}}
{"id": "2601.02721", "pdf": "https://arxiv.org/pdf/2601.02721", "abs": "https://arxiv.org/abs/2601.02721", "authors": ["Guoquan Zheng", "Jie Hao", "Huiyu Duan", "Yongming Han", "Liang Yuan", "Dong Zhang", "Guangtao Zhai"], "title": "Robust Mesh Saliency GT Acquisition in VR via View Cone Sampling and Geometric Smoothing", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Reliable 3D mesh saliency ground truth (GT) is essential for human-centric visual modeling in virtual reality (VR). However, current 3D mesh saliency GT acquisition methods are generally consistent with 2D image methods, ignoring the differences between 3D geometry topology and 2D image array. Current VR eye-tracking pipelines rely on single ray sampling and Euclidean smoothing, triggering texture attention and signal leakage across gaps. This paper proposes a robust framework to address these limitations. We first introduce a view cone sampling (VCS) strategy, which simulates the human foveal receptive field via Gaussian-distributed ray bundles to improve sampling robustness for complex topologies. Furthermore, a hybrid Manifold-Euclidean constrained diffusion (HCD) algorithm is developed, fusing manifold geodesic constraints with Euclidean scales to ensure topologically-consistent saliency propagation. By mitigating \"topological short-circuits\" and aliasing, our framework provides a high-fidelity 3D attention acquisition paradigm that aligns with natural human perception, offering a more accurate and robust baseline for 3D mesh saliency research.", "AI": {"tldr": "本文提出了一种鲁棒框架，用于在虚拟现实中通过视锥采样和几何平滑获取可靠的3D网格注意力基线。", "motivation": "当前的3D网格注意力基线获取方法依赖于2D图像技术，忽略了3D几何拓扑与2D图像之间的差异。现有的VR眼动追踪管道依赖单一射线采样和欧几里得平滑，导致纹理注意和信号泄漏跨越间隙。", "method": "本文提出了一种视锥采样策略（VCS），通过高斯分布的光线束模拟人类中心视野，提高复杂拓扑结构下的采样鲁棒性。还开发了混合流形-欧几里得约束扩散算法（HCD），融合了流形测地线限制和欧氏尺度以确保一致性的注意力传播。", "result": "该框架通过减少“拓扑短路”和混叠现象，提供了与自然人类感知相匹配的高保真度3D注意力获取范式。", "conclusion": "本文提出的方法为3D网格注意研究提供了一个更加准确和鲁棒的基础线。"}}
{"id": "2601.02720", "pdf": "https://arxiv.org/pdf/2601.02720", "abs": "https://arxiv.org/abs/2601.02720", "authors": ["Yuqiao Xu", "Mina Namazi", "Sahith Reddy Jalapally", "Osama Zafar", "Youngjin Yoo", "Erman Ayday"], "title": "Privacy-Preserving AI-Enabled Decentralized Learning and Employment Records System", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Learning and Employment Record (LER) systems are emerging as critical infrastructure for securely compiling and sharing educational and work achievements. Existing blockchain-based platforms leverage verifiable credentials but typically lack automated skill-credential generation and the ability to incorporate unstructured evidence of learning. In this paper,a privacy-preserving, AI-enabled decentralized LER system is proposed to address these gaps. Digitally signed transcripts from educational institutions are accepted, and verifiable self-issued skill credentials are derived inside a trusted execution environment (TEE) by a natural language processing pipeline that analyzes formal records (e.g., transcripts, syllabi) and informal artifacts. All verification and job-skill matching are performed inside the enclave with selective disclosure, so raw credentials and private keys remain enclave-confined. Job matching relies solely on attested skill vectors and is invariant to non-skill resume fields, thereby reducing opportunities for screening bias.The NLP component was evaluated on sample learner data; the mapping follows the validated Syllabus-to-O*NET methodology,and a stability test across repeated runs observed <5% variance in top-ranked skills. Formal security statements and proof sketches are provided showing that derived credentials are unforgeable and that sensitive information remains confidential. The proposed system thus supports secure education and employment credentialing, robust transcript verification,and automated, privacy-preserving skill extraction within a decentralized framework.", "AI": {"tldr": "本文提出了一种隐私保护的AI增强型去中心化学习与就业记录系统，用于自动化生成技能凭证并分析非结构化的学习证据。", "motivation": "现有的基于区块链的学习和就业记录平台缺乏自动化的技能凭证生成能力和对非结构化学习证据的支持。本文旨在通过隐私保护的方法解决这些问题，并减少简历中的筛选偏差机会。", "method": "该系统利用自然语言处理（NLP）管道在受信任的执行环境中分析正式与非正式的学习材料，从而生成可验证的技能凭证。所有验证和工作匹配均在隔离执行环境内进行，以保持原始凭证和个人密钥的安全性。", "result": "通过对示例学习者数据进行评估，自然语言处理组件表现出较高的稳定性和准确性，在多次运行中观察到前排技能分类的一致性低于5%的方差。正式的安全声明和证明草图也表明生成的凭证无法伪造且敏感信息保持保密。", "conclusion": "本文提出的方法支持安全的教育和就业认证，加强了成绩单验证，并在去中心化的框架内实现了自动化、隐私保护技能提取功能。"}}
{"id": "2601.02716", "pdf": "https://arxiv.org/pdf/2601.02716", "abs": "https://arxiv.org/abs/2601.02716", "authors": ["Taeyeon Kim", "Youngju Na", "Jumin Lee", "Minhyuk Sung", "Sung-Eui Yoon"], "title": "CAMO: Category-Agnostic 3D Motion Transfer from Monocular 2D Videos", "categories": ["cs.CV"], "comment": "Project website: https://camo-project-page.github.io/", "summary": "Motion transfer from 2D videos to 3D assets is a challenging problem, due to inherent pose ambiguities and diverse object shapes, often requiring category-specific parametric templates. We propose CAMO, a category-agnostic framework that transfers motion to diverse target meshes directly from monocular 2D videos without relying on predefined templates or explicit 3D supervision. The core of CAMO is a morphology-parameterized articulated 3D Gaussian splatting model combined with dense semantic correspondences to jointly adapt shape and pose through optimization. This approach effectively alleviates shape-pose ambiguities, enabling visually faithful motion transfer for diverse categories. Experimental results demonstrate superior motion accuracy, efficiency, and visual coherence compared to existing methods, significantly advancing motion transfer in varied object categories and casual video scenarios.", "AI": {"tldr": "本文提出了一种从单目二维视频中进行三维运动转移的方法CAMO，该方法无需预定义模板或显式三维监督。", "motivation": "现有的运动转移技术通常需要特定类别的参数化模板，并且难以处理不同形状和姿势的物体。作者希望通过一种不依赖于类别信息的方法来解决这个问题。", "method": "核心是形态参数化的3D高斯点模型，结合密集语义对应关系，通过优化同时调整形状和姿态，从而减轻形体与姿态之间的歧义问题。", "result": "实验结果表明，在运动准确性、效率和视觉一致性方面均优于现有方法，并且在各种物体类别和日常视频场景中表现出色。", "conclusion": "本文提出的CAMO框架成功实现了从单目二维视频到三维模型的跨类别的运动转移，具有广泛的应用前景。"}}
{"id": "2601.02714", "pdf": "https://arxiv.org/pdf/2601.02714", "abs": "https://arxiv.org/abs/2601.02714", "authors": ["Zhi Liu", "Guangzhi Wang"], "title": "Time-Scaling Is What Agents Need Now", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities. Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency. This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.", "AI": {"tldr": "论文提出时间尺度(Time-Scaling)概念，以提高智能体的深度推理和问题解决能力。", "motivation": "早期大型语言模型虽然能生成流畅文本，但缺乏稳健的语义推理能力。通过时间尺度可以系统性扩展并优化智能体的时间推理路径，从而更深入地探索问题空间、动态调整策略并增强元认知控制。", "method": "提出时间尺度概念，利用延长的时间路径架构实现深度问题空间搜索和动态策略调整。", "result": "未具体提及实验结果或数据验证，强调时间尺度对于提高模型性能的重要性。", "conclusion": "将时间尺度原理置于智能体设计的核心位置，以增强其深层推理和解决问题的能力。"}}
{"id": "2601.02712", "pdf": "https://arxiv.org/pdf/2601.02712", "abs": "https://arxiv.org/abs/2601.02712", "authors": ["Alican Nalci", "Hilmi E. Egilmez", "Madhu P. Krishnan", "Keng-Shih Lu", "Joe Young", "Debargha Mukherjee", "Lin Zheng", "Jingning Han", "Joel Sole", "Xin Zhao", "Tianqi Liu", "Liang Zhao", "Todd Nguyen", "Urvang Joshi", "Kruthika Koratti Sivakumar", "Luhang Xu", "Zhijun Lei", "Yue Yu", "Aki Kuusela", "Minhua Zhou", "Andrey Norkin", "Adrian Grange"], "title": "Transform and Entropy Coding in AV2", "categories": ["eess.IV", "cs.MM"], "comment": null, "summary": "AV2 is the successor to the AV1 royalty-free video coding standard developed by the Alliance for Open Media (AOMedia). Its primary objective is to deliver substantial compression gains and subjective quality improvements while maintaining low-complexity encoder and decoder operations. This paper describes the transform, quantization and entropy coding design in AV2, including redesigned transform kernels and data-driven transforms, expanded transform partitioning, and a mode & coefficient dependent transform signaling. AV2 introduces several new coding tools including Intra/Inter Secondary Transforms (IST), Trellis Coded Quantization (TCQ), Adaptive Transform Coding (ATC), Probability Adaptation Rate Adjustment (PARA), Forward Skip Coding (FSC), Cross Chroma Component Transforms (CCTX), Parity Hiding (PH) tools and improved lossless coding. These advances enable AV2 to deliver the highest quality video experience for video applications at a significantly reduced bitrate.", "AI": {"tldr": "AV2标准中变换、量化和熵编码的设计", "motivation": "降低比特率同时提高视频质量", "method": "引入了IST、TCQ、ATC、PARA、FSC等新工具", "result": "提高了视频质量和压缩效率", "conclusion": "新的编码技术使AV2能够在更低的比特率下提供更好的视频体验"}}
{"id": "2601.02709", "pdf": "https://arxiv.org/pdf/2601.02709", "abs": "https://arxiv.org/abs/2601.02709", "authors": ["Shuman He", "Xiehua Li", "Xioaju Yang", "Yang Xiong", "Keqin Li"], "title": "GRRE: Leveraging G-Channel Removed Reconstruction Error for Robust Detection of AI-Generated Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of generative models, particularly diffusion models and GANs, has greatly increased the difficulty of distinguishing synthetic images from real ones. Although numerous detection methods have been proposed, their accuracy often degrades when applied to images generated by novel or unseen generative models, highlighting the challenge of achieving strong generalization. To address this challenge, we introduce a novel detection paradigm based on channel removal reconstruction. Specifically, we observe that when the green (G) channel is removed from real images and reconstructed, the resulting reconstruction errors differ significantly from those of AI-generated images. Building upon this insight, we propose G-channel Removed Reconstruction Error (GRRE), a simple yet effective method that exploits this discrepancy for robust AI-generated image detection. Extensive experiments demonstrate that GRRE consistently achieves high detection accuracy across multiple generative models, including those unseen during training. Compared with existing approaches, GRRE not only maintains strong robustness against various perturbations and post-processing operations but also exhibits superior cross-model generalization. These results highlight the potential of channel-removal-based reconstruction as a powerful forensic tool for safeguarding image authenticity in the era of generative AI.", "AI": {"tldr": "本文提出了一种基于通道移除重构的新颖检测方法，通过观察真实图像去除绿色(G)通道后的重构误差与AI生成图像的差异来识别AI生成图片。", "motivation": "随着扩散模型和GAN等生成模型的发展，区分合成图像和真实图像变得越来越困难。尽管已提出许多检测方法，但它们在应用于新或未见的生成模型时准确性下降。", "method": "本文观察到去除G通道后的真实图像重构误差与AI生成图像之间存在显著差异，并据此提出了利用这种差异的新方法GRRE。", "result": "实验表明，GRRE在多种不同生成模型上均能保持高检测精度和强大的泛化能力。", "conclusion": "GRRE显示了基于通道移除的重构作为防护图像真实性工具的巨大潜力，在对抗各种干扰和后处理操作方面表现出色。"}}
{"id": "2601.02708", "pdf": "https://arxiv.org/pdf/2601.02708", "abs": "https://arxiv.org/abs/2601.02708", "authors": ["HuiJeong Son", "Hyeongu Kang", "Sunho Kim", "Subeen Ho", "SeongKu Kang", "Dongha Lee", "Susik Yoon"], "title": "CREAM: Continual Retrieval on Dynamic Streaming Corpora with Adaptive Soft Memory", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to KDD 2026", "summary": "Information retrieval (IR) in dynamic data streams is emerging as a challenging task, as shifts in data distribution degrade the performance of AI-powered IR systems. To mitigate this issue, memory-based continual learning has been widely adopted for IR. However, existing methods rely on a fixed set of queries with ground-truth relevant documents, which limits generalization to unseen queries and documents, making them impractical for real-world applications. To enable more effective learning with unseen topics of a new corpus without ground-truth labels, we propose CREAM, a self-supervised framework for memory-based continual retrieval. CREAM captures the evolving semantics of streaming queries and documents into dynamically structured soft memory and leverages it to adapt to both seen and unseen topics in an unsupervised setting. We realize this through three key techniques: fine-grained similarity estimation, regularized cluster prototyping, and stratified coreset sampling. Experiments on two benchmark datasets demonstrate that CREAM exhibits superior adaptability and retrieval accuracy, outperforming the strongest method in a label-free setting by 27.79\\% in Success@5 and 44.5\\% in Recall@10 on average, and achieving performance comparable to or even exceeding that of supervised methods.", "AI": {"tldr": "该论文提出了一种名为CREAM的自监督框架，用于在动态数据流中实现记忆基连续检索。", "motivation": "现有的信息检索方法依赖于固定的查询和相关文档集，限制了其对未见过的问题和文档的学习能力，并不适用于真实世界的场景。为此，作者提出了CREAM来解决这一问题。", "method": "CREAM通过三个关键技术实现了动态软记忆的构建：细粒度相似性估计、正则化聚类原型以及分层核心集采样。", "result": "实验表明，在两个基准数据集上，CREAM表现出色，优于其他无标签方法，并且其性能与有监督的方法相当甚至更好。", "conclusion": "该研究提出了一种有效的自监督框架CREAM，能够适应动态变化的数据流并提高检索准确性。"}}
{"id": "2601.02704", "pdf": "https://arxiv.org/pdf/2601.02704", "abs": "https://arxiv.org/abs/2601.02704", "authors": ["Kento Kawaharazuka", "Keita Yoneda", "Takahiro Hattori", "Shintaro Inoue", "Kei Okada"], "title": "Analysis of Various Manipulator Configurations Based on Multi-Objective Black-Box Optimization", "categories": ["cs.RO"], "comment": "Accepted to Advanced Robotics, website: https://haraduka.github.io/bbo-manip-design", "summary": "Various 6-degree-of-freedom (DOF) and 7-DOF manipulators have been developed to date. Over a long history, their joint configurations and link length ratios have been determined empirically. In recent years, the development of robotic foundation models has become increasingly active, leading to the continuous proposal of various manipulators to support these models. However, none of these manipulators share exactly the same structure, as the order of joints and the ratio of link lengths differ among robots. Therefore, in order to discuss the optimal structure of a manipulator, we performed multi-objective optimization from the perspectives of end-effector reachability and joint torque. We analyze where existing manipulator structures stand within the sampling results of the optimization and provide insights for future manipulator design.", "AI": {"tldr": "基于多目标黑盒优化分析不同机械臂配置的最优结构", "motivation": "为了确定最佳机械臂结构，解决当前各种机械臂由于关节顺序和连杆长度比例的不同而无法统一的问题。", "method": "从末端执行器可达性和关节扭矩的角度进行多目标优化，并将现有机械臂结构与采样结果对比分析。", "result": "提供了未来机械臂设计的见解，展示了现有机械臂在优化方案中的位置。", "conclusion": "通过多目标黑盒优化方法找到了当前各种机械臂配置的优势和局限性，为未来的机械臂设计指明了方向。"}}
{"id": "2601.02702", "pdf": "https://arxiv.org/pdf/2601.02702", "abs": "https://arxiv.org/abs/2601.02702", "authors": ["Shuhaib Mehri", "Priyanka Kargupta", "Tal August", "Dilek Hakkani-Tür"], "title": "Learning User Preferences Through Interaction for Long-Term Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "As conversational agents accumulate experience collaborating with users, adapting to user preferences is essential for fostering long-term relationships and improving collaboration quality over time. We introduce MultiSessionCollab, a benchmark that evaluates how well agents can learn user preferences and leverage them to improve collaboration quality throughout multiple sessions. To develop agents that succeed in this setting, we present long-term collaborative agents equipped with a memory that persists and refines user preference as interaction experience accumulates. Moreover, we demonstrate that learning signals can be derived from user simulator behavior in MultiSessionCollab to train agents to generate more comprehensive reflections and update their memory more effectively. Extensive experiments show that equipping agents with memory improves long-term collaboration, yielding higher task success rates, more efficient interactions, and reduced user effort. Finally, we conduct a human user study that demonstrates that memory helps improve user experience in real-world settings.", "AI": {"tldr": "本文介绍了MultiSessionCollab，一个评估代理学习用户偏好并提升长期合作质量能力的基准。", "motivation": "对话代理与用户的交互中积累经验时，适应用户偏好的重要性在于促进长期关系和提高协作质量。", "method": "提出了具有持久且可以随交互经验积累而细化用户偏好的记忆机制的长期协作代理，并通过多轮会话中的模拟者行为来训练生成更全面反思和有效更新记忆的能力。", "result": "实验显示，装备有记忆功能的代理人能够提升长期合作的质量，提高任务成功率、互动效率并减少用户努力。", "conclusion": "人机研究还表明，在现实世界环境中，记忆机制有助于改善用户体验。"}}
{"id": "2601.02700", "pdf": "https://arxiv.org/pdf/2601.02700", "abs": "https://arxiv.org/abs/2601.02700", "authors": ["Agniv Roy Choudhury", "Vignesh Ponselvan Rajasingh"], "title": "Adversarial Question Answering Robustness: A Multi-Level Error Analysis and Mitigation Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering (QA) systems achieve impressive performance on standard benchmarks like SQuAD, but remain vulnerable to adversarial examples. This project investigates the adversarial robustness of transformer models on the AddSent adversarial dataset through systematic experimentation across model scales and targeted mitigation strategies. We perform comprehensive multi-level error analysis using five complementary categorization schemes, identifying negation confusion and entity substitution as the primary failure modes. Through systematic evaluation of adversarial fine-tuning ratios, we identify 80% clean + 20% adversarial data as optimal. Data augmentation experiments reveal a capacity bottleneck in small models. Scaling from ELECTRA-small (14M parameters) to ELECTRA-base (110M parameters) eliminates the robustness-accuracy trade-off, achieving substantial improvements on both clean and adversarial data. We implement three targeted mitigation strategies, with Entity-Aware contrastive learning achieving best performance: 89.89% AddSent Exact Match (EM) and 90.73% SQuAD EM, representing 94.9% closure of the adversarial gap. To our knowledge, this is the first work integrating comprehensive linguistic error analysis with Named Entity Recognition (NER)-guided contrastive learning for adversarial QA, demonstrating that targeted mitigation can achieve near-parity between clean and adversarial performance.", "AI": {"tldr": "该论文研究了在AddSent对抗数据集上变压器模型的问答系统的鲁棒性，并通过系统实验和策略缓解来提高其性能。", "motivation": "尽管问答系统在标准基准测试中表现出色，但仍然容易受到对抗样本的影响。这项工作旨在调查这些模型的鲁棒性和准确性之间的权衡，并探索有效的缓解策略。", "method": "论文采用多级错误分析方法，使用五种互补分类方案来识别主要失败模式；通过系统评估对抗细调比率和数据增强实验以找到最佳配置；实施了三种针对性缓解策略：实体感知对比学习等。", "result": "研究发现80%的清洁数据加20%对抗性数据是最优比例；从小规模模型扩展到大规模模型可以消除鲁棒性和准确性之间的权衡，实现对清洁和对抗数据的显著改进；通过名为Entity-Aware对比学习的针对性缓解策略实现了最佳性能：在AddSent上的精确匹配（EM）得分为89.89%，SQuAD EM为90.73%。", "conclusion": "该论文展示了多级错误分析与基于命名实体识别(NER)导向的对比学习相结合的有效性，证明了针对性缓解策略能够实现清洁和对抗性能之间的接近平衡。"}}
{"id": "2601.02688", "pdf": "https://arxiv.org/pdf/2601.02688", "abs": "https://arxiv.org/abs/2601.02688", "authors": ["Guo Yifan", "Tian Yao", "Suo Hongbin", "Wan Yulong"], "title": "Multi-channel multi-speaker transformer for speech recognition", "categories": ["cs.SD", "cs.AI"], "comment": "Proc. INTERSPEECH 2023, 5 pages", "summary": "With the development of teleconferencing and in-vehicle voice assistants, far-field multi-speaker speech recognition has become a hot research topic. Recently, a multi-channel transformer (MCT) has been proposed, which demonstrates the ability of the transformer to model far-field acoustic environments. However, MCT cannot encode high-dimensional acoustic features for each speaker from mixed input audio because of the interference between speakers. Based on these, we propose the multi-channel multi-speaker transformer (M2Former) for far-field multi-speaker ASR in this paper. Experiments on the SMS-WSJ benchmark show that the M2Former outperforms the neural beamformer, MCT, dual-path RNN with transform-average-concatenate and multi-channel deep clustering based end-to-end systems by 9.2%, 14.3%, 24.9%, and 52.2% respectively, in terms of relative word error rate reduction.", "AI": {"tldr": "提出了多通道多说话人变压器（M2Former）以改善远场多说话人的语音识别。", "motivation": "随着远程会议和车载语音助手的发展，远场多说话人的语音识别成为了一个热门的研究领域。当前模型在处理混合音频中的每个说话人的高维声学特征时存在干扰问题。", "method": "提出了一种新的多通道多说话人变压器（M2Former），通过改进的架构能够更好地编码来自混合输入音频中各个说话人的高维度声学特性，提升了远场多说话人语音识别性能。", "result": "实验结果表明，在SMS-WSJ基准测试上，M2Former比其他系统在相对词错误率减少方面表现出色：分别比神经波束成形器、多通道变压器（MCT）、双路径RNN与变换平均连接和基于多通道深度聚类的端到端系统提高了9.2%，14.3%，24.9%和52.2%。", "conclusion": "所提出的M2Former模型在处理远场环境下的多说话人语音识别任务中取得了显著的效果提升，展示了其潜在的应用价值。"}}
{"id": "2601.02686", "pdf": "https://arxiv.org/pdf/2601.02686", "abs": "https://arxiv.org/abs/2601.02686", "authors": ["Haixin Jin", "Nikhil Uday Shinde", "Soofiyan Atar", "Hongzhan Yu", "Dylan Hirsch", "Sicun Gao", "Michael C. Yip", "Sylvia Herbert"], "title": "Learning to Nudge: A Scalable Barrier Function Framework for Safe Robot Interaction in Dense Clutter", "categories": ["cs.RO"], "comment": null, "summary": "Robots operating in everyday environments must navigate and manipulate within densely cluttered spaces, where physical contact with surrounding objects is unavoidable. Traditional safety frameworks treat contact as unsafe, restricting robots to collision avoidance and limiting their ability to function in dense, everyday settings. As the number of objects grows, model-based approaches for safe manipulation become computationally intractable; meanwhile, learned methods typically tie safety to the task at hand, making them hard to transfer to new tasks without retraining. In this work we introduce Dense Contact Barrier Functions(DCBF). Our approach bypasses the computational complexity of explicitly modeling multi-object dynamics by instead learning a composable, object-centric function that implicitly captures the safety constraints arising from physical interactions. Trained offline on interactions with a few objects, the learned DCBFcomposes across arbitrary object sets at runtime, producing a single global safety filter that scales linearly and transfers across tasks without retraining. We validate our approach through simulated experiments in dense clutter, demonstrating its ability to enable collision-free navigation and safe, contact-rich interaction in suitable settings.", "AI": {"tldr": "本文提出了密集接触屏障函数（DCBF），以解决机器人在复杂环境中安全交互的问题。", "motivation": "传统安全框架将物理接触视为不安全，限制了机器人的功能。模型方法因计算复杂性而难以应用，学习方法又难以跨任务转移。", "method": "通过训练一个可组合的对象中心函数来捕捉物理互动的安全约束，该方法在离线阶段使用少量对象进行训练，并在线时可以在任意对象集合中运行。", "result": "实验表明，所提出的方法能够在密集环境中实现无碰撞导航和安全交互。", "conclusion": "DCBF 方法证明了其能够有效解决机器人在复杂环境中的安全问题，同时具备良好的可扩展性和跨任务转移能力。"}}
{"id": "2601.02683", "pdf": "https://arxiv.org/pdf/2601.02683", "abs": "https://arxiv.org/abs/2601.02683", "authors": ["Dongyu Chen", "Jian Ma", "Xianpeng Zhang", "Lei Zhang", "Haonan Lu", "Chen Chen", "Chuangchuang Wang", "Kai Tang"], "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.", "AI": {"tldr": "本论文提出了一种分层属性提示优化框架（HAPO），旨在解决现有提示工程方法中的问题，如提示漂移和缺乏可解释性。", "motivation": "现有的提示优化方法常存在提示漂移的问题，并且从头生成的提示可能会破坏可解释性。因此，需要一种新的方法来提高提示性能并保持其可解释性。", "method": "本论文提出的方法包括动态属性机制、语义单元优化和多模态友好的进展，这三个创新点共同构成了HAPO框架，用于解决上述问题。", "result": "在单/多图QA等任务中应用后，HAPO显示出更高的优化效率，并优于现有的自动化提示优化方法。", "conclusion": "通过引入HAPO框架，该研究为可扩展的提示工程设定了一个有前景的方向。"}}
{"id": "2601.02682", "pdf": "https://arxiv.org/pdf/2601.02682", "abs": "https://arxiv.org/abs/2601.02682", "authors": ["Jie Peng", "Weiyu Li", "Stefan Vlaski", "Qing Ling"], "title": "Topology-Independent Robustness of the Weighted Mean under Label Poisoning Attacks in Heterogeneous Decentralized Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Robustness to malicious attacks is crucial for practical decentralized signal processing and machine learning systems. A typical example of such attacks is label poisoning, meaning that some agents possess corrupted local labels and share models trained on these poisoned data. To defend against malicious attacks, existing works often focus on designing robust aggregators; meanwhile, the weighted mean aggregator is typically considered a simple, vulnerable baseline. This paper analyzes the robustness of decentralized gradient descent under label poisoning attacks, considering both robust and weighted mean aggregators. Theoretical results reveal that the learning errors of robust aggregators depend on the network topology, whereas the performance of weighted mean aggregator is topology-independent. Remarkably, the weighted mean aggregator, although often considered vulnerable, can outperform robust aggregators under sufficient heterogeneity, particularly when: (i) the global contamination rate (i.e., the fraction of poisoned agents for the entire network) is smaller than the local contamination rate (i.e., the maximal fraction of poisoned neighbors for the regular agents); (ii) the network of regular agents is disconnected; or (iii) the network of regular agents is sparse and the local contamination rate is high. Empirical results support our theoretical findings, highlighting the important role of network topology in the robustness to label poisoning attacks.", "AI": {"tldr": "分析了加权平均聚合器在标签污染攻击下的鲁棒性，并对比了其与健壮聚合器的表现。", "motivation": "探讨分布式学习系统中如何防御恶意标签污染攻击，研究不同聚合策略的鲁棒性差异。", "method": "通过理论分析和实证实验，比较了加权平均聚合器和健壮聚合器在面对标签污染时的表现。", "result": "发现加权平均聚合器即使在网络拓扑不同的情况下也表现出稳定的性能，并且在某些特定条件下可以优于健壮聚合器。", "conclusion": "研究结果表明，在分布式学习中，网络拓扑对抵御标签污染攻击的鲁棒性有重要影响。"}}
{"id": "2601.02671", "pdf": "https://arxiv.org/pdf/2601.02671", "abs": "https://arxiv.org/abs/2601.02671", "authors": ["Ahmed Ahmed", "A. Feder Cooper", "Sanmi Koyejo", "Percy Liang"], "title": "Extracting books from production language models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "We ran experiments from mid-August to mid-September 2025, notified affected providers shortly after, and now make our findings public after a 90-day disclosure window", "summary": "Many unresolved legal questions over LLMs and copyright center on memorization: whether specific training data have been encoded in the model's weights during training, and whether those memorized data can be extracted in the model's outputs. While many believe that LLMs do not memorize much of their training data, recent work shows that substantial amounts of copyrighted text can be extracted from open-weight models. However, it remains an open question if similar extraction is feasible for production LLMs, given the safety measures these systems implement. We investigate this question using a two-phase procedure: (1) an initial probe to test for extraction feasibility, which sometimes uses a Best-of-N (BoN) jailbreak, followed by (2) iterative continuation prompts to attempt to extract the book. We evaluate our procedure on four production LLMs -- Claude 3.7 Sonnet, GPT-4.1, Gemini 2.5 Pro, and Grok 3 -- and we measure extraction success with a score computed from a block-based approximation of longest common substring (nv-recall). With different per-LLM experimental configurations, we were able to extract varying amounts of text. For the Phase 1 probe, it was unnecessary to jailbreak Gemini 2.5 Pro and Grok 3 to extract text (e.g, nv-recall of 76.8% and 70.3%, respectively, for Harry Potter and the Sorcerer's Stone), while it was necessary for Claude 3.7 Sonnet and GPT-4.1. In some cases, jailbroken Claude 3.7 Sonnet outputs entire books near-verbatim (e.g., nv-recall=95.8%). GPT-4.1 requires significantly more BoN attempts (e.g., 20X), and eventually refuses to continue (e.g., nv-recall=4.0%). Taken together, our work highlights that, even with model- and system-level safeguards, extraction of (in-copyright) training data remains a risk for production LLMs.", "AI": {"tldr": "本文探讨了从生产语言模型中提取训练数据的可能性。", "motivation": "许多关于LLM和版权的未决法律问题集中在记忆化上：即特定的训练数据是否在模型权重中编码，以及这些被记住的数据是否可以通过模型输出获取。尽管许多人认为LLMs不会记住大量训练数据，但近期的研究表明可以从开放权重模型中提取大量的受版权保护文本。", "method": "研究采用两阶段方法进行探究：首先是一个初步探查以测试提取可行性，有时使用最佳N（BoN）破解手段；然后是迭代延续提示尝试提取书籍。本文评估了四种生产LLM——Claude 3.7 Sonnet、GPT-4.1、Gemini 2.5 Pro和Grok 3，并通过基于块的最长公共子串近似计算nv-recall分数来衡量提取成功率。", "result": "研究发现，在初步探查阶段，Gemini 2.5 Pro和Grok 3无需破解就能成功提取文本（例如，哈利波特与魔法石的nv-recall分别为76.8%和70.3%），而Claude 3.7 Sonnet和GPT-4.1则需要破解。在某些情况下，被破解后的Claude 3.7 Sonnet可以近乎逐字地输出整本书（例如，nv-recall=95.8%）。相比之下，GPT-4.1需要更多的BoN尝试，并最终拒绝继续生成（例如，nv-recall=4.0%）。", "conclusion": "研究结果表明，即使存在模型和系统级的安全措施，提取训练数据仍对生产LLM构成风险。"}}
{"id": "2601.02666", "pdf": "https://arxiv.org/pdf/2601.02666", "abs": "https://arxiv.org/abs/2601.02666", "authors": ["Hadi Partovi Aria", "Zhe Xu"], "title": "Inferring Causal Graph Temporal Logic Formulas to Expedite Reinforcement Learning in Temporally Extended Tasks", "categories": ["cs.AI", "cs.LO"], "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "Decision-making tasks often unfold on graphs with spatial-temporal dynamics. Black-box reinforcement learning often overlooks how local changes spread through network structure, limiting sample efficiency and interpretability. We present GTL-CIRL, a closed-loop framework that simultaneously learns policies and mines Causal Graph Temporal Logic (Causal GTL) specifications. The method shapes rewards with robustness, collects counterexamples when effects fail, and uses Gaussian Process (GP) driven Bayesian optimization to refine parameterized cause templates. The GP models capture spatial and temporal correlations in the system dynamics, enabling efficient exploration of complex parameter spaces. Case studies in gene and power networks show faster learning and clearer, verifiable behavior compared to standard RL baselines.", "AI": {"tldr": "本文提出了GTL-CIRL框架，通过学习策略和挖掘因果图时态逻辑公式来提高在具有时空动态的决策任务中的强化学习效率。", "motivation": "传统的黑盒强化学习忽略了局部变化在网络结构中传播的方式，限制了样本效率和可解释性。因此，本文旨在开发一种新的方法以解决这些问题。", "method": "GTL-CIRL框架同时学习策略并挖掘因果图时态逻辑规范。该方法使用具有鲁棒性的奖励塑造、收集反例以及利用高斯过程驱动的贝叶斯优化来改进参数化的因果模板。", "result": "案例研究显示，与标准RL基准相比，本文的方法在基因网络和电力网络中实现了更快的学习速度，并且能够提供清晰可验证的行为。", "conclusion": "GTL-CIRL通过同时学习策略并挖掘因果图时态逻辑规范来提高强化学习的效率和可解释性。"}}
{"id": "2601.02663", "pdf": "https://arxiv.org/pdf/2601.02663", "abs": "https://arxiv.org/abs/2601.02663", "authors": ["Subha Ghoshal", "Ali Al-Bustami"], "title": "When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan-execute-replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\\% $\\rightarrow$ 67.5\\% for GPT-4o) while increasing latency by orders of magnitude ($\\sim$8s $\\rightarrow$ $\\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\\% at $\\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.", "AI": {"tldr": "本文研究了大型语言模型在推理时使用工具和计划对回答准确性的影响，并评估其成本与延迟。", "motivation": "现代大型语言模型依赖于推理时间的规划和外部工具来改善逻辑推理能力，但这些方法如何影响任务完成的质量、速度和经济性尚不清楚。因此，本文通过实际应用场景评估了这种方法的有效性和代价。", "method": "使用LangChain和LangGraph框架，在事件问答（Event-QA）和Reddit ChangeMyView论辩生成两个场景中进行了对比实验，比较了一次性提示基线与计划-执行-再规划的增强代理在不同工具辅助下的表现，并评估了模型大小、任务特定选择的成本意识。", "result": "对于Event-QA，最佳工具增强配置提高了准确性但大幅增加了延迟；而CMV场景中，一次性的引导策略效果最好，增加复杂性并未带来一致的好处。此外，在复杂的多工具协调下，较小的模型性能会下降。", "conclusion": "研究结果强调了任务特定、成本意识的选择对于模型规模和代理/工具复杂度的重要性。"}}
{"id": "2601.02649", "pdf": "https://arxiv.org/pdf/2601.02649", "abs": "https://arxiv.org/abs/2601.02649", "authors": ["Jiangyi Fang", "Bowen Zhou", "Haotian Wang", "Xin Zhu", "Leye Wang"], "title": "Effective Online 3D Bin Packing with Lookahead Parcels Using Monte Carlo Tree Search", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Online 3D Bin Packing (3D-BP) with robotic arms is crucial for reducing transportation and labor costs in modern logistics. While Deep Reinforcement Learning (DRL) has shown strong performance, it often fails to adapt to real-world short-term distribution shifts, which arise as different batches of goods arrive sequentially, causing performance drops. We argue that the short-term lookahead information available in modern logistics systems is key to mitigating this issue, especially during distribution shifts. We formulate online 3D-BP with lookahead parcels as a Model Predictive Control (MPC) problem and adapt the Monte Carlo Tree Search (MCTS) framework to solve it. Our framework employs a dynamic exploration prior that automatically balances a learned RL policy and a robust random policy based on the lookahead characteristics. Additionally, we design an auxiliary reward to penalize long-term spatial waste from individual placements. Extensive experiments on real-world datasets show that our method consistently outperforms state-of-the-art baselines, achieving over 10\\% gains under distributional shifts, 4\\% average improvement in online deployment, and up to more than 8\\% in the best case--demonstrating the effectiveness of our framework.", "AI": {"tldr": "论文提出了一种结合蒙特卡洛树搜索（MCTS）和短期预见信息的在线三维箱包装方法，以解决物流中动态分配问题。", "motivation": "深度强化学习在处理实时数据波动时表现不佳，特别是面对不同批次货物连续到达的情况。通过利用现代物流中的短期预测信息，可以改善这种状况。", "method": "论文将在线3D箱包装与预见包裹形式化为模型预测控制（MPC）问题，并采用蒙特卡洛树搜索框架解决。该方法使用动态探索优先级来平衡学习的强化学习策略和鲁棒随机策略。此外，还设计了一种辅助奖励机制以减少长期空间浪费。", "result": "实验结果表明，在现实世界的数据集上，所提方法在分布变化下表现优于现有技术10％以上，并且在线部署平均改进4%。", "conclusion": "论文提出的方法证明了其在处理物流中动态分配问题上的有效性和优越性。"}}
{"id": "2601.02648", "pdf": "https://arxiv.org/pdf/2601.02648", "abs": "https://arxiv.org/abs/2601.02648", "authors": ["Mehdi Fatemi"], "title": "Prioritized Replay for RL Post-training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce a problem-level prioritization framework for RL post-training of large language models. Building on insights from prioritized replay in deep RL, as well as prior observations that rollouts with intermediate success rates tend to produce stronger learning signals under methods such as GRPO, our approach selects problems according to a simple, model-driven priority score derived from empirical success statistics. In contrast to conventional curriculum strategies that emphasize easier tasks early in training, the resulting schedule naturally focuses training on problems that are neither consistently solved nor consistently failed, while deprioritizing those that contribute little gradient information. The method yields a continuously adapting and automatic prioritization process that requires no predefined difficulty tiers, auxiliary predictors, or external labels. We further introduce lightweight mechanisms for practical deployment, including heap-based prioritized sampling and periodic retesting of solved and unsolved problems to mitigate starvation and forgetting. Overall, the approach offers a principled and scalable alternative to manually designed curricula while aligning data selection directly with the dynamics of GRPO-based post-training.", "AI": {"tldr": "提出了一种基于优先级回放的强化学习后期训练框架。", "motivation": "通过借鉴深度强化学习中的优先级回放策略，并结合GRPO方法中观察到的现象，即中间成功率的序列产生更强的学习信号，此论文旨在创建一种更有效、自动化的训练过程来处理大型语言模型。", "method": "该方法利用模型驱动的优先级评分从经验成功统计数据中选择问题。同时引入了轻量级机制以支持实际部署，如堆基优先级采样和周期性重新测试解决与未解决问题以避免饥饿和遗忘。", "result": "结果显示这种方法提供了一种原则性和可扩展性的替代方案来手动设计课程，并直接根据GRPO基础后期训练的动力学进行数据选择。", "conclusion": "该论文提出的方法为大型语言模型的后期强化学习提供了有效且自动化的优先级分配机制，避免了传统的困难级别预定义和外部标签的需求。"}}
{"id": "2601.02646", "pdf": "https://arxiv.org/pdf/2601.02646", "abs": "https://arxiv.org/abs/2601.02646", "authors": ["Aniruddha Mahapatra", "Long Mai", "Cusuh Ham", "Feng Liu"], "title": "DreamLoop: Controllable Cinemagraph Generation from a Single Photograph", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://anime26398.github.io/dreamloop.github.io/", "summary": "Cinemagraphs, which combine static photographs with selective, looping motion, offer unique artistic appeal. Generating them from a single photograph in a controllable manner is particularly challenging. Existing image-animation techniques are restricted to simple, low-frequency motions and operate only in narrow domains with repetitive textures like water and smoke. In contrast, large-scale video diffusion models are not tailored for cinemagraph constraints and lack the specialized data required to generate seamless, controlled loops. We present DreamLoop, a controllable video synthesis framework dedicated to generating cinemagraphs from a single photo without requiring any cinemagraph training data. Our key idea is to adapt a general video diffusion model by training it on two objectives: temporal bridging and motion conditioning. This strategy enables flexible cinemagraph generation. During inference, by using the input image as both the first- and last- frame condition, we enforce a seamless loop. By conditioning on static tracks, we maintain a static background. Finally, by providing a user-specified motion path for a target object, our method provides intuitive control over the animation's trajectory and timing. To our knowledge, DreamLoop is the first method to enable cinemagraph generation for general scenes with flexible and intuitive controls. We demonstrate that our method produces high-quality, complex cinemagraphs that align with user intent, outperforming existing approaches.", "AI": {"tldr": "DreamLoop 是一种从单张照片生成可控动图的技术。", "motivation": "现有的图像动画技术仅限于简单、低频的运动，并且只能应用于具有重复纹理的小范围领域。大型视频扩散模型则不具备针对动图制作的需求和特定数据，无法生成无缝循环的效果。", "method": "DreamLoop 是一种专为从单张照片中生成可控动图设计的视频合成框架，通过适应通用视频扩散模型并进行时间过渡训练与运动条件调整来实现。该方法在推理过程中使用输入图像作为首尾帧以保证无缝循环，并利用静态轨道保持背景静止。", "result": "DreamLoop 方法可以生成高质量、复杂的动图，其效果符合用户意图并且超越了现有技术。", "conclusion": "DreamLoop 是首个能够为普通场景提供灵活且直观控制的动图生成方法。"}}
{"id": "2601.02645", "pdf": "https://arxiv.org/pdf/2601.02645", "abs": "https://arxiv.org/abs/2601.02645", "authors": ["Samarth Kalluraya", "Yiannis Kantaros"], "title": "Making Infeasible Tasks Feasible: Planning to Reconfigure Disconnected 3D Environments with Movable Objects", "categories": ["cs.RO"], "comment": null, "summary": "Several planners have been developed to compute dynamically feasible, collision-free robot paths from an initial to a goal configuration. A key assumption in these works is that the goal region is reachable; an assumption that often fails in practice when environments are disconnected. Motivated by this limitation, we consider known 3D environments comprising objects, also called blocks, that form distinct navigable support surfaces (planes), and that are either non-movable (e.g., tables) or movable (e.g., boxes). These surfaces may be mutually disconnected due to height differences, holes, or lateral separations. Our focus is on tasks where the robot must reach a goal region residing on an elevated plane that is unreachable. Rather than declaring such tasks infeasible, an effective strategy is to enable the robot to interact with the environment, rearranging movable objects to create new traversable connections; a problem known as Navigation Among Movable Objects (NAMO). Existing NAMO planners typically address 2D environments, where obstacles are pushed aside to clear a path. These methods cannot directly handle the considered 3D setting; in such cases, obstacles must be placed strategically to bridge these physical disconnections. We address this challenge by developing BRiDGE (Block-based Reconfiguration in Disconnected 3D Geometric Environments), a sampling-based planner that incrementally builds trees over robot and object configurations to compute feasible plans specifying which objects to move, where to place them, and in what order, while accounting for a limited number of movable objects. To accelerate planning, we introduce non-uniform sampling strategies. We show that our method is probabilistically complete and we provide extensive numerical and hardware experiments validating its effectiveness.", "AI": {"tldr": "本文开发了一种名为BRiDGE的采样算法，用于在包含可移动物体和非连通平面的三维环境中规划机器人路径。", "motivation": "许多现有的规划器假设目标区域是可达的，但在实际中由于高度差异、孔洞或横向分离导致环境不连通时，这个假设往往失效。本文旨在解决这种限制，使机器人能够通过移动物体重新配置环境以实现任务可行性。", "method": "提出了BRiDGE算法，它构建了基于机器人的状态和物体的位置的增量树来规划可行路径，并引入非均匀采样策略加速计算。", "result": "实验结果验证了该方法的有效性并展示了其在三维环境中解决导航问题的能力。", "conclusion": "本文提出的方法能够在有限数量可移动物体的情况下生成可行的机器人运动计划，解决了目标区域不可达的问题。"}}
{"id": "2601.02643", "pdf": "https://arxiv.org/pdf/2601.02643", "abs": "https://arxiv.org/abs/2601.02643", "authors": ["Mehmet Kurmaz"], "title": "AWARE-US: Benchmark for Preference-Aware Resolution in Tool-Calling Agents", "categories": ["cs.AI"], "comment": "19 pages, 2 figures, 6 tables", "summary": "Tool-calling conversational agents querying structured databases often face two linked failures: underspecification (missing constraints needed to run a precise query) and infeasibility (the fully specified query returns an empty set because no item satisfies all constraints). Existing work often responds with \"no results\" or relaxes constraints using ad hoc rules, which can violate user intent by discarding requirements the user cares about most. We frame infeasibility handling as a preference-aware query repair problem: when a query is unsatisfiable, the agent should relax the least important constraints to the user. We propose three LLM-based methods for inferring relative constraint importance from dialogue: (1) local weighting, (2) global one-shot weighting, and (3) pairwise ranking. Experiments show local weighting achieves the best preference alignment, while global weighting performs best on correct constraint relaxation. We also introduce AWARE-US, a benchmark of persona-grounded queries requiring agents to disambiguate requests via conversation and resolve infeasibility in a way consistent with persona-implied preferences.", "AI": {"tldr": "该论文提出了一种针对工具调用代理在处理数据库查询时的不可行性问题的方法，通过对话推理相对约束的重要性来修复查询。", "motivation": "现有的方法在解决查询不满足条件的问题时容易违背用户的意图。作者希望通过偏好感知的方式来放松最不重要的约束，以更好地符合用户的需求。", "method": "论文提出了三种基于LLM的方法：局部加权、全局一次性加权和成对排名来推断相对约束的重要性。", "result": "实验表明局部加权方法在偏好一致性上表现最好，而全局加权方法则在正确的约束放松方面表现最佳。", "conclusion": "论文引入了AWARE-US基准测试集，并通过对话解决了需求模糊性和不可行性问题。"}}
{"id": "2601.02641", "pdf": "https://arxiv.org/pdf/2601.02641", "abs": "https://arxiv.org/abs/2601.02641", "authors": ["Jeiyoon Park", "Daehwan Lee", "Changmin Yeo", "Yongshin Han", "Minseop Kim"], "title": "An Empirical Study of On-Device Translation for Real-Time Live-Stream Chat on Mobile Devices", "categories": ["cs.AI"], "comment": "preprint", "summary": "Despite its efficiency, there has been little research on the practical aspects required for real-world deployment of on-device AI models, such as the device's CPU utilization and thermal conditions. In this paper, through extensive experiments, we investigate two key issues that must be addressed to deploy on-device models in real-world services: (i) the selection of on-device models and the resource consumption of each model, and (ii) the capability and potential of on-device models for domain adaptation. To this end, we focus on a task of translating live-stream chat messages and manually construct LiveChatBench, a benchmark consisting of 1,000 Korean-English parallel sentence pairs. Experiments on five mobile devices demonstrate that, although serving a large and heterogeneous user base requires careful consideration of highly constrained deployment settings and model selection, the proposed approach nevertheless achieves performance comparable to commercial models such as GPT-5.1 on the well-targeted task. We expect that our findings will provide meaningful insights to the on-device AI community.", "AI": {"tldr": "本文通过实验证明了在移动设备上实时翻译直播聊天消息的实际可行性。", "motivation": "研究如何选择适合的模型并在资源受限条件下进行部署，同时探讨领域适应性问题。", "method": "构建LiveChatBench基准测试集并对比五款移动设备上的不同模型性能。", "result": "所提方法在特定任务上表现出色，性能接近商用级翻译模型GPT-5.1。", "conclusion": "研究成果为移动端AI应用提供了有价值的见解。"}}
{"id": "2601.02636", "pdf": "https://arxiv.org/pdf/2601.02636", "abs": "https://arxiv.org/abs/2601.02636", "authors": ["Byungwoo Kang", "Maceo Richards", "Bernardo Sabatini"], "title": "Credit Assignment via Neural Manifold Noise Correlation", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Credit assignment--how changes in individual neurons and synapses affect a network's output--is central to learning in brains and machines. Noise correlation, which estimates gradients by correlating perturbations of activity with changes in output, provides a biologically plausible solution to credit assignment but scales poorly as accurately estimating the Jacobian requires that the number of perturbations scale with network size. Moreover, isotropic noise conflicts with neurobiological observations that neural activity lies on a low-dimensional manifold. To address these drawbacks, we propose neural manifold noise correlation (NMNC), which performs credit assignment using perturbations restricted to the neural manifold. We show theoretically and empirically that the Jacobian row space aligns with the neural manifold in trained networks, and that manifold dimensionality scales slowly with network size. NMNC substantially improves performance and sample efficiency over vanilla noise correlation in convolutional networks trained on CIFAR-10, ImageNet-scale models, and recurrent networks. NMNC also yields representations more similar to the primate visual system than vanilla noise correlation. These findings offer a mechanistic hypothesis for how biological circuits could support credit assignment, and suggest that biologically inspired constraints may enable, rather than limit, effective learning at scale.", "AI": {"tldr": "本文提出了一种新的信用分配方法——神经流形噪声相关性（NMNC），以解决现有噪声相关性在大规模网络中效率低下的问题，并且更符合生物现实。", "motivation": "现有的噪声相关性方法存在两个主要缺点：一是计算量随网络规模增加而显著增大；二是均匀分布的扰动与神经活动受限于低维流形的事实不符。这些问题限制了其应用于大型模型的有效性和可行性。", "method": "NMNC通过将扰动限定在神经流形上来解决信用分配问题，理论和实证研究表明，训练后的网络中雅可比行列式的行空间与该流形一致，并且流形维度随着网络规模的增加缓慢增长。", "result": "NMNC方法相较于传统噪声相关性，在卷积、图像大规模模型及循环神经网络上都表现出更好的性能和样本效率；同时生成的表示也更加接近于灵长类视觉系统。", "conclusion": "本文研究为生物学电路如何支持信用分配提供了一个机理假设，表明生物启发的约束可能使有效学习在大型规模下成为可能而非限制。"}}
{"id": "2601.02632", "pdf": "https://arxiv.org/pdf/2601.02632", "abs": "https://arxiv.org/abs/2601.02632", "authors": ["Alireza Ezaz", "Ghazal Khodabandeh", "Majid Babaei", "Naser Ezzati-Jivan"], "title": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to ICSE 2026. DOI 10.1145/3744916.3787832", "summary": "Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.", "AI": {"tldr": "TAAF框架结合时间索引、知识图谱和大型语言模型，将原始跟踪数据转化为可操作的见解。", "motivation": "当前工具依赖预定义分析且自定义洞察需要编写特定领域的脚本，耗时且易出错。TAAF旨在通过结合KGs和LLMs来提高复杂软件系统追踪信息的理解、调试及优化效率。", "method": "构建时间索引的知识图谱以捕捉实体间关系，并用大型语言模型解释子图回答自然语言问题。", "result": "实验显示，与三种不同LLM在多个时态设置下相比，TAAF提升了最多达31.2%的答案准确性，特别是在多跳因果推理任务中表现突出。", "conclusion": "TAAF提供了一种新的追踪分析方法，能够有效利用知识图谱和语言模型提高问题解答准确率，尽管仍存在限制但为下一代追踪工具奠定了基础。"}}
{"id": "2601.02627", "pdf": "https://arxiv.org/pdf/2601.02627", "abs": "https://arxiv.org/abs/2601.02627", "authors": ["Nelvin Tan", "Yaowen Zhang", "James Asikin Cheung", "Fusheng Liu", "Yu-Ching Shih", "Dong Yang"], "title": "Improved Evidence Extraction for Document Inconsistency Detection with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their impressive abilities that arise from large training datasets and large model sizes. However, research on LLM-based approaches to document inconsistency detection is relatively limited. There are two key aspects of document inconsistency detection: (i) classification of whether there exists any inconsistency, and (ii) providing evidence of the inconsistent sentences. We focus on the latter, and introduce new comprehensive evidence-extraction metrics and a redact-and-retry framework with constrained filtering that substantially improves LLM-based document inconsistency detection over direct prompting. We back our claims with promising experimental results.", "AI": {"tldr": "改进了基于大型语言模型的文档不一致检测中的证据提取方法。", "motivation": "尽管大语言模型在许多领域表现出色，但关于其用于文档不一致检测的研究相对有限。本研究旨在提升提供不一致证据的能力，以改善整体检测效果。", "method": "引入了新的全面证据抽取指标和限制过滤的重试框架，改进了直接提示法下的大型语言模型性能。", "result": "通过实验验证表明，提出的改进方法在文档不一致性检测中取得了显著提升。", "conclusion": "研究展示了利用红字-重试框架和约束过滤可以有效提高基于大语言模型的证据提取能力，从而提升了整体的文档不一致检测效果。"}}
{"id": "2601.02624", "pdf": "https://arxiv.org/pdf/2601.02624", "abs": "https://arxiv.org/abs/2601.02624", "authors": ["Md Ajoad Hasan", "Dipayan Saha", "Khan Thamid Hasan", "Nashmin Alam", "Azim Uddin", "Sujan Kumar Saha", "Mark Tehranipoor", "Farimah Farahmandi"], "title": "LAsset: An LLM-assisted Security Asset Identification Framework for System-on-Chip (SoC) Verification", "categories": ["cs.CR", "cs.AI"], "comment": "6 pages", "summary": "The growing complexity of modern system-on-chip (SoC) and IP designs is making security assurance difficult day by day. One of the fundamental steps in the pre-silicon security verification of a hardware design is the identification of security assets, as it substantially influences downstream security verification tasks, such as threat modeling, security property generation, and vulnerability detection. Traditionally, assets are determined manually by security experts, requiring significant time and expertise. To address this challenge, we present LAsset, a novel automated framework that leverages large language models (LLMs) to identify security assets from both hardware design specifications and register-transfer level (RTL) descriptions. The framework performs structural and semantic analysis to identify intra-module primary and secondary assets and derives inter-module relationships to systematically characterize security dependencies at the design level. Experimental results show that the proposed framework achieves high classification accuracy, reaching up to 90% recall rate in SoC design, and 93% recall rate in IP designs. This automation in asset identification significantly reduces manual overhead and supports a scalable path forward for secure hardware development.", "AI": {"tldr": "提出了一种利用大型语言模型自动识别系统级芯片（SoC）安全资产的框架LAsset。", "motivation": "现代SoC和IP设计复杂性增加，使得手动由安全专家进行的安全资产识别耗时且需要专业知识。因此，提出了一个自动化的方法来提高效率。", "method": "通过利用大型语言模型对硬件设计规范和寄存器传输级描述进行结构化和语义分析，自动识别SoC中的安全资产，并建立模块间的关系以系统地表征设计层面的安全依赖性。", "result": "实验结果显示，该框架在SoC设计中实现了高达90%的召回率，在IP设计中达到了93%的召回率。", "conclusion": "LAsset显著减少了手动工作量，支持了安全硬件开发的可扩展路径。"}}
{"id": "2601.02618", "pdf": "https://arxiv.org/pdf/2601.02618", "abs": "https://arxiv.org/abs/2601.02618", "authors": ["Aakash Sarkar", "Marc W. Howard"], "title": "Hierarchical temporal receptive windows and zero-shot timescale generalization in biologically constrained scale-invariant deep networks", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "comment": null, "summary": "Human cognition integrates information across nested timescales. While the cortex exhibits hierarchical Temporal Receptive Windows (TRWs), local circuits often display heterogeneous time constants. To reconcile this, we trained biologically constrained deep networks, based on scale-invariant hippocampal time cells, on a language classification task mimicking the hierarchical structure of language (e.g., 'letters' forming 'words'). First, using a feedforward model (SITHCon), we found that a hierarchy of TRWs emerged naturally across layers, despite the network having an identical spectrum of time constants within layers. We then distilled these inductive priors into a biologically plausible recurrent architecture, SITH-RNN. Training a sequence of architectures ranging from generic RNNs to this restricted subset showed that the scale-invariant SITH-RNN learned faster with orders-of-magnitude fewer parameters, and generalized zero-shot to out-of-distribution timescales. These results suggest the brain employs scale-invariant, sequential priors - coding \"what\" happened \"when\" - making recurrent networks with such priors particularly well-suited to describe human cognition.", "AI": {"tldr": "本文研究了基于生物约束的尺度不变深度网络在语言分类任务中的表现，探讨其时间感知能力和零样本泛化能力。", "motivation": "人类认知整合信息跨越嵌套的时间尺度。大脑皮层表现出层次化的时空窗（TRWs），而局部电路通常展示出异质性的时间常数。为了统一这些差异，本文研究了在生物约束下训练的深度网络如何处理和理解时间信息。", "method": "作者首先使用前馈模型SITHCon进行实验，发现层次化的时间感知窗口自然地跨层出现。然后将这些先验知识提炼到一个生物学上可解释的递归架构SITH-RNN中，并通过一系列从通用RNN到这个受限子集的架构训练展示了其优势。", "result": "结果表明，基于尺度不变性的时间细胞模型能够更快学习并具有零样本泛化能力，在未知时间尺度下也能表现出色。", "conclusion": "研究表明大脑可能使用了尺度不变性和序列先验知识来编码“什么”和“何时”的信息，这使得带有此类先验的递归网络特别适合描述人类认知。"}}
{"id": "2601.02609", "pdf": "https://arxiv.org/pdf/2601.02609", "abs": "https://arxiv.org/abs/2601.02609", "authors": ["Arjun S. Nair"], "title": "Chronicals: A High-Performance Framework for LLM Fine-Tuning with 3.51x Speedup over Unsloth", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "stat.ML"], "comment": "61 pages, 25 figures, open-source framework available at https://github.com/Ajwebdevs/Chronicals and pip install chronicals", "summary": "Large language model fine-tuning is bottlenecked by memory: a 7B parameter model requires 84GB--14GB for weights, 14GB for gradients, and 56GB for FP32 optimizer states--exceeding even A100-40GB capacity. We present Chronicals, an open-source training framework achieving 3.51x speedup over Unsloth through four synergistic optimizations: (1) fused Triton kernels eliminating 75% of memory traffic via RMSNorm (7x), SwiGLU (5x), and QK-RoPE (2.3x) fusion; (2) Cut Cross-Entropy reducing logit memory from 5GB to 135MB through online softmax computation; (3) LoRA+ with theoretically-derived 16x differential learning rates between adapter matrices; and (4) Best-Fit Decreasing sequence packing recovering 60-75% of compute wasted on padding. On Qwen2.5-0.5B with A100-40GB, Chronicals achieves 41,184 tokens/second for full fine-tuning versus Unsloth's 11,736 tokens/second (3.51x). For LoRA at rank 32, we reach 11,699 tokens/second versus Unsloth MAX's 2,857 tokens/second (4.10x). Critically, we discovered that Unsloth's reported 46,000 tokens/second benchmark exhibited zero gradient norms--the model was not training. We provide complete mathematical foundations: online softmax correctness proofs, FlashAttention IO complexity bounds O(N^2 d^2 M^{-1}), LoRA+ learning rate derivations from gradient magnitude analysis, and bin-packing approximation guarantees. All implementations, benchmarks, and proofs are available at https://github.com/Ajwebdevs/Chronicals with pip installation via https://pypi.org/project/chronicals/.", "AI": {"tldr": "Chronicals 是一种高性能的大型语言模型微调框架，通过四个协同优化实现了比 Unsloth 快 3.51 倍的速度提升。", "motivation": "大语言模型微调受限于内存瓶颈，一个70亿参数的模型需要84GB内存（权重占14GB、梯度占14GB、FP32优化器状态占56GB），超过了A100-40GB显卡的能力。因此开发了Chronicals框架。", "method": "通过四个协同优化实现加速：(1) 融合Triton内核以减少75%的内存传输；(2) 剪裁交叉熵降低logit内存使用量；(3) 使用LoRA+技术并结合理论推导出不同适配器矩阵之间的学习率差异；(4) 最佳递减序列打包恢复由于填充而浪费掉60-75%的计算。", "result": "在A100-40GB显卡上，Chronicals对于Qwen2.5-0.5B模型实现了每秒41,184个token的速度（相比Unsloth快3.51倍）。而对于LoRA rank为32时，达到每秒11,699个token速度（比Unsloth MAX快4.10倍）。", "conclusion": "Chronicals提供了完整的数学基础证明，包括在线softmax正确性证明、FlashAttention IO复杂度界限以及LoRA+学习率推导等。所有实现、基准测试和证明均在GitHub上公开。"}}
{"id": "2601.02598", "pdf": "https://arxiv.org/pdf/2601.02598", "abs": "https://arxiv.org/abs/2601.02598", "authors": ["Yiyang Li", "Zheyuan Zhang", "Tianyi Ma", "Zehong Wang", "Keerthiram Murugesan", "Chuxu Zhang", "Yanfang Ye"], "title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis", "categories": ["cs.DL", "cs.AI"], "comment": null, "summary": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.", "AI": {"tldr": "介绍LongDA，一个用于评估基于LLM的代理在处理长文档数据分析任务时性能的数据分析基准。", "motivation": "现有的数据评估基准通常假设明确指定的模式和输入，但现实世界中涉及大量复杂文档的情况更加常见。因此，需要一个新的标准来评估这些场景下的数据代理。", "method": "从17个公开的美国国家级调查中提取了505个真实分析查询，并创建了一个允许访问、检索文件和执行代码的数据代理框架LongTA进行系统性评价。", "result": "实验显示即使是最先进的模型也存在显著性能差距，这表明在实际高风险分析场景中应用数据代理前需谨慎考虑的挑战。", "conclusion": "LongDA揭示了当前LLM代理在处理长文档数据分析任务中的不足，并强调了未来研究方向和实际应用时需要注意的问题。"}}
{"id": "2601.02594", "pdf": "https://arxiv.org/pdf/2601.02594", "abs": "https://arxiv.org/abs/2601.02594", "authors": ["Jyothi Rikhab Chand", "Mathews Jacob"], "title": "Annealed Langevin Posterior Sampling (ALPS): A Rapid Algorithm for Image Restoration with Multiscale Energy Models", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Solving inverse problems in imaging requires models that support efficient inference, uncertainty quantification, and principled probabilistic reasoning. Energy-Based Models (EBMs), with their interpretable energy landscapes and compositional structure, are well-suited for this task but have historically suffered from high computational costs and training instability. To overcome the historical shortcomings of EBMs, we introduce a fast distillation strategy to transfer the strengths of pre-trained diffusion models into multi-scale EBMs. These distilled EBMs enable efficient sampling and preserve the interpretability and compositionality inherent to potential-based frameworks. Leveraging EBM compositionality, we propose Annealed Langevin Posterior Sampling (ALPS) algorithm for Maximum-A-Posteriori (MAP), Minimum Mean Square Error (MMSE), and uncertainty estimates for inverse problems in imaging. Unlike diffusion models that use complex guidance strategies for latent variables, we perform annealing on static posterior distributions that are well-defined and composable. Experiments on image inpainting and MRI reconstruction demonstrate that our method matches or surpasses diffusion-based baselines in both accuracy and efficiency, while also supporting MAP recovery. Overall, our framework offers a scalable and principled solution for inverse problems in imaging, with potential for practical deployment in scientific and clinical settings. ALPS code is available at the GitHub repository \\href{https://github.com/JyoChand/ALPS}{ALPS}.", "AI": {"tldr": "提出了一种快速算法ALPS，用于基于能量模型的图像恢复任务。", "motivation": "为了解决传统能基模型在计算成本和训练稳定性方面的问题，并提供高效推理、不确定性量化及合理的概率性推断方法。", "method": "采用蒸馏策略将预训练扩散模型的优点转移到多尺度能基模型中，提出ALPS算法进行最大后验估计（MAP）、最小均方误差估计以及逆问题的不确定性评估。", "result": "实验表明该方法在图像插值和MRI重建任务上与基于扩散的方法相比，在准确性和效率方面表现相当或更优，并支持MAP恢复。", "conclusion": "ALPS框架为成像领域的逆问题提供了一个可扩展且合理的解决方案，具有实际部署于科研及临床环境的潜力。"}}
{"id": "2601.02591", "pdf": "https://arxiv.org/pdf/2601.02591", "abs": "https://arxiv.org/abs/2601.02591", "authors": ["Daeun Hwang", "Xuyuan Cai", "Edward F. Melcer", "Elin Carstensdottir"], "title": "A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games", "categories": ["cs.SD", "cs.IR"], "comment": "3 pages, 1 figure. D. Hwang, X. Cai, E. Melcer, and E. Carstensdottir, A Music Information Retrieval Approach to Classify Sub-Genres in Role Playing Games, in Extended Abstracts for the Late-Breaking Demo Session of the 25th Int. Society for Music Information Retrieval Conf., San Francisco, United States, 2024", "summary": "Video game music (VGM) is often studied under the same lens as film music, which largely focuses on its theoretical functionality with relation to the identified genres of the media. However, till date, we are unaware of any systematic approach that analyzes the quantifiable musical features in VGM across several identified game genres. Therefore, we extracted musical features from VGM in games from three sub-genres of Role-Playing Games (RPG), and then hypothesized how different musical features are correlated to the perceptions and portrayals of each genre. This observed correlation may be used to further suggest such features are relevant to the expected storytelling elements or play mechanics associated with the sub-genre.", "AI": {"tldr": "视频游戏音乐（VGM）的分类研究，通过分析不同子类型角色扮演游戏中的可量化音乐特征来区分不同的次流派。", "motivation": "目前尚未有系统性方法分析视频游戏中所识别的各种游戏类型的量化音乐特性。本研究旨在填补这一空白，并探讨音乐特性如何与每个次流派的故事讲述元素或玩法机制相关联。", "method": "从三个角色扮演游戏的子类型中提取了可量化的音乐特征，然后假设这些不同的音乐特征是如何关联到各个次流派中的感知和描绘的。", "result": "观察到了不同音乐特性与次流派之间的某些关联，并据此推测出这些特征对于相关次流派预期的故事讲述元素或玩法机制的重要性。", "conclusion": "通过分析视频游戏音乐（VGM）中可量化的音乐特征，能够更好地理解并区分不同类型角色扮演游戏中的音乐风格及其对故事叙述和游戏体验的影响。"}}
{"id": "2601.02589", "pdf": "https://arxiv.org/pdf/2601.02589", "abs": "https://arxiv.org/abs/2601.02589", "authors": ["Kris W Pan", "Yongmin Yoo"], "title": "FlowPlan-G2P: A Structured Generation Framework for Transforming Scientific Papers into Patent Descriptions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Over 3.5 million patents are filed annually, with drafting patent descriptions requiring deep technical and legal expertise. Transforming scientific papers into patent descriptions is particularly challenging due to their differing rhetorical styles and stringent legal requirements. Unlike black-box text-to-text approaches that struggle to model structural reasoning and legal constraints, we propose FlowPlan-G2P, a novel framework that mirrors the cognitive workflow of expert drafters by reformulating this task into three stages: (1) Concept Graph Induction, extracting technical entities and relationships into a directed graph via expert-like reasoning; (2) Paragraph and Section Planning, reorganizing the graph into coherent clusters aligned with canonical patent sections; and (3) Graph-Conditioned Generation, producing legally compliant paragraphs using section-specific subgraphs and tailored prompts. Experiments demonstrate that FlowPlan-G2P significantly improves logical coherence and legal compliance over end-to-end LLM baselines. Our framework establishes a new paradigm for paper-to-patent generation and advances structured text generation for specialized domains.", "AI": {"tldr": "本文提出了一种将科学论文转换为专利描述的框架FlowPlan-G2P。", "motivation": "每年有超过350万份专利申请，将其转化为具有法律合规性的专利描述面临挑战。现有的文本到文本方法无法有效处理结构性推理和法律规定。", "method": "提出了一个三阶段的方法：概念图生成、段落与章节规划以及基于图的有条件生成，来模拟专家撰写者的认知流程。", "result": "实验表明FlowPlan-G2P在逻辑一致性和法律合规性方面优于端到端LLM基线。", "conclusion": "该框架为论文转化为专利提供了新的范例，并推进了结构化文本生成领域的研究。"}}
{"id": "2601.02586", "pdf": "https://arxiv.org/pdf/2601.02586", "abs": "https://arxiv.org/abs/2601.02586", "authors": ["Daeun Hwang", "Hyeonbin Hwang"], "title": "Understanding Human Perception of Music Plagiarism Through a Computational Approach", "categories": ["cs.SD", "cs.IR"], "comment": "3 pages, D. Hwang and H. Hwang, Understanding Human Perception of Music Plagiarism Through a Computational Approach, in Extended Abstracts for the Late-Breaking Demo Session of the 25th Int. Society for Music Information Retrieval Conf., San Francisco, United States, 2024", "summary": "There is a wide variety of music similarity detection algorithms, while discussions about music plagiarism in the real world are often based on audience perceptions. Therefore, we aim to conduct a study to examine the key criteria of human perception of music plagiarism, focusing on the three commonly used musical features in similarity analysis: melody, rhythm, and chord progression. After identifying the key features and levels of variation humans use in perceiving musical similarity, we propose a LLM-as-a-judge framework that applies a systematic, step-by-step approach, drawing on modules that extract such high-level attributes.", "AI": {"tldr": "通过计算方法研究人类对音乐抄袭的感知", "motivation": "探讨听众在现实世界中关于音乐抄袭的看法，基于三种常用相似性分析特征：旋律、节奏和和弦进程进行研究", "method": "识别人类感知音乐相似性的关键特征及其变异程度，并提出一个LLM作为评判框架的方法", "result": "尚未提及具体结果", "conclusion": "旨在通过系统化步骤揭示人类感知音乐抄袭的关键要素"}}
{"id": "2601.02580", "pdf": "https://arxiv.org/pdf/2601.02580", "abs": "https://arxiv.org/abs/2601.02580", "authors": ["Christopher Ormerod"], "title": "Reconstructing Item Characteristic Curves using Fine-Tuned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 5 tables, 3 figures", "summary": "Traditional methods for determining assessment item parameters, such as difficulty and discrimination, rely heavily on expensive field testing to collect student performance data for Item Response Theory (IRT) calibration. This study introduces a novel approach that implicitly models these psychometric properties by fine-tuning Large Language Models (LLMs) to simulate student responses across a spectrum of latent abilities. Leveraging the Qwen-3 dense model series and Low-Rank Adaptation (LoRA), we train models to generate responses to multiple choice questions conditioned on discrete ability descriptors. We reconstruct the probability of a correct response as a function of student ability, effectively generating synthetic Item Characteristic Curves (ICCs) to estimate IRT parameters. Evaluation on a dataset of Grade 6 English Language Arts (ELA) items and the BEA 2024 Shared Task dataset demonstrates that this method competes with or outperforms baseline approaches. This simulation-based technique seems particularly effective at modeling item discrimination.", "AI": {"tldr": "利用微调的大型语言模型来模拟学生响应，以重构项目特性曲线（ICC）并估算IRT参数。", "motivation": "传统方法依赖昂贵的实地测试数据来确定评估项目的参数。该研究提出了一种新的方法，通过微调大型语言模型来隐式建模心理测量属性，并生成合成的ICC以估计IRT参数。", "method": "使用Qwen-3密集型模型系列和低秩适应（LoRA）训练模型，使其根据不同的能力描述符生成多项选择题的回答。通过这种方式模拟学生响应并重构正确答案的概率函数。", "result": "在Grade 6英语语言艺术项目和BEA 2024共享任务数据集上的评估表明，该方法与基线方法相比具有竞争力甚至更优。", "conclusion": "基于仿真的技术特别有效于建模项目的区分度。"}}
{"id": "2601.02577", "pdf": "https://arxiv.org/pdf/2601.02577", "abs": "https://arxiv.org/abs/2601.02577", "authors": ["Alexander Roman", "Jacob Roman"], "title": "Orchestral AI: A Framework for Agent Orchestration", "categories": ["cs.AI", "astro-ph.IM", "hep-ph"], "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com", "summary": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.", "AI": {"tldr": "本论文提出了Orchestral框架，旨在解决多LLM提供者之间工具调用集成的挑战，提供一种轻量级、统一且类型安全的方法来构建跨提供商的LLM代理。", "motivation": "由于LLM代理框架迅速发展，开发者面临着选择困境：要么接受特定供应商锁定的SDK，要么使用复杂的多包生态系统，这会隐藏控制流并阻碍可重现性。整合多个提供者的工具调用存在核心工程挑战，包括分散的API、不兼容的消息格式以及不同的流式传输和工具调用行为。", "method": "Orchestral是一个轻量级Python框架，它定义了一种单一的通用表示方式来处理消息、工具及LLM使用情况，并在不同提供者间无缝操作。通过自动从Python类型提示生成工具模式，消除了手动格式转换的需求并维持了跨提供商边界的类型安全性。", "result": "该框架支持同步执行模型和流式传输，保证了确定性行为、简单调试以及实时交互，而无需引入服务器依赖。Orchestral的模块化架构清晰地分离出提供者整合、工具执行、对话编排及用户界面，实现可扩展性而不造成架构缠绕。", "conclusion": "通过这些特性，Orchestral为构建跨多个LLM提供商的可靠代理系统提供了强大的支持。"}}
{"id": "2601.02574", "pdf": "https://arxiv.org/pdf/2601.02574", "abs": "https://arxiv.org/abs/2601.02574", "authors": ["Haoran Wang", "Maryam Khalid", "Qiong Wu", "Jian Gao", "Cheng Cao"], "title": "Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.", "AI": {"tldr": "提出了一种基于概率确定性和一致性的框架（PCC），用于改进大型语言模型的事实核查。", "motivation": "现有事实核查方法依赖外部证据，忽略了内部知识，并可能引入无关信息。缺乏针对性解决不确定性的机制。因此需要一种新的框架来提高准确性和效率。", "method": "引入了基于概率确定性和一致性的PCC框架，估计事实的信心并通过自适应验证策略，在直接回答、触发特定检索和深度搜索之间切换。", "result": "实验表明PCC在不确定性量化方面优于口头表达的信心，并且在各种基准测试中始终优于强大的LLM事实核查基线。此外，还展示了其跨不同LLM的良好泛化能力。", "conclusion": "PCC框架改进了大型语言模型的事实核查准确性与效率，并具有广泛的适用性和良好的泛化性能。"}}
{"id": "2601.02573", "pdf": "https://arxiv.org/pdf/2601.02573", "abs": "https://arxiv.org/abs/2601.02573", "authors": ["Kiarash Shamsi", "Danijel Novokmet", "Joshua Peters", "Mao Lin Liu", "Paul K Edwards", "Vahab Khoshdel"], "title": "LendNova: Towards Automated Credit Risk Assessment with Language Models", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": "ef:AAAI 2026, Workshop on Agentic AI in Financial Services", "summary": "Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.", "AI": {"tldr": "LendNova是一种利用先进NLP技术自动化的信贷风险评估系统，直接在原始信用记录文本上操作以进行风险建模。", "motivation": "传统的基于特征的模型成本高且不能充分利用原始信用记录中的所有信息。LendNova旨在通过语言模型自动化信贷风险评估流程，降低运营成本并提高可扩展性。", "method": "LendNova利用先进的NLP技术操作原始、专业术语密集的信用局文本，自动捕捉嵌入其中的风险信号和模式，取代了手动预处理步骤。", "result": "在真实世界数据上的评估证明了其准确且高效的信贷风险评估能力。它为智能信贷风险管理设定了基线，并展示了语言模型在此领域的可行性。", "conclusion": "LendNova为更准确、灵活和自动化的金融决策奠定了基础，推动了未来的研究方向。"}}
{"id": "2601.02566", "pdf": "https://arxiv.org/pdf/2601.02566", "abs": "https://arxiv.org/abs/2601.02566", "authors": ["Junbin Zhang", "Hamid Reza Tohidypour", "Yixiao Wang", "Panos Nasiopoulos"], "title": "Shallow- and Deep-fake Image Manipulation Localization Using Vision Mamba and Guided Graph Neural Network", "categories": ["cs.CV"], "comment": "Under review for journal publication", "summary": "Image manipulation localization is a critical research task, given that forged images may have a significant societal impact of various aspects. Such image manipulations can be produced using traditional image editing tools (known as \"shallowfakes\") or advanced artificial intelligence techniques (\"deepfakes\"). While numerous studies have focused on image manipulation localization on either shallowfake images or deepfake videos, few approaches address both cases. In this paper, we explore the feasibility of using a deep learning network to localize manipulations in both shallow- and deep-fake images, and proposed a solution for such purpose. To precisely differentiate between authentic and manipulated pixels, we leverage the Vision Mamba network to extract feature maps that clearly describe the boundaries between tampered and untouched regions. To further enhance this separation, we propose a novel Guided Graph Neural Network (G-GNN) module that amplifies the distinction between manipulated and authentic pixels. Our evaluation results show that our proposed method achieved higher inference accuracy compared to other state-of-the-art methods.", "AI": {"tldr": "本文探讨了使用深度学习网络定位浅假和深伪图像篡改的可行性，并提出了一种解决方案。", "motivation": "鉴于伪造图片可能对社会产生重大影响，研究者们致力于解决图像篡改定位任务。当前大多数研究集中于浅假或深伪视频上的图像篡改定位，而本文则试图同时处理这两种情况。", "method": "本文利用Vision Mamba网络提取描述篡改和未篡改区域边界的特征图，并提出了一种新的引导式图形神经网络（G-GNN）模块以增强两者之间的区别。", "result": "实验结果表明，所提出的方案在推理准确率方面优于现有的先进方法。", "conclusion": "本文成功探索了同时定位浅假和深伪图像篡改的深度学习技术，并展示了其优越性。"}}
{"id": "2601.02564", "pdf": "https://arxiv.org/pdf/2601.02564", "abs": "https://arxiv.org/abs/2601.02564", "authors": ["Nedim Muzoglu"], "title": "Comparative Analysis of Binarization Methods For Medical Image Hashing On Odir Dataset", "categories": ["eess.IV", "cs.CV", "cs.IR"], "comment": "17th International İstanbul Scientific Research Congress", "summary": "In this study, we evaluated four binarization methods. Locality-Sensitive Hashing (LSH), Iterative Quantization (ITQ), Kernel-based Supervised Hashing (KSH), and Supervised Discrete Hashing (SDH) on the ODIR dataset using deep feature embeddings. Experimental results show that SDH achieved the best performance, with an mAP@100 of 0.9184 using only 32-bit codes, outperforming LSH, ITQ, and KSH. Compared with prior studies, our method proved highly competitive: Fang et al. reported 0.7528 (Fundus-iSee, 48 bits) and 0.8856 (ASOCT-Cataract, 48 bits), while Wijesinghe et al. achieved 94.01 (KVASIR, 256 bits). Despite using significantly fewer bits, our SDH-based framework reached retrieval accuracy close to the state-of-the-art. These findings demonstrate that SDH is the most effective approach among those tested, offering a practical balance of accuracy, storage, and efficiency for medical image retrieval and device inventory management.", "AI": {"tldr": "本论文对比了四种二值化方法在ODIR数据集上的性能，使用深度特征嵌入进行医学图像哈希。", "motivation": "研究目的是评估不同的二值化方法对医学图像检索和设备库存管理的适用性，并找到一种准确、高效且存储空间需求低的方法。", "method": "论文中测试了四种二值化方法：局部敏感哈希（LSH）、迭代量化（ITQ）、基于核监督哈希（KSH）以及监督离散哈希（SDH）。这些方法使用深度特征嵌入在ODIR数据集上进行了实验。", "result": "结果表明，监督离散哈希（SDH）表现最佳，在32位代码的情况下mAP@100达到了0.9184，超过了LSH、ITQ和KSH。与之前的研究所报告的结果相比，该方法表现出更高的竞争力。", "conclusion": "研究表明，尽管使用了较少的比特数，基于监督离散哈希（SDH）的方法在医学图像检索中依然能达到接近最新技术水平的精度，同时提供了存储空间和效率上的优势。"}}
{"id": "2601.02562", "pdf": "https://arxiv.org/pdf/2601.02562", "abs": "https://arxiv.org/abs/2601.02562", "authors": ["Rohit Kaushik", "Eva Kaushik"], "title": "CutisAI: Deep Learning Framework for Automated Dermatology and Cancer Screening", "categories": ["cs.LG", "eess.IV"], "comment": "10 pages, 3 figures", "summary": "The rapid growth of dermatological imaging and mobile diagnostic tools calls for systems that not only demonstrate empirical performance but also provide strong theoretical guarantees. Deep learning models have shown high predictive accuracy; however, they are often criticized for lacking well, calibrated uncertainty estimates without which these models are hardly deployable in a clinical setting. To this end, we present the Conformal Bayesian Dermatological Classifier (CBDC), a well, founded framework that combines Statistical Learning Theory, Topological Data Analysis (TDA), and Bayesian Conformal Inference. CBDC offers distribution, dependent generalization bounds that reflect dermatological variability, proves a topological stability theorem that guarantees the invariance of convolutional neural network embeddings under photometric and morphological perturbations and provides finite conformal coverage guarantees for trustworthy uncertainty quantification. Through exhaustive experiments on the HAM10000, PH2, and ISIC 2020 datasets, we show that CBDC not only attains classification accuracy but also generates calibrated predictions that are interpretable from a clinical perspective. This research constitutes a theoretical and practical leap for deep dermatological diagnostics, thereby opening the machine learning theory clinical applicability interface.", "AI": {"tldr": "CutisAI是一种结合深度学习、统计学理论和拓扑数据分析的自动皮肤病筛查框架。", "motivation": "为了提高临床环境中皮肤疾病的诊断准确性并提供可靠的风险评估，研究人员开发了具有校准不确定性估计的新模型。", "method": "利用统计学习理论、拓扑数据分析（TDA）以及贝叶斯一致性推理提出了CBDC框架，该方法提供了反映皮肤病学变异性的一般化边界保证，并通过实验验证其有效性。", "result": "在HAM10000、PH2和ISIC 2020数据集上的全面试验表明，CBDC不仅实现了分类准确性还生成了临床可解释的校准预测。", "conclusion": "该研究为深度皮肤病诊断提供了理论与实践层面的重要进展，从而开启了机器学习理论在临床应用中的接口。"}}
{"id": "2601.02560", "pdf": "https://arxiv.org/pdf/2601.02560", "abs": "https://arxiv.org/abs/2601.02560", "authors": ["Emre Sariyildiz"], "title": "AMC26: High-performance DOb for robust position control", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "This paper presents a new HPDOb that significantly improves disturbance estimation accuracy and robustness in motion control systems, surpassing the capabilities of conventional DObs. The proposed observer is analysed and synthesised in the discrete-time domain, providing a realistic representation of their dynamic behaviour and enabling enhanced controller design for practical applications. The core contribution of the HPDOb is a novel synthesis method that incorporates higher-order truncation error dynamics into disturbance estimation. Unlike conventional DObs, which are limited to zero-order truncation error, the HPDOb achieves first-order truncation error, yielding markedly improved estimation accuracy and robustness against disturbances in motion control systems. Simulation and experiments verify the stability and performance of HPDOb.", "AI": {"tldr": "提出了一种高性能DOb，提升了运动控制系统中的扰动估计精度和鲁棒性。", "motivation": "为了提高运动控制系统的扰动估计精度和稳定性，开发一种超越传统DObs的新型观察器。", "method": "通过在离散时间域内合成并分析HPDOb，引入更高阶截断误差动态以改进扰动估计方法。", "result": "仿真及实验验证了HPDOb具有更好的稳定性和性能表现。", "conclusion": "HPDOb通过改进的估计算法实现了更高的精度和鲁棒性，在实际应用中表现出优越性。"}}
{"id": "2601.02559", "pdf": "https://arxiv.org/pdf/2601.02559", "abs": "https://arxiv.org/abs/2601.02559", "authors": ["Lauren Olson", "Emitzá Guzmán", "Florian Kunneman"], "title": "PerspectiveCoach: Exploring LLMs for Developer Reflection", "categories": ["cs.SE", "cs.HC"], "comment": "48th International Conference of Software Engineering", "summary": "Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.", "AI": {"tldr": "本文介绍了PerspectiveCoach，一个利用大型语言模型引导开发者进行结构化视角转换练习的工具。", "motivation": "尽管对软件开发中的伦理挑战的认识日益提高，但从业人员仍然缺乏帮助他们批判性地参与边缘用户生活经验的结构化工具。该论文旨在解决这一问题，并探讨如何通过使用LLM来支持开发人员对技术决策的社会影响进行更深入的自我反省。", "method": "研究者进行了一个有18名前端开发者参加的控制实验，这些开发者与PerspectiveCoach互动并处理了一个真实的在线性别骚扰案例。参与者在多次尝试中对其重述的内容进行了文本相似性分析，并且通过了一个人类对人类的研究来进一步验证其发现。", "result": "结果显示，使用该工具后开发者的自我意识增强，视角更加宽广，并能够更细致地表达伦理观念。此外，参与者对该工具的易用性和相关性给予了高度评价。", "conclusion": "这项工作为利用LLM支持端用户观点取向的设计提供了探索性的思路，有助于促进开发者进行批判性的、道德上的自我反省，并为进一步发展更加包容和具有社会责任感的技术提供了实证见解。"}}
{"id": "2601.02554", "pdf": "https://arxiv.org/pdf/2601.02554", "abs": "https://arxiv.org/abs/2601.02554", "authors": ["Morgan R. Frank", "Alireza Javadian Sabet", "Lisa Simon", "Sarah H. Bana", "Renzhe Yu"], "title": "AI-exposed jobs deteriorated before ChatGPT", "categories": ["econ.GN", "cs.AI", "cs.CY"], "comment": null, "summary": "Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, months before ChatGPT. Analyzing millions of LinkedIn profiles, we show that graduate cohorts from 2021 onward entered AI-exposed jobs at lower rates than earlier cohorts, with gaps opening before late 2022. Finally, from millions of university syllabi, we find that graduates taking more AI-exposed curricula had higher first-job pay and shorter job searches after ChatGPT. Together, these results point to forces pre-dating generative AI and to the ongoing value of LLM-relevant education.", "AI": {"tldr": "文章研究了AI暴露职业的就业前景恶化是否与ChatGPT发布有关，通过分析失业保险记录、LinkedIn资料和大学课程等数据来探讨这一现象。", "motivation": "公共辩论将2022年底ChatGPT发布后AI暴露职业的工作前景恶化联系在一起。然而，作者想探究这种恶化是否真的始于ChatGPT的发布之前，并评估相关教育的价值。", "method": "使用美国每月失业保险记录来测量特定于行业的失业风险；分析数百万LinkedIn个人资料以显示2021及以后毕业进入AI暴露工作的学生比例下降趋势；从大学课程大纲中提取信息，研究毕业生在ChatGPT发布后的第一份工作工资和求职时间。", "result": "作者发现，在ChatGPT发布之前几个月，AI暴露职业的失业风险就已经开始上升。同时，2021及之后毕业进入这些岗位的学生比例下降；然而，学习更多与LLM相关的课程的学生在首次就业时薪水更高且求职时间更短。", "conclusion": "研究结果表明，在ChatGPT发布之前已经存在影响AI暴露职业的某些因素，并强调了相关教育的价值。"}}
{"id": "2601.02553", "pdf": "https://arxiv.org/pdf/2601.02553", "abs": "https://arxiv.org/abs/2601.02553", "authors": ["Jiaqi Liu", "Yaofeng Su", "Peng Xia", "Siwei Han", "Zeyu Zheng", "Cihang Xie", "Mingyu Ding", "Huaxiu Yao"], "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "AI": {"tldr": "本文提出了SimpleMem，一种基于语义无损压缩的高效内存框架，用于支持大规模语言模型代理在复杂环境中的长期交互。", "motivation": "现有的记忆系统要么通过被动上下文扩展保留完整的历史记录，导致冗余；要么依赖迭代推理来过滤噪声，增加代币成本。为了应对这些挑战，我们设计了一种新的解决方案SimpleMem。", "method": "SimpleMem包括三个阶段：语义结构化压缩、递归内存整合和自适应查询感知检索。通过这三个步骤实现信息密度最大化并优化令牌使用效率。", "result": "实验结果表明，在准确性、检索效率和推理成本方面，我们的方法优于基线方法，平均F1值提高了26.4%，同时将推理时间的代币消耗减少了多达30倍。", "conclusion": "SimpleMem展示了一种在性能与效率之间取得平衡的方法。"}}
{"id": "2601.02543", "pdf": "https://arxiv.org/pdf/2601.02543", "abs": "https://arxiv.org/abs/2601.02543", "authors": ["Linfeng Ye", "Zhixiang Chi", "Konstantinos N. Plataniotis", "En-hui Yang"], "title": "Normalized Conditional Mutual Information Surrogate Loss for Deep Neural Classifiers", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT"], "comment": "8 pages, 4 figures", "summary": "In this paper, we propose a novel information theoretic surrogate loss; normalized conditional mutual information (NCMI); as a drop in alternative to the de facto cross-entropy (CE) for training deep neural network (DNN) based classifiers. We first observe that the model's NCMI is inversely proportional to its accuracy. Building on this insight, we introduce an alternating algorithm to efficiently minimize the NCMI. Across image recognition and whole-slide imaging (WSI) subtyping benchmarks, NCMI-trained models surpass state of the art losses by substantial margins at a computational cost comparable to that of CE. Notably, on ImageNet, NCMI yields a 2.77% top-1 accuracy improvement with ResNet-50 comparing to the CE; on CAMELYON-17, replacing CE with NCMI improves the macro-F1 by 8.6% over the strongest baseline. Gains are consistent across various architectures and batch sizes, suggesting that NCMI is a practical and competitive alternative to CE.", "AI": {"tldr": "提出了一种新的信息理论替代损失函数NCMI，作为深度神经网络分类器训练中的交叉熵的替代方案。", "motivation": "观察到模型的条件互信息（NCMI）与其准确性成反比，从而引入了NCMI作为更有效的训练工具以提高模型精度。", "method": "通过推导和优化基于NCMI的新损失函数，并开发出一种交替算法来有效最小化该函数。", "result": "在图像识别和WSI子类型基准测试中，使用NCMI训练的模型相比现有最佳损失函数提高了显著的准确性。例如，在ImageNet数据集上与ResNet-50结合时，NCMI使top-1准确度提高了2.77%；而在CAMELYON-17数据集中，宏F1分数提升了8.6%。", "conclusion": "证明了NCMI是一个实用且有竞争力的损失函数，可以作为交叉熵的有效替代方案。"}}
{"id": "2601.02538", "pdf": "https://arxiv.org/pdf/2601.02538", "abs": "https://arxiv.org/abs/2601.02538", "authors": ["Sam Narimani", "Solveig Roth Hoff", "Kathinka Dæhli Kurz", "Kjell-Inge Gjesdal", "Jürgen Geisler", "Endre Grøvik"], "title": "A Green Solution for Breast Region Segmentation Using Deep Active Learning", "categories": ["physics.med-ph", "cs.CV", "eess.IV"], "comment": null, "summary": "Purpose: Annotation of medical breast images is an essential step toward better diagnostic but a time consuming task. This research aims to focus on different selecting sample strategies within deep active learning on Breast Region Segmentation (BRS) to lessen computational cost of training and effective use of resources. Methods: The Stavanger breast MRI dataset containing 59 patients was used in this study, with FCN-ResNet50 adopted as a sustainable deep learning (DL) model. A novel sample selection approach based on Breast Anatomy Geometry (BAG) analysis was introduced to group data with similar informative features for DL. Patient positioning and Breast Size were considered the key selection criteria in this process. Four selection strategies including Random Selection, Nearest Point, Breast Size, and a hybrid of all three strategies were evaluated using an active learning framework. Four training data proportions of 10%, 20%, 30%, and 40% were used for model training, with the remaining data reserved for testing. Model performance was assessed using Dice score, Intersection over Union, precision, and recall, along with 5-fold cross-validation to enhance generalizability. Results: Increasing the training data proportion from 10% to 40% improved segmentation performance for nearly all strategies, except for Random Selection. The Nearest Point strategy consistently achieved the lowest carbon footprint at 30% and 40% data proportions. Overall, combining the Nearest Point strategy with 30% of the training data provided the best balance between segmentation performance, efficiency, and environmental sustainability. Keywords: Deep Active Learning, Breast Region Segmentation, Human-center analysis", "AI": {"tldr": "使用深度主动学习方法减少乳腺图像标注的时间和计算成本，提高资源利用效率。", "motivation": "通过不同样本选择策略降低训练的计算成本，并有效利用资源，以减轻医疗图像注释的工作负担。", "method": "采用FCN-ResNet50模型并结合基于乳房解剖几何分析的新颖样本选择方法。四种样本选择策略包括随机选择、最近点选择、乳房大小和三者的混合策略，在不同训练数据比例下进行实验评估。", "result": "增加训练数据的比例从10%到40%，大多数策略的分割性能有所提高，除了随机选择策略外；最近点策略在30%和40%的数据比例下表现出最低的碳足迹。结合最近点策略与30%的训练数据提供了最佳平衡。", "conclusion": "结合最近点策略使用30%的训练数据不仅提高了分割性能，还提升了效率和环境可持续性。"}}
{"id": "2601.02536", "pdf": "https://arxiv.org/pdf/2601.02536", "abs": "https://arxiv.org/abs/2601.02536", "authors": ["Shaden Shaar", "Bradon Thymes", "Sirawut Chaixanien", "Claire Cardie", "Bharath Hariharan"], "title": "MovieRecapsQA: A Multimodal Open-Ended Video Question-Answering Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Understanding real-world videos such as movies requires integrating visual and dialogue cues to answer complex questions. Yet existing VideoQA benchmarks struggle to capture this multimodal reasoning and are largely not open-ended, given the difficulty of evaluating free-form answers. In this paper, we introduce a novel open-ended multi-modal VideoQA benchmark, MovieRecapsQA created using movie recap videos--a distinctive type of YouTube content that summarizes a film by presenting its key events through synchronized visual (recap video) and textual (recap summary) modalities. Using the recap summary, we generate $\\approx 8.2$ K question-answer (QA) pairs (aligned with movie-subtitles) and provide the necessary \"facts\" needed to verify an answer in a reference-free manner. To our knowledge, this is the first open-ended VideoQA benchmark that supplies explicit textual context of the input (video and/or text); which we use for evaluation. Our benchmark provides videos of multiple lengths (i.e., recap-segments, movie-segments) and categorizations of questions (by modality and type) to enable fine-grained analysis. We evaluate the performance of seven state-of-the-art MLLMs using our benchmark and observe that: 1) visual-only questions remain the most challenging; 2) models default to textual inputs whenever available; 3) extracting factually accurate information from video content is still difficult for all models; and 4) proprietary and open-source models perform comparably on video-dependent questions.", "AI": {"tldr": "电影摘要问答（MovieRecapsQA）是一个新的多模态开放式视频问答基准，使用电影回顾视频生成约8200对问题-答案", "motivation": "现有的视频问答基准难以捕捉到跨视觉和对话线索的复杂推理能力，并且大多是封闭式的，因此该论文提出了一个全新的开放性多模式视频问答数据集。", "method": "基于YouTube上的电影回顾视频创建了一个包含约8200个问题-答案对的数据集。每个问答对都与电影字幕相匹配，并提供用于验证答案的必要“事实”。此外还提供了不同长度的视频片段和问题分类，以进行细粒度分析。", "result": "通过使用MovieRecapsQA评估七个最先进的多模态语言模型，结果表明视觉单独的问题最具有挑战性；当文本输入可用时，模型倾向于依赖其；从视频内容中提取事实准确信息对所有模型来说仍是一项困难的任务；专有和开源模型在与视频相关的问答任务上表现相当。", "conclusion": "提出了一种新的开放型多模态视频问答基准MovieRecapsQA。该研究为评估多模态语言理解能力提供了有效的工具，并揭示了当前模型存在的问题，为未来的研究指明了方向。"}}
{"id": "2601.02535", "pdf": "https://arxiv.org/pdf/2601.02535", "abs": "https://arxiv.org/abs/2601.02535", "authors": ["Hyeong Kyu Choi", "Sharon Li"], "title": "ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX-Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks -- including text summarization, code generation, and mathematical reasoning -- our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.", "AI": {"tldr": "提出了一种无需外部评估器的Best-of-N选择框架ModeX，用于从多个随机生成中选出高质量输出。", "motivation": "现有的最佳输出选择方法依赖于外部评估器或奖励模型，限制了其适用性和效率。在开放式任务中，需要一种通用且高效的方法来识别高质量的答案。", "method": "构建相似性图并应用谱聚类来选取代表性的中心点，以选出主导语义共识的模式化输出。", "result": "在文本摘要、代码生成和数学推理等开放性任务上，ModeX及其改进版ModeX-Lite优于标准单路径或多路径基线方法。", "conclusion": "提出的方法提供了一种计算效率高且鲁棒性强的选择高质量输出的解决方案。"}}
{"id": "2601.02532", "pdf": "https://arxiv.org/pdf/2601.02532", "abs": "https://arxiv.org/abs/2601.02532", "authors": ["Manuel Lafond", "Francis Sarrazin"], "title": "A $O^*((2 + ε)^k)$ Time Algorithm for Cograph Deletion Using Unavoidable Subgraphs in Large Prime Graphs", "categories": ["cs.DS"], "comment": null, "summary": "We study the parameterized complexity of the Cograph Deletion problem, which asks whether one can delete at most $k$ edges from a graph to make it $P_4$-free. This is a well-known graph modification problem with applications in computation biology and social network analysis. All current parameterized algorithms use a similar strategy, which is to find a $P_4$ and explore the local structure around it to perform an efficient recursive branching. The best known algorithm achieves running time $O^*(2.303^k)$ and requires an automated search of the branching cases due to their complexity. Since it appears difficult to further improve the current strategy, we devise a new approach using modular decompositions. We solve each module and the quotient graph independently, with the latter being the core problem. This reduces the problem to solving on a prime graph, in which all modules are trivial. We then use a characterization of Chudnovsky et al. stating that any large enough prime graph has one of seven structures as an induced subgraph. These all have many $P_4$s, with the quantity growing linearly with the graph size, and we show that these allow a recursive branch tree algorithm to achieve running time $O^*((2 + ε)^k)$ for any $ε> 0$. This appears to be the first algorithmic application of the prime graph characterization and it could be applicable to other modification problems. Towards this goal, we provide the exact set of graph classes $\\H$ for which the $\\H$-free editing problem can make use of our reduction to a prime graph, opening the door to improvements for other modification problems.", "AI": {"tldr": "设计了一种新的算法来解决图的Cograph 删除问题，该算法的时间复杂度为O^*((2 + ε)^k)，其中ε>0。", "motivation": "当前最优算法在参数化复杂性方面难以进一步优化，因此采用模块分解的新方法来简化并加速算法过程。", "method": "通过使用Chudnovsky等人的结构特征减少问题到解决主要图上的任务，并展示这些结构允许递归分支树算法实现更好的时间复杂度O^*((2 + ε)^k)。这种方法为其他修改问题的改进提供了可能。", "result": "提出了一个新的算法，该算法的时间复杂度比以前的方法（即$O^*(2.303^k)$）要好，并且是首次将主要图结构应用于算法中的应用。", "conclusion": "这项工作提供了一种新颖的方法来解决Cograph 删除问题，并可能适用于其他修改问题。它证明了使用模块分解和特征结构可以改进参数化复杂性算法。"}}
{"id": "2601.02531", "pdf": "https://arxiv.org/pdf/2601.02531", "abs": "https://arxiv.org/abs/2601.02531", "authors": ["Mattia Ottoborgo", "Daniele Rege Cambrin", "Paolo Garza"], "title": "Losses that Cook: Topological Optimal Transport for Structured Recipe Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cooking recipes are complex procedures that require not only a fluent and factual text, but also accurate timing, temperature, and procedural coherence, as well as the correct composition of ingredients. Standard training procedures are primarily based on cross-entropy and focus solely on fluency. Building on RECIPE-NLG, we investigate the use of several composite objectives and present a new topological loss that represents ingredient lists as point clouds in embedding space, minimizing the divergence between predicted and gold ingredients. Using both standard NLG metrics and recipe-specific metrics, we find that our loss significantly improves ingredient- and action-level metrics. Meanwhile, the Dice loss excels in time/temperature precision, and the mixed loss yields competitive trade-offs with synergistic gains in quantity and time. A human preference analysis supports our finding, showing our model is preferred in 62% of the cases.", "AI": {"tldr": "本文提出了一个基于拓扑最优传输的损失函数，用于改善食谱生成过程中的准确性与连贯性。", "motivation": "当前的标准训练方法主要依靠交叉熵来保证文本流畅度，而忽略了烹饪过程中时间、温度和步骤的一致性。因此，作者提出了一种新的复合目标以提升这些方面的性能。", "method": "通过将原料列表表示为嵌入空间中的点云并最小化预测与真实数据之间的散度，引入了一个新的拓扑损失函数。同时结合了Dice损失和其他标准NLG指标来优化模型。", "result": "实验结果显示该方法显著提高了食材级别和动作级别的准确性，并且在时间和温度精度方面表现优异。", "conclusion": "综合评估表明新提出的混合损失能够提供更好的性能平衡，其生成的食谱更受人类偏爱。"}}
{"id": "2601.02521", "pdf": "https://arxiv.org/pdf/2601.02521", "abs": "https://arxiv.org/abs/2601.02521", "authors": ["Amirreza Parvahan", "Mohammad Hoseyni", "Javad Khoramdel", "Amirhossein Nikoofard"], "title": "CT Scans As Video: Efficient Intracranial Hemorrhage Detection Using Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Automated analysis of volumetric medical imaging on edge devices is severely constrained by the high memory and computational demands of 3D Convolutional Neural Networks (CNNs). This paper develops a lightweight computer vision framework that reconciles the efficiency of 2D detection with the necessity of 3D context by reformulating volumetric Computer Tomography (CT) data as sequential video streams. This video-viewpoint paradigm is applied to the time-sensitive task of Intracranial Hemorrhage (ICH) detection using the Hemorica dataset. To ensure operational efficiency, we benchmarked multiple generations of the YOLO architecture (v8, v10, v11 and v12) in their Nano configurations, selecting the version with the highest mAP@50 to serve as the slice-level backbone. A ByteTrack algorithm is then introduced to enforce anatomical consistency across the $z$-axis. To address the initialization lag inherent in video trackers, a hybrid inference strategy and a spatiotemporal consistency filter are proposed to distinguish true pathology from transient prediction noise. Experimental results on independent test data demonstrate that the proposed framework serves as a rigorous temporal validator, increasing detection Precision from 0.703 to 0.779 compared to the baseline 2D detector, while maintaining high sensitivity. By approximating 3D contextual reasoning at a fraction of the computational cost, this method provides a scalable solution for real-time patient prioritization in resource-constrained environments, such as mobile stroke units and IoT-enabled remote clinics.", "AI": {"tldr": "使用视频流形式处理CT扫描数据，通过多目标跟踪提高颅内出血检测效率。", "motivation": "减少三维卷积神经网络在边缘设备上的内存和计算需求，实现高效自动化的医疗影像分析。", "method": "将CT扫描数据视为连续的视频帧，采用轻量级YOLO架构及ByteTrack算法进行多目标跟踪，提高检测准确性与效率。", "result": "相较于基线二维检测器，所提框架将检测精度从0.703提升到0.779，并保持高灵敏度。", "conclusion": "这种方法在资源受限环境中提供了可扩展的实时患者优先级处理方案。"}}
{"id": "2601.02514", "pdf": "https://arxiv.org/pdf/2601.02514", "abs": "https://arxiv.org/abs/2601.02514", "authors": ["Ahmad Terra", "Mohit Ahmed", "Rafia Inam", "Elena Fersman", "Martin Törngren"], "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy", "categories": ["cs.AI"], "comment": null, "summary": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.", "AI": {"tldr": "本文提出了一种生成可解释强化学习策略文本说明的新框架。", "motivation": "确保自主代理的行为符合人类预期是至关重要的，而理解其行为的一种方法就是使用可解释的强化学习技术。尽管文本说明易于被人类理解，但它们的质量和准确性仍存在问题。", "method": "该论文提出了一种新框架来生成、转换为规则、提高质量并评估这些文本说明。框架利用大型语言模型和聚类技术识别频繁条件，并转化为规则进行性能测试。此外还开发了自动谓词发生器以确定状态的语义信息。", "result": "在三个开源环境及一个电信用例中，实验表明该框架能有效提高策略解释的质量并减少冲突信息，同时实现了可接受的任务性能。", "conclusion": "此研究解决了现有方法的一些局限性，并提出了一种新的、系统化的评估文本说明的方法。"}}
{"id": "2601.02505", "pdf": "https://arxiv.org/pdf/2601.02505", "abs": "https://arxiv.org/abs/2601.02505", "authors": ["Jiazhen Liu", "Glen Neville", "Jinwoo Park", "Sonia Chernova", "Harish Ravichandar"], "title": "Learning and Optimizing the Efficacy of Spatio-Temporal Task Allocation under Temporal and Resource Constraints", "categories": ["cs.RO"], "comment": "The journal extension version of our conference paper: arXiv:2404.07902, which has been accepted by ISRR 2024", "summary": "Complex multi-robot missions often require heterogeneous teams to jointly optimize task allocation, scheduling, and path planning to improve team performance under strict constraints. We formalize these complexities into a new class of problems, dubbed Spatio-Temporal Efficacy-optimized Allocation for Multi-robot systems (STEAM). STEAM builds upon trait-based frameworks that model robots using their capabilities (e.g., payload and speed), but goes beyond the typical binary success-failure model by explicitly modeling the efficacy of allocations as trait-efficacy maps. These maps encode how the aggregated capabilities assigned to a task determine performance. Further, STEAM accommodates spatio-temporal constraints, including a user-specified time budget (i.e., maximum makespan). To solve STEAM problems, we contribute a novel algorithm named Efficacy-optimized Incremental Task Allocation Graph Search (E-ITAGS) that simultaneously optimizes task performance and respects time budgets by interleaving task allocation, scheduling, and path planning. Motivated by the fact that trait-efficacy maps are difficult, if not impossible, to specify, E-ITAGS efficiently learns them using a realizability-aware active learning module. Our approach is realizability-aware since it explicitly accounts for the fact that not all combinations of traits are realizable by the robots available during learning. Further, we derive experimentally-validated bounds on E-ITAGS' suboptimality with respect to efficacy. Detailed numerical simulations and experiments using an emergency response domain demonstrate that E-ITAGS generates allocations of higher efficacy compared to baselines, while respecting resource and spatio-temporal constraints. We also show that our active learning approach is sample efficient and establishes a principled tradeoff between data and computational efficiency.", "AI": {"tldr": "研究一种优化时空任务分配的新方法，以提升多机器人系统的性能。", "motivation": "复杂多机器人任务需要在严格的资源和时间限制下同时进行任务分配、调度和路径规划，现有的二元成功失败模型无法有效描述这种情形下的效率优化问题。", "method": "提出一种新的时空效能优化任务分配算法E-ITAGS，该算法结合了增量任务分配图搜索策略，并通过实现感知型主动学习模块来高效学习难以明确的特质效能映射。", "result": "实验结果表明，相较于基准方法，E-ITAGS能够在满足资源和时空间约束的同时，生成更高效的分配方案。此外，还展示了该算法在样本效率方面的优越性以及数据与计算效率之间的权衡原则。", "conclusion": "该研究提出了一种新的时空效能优化任务分配框架STEAM及其对应的解决方案E-ITAGS，并通过详细的实验验证了其有效性和高效性。"}}
{"id": "2601.02504", "pdf": "https://arxiv.org/pdf/2601.02504", "abs": "https://arxiv.org/abs/2601.02504", "authors": ["Elizaveta Artser", "Daniil Karol", "Anna Potriasaeva", "Aleksei Rostovskii", "Katsiaryna Dzialets", "Ekaterina Koshchenko", "Xiaotian Su", "April Yi Wang", "Anastasiia Birillo"], "title": "Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "Accepted at ICSE SEET 2026, 6 pages, 2 figures", "summary": "Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.", "AI": {"tldr": "介绍了一种集成在IDE中的AI辅助调试工具，旨在提高编程教育和软件开发中的调试效率。", "motivation": "当前计算机科学课程中往往忽视了对调试技能的培养，而该工具通过实时分析代码、建议断点及提供上下文提示来提升这一重要能力。", "method": "采用RAG与LLM技术、程序切片以及定制化启发式算法，以减少LLM调用次数并提高准确性，同时进行了三层次评估：技术分析、用户体验研究和课堂测试。", "result": "该工具在多个层面展示了其促进调试技能教学的潜力。", "conclusion": "此AI辅助调试工具具备改善编程教育中调试能力培养的巨大潜能。"}}
{"id": "2601.02500", "pdf": "https://arxiv.org/pdf/2601.02500", "abs": "https://arxiv.org/abs/2601.02500", "authors": ["Brian Tekmen", "Jason Yin", "Qianqian Tong"], "title": "GEM-Style Constraints for PEFT with Dual Gradient Projection in LoRA", "categories": ["cs.LG", "cs.AI"], "comment": "Work accepted to the NSF REU Symposium at the 2025 IEEE International Conference on Data Mining (ICDM). Correspondence to: betekmen@uncg.edu", "summary": "Full fine-tuning of Large Language Models (LLMs) is computationally costly, motivating Continual Learning (CL) approaches that utilize parameter-efficient adapters. We revisit Gradient Episodic Memory (GEM) within the Low-Rank Adapter (LoRA) subspace and introduce I-GEM: a fixed-budget, GPU-resident dual projected-gradient approximation to GEM's quadratic projection. By constraining non-interference solely within the adapter parameters, I-GEM preserves GEM-like stability with orders-of-magnitude lower mean projection overhead. On a 3-task AG News split with induced domain drift, using GPT-2 (355M) and LoRA ($r=8$), I-GEM matches GEM's average accuracy (within $\\sim\\!0.04$ pts) and outperforms A-GEM by $\\sim\\!1.4$ pts. Crucially, it reduces projection time vs.\\ GEM by a factor of $\\sim\\!10^3$. These results suggest that applying GEM constraints in the LoRA subspace is a practical pathway for continual learning at the LLM scale.", "AI": {"tldr": "该论文提出了I-GEM方法，通过在LoRA子空间中应用GEM约束来实现大规模语言模型的持续学习。", "motivation": "全量微调大型语言模型计算成本高昂，因此需要利用参数高效的适配器来进行持续学习。研究者重新审视了Gradient Episodic Memory（GEM）并在LoRA子空间内引入了一种新的方法I-GEM，以实现更低的投影开销和更稳定的性能。", "method": "该论文提出了一种基于低秩适应器（LoRA）的空间内的固定预算、GPU常驻的双梯度投影近似算法I-GEM。通过仅约束适配器参数中的非干扰部分，这种方法保持了GEM样式的稳定性，并大幅度减少了平均投影开销。", "result": "在包含三个任务的AG News数据集上，使用GPT-2（355M）和LoRA（r=8），I-GEM能够匹配GEM的平均精度（误差不超过0.04分），并比A-GEM高出大约1.4个百分点。此外，它将投影时间减少了大约三个数量级。", "conclusion": "这些结果表明，在LoRA子空间中应用GEM约束为大规模语言模型的持续学习提供了一条实用路径。"}}
{"id": "2601.02457", "pdf": "https://arxiv.org/pdf/2601.02457", "abs": "https://arxiv.org/abs/2601.02457", "authors": ["Souhail Hadgi", "Bingchen Gong", "Ramana Sundararaman", "Emery Pierson", "Lei Li", "Peter Wonka", "Maks Ovsjanikov"], "title": "PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding", "categories": ["cs.CV"], "comment": "Project website: https://souhail-hadgi.github.io/patchalign3dsite/", "summary": "Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/", "AI": {"tldr": "该论文提出了PatchAlign3D，一种通过点云直接生成与语言对齐的局部特征的三维模型，以实现零样本的3D零件分割。", "motivation": "现有的基础模型在全局任务中表现良好，但在局部部件级别的推理上转移性差。本文旨在解决基于多视角渲染和文本查询的方法中存在的问题，并提出一种直接从点云生成与语言对齐的局部特征的新方法。", "method": "该论文提出了PatchAlign3D，通过两阶段训练来实现：第一阶段是从视觉编码器中蒸馏出密集2D特征到3D补丁；第二阶段是通过多正样本对比目标将这些补丁嵌入与部件级别的文本嵌入对齐。", "result": "实验表明，该模型在无需任何测试时间的多视角渲染的情况下实现了零样本3D零件分割，并且在多个3D零件分割基准上显著优于基于渲染和前馈的方法。", "conclusion": "PatchAlign3D通过直接从点云生成与语言对齐的局部特征解决了现有方法的问题，展示了其在快速准确地执行3D零件分割任务上的优势。"}}
{"id": "2601.02456", "pdf": "https://arxiv.org/pdf/2601.02456", "abs": "https://arxiv.org/abs/2601.02456", "authors": ["Junhao Cai", "Zetao Cai", "Jiafei Cao", "Yilun Chen", "Zeyu He", "Lei Jiang", "Hang Li", "Hengjie Li", "Yang Li", "Yufei Liu", "Yanan Lu", "Qi Lv", "Haoxiang Ma", "Jiangmiao Pang", "Yu Qiao", "Zherui Qiu", "Yanqing Shen", "Xu Shi", "Yang Tian", "Bolun Wang", "Hanqing Wang", "Jiaheng Wang", "Tai Wang", "Xueyuan Wei", "Chao Wu", "et al. (17 additional authors not shown)"], "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation", "categories": ["cs.RO"], "comment": "Homepage: https://internrobotics.github.io/internvla-a1.github.io/", "summary": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.", "AI": {"tldr": "本文提出了InternVLA-A1模型，该模型结合了语义理解和动态预测能力，用于机器人操作。", "motivation": "现有的Vision-Language-Action（VLA）模型虽然在语义理解方面表现出色，但缺乏对物理世界动力学的推理能力。同时，基于视频预测的世界模型方法往往缺乏语义基础并且处理预测错误时较为脆弱。因此，本文旨在将语义理解与动态预测相结合。", "method": "InternVLA-A1采用了统一的Mixture-of-Transformers架构，并通过统一的掩码自注意力机制协调了场景理解和视觉预见生成、动作执行三个专家模块。该模型在2B和3B参数规模上进行了预训练，使用包含合成数据与真实数据混合的大型数据集InternData-A1和Agibot-World。", "result": "通过评估12种现实世界机器人任务以及模拟基准测试，InternVLA-A1的表现优于pi0和GR00T N1.5等领先模型，在日常任务中提高幅度为14.5%，在动态设置如传送带分类中的性能提升了40%至73.3%。", "conclusion": "本文展示了InternVLA-A1如何通过整合语义理解和动态预测能力，显著提高了机器人操作的性能。"}}
{"id": "2601.02455", "pdf": "https://arxiv.org/pdf/2601.02455", "abs": "https://arxiv.org/abs/2601.02455", "authors": ["Xinyu Wang", "Yajie Luo", "Yihong Wu", "Liheng Ma", "Ziyu Zhao", "Jingrui Tian", "Lei Ding", "Yufei Cui", "Xiao-Wen Chang"], "title": "Dynamic Quantization Error Propagation in Encoder-Decoder ASR Quantization", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "9 pages, 4 figures, 3 tables", "summary": "Running Automatic Speech Recognition (ASR) models on memory-constrained edge devices requires efficient compression. While layer-wise post-training quantization is effective, it suffers from error accumulation, especially in encoder-decoder architectures. Existing solutions like Quantization Error Propagation (QEP) are suboptimal for ASR due to the model's heterogeneity, processing acoustic features in the encoder while generating text in the decoder. To address this, we propose Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE), which adaptively controls the trade-off between cross-layer error correction and local quantization. Experiments show that FADE significantly improves stability by reducing performance variance across runs, while simultaneously surpassing baselines in mean WER.", "AI": {"tldr": "提出了一种新的动态量化误差传播方法FADE，用于改进ASR模型在边缘设备上的压缩效率。", "motivation": "传统的逐层后训练量化技术虽然有效，但在编码器-解码器架构中会积累误差。现有解决方案如QEP对于处理异构的ASR模型效果不佳，因此需要一种新的方法来解决这个问题。", "method": "提出Fine-grained Alpha for Dynamic Quantization Error Propagation (FADE)，该方法通过自适应控制跨层误差校正与局部量化之间的权衡，以提高压缩效率和稳定性。", "result": "实验表明，FADE能够显著减少运行间性能差异，并且在平均WER上超过了基线模型。", "conclusion": "FADE是一种有效的动态量化误差传播方法，适用于ASR模型的高效压缩，可以在边缘设备上实现更稳定的语音识别性能。"}}
{"id": "2601.02454", "pdf": "https://arxiv.org/pdf/2601.02454", "abs": "https://arxiv.org/abs/2601.02454", "authors": ["Saba Naqvi", "Mohammad Baqar", "Nawaz Ali Mohammad"], "title": "The Rise of Agentic Testing: Multi-Agent Systems for Robust Software Quality Assurance", "categories": ["cs.SE", "cs.AI"], "comment": "11 Pages", "summary": "Software testing has progressed toward intelligent automation, yet current AI-based test generators still suffer from static, single-shot outputs that frequently produce invalid, redundant, or non-executable tests due to the lack of execution aware feedback. This paper introduces an agentic multi-model testing framework a closed-loop, self-correcting system in which a Test Generation Agent, an Execution and Analysis Agent, and a Review and Optimization Agent collaboratively generate, execute, analyze, and refine tests until convergence. By using sandboxed execution, detailed failure reporting, and iterative regeneration or patching of failing tests, the framework autonomously improves test quality and expands coverage. Integrated into a CI/CD-compatible pipeline, it leverages reinforcement signals from coverage metrics and execution outcomes to guide refinement. Empirical evaluations on microservice based applications show up to a 60% reduction in invalid tests, 30% coverage improvement, and significantly reduced human effort compared to single-model baselines demonstrating that multi-agent, feedback-driven loops can evolve software testing into an autonomous, continuously learning quality assurance ecosystem for self-healing, high-reliability codebases.", "AI": {"tldr": "提出了一种基于多代理的软件测试框架，该框架通过闭环反馈机制自动生成、执行和优化测试用例。", "motivation": "当前AI生成的测试用例存在静态单一输出问题，导致产生无效或重复的测试。为此，提出了一个多代理系统来解决这些挑战。", "method": "设计了一个多模型测试框架，其中包括一个测试生成代理、一个执行与分析代理以及一个审查和优化代理，通过沙箱执行、详细故障报告和迭代再生或修复失败测试来提高测试质量和覆盖率。", "result": "在微服务应用上的实验证明该方法可减少60%无效测试数量，并提升30%的代码覆盖度。相比单一模型基线系统，减少了大量的人工工作量。", "conclusion": "多代理、反馈驱动的闭环可以将软件测试演化为一个自主学习的质量保证生态系统，适用于自我修复和高可靠性代码库。"}}
{"id": "2601.02451", "pdf": "https://arxiv.org/pdf/2601.02451", "abs": "https://arxiv.org/abs/2601.02451", "authors": ["Subhankar Mishra"], "title": "mHC-GNN: Manifold-Constrained Hyper-Connections for Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) suffer from over-smoothing in deep architectures and expressiveness bounded by the 1-Weisfeiler-Leman (1-WL) test. We adapt Manifold-Constrained Hyper-Connections (\\mhc)~\\citep{xie2025mhc}, recently proposed for Transformers, to graph neural networks. Our method, mHC-GNN, expands node representations across $n$ parallel streams and constrains stream-mixing matrices to the Birkhoff polytope via Sinkhorn-Knopp normalization. We prove that mHC-GNN exhibits exponentially slower over-smoothing (rate $(1-γ)^{L/n}$ vs.\\ $(1-γ)^L$) and can distinguish graphs beyond 1-WL. Experiments on 10 datasets with 4 GNN architectures show consistent improvements. Depth experiments from 2 to 128 layers reveal that standard GNNs collapse to near-random performance beyond 16 layers, while mHC-GNN maintains over 74\\% accuracy even at 128 layers, with improvements exceeding 50 percentage points at extreme depths. Ablations confirm that the manifold constraint is essential: removing it causes up to 82\\% performance degradation. Code is available at \\href{https://github.com/smlab-niser/mhc-gnn}{https://github.com/smlab-niser/mhc-gnn}", "AI": {"tldr": "提出了mHC-GNN，一种用于图神经网络的流形约束超连接方法，旨在解决深度GNN中的过度平滑问题并提高表达能力。", "motivation": "为了解决深度GNN架构中的过度平滑问题和1-WL测试限制下的低表达性，提出了一种新的方法来改进GNN性能。", "method": "通过引入流形约束超连接（mHC），将节点表示扩展到n个并行流中，并使用Sinkhorn-Knopp归一化约束流混合矩阵在Birkhoff多面体内。此方法证明了较慢的过度平滑率和超出1-WL测试的能力。", "result": "实验表明，与标准GNN相比，在深度达到128层时，mHC-GNN仍能保持超过74%的准确性，并且性能在极端深度下提高了50个百分点以上。", "conclusion": "提出的mHC-GNN方法有效缓解了过度平滑问题并提升了图神经网络的表达能力。"}}
{"id": "2601.02447", "pdf": "https://arxiv.org/pdf/2601.02447", "abs": "https://arxiv.org/abs/2601.02447", "authors": ["Bennet Kahrs", "Julia Andresen", "Fenja Falta", "Monty Santarossa", "Heinz Handels", "Timo Kepp"], "title": "Don't Mind the Gaps: Implicit Neural Representations for Resolution-Agnostic Retinal OCT Analysis", "categories": ["cs.CV"], "comment": "Extended journal version of the proceedings paper \"Bridging Gaps in Retinal Imaging: Fusing OCT and SLO Information with Implicit Neural Representations for Improved Interpolation and Segmentation\" from the German Conference on Medical Image Computing (BVM 2025; DOI:10.1007/978-3-658-47422-5_24). Under review for a MELBA Special Issue. Minor revision resubmitted; decision pending", "summary": "Routine clinical imaging of the retina using optical coherence tomography (OCT) is performed with large slice spacing, resulting in highly anisotropic images and a sparsely scanned retina. Most learning-based methods circumvent the problems arising from the anisotropy by using 2D approaches rather than performing volumetric analyses. These approaches inherently bear the risk of generating inconsistent results for neighboring B-scans. For example, 2D retinal layer segmentations can have irregular surfaces in 3D. Furthermore, the typically used convolutional neural networks are bound to the resolution of the training data, which prevents their usage for images acquired with a different imaging protocol. Implicit neural representations (INRs) have recently emerged as a tool to store voxelized data as a continuous representation. Using coordinates as input, INRs are resolution-agnostic, which allows them to be applied to anisotropic data. In this paper, we propose two frameworks that make use of this characteristic of INRs for dense 3D analyses of retinal OCT volumes. 1) We perform inter-B-scan interpolation by incorporating additional information from en-face modalities, that help retain relevant structures between B-scans. 2) We create a resolution-agnostic retinal atlas that enables general analysis without strict requirements for the data. Both methods leverage generalizable INRs, improving retinal shape representation through population-based training and allowing predictions for unseen cases. Our resolution-independent frameworks facilitate the analysis of OCT images with large B-scan distances, opening up possibilities for the volumetric evaluation of retinal structures and pathologies.", "AI": {"tldr": "本论文提出了两种框架，利用隐式神经表示（INR）进行视网膜OCT体积的密集三维分析。", "motivation": "现有的学习方法通过二维方法绕过了由各向异性带来的问题，这可能导致邻近B扫描生成不一致的结果。同时，卷积神经网络依赖于训练数据分辨率，无法用于不同成像协议获取的图像。INR作为一种工具可以将体素化数据作为连续表示存储，不受分辨率限制。", "method": "论文提出了两种框架：1）通过从面内模式中整合额外信息进行B扫描插值；2）创建一个分辨率无关的视网膜图谱以支持一般分析而不严格要求数据。两者都利用了可泛化的INR，通过基于人群的训练改进视网膜形状表示，并允许对未见过的情况做出预测。", "result": "论文的方法能够在不同B扫描间距下进行OCT图像的密集三维分析，为视网膜结构和病理学的体积评估开辟新的可能性。", "conclusion": "该研究通过引入隐式神经表示技术解决了常规临床成像中的分辨率问题，实现了视网膜OCT体积的大规模一致性和准确性分析。"}}
{"id": "2601.02445", "pdf": "https://arxiv.org/pdf/2601.02445", "abs": "https://arxiv.org/abs/2601.02445", "authors": ["Parashjyoti Borah", "Sanghamitra Sarkar", "Ranjan Phukan"], "title": "A Spatio-Temporal Deep Learning Approach For High-Resolution Gridded Monsoon Prediction", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 3 figures, 2 Tables, to be submitted to \"IEEE Transactions on Geoscience and Remote Sensing\"", "summary": "The Indian Summer Monsoon (ISM) is a critical climate phenomenon, fundamentally impacting the agriculture, economy, and water security of over a billion people. Traditional long-range forecasting, whether statistical or dynamical, has predominantly focused on predicting a single, spatially-averaged seasonal value, lacking the spatial detail essential for regional-level resource management. To address this gap, we introduce a novel deep learning framework that reframes gridded monsoon prediction as a spatio-temporal computer vision task. We treat multi-variable, pre-monsoon atmospheric and oceanic fields as a sequence of multi-channel images, effectively creating a video-like input tensor. Using 85 years of ERA5 reanalysis data for predictors and IMD rainfall data for targets, we employ a Convolutional Neural Network (CNN)-based architecture to learn the complex mapping from the five-month pre-monsoon period (January-May) to a high-resolution gridded rainfall pattern for the subsequent monsoon season. Our framework successfully produces distinct forecasts for each of the four monsoon months (June-September) as well as the total seasonal average, demonstrating its utility for both intra-seasonal and seasonal outlooks.", "AI": {"tldr": "本文提出了一种基于深度学习的框架，用于印度夏季季风的高分辨率网格预测。", "motivation": "传统长范围预报缺乏空间细节，难以满足区域资源管理的需求。为了弥补这一空白，作者引入了新的深度学习方法来解决这个问题。", "method": "将多变量预季风雨前的大气和海洋场视为一个多通道图像序列，并使用85年的ERA5再分析数据作为预测因子，IMD降水数据作为目标值，通过卷积神经网络架构学习从五个月的雨前时期到随后的季风季节高分辨率网格降雨模式之间的复杂映射。", "result": "该框架成功地为每个季风月份（6月至9月）以及整个季节平均值生成了不同的预测，展示了其在短期和长期展望中的实用性。", "conclusion": "所提出的深度学习方法能够提供更准确的高分辨率网格降水预测，有助于更好地进行资源管理和决策。"}}
{"id": "2601.02444", "pdf": "https://arxiv.org/pdf/2601.02444", "abs": "https://arxiv.org/abs/2601.02444", "authors": ["Maryam Abbasihafshejani", "AHM Nazmus Sakib", "Murtuza Jadliwala"], "title": "VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "comment": null, "summary": "The rapid advancement of speech synthesis technologies, including text-to-speech (TTS) and voice conversion (VC), has intensified security and privacy concerns related to voice cloning. Recent defenses attempt to prevent unauthorized cloning by embedding protective perturbations into speech to obscure speaker identity while maintaining intelligibility. However, adversaries can apply advanced purification techniques to remove these perturbations, recover authentic acoustic characteristics, and regenerate cloneable voices. Despite the growing realism of such attacks, the robustness of existing defenses under adaptive purification remains insufficiently studied. Most existing purification methods are designed to counter adversarial noise in automatic speech recognition (ASR) systems rather than speaker verification or voice cloning pipelines. As a result, they fail to suppress the fine-grained acoustic cues that define speaker identity and are often ineffective against speaker verification attacks (SVA). To address these limitations, we propose Diffusion-Bridge (VocalBridge), a purification framework that learns a latent mapping from perturbed to clean speech in the EnCodec latent space. Using a time-conditioned 1D U-Net with a cosine noise schedule, the model enables efficient, transcript-free purification while preserving speaker-discriminative structure. We further introduce a Whisper-guided phoneme variant that incorporates lightweight temporal guidance without requiring ground-truth transcripts. Experimental results show that our approach consistently outperforms existing purification methods in recovering cloneable voices from protected speech. Our findings demonstrate the fragility of current perturbation-based defenses and highlight the need for more robust protection mechanisms against evolving voice-cloning and speaker verification threats.", "AI": {"tldr": "本文提出了VocalBridge框架，用于去除语音中的防护扰动以恢复可克隆的语音。", "motivation": "当前基于防护扰动的防御机制在面对适应性净化技术时显得脆弱，无法有效阻止未经授权的声纹复制。因此，需要一种新的净化方法来应对这些问题。", "method": "提出了一种称为Diffusion-Bridge (VocalBridge) 的框架，该框架通过学习受扰语音到未受扰语音的潜在映射，在EnCodec潜在空间中进行高效、无转录文本指导的净化，并保持说话人鉴别结构。引入了Whisper引导音素变体，以轻量级时序指导进行改进。", "result": "实验结果显示，该方法在从保护语音中恢复可克隆声音方面始终优于现有净化技术。", "conclusion": "研究结果表明现有的基于扰动的防御机制脆弱，并强调需要更强大的保护措施来应对不断发展的声纹复制和说话人验证威胁。"}}
{"id": "2601.02443", "pdf": "https://arxiv.org/pdf/2601.02443", "abs": "https://arxiv.org/abs/2601.02443", "authors": ["Li Wang", "Xi Chen", "XiangWen Deng", "HuaHui Yi", "ZeKun Jiang", "Kang Li", "Jian Li"], "title": "Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.", "AI": {"tldr": "评估多模态大型语言模型在膝关节骨性关节炎（OA）影像学分类任务中的诊断能力。", "motivation": "尽管多模态大型语言模型（MLLMs）在医学视觉问答和报告生成方面表现出色，但其解释能力和特定疾病分类之间的转移效果并不可靠。研究者通过系统消融实验来评估各组件对诊断准确性的贡献，并探讨数据平衡和质量的重要性。", "method": "通过对视觉编码器、连接器以及大型语言模型（LLM）进行系统的消融研究，研究了不同训练策略下每个组件的分类准确性贡献；比较了完全MLLM管道与单独使用训练好的视觉编码器的表现；评估了LoRA微调在小规模平衡数据集上的效果。", "result": "独立的视觉编码器能够超越完整的MLLM管线，在膝关节OA影像学分类任务中表现出更高的准确率。LLM的微调并没有显著改善表现，提示良好的指导策略比直接训练更有效。此外，较小但平衡的数据集在LoRA微调上取得了更好的结果。", "conclusion": "对于特定医学领域的诊断分类任务，大型语言模型更适合作为解释者和报告生成器而不是主要分类器。优化视觉编码器并精心策划数据集是开发临床应用系统的关键步骤。"}}
{"id": "2601.02441", "pdf": "https://arxiv.org/pdf/2601.02441", "abs": "https://arxiv.org/abs/2601.02441", "authors": ["Yuan Li", "Shin'ya Nishida"], "title": "Understanding Pure Textual Reasoning for Blind Image Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": "Code available at https://anonymous.4open.science/r/Bridging-Image-Text-Gap-for-BIQA-CF5B/. This work is under review", "summary": "Textual reasoning has recently been widely adopted in Blind Image Quality Assessment (BIQA). However, it remains unclear how textual information contributes to quality prediction and to what extent text can represent the score-related image contents. This work addresses these questions from an information-flow perspective by comparing existing BIQA models with three paradigms designed to learn the image-text-score relationship: Chain-of-Thought, Self-Consistency, and Autoencoder. Our experiments show that the score prediction performance of the existing model significantly drops when only textual information is used for prediction. Whereas the Chain-of-Thought paradigm introduces little improvement in BIQA performance, the Self-Consistency paradigm significantly reduces the gap between image- and text-conditioned predictions, narrowing the PLCC/SRCC difference to 0.02/0.03. The Autoencoder-like paradigm is less effective in closing the image-text gap, yet it reveals a direction for further optimization. These findings provide insights into how to improve the textual reasoning for BIQA and high-level vision tasks.", "AI": {"tldr": "本文通过信息流视角探讨了文本推理在盲图像质量评估中的作用，并比较了几种范式的表现。", "motivation": "目前，关于文本如何帮助预测图像质量和文本能否有效代表与评分相关的图像内容仍不明确。为了填补这一研究空白，作者从信息流动的角度进行探索。", "method": "实验设计了三种模型：链式思考、自一致性以及自动编码器类范式来学习图像-文本-分数关系，并在仅使用文字信息预测时评估现有模型的性能变化。", "result": "结果显示，当只有文本信息用于预测时，现有BIQA模型的表现显著下降。虽然链式思考范式对BIQA性能提升不大，但自一致性范式能大幅缩小基于图像和基于文本条件下的预测差距。", "conclusion": "通过研究结果得出了改进文本推理在BIQA及其他高级视觉任务中应用的方法方向。"}}
{"id": "2601.02440", "pdf": "https://arxiv.org/pdf/2601.02440", "abs": "https://arxiv.org/abs/2601.02440", "authors": ["Jungi Lee", "Jungkwon Kim", "Chi Zhang", "Sangmin Kim", "Kwangsun Yoo", "Seok-Joo Byun"], "title": "Mitigating Long-Tailed Anomaly Score Distributions with Importance-Weighted Loss", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "8 pages, Published as a conference paper at IJCNN 2025", "summary": "Anomaly detection is crucial in industrial applications for identifying rare and unseen patterns to ensure system reliability. Traditional models, trained on a single class of normal data, struggle with real-world distributions where normal data exhibit diverse patterns, leading to class imbalance and long-tailed anomaly score distributions (LTD). This imbalance skews model training and degrades detection performance, especially for minority instances. To address this issue, we propose a novel importance-weighted loss designed specifically for anomaly detection. Compared to the previous method for LTD in classification, our method does not require prior knowledge of normal data classes. Instead, we introduce a weighted loss function that incorporates importance sampling to align the distribution of anomaly scores with a target Gaussian, ensuring a balanced representation of normal data. Extensive experiments on three benchmark image datasets and three real-world hyperspectral imaging datasets demonstrate the robustness of our approach in mitigating LTD-induced bias. Our method improves anomaly detection performance by 0.043, highlighting its effectiveness in real-world applications.", "AI": {"tldr": "本文提出了一种针对异常检测的重要性加权损失方法，用于解决长尾异常分数分布问题。", "motivation": "传统模型在处理具有多样化正常数据模式的真实世界场景时表现不佳，导致分类不平衡和长尾异常分数分布。这影响了模型训练并降低了对少数实例的检测性能。", "method": "提出了一种新颖的重要性加权损失方法，该方法通过引入权重损失函数来解决长尾问题，以平衡正常数据的表现，并不依赖于正常数据类别的先验知识。", "result": "在三个基准图像数据集和三个真实世界的高光谱成像数据集中进行了广泛的实验，证明了该方法能够缓解由LTD引起的偏差。与先前的方法相比，我们的方法提高了0.043的异常检测性能。", "conclusion": "提出的重要性加权损失方法有效解决了长尾异常分数分布问题，并且在实际应用中表现出了优越性。"}}
{"id": "2601.02439", "pdf": "https://arxiv.org/pdf/2601.02439", "abs": "https://arxiv.org/abs/2601.02439", "authors": ["Hao Bai", "Alexey Taymanov", "Tong Zhang", "Aviral Kumar", "Spencer Whitehead"], "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.", "AI": {"tldr": "本文介绍了WebGym，这是目前最大的开源环境，用于训练视觉网络代理。", "motivation": "现实中的网站具有非平稳性且多样化，使得人工或小型任务集难以支持鲁棒策略学习。因此需要一个包含大量真实世界任务的大型任务集来确保有效的视觉web代理训练。", "method": "WebGym通过开发高吞吐量异步回放系统来加速样本轨迹采样，使强化学习（RL）得以在大规模的任务集中有效执行，并利用任务奖励作为反馈指导学习过程。同时通过调整任务集的广度、深度和规模以实现性能提升。", "result": "使用WebGym对Qwen-3-VL-8B-Instruct进行微调后，成功率为42.9%，显著优于基于GPT-4o（成功率27.1%）和GPT-5-Thinking（成功率29.8%）的代理。", "conclusion": "WebGym通过提供大规模真实网站任务集提升了视觉web代理训练的效果，并展示了其在未见过的网络环境中的卓越性能。"}}
{"id": "2601.02438", "pdf": "https://arxiv.org/pdf/2601.02438", "abs": "https://arxiv.org/abs/2601.02438", "authors": ["Yun Bian", "Yi Chen", "HaiQuan Wang", "ShiHao Li", "Zhe Cui"], "title": "Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.", "AI": {"tldr": "提出TaCCS-DFA框架，利用Fisher信息进行任务导向互补融合，提高软件漏洞检测的准确性。", "motivation": "现有多模态方法在融合自然代码序列和代码属性图表示时存在冗余问题及噪声传播风险，影响分类效果。为此，作者引入Fisher信息作为几何度量，实现更精确的任务导向融合。", "method": "TaCCS-DFA框架通过在线估计低秩主Fisher子空间并限制跨模态注意力到任务敏感方向，从代码属性图中提取结构特征以补充序列模式；同时采用自适应门控机制动态调整每一样本的图形模态贡献度，抑制噪声传播。", "result": "实验结果表明，在BigVul、Devign和ReVeal数据集上，TaCCS-DFA在多种骨干网络下表现出色。使用CodeT5作为骨干网络时，TaCCS-DFA在高度不平衡的数据集中F1得分达87.80%，较基线模型提升了6.3个百分点。", "conclusion": "TaCCS-DFA通过引入Fisher信息进行任务导向融合和自适应噪声抑制机制，在软件漏洞检测中实现了更优性能。"}}
{"id": "2601.02437", "pdf": "https://arxiv.org/pdf/2601.02437", "abs": "https://arxiv.org/abs/2601.02437", "authors": ["Zhibo Wang", "Zuoyuan Zhang", "Xiaoyi Pang", "Qile Zhang", "Xuanyi Hao", "Shuguo Zhuo", "Peng Sun"], "title": "TAP-ViTs: Task-Adaptive Pruning for On-Device Deployment of Vision Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated strong performance across a wide range of vision tasks, yet their substantial computational and memory demands hinder efficient deployment on resource-constrained mobile and edge devices. Pruning has emerged as a promising direction for reducing ViT complexity. However, existing approaches either (i) produce a single pruned model shared across all devices, ignoring device heterogeneity, or (ii) rely on fine-tuning with device-local data, which is often infeasible due to limited on-device resources and strict privacy constraints. As a result, current methods fall short of enabling task-customized ViT pruning in privacy-preserving mobile computing settings. This paper introduces TAP-ViTs, a novel task-adaptive pruning framework that generates device-specific pruned ViT models without requiring access to any raw local data. Specifically, to infer device-level task characteristics under privacy constraints, we propose a Gaussian Mixture Model (GMM)-based metric dataset construction mechanism. Each device fits a lightweight GMM to approximate its private data distribution and uploads only the GMM parameters. Using these parameters, the cloud selects distribution-consistent samples from public data to construct a task-representative metric dataset for each device. Based on this proxy dataset, we further develop a dual-granularity importance evaluation-based pruning strategy that jointly measures composite neuron importance and adaptive layer importance, enabling fine-grained, task-aware pruning tailored to each device's computational budget. Extensive experiments across multiple ViT backbones and datasets demonstrate that TAP-ViTs consistently outperforms state-of-the-art pruning methods under comparable compression ratios.", "AI": {"tldr": "TAP-ViTs 提出了一个任务自适应剪枝框架，用于在资源受限的设备上部署 ViT 模型。", "motivation": "现有的 ViT 剪枝方法要么生成单一模型无法满足不同设备需求，要么需要本地数据进行微调受制于隐私和资源限制。", "method": "通过 GMM 构建任务代表性元数据集并在其上执行双粒度重要性评估的剪枝策略。", "result": "实验表明 TAP-ViTs 在各种 ViT 模型和数据集上的压缩比下优于现有方法。", "conclusion": "TAP-ViTs 实现了在不违反隐私的前提下，为不同设备提供定制化的任务适应性剪枝。"}}
{"id": "2601.02436", "pdf": "https://arxiv.org/pdf/2601.02436", "abs": "https://arxiv.org/abs/2601.02436", "authors": ["Pinzhen Chen", "Libo Xu", "Boyang Pan", "Jing Li", "Yuting Wang", "Ran Xiong", "Xiaoli Gou", "Long Qing", "Wenjing Hou", "Nan-jie Gong", "Wei Chen"], "title": "Deep Learning Superresolution for 7T Knee MR Imaging: Impact on Image Quality and Diagnostic Performance", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Background: Deep learning superresolution (SR) may enhance musculoskeletal MR image quality, but its diagnostic value in knee imaging at 7T is unclear. Objectives: To compare image quality and diagnostic performance of SR, low-resolution (LR), and high-resolution (HR) 7T knee MRI. Methods: In this prospective study, 42 participants underwent 7T knee MRI with LR (0.8*0.8*2 mm3) and HR (0.4*0.4*2 mm3) sequences. SR images were generated from LR data using a Hybrid Attention Transformer model. Three radiologists assessed image quality, anatomic conspicuity, and detection of knee pathologies. Arthroscopy served as reference in 10 cases. Results: SR images showed higher overall quality than LR (median score 5 vs 4, P<.001) and lower noise than HR (5 vs 4, P<.001). Visibility of cartilage, menisci, and ligaments was superior in SR and HR compared to LR (P<.001). Detection rates and diagnostic performance (sensitivity, specificity, AUC) for intra-articular pathology were similar across image types (P>=.095). Conclusions: Deep learning superresolution improved subjective image quality in 7T knee MRI but did not increase diagnostic accuracy compared with standard LR imaging.", "AI": {"tldr": "研究比较了深度学习超分辨率（SR）、低分辨率（LR）和高分辨率（HR）7T膝关节MRI在图像质量和诊断性能上的差异", "motivation": "评估深度学习超分辨率技术对7T膝关节MR成像质量的影响及其潜在的临床应用价值", "method": "使用Hybrid Attention Transformer模型从低分辨率数据生成超分辨率图像，由三位放射科医生进行图像质量和解剖结构可见性以及病变检测的评估，部分病例以关节镜检查为参考标准", "result": "与LR相比，SR图像的整体质量更高且噪声更低；在软组织（如软骨、半月板和韧带）的可视性和病变检测率方面，SR和HR优于LR；但不同类型图像之间的诊断准确性相似", "conclusion": "深度学习超分辨率技术改善了7T膝关节MRI的主观图像质量，但在与标准低分辨率成像相比时，并未提高诊断准确度"}}
{"id": "2601.02432", "pdf": "https://arxiv.org/pdf/2601.02432", "abs": "https://arxiv.org/abs/2601.02432", "authors": ["Ha Tran", "Bipasha Kashyap", "Pubudu N. Pathirana"], "title": "Quantifying Quanvolutional Neural Networks Robustness for Speech in Healthcare Applications", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Speech-based machine learning systems are sensitive to noise, complicating reliable deployment in emotion recognition and voice pathology detection. We evaluate the robustness of a hybrid quantum machine learning model, quanvolutional neural networks (QNNs) against classical convolutional neural networks (CNNs) under four acoustic corruptions (Gaussian noise, pitch shift, temporal shift, and speed variation) in a clean-train/corrupted-test regime. Using AVFAD (voice pathology) and TESS (speech emotion), we compare three QNN models (Random, Basic, Strongly) to a simple CNN baseline (CNN-Base), ResNet-18 and VGG-16 using accuracy and corruption metrics (CE, mCE, RCE, RmCE), and analyze architectural factors (circuit complexity or depth, convergence) alongside per-emotion robustness. QNNs generally outperform the CNN-Base under pitch shift, temporal shift, and speed variation (up to 22% lower CE/RCE at severe temporal shift), while the CNN-Base remains more resilient to Gaussian noise. Among quantum circuits, QNN-Basic achieves the best overall robustness on AVFAD, and QNN-Random performs strongest on TESS. Emotion-wise, fear is most robust (80-90% accuracy under severe corruptions), neutral can collapse under strong Gaussian noise (5.5% accuracy), and happy is most vulnerable to pitch, temporal, and speed distortions. QNNs also converge up to six times faster than the CNN-Base. To our knowledge, this is a systematic study of QNN robustness for speech under common non-adversarial acoustic corruptions, indicating that shallow entangling quantum front-ends can improve noise resilience while sensitivity to additive noise remains a challenge.", "AI": {"tldr": "该论文评估了混合量子机器学习模型quanvolutional神经网络(QNN)在语音处理中的鲁棒性，特别是在医疗保健应用中。", "motivation": "基于语音的机器学习系统对噪声敏感，这使得它们难以可靠地应用于情感识别和声音病理检测。为了应对这个问题，作者比较了QNN与经典卷积神经网络(CNN)，以评估其在不同噪声情况下的性能。", "method": "通过在AVFAD(语音病理)和TESS(语音情绪)数据集上进行实验，并使用多种准确性指标（包括CE、mCE、RCE、RmCE），作者比较了三种QNN模型与CNN基线(CNN-Base)，ResNet-18和VGG-16的性能。此外，还分析了量子电路复杂度或深度以及收敛性等因素。", "result": "在某些噪声条件下（例如音高变化、时间偏移和速度变化），QNN优于CNN-Base，并且能够在一定程度上提高鲁棒性(最多降低22%CE/RCE)；然而，在高斯噪声下，CNN-Base更稳健。QNN-Basic在AVFAD数据集上的整体鲁棒性最好，而QNN-Random对TESS的表现最强。", "conclusion": "这是系统研究了QNN在非对抗性声学干扰下语音处理的鲁棒性的首次尝试，表明浅层纠缠量子前端可以提高噪声抵御能力，但仍然存在对加性噪声敏感的问题。"}}
{"id": "2601.02430", "pdf": "https://arxiv.org/pdf/2601.02430", "abs": "https://arxiv.org/abs/2601.02430", "authors": ["Chenxu Liu", "Yingjie Fu", "Wei Yang", "Ying Zhang", "Tao Xie"], "title": "WebCoderBench: Benchmarking Web Application Generation with Comprehensive and Interpretable Evaluation Metrics", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Web applications (web apps) have become a key arena for large language models (LLMs) to demonstrate their code generation capabilities and commercial potential. However, building a benchmark for LLM-generated web apps remains challenging due to the need for real-world user requirements, generalizable evaluation metrics without relying on ground-truth implementations or test cases, and interpretable evaluation results. To address these challenges, we introduce WebCoderBench, the first real-world-collected, generalizable, and interpretable benchmark for web app generation. WebCoderBench comprises 1,572 real user requirements, covering diverse modalities and expression styles that reflect realistic user intentions. WebCoderBench provides 24 fine-grained evaluation metrics across 9 perspectives, combining rule-based and LLM-as-a-judge paradigm for fully automated, objective, and general evaluation. Moreover, WebCoderBench adopts human-preference-aligned weights over metrics to yield interpretable overall scores. Experiments across 12 representative LLMs and 2 LLM-based agents show that there exists no dominant model across all evaluation metrics, offering an opportunity for LLM developers to optimize their models in a targeted manner for a more powerful version.", "AI": {"tldr": "构建用于评估大型语言模型生成的网页应用的基准测试WebCoderBench。", "motivation": "现有的网络应用程序生成基准测试缺乏真实用户需求、通用可扩展性以及解释性的评价结果。为了克服这些挑战，提出了一个全面和可解释的网络应用生成基准测试。", "method": "收集了1572个真实的用户需求，并提供了24项细粒度评价指标，在9个视角下评估模型性能。结合规则基线和语言模型作为评委的方法实现全自动、客观且通用化的评估。", "result": "实验显示，没有一个模型能在所有评价指标上表现出色，为模型优化提供机会。", "conclusion": "WebCoderBench是一个基于真实世界需求的网络应用生成基准测试平台，有助于提高大型语言模型在代码生成能力方面的表现。"}}
{"id": "2601.02428", "pdf": "https://arxiv.org/pdf/2601.02428", "abs": "https://arxiv.org/abs/2601.02428", "authors": ["Okan Bursa"], "title": "A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance", "categories": ["cs.IR", "cs.AI"], "comment": "6 Pages, 2 figures", "summary": "We introduce \\emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \\emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\\approx$ 0.940, Recall@5 $=1.000$) with only $\\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings. ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.", "AI": {"tldr": "介绍了一种基于动态记忆机制的检索增强生成框架ARM，通过选择性遗忘和巩固策略优化内存使用。", "motivation": "为了提高检索增强生成系统（RAG）在资源有限条件下的性能，并使其更加高效、灵活且适应性强。该方法灵感来自认知科学中的记忆原则。", "method": "设计了一个基于动态记忆机制的框架ARM，利用选择性遗忘和巩固策略来管理内存中的项目。此外，还优化了DynamicRAG的实现以提高效率。", "result": "在轻量级检索基准测试中达到了接近最先进的性能，并且实现了高效的参数使用（约22M）。通过比较不同模型与静态或动态RAG组合的效果，发现ARM框架提供了质量和延迟之间的良好平衡。", "conclusion": "ARM不仅提高了生成质量而且增强了内存效率和可解释性，同时还能适应不同的应用场景需求。"}}
{"id": "2601.02427", "pdf": "https://arxiv.org/pdf/2601.02427", "abs": "https://arxiv.org/abs/2601.02427", "authors": ["Loïc Magne", "Anas Awadalla", "Guanzhi Wang", "Yinzhen Xu", "Joshua Belofsky", "Fengyuan Hu", "Joohwan Kim", "Ludwig Schmidt", "Georgia Gkioxari", "Jan Kautz", "Yisong Yue", "Yejin Choi", "Yuke Zhu", "Linxi \"Jim\" Fan"], "title": "NitroGen: An Open Foundation Model for Generalist Gaming Agents", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "We introduce NitroGen, a vision-action foundation model for generalist gaming agents that is trained on 40,000 hours of gameplay videos across more than 1,000 games. We incorporate three key ingredients: 1) an internet-scale video-action dataset constructed by automatically extracting player actions from publicly available gameplay videos, 2) a multi-game benchmark environment that can measure cross-game generalization, and 3) a unified vision-action model trained with large-scale behavior cloning. NitroGen exhibits strong competence across diverse domains, including combat encounters in 3D action games, high-precision control in 2D platformers, and exploration in procedurally generated worlds. It transfers effectively to unseen games, achieving up to 52% relative improvement in task success rates over models trained from scratch. We release the dataset, evaluation suite, and model weights to advance research on generalist embodied agents.", "AI": {"tldr": "介绍NitroGen，一个专为游戏代理设计的视觉行动基础模型。", "motivation": "通过使用大规模数据集和多游戏基准环境来训练具有跨游戏泛化能力的游戏代理。", "method": "利用互联网规模的视频动作数据集、多游戏基准环境及行为克隆方法进行大型统一视觉行动模型的训练。", "result": "NitroGen在多种游戏中表现出色，包括3D动作游戏中的战斗场景、2D平台游戏中的高精度控制以及程序生成世界中的探索任务。其性能优于从头开始训练的模型，在任务成功率上最高提高了52%。", "conclusion": "发布数据集、评估套件和模型权重以推进一般化实体代理研究。"}}
{"id": "2601.02424", "pdf": "https://arxiv.org/pdf/2601.02424", "abs": "https://arxiv.org/abs/2601.02424", "authors": ["Kai Gu", "Yingping Liang", "Senliang Peng", "Aotian Guo", "Haizheng Zhong", "Ying Fu"], "title": "A large-scale nanocrystal database with aligned synthesis and properties enabling generative inverse design", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "The synthesis of nanocrystals has been highly dependent on trial-and-error, due to the complex correlation between synthesis parameters and physicochemical properties. Although deep learning offers a potential methodology to achieve generative inverse design, it is still hindered by the scarcity of high-quality datasets that align nanocrystal synthesis routes with their properties. Here, we present the construction of a large-scale, aligned Nanocrystal Synthesis-Property (NSP) database and demonstrate its capability for generative inverse design. To extract structured synthesis routes and their corresponding product properties from literature, we develop NanoExtractor, a large language model (LLM) enhanced by well-designed augmentation strategies. NanoExtractor is validated against human experts, achieving a weighted average score of 88% on the test set, significantly outperforming chemistry-specialized (3%) and general-purpose LLMs (38%). The resulting NSP database contains nearly 160,000 aligned entries and serves as training data for our NanoDesigner, an LLM for inverse synthesis design. The generative capability of NanoDesigner is validated through the successful design of viable synthesis routes for both well-established PbSe nanocrystals and rarely reported MgF2 nanocrystals. Notably, the model recommends a counter-intuitive, non-stoichiometric precursor ratio (1:1) for MgF2 nanocrystals, which is experimentally confirmed as critical for suppressing byproducts. Our work bridges the gap between unstructured literature and data-driven synthesis, and also establishes a powerful human-AI collaborative paradigm for accelerating nanocrystal discovery.", "AI": {"tldr": "构建了一个大规模的纳米晶体合成与性质数据库，并开发了NanoExtractor和NanoDesigner模型，实现了基于生成逆向设计的方法。", "motivation": "当前纳米晶体合成依赖于试错法，因为难以准确预测合成参数与其物理化学性质之间的复杂关系。深度学习可以提供潜在的方法来实现生成逆向设计，但由于高质量的数据集缺乏而受阻。", "method": "开发了一个名为NanoExtractor的大规模语言模型（LLM），用于从文献中提取结构化的纳米晶体合成路线及其相应的产品属性，并通过实验验证了其性能；基于构建的数据库训练了一种名为NanoDesigner的逆向设计模型，实现了生成性的逆向设计方法。", "result": "该研究成功地设计出了铅硒和镁氟化物两种不同类型的纳米晶体的合成路线，特别是推荐了一个反直觉而非化学计量比的前驱体比例（1:1）用于镁氟化物纳米晶体的合成，并通过实验验证了其有效性。", "conclusion": "该研究填补了文献信息与数据驱动合成之间的空白，同时也建立了一个人机协作的强大范式，可以加速纳米晶体发现。"}}
{"id": "2601.02422", "pdf": "https://arxiv.org/pdf/2601.02422", "abs": "https://arxiv.org/abs/2601.02422", "authors": ["Wenting Lu", "Didi Zhu", "Tao Shen", "Donglin Zhu", "Ayong Ye", "Chao Wu"], "title": "Watch Wider and Think Deeper: Collaborative Cross-modal Chain-of-Thought for Complex Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal reasoning requires the seamless integration of visual and linguistic cues, yet existing Chain-of-Thought methods suffer from two critical limitations in cross-modal scenarios: (1) over-reliance on single coarse-grained image regions, and (2) semantic fragmentation between successive reasoning steps. To address these issues, we propose the CoCoT (Collaborative Coross-modal Thought) framework, built upon two key innovations: a) Dynamic Multi-Region Grounding to adaptively detect the most relevant image regions based on the question, and b) Relation-Aware Reasoning to enable multi-region collaboration by iteratively aligning visual cues to form a coherent and logical chain of thought. Through this approach, we construct the CoCoT-70K dataset, comprising 74,691 high-quality samples with multi-region annotations and structured reasoning chains. Extensive experiments demonstrate that CoCoT significantly enhances complex visual reasoning, achieving an average accuracy improvement of 15.4% on LLaVA-1.5 and 4.0% on Qwen2-VL across six challenging benchmarks. The data and code are available at: https://github.com/deer-echo/CoCoT.", "AI": {"tldr": "本文提出了CoCoT框架，用于改进复杂视觉推理任务中的跨模态Chain-of-Thought方法。", "motivation": "现有的Chain-of-Thought方法在处理跨模态场景时存在过度依赖单一粗粒度图像区域以及语义碎片化的问题。为解决这些问题，作者提出了一种新的框架CoCoT。", "method": "该方法包括两个关键创新：动态多区域接地和关系感知推理。前者基于问题自适应地检测最相关的图像区域；后者通过迭代对齐视觉线索实现多个区域之间的协作，形成连贯的逻辑思维链。", "result": "实验表明，CoCoT框架在LLaVA-1.5上平均准确度提高了15.4%，在Qwen2-VL上提高了4.0%。这些结果是在六个具有挑战性的基准测试中取得的。", "conclusion": "通过引入动态多区域接地和关系感知推理，CoCoT框架显著提升了复杂视觉推理任务的表现。"}}
{"id": "2601.02415", "pdf": "https://arxiv.org/pdf/2601.02415", "abs": "https://arxiv.org/abs/2601.02415", "authors": ["Wangyuan Zhu", "Jun Yu"], "title": "Multimodal Sentiment Analysis based on Multi-channel and Symmetric Mutual Promotion Feature Fusion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal sentiment analysis is a key technology in the fields of human-computer interaction and affective computing. Accurately recognizing human emotional states is crucial for facilitating smooth communication between humans and machines. Despite some progress in multimodal sentiment analysis research, numerous challenges remain. The first challenge is the limited and insufficiently rich features extracted from single modality data. Secondly, most studies focus only on the consistency of inter-modal feature information, neglecting the differences between features, resulting in inadequate feature information fusion. In this paper, we first extract multi-channel features to obtain more comprehensive feature information. We employ dual-channel features in both the visual and auditory modalities to enhance intra-modal feature representation. Secondly, we propose a symmetric mutual promotion (SMP) inter-modal feature fusion method. This method combines symmetric cross-modal attention mechanisms and self-attention mechanisms, where the cross-modal attention mechanism captures useful information from other modalities, and the self-attention mechanism models contextual information. This approach promotes the exchange of useful information between modalities, thereby strengthening inter-modal interactions. Furthermore, we integrate intra-modal features and inter-modal fused features, fully leveraging the complementarity of inter-modal feature information while considering feature information differences. Experiments conducted on two benchmark datasets demonstrate the effectiveness and superiority of our proposed method.", "AI": {"tldr": "本文提出了一种基于多通道和对称互促特征融合的多模态情感分析方法，旨在提高人类-机器交互中情绪状态识别的准确性。", "motivation": "在单模态数据中提取有限且不足的特征，以及大多数研究只关注跨模态特性信息的一致性而忽视了差异的问题。这些问题是多模态情感分析领域面临的挑战。", "method": "首先从多个通道中提取更多的全面的特征信息，并引入双通道视觉和听觉模态增强内部特征表示；其次提出了一个对称互促（SMP）跨模态特征融合方法，结合了对称交叉模式注意机制和自我注意力机制。实验表明该方法有效。", "result": "在两个基准数据集上的实验验证了所提方法的有效性和优越性。", "conclusion": "通过多通道特征提取及对称互促跨模态特征融合的方法提升了情感分析的准确性，为促进人机交互提供了更好的基础。"}}
{"id": "2601.02414", "pdf": "https://arxiv.org/pdf/2601.02414", "abs": "https://arxiv.org/abs/2601.02414", "authors": ["Jichao Zhu", "Jun Yu"], "title": "MIAR: Modality Interaction and Alignment Representation Fuison for Multimodal Emotion", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal Emotion Recognition (MER) aims to perceive human emotions through three modes: language, vision, and audio. Previous methods primarily focused on modal fusion without adequately addressing significant distributional differences among modalities or considering their varying contributions to the task. They also lacked robust generalization capabilities across diverse textual model features, thus limiting performance in multimodal scenarios. Therefore, we propose a novel approach called Modality Interaction and Alignment Representation (MIAR). This network integrates contextual features across different modalities using a feature interaction to generate feature tokens to represent global representations of this modality extracting information from other modalities. These four tokens represent global representations of how each modality extracts information from others. MIAR aligns different modalities using contrastive learning and normalization strategies. We conduct experiments on two benchmarks: CMU-MOSI and CMU-MOSEI datasets, experimental results demonstrate the MIAR outperforms state-of-the-art MER methods.", "AI": {"tldr": "本文提出了MIAR方法，旨在通过模态交互和对齐表示来解决多模态情感识别中的分布差异问题。", "motivation": "传统方法在多模态情感识别中主要关注模态融合，但忽视了不同模态之间的分布差异以及它们各自在任务中的贡献。此外，这些方法在处理多样化文本模型特征时的泛化能力较差。", "method": "MIAR通过特征交互将上下文信息跨不同的模态进行整合，并使用对比学习和归一化策略对齐不同模态。", "result": "实验结果表明，在CMU-MOSI和CMU-MOSEI数据集上，MIAR优于当前最先进的多模态情感识别方法。", "conclusion": "通过引入MIAR方法解决了传统多模态情感识别中存在的一些问题，并取得了更好的性能。"}}
{"id": "2601.02412", "pdf": "https://arxiv.org/pdf/2601.02412", "abs": "https://arxiv.org/abs/2601.02412", "authors": ["Lukas Schüepp", "Carmen Amo Alonso", "Florian Dörfler", "Giulia De Pasquale"], "title": "Socially-Aware Recommender Systems Mitigate Opinion Clusterization", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender systems shape online interactions by matching users with creators content to maximize engagement. Creators, in turn, adapt their content to align with users preferences and enhance their popularity. At the same time, users preferences evolve under the influence of both suggested content from the recommender system and content shared within their social circles. This feedback loop generates a complex interplay between users, creators, and recommender algorithms, which is the key cause of filter bubbles and opinion polarization. We develop a social network-aware recommender system that explicitly accounts for this user-creators feedback interaction and strategically exploits the topology of the user's own social network to promote diversification. Our approach highlights how accounting for and exploiting user's social network in the recommender system design is crucial to mediate filter bubble effects while balancing content diversity with personalization. Provably, opinion clusterization is positively correlated with the influence of recommended content on user opinions. Ultimately, the proposed approach shows the power of socially-aware recommender systems in combating opinion polarization and clusterization phenomena.", "AI": {"tldr": "开发一种社交网络感知的推荐系统，以减轻意见极化和群体化现象。", "motivation": "当前推荐系统的反馈循环会导致过滤气泡和个人偏见加剧。通过考虑用户的社交网络影响来设计推荐算法可以缓解这些负面效果。", "method": "提出了一种基于用户社交关系图的推荐机制，旨在利用社交结构促进内容多样化的同时保持个性化推荐。", "result": "该方法证明了意见群集化与推荐内容对用户观点的影响正相关。实验表明，这种社交感知的推荐系统在对抗意见极化和群体化方面表现出强大的能力。", "conclusion": "研究结果强调，在设计推荐系统时考虑用户的社交网络是必要的，可以有效缓解过滤气泡问题，并保持个性化的用户体验。"}}
{"id": "2601.02411", "pdf": "https://arxiv.org/pdf/2601.02411", "abs": "https://arxiv.org/abs/2601.02411", "authors": ["Kaiwen Tang", "Jiaqi Zheng", "Yuze Jin", "Yupeng Qiu", "Guangda Sun", "Zhanglu Yan", "Weng-Fai Wong"], "title": "SpikySpace: A Spiking State Space Model for Energy-Efficient Time Series Forecasting", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "Time-series forecasting often operates under tight power and latency budgets in fields like traffic management, industrial condition monitoring, and on-device sensing. These applications frequently require near real-time responses and low energy consumption on edge devices. Spiking neural networks (SNNs) offer event-driven computation and ultra-low power by exploiting temporal sparsity and multiplication-free computation. Yet existing SNN-based time-series forecasters often inherit complex transformer blocks, thereby losing much of the efficiency benefit. To solve the problem, we propose SpikySpace, a spiking state-space model (SSM) that reduces the quadratic cost in the attention block to linear time via selective scanning. Further, we replace dense SSM updates with sparse spike trains and execute selective scans only on spike events, thereby avoiding dense multiplications while preserving the SSM's structured memory. Because complex operations such as exponentials and divisions are costly on neuromorphic chips, we introduce simplified approximations of SiLU and Softplus to enable a neuromorphic-friendly model architecture. In matched settings, SpikySpace reduces estimated energy consumption by 98.73% and 96.24% compared to two state-of-the-art transformer based approaches, namely iTransformer and iSpikformer, respectively. In standard time series forecasting datasets, SpikySpace delivers competitive accuracy while substantially reducing energy cost and memory traffic. As the first full spiking state-space model, SpikySpace bridges neuromorphic efficiency with modern sequence modeling, marking a practical and scalable path toward efficient time series forecasting systems.", "AI": {"tldr": "提出了SpikySpace模型，这是一种基于脉冲神经网络的高效时间序列预测方法。", "motivation": "当前的时间序列预测应用需要在边缘设备上实现低能耗和近实时响应。现有的基于SNN的方法由于使用复杂的变压器模块而未能充分利用其节能潜力。", "method": "SpikySpace通过选择性扫描减少注意力块中的二次成本，并将密集的SSM更新替换为稀疏脉冲序列，从而避免了密集乘法操作。同时引入简化版SiLU和Softplus函数以适应神经形态芯片。", "result": "在对比实验中，SpikySpace相较于iTransformer和iSpikformer分别节省了98.73%和96.24%的能耗，并且在标准时间序列数据集上保持了竞争性的准确率。", "conclusion": "作为首个全脉冲状态空间模型，SpikySpace展现了神经形态计算与现代序列建模相结合的实际应用前景。"}}
{"id": "2601.02410", "pdf": "https://arxiv.org/pdf/2601.02410", "abs": "https://arxiv.org/abs/2601.02410", "authors": ["Aizierjiang Aiersilan"], "title": "The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.GR"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \\textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \\textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \\textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \\textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \\textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.", "AI": {"tldr": "提出了Vibe-Check Protocol（VCP）来量化AI编程中的认知卸载，以评估使用自然语言与开发人员意图进行交互的代码生成是否更有利于软件工程教育。", "motivation": "探讨了在软件工程教育中，通过自然语言与LLM互动进行“Vibe编码”的方式是否会比传统方法更好。存在对学生技能保留和概念理解的影响担忧，需要一个理论框架来研究这个问题。", "method": "提出了包含冷启动重构（$M_{CSR}$）、幻觉陷阱检测（$M_{HT}$）及可解释性差距（$E_{gap}$）三个量化指标的Vibe-Check Protocol，用于评估认知卸载的影响。", "result": "通过控制比较实验，使用VCP框架为教育者提供了关于在哪些情况下“Vibe编码”有助于真正掌握知识和哪些情况下会导致隐性技术债务及浅层理解的信息。", "conclusion": "基于量化结果，确定了利用AI进行编程加速与认知卸载之间的差异，帮助教育者识别出更适合使用或避免使用“Vibe编码”的情境。"}}
{"id": "2601.02409", "pdf": "https://arxiv.org/pdf/2601.02409", "abs": "https://arxiv.org/abs/2601.02409", "authors": ["Longwei Wang", "Ifrat Ikhtear Uddin", "KC Santosh"], "title": "Expert-Guided Explainable Few-Shot Learning with Active Sample Selection for Medical Image Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted for publication in IEEE Journal of Biomedical and Health Informatics, 2025", "summary": "Medical image analysis faces two critical challenges: scarcity of labeled data and lack of model interpretability, both hindering clinical AI deployment. Few-shot learning (FSL) addresses data limitations but lacks transparency in predictions. Active learning (AL) methods optimize data acquisition but overlook interpretability of acquired samples. We propose a dual-framework solution: Expert-Guided Explainable Few-Shot Learning (EGxFSL) and Explainability-Guided AL (xGAL). EGxFSL integrates radiologist-defined regions-of-interest as spatial supervision via Grad-CAM-based Dice loss, jointly optimized with prototypical classification for interpretable few-shot learning. xGAL introduces iterative sample acquisition prioritizing both predictive uncertainty and attention misalignment, creating a closed-loop framework where explainability guides training and sample selection synergistically. On the BraTS (MRI), VinDr-CXR (chest X-ray), and SIIM-COVID-19 (chest X-ray) datasets, we achieve accuracies of 92\\%, 76\\%, and 62\\%, respectively, consistently outperforming non-guided baselines across all datasets. Under severe data constraints, xGAL achieves 76\\% accuracy with only 680 samples versus 57\\% for random sampling. Grad-CAM visualizations demonstrate guided models focus on diagnostically relevant regions, with generalization validated on breast ultrasound confirming cross-modality applicability.", "AI": {"tldr": "提出了一个结合专家指导和可解释性的少样本学习框架，用于医学影像分析。", "motivation": "为了应对医学图像分析中的标记数据稀缺以及模型不可解释性的问题，同时解决传统少样本学习方法缺乏透明度及主动学习忽视样本可解释性的问题。", "method": "提出了EGxFSL和xGAL两个框架：前者通过Grad-CAM与Dice损失的结合进行空间监督，后者则引入了迭代样本选择机制以提高预测不确定性和注意力不一致。两者相互作用形成了闭环系统。", "result": "在BraTS、VinDr-CXR及SIIM-COVID-19数据集上，所提方法实现了92%、76%和62%的准确率，并且相较于无引导基线模型表现出更佳性能。此外，在严重的数据限制条件下，xGAL使用仅680个样本即可达到76%的准确率。", "conclusion": "该研究证明了结合专家指导与可解释性的少样本学习框架的有效性及其在医学影像分析中的适用性，并展示了其跨模态的应用潜力。"}}
{"id": "2601.02407", "pdf": "https://arxiv.org/pdf/2601.02407", "abs": "https://arxiv.org/abs/2601.02407", "authors": ["Oguzhan Yildirim"], "title": "Evolving Personalities in Chaos: An LLM-Augmented Framework for Character Discovery in the Iterated Prisoners Dilemma under Environmental Stress", "categories": ["cs.NE", "cs.GT"], "comment": "10 pages, 5 figures. Project assignment; exploratory study on LLM-based adaptive agents", "summary": "Standard simulations of the Iterated Prisoners Dilemma (IPD) operate in deterministic, noise-free environments, producing strategies that may be theoretically optimal but fragile when confronted with real-world uncertainty. This paper addresses two critical gaps in evolutionary game theory research: (1) the absence of realistic environmental stressors during strategy evolution, and (2) the Interpretability Gap, where evolved genetic strategies remain opaque binary sequences devoid of semantic meaning. We introduce a novel framework combining stochastic environmental perturbations (God Mode) with Large Language Model (LLM)-based behavioral profiling to transform evolved genotypes into interpretable character archetypes. Our experiments demonstrate that strategies evolved under chaotic conditions exhibit superior resilience and present distinct behavioral phenotypes, ranging from Ruthless Capitalists to Diplomatic Enforcers. These phenotypes are readily classified by LLMs but remain nearly impossible to interpret through manual genome inspection alone. This work bridges evolutionary computation with explainable AI and provides a template for automated agent characterization in multi-agent systems.", "AI": {"tldr": "本文提出了一种结合随机环境扰动和大型语言模型的行为分析框架，以在迭代囚徒困境中演化出具有解释性的角色类型。", "motivation": "解决标准IPD模拟中的两个关键问题：缺乏现实的环境压力和不可理解的进化策略。引入框架来提高策略的鲁棒性和可解释性。", "method": "通过添加随机环境扰动并使用LLM进行行为分析，将遗传策略转化为具有解释性的角色类型。", "result": "演化出的表现型在混乱条件下展现出更好的适应性，并且可以被LLMs分类而难以通过手动基因检查理解。", "conclusion": "该工作结合进化计算和可解释的人工智能，为多代理系统中的自动化代理特征分析提供了一个模板。"}}
{"id": "2601.02404", "pdf": "https://arxiv.org/pdf/2601.02404", "abs": "https://arxiv.org/abs/2601.02404", "authors": ["Inpyo Song", "Eunji Jeon", "Jangwon Lee"], "title": "PCEval: A Benchmark for Evaluating Physical Computing Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Code and Dataset available at https://github.com/Null99-Dog/PCEval-Dataset", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including software development, education, and technical assistance. Among these, software development is one of the key areas where LLMs are increasingly adopted. However, when hardware constraints are considered-for instance, in physical computing, where software must interact with and control physical hardware -their effectiveness has not been fully explored. To address this gap, we introduce \\textsc{PCEval} (Physical Computing Evaluation), the first benchmark in physical computing that enables a fully automatic evaluation of the capabilities of LLM in both the logical and physical aspects of the projects, without requiring human assessment. Our evaluation framework assesses LLMs in generating circuits and producing compatible code across varying levels of project complexity. Through comprehensive testing of 13 leading models, \\textsc{PCEval} provides the first reproducible and automatically validated empirical assessment of LLMs' ability to reason about fundamental hardware implementation constraints within a simulation environment. Our findings reveal that while LLMs perform well in code generation and logical circuit design, they struggle significantly with physical breadboard layout creation, particularly in managing proper pin connections and avoiding circuit errors. \\textsc{PCEval} advances our understanding of AI assistance in hardware-dependent computing environments and establishes a foundation for developing more effective tools to support physical computing education.", "AI": {"tldr": "本文介绍了PCEval，一个用于评估大型语言模型在物理计算能力上的基准。", "motivation": "针对硬件约束下大型语言模型的有效性未充分探索的问题，提出了PCEval来自动评估这些模型的逻辑和物理方面的能力。", "method": "通过生成电路和编写兼容代码，在不同复杂度水平上测试了13个领先模型，并在模拟环境中对其进行了验证。", "result": "研究发现，大型语言模型在代码生成和逻辑电路设计中表现良好，但在物理面包板布局创建方面存在困难，尤其是在管理正确引脚连接和避免电路错误时。", "conclusion": "PCEval推进了对硬件依赖计算环境中AI辅助的理解，并为开发更有效的支持物理计算教育的工具奠定了基础。"}}
{"id": "2601.02401", "pdf": "https://arxiv.org/pdf/2601.02401", "abs": "https://arxiv.org/abs/2601.02401", "authors": ["Buqing Cao", "Qian Peng", "Xiang Xie", "Liang Chen", "Min Shi", "Jianxun Liu"], "title": "Spiking Heterogeneous Graph Attention Networks", "categories": ["cs.NE", "cs.LG", "cs.SI"], "comment": "This paper has been accepted by AAAI 2026", "summary": "Real-world graphs or networks are usually heterogeneous, involving multiple types of nodes and relationships. Heterogeneous graph neural networks (HGNNs) can effectively handle these diverse nodes and edges, capturing heterogeneous information within the graph, thus exhibiting outstanding performance. However, most methods of HGNNs usually involve complex structural designs, leading to problems such as high memory usage, long inference time, and extensive consumption of computing resources. These limitations pose certain challenges for the practical application of HGNNs, especially for resource-constrained devices. To mitigate this issue, we propose the Spiking Heterogeneous Graph Attention Networks (SpikingHAN), which incorporates the brain-inspired and energy-saving properties of Spiking Neural Networks (SNNs) into heterogeneous graph learning to reduce the computing cost without compromising the performance. Specifically, SpikingHAN aggregates metapath-based neighbor information using a single-layer graph convolution with shared parameters. It then employs a semantic-level attention mechanism to capture the importance of different meta-paths and performs semantic aggregation. Finally, it encodes the heterogeneous information into a spike sequence through SNNs, simulating bioinformatic processing to derive a binarized 1-bit representation of the heterogeneous graph. Comprehensive experimental results from three real-world heterogeneous graph datasets show that SpikingHAN delivers competitive node classification performance. It achieves this with fewer parameters, quicker inference, reduced memory usage, and lower energy consumption. Code is available at https://github.com/QianPeng369/SpikingHAN.", "AI": {"tldr": "提出了一种名为SpikingHAN的模型，该模型结合了脉冲神经网络和异构图注意力机制，旨在减少计算成本的同时保持性能。", "motivation": "为了应对大多数异构图神经网络方法中存在的复杂结构设计问题，如高内存使用量、长推理时间以及大量的计算资源消耗，从而限制其在资源受限设备上的应用。作者提出了一种新的模型SpikingHAN，该模型结合了脉冲神经网络和注意力机制的优点以减少计算成本。", "method": "SpikingHAN通过元路径聚合邻居信息，使用单层图卷积与共享参数进行聚合，并采用语义级注意机制捕获不同元路径的重要性。随后，利用SNN将异构信息编码为脉冲序列，生成二值的1位表示。", "result": "实验结果表明，在三个真实世界的异构图数据集上，SpikingHAN实现了与传统方法相当甚至更好的节点分类性能，同时在参数数量、推理速度、内存使用量和能耗方面有所改善。", "conclusion": "SpikingHAN是一种有效的模型，它结合了脉冲神经网络的节能特性和注意力机制的优势，在减少计算成本的同时保持了优异的异构图学习能力。"}}
{"id": "2601.02399", "pdf": "https://arxiv.org/pdf/2601.02399", "abs": "https://arxiv.org/abs/2601.02399", "authors": ["Jiaxin Ai", "Yukang Feng", "Fanrui Zhang", "Jianwen Sun", "Zizhen Li", "Chuanhao Li", "Yifan Chang", "Wenxiao Wu", "Ruoxi Wang", "Mingliang Zhai", "Kaipeng Zhang"], "title": "ProSoftArena: Benchmarking Hierarchical Capabilities of Multimodal Agents in Professional Software Environments", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Multimodal agents are making rapid progress on general computer-use tasks, yet existing benchmarks remain largely confined to browsers and basic desktop applications, falling short in professional software workflows that dominate real-world scientific and industrial practice. To close this gap, we introduce ProSoftArena, a benchmark and platform specifically for evaluating multimodal agents in professional software environments. We establish the first capability hierarchy tailored to agent use of professional software and construct a benchmark of 436 realistic work and research tasks spanning 6 disciplines and 13 core professional applications. To ensure reliable and reproducible assessment, we build an executable real-computer environment with an execution-based evaluation framework and uniquely incorporate a human-in-the-loop evaluation paradigm. Extensive experiments show that even the best-performing agent attains only a 24.4\\% success rate on L2 tasks and completely fails on L3 multi-software workflow. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents in professional software settings. This project is available at: https://prosoftarena.github.io.", "AI": {"tldr": "ProSoftArena是一个评估多模态代理在专业软件环境中能力的基准和平台。", "motivation": "当前基准测试集中在浏览器和基本桌面应用上，忽略了现实世界中科学研究和工业实践中占据主导地位的专业软件工作流程。因此需要一个专门针对此类环境设计的新基准。", "method": "建立了首个面向代理使用专业软件的能力层次结构，并构建了一个包含436个实际工作任务的基准。该平台还引入了可执行的真实计算机环境，基于执行框架评估方法和人机交互评价方案。", "result": "实验表明，在L2任务中表现最好的代理仅达到了24.4%的成功率；而在多软件工作流程的L3任务中则完全失败。", "conclusion": "该项目为解决现有代理在专业软件环境中的局限性提供了宝贵的见解，指出了更有效的设计原则，并为进一步构建更强大的代理奠定了基础。"}}
{"id": "2601.02398", "pdf": "https://arxiv.org/pdf/2601.02398", "abs": "https://arxiv.org/abs/2601.02398", "authors": ["S. Zhang", "M. Feizarefi", "A. F. Mirzaei"], "title": "AI-Native Integrated Sensing and Communications for Self-Organizing Wireless Networks: Architectures, Learning Paradigms, and System-Level Design", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Integrated Sensing and Communications (ISAC) is emerging as a foundational paradigm for next-generation wireless networks, enabling communication infrastructures to simultaneously support data transmission and environment sensing. By tightly coupling radio sensing with communication functions, ISAC unlocks new capabilities for situational awareness, localization, tracking, and network adaptation. At the same time, the increasing scale, heterogeneity, and dynamics of future wireless systems demand self-organizing network intelligence capable of autonomously managing resources, topology, and services. Artificial intelligence (AI), particularly learning-driven and data-centric methods, has become a key enabler for realizing this vision. This survey provides a comprehensive and system-level review of AI-native ISAC-enabled self-organizing wireless networks. We develop a unified taxonomy that spans: (i) ISAC signal models and sensing modalities, (ii) network state abstraction and perception from sensing-aware radio data, (iii) learning-driven self-organization mechanisms for resource allocation, topology control, and mobility management, and (iv) cross-layer architectures integrating sensing, communication, and network intelligence. We further examine emerging learning paradigms, including deep reinforcement learning, graph-based learning, multi-agent coordination, and federated intelligence that enable autonomous adaptation under uncertainty, mobility, and partial observability. Practical considerations such as sensing-communication trade-offs, scalability, latency, reliability, and security are discussed alongside representative evaluation methodologies and performance metrics. Finally, we identify key open challenges and future research directions toward deployable, trustworthy, and scalable AI-native ISAC systems for 6G and beyond.", "AI": {"tldr": "本文综述了AI原生的集成感知与通信(ISAC)支持的自组织无线网络，涵盖了信号模型、感知模态、学习驱动的自我组织机制以及跨层架构等方面。", "motivation": "为了实现下一代无线网络中同时支持数据传输和环境感知的基础性范式，ISAC应运而生。随着未来无线系统的规模扩大、异构性和动态性的增强，需要能够自主管理资源、拓扑和服务的自组织网络智能。人工智能(AI)已成为实现这一愿景的关键驱动力。", "method": "本文提供了AI原生的集成感知与通信(ISAC)支持的自组织无线网络的系统级综述，并发展了一种统一分类方法，涵盖了ISAC信号模型和感知模态、从感测感知无线电数据中提取网络状态抽象以及学习驱动的资源分配、拓扑控制和移动性管理等。", "result": "本文探讨了包括深度强化学习、图基学习、多代理协调和联邦智能在内的新兴学习范式，这些范式能够在不确定性和部分可观察性的条件下实现自主适应。同时讨论了诸如感测-通信权衡、扩展性、延迟、可靠性和安全性等实践考虑因素。", "conclusion": "本文指出了部署、可信和可伸缩的AI原生ISAC系统的关键开放挑战以及6G及以后的研究方向。"}}
{"id": "2601.02397", "pdf": "https://arxiv.org/pdf/2601.02397", "abs": "https://arxiv.org/abs/2601.02397", "authors": ["Alireza Rezaee"], "title": "Evolutionary Algorithms for Computing Nash Equilibria in Dynamic Games", "categories": ["cs.NE", "cs.GT", "cs.MA"], "comment": null, "summary": "Dynamic nonzero sum games are widely used to model multi agent decision making in control, economics, and related fields. Classical methods for computing Nash equilibria, especially in linear quadratic settings, rely on strong structural assumptions and become impractical for nonlinear dynamics, many players, or long horizons, where multiple local equilibria may exist. We show through examples that such methods can fail to reach the true global Nash equilibrium even in relatively small games. To address this, we propose two population based evolutionary algorithms for general dynamic games with linear or nonlinear dynamics and arbitrary objective functions: a co evolutionary genetic algorithm and a hybrid genetic algorithm particle swarm optimization scheme. Both approaches search directly over joint strategy spaces without restrictive assumptions and are less prone to getting trapped in local Nash equilibria, providing more reliable approximations to global Nash solutions.", "AI": {"tldr": "提出两种基于种群的进化算法来计算动态游戏中的纳什均衡", "motivation": "传统方法在处理非线性动力学、多玩家或长时间框架时存在局限，可能导致无法找到真正的全局纳什均衡。", "method": "提出了协同演化遗传算法和混合遗传算法粒子群优化方案，直接搜索联合策略空间而不受限制假设的影响。", "result": "这两种方法避免陷入局部纳什均衡，能提供更可靠的全局纳什解近似值。", "conclusion": "所提出的方法在解决动态游戏中的纳什均衡计算问题上具有更高的可靠性和适用性。"}}
{"id": "2601.02392", "pdf": "https://arxiv.org/pdf/2601.02392", "abs": "https://arxiv.org/abs/2601.02392", "authors": ["Mo Chen"], "title": "Self-Supervised Masked Autoencoders with Dense-Unet for Coronary Calcium Removal in limited CT Data", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, in Chinese language, 2 figures", "summary": "Coronary calcification creates blooming artifacts in Computed Tomography Angiography (CTA), severely hampering the diagnosis of lumen stenosis. While Deep Convolutional Neural Networks (DCNNs) like Dense-Unet have shown promise in removing these artifacts via inpainting, they often require large labeled datasets which are scarce in the medical domain. Inspired by recent advancements in Masked Autoencoders (MAE) for 3D point clouds, we propose \\textbf{Dense-MAE}, a novel self-supervised learning framework for volumetric medical data. We introduce a pre-training strategy that randomly masks 3D patches of the vessel lumen and trains the Dense-Unet to reconstruct the missing geometry. This forces the encoder to learn high-level latent features of arterial topology without human annotation. Experimental results on clinical CTA datasets demonstrate that initializing the Calcium Removal network with our MAE-based weights significantly improves inpainting accuracy and stenosis estimation compared to training from scratch, specifically in few-shot scenarios.", "AI": {"tldr": "论文提出了一种用于从有限CT数据中移除冠状钙化的自监督学习框架Dense-MAE。", "motivation": "冠状钙化在计算机断层扫描血管造影中会产生影响诊断的伪影，而深度卷积神经网络在去除这些伪影时通常需要大量的标注数据，这在医疗领域较为稀缺。因此作者提出了一个自监督学习框架来解决这一问题。", "method": "引入了预训练策略，在3D体积中随机遮挡血管腔，并通过密集U-Net重建丢失的几何结构以生成高阶隐含特征，无需人工注释。", "result": "实验证明在临床CTA数据集上，使用MAE权重初始化钙去除网络显著提高了图像修复精度和狭窄评估准确性，特别是在少量样本场景下表现优异。", "conclusion": "Dense-MAE框架可以有效地减少冠状钙化的伪影，并提高诊断的准确性。"}}
{"id": "2601.02391", "pdf": "https://arxiv.org/pdf/2601.02391", "abs": "https://arxiv.org/abs/2601.02391", "authors": ["Zhaojiang Lin", "Yong Xu", "Kai Sun", "Jing Zheng", "Yin Huang", "Surya Teja Appini", "Krish Narang", "Renjie Tao", "Ishan Kapil Jain", "Siddhant Arora", "Ruizhi Li", "Yiteng Huang", "Kaushik Patnaik", "Wenfang Xu", "Suwon Shon", "Yue Liu", "Ahmed A Aly", "Anuj Kumar", "Florian Metze", "Xin Luna Dong"], "title": "WearVox: An Egocentric Multichannel Voice Assistant Benchmark for Wearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Wearable devices such as AI glasses are transforming voice assistants into always-available, hands-free collaborators that integrate seamlessly with daily life, but they also introduce challenges like egocentric audio affected by motion and noise, rapid micro-interactions, and the need to distinguish device-directed speech from background conversations. Existing benchmarks largely overlook these complexities, focusing instead on clean or generic conversational audio. To bridge this gap, we present WearVox, the first benchmark designed to rigorously evaluate voice assistants in realistic wearable scenarios. WearVox comprises 3,842 multi-channel, egocentric audio recordings collected via AI glasses across five diverse tasks including Search-Grounded QA, Closed-Book QA, Side-Talk Rejection, Tool Calling, and Speech Translation, spanning a wide range of indoor and outdoor environments and acoustic conditions. Each recording is accompanied by rich metadata, enabling nuanced analysis of model performance under real-world constraints. We benchmark leading proprietary and open-source speech Large Language Models (SLLMs) and find that most real-time SLLMs achieve accuracies on WearVox ranging from 29% to 59%, with substantial performance degradation on noisy outdoor audio, underscoring the difficulty and realism of the benchmark. Additionally, we conduct a case study with two new SLLMs that perform inference with single-channel and multi-channel audio, demonstrating that multi-channel audio inputs significantly enhance model robustness to environmental noise and improve discrimination between device-directed and background speech. Our results highlight the critical importance of spatial audio cues for context-aware voice assistants and establish WearVox as a comprehensive testbed for advancing wearable voice AI research.", "AI": {"tldr": "穿戴设备如AI眼镜将语音助手转变为全天候、手自由的协作工具，但它们带来了诸如自我中心音频受运动和噪声影响等问题。WearVox是首个评估语音助手中的真实可穿戴场景的基准。", "motivation": "当前的基准测试忽略了由穿戴设备带来的复杂性，如自我中心音频受影响于动作和噪音等。为解决此问题，提出了WearVox。", "method": "WearVox收集了3842个跨五项任务（包括搜索定向问答、封闭式问答、旁白拒绝、工具调用及语音翻译）的多通道自我中心音频记录，并评估了领先专有和开源语音大语言模型的表现。", "result": "大多数实时SLLM在WearVox上的准确率介于29%至59%，表明了基准测试的真实性和挑战性。案例研究表明，使用多通道音频输入可显著增强环境噪声下的模型鲁棒性，并提高对设备定向与背景语音的区分。", "conclusion": "研究结果强调了空间音频线索对于上下文感知语音助手的重要性，并将WearVox确立为推动穿戴式语音AI研究的重要测试平台。"}}
{"id": "2601.02386", "pdf": "https://arxiv.org/pdf/2601.02386", "abs": "https://arxiv.org/abs/2601.02386", "authors": ["Hanyang Yuan", "Ning Tang", "Tongya Zheng", "Jiarong Xu", "Xintong Hu", "Renhong Huang", "Shunyu Liu", "Jiacong Hu", "Jiawei Chen", "Mingli Song"], "title": "Tree of Preferences for Diversified Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency.", "AI": {"tldr": "本文提出了一种利用大型语言模型（LLM）来发现未被充分探索的用户偏好，从而提供多样化推荐的方法。", "motivation": "现有的推荐方法主要依赖于观察到的用户反馈来推断用户的多样性偏好。然而，这种数据偏差可能导致未能捕捉到用户的深层次偏好，从而导致推荐结果不够多样。", "method": "本文提出了一种新颖的数据结构——偏好树（ToP），该结构从粗粒度到细粒度地建模了用户偏好。通过这个结构，大型语言模型能够系统性地推理出用户行为背后的理由，并揭示其未被充分探索的偏好。此外，为了生成多样化的推荐结果，本文采取了一种数据驱动的方法，识别符合用户偏好的候选项目，并创建合成交互以反映这些未被充分探索的偏好。", "result": "实验结果显示，该方法在大多数情况下优于现有方法，并在其他情况下接近最优性能，同时具有合理的推理延迟。", "conclusion": "本文提出了一种有效利用大型语言模型来发现和应用用户深层次偏好的新方法，从而改善了推荐系统的多样性和相关性。"}}
{"id": "2601.02385", "pdf": "https://arxiv.org/pdf/2601.02385", "abs": "https://arxiv.org/abs/2601.02385", "authors": ["Mohammed Mallik", "Guillaume Villemaud"], "title": "Base Station Deployment under EMF constrain by Deep Reinforcement learning", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "As 5G networks rapidly expand and 6G technologies emerge, characterized by dense deployments, millimeter-wave communications, and dynamic beamforming, the need for scalable simulation tools becomes increasingly critical. These tools must support efficient evaluation of key performance metrics such as coverage and radio-frequency electromagnetic field (RF-EMF) exposure, inform network design decisions, and ensure compliance with safety regulations. Moreover, base station (BS) placement is a crucial task in the network design, where satisfying coverage requirements is essential. To address these, based on our previous work, we first propose a conditional generative adversarial network (cGAN) that predicts location specific received signal strength (RSS), and EMF exposure simultaneously from the network topology, as images. As a network designing application, we propose a Deep Q Network (DQN) framework, using the trained cGAN, for optimal base station (BS) deployment in the network. Compared to conventional ray tracing simulations, the proposed cGAN reduces inference and deployment time from several hours to seconds. Unlike a standalone cGAN, which provides static performance maps, the proposed GAN-DQN framework enables sequential decision making under coverage and exposure constraints, learning effective deployment strategies that directly solve the BS placement problem. Thus making it well suited for real time design and adaptation in dynamic scenarios in order to satisfy pre defined network specific heterogeneous performance goals.", "AI": {"tldr": "该论文提出了一种基于深度强化学习的方法，用于在满足电磁场（RF-EMF）约束的情况下优化基站部署。", "motivation": "随着5G网络的迅速扩展和6G技术的发展，密集部署、毫米波通信和动态波束成形等特征使得需要高效的仿真工具来评估性能指标。该论文旨在提出一种新的方法以满足这些需求，特别是在基站布置时要同时考虑覆盖率和EMF暴露。", "method": "利用条件生成对抗网络（cGAN）从网络拓扑预测信号强度和EMF暴露，并通过深度Q网络(DQN)框架实现基于此模型的实时动态场景下的优化部署策略。", "result": "相比传统的光线追踪模拟，该方法能够将推理和部署时间显著减少到几秒级别。此外，新提出的GAN-DQN框架支持在覆盖及曝光限制下进行序列决策，从而学习有效的部署策略来直接解决基站放置问题。", "conclusion": "通过引入cGAN与DQN的结合使用，论文提供了一种能够实现实时设计和适应动态场景的方法，并能满足特定网络性能目标。"}}
{"id": "2601.02382", "pdf": "https://arxiv.org/pdf/2601.02382", "abs": "https://arxiv.org/abs/2601.02382", "authors": ["Nathan Conger", "Nathan Scollar", "Kemal Davaslioglu", "Yalin E. Sagduyu", "Sastry Kompella"], "title": "How to Discover Knowledge for FutureG: Contextual RAG and LLM Prompting for O-RAN", "categories": ["cs.NI", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "We present a retrieval-augmented question answering framework for 5G/6G networks, where the Open Radio Access Network (O-RAN) has become central to disaggregated, virtualized, and AI-driven wireless systems. While O-RAN enables multi-vendor interoperability and cloud-native deployments, its fast-changing specifications and interfaces pose major challenges for researchers and practitioners. Manual navigation of these complex documents is labor-intensive and error-prone, slowing system design, integration, and deployment. To address this challenge, we adopt Contextual Retrieval-Augmented Generation (Contextual RAG), a strategy in which candidate answer choices guide document retrieval and chunk-specific context to improve large language model (LLM) performance. This improvement over traditional RAG achieves more targeted and context-aware retrieval, which improves the relevance of documents passed to the LLM, particularly when the query alone lacks sufficient context for accurate grounding. Our framework is designed for dynamic domains where data evolves rapidly and models must be continuously updated or redeployed, all without requiring LLM fine-tuning. We evaluate this framework using the ORANBenchmark-13K dataset, and compare three LLMs, namely, Llama3.2, Qwen2.5-7B, and Qwen3.0-4B, across both Direct Question Answering (Direct Q&A) and Chain-of-Thought (CoT) prompting strategies. We show that Contextual RAG consistently improves accuracy over standard RAG and base prompting, while maintaining competitive runtime and CO2 emissions. These results highlight the potential of Contextual RAG to serve as a scalable and effective solution for domain-specific Q&A in ORAN and broader 5G/6G environments, enabling more accurate interpretation of evolving standards while preserving efficiency and sustainability.", "AI": {"tldr": "本文提出了一种用于5G/6G网络的基于O-RAN的检索增强问答框架，旨在解决动态领域中数据快速变化和模型持续更新或重新部署的问题。", "motivation": "为了应对O-RAN快速变化的标准和接口带来的挑战，该论文采用Contextual RAG策略来改善大型语言模型的表现，通过候选答案引导文档检索以提高相关性。", "method": "本文采用了Contextual Retrieval-Augmented Generation（Contextual RAG）框架，并使用ORANBenchmark-13K数据集对Llama3.2、Qwen2.5-7B和Qwen3.0-4B这三种大型语言模型进行评估，比较了Direct Q&A和Chain-of-Thought (CoT)两种提示策略。", "result": "实验结果表明，Contextual RAG在准确性上优于传统RAG和基础提示方式，同时保持了竞争性的运行时间和CO2排放量。", "conclusion": "论文结论指出，Contextual RAG框架可以作为解决特定领域Q&A问题的可扩展有效解决方案，在O-RAN以及更广泛的5G/6G环境中提高对不断变化标准的理解准确性的同时保持效率和可持续性。"}}
{"id": "2601.02380", "pdf": "https://arxiv.org/pdf/2601.02380", "abs": "https://arxiv.org/abs/2601.02380", "authors": ["Elchanan Mossel"], "title": "The Refutability Gap: Challenges in Validating Reasoning by Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": "he authors explicitly reserve all rights in this work. No permission is granted for the reproduction, storage, or use of this document for the purpose of training artificial intelligence systems or for text and data mining (TDM), including but not limited to the generation of embeddings, summaries, or synthetic derivatives", "summary": "Recent reports claim that Large Language Models (LLMs) have achieved the ability to derive new science and exhibit human-level general intelligence. We argue that such claims are not rigorous scientific claims, as they do not satisfy Popper's refutability principle (often termed falsifiability), which requires that scientific statements be capable of being disproven. We identify several methodological pitfalls in current AI research on reasoning, including the inability to verify the novelty of findings due to opaque and non-searchable training data, the lack of reproducibility caused by continuous model updates, and the omission of human-interaction transcripts, which obscures the true source of scientific discovery. Additionally, the absence of counterfactuals and data on failed attempts creates a selection bias that may exaggerate LLM capabilities. To address these challenges, we propose guidelines for scientific transparency and reproducibility for research on reasoning by LLMs. Establishing such guidelines is crucial for both scientific integrity and the ongoing societal debates regarding fair data usage.", "AI": {"tldr": "本文讨论了大型语言模型（LLM）在科学推理方面的能力验证存在的问题，并提出了提高科学研究透明度和可重复性的建议。", "motivation": "作者认为当前对大型语言模型能力的评估存在方法学上的缺陷，未能满足科学验证的基本要求——证伪性。这些缺陷包括无法验证发现的新颖性、缺乏再现性和数据使用公平性的争议等问题。", "method": "文章没有具体的方法描述，而是通过分析现有研究中的问题来提出建议，如训练数据不透明和不可搜索的问题以及模型更新频繁导致的可重复性挑战等。", "result": "未直接给出实验结果。但是强调了需要建立科学透明度和可重复性的指南以改进当前的研究方法。", "conclusion": "为了确保科学研究的完整性并解决与公平数据使用有关的社会争论，必须确立针对大型语言模型推理研究的具体指导原则。"}}
{"id": "2601.02379", "pdf": "https://arxiv.org/pdf/2601.02379", "abs": "https://arxiv.org/abs/2601.02379", "authors": ["Nolan B. Gutierrez", "William J. Beksi"], "title": "Movement Primitives in Robotics: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": "105 pages, 3 figures, and 6 tables", "summary": "Biological systems exhibit a continuous stream of movements, consisting of sequential segments, that allow them to perform complex tasks in a creative and versatile fashion. This observation has led researchers towards identifying elementary building blocks of motion known as movement primitives, which are well-suited for generating motor commands in autonomous systems, such as robots. In this survey, we provide an encyclopedic overview of movement primitive approaches and applications in chronological order. Concretely, we present movement primitive frameworks as a way of representing robotic control trajectories acquired through human demonstrations. Within the area of robotics, movement primitives can encode basic motions at the trajectory level, such as how a robot would grasp a cup or the sequence of motions necessary to toss a ball. Furthermore, movement primitives have been developed with the desirable analytical properties of a spring-damper system, probabilistic coupling of multiple demonstrations, using neural networks in high-dimensional systems, and more, to address difficult challenges in robotics. Although movement primitives have widespread application to a variety of fields, the goal of this survey is to inform practitioners on the use of these frameworks in the context of robotics. Specifically, we aim to (i) present a systematic review of major movement primitive frameworks and examine their strengths and weaknesses; (ii) highlight applications that have successfully made use of movement primitives; and (iii) examine open questions and discuss practical challenges when applying movement primitives in robotics.", "AI": {"tldr": "综述了机器人运动原语方法和应用的全面概述，按时间顺序进行。", "motivation": "介绍生物系统中的动作序列如何启发研究人员识别出适用于自动生成机器人控制命令的基本运动构建块。", "method": "以时间为序，系统性地回顾主要的运动原语框架，并探讨其优缺点；列举成功利用运动原语的应用案例；并讨论在机器人领域应用运动原语时面临的实际挑战和开放问题。", "result": "提供了全面的、时间顺序的运动原语方法及应用综述，旨在为研究者提供详尽的信息。", "conclusion": "强调了运动原语理论框架对机器人领域的贡献，并指出未来可能的研究方向和实践中的难点。"}}
{"id": "2601.02378", "pdf": "https://arxiv.org/pdf/2601.02378", "abs": "https://arxiv.org/abs/2601.02378", "authors": ["Biyuan Liu", "Daigang Xu", "Lei Jiang", "Wenjun Guo", "Ping Chen"], "title": "Modeling the Mental World for Embodied AI: A Comprehensive Review", "categories": ["cs.RO"], "comment": null, "summary": "As the application of Embodied AI Agents in avatars, wearable devices, and robotic systems continues to deepen, their core research challenges have gradually shifted from physical environment interaction to the accurate understanding of social interactions. Traditional physical world models (PWM) focus on quantifiable physical attributes such as space and motion, failing to meet the needs of social intelligence modeling. In contrast, the Mental World Model (MWM), as a structured representation of humans' internal mental states, has become the critical cognitive foundation for embodied agents to achieve natural human-machine collaboration and dynamic social adaptation. However, current MWM research faces significant bottlenecks: such as fragmented conceptual framework with vague boundaries between MWM and PWM, disjointed reasoning mechanisms for the technical pathways and applicable scenarios of different Theory of Mind (ToM) reasoning paradigms, and detachment between evaluation and practice. To address these issues, this review systematically synthesizes over 100 authoritative studies to provide a comprehensive overview of MWM research for embodied AI. Its core contributions are threefold: First, it constructs a complete theoretical framework for MWM for the first time. Specifically, it distinguishes the essential differences between MWM and PWMs. Second, it systematically defines the key components of MWM through two paradigms for mental element representation. Third, it comprehensively analyzes two core ToM reasoning paradigms with 19 ToM methods. Finally, it also clarifies the integration trend of neuro-symbolic hybrid architectures, and synthesizes 26 ToM evaluation benchmarks. This work aims to promote the integration of embodied agents into human society and advance the in-depth development of human-machine collaborative interaction.", "AI": {"tldr": "构建一个全面的心理世界模型框架，以促进具身AI与人类社会的融合。", "motivation": "传统物理世界模型无法满足社交智能建模的需求，当前心理世界模型研究存在诸多瓶颈，包括理论框架不完整、推理机制脱节等问题。", "method": "系统综述超过100篇权威研究论文，构建了一个完整的心理世界模型理论框架，并定义了MWM的关键组件以及两个核心的ToM推理范式。", "result": "提出了一个全面的心理世界模型框架，明确了心理世界模型与物理世界模型的区别，定义了两种心理元素表示方式及其关键组成部分；分析了19种不同的ToM方法和26个评估基准。", "conclusion": "促进了具身AI在人类社会中的应用和发展，推进了人机协作互动的深入研究。"}}
{"id": "2601.02377", "pdf": "https://arxiv.org/pdf/2601.02377", "abs": "https://arxiv.org/abs/2601.02377", "authors": ["Xinyu Huang", "Shyam Karthick V B", "Taozhao Chen", "Mitch Bryson", "Thomas Chaffey", "Huaming Chen", "Kim-Kwang Raymond Choo", "Ian R. Manchester"], "title": "Trust in LLM-controlled Robotics: a Survey of Security Threats, Defenses and Challenges", "categories": ["cs.RO"], "comment": null, "summary": "The integration of Large Language Models (LLMs) into robotics has revolutionized their ability to interpret complex human commands and execute sophisticated tasks. However, such paradigm shift introduces critical security vulnerabilities stemming from the ''embodiment gap'', a discord between the LLM's abstract reasoning and the physical, context-dependent nature of robotics. While security for text-based LLMs is an active area of research, existing solutions are often insufficient to address the unique threats for the embodied robotic agents, where malicious outputs manifest not merely as harmful text but as dangerous physical actions. In this work, we present a systematic survey, summarizing the emerging threat landscape and corresponding defense strategies for LLM-controlled robotics. Specifically, we discuss a comprehensive taxonomy of attack vectors, covering topics such as jailbreaking, backdoor attacks, and multi-modal prompt injection. In response, we analyze and categorize a range of defense mechanisms, from formal safety specifications and runtime enforcement to multi-LLM oversight and prompt hardening. Furthermore, we review key datasets and benchmarks used to evaluate the robustness of these embodied systems. By synthesizing current research, this work highlights the urgent need for context-aware security solutions and provides a foundational roadmap for the development of safe, secure, and reliable LLM-controlled robotics.", "AI": {"tldr": "本文对大型语言模型（LLM）控制的机器人中出现的安全威胁、防御策略及挑战进行了系统性综述。", "motivation": "随着大型语言模型在机器人中的应用，安全漏洞随之显现。这些漏洞源于LLM的抽象推理与机器人的物理现实之间的差距，导致恶意输出可能引发危险行为。", "method": "本文提出了一种全面攻击向量分类，涵盖破解、后门攻击和多模态提示注入等主题，并分析了包括正式安全规范、运行时执行以及多模型监控在内的防御机制。此外还回顾了一些用于评估这些系统鲁棒性的关键数据集和基准。", "result": "综述表明现有解决方案往往不足以应对机器人特有的威胁，提出了亟待开发上下文感知的安全方案以确保LLM控制的机器人的安全性。", "conclusion": "本文总结了当前研究，并为开发安全可靠的LLM驱动型机器人提供了一个基础路线图。"}}
{"id": "2601.02375", "pdf": "https://arxiv.org/pdf/2601.02375", "abs": "https://arxiv.org/abs/2601.02375", "authors": ["Madison Bochard", "Tim Conser", "Alyssa Duran", "Lazaro Martull", "Pu Tian", "Yalong Wu"], "title": "LeafTutor: An AI Agent for Programming Assignment Tutoring", "categories": ["cs.CY", "cs.AI", "cs.SE"], "comment": null, "summary": "High enrollment in STEM-related degree programs has created increasing demand for scalable tutoring support, as universities experience a shortage of qualified instructors and teaching assistants (TAs). To address this challenge, LeafTutor, an AI tutoring agent powered by large language models (LLMs), was developed to provide step-by-step guidance for students. LeafTutor was evaluated through real programming assignments. The results indicate that the system can deliver step-by-step programming guidance comparable to human tutors. This work demonstrates the potential of LLM-driven tutoring solutions to enhance and personalize learning in STEM education. If any reader is interested in collaboration with our team to improve or test LeafTutor, please contact Pu Tian (pu.tian@stockton.edu) or Yalong Wu (wuy@uhcl.edu).", "AI": {"tldr": "该论文介绍了LeafTutor，一个基于大型语言模型的AI编程导师系统。", "motivation": "为了应对STEM相关课程中学生数量增加和合格教师短缺的问题，开发了一个能够提供个性化编程指导的AI导师。", "method": "利用大型语言模型创建了一个名为LeafTutor的AI导师来为学生提供分步骤的编程指引，并通过真实编程作业进行评估。", "result": "实验结果显示，该系统可以提供与人类导师相媲美的编程辅导。", "conclusion": "这项工作展示了LLM驱动的教育解决方案在STEM领域中增强和个性化学习方面的潜力。"}}
{"id": "2601.02371", "pdf": "https://arxiv.org/pdf/2601.02371", "abs": "https://arxiv.org/abs/2601.02371", "authors": ["Samuele Marro", "Alan Chan", "Xinxing Ren", "Lewis Hammond", "Jesse Wright", "Gurjyot Wanga", "Tiziano Piccardi", "Nuno Campos", "Tobin South", "Jialin Yu", "Alex Pentland", "Philip Torr", "Jiaxin Pei"], "title": "Permission Manifests for Web Agents", "categories": ["cs.CY", "cs.AI", "cs.MA", "cs.NI"], "comment": "Authored by the Lightweight Agent Standards Working Group https://las-wg.org/", "summary": "The rise of Large Language Model (LLM)-based web agents represents a significant shift in automated interactions with the web. Unlike traditional crawlers that follow simple conventions, such as robots.txt, modern agents engage with websites in sophisticated ways: navigating complex interfaces, extracting structured information, and completing end-to-end tasks. Existing governance mechanisms were not designed for these capabilities. Without a way to specify what interactions are and are not allowed, website owners increasingly rely on blanket blocking and CAPTCHAs, which undermine beneficial applications such as efficient automation, convenient use of e-commerce services, and accessibility tools. We introduce agent-permissions.json, a robots.txt-style lightweight manifest where websites specify allowed interactions, complemented by API references where available. This framework provides a low-friction coordination mechanism: website owners only need to write a simple JSON file, while agents can easily parse and automatically implement the manifest's provisions. Website owners can then focus on blocking non-compliant agents, rather than agents as a whole. By extending the spirit of robots.txt to the era of LLM-mediated interaction, and complementing data use initiatives such as AIPref, the manifest establishes a compliance framework that enables beneficial agent interactions while respecting site owners' preferences.", "AI": {"tldr": "本文提出了一个名为agent-permissions.json的轻量级文件，用于网站指定允许的交互方式，并提供API引用。该框架为现代Web代理和网站所有者之间提供了协调机制。", "motivation": "现有治理机制无法处理新型LLM驱动的Web代理与网站之间的复杂互动。这导致了过度使用屏蔽和验证码策略，阻碍了自动化、电子商务服务和辅助工具等有益应用的发展。", "method": "引入了一个类似于robots.txt的manifest文件（agent-permissions.json），该文件由网站所有者创建以指定允许的操作，并且可以包含API引用。代理能够解析并自动遵循这些规定。", "result": "通过提供低摩擦的协调机制，agent-permissions.json使网站所有者能够专注于阻止不合规行为，而不是全面屏蔽代理。同时，它还支持数据使用倡议如AIPref。", "conclusion": "该框架为LLM驱动交互的时代扩展了robots.txt的精神，并建立了一个既促进有益代理互动又尊重网站所有者偏好的合规性框架。"}}
{"id": "2601.02368", "pdf": "https://arxiv.org/pdf/2601.02368", "abs": "https://arxiv.org/abs/2601.02368", "authors": ["Ruibing Wang", "Shuhan Guo", "Haotong Du", "Quanming Yao"], "title": "Distillation-based Scenario-Adaptive Mixture-of-Experts for the Matching Stage of Multi-scenario Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Multi-scenario recommendation is pivotal for optimizing user experience across diverse contexts. While Multi-gate Mixture-of-Experts (MMOE) thrives in ranking, its transfer to the matching stage is hindered by the blind optimization inherent to independent two-tower architectures and the parameter dominance of head scenarios. To address these structural and distributional bottlenecks, we propose Distillation-based Scenario-Adaptive Mixture-of-Experts (DSMOE). Specially, we devise a Scenario-Adaptive Projection (SAP) module to generate lightweight, context-specific parameters, effectively preventing expert collapse in long-tail scenarios. Concurrently, we introduce a cross-architecture knowledge distillation framework, where an interaction-aware teacher guides the two-tower student to capture complex matching patterns. Extensive experiments on real-world datasets demonstrate DSMOE's superiority, particularly in significantly improving retrieval quality for under-represented, data-sparse scenarios.", "AI": {"tldr": "提出了一种基于蒸馏的场景自适应专家混合模型（DSMOE），用于多情景推荐中的匹配阶段，以优化用户体验。", "motivation": "为了克服多门混合专家（MMOE）在排名阶段的优势无法转移到匹配阶段的问题，解决独立双塔架构中存在的盲目优化以及头部场景参数主导导致的瓶颈问题。", "method": "设计了一种场景自适应投影模块(SAP)，生成轻量级、上下文特定的参数以防止长尾场景中专家模型崩溃；同时引入一种跨架构知识蒸馏框架，交互感知教师指导双塔学生捕获复杂的匹配模式。", "result": "在真实世界数据集上的广泛实验表明，DSMOE显著提高了稀疏和代表性不足的情景中的检索质量。", "conclusion": "通过创新性的场景自适应投影模块和跨架构知识蒸馏框架，DSMOE有效解决了多情景推荐中匹配阶段的挑战，并且在提高长尾场景检索精度方面表现出色。"}}
{"id": "2601.02367", "pdf": "https://arxiv.org/pdf/2601.02367", "abs": "https://arxiv.org/abs/2601.02367", "authors": ["Despoina Antonakaki", "Sotiris Ioannidis"], "title": "Cross-Platform Digital Discourse Analysis of the Israel-Hamas Conflict: Sentiment, Topics, and Event Dynamics", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.SI"], "comment": null, "summary": "The Israeli-Palestinian conflict remains one of the most polarizing geopolitical issues, with the October 2023 escalation intensifying online debate. Social media platforms, particularly Telegram, have become central to real-time news sharing, advocacy, and propaganda. In this study, we analyze Telegram, Twitter/X, and Reddit to examine how conflict narratives are produced, amplified, and contested across different digital spheres. Building on our previous work on Telegram discourse during the 2023 escalation, we extend the analysis longitudinally and cross-platform using an updated dataset spanning October 2023 to mid-2025. The corpus includes more than 187,000 Telegram messages, 2.1 million Reddit comments, and curated Twitter/X posts. We combine Latent Dirichlet Allocation (LDA), BERTopic, and transformer-based sentiment and emotion models to identify dominant themes, emotional dynamics, and propaganda strategies. Telegram channels provide unfiltered, high-intensity documentation of events; Twitter/X amplifies frames to global audiences; and Reddit hosts more reflective and deliberative discussions. Our findings reveal persistent negative sentiment, strong coupling between humanitarian framing and solidarity expressions, and platform-specific pathways for the diffusion of pro-Palestinian and pro-Israeli narratives. This paper offers three contributions: (1) a multi-platform, FAIR-compliant dataset on the Israel-Hamas war, (2) an integrated pipeline combining topic modeling, sentiment and emotion analysis, and spam filtering for large-scale conflict discourse, and (3) empirical insights into how platform affordances and affective publics shape the evolution of digital conflict communication.", "AI": {"tldr": "该论文通过分析Telegram，Twitter/X和Reddit上的以色列-哈马斯冲突的数字话语，揭示了不同平台上的冲突叙事如何产生、放大和被质疑。", "motivation": "研究旨在理解以色列-巴勒斯坦冲突在社交媒体中的传播方式，并探讨跨平台如何影响这些信息的扩散。通过分析不同平台的特点，论文试图提供对这种复杂国际关系中情感动态和叙述策略的理解。", "method": "利用Latent Dirichlet Allocation (LDA)，BERTopic以及基于变压器的情感和情绪模型结合，从超过187,000条Telegram消息、2.1百万Reddit评论及Twitter/X上的精选帖子中识别出主导主题、情感动态和宣传策略。", "result": "研究揭示了持续负面情绪的存在，人道主义框架与团结表达之间的强烈联系，并展示了平台特定路径如何促进亲巴勒斯坦或亲以色列叙述的传播。", "conclusion": "该论文贡献了一个多平台、FAIR合规的数据集关于以色列-哈马斯战争；提供了一种结合主题建模、情感和情绪分析以及垃圾邮件过滤的大规模冲突话语综合流程，并为数字冲突通信如何演变提供了实证见解。"}}
{"id": "2601.02366", "pdf": "https://arxiv.org/pdf/2601.02366", "abs": "https://arxiv.org/abs/2601.02366", "authors": ["Yiwen Chen", "Yiqing Wu", "Huishi Luo", "Fuzhen Zhuang", "Deqing Wang"], "title": "TextBridgeGNN: Pre-training Graph Neural Network for Cross-Domain Recommendation via Text-Guided Transfer", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Graph-based recommendation has achieved great success in recent years. The classical graph recommendation model utilizes ID embedding to store essential collaborative information. However, this ID-based paradigm faces challenges in transferring to a new domain, making it hard to build a pre-trained graph recommendation model. This phenomenon primarily stems from two inherent challenges: (1) the non-transferability of ID embeddings due to isolated domain-specific ID spaces, and (2) structural incompatibility between heterogeneous interaction graphs across domains. To address these issues, we propose TextBridgeGNN, a pre-training and fine-tuning framework that can effectively transfer knowledge from a pre-trained GNN to downstream tasks. We believe the key lies in how to build the relationship between domains. Specifically, TextBridgeGNN uses text as a semantic bridge to connect domains through multi-level graph propagation. During the pre-training stage, textual information is utilized to break the data islands formed by multiple domains, and hierarchical GNNs are designed to learn both domain-specific and domain-global knowledge with text features, ensuring the retention of collaborative signals and the enhancement of semantics. During the fine-tuning stage, a similarity transfer mechanism is proposed. This mechanism initializes ID embeddings in the target domain by transferring from semantically related nodes, successfully transferring the ID embeddings and graph pattern. Experiments demonstrate that TextBridgeGNN outperforms existing methods in cross-domain, multi-domain, and training-free settings, highlighting its ability to integrate Pre-trained Language Model (PLM)-driven semantics with graph-based collaborative filtering without costly language model fine-tuning or real-time inference overhead.", "AI": {"tldr": "本文提出了TextBridgeGNN，一种通过文本引导的转移学习框架来解决跨域推荐问题的方法。", "motivation": "传统的基于图的推荐模型在新领域应用时面临ID嵌入非可迁移性及异构交互图结构不兼容的问题。", "method": "TextBridgeGNN利用文本作为语义桥连接不同领域，通过多层次图传播打破数据孤岛，并设计分层GNN学习特定域和全局知识。此外，在微调阶段提出了相似度转移机制以初始化目标领域的ID嵌入。", "result": "实验表明，TextBridgeGNN在跨域、多域推荐任务中优于现有方法，展示了其结合预训练语言模型驱动的语义与图基协同过滤的能力。", "conclusion": "本文提出的框架成功解决了传统图推荐模型在新领域的迁移难题，并且能够有效整合PLM驱动的语义和基于图的协同过滤。"}}
{"id": "2601.02365", "pdf": "https://arxiv.org/pdf/2601.02365", "abs": "https://arxiv.org/abs/2601.02365", "authors": ["Tushar Vatsa", "Vibha Belavadi", "Priya Shanmugasundaram", "Suhas Suresha", "Dewang Sultania"], "title": "FUSE : Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "ICDM MMSR 2025: Workshop on Multimodal Search and Recommendations", "summary": "Multimodal creative assistants decompose user goals and route tasks to subagents for layout, styling, retrieval, and generation. Retrieval quality is pivotal, yet failures can arise at several stages: understanding user intent, choosing content types, finding candidates (recall), or ranking results. Meanwhile, sending and processing images is costly, making naive multimodal approaches impractical. We present FUSE: Failure-aware Usage of Subagent Evidence for MultiModal Search and Recommendation. FUSE replaces most raw-image prompting with a compact Grounded Design Representation (GDR): a selection aware JSON of canvas elements (image, text, shape, icon, video, logo), structure, styles, salient colors, and user selection provided by the Planner team. FUSE implements seven context budgeting strategies: comprehensive baseline prompting, context compression, chain-of-thought reasoning, mini-shot optimization, retrieval-augmented context, two-stage processing, and zero-shot minimalism. Finally, a pipeline attribution layer monitors system performance by converting subagent signals into simple checks: intent alignment, content-type/routing sanity, recall health (e.g., zero-hit and top-match strength), and ranking displacement analysis. We evaluate the seven context budgeting variants across 788 evaluation queries from diverse users and design templates (refer Figure 3). Our systematic evaluation reveals that Context Compression achieves optimal performance across all pipeline stages, with 93.3% intent accuracy, 86.8% routing success(with fallbacks), 99.4% recall, and 88.5% NDCG@5. This approach demonstrates that strategic context summarization outperforms both comprehensive and minimal contextualization strategies.", "AI": {"tldr": "FUSE是一种用于多模态搜索和推荐的失败感知子代理证据使用方法，通过紧凑的Grounded Design Representation替换大部分原始图像提示。", "motivation": "为了提高检索质量和降低多模态创意助手中的成本，同时避免理解用户意图、选择内容类型等方面的故障。", "method": "FUSE采用七种上下文预算策略：全面基线提示、上下文压缩、链式思考推理、迷你镜头优化、增强检索的上下文、两阶段处理和零镜头简约。通过转换子代理信号为简单的检查，监控系统的性能表现。", "result": "在788个评估查询中，上下文压缩策略获得了最佳性能，在所有管道阶段分别达到了93.3％意图准确率，86.8％路由成功率（包括备选方案），99.4％召回率和88.5％NDCG@5。", "conclusion": "这种方法显示了战略性的上下文总结优于全面或最小化的上下文策略。"}}
{"id": "2601.02364", "pdf": "https://arxiv.org/pdf/2601.02364", "abs": "https://arxiv.org/abs/2601.02364", "authors": ["Chung Park", "Taesan Kim", "Hyeongjun Yun", "Dongjoon Hong", "Junui Hong", "Kijung Park", "MinCheol Cho", "Mira Myong", "Jihoon Oh", "Min sung Choi"], "title": "Towards Trustworthy LLM-Based Recommendation via Rationale Integration", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted at RS4SD'25 (CIKM'25 Workshop)", "summary": "Traditional recommender systems (RS) have been primarily optimized for accuracy and short-term engagement, often overlooking transparency and trustworthiness. Recently, platforms such as Amazon and Instagram have begun providing recommendation rationales to users, acknowledging their critical role in fostering trust and enhancing engagement; however, most existing systems still treat them as post-hoc artifacts. We propose an LLM-based recommender (LLM-Rec) that not only predicts items but also generates logically grounded rationales. Our approach leverages a self-annotated rationale dataset and instruction tuning in a rationale-first format, where the model generates an explanation before outputting the recommended item. By adopting this strategy and representing rationales in a chain-of-thought (CoT) style, LLM-Rec strengthens both interpretability and recommendation performance. Experiments on the Fashion and Scientific domains of the Amazon Review dataset demonstrate significant improvements over well-established baselines. To encourage reproducibility and future research, we publicly release a rationale-augmented recommendation dataset containing user histories, rationales, and recommended items.", "AI": {"tldr": "提出了一种基于大语言模型的推荐系统，该系统不仅能预测项目还能生成逻辑支持的理由。", "motivation": "传统的推荐系统主要优化准确性和短期参与度，忽视了透明度和信任。本研究旨在通过提供推荐理由来增强用户对系统的信任，并提高互动。", "method": "采用自注释的推理数据集及指令调整，在一种‘理由优先’的形式下训练模型，即先生成解释再输出推荐项目。该方法使用链式思维表示法以加强可解释性和推荐性能。", "result": "在亚马逊评论数据库中的时尚和科学领域实验结果表明，提出的系统相比现有基准有显著提升。", "conclusion": "通过提供理由增强推荐系统的透明度和信任度，从而提高了用户参与度。研究展示了大语言模型用于生成逻辑支持的理由的有效性，并发布了增强的推理数据集以促进可重复性和后续研究。"}}
{"id": "2601.02363", "pdf": "https://arxiv.org/pdf/2601.02363", "abs": "https://arxiv.org/abs/2601.02363", "authors": ["Laura Aymerich-Franch", "Tarek Taha", "Hiroko Kamide", "Takahiro Miyashita", "Hiroshi Ishiguro", "Paolo Dario"], "title": "Acceptance of cybernetic avatars for capability enhancement: a large-scale survey", "categories": ["cs.HC", "cs.CY"], "comment": "22 pages, 1 Table, 1 Figure", "summary": "Avatar embodiment experiences have the potential to enhance human capabilities by extending human senses, body, and mind. This study investigates social acceptance of robotic and virtual avatars as enablers of capability enhancement in six domains: identity exploration, well-being and behavioral transformation, expanded travel capabilities, expanded bodily and sensory abilities, cognitive augmentation, and immortality. We conducted a large-scale survey (n = 1001) in Dubai to explore acceptance of sixteen capability enhancement scenarios within these domains. The highest levels of agreement were observed for multilingual communication (77.5%) and learning capabilities (68.7%), followed by assisting individuals with reduced mobility (64.5%) and behavioral transformation (59.5%). Scenarios involving immortality through consciousness transfer received the least support (34.9%). These findings contribute to a deeper understanding of public attitudes toward avatar-based human enhancement and offer practical guidance for the responsible design, development, and integration of cybernetic avatars in the society, ensuring their societal acceptance and fostering a harmonious human-avatar coexistence.", "AI": {"tldr": "研究通过大规模调查探讨了社会对增强人类能力的机器人和虚拟化身接受程度。", "motivation": "为了理解公众对于基于化身的人类提升的态度，并为负责设计、开发和融合赛博格化身提供实际指导，促进和谐共存。", "method": "进行了涵盖六个领域的十六个能力提升场景的大规模调查（n = 1001），地点在迪拜。", "result": "结果显示，多语言交流和支持行动受限个体的方案最被接受；而涉及意识转移实现永生的情境则支持度最低。", "conclusion": "研究深入理解了公众对基于化身的人类提升的态度，并为设计、开发和融合赛博格化身提供了指导。"}}
{"id": "2601.02362", "pdf": "https://arxiv.org/pdf/2601.02362", "abs": "https://arxiv.org/abs/2601.02362", "authors": ["Itzhak Ziv", "Moshe Unger", "Hilah Geva"], "title": "The Impact of LLM-Generated Reviews on Recommender Systems: Textual Shifts, Performance Effects, and Strategic Platform Control", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The rise of generative AI technologies is reshaping content-based recommender systems (RSes), which increasingly encounter AI-generated content alongside human-authored content. This study examines how the introduction of AI-generated reviews influences RS performance and business outcomes. We analyze two distinct pathways through which AI content can enter RSes: user-centric, in which individuals use AI tools to refine their reviews, and platform-centric, in which platforms generate synthetic reviews directly from structured metadata. Using a large-scale dataset of hotel reviews from TripAdvisor, we generate synthetic reviews using LLMs and evaluate their impact across the training and deployment phases of RSes. We find that AI-generated reviews differ systematically from human-authored reviews across multiple textual dimensions. Although both user- and platform-centric AI reviews enhance RS performance relative to models without textual data, models trained on human reviews consistently achieve superior performance, underscoring the quality of authentic human data. Human-trained models generalize robustly to AI content, whereas AI-trained models underperform on both content types. Furthermore, tone-based framing strategies (encouraging, constructive, or critical) substantially enhance platform-generated review effectiveness. Our findings highlight the strategic importance of platform control in governing the generation and integration of AI-generated reviews, ensuring that synthetic content complements recommendation robustness and sustainable business value.", "AI": {"tldr": "研究探讨了LLM生成的评论对推荐系统性能和商业结果的影响，分析了用户和个人平台两种生成方式，并通过实验对比发现人类撰写的评论优于AI生成的评论。", "motivation": "随着生成式人工智能技术的发展，内容为基础的推荐系统开始面对更多由AI生成的内容。研究旨在探讨这些新生成内容对推荐系统的性能及商业结果的影响，强调了平台在治理和整合这种合成数据中的战略性作用。", "method": "通过使用TripAdvisor上酒店评论的大规模数据集，在用户和个人平台两种模式下生成合成评论，并评估其对训练期与部署阶段的推荐系统性能影响。比较了人类撰写评论与AI生成评论的区别以及不同语气策略的影响。", "result": "发现AI生成的评论在多个文本维度上有别于人工撰写的评论，两者都能提升RS性能，但基于人类评论的数据模型始终表现更佳。此外，以鼓励、建设性或批判性的基调来制定策略可以显著增强平台生成评论的有效性。", "conclusion": "研究强调了推荐系统中合成数据的质量和治理的重要性，并揭示了战略控制在确保推荐系统的稳健性和可持续商业价值中的关键作用。"}}
{"id": "2601.02359", "pdf": "https://arxiv.org/pdf/2601.02359", "abs": "https://arxiv.org/abs/2601.02359", "authors": ["Kaede Shiohara", "Toshihiko Yamasaki", "Vladislav Golyanik"], "title": "ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors", "categories": ["cs.CV"], "comment": "17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/", "summary": "Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.", "AI": {"tldr": "提出了一种基于自监督的扩散模型，用于生成表情序列，并通过个性化训练来检测人脸伪造。", "motivation": "当前最先进的方法难以泛化到未知的人脸伪造技术，因为它们依赖于特定的伪造样本进行监督训练。而自我监督的方法则有望提供更广泛的适应性，但现有的工作很难仅凭自我监督学习出有区分性的表示。", "method": "通过使用音频生成表情序列的扩散模型，并通过个性化训练来计算疑似视频与目标人物的身份距离，从而实现对人脸伪造检测。", "result": "实验结果表明，在DF-TIMIT, DFDCP, KoDF和IDForge等数据集上，该方法在平均AUC上的表现优于先前的最佳方法4.22个百分点，并且能够有效检测Sora2生成的视频，同时具有较高的鲁棒性以应对模糊、压缩等情况。", "conclusion": "提出的方法提供了一种有效的自我监督学习策略，用于人脸伪造检测。该方法不仅能泛化到未知的人脸伪造技术，还展现了强大的实用性和可靠性。"}}
{"id": "2601.02358", "pdf": "https://arxiv.org/pdf/2601.02358", "abs": "https://arxiv.org/abs/2601.02358", "authors": ["Junyi Chen", "Tong He", "Zhoujie Fu", "Pengfei Wan", "Kun Gai", "Weicai Ye"], "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "categories": ["cs.CV"], "comment": "Project page: https://sotamak1r.github.io/VINO-web/", "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "AI": {"tldr": "VINO是一个统一的视觉生成器，可以在单一框架中执行图像和视频的生成与编辑。", "motivation": "当前任务特定模型或独立模块难以实现跨模态通用性，因此提出一种共享扩散骨干的设计来整合文本、图像和视频条件输入，以支持广泛的视觉创作和编辑任务。", "method": "VINO结合了视觉-语言模型（VLM）与多模态扩散变压器（MMDiT），其中多模态输入被编码为交错的条件令牌，并用于引导扩散过程。设计避免了特定于模式的架构组件，通过逐步训练管道将视频生成基础模型扩展到统一的多功能生成器。", "result": "在多样化的生成和编辑基准上，VINO展示了强大的视觉质量、忠实的任务遵循性、改进的参考保留和更可控的身份编辑。", "conclusion": "研究结果强调了一条实现可扩展统一视觉生成的实际路径，并指出了交错上下文计算作为通用视觉创作基础架构的价值。"}}
{"id": "2601.02357", "pdf": "https://arxiv.org/pdf/2601.02357", "abs": "https://arxiv.org/abs/2601.02357", "authors": ["Trey Brosnan"], "title": "DARC: Drum accompaniment generation with fine-grained rhythm control", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.", "AI": {"tldr": "DARC是一种生成鼓伴奏的模型，能够在考虑音乐上下文的同时实现细粒度节奏控制。", "motivation": "当前的音乐生成工具在快速原型制作中表现不佳，难以同时提供结构控制和风格灵活性。现有的方法要么缺乏对节奏的精细控制，要么无法根据音乐背景进行调整。", "method": "DARC通过参数高效的微调技术，在STAGE的基础上增加了细粒度的节奏控制功能，并保持了对音乐上下文的理解能力。", "result": "结果表明，DARC能够在生成鼓伴奏时提供更细致的节奏控制，同时保留原有的音乐背景信息。", "conclusion": "DARC为用户提供了一种新的工具，使得在考虑音乐上下文的同时能够实现精细的节奏控制。"}}
{"id": "2601.02356", "pdf": "https://arxiv.org/pdf/2601.02356", "abs": "https://arxiv.org/abs/2601.02356", "authors": ["Jing Tan", "Zhaoyang Zhang", "Yantao Shen", "Jiarui Cai", "Shuo Yang", "Jiajun Wu", "Wei Xia", "Zhuowen Tu", "Stefano Soatto"], "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes", "categories": ["cs.CV"], "comment": "Project page: https://sparkstj.github.io/talk2move", "summary": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.", "AI": {"tldr": "本文介绍了Talk2Move，一种基于强化学习的扩散框架，用于通过自然语言指令对场景中的对象进行几何变换。", "motivation": "现有的文本指导的方法可以调整物体的外观或风格，但在执行对象级别的几何变换（如平移、旋转或缩放）方面存在困难。这些方法受限于稀疏配对监督和像素级优化限制。", "method": "Talk2Move采用Group Relative Policy Optimization (GRPO) 来探索通过输入图像和轻量文本变化生成的多样rollout的几何动作，不需要昂贵的配对数据。同时，通过空间奖励引导模型将几何变换与语言描述对齐，并利用离策略步骤评估和主动步采样提高学习效率。", "result": "实验表明Talk2Move在精准性、一致性和语义忠实度方面超越了现有的文本指导编辑方法，在空间准确性和场景一致性上表现更佳。", "conclusion": "本文提出的方法能够实现精确且连贯的对象变换，是未来自然语言指导下执行对象级别几何操作的一个有力工具。"}}
{"id": "2601.02353", "pdf": "https://arxiv.org/pdf/2601.02353", "abs": "https://arxiv.org/abs/2601.02353", "authors": ["Shahnawaz Alam", "Mohammed Mudassir Uddin", "Mohammed Kaif Pasha"], "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\\% while maintaining 92.3\\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.", "AI": {"tldr": "提出了一种结合神经网络剪枝和少量样本学习的方法，用于在边缘设备上快速准确地识别植物疾病。", "motivation": "为了解决偏远地区农民缺乏实验室或高性能计算资源的问题，并减少收集大量标注病害图像的成本和时间。", "method": "提出了Disease-Aware Channel Importance Scoring（DACIS）方法，在一个三阶段的剪枝-元学习-再剪枝（PMP）流程中，用于识别网络中的关键部分并进行模型压缩。", "result": "在PlantVillage和PlantDoc数据集上实验表明，该方法可以将模型大小减少78％，同时保持92.3％的原始精度，并且压缩后的模型可以在Raspberry Pi 4上以每秒7帧的速度运行。", "conclusion": "所提出的方法使实时现场诊断对于小规模农民成为可能。"}}
{"id": "2601.02347", "pdf": "https://arxiv.org/pdf/2601.02347", "abs": "https://arxiv.org/abs/2601.02347", "authors": ["Ishani Karmarkar", "Liam O'Carroll", "Aaron Sidford"], "title": "Solving Matrix Games with Even Fewer Matrix-Vector Products", "categories": ["math.OC", "cs.DS", "cs.GT"], "comment": null, "summary": "We study the problem of computing an $ε$-approximate Nash equilibrium of a two-player, bilinear, zero-sum game with a bounded payoff matrix $A \\in \\mathbb{R}^{m \\times n}$, when the players' strategies are constrained to lie in simple sets. We provide algorithms which solve this problem in $\\tilde{O}(ε^{-2/3})$ matrix-vector multiplies (matvecs) in two well-studied cases: $\\ell_1$-$\\ell_1$ games, where the players' strategies are both in the probability simplex, and $\\ell_2$-$\\ell_1$ games, where the players' strategies are in the unit Euclidean ball and probability simplex respectively. These results improve upon the previous state-of-the-art complexities of $\\tilde{O}(ε^{-8/9})$ for $\\ell_1$-$\\ell_1$ and of $\\tilde{O}(ε^{-7/9})$ for $\\ell_2$-$\\ell_1$ due to [KOS '25]. In particular, our result for $\\ell_2$-$\\ell_1$, which corresponds to hard-margin support vector machines (SVMs), matches the lower bound of [KS '25] up to polylogarithmic factors.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.02346", "pdf": "https://arxiv.org/pdf/2601.02346", "abs": "https://arxiv.org/abs/2601.02346", "authors": ["Falcon LLM Team", "Iheb Chaabane", "Puneesh Khanna", "Suhail Mohmad", "Slim Frikha", "Shi Hu", "Abdalgader Abubaker", "Reda Alami", "Mikhail Lubinets", "Mohamed El Amine Seddik", "Hakim Hacid"], "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "categories": ["cs.AI"], "comment": null, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "AI": {"tldr": "Falcon-H1R是一种参数优化的推理模型，旨在通过小规模语言模型实现竞争力强的推理性能。", "motivation": "提高推理效率和准确性的同时减少模型大小以降低成本", "method": "使用精心策划的数据集、高效的微调和强化学习策略进行训练，并结合混合并行架构设计以提升推理速度", "result": "Falcon-H1R在各种推理密集型基准测试中表现出色，与参数量为2倍至7倍的大模型相比，性能相当或更好。同时展示了卓越的实时扩展效率。", "conclusion": "通过精巧的设计和训练策略，小规模的语言模型如Falcon-H1R能够在保持高准确度的同时提供强大的推理能力和成本效益。"}}
{"id": "2601.02339", "pdf": "https://arxiv.org/pdf/2601.02339", "abs": "https://arxiv.org/abs/2601.02339", "authors": ["Jingming He", "Chongyi Li", "Shiqi Wang", "Sam Kwong"], "title": "Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.", "AI": {"tldr": "本文提出了一种联合提升框架，用于改进3D语义高斯建模。", "motivation": "现有方法在处理3D语义和渲染任务时通常将两者分开，并且仅依赖2D监督而忽略3D几何信息。此外，当前的适应策略主要依据渲染梯度调整高斯集合，在某些区域可能不足。", "method": "引入了一种各向异性3D高斯Chebyshev描述符来捕捉细粒度的3D形状细节，并结合局部语义和形状信号自适应地调节高斯分配和球谐函数，同时采用跨场景知识转移模块。", "result": "实验结果显示，在多个数据集上改进了分割精度和渲染质量，同时保持了较高的渲染帧率。", "conclusion": "该方法通过联合优化3D语义建模的两个分支，提升了模型的表现，并且在新场景中能够快速收敛并维持强大的形状表征。"}}
{"id": "2601.02329", "pdf": "https://arxiv.org/pdf/2601.02329", "abs": "https://arxiv.org/abs/2601.02329", "authors": ["Laurent Caraffa"], "title": "BEDS: Bayesian Emergent Dissipative Structures", "categories": ["cs.CV"], "comment": "19 pages", "summary": "We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence. We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems. As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.", "AI": {"tldr": "提出BEDS理论框架，统一了非平衡热力学、贝叶斯推理、信息几何和机器学习的概念，并探讨了熵释放与结构形成的关系。", "motivation": "探索物理系统、生物系统和计算系统的共同点：通过熵的转化实现从流到结构的学习过程。利用普里戈金的耗散结构理论，建立热力学过程与贝叶斯更新之间的正式同构关系。", "method": "提出BEDS框架，结合非平衡态热力学、贝叶斯推理及信息几何学原理，推导出学习系统必须遵循的耗散模式，并通过最小公理验证基本数学常数作为贝叶斯推断固定点出现的可能性。将哥德尔不完备定理与热力学约束联系起来。", "result": "设计了一种基于BEDS原则的P2P网络架构，在能源效率上比现有分布式共识系统提高了六个数量级，同时支持持续学习。", "conclusion": "该工作连接了基础物理、数学逻辑和实际系统设计，提供了对计算与学习本质的理解，并提出实现可持续人工智能的具体路径。"}}
{"id": "2601.02318", "pdf": "https://arxiv.org/pdf/2601.02318", "abs": "https://arxiv.org/abs/2601.02318", "authors": ["Roja Sahoo", "Anoop Namboodiri"], "title": "Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching", "categories": ["cs.CV"], "comment": "15 pages, 8 figures, 5 tables. Submitted to ICPR 2026", "summary": "Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).", "AI": {"tldr": "提出了一种融合闪光和非闪光接触式指纹图像的框架Fusion2Print，以提高接触式指纹识别性能。", "motivation": "旨在解决接触式指纹图像由于光照变化、皮下肤色差异等因素导致的脊线清晰度降低问题，同时减少噪声并保持脊线对比度。", "method": "通过构建一个定制化的闪光和非闪光配对数据库，并使用轻量级注意力融合网络来整合两种模式，突出有用信息并抑制噪声。然后使用U-Net增强模块生成优化加权的灰度图像，最后用跨域兼容性深度嵌入模型生成适用于接触式和非接触式的指纹识别。", "result": "实验结果表明Fusion2Print在提高脊线清晰度方面优于单一采集基准（AUC=0.999, EER=1.12%），验证了该方法的有效性。", "conclusion": "提出了有效的闪光和非闪光接触式指纹图像融合技术，显著提升了识别性能，并为未来研究提供了新的思路。"}}
{"id": "2601.02316", "pdf": "https://arxiv.org/pdf/2601.02316", "abs": "https://arxiv.org/abs/2601.02316", "authors": ["Siddharth Joshi", "Haoli Yin", "Rishabh Adiga", "Ricardo Monti", "Aldo Carranza", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Parth Doshi", "Paul Burstein", "Pratyush Maini", "Scott Loftin", "Spandan Das", "et al. (6 additional authors not shown)"], "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.", "AI": {"tldr": "提出了评估基础模型的新标准，开发了改进的视觉语言模型（VLM）评估框架DatBench。", "motivation": "现有VLM评估方法存在不准确、效率低等问题。希望通过更严格的评估方法促进研究进展。", "method": "识别并修正了现有评估方法中的不足，并通过转换和过滤现存数据集，提高了评估的准确性与效率。", "result": "开发出了新的评估框架DatBench-Full以及具有更强区分度和更高效率的子集DatBench。这些改进显著提升了模型评估的质量。", "conclusion": "本文的工作为未来VLM的发展提供了一种更严谨、可持续性的评估方法，有助于推动该领域研究的进步。"}}
{"id": "2601.02315", "pdf": "https://arxiv.org/pdf/2601.02315", "abs": "https://arxiv.org/abs/2601.02315", "authors": ["Saurabh Kaushik", "Lalit Maurya", "Beth Tellman"], "title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping", "categories": ["cs.CV"], "comment": "Accepted at CV4EO Workshop @ WACV 2026", "summary": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}", "AI": {"tldr": "本文提出了一种新的模型Prithvi-CAFE，用于提高洪水淹没地图绘制的效果。", "motivation": "现有的Geo-Foundation Models（GFMs）在使用Sen1Flood11数据集进行洪水映射时无法超越基线U-Net，表明它们难以捕捉关键的局部细节。因此，提出了Prithvi-CAFE模型以解决这一问题。", "method": "通过将预训练的Prithvi GFM编码器与增强有Convolutional Attention Modules（CAM）的并行CNN残差分支集成，实现了快速有效的微调和多尺度、多层次融合，从而捕捉关键局部细节同时保持长期依赖性。", "result": "在Sen1Flood11和FloodPlanet数据集上，Prithvi-CAFE均取得了最先进的结果。例如，在Sen1Flood11测试数据中，Prithvi-CAFE的IoU达到了83.41，优于其他模型；在FloodPlanet上也超过了基线U-Net和其他GFMs。", "conclusion": "提出的简单而有效的Prithvi-CAFE展示了增强分割任务中的多通道、多模态数据互补信息以及关键局部细节能力的潜力。"}}
{"id": "2601.02314", "pdf": "https://arxiv.org/pdf/2601.02314", "abs": "https://arxiv.org/abs/2601.02314", "authors": ["Sourena Khanzadeh"], "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.", "AI": {"tldr": "该项目提出了Ariadne项目，一个使用结构因果模型和反事实逻辑来审计大型语言模型代理的推理过程的方法。", "motivation": "随着大型语言模型在高风险决策中的应用越来越多，其透明性成为一个重要的安全问题。现有的解释方法不能有效检测出模型生成的人类可读理由是否真实反映了其决定过程。", "method": "Ariadne项目利用结构因果模型和反事实逻辑进行硬干预（如逆向推理节点），测量终端答案的因果敏感度，以评估代理决策的因果完整性。", "result": "在对最先进的模型的实证评估中发现了一致的“忠实性差距”，其中代理存在高达0.77违反密度的情况。这表明其逻辑理由与实际决定过程不符。", "conclusion": "当前的大型语言模型解释结构倾向于提供误导性的推理，而不是真实的决策过程说明，提出了Ariadne评分作为评估这种一致性的新标准。"}}
{"id": "2601.02311", "pdf": "https://arxiv.org/pdf/2601.02311", "abs": "https://arxiv.org/abs/2601.02311", "authors": ["Deep Pankajbhai Mehta"], "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies", "categories": ["cs.DC", "cs.AI"], "comment": "8 pages, 3 tables", "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.", "AI": {"tldr": "本文提出了一种放置语义框架，用于分析分布式深度学习中的并行策略。", "motivation": "目前在训练大型语言模型时选择并行化策略（如数据、张量、管道和ZeRO）是通过试错来实现的，缺乏统一且系统的预测方法。", "method": "通过定义每个策略如何放置四个训练状态（参数、优化器、梯度和激活）于设备上，从放置本身而非具体的实施方案中推导出内存消耗与通信量。证明了两个必要条件（梯度完整性、状态一致性），并提供了组合规则以安全地结合不同策略。", "result": "预测结果与已发表的结果一致：ZeRO-3在使用8倍少的内存时，其通信成本为数据并行性的1.5倍。", "conclusion": "本文框架统一了ZeRO阶段1-3、完全分片的数据并行性（FSDP）、张量并行性和管道并行性的实例选择。"}}
{"id": "2601.02309", "pdf": "https://arxiv.org/pdf/2601.02309", "abs": "https://arxiv.org/abs/2601.02309", "authors": ["Xiaopeng Guo", "Yinzhe Xu", "Huajian Huang", "Sai-Kit Yeung"], "title": "360DVO: Deep Visual Odometry for Monocular 360-Degree Camera", "categories": ["cs.CV"], "comment": "12 pages. Received by RA-L", "summary": "Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage", "AI": {"tldr": "本文提出了一种基于深度学习的全方位视觉里程计框架，用于提高单目全方位相机在复杂环境下的鲁棒性和精度。", "motivation": "现有方法依赖于手工特征或光度目标函数，在剧烈运动和变化光照等挑战性场景下缺乏鲁棒性。为解决这一问题，本文提出了一种基于深度学习的全向视觉里程计框架。", "method": "该方法引入了畸变感知球面特征提取器（DAS-Feat），能够自适应地从360度图像中学习到抗畸变特性，并利用这些稀疏特征块在新的全方位可微束调整模块中建立约束，实现有效的姿态估计。", "result": "实验结果表明，相较于最先进的基线方法（包括360VO和OpenVSLAM），所提出的方法提高了50%的鲁棒性和37.5%的精度。", "conclusion": "本文展示了首个基于深度学习的全向视觉里程计框架，并通过新的真实世界基准测试证明了其优越性。"}}
{"id": "2601.02299", "pdf": "https://arxiv.org/pdf/2601.02299", "abs": "https://arxiv.org/abs/2601.02299", "authors": ["Sara Inácio", "Hugo Proença", "João C. Neves"], "title": "SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting", "categories": ["cs.CV"], "comment": "9 pages", "summary": "The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.", "AI": {"tldr": "SortWaste数据集用于工业废物分拣中的物体检测，旨在推动自动化废物管理的发展。", "motivation": "当前人工废物分类效率低下且存在健康风险；现有的自动系统难以处理高变异性、堆积和视觉复杂性的实际废物流。缺乏真实世界的数据集阻碍了这些系统的开发。", "method": "创建SortWaste数据集，提供密集注释的物体检测样本，并提出ClutterScore作为衡量场景难度的标准指标。", "result": "使用标准对象检测模型评估性能，在仅塑料检测任务中达到59.7%的mAP，但在高度混乱的场景下表现显著下降。", "conclusion": "尽管取得了一些成果，但现有数据集和方法在处理复杂废物分类挑战方面仍有不足。需要更具有挑战性的新数据集来推动这一领域的进一步研究和发展。"}}
{"id": "2601.02295", "pdf": "https://arxiv.org/pdf/2601.02295", "abs": "https://arxiv.org/abs/2601.02295", "authors": ["Chenyang Ma", "Guangyu Yang", "Kai Lu", "Shitong Xu", "Bill Byrne", "Niki Trigoni", "Andrew Markham"], "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding", "categories": ["cs.RO"], "comment": "Project Page: https://dannymcy.github.io/cyclevla/", "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/", "AI": {"tldr": "CycleVLA是一种能够预见并纠正机器人执行过程中的潜在失败的系统。", "motivation": "现有的机器人故障检测和修正工作通常采用事后处理的方式，只在错误发生后进行分析和修改。CycleVLA旨在通过前瞻性的自我纠正能力来改进这个问题。", "method": "CycleVLA集成了一种进度感知的Vision-Language-Action模型（VLAs），能够在关键子任务过渡点标记潜在失败，并结合一个基于视觉语言模型的故障预测器与规划器，以及一种基于最小贝叶斯风险（MBR）解码策略来提高重试成功率。", "result": "CycleVLA在对训练良好和不足的Vision-Language-Action模型进行实验时都表现出更好的性能。同时证明了MBR作为零样本测试时间缩放策略的有效性。", "conclusion": "通过前瞻性的自我纠正机制，CycleVLA能够有效提高任务的成功率，并且为Vision-Language-Action模型提供了一种有效的零样本测试时间缩放方法。"}}
{"id": "2601.02289", "pdf": "https://arxiv.org/pdf/2601.02289", "abs": "https://arxiv.org/abs/2601.02289", "authors": ["Tom Burgert", "Leonard Hackel", "Paolo Rota", "Begüm Demir"], "title": "Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery", "categories": ["cs.CV"], "comment": "accepted for publication at IEEE/CVF Winter Conference on Applications of Computer Vision", "summary": "Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.", "AI": {"tldr": "本文提出了一种新的基于等级的地理正则化方法GeoRank，用于对比自我监督学习以改善多光谱遥感图像中的地理关系。", "motivation": "传统对比自我监督学习在处理多光谱遥感图像时面临挑战，因为这些图像具有地理和时间上的变异性。通过引入直接优化球面距离的新型正则化方法GeoRank来增强特征空间中地理关系的嵌入。", "method": "提出了一种新的基于等级的地理正则化方法GeoRank，用于对比自我监督学习。这种方法通过优化球面距离将地理信息融入到特征学习过程中，并进行了数据增强、图像大小和时间视图影响的系统研究。", "result": "GeoRank在多种对比自监督算法中表现优异或持平，并且改进了其他包含地理元数据的方法。", "conclusion": "基于等级的地理正则化方法GeoRank有效提升了多光谱遥感图像的对比自我监督学习性能，其代码已在GitHub上公开。"}}
{"id": "2601.02285", "pdf": "https://arxiv.org/pdf/2601.02285", "abs": "https://arxiv.org/abs/2601.02285", "authors": ["Tobias Schimanski", "Imene Kolli", "Yu Fan", "Ario Saeid Vaghefi", "Jingwei Ni", "Elliott Ash", "Markus Leippold"], "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).", "AI": {"tldr": "本文介绍了pdfQA，这是一个包含2000个真实标注和2000个合成的问答对的数据集，旨在评估PDF文档中的问题回答系统。", "motivation": "当前的QA数据集主要来源于文本源或特定领域，缺乏多样性和挑战性。为了填补这一空白，本文创建了pdfQA，以涵盖更多复杂维度的问题，并测试各种技能和优化策略。", "method": "该研究构建了一个包含真实PDF标注和合成问答对的数据集，这些对涵盖了十个不同的复杂度维度。然后应用质量及难度筛选器，选取有效的问答对并用开源的大语言模型进行回答测试。", "result": "实验显示了pdfQA数据集中问题与现有挑战的相关性，并且证明该数据集可以作为端到端的QA系统评估的基础。", "conclusion": "通过构建包含真实和合成问答对的数据集，本文提供了新的机会来研究PDF中的复杂信息抽取及回答问题的能力。"}}
{"id": "2601.02281", "pdf": "https://arxiv.org/pdf/2601.02281", "abs": "https://arxiv.org/abs/2601.02281", "authors": ["Shuai Yuan", "Yantai Yang", "Xiaotian Yang", "Xupeng Zhang", "Zhonghao Zhao", "Lingming Zhang", "Zhipeng Zhang"], "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "categories": ["cs.CV"], "comment": null, "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "AI": {"tldr": "本文提出了一种名为InfiniteVGGT的视觉几何变换器，旨在解决大规模实时三维视觉几何理解中的长序列稳定性问题。", "motivation": "现有的离线模型如VGGT虽然在几何能力方面表现出色，但其批次特性使其无法适用于实时系统。流式架构尽管是为了解决这一难题而设计的，却仍存在信息漂移等问题，不能支持无限时间尺度输入或在长时间序列中保持稳定性。", "method": "InfiniteVGGT通过引入一种具有可扩展、自适应和持续表达能力的KV缓存机制解决了上述问题。该方法利用这种机制实现了一种无需训练且不受注意力影响的信息修剪策略，有效地“滚动”内存以应对每个新帧。", "result": "实验结果表明，InfiniteVGGT在长序列稳定性方面优于现有流式方法，并能支持无限时间尺度的实时操作。", "conclusion": "为了验证这种系统的能力，作者引入了Long3D基准测试，这为未来关于长期三维几何理解的研究提供了一个严格的评估平台。"}}
{"id": "2601.02273", "pdf": "https://arxiv.org/pdf/2601.02273", "abs": "https://arxiv.org/abs/2601.02273", "authors": ["Salim Khazem"], "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \\textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \\textbf{5.2\\%} of model parameters ($\\sim$4.9M). On the challenging CHASE\\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git", "AI": {"tldr": "本文提出了一种基于低秩适应（LoRA）的拓扑感知参数高效调整框架TopoLoRA-SAM，用于细长结构和跨域二值语义分割任务。", "motivation": "基础分割模型如SAM在大规模预训练下具有强大的零样本泛化能力，但在特定领域中的语义分割中仍存在挑战，特别是在处理细长结构（例如视网膜血管）和噪声模式（例如SAR图像）时。全量微调计算成本高且容易遗忘先前的知识。", "method": "TopoLoRA-SAM将Low-Rank Adaptation (LoRA)注入冻结的ViT编码器中，并添加了一个轻量级的空间卷积适配器以及可选的拓扑感知监督，通过不同的iable clDice实现。", "result": "在五个基准数据集上进行评估后，TopoLoRA-SAM取得了最佳的整体平均Dice系数和视网膜平均Dice分数，同时仅训练了模型参数的5.2%（约490万个参数）。在具有挑战性的CHASE_DB1数据集上，方法显著提高了分割准确性和鲁棒性。", "conclusion": "研究表明，拓扑感知的参数高效调整可以匹配或超过全量微调的专业模型。"}}
{"id": "2601.02270", "pdf": "https://arxiv.org/pdf/2601.02270", "abs": "https://arxiv.org/abs/2601.02270", "authors": ["Gabriel Timothy", "Syeda Amna Rizvi", "Muhammad Umair", "Athman Bouguettaya", "Balsam Alkouz"], "title": "Modeling Inter-drone Interference as a Service in Skyway Networks", "categories": ["cs.ET"], "comment": null, "summary": "We present a novel investigation into the impact of inter-drone interference on delivery efficiencies within multi-drone skyway networks. We conduct controlled experiments to analyze the behavior of drones in an indoor testbed environment. Our study compares performance between solo flights and concurrent multi-drone operations along predefined routes. This analysis captures interference occurring during both flight and at charging stations, providing a comprehensive evaluation of its effects on overall network performance. We conduct a comprehensive series of experiments across diverse scenarios to systematically understand and model the dynamics of inter-drone interference. Key metrics, such as power consumption and delivery times, are considered. This generates a comprehensive dataset for in-depth analysis of interference at both the node and segment levels. These findings are then formalized into a predictive model. The results validate the effectiveness of the developed model, demonstrating its potential to accurately forecast inter-drone interferences.", "AI": {"tldr": "本文研究了多无人机网络中无人机间的干扰对其配送效率的影响，并提出了一种预测模型。", "motivation": "为了提高多无人机系统在空中网络中的性能，理解并减少由无人机间干扰带来的影响是必要的。", "method": "通过控制实验和多样化的场景测试来分析室内环境下单机飞行与多机并发操作时的相互干扰情况，并基于关键指标构建预测模型。", "result": "研究结果表明所建立的模型能准确预测多无人机系统中的相互干扰情况。", "conclusion": "本文的研究揭示了无人机间干扰对网络性能的影响，并提供了一种有效的方法来预测这种干扰，有助于优化未来的无人机配送服务。"}}
{"id": "2601.02267", "pdf": "https://arxiv.org/pdf/2601.02267", "abs": "https://arxiv.org/abs/2601.02267", "authors": ["Renke Wang", "Zhenyu Zhang", "Ying Tai", "Jian Yang"], "title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies", "categories": ["cs.CV"], "comment": "Page: https://wrk226.github.io/DiffProxy.html, Code: https://github.com/wrk226/DiffProxy", "summary": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html", "AI": {"tldr": "提出了一种名为DiffProxy的框架，用于生成多视角一致的人体代理以进行网格恢复。", "motivation": "现有数据集中的标注不准确会影响模型训练效果，而合成数据虽然标注精确但存在域差距问题。为解决这些问题，提出了DiffProxy来增强真实世界中的零样本泛化能力。", "method": "利用扩散生成先验技术生成多视角一致的人体代理；引入手部细化模块提升细节；使用不确定性感知测试时间缩放方法提高鲁棒性。", "result": "在五个真实世界的基准上实现了最先进的性能，特别是在存在遮挡和部分视图的情况下表现尤为出色。", "conclusion": "DiffProxy通过合成数据训练，在多视角人体网格恢复中取得了优秀的结果，并展示了强大的零样本泛化能力。"}}
{"id": "2601.02257", "pdf": "https://arxiv.org/pdf/2601.02257", "abs": "https://arxiv.org/abs/2601.02257", "authors": ["Joel Daniel Andersson", "Palak Jain", "Satchit Sivakumar"], "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization", "categories": ["cs.CR", "cs.DS", "cs.LG"], "comment": null, "summary": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties. We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.", "AI": {"tldr": "Task description failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2601.02256", "pdf": "https://arxiv.org/pdf/2601.02256", "abs": "https://arxiv.org/abs/2601.02256", "authors": ["Shikun Sun", "Liao Qu", "Huichao Zhang", "Yiheng Liu", "Yangyang Song", "Xian Li", "Xu Wang", "Yi Jiang", "Daniel K. Du", "Xinglong Wu", "Jia Jia"], "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://github.com/ByteVisionLab/NextFlow", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "AI": {"tldr": "该论文提出了一种新的框架，通过管理异步策略冲突来增强组相对政策优化（GRPO），以解决视觉自回归生成模型中的问题。", "motivation": "视觉生成领域存在三个主要范式：自回归、扩散和视觉自回归。后者的异构输入结构在生成步骤间产生严重的时间不对齐的策略冲突，特别是在强化学习场景中导致训练不稳定和对齐不足。", "method": "本文提出了一种集成三种协同组件的新框架来解决这一问题，包括一种稳定中间奖励机制、动态时间步长重权方案以及从反馈学习原则推导出的新型掩码传播算法。", "result": "该方法在样本质量和目标对齐方面显著优于基准组相对政策优化（GRPO），证明了其有效性和稳健性。", "conclusion": "通过管理视觉自回归生成模型中的异步策略冲突，所提出的方法实现了更高质量的样本和更好的任务对齐。"}}
{"id": "2601.02253", "pdf": "https://arxiv.org/pdf/2601.02253", "abs": "https://arxiv.org/abs/2601.02253", "authors": ["Emrah Mete", "Emin Erkan Korkmaz"], "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission", "categories": ["cs.LG", "cs.AR", "cs.CV"], "comment": "9 pages, 4 figures", "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.", "AI": {"tldr": "提出了一种基于生物信号传输机制的乘法运算无依赖神经网络架构，以减少AI对昂贵硬件的依赖。", "motivation": "传统的深度学习模型严重依赖于高性能硬件和矩阵乘法运算，这导致了高昂的成本、能源消耗以及供应不足。受生物神经系统启发，旨在开发一种新的无需乘法运算的神经网络架构，以便在边缘设备上部署复杂的人工智能应用。", "method": "通过引入通道宽度来代替传统权重，并使用一个次级参数模拟神经递质来调节信号传输。前向传播仅依赖于加减和位操作（最小值、符号逻辑），完全消除了浮点乘法运算。", "result": "证明了在不使用乘法权重的情况下，新架构可以利用标准反向传播解决非线性可分问题如XOR和多数函数，准确率达到100%。这表明新的神经网络架构具备构建复杂决策边界的潜力。", "conclusion": "此新型的无乘法运算神经网络架构为下一代神经形态硬件提供了高效替代方案，并且能够在普通CPU或超低功耗芯片上运行复杂的模型而不依赖昂贵的GPU集群。"}}
{"id": "2601.02249", "pdf": "https://arxiv.org/pdf/2601.02249", "abs": "https://arxiv.org/abs/2601.02249", "authors": ["Xiantai Xiang", "Guangyao Zhou", "Zixiao Wen", "Wenshuai Li", "Ben Niu", "Feng Wang", "Lijia Huang", "Qiantong Wang", "Yuhan Liu", "Zongxu Pan", "Yuxin Hu"], "title": "SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.", "AI": {"tldr": "SLGNet 是一种利用结构先验和语言引导调制提高多模态物体检测性能的参数高效框架。", "motivation": "当前基于适配器的方法在RGB和红外图像的多模态对象检测中效率高，但牺牲了跨模式结构性的一致性。为了应对这些问题以及复杂动态场景中的限制，SLGNet 被提出以增强环境感知能力并改进性能。", "method": "SLGNet 设计了一种结构感知适配器来提取分层的结构表示，并将其注入冻结的基础模型中；同时提出了语言引导调制模块，利用视觉语言模型生成的结构化描述动态重新校准视觉特征。", "result": "在多个数据集上的实验表明，SLGNet 达到了新的最先进性能，在LLVIP基准测试上实现了66.1 mAP，并减少了约87%的可训练参数。", "conclusion": "SLGNet 是一种既高效又强大的解决方案，适用于多模态感知任务。"}}
{"id": "2601.02246", "pdf": "https://arxiv.org/pdf/2601.02246", "abs": "https://arxiv.org/abs/2601.02246", "authors": ["Annoor Sharara Akhand"], "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.", "AI": {"tldr": "本文比较了在多个视觉数据集上从头训练自定义CNN、使用预训练模型作为特征提取器和迁移学习的方法。", "motivation": "卷积神经网络（CNN）由于其能够从原始像素中学习层次化表示，成为视觉识别的标准方法。实践中，研究人员通常选择三种策略：从头开始训练紧凑的自定义CNN；将大型预训练CNN用作固定特征提取器；通过部分或完全微调预先训练的骨干网进行迁移学习。", "method": "在五个实际图像分类数据集上（包括道路缺陷识别、农业品种识别、水果/叶子疾病识别、行人通道侵入检测和未经授权车辆识别）进行了这些三种方法的控制比较。使用准确率和宏F1得分评估模型，并辅以训练时间每轮和参数计数等效率指标。", "result": "结果显示，迁移学习始终提供最强的预测性能，而自定义CNN在计算和内存预算受限时提供了吸引人的效率-准确性权衡。", "conclusion": "迁移学习在各种数据集上都表现出最好的预测性能。然而，在资源有限的情况下，定制的CNN可以作为一种有效的替代方案。"}}
{"id": "2601.02242", "pdf": "https://arxiv.org/pdf/2601.02242", "abs": "https://arxiv.org/abs/2601.02242", "authors": ["Grigorii Alekseenko", "Aleksandr Gordeev", "Irina Tolstykh", "Bulat Suleimanov", "Vladimir Dokholyan", "Georgii Fedorov", "Sergey Yakubson", "Aleksandra Tsybina", "Mikhail Chernyshov", "Maksim Kuprashevich"], "title": "VIBE: Visual Instruction Based Editor", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.", "AI": {"tldr": "该论文提出了一种基于视觉指令的紧凑型图像编辑管道，使用现代Qwen3-VL模型引导编辑过程，并利用Sana1.5扩散模型生成图像。", "motivation": "现有的开放源代码方法在实际质量方面存在限制，同时主流的大型扩散模型计算成本高昂。因此，论文旨在开发一种低成本、高性能的图像编辑解决方案。", "method": "通过Qwen3-VL模型引导编辑过程，并使用Sana1.5扩散模型生成图像。设计决策涵盖架构、数据处理、训练配置和评估，以实现低推断成本和严格的源一致性，同时保持高质量。", "result": "在ImgEdit和GEdit基准测试中，所提出的模型与参数量更大的基线相比，在性能上相当或更高，并且特别擅长需要保留输入图像的编辑任务。该模型能在24 GB GPU内存内运行，并以每秒不超过4秒的速度生成2K分辨率的编辑图。", "conclusion": "论文展示了VIBE系统在多个基准测试中的优越表现，证明了其在低计算成本条件下保持高质量编辑结果的能力"}}
{"id": "2601.02233", "pdf": "https://arxiv.org/pdf/2601.02233", "abs": "https://arxiv.org/abs/2601.02233", "authors": ["Leon Müller", "Adelina Bärligea", "Alexander Knapp", "Jakob S. Kottmann"], "title": "PauliEngine: High-Performant Symbolic Arithmetic for Quantum Operations", "categories": ["quant-ph", "cs.ET", "cs.SE", "physics.comp-ph"], "comment": ":D.0; E.1", "summary": "Quantum computation is inherently hybrid, and fast classical manipulation of qubit operators is necessary to ensure scalability in quantum software. We introduce PauliEngine, a high-performance C++ framework that provides efficient primitives for Pauli string multiplication, commutators, symbolic phase tracking, and structural transformations. Built on a binary symplectic representation and optimized bit-wise operations, PauliEngine supports both numerical and symbolic coefficients and is accessible through a Python interface. Runtime benchmarks demonstrate substantial speedups over state-of-the-art implementations. PauliEngine provides a scalable backend for operator-based quantum software tools and simulations.", "AI": {"tldr": "本文介绍了PauliEngine，一种用于量子操作高效符号算术的高性能C++框架。", "motivation": "为了确保量子软件在可扩展性方面取得进展，需要快速的经典计算方法来处理比特运算符。因此开发了PauliEngine来解决这一问题。", "method": "基于二元辛表示法和优化后的位操作，PauliEngine实现了高效的Pauli字符串乘法、交换子、符号相位跟踪及结构转换等基本算术操作，并提供了Python接口。", "result": "运行时基准测试表明，相比现有最佳实现方法，PauliEngine在性能上取得了显著提升。", "conclusion": "PauliEngine作为基于操作符的量子软件工具和仿真后端提供了一个可扩展的基础平台。"}}
